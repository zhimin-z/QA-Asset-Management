[
    {
        "Question_id":63188242,
        "Question_title":"AWS SageMaker: Use S3 pickled models instead of hosting on sagemaker",
        "Question_body":"<p>I am working on a use-case for which I have to use Amazon SageMaker notebook instances. Amazon SM resources are filled with material that works well for single model i.e. you do your thing locally on NB Instance and then deploy the model as an endpoint. My use-case on the other hand has multiple models for multiple customers and this needs to be automated. i.e. once a customer uploads a file, a model needs to be automatically created and stored.<\/p>\n<p>Current approach is to automate SageMaker instances through lambda for picking up the train data, training the data and saving the model back to S3 before closing the instance.<\/p>\n<p>My question is, is this the right approach? Or should I create an endpoint for each model for each customer? Somehow since the data size is going to be small and I am working with SageMaker for the first time, I am more comfortable with saving the models in S3 than deploying many many endpoints.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596183617173,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|scikit-learn|xgboost|amazon-sagemaker",
        "Question_view_count":162,
        "Owner_creation_time":1443176197327,
        "Owner_last_access_time":1606295340757,
        "Owner_location":"Dallas, TX, USA",
        "Owner_reputation":19,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1596197428707,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63188242",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59462107,
        "Question_title":"AWS SageMaker TensorFlow Serving - Endpoint failure - CloudWatch log ref: \"NET_LOG: Entering the event loop ...\"",
        "Question_body":"<p>It's my first time using sagemaker to serve my own custom tensorflow model so I have been using the medium articles to get me started:<\/p>\n\n<p><a href=\"https:\/\/medium.com\/ml-bytes\/how-to-create-a-tensorflow-serving-container-for-aws-sagemaker-4853842c9751\" rel=\"noreferrer\">How to Create a TensorFlow Serving Container for AWS SageMaker<\/a><br\/>\n<a href=\"https:\/\/medium.com\/ml-bytes\/how-to-push-a-docker-image-to-aws-ecs-repository-fba579a9f419?\" rel=\"noreferrer\">How to Push a Docker Image to AWS ECS Repository<\/a><br\/>\n<a href=\"https:\/\/medium.com\/ml-bytes\/how-to-deploy-an-aws-sagemaker-container-using-tensorflow-serving-4587dad76169?\" rel=\"noreferrer\">How to Deploy an AWS SageMaker Container Using TensorFlow Serving<\/a><br\/>\n<a href=\"https:\/\/medium.com\/ml-bytes\/how-to-make-predictions-against-a-sagemaker-endpoint-using-tensorflow-serving-8b423b9b316a\" rel=\"noreferrer\">How to Make Predictions Against a SageMaker Endpoint Using TensorFlow Serving<\/a><\/p>\n\n<p>I managed to create my serving container, push it successfully to ECR, and create the sagemaker model from my docker image. However, when i tried to create the endpoints it started creating but after 3-5 minutes ended with the failure message:<\/p>\n\n<blockquote>\n  <p>\"The primary container for production variant Default did not pass the\n  ping health check. Please check CloudWatch logs for this endpoint.\"<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/iXCnK.png\" rel=\"noreferrer\">Failure Image<\/a><\/p>\n\n<p>I then checked my cloud watch logs which looked like this...<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/pdp4S.png\" rel=\"noreferrer\">CloudWatch Logs<\/a><\/p>\n\n<p>...ending with \"NET_LOG: Entering the event loop ...\"<\/p>\n\n<p>I tried to google more about this log message in relation to deploying sagemaker models with tf-serving, but could not find any helpful solutions.<\/p>\n\n<p>To give more context, before running into this problem I encountered 2 other issues:<\/p>\n\n<blockquote>\n  <ol>\n  <li>\"FileSystemStoragePathSource encountered a file-system access error:\n  Could not find base path <p>&lsaquo;MODEL_PATH&rsaquo;\/&lsaquo;MODEL_NAME&rsaquo;\/ for &lsaquo;MODEL_NAME&rsaquo;\"<\/li>\n  <li>\"No versions of servable  found under base path\"<\/li>\n  <\/ol>\n<\/blockquote>\n\n<p>Both of which I managed to solve using the following links:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/599\" rel=\"noreferrer\">[Documentation] TensorFlowModel endpoints need the <code>export\/Servo<\/code> folder structure, but this is not documented<\/a><br><br>\n<a href=\"https:\/\/github.com\/tensorflow\/serving\/issues\/697\" rel=\"noreferrer\">Failed Reason: The primary container for production variant AllTraffic did not pass the ping health check.<\/a><br\/><\/p>\n\n<p>It's also worth noting that my Tensorflow model was created using TF version 2.0 (hence why I needed the docker container). I solely used AWS CLI to carry out my tensorflow serving instead of the sagemaker SDK.<\/p>\n\n<p>Here are snippets of my shell scripts:<\/p>\n\n<p><strong>nginx.config<\/strong><\/p>\n\n<pre><code>events {\n    # determines how many requests can simultaneously be served\n    # https:\/\/www.digitalocean.com\/community\/tutorials\/how-to-optimize-nginx-configuration\n    # for more information\n    worker_connections 2048;\n}\n\nhttp {\n  server {\n    # configures the server to listen to the port 8080\n    # Amazon SageMaker sends inference requests to port 8080.\n    # For more information: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-code-container-response\n    listen 8080 deferred;\n\n    # redirects requests from SageMaker to TF Serving\n    location \/invocations {\n      proxy_pass http:\/\/localhost:8501\/v1\/models\/pornilarity_model:predict;\n    }\n\n    # Used by SageMaker to confirm if server is alive.\n    # https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-algo-ping-requests\n    location \/ping {\n      return 200 \"OK\";\n    }\n  }\n}\n<\/code><\/pre>\n\n<p><strong>Dockerfile<\/strong><\/p>\n\n<pre><code>\n# RUN pip install sagemaker-containers\n\n# Installing NGINX, used to reverse proxy the predictions from SageMaker to TF Serving\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends nginx git\n\n# Copy our model folder to the container \n# NB: Tensorflow serving requires you manually assign version numbering to models e.g. model_path\/1\/\n# see below links: \n\n# https:\/\/stackoverflow.com\/questions\/45544928\/tensorflow-serving-no-versions-of-servable-model-found-under-base-path\n# https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/599\nCOPY pornilarity_model \/opt\/ml\/model\/export\/Servo\/1\/\n\n# Copy NGINX configuration to the container\nCOPY nginx.conf \/opt\/ml\/code\/nginx.conf\n\n# Copies the hosting code inside the container\n# COPY serve.py \/opt\/ml\/code\/serve.py\n\n# Defines serve.py as script entrypoint\n# ENV SAGEMAKER_PROGRAM serve.py\n\n# starts NGINX and TF serving pointing to our model\nENTRYPOINT service nginx start | tensorflow_model_server --rest_api_port=8501 \\\n --model_name=pornilarity_model \\\n --model_base_path=\/opt\/ml\/model\/export\/Servo\/\n<\/code><\/pre>\n\n<p><strong>Build and push<\/strong><\/p>\n\n<pre><code>%%sh\n\n# The name of our algorithm\necr_repo=sagemaker-tf-serving\ndocker_image=sagemaker-tf-serving\n\ncd container\n\n# chmod a+x container\/serve.py\n\naccount=$(aws sts get-caller-identity --query Account --output text)\n\n# Get the region defined in the current configuration (default to us-west-2 if none defined)\nregion=$(aws configure get region)\nregion=${region:-eu-west-2}\n\nfullname=\"${account}.dkr.ecr.${region}.amazonaws.com\/${ecr_repo}:latest\"\n\n# If the repository doesn't exist in ECR, create it.\n\naws ecr describe-repositories --repository-names \"${ecr_repo}\" &gt; \/dev\/null 2&gt;&amp;1\n\nif [ $? -ne 0 ]\nthen\n    aws ecr create-repository --repository-name \"${ecr_repo}\" &gt; \/dev\/null\nfi\n\n# Get the login command from ECR and execute it directly\n$(aws ecr get-login --region ${region} --no-include-email)\n\n# Build the docker image locally with the image name and then push it to ECR\n# with the full name.\n\ndocker build -t ${docker_image} .\n# docker tag ${docker_image} ${fullname}\ndocker tag ${docker_image}:latest ${fullname}\n\ndocker push ${fullname}\n<\/code><\/pre>\n\n<p><strong>Create SageMaker Model<\/strong><\/p>\n\n<pre><code>#!\/usr\/bin\/env bash\n\nCONTAINER_NAME=\"Pornilarity-Container\"\nMODEL_NAME=pornilarity-model-v1\n\n# the role named created with\n# https:\/\/gist.github.com\/mvsusp\/599311cb9f4ee1091065f8206c026962\nROLE_NAME=AmazonSageMaker-ExecutionRole-20191202T133391\n\n# the name of the image created with\n# https:\/\/gist.github.com\/mvsusp\/07610f9cfecbec13fb2b7c77a2e843c4\nECS_IMAGE_NAME=sagemaker-tf-serving\n# the role arn of the role\nEXECUTION_ROLE_ARN=$(aws iam get-role --role-name ${ROLE_NAME} | jq -r .Role.Arn)\n\n# the ECS image URI\nECS_IMAGE_URI=$(aws ecr describe-repositories --repository-name ${ECS_IMAGE_NAME} |\\\njq -r .repositories[0].repositoryUri)\n\n# defines the SageMaker model primary container image as the ECS image\nPRIMARY_CONTAINER=\"ContainerHostname=${CONTAINER_NAME},Image=${ECS_IMAGE_URI}\"\n\n# Createing the model\naws sagemaker create-model --model-name ${MODEL_NAME} \\\n--primary-container=${PRIMARY_CONTAINER}  --execution-role-arn ${EXECUTION_ROLE_ARN}\n<\/code><\/pre>\n\n<p><strong>Endpoint config<\/strong><\/p>\n\n<pre><code>#!\/usr\/bin\/env bash\n\nMODEL_NAME=pornilarity-model-v1\n\nENDPOINT_CONFIG_NAME=pornilarity-model-v1-config\n\nENDPOINT_NAME=pornilarity-v1-endpoint\n\nPRODUCTION_VARIANTS=\"VariantName=Default,ModelName=${MODEL_NAME},\"\\\n\"InitialInstanceCount=1,InstanceType=ml.c5.large\"\n\naws sagemaker create-endpoint-config --endpoint-config-name ${ENDPOINT_CONFIG_NAME} \\\n--production-variants ${PRODUCTION_VARIANTS}\n\naws sagemaker create-endpoint --endpoint-name ${ENDPOINT_NAME} \\\n--endpoint-config-name ${ENDPOINT_CONFIG_NAME}\n<\/code><\/pre>\n\n<p><strong>Docker Container Folder Structure<\/strong><\/p>\n\n<pre><code>\u251c\u2500\u2500 container\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 nginx.conf\n\u2502   \u251c\u2500\u2500 pornilarity_model\n\u2502   \u2502   \u251c\u2500\u2500 assets\n\u2502   \u2502   \u251c\u2500\u2500 saved_model.pb\n\u2502   \u2502   \u2514\u2500\u2500 variables\n\u2502   \u2502       \u251c\u2500\u2500 variables.data-00000-of-00002\n\u2502   \u2502       \u251c\u2500\u2500 variables.data-00001-of-00002\n\u2502   \u2502       \u2514\u2500\u2500 variables.index\n<\/code><\/pre>\n\n<p>Any guidance would be much appreciated!!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1577141884527,
        "Question_score":5,
        "Question_tags":"docker|tensorflow|nginx|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":1220,
        "Owner_creation_time":1459173531210,
        "Owner_last_access_time":1664040349470,
        "Owner_location":null,
        "Owner_reputation":71,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59462107",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60868257,
        "Question_title":"AWS SageMaker on GPU",
        "Question_body":"<p>I am trying to train a neural network (Tensorflow) on AWS. I have some AWS credits. From my understanding AWS SageMaker is the one best for the job. I managed to load the Jupyter Lab console on SageMaker and tried to find a GPU kernel since, I know it is the best for training neural networks. However, I could not find such kernel. <\/p>\n\n<p>Would anyone be able to help in this regard.<\/p>\n\n<p>Thanks &amp; Best Regards<\/p>\n\n<p>Michael<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1585228922137,
        "Question_score":11,
        "Question_tags":"amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":13664,
        "Owner_creation_time":1484485648903,
        "Owner_last_access_time":1663903489200,
        "Owner_location":"Melbourne Australia",
        "Owner_reputation":605,
        "Owner_up_votes":54,
        "Owner_down_votes":0,
        "Owner_views":133,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You train models on GPU in the SageMaker ecosystem via 2 different components:<\/p>\n\n<ol>\n<li><p>You can instantiate a GPU-powered <strong><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi.html\" rel=\"noreferrer\">SageMaker Notebook Instance<\/a><\/strong>, for example <code>p2.xlarge<\/code> (NVIDIA K80) or <code>p3.2xlarge<\/code> (NVIDIA V100). This is convenient for interactive development - you have the GPU right under your notebook and can run code on the GPU interactively and monitor the GPU via <code>nvidia-smi<\/code> in a terminal tab - a great development experience. However when you develop directly from a GPU-powered machine, there are times when you may not use the GPU. For example when you write code or browse some documentation. All that time you pay for a GPU that sits idle. In that regard, it may not be the most cost-effective option for your use-case. <\/p><\/li>\n<li><p>Another option is to use a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\" rel=\"noreferrer\"><strong>SageMaker Training Job<\/strong><\/a> running on a GPU instance. This is a preferred option for training, because training metadata (data and model path, hyperparameters, cluster specification, etc) is persisted in the SageMaker metadata store, logs and metrics stored in Cloudwatch and the instance automatically shuts down itself at the end of training. Developing on a small CPU instance and launching training tasks using SageMaker Training API will help you make the most of your budget, while helping you retain metadata and artifacts of all your experiments. You can see <a href=\"https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/using-tensorflow-eager-execution-with-amazon-sagemaker-script-mode\/\" rel=\"noreferrer\">here a well documented TensorFlow example<\/a><\/p><\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1585261947503,
        "Answer_score":20.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60868257",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73828741,
        "Question_title":"Accelerate BERT training with HuggingFace Model Parallelism",
        "Question_body":"<p>I am currently using SageMaker to train BERT and trying to improve the BERT training time. I use PyTorch and Huggingface on AWS g4dn.12xlarge instance type.<\/p>\n<p>However when I run parallel training it is far from achieving linear improvement. I'm looking for some hints on distributed training to improve the BERT training time in SageMaker.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663940876633,
        "Question_score":0,
        "Question_tags":"pytorch|amazon-sagemaker|huggingface-transformers|bert-language-model|distributed-training",
        "Question_view_count":12,
        "Owner_creation_time":1389887039673,
        "Owner_last_access_time":1664076128463,
        "Owner_location":"Singapore",
        "Owner_reputation":5854,
        "Owner_up_votes":155,
        "Owner_down_votes":70,
        "Owner_views":794,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73828741",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66570138,
        "Question_title":"How to setup AWS sagemaker - Resource limit Error",
        "Question_body":"<p>I'm trying to set up my first SageMaker Studio so my team and myself can run some post processing scripts in a shared environment but I'm having issues.<\/p>\n<p>I've followed the steps in this video(<a href=\"https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices<\/a>) which are:<\/p>\n<ol>\n<li>Select Standard setup<\/li>\n<li>Select AWS Identity and Access Management (IAM)<\/li>\n<li>Under permissions - Create and select new execution role<\/li>\n<li>Under Network and storage - Select VPC, Subnet and Security group<\/li>\n<li>Hit the submit button at the bottom of the page.<\/li>\n<\/ol>\n<p>In the video, he clicks submit and is taken to the control panel where he starts the next phase of adding users, however I'm greeted with this error.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/g4k2g.png\" rel=\"nofollow noreferrer\"> Resource limit Error<\/a><\/p>\n<p>I've checked my Registered domains under route 53 and it says No domains to display, I've also checked my S2 and I have no instances so I have no idea where the 2 domains being utilized are.<\/p>\n<p>My dashboard, image and Notebooks are all empty so as far as I know there's nothing setup on this Sage Maker account.<\/p>\n<p>Could anyone tell me how to resolve this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615398273417,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-ec2|aws-lambda|amazon-sagemaker",
        "Question_view_count":237,
        "Owner_creation_time":1530468231707,
        "Owner_last_access_time":1663982618347,
        "Owner_location":null,
        "Owner_reputation":357,
        "Owner_up_votes":65,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can have maximum 1 studio domain per region, by the default limits. Though, it seems like you have two domains already provisioned. Try to delete all the domains through the AWS cli and recreate with the AWS Management Console.<\/p>\n<p>Unfortunately, AWS Management Console cannot visualize more than one Studio domain.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1626975605176,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66570138",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53624568,
        "Question_title":"AWS Ground Truth text classification manifest using \"source-ref\" not displaying text",
        "Question_body":"<h2>Background<\/h2>\n\n<p>I'm trying out SageMaker Ground Truth, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms.html\" rel=\"nofollow noreferrer\">an AWS service to help you label your data before using it in your ML algorithms<\/a>.<\/p>\n\n<p>The labeling job requires a manifest file which contains a JSON object per row that contains a <code>source<\/code> or a <code>source-ref<\/code>, see also the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html\" rel=\"nofollow noreferrer\">Input Data section<\/a> of the documentation. <\/p>\n\n<h2>Setup<\/h2>\n\n<p>Source-ref is a reference to where the document is located in an S3 bucket like so<\/p>\n\n<pre><code>my-bucket\/data\/manifest.json\nmy-bucket\/data\/123.txt\nmy-bucket\/data\/124.txt\n\n...\n<\/code><\/pre>\n\n<p>The manifest file looks like this (based on the <a href=\"https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-ground-truth-build-highly-accurate-datasets-and-reduce-labeling-costs-by-up-to-70\/\" rel=\"nofollow noreferrer\">blog example<\/a>) :<\/p>\n\n<pre><code>{\"source-ref\": \"s3:\/\/my-bucket\/data\/123.txt\"}\n{\"source-ref\": \"s3:\/\/my-bucket\/data\/124.txt\"}\n...\n<\/code><\/pre>\n\n<h2>The problem<\/h2>\n\n<p>When I create the job, all I get is the <code>source-ref<\/code> value: <strong>s3:\/\/my-bucket\/data\/123.txt<\/strong> as the text, the contents of the file are not displayed.<\/p>\n\n<p>I have tried creating jobs using a manifest that does not contain the s3 protocol, but I get the same result.<\/p>\n\n<p>Is this a bug on their end or I'm I missing something?<\/p>\n\n<h2>Observations<\/h2>\n\n<ul>\n<li>I have tried to make all files public, thinking there may maybe permissions issue? but no<\/li>\n<li>I ensured that the content type of the file was text (s3 -> object -> properties -> metadata)<\/li>\n<li>If I use \"source\" and inline the text, it works properly, but I should be able to use individual documents as there is a limit on the file size specially if I have to label many or large documents!<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1543979158243,
        "Question_score":2,
        "Question_tags":"amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":1506,
        "Owner_creation_time":1250097276470,
        "Owner_last_access_time":1663732003037,
        "Owner_location":"Santa Monica, CA, USA",
        "Owner_reputation":7013,
        "Owner_up_votes":2704,
        "Owner_down_votes":18,
        "Owner_views":445,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53624568",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51391639,
        "Question_title":"Is it possible to predict in sagemaker without using s3",
        "Question_body":"<p>I have a .pkl which I would like to put into production. I would like to do a daily query of my SQL server and do a prediction on about 1000 rows. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html\" rel=\"nofollow noreferrer\">documentation<\/a> implies I have to load the daily data into s3. Is there a way around this? It should be able to fit in memory no problem. <\/p>\n\n<p>The answer to \" <a href=\"https:\/\/stackoverflow.com\/questions\/48319893\/is-there-some-kind-of-persistent-local-storage-in-aws-sagemaker-model-training\">is there some kind of persistent local storage in aws sagemaker model training?<\/a> \" says that \"<em>The notebook instance is coming with a local EBS (5GB) that you can use to copy some data into it and run the fast development iterations without copying the data every time from S3.<\/em>\" The 5GB could be enough but I am not sure you can actually run from a notebook in this manner. If I set up a VPN could I just query using pyodbc?<\/p>\n\n<p>Is there sagemaker integration with AWS Lambda? That in combination with a docker container would suit my needs.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1531871216253,
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-s3|aws-lambda|amazon-sagemaker",
        "Question_view_count":1576,
        "Owner_creation_time":1400315847157,
        "Owner_last_access_time":1663788698353,
        "Owner_location":null,
        "Owner_reputation":4342,
        "Owner_up_votes":318,
        "Owner_down_votes":25,
        "Owner_views":315,
        "Question_last_edit_time":1531893078607,
        "Answer_body":"<p>While you need to to specify a s3 \"folder\" as input, this folder can contain only a dummy file. \nAlso if you bring your own docker container for training like in <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">this example<\/a>, you can do pretty much everthing in it. So you could do your daily query inside your docker container, because they have access to the internet. <\/p>\n\n<p>Also inside this container you have access to all the other aws services. Your access is defined by the role you're passing to your training job.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1531919252070,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51391639",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67396101,
        "Question_title":"read data from rds mysql in sagemaker",
        "Question_body":"<p>Im new in AWS world. I have a rds mysql database. And i wanna read data from this db in sagemaker, but there is a problem.\nActually on local I can read this data like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def readFromAWS():\n    conn = pymysql.connect(host='MY-ENDPOINT', port=3306, user='MY-USERNAME', password='MY-PASSWORD', db='DB-NAME')\n\n    sqlText = &quot;&quot;&quot;SELECT * FROM DB-NAME.XXX order by ID&quot;&quot;&quot;\n\n    df = pd.read_sql(sqlText, conn)\n    conn.close()\n    return df\ndf=readFromAWS()\n<\/code><\/pre>\n<p>And on local it works well. But in sagemaker this code is useless.\nI need some advice. Are there any other way to read this? Should i use other services to read this data in sagemaker? Can you give me some advice??<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1620195792650,
        "Question_score":2,
        "Question_tags":"mysql|amazon-web-services|amazon-s3|amazon-rds|amazon-sagemaker",
        "Question_view_count":426,
        "Owner_creation_time":1587688322720,
        "Owner_last_access_time":1661976002297,
        "Owner_location":"\u0130zmir, T\u00fcrkiye",
        "Owner_reputation":63,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67396101",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55549880,
        "Question_title":"Transforming a PNG image in application\/x-recordio-protobuf for calling a sagemaker endpoint in C#",
        "Question_body":"<p>I am trying to do some inferences on a SageMaker endpoint in C# using a <code>InvokeEndpointRequest<\/code> object. My inference body is a PNG or JPEG image. However, SageMaker requires an <code>application\/x-recordio-protobuf<\/code> format. How can I convert my image file into this format to be able to use InvokeEndpoint with the above object.<\/p>\n<pre><code>InvokeEndpointRequest invokeRequest = new InvokeEndpointRequest\n{\n  EndpointName = &quot;kmeans-2019-xx-xx-xx-xx-xx-xxx&quot;,\n  Body= GetImageFromFile(),\n  ContentType= &quot;application\/x-recordio-protobuf&quot;\n};\nInvokeEndpointResponse invokeResponse = smClient.InvokeEndpoint(invokeRequest);\n<\/code><\/pre>\n<p>For the moment the <code>GetImageFromFile<\/code> method just reads an image file and transforms it in <code>MemoryStream<\/code>:<\/p>\n<pre><code>Stream stream = openFileDialog.OpenFile();\n\nbyte[] data = new byte[stream.Length];\nstream.Read(data, 0, (int)stream.Length);\nMemoryStream ms=new MemoryStream(data);\n            \nreturn ms;\n<\/code><\/pre>\n<p>I tried to serialize the <code>MemoryStream<\/code> by using <code>Protobuf-net<\/code>, but it does not work.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1554559637090,
        "Question_score":0,
        "Question_tags":"c#|amazon-sagemaker",
        "Question_view_count":973,
        "Owner_creation_time":1554558813163,
        "Owner_last_access_time":1560410909097,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1604106646103,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55549880",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49582307,
        "Question_title":"How to schedule tasks on SageMaker",
        "Question_body":"<p>I have a notebook on SageMaker I would like to run every night. What's the best way to schedule this task. Is there a way to run a bash script and schedule Cron job from SageMaker?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1522449441927,
        "Question_score":14,
        "Question_tags":"amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":17339,
        "Owner_creation_time":1420001102893,
        "Owner_last_access_time":1638226427930,
        "Owner_location":null,
        "Owner_reputation":173,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Amazon SageMaker is a set of API that can help various machine learning and data science tasks. These API can be invoked from various sources, such as CLI, <a href=\"https:\/\/aws.amazon.com\/tools\/\" rel=\"noreferrer\">SDK<\/a> or specifically from schedule AWS Lambda functions (see here for documentation: <a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-scheduled-events.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-scheduled-events.html<\/a> )<\/p>\n\n<p>The main parts of Amazon SageMaker are notebook instances, training and tuning jobs, and model hosting for real-time predictions. Each one has different types of schedules that you might want to have. The most popular are:<\/p>\n\n<ul>\n<li><strong>Stopping and Starting Notebook Instances<\/strong> - Since the notebook instances are used for interactive ML models development, you don't really need them running during the nights or weekends. You can schedule a Lambda function to call the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StopNotebookInstance.html\" rel=\"noreferrer\">stop-notebook-instance<\/a> API at the end of the working day (8PM, for example), and the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StartNotebookInstance.html\" rel=\"noreferrer\">start-notebook-instance<\/a> API in the morning. Please note that you can also run crontab on the notebook instances (after opening the local terminal from the Jupyter interface).<\/li>\n<li><strong>Refreshing an ML Model<\/strong> - Automating the re-training of models, on new data that is flowing into the system all the time, is a common issue that with SageMaker is easier to solve. Calling <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html\" rel=\"noreferrer\">create-training-job<\/a> API from a scheduled Lambda function (or even from a <a href=\"https:\/\/docs.aws.amazon.com\/AmazonCloudWatch\/latest\/events\/WhatIsCloudWatchEvents.html\" rel=\"noreferrer\">CloudWatch Event<\/a> that is monitoring the performance of the existing models), pointing to the S3 bucket where the old and new data resides, can <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateModel.html\" rel=\"noreferrer\">create a refreshed model<\/a> that you can now deploy into an <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_UpdateEndpointWeightsAndCapacities.html\" rel=\"noreferrer\">A\/B testing environment<\/a> .<\/li>\n<\/ul>\n\n<p>----- UPDATE (thanks to @snat2100 comment) -----<\/p>\n\n<ul>\n<li><strong>Creating and Deleting Real-time Endpoints<\/strong> - If your realtime endpoints are not needed 24\/7 (for example, serving internal company users working during workdays and hours), you can also <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpoint.html\" rel=\"noreferrer\">create the endpoints<\/a> in the morning and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_DeleteEndpoint.html\" rel=\"noreferrer\">delete them<\/a> at night. <\/li>\n<\/ul>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1523213115336,
        "Answer_score":19.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":1570200701000,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49582307",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64945483,
        "Question_title":"How to store a .tar.gz formatted model to AWS SageMaker and use it as a deployed model?",
        "Question_body":"<p>I have a pre-trained BERT model which was trained on Google Cloud Platform, and the model is stored in a .tar.gz formatted file, I wanted to deploy this model to SageMaker and also be able to trigger the model via API, how can I achieve this?<\/p>\n<p>I found <a href=\"https:\/\/stackoverflow.com\/questions\/54916866\/with-aws-sagemaker-is-it-possible-to-deploy-a-pre-trained-model-using-the-sagem\">this question<\/a> is a little bit related to what I'm asking here, but it's for a scikit-learn model, I'm new to this area, can someone give me some guidance regarding this? Many thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1605976621920,
        "Question_score":1,
        "Question_tags":"amazon-web-services|machine-learning|deployment|amazon-sagemaker|bert-language-model",
        "Question_view_count":649,
        "Owner_creation_time":1540920956270,
        "Owner_last_access_time":1663875036883,
        "Owner_location":"United Kingdom",
        "Owner_reputation":2385,
        "Owner_up_votes":1007,
        "Owner_down_votes":16,
        "Owner_views":585,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64945483",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68606277,
        "Question_title":"AWS Sagemaker DeepAR Validation Error Additional Properties not allowed ('training' was unexpected)",
        "Question_body":"<p>I don't know what the issue is. Here is the code:<\/p>\n<pre><code>estimator = sagemaker.estimator.Estimator(\n    image_uri=image_name,\n    sagemaker_session=sagemaker_session,\n    role=role,\n    train_instance_count=1,\n    train_instance_type=&quot;ml.m5.large&quot;,\n    base_job_name=&quot;deepar-stock&quot;,\n    output_path=s3_output_path,\n)\n\nhyperparameters = {\n    &quot;time_freq&quot;: &quot;24H&quot;,\n    &quot;epochs&quot;: &quot;100&quot;,\n    &quot;early_stopping_patience&quot;: &quot;10&quot;,\n    &quot;mini_batch_size&quot;: &quot;64&quot;,\n    &quot;learning_rate&quot;: &quot;5E-4&quot;,\n    &quot;context_length&quot;: str(context_length),\n    &quot;prediction_length&quot;: str(prediction_length),\n    &quot;likelihood&quot;: &quot;gaussian&quot;,\n}\n\nestimator.set_hyperparameters(**hyperparameters)\n\n%%time\n\nestimator.fit(inputs=f&quot;{s3_data_path}\/train\/&quot;)\n<\/code><\/pre>\n<p>And when I try to train the model I get the following error (in its entirety).<\/p>\n<pre><code>------------------------------------------------------------------------\n\n---\nUnexpectedStatusException                 Traceback (most recent call last)\n&lt;timed eval&gt; in &lt;module&gt;\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n    681         self.jobs.append(self.latest_training_job)\n    682         if wait:\n--&gt; 683             self.latest_training_job.wait(logs=logs)\n    684 \n    685     def _compilation_job_name(self):\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n   1626         # If logs are requested, call logs_for_jobs.\n   1627         if logs != &quot;None&quot;:\n-&gt; 1628             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   1629         else:\n   1630             self.sagemaker_session.wait_for_job(self.job_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in logs_for_job(self, job_name, wait, poll, log_type)\n   3658 \n   3659         if wait:\n-&gt; 3660             self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n   3661             if dot:\n   3662                 print()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   3218                 ),\n   3219                 allowed_statuses=[&quot;Completed&quot;, &quot;Stopped&quot;],\n-&gt; 3220                 actual_status=status,\n   3221             )\n   3222 \nUnexpectedStatusException: Error for Training job deepar-2021-07-31-22-25-54-110: Failed. Reason: ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)\n\nCaused by: Additional properties are not allowed ('training' was unexpected)\n\nFailed validating 'additionalProperties' in schema:\n    {'$schema': 'http:\/\/json-schema.org\/draft-04\/schema#',\n     'additionalProperties': False,\n     'anyOf': [{'required': ['train']}, {'required': ['state']}],\n     'definitions': {'data_channel': {'properties': {'ContentType': {'enum': ['json',\n                                                                              'json.gz',\n                                                                              'parquet',\n                                                                              'auto'],\n                                                                     'type': 'string'},\n                                                     'RecordWrapperType': {'enum': ['None'],\n\nOn instance:\n    {'training': {'RecordWrapperType': 'None',\n                  'S3DistributionType': 'FullyReplicated',\n                  'TrainingInputMode': 'File'}}\n<\/code><\/pre>\n<p>Here it says <code>'training' was unexpected<\/code>. I don't know why it says <code>'training'<\/code> on that last line <code>On instance:<\/code>. I don't know how to solve this. I've looked at other pages for help but I can't find a straight answer. I know that my data is structured right. The errors seem to be with the hyperparameters but I don't know that for sure. Please help!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1627771170133,
        "Question_score":1,
        "Question_tags":"python|amazon-sagemaker|deepar",
        "Question_view_count":157,
        "Owner_creation_time":1405609706593,
        "Owner_last_access_time":1660146579013,
        "Owner_location":"Michigan, United States",
        "Owner_reputation":3113,
        "Owner_up_votes":145,
        "Owner_down_votes":5,
        "Owner_views":317,
        "Question_last_edit_time":1627771514767,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68606277",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62111580,
        "Question_title":"Trouble opening audio files stored on S3 in SageMaker",
        "Question_body":"<p>I stored like 300 GB of audio data (mp3\/wav mostly) on Amazon S3 and am trying to access it in a SageMaker notebook instance to do some data transformations. I'm trying to use either torchaudio or librosa to load a file as a waveform. torchaudio expects the file path as the input, librosa can either use a file path or file-like object. I tried using s3fs to get the url to the file but torchaudio doesn't recognize it as a file. And apparently SageMaker has problems installing librosa so I can't use that. What should I do?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1590899282700,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|pytorch|amazon-sagemaker",
        "Question_view_count":749,
        "Owner_creation_time":1590899033110,
        "Owner_last_access_time":1655505282813,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I ended up not using SageMaker for this, but for anybody else having similar problems, I solved this by opening the file using s3fs and writing it to a <code>tempfile.NamedTemporaryFile<\/code>. This gave me a file path that I could pass into either <code>torchaudio.load<\/code> or <code>librosa.core.load<\/code>. This was also important because I wanted the extra resampling functionality of <code>librosa.core.load<\/code>, but it doesn't accept file-like objects for loading mp3s.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1592257562683,
        "Answer_score":0.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62111580",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72120382,
        "Question_title":"AWS EventBridge: Add multiple event details to a target parameter",
        "Question_body":"<p>I created an EventBridge rule that triggers a Sagemaker Pipeline when someone uploads a new file to an S3 bucket. As new input files become available, they will be uploaded to the bucket for processing. I'd like the pipeline to process only the uploaded file, and so thought to pass in the S3 URL of the file as a parameter to the Pipeline. Since the full URL doesn't exist as a single field value in the S3 event, I was wondering if there is some way to concatenate multiple field values into a single parameter value that EventBridge will pass on to the target.<\/p>\n<p>For example, I know the name of the uploaded file can be sent from EventBridge using <code>$.detail.object.key<\/code> and the bucket name can be sent using <code>$.detail.bucket.name<\/code>, so I'm wondering if I can send both somehow to get something like this to the Sagemaker Pipeline <code>s3:\/\/my-bucket\/path\/to\/file.csv<\/code><\/p>\n<p>For what it's worth, I tried splitting the parameter into two (one being <code>s3:\/\/bucket-name\/<\/code> and the other being <code>default_file.csv<\/code>) when defining the pipeline, but got an error saying <code>Pipeline variables do not support concatenation<\/code> when combining the two into one.<\/p>\n<p>The relevant pipeline step is<\/p>\n<p><code>step_transform = TransformStep(name = &quot;Name&quot;, transformer=transformer,inputs=TransformInput(data=variable_of_s3_path)<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651705997730,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker|aws-event-bridge",
        "Question_view_count":809,
        "Owner_creation_time":1324682328743,
        "Owner_last_access_time":1663467557403,
        "Owner_location":null,
        "Owner_reputation":969,
        "Owner_up_votes":768,
        "Owner_down_votes":10,
        "Owner_views":130,
        "Question_last_edit_time":null,
        "Answer_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/eventbridge\/latest\/userguide\/eb-transform-target-input.html\" rel=\"nofollow noreferrer\">Input transformers<\/a> manipulate the event payload that EventBridge sends to the target.  Transforms consist of (1) an &quot;input path&quot; that maps substitution variable names to JSON-paths in the event and (2) a &quot;template&quot; that references the substitution variables.<\/p>\n<p>Input path:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;detail-bucket-name&quot;: &quot;$.detail.bucket.name&quot;,\n  &quot;detail-object-key&quot;: &quot;$.detail.object.key&quot;\n}\n<\/code><\/pre>\n<p>Input template that concatenates the s3 url and outputs it along with the original event payload:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;s3Url&quot;: &quot;s3:\/\/&lt;detail-bucket-name&gt;\/&lt;detail-object-key&gt;&quot;,\n  &quot;original&quot;: &quot;$&quot;\n}\n<\/code><\/pre>\n<p>Define the transform in the EventBridge console by editing the rule: <code>Rule &gt; Select Targets &gt; Additional Settings<\/code>.<\/p>",
        "Answer_comment_count":8.0,
        "Answer_creation_time":1651734900863,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72120382",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64448720,
        "Question_title":"AWS Sagemaker Multi-Model Endpoint with Scikit Learn: UnexpectedStatusException whilst using a training script",
        "Question_body":"<p>I am trying to create a multi-model endpoint in AWS sagemaker using Scikit-learn and a custom training script. When I attempt to train my model using the following code:<\/p>\n<pre><code>estimator = SKLearn(\n    entry_point=TRAINING_FILE, # script to use for training job\n    role=role,\n    source_dir=SOURCE_DIR, # Location of scripts\n    train_instance_count=1,\n    train_instance_type=TRAIN_INSTANCE_TYPE,\n    framework_version='0.23-1',\n    output_path=s3_output_path,# Where to store model artifacts\n    base_job_name=_job,\n    code_location=code_location,# This is where the .tar.gz of the source_dir will be stored\n    hyperparameters = {'max-samples'    : 100,\n                       'model_name'     : key})\n\nDISTRIBUTION_MODE = 'FullyReplicated'\n\ntrain_input = sagemaker.s3_input(s3_data=inputs+'\/train', \n                                  distribution=DISTRIBUTION_MODE, content_type='csv')\n    \nestimator.fit({'train': train_input}, wait=True)\n<\/code><\/pre>\n<p>where 'TRAINING_FILE' contains:<\/p>\n<pre><code>\nimport argparse\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport sys\n\nfrom sklearn.ensemble import IsolationForest\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--max_samples', type=int, default=100)\n    \n    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--model_name', type=str)\n\n    args, _ = parser.parse_known_args()\n\n    print('reading data. . .')\n    print('model_name: '+args.model_name)    \n    \n    train_file = os.path.join(args.train, args.model_name + '_train.csv')    \n    train_df = pd.read_csv(train_file) # read in the training data\n    train_tgt = train_df.iloc[:, 1] # target column is the second column\n    \n    clf = IsolationForest(max_samples = args.max_samples)\n    clf = clf.fit([train_tgt])\n    \n    path = os.path.join(args.model_dir, 'model.joblib')\n    joblib.dump(clf, path)\n    print('model persisted at ' + path)\n<\/code><\/pre>\n<p>The training script succeeds but sagemaker throws an <code>UnexpectedStatusException<\/code>:\n<a href=\"https:\/\/i.stack.imgur.com\/mXgjS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mXgjS.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Has anybody ever experienced anything like this before? I've checked all the cloudwatch logs and found nothing of use, and I'm completely stumped on what to try next.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1603208778703,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|scikit-learn|amazon-sagemaker",
        "Question_view_count":263,
        "Owner_creation_time":1592311727163,
        "Owner_last_access_time":1647692327233,
        "Owner_location":null,
        "Owner_reputation":153,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":null,
        "Answer_body":"<p>To anyone that comes across this issue in future, the problem has been solved.<\/p>\n<p>The issue was nothing to do with the training, but with invalid characters in directory names being sent to S3. So the script would produce the artifacts correctly, but sagemaker would throw an exception when trying to save them to S3<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1603707766790,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64448720",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57630271,
        "Question_title":"AWS Sagemaker - using cross validation instead of dedicated validation set?",
        "Question_body":"<p>When I train my model locally I use a 20% test set and then cross validation. Sagameker seems like it needs a dedicated valdiation set (at least in the tutorials I've followed). Currently I have 20% test, 10% validation leaving 70% to train - so I lose 10% of my training data compared to when I train locally, and there is some performance loss as a results of this. <\/p>\n\n<p>I could just take my locally trained models and overwrite the sagemaker models stored in s3, but that seems like a bit of a work around. Is there a way to use Sagemaker without having to have a dedicated validation set? <\/p>\n\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1566578862943,
        "Question_score":3,
        "Question_tags":"amazon-web-services|cross-validation|amazon-sagemaker",
        "Question_view_count":1401,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57630271",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61389632,
        "Question_title":"Import error while Executing AWS Predictive Maintenance Using Machine Learning Sample",
        "Question_body":"<p>We are trying to execute and check what kind of output is provided by Predictive Maintenance Using Machine Learning on AWS sample data. We are referring <a href=\"https:\/\/aws.amazon.com\/solutions\/predictive-maintenance-using-machine-learning\/\" rel=\"nofollow noreferrer\">Predictive Maintenance Using Machine Learning<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/solutions\/latest\/predictive-maintenance-using-machine-learning\/welcome.html\" rel=\"nofollow noreferrer\">AWS Guide<\/a> to launch the sample template provided by the AWS. The template is executed properly and we can see the resources in account. Whenever we run the sagemaker notebook for the given example we are getting the error in CloudWatch logs as follows<\/p>\n\n<pre><code>ImportError: cannot import name 'replace_file' on line from mxnet.gluon.utils import download, check_sha1, _get_repo_file_url, replace_file.\n<\/code><\/pre>\n\n<p>This is the stage where the invoke the training job. We have tried following options to resolve the issue.<\/p>\n\n<ul>\n<li>Upgrading the mxnet module<\/li>\n<li>Upgrading the tensorflow module<\/li>\n<\/ul>\n\n<p>But no success.<\/p>\n\n<p>Thanks in advance.<\/p>\n\n<p>Error Traceback is as follows<\/p>\n\n<pre><code>  File \"\/usr\/lib\/python3.5\/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"\/usr\/lib\/python3.5\/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/ml\/code\/sagemaker_predictive_maintenance_entry_point.py\", line 10, in &lt;module&gt;\n    import gluonnlp\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/__init__.py\", line 25, in &lt;module&gt;\n    from . import data\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/data\/__init__.py\", line 23, in &lt;module&gt;\n    from . import (batchify, candidate_sampler, conll, corpora, dataloader,\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/data\/question_answering.py\", line 31, in &lt;module&gt;\n    from mxnet.gluon.utils import download, check_sha1, _get_repo_file_url, replace_file\n    ImportError: cannot import name 'replace_file'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1587652058893,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":398,
        "Owner_creation_time":1473770138817,
        "Owner_last_access_time":1662976446233,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":"<p>A fix for this issue is being deployed to the official solution. In the meantime, you can make the changes described <a href=\"https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/pull\/7\/files\" rel=\"nofollow noreferrer\">here<\/a> in your SageMaker environment by following the instructions below:<\/p>\n\n<p>1) In the notebook, please change the <code>framework_version<\/code> to <code>1.6.0<\/code>.<\/p>\n\n<pre><code>MXNet(entry_point='sagemaker_predictive_maintenance_entry_point.py',\n          source_dir='sagemaker_predictive_maintenance_entry_point',\n          py_version='py3',\n          role=role, \n          train_instance_count=1, \n          train_instance_type=train_instance_type,\n          output_path=output_location,\n          hyperparameters={'num-datasets' : len(train_df),\n                           'num-gpus': 1,\n                           'epochs': 500,\n                           'optimizer': 'adam',\n                           'batch-size':1,\n                           'log-interval': 100},\n         input_mode='File',\n         train_max_run=7200,\n         framework_version='1.6.0')  &lt;- Change this to 1.6.0.\n<\/code><\/pre>\n\n<p>2) This will likely fix things, but just to be sure you don't have any stale packages, change the <code>requirements.txt<\/code> file as well.<\/p>\n\n<p>You'll need to open up a terminal in SageMaker.\n<a href=\"https:\/\/i.stack.imgur.com\/0Vn6l.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0Vn6l.png\" alt=\"enter image description here\"><\/a>\nimage taken from <a href=\"https:\/\/medium.com\/swlh\/jupyter-notebook-on-amazon-sagemaker-getting-started-55489f500439\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/swlh\/jupyter-notebook-on-amazon-sagemaker-getting-started-55489f500439<\/a><\/p>\n\n<p>and run<\/p>\n\n<pre><code>cd SageMaker\/sagemaker_predictive_maintenance_entry_point\/\nsudo vim requirements.txt  # (or sudo nano requirements.txt)\n<\/code><\/pre>\n\n<p>Change the contents to:<\/p>\n\n<pre><code>gluonnlp==0.9.1\npandas==0.22\n<\/code><\/pre>\n\n<p>Save it, and then run the example again.<\/p>\n\n<p>Feel free to comment on the issue as well:\n<a href=\"https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/issues\/6\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/issues\/6<\/a> <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1588063849632,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61389632",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67213383,
        "Question_title":"Training on Sagemaker GPU is too slow",
        "Question_body":"<p>I've launched a training on CelebA dataset for a binary classification with PyTorch, in Sagemaker Studio.<\/p>\n<p>I've made sure all, model, tensors are sent to cuda().<\/p>\n<p>My image dataset is in S3, and I'm accessing it via this import and code:<\/p>\n<pre><code>from PIL import Image\nimport s3fs\n\nfs = s3fs.S3FileSystem()\n\n# example\nf = fs.open(f's3:\/\/aoha-bucket\/img_celeba\/dataset\/000001.jpg')\n<\/code><\/pre>\n<p>And of course my PyTorch DataLoader class, which is using of s3fs to load data into DataLoaders.<\/p>\n<pre><code>class myDataset(Dataset):\n    def __init__(self, csv_file, root_dir, target, length, adv = None, transform=None):\n        self.annotations = pd.read_csv(csv_file).iloc[:length,:]\n        self.root_dir = root_dir\n        self.transform = transform\n        self.target = target\n        self.length = length\n        self.adv = adv\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        img_path = fs.open(os.path.join(self.root_dir, self.annotations.loc[index, 'image_id']))\n        image = Image.open(img_path)\n        image = np.array(image)\n\n        if self.transform:\n            image = self.transform(image=image)[&quot;image&quot;]\n\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        image = torch.Tensor(image)\n\n        y_label = torch.tensor(int(self.annotations.loc[index, str(self.target)]))\n\n        if self.adv is None:\n            return image, y_label\n\n        if self.adv :\n            z_label = torch.tensor(int(self.annotations.loc[index, 'origin']))\n            return image, y_label, z_label\n<\/code><\/pre>\n<p>when I run this function, I get True:<\/p>\n<pre><code>next(model.parameters()).is_cuda\n<\/code><\/pre>\n<p>My issue is that, I don't know the training is too slow, even slower than my local CPU (not that powerful). It says for example, that one epoch is needing 1h45minutes, which is way too much.<\/p>\n<p>Im' using a GPU optimized PyTorch instance of Studio.<\/p>\n<p>Have you ever launched a training on GPU in Sagemaker using PyTorch ?\nCould you please help ?<\/p>\n<p>Thank you very much,\nHabib<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1619094768503,
        "Question_score":0,
        "Question_tags":"computer-vision|pytorch|amazon-sagemaker",
        "Question_view_count":119,
        "Owner_creation_time":1465645376333,
        "Owner_last_access_time":1662979460033,
        "Owner_location":"Boulogne-Billancourt, France",
        "Owner_reputation":29,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":1619129413807,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67213383",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64158911,
        "Question_title":"Load Python Pickle File from S3 Bucket to Sagemaker Notebook",
        "Question_body":"<p>I have attempted the code on the many posts on how to load a pickle file (1.9GB) from an S3 bucket, but none seem to work for our notebook instance on AWS Sagemaker.  Notebook size is 50GB.<\/p>\n<p>Some of the methods attempted:<\/p>\n<p>Method 1<\/p>\n<pre><code>import io\nimport boto3\n\nclient = boto3.client('s3')\nbytes_buffer = io.BytesIO()\nclient.download_fileobj(Bucket=my_bucket, Key=my_key_path, Fileobj=bytes_buffer)\n\nbytes_io.seek(0) \nbyte_value = pickle.load(bytes_io)\n<\/code><\/pre>\n<p>This gives:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rMmJx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rMmJx.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Method 2: This actually gets me something back with no error:<\/p>\n<pre><code>client = boto3.client('s3')\nbytes_buffer = io.BytesIO()\nclient.download_fileobj(Bucket=my_bucket, Key=my_key_path, Fileobj=bytes_buffer)\nbyte_value = bytes_buffer.getvalue()\nimport sys\nsys.getsizeof(byte_value)\/(1024**3)\n<\/code><\/pre>\n<p>this returns: 1.93<\/p>\n<p>but how do I convert the byte_value into the pickled object?\nI tried this:<\/p>\n<pre><code>pickled_data = pickle.loads(byte_value)\n<\/code><\/pre>\n<p>But the kernel &quot;crashed&quot; - went idle and I lost all variables.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1601567432000,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|amazon-s3|boto3|amazon-sagemaker",
        "Question_view_count":2485,
        "Owner_creation_time":1338137210000,
        "Owner_last_access_time":1661852953917,
        "Owner_location":null,
        "Owner_reputation":1937,
        "Owner_up_votes":249,
        "Owner_down_votes":0,
        "Owner_views":221,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64158911",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66793845,
        "Question_title":"Customer Error: imread read blank (None) image for file- Sagemaker AWS",
        "Question_body":"<p>I am following this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_mscoco_multi_label\/Image-classification-multilabel-lst.ipynb\" rel=\"nofollow noreferrer\">tutorial<\/a> with my custom data and my custom S3 buckets where train and validation data are. I am getting the following error:<\/p>\n<pre><code>Customer Error: imread read blank (None) image for file: \/opt\/ml\/input\/data\/train\/s3:\/\/image-classification\/image_classification_model_data\/train\/img-001.png\n<\/code><\/pre>\n<p>I have all my training data are in one folder named '<code>train<\/code>' I have set up my <code>lst<\/code> file like this suggested by <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">doc<\/a>,<\/p>\n<pre><code>22  1   s3:\/\/image-classification\/image_classification_model_data\/train\/img-001.png\n86  0   s3:\/\/image-classification\/image_classification_model_data\/train\/img-002.png\n...\n<\/code><\/pre>\n<p>My other configurations:<\/p>\n<pre><code>s3_bucket = 'image-classification'\nprefix =  'image_classification_model_data'\n\n\ns3train = 's3:\/\/{}\/{}\/train\/'.format(s3_bucket, prefix)\ns3validation = 's3:\/\/{}\/{}\/validation\/'.format(s3_bucket, prefix)\n\ns3train_lst = 's3:\/\/{}\/{}\/train_lst\/'.format(s3_bucket, prefix)\ns3validation_lst = 's3:\/\/{}\/{}\/validation_lst\/'.format(s3_bucket, prefix)\n\n\n\ntrain_data = sagemaker.inputs.TrainingInput(s3train, distribution='FullyReplicated', \n                        content_type='application\/x-image', s3_data_type='S3Prefix')\n\nvalidation_data = sagemaker.inputs.TrainingInput(s3validation, distribution='FullyReplicated', \n                             content_type='application\/x-image', s3_data_type='S3Prefix')\n\ntrain_data_lst = sagemaker.inputs.TrainingInput(s3train_lst, distribution='FullyReplicated', \n                        content_type='application\/x-image', s3_data_type='S3Prefix')\n\nvalidation_data_lst = sagemaker.inputs.TrainingInput(s3validation_lst, distribution='FullyReplicated', \n                             content_type='application\/x-image', s3_data_type='S3Prefix')\n\n\ndata_channels = {'train': train_data, 'validation': validation_data, 'train_lst': train_data_lst, \n                 'validation_lst': validation_data_lst}\n<\/code><\/pre>\n<p>I checked the images downloaded and checked physically, I see the image. Now sure what this error gets thrown out as <code>blank<\/code>. Any suggestion would be great.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616651745117,
        "Question_score":0,
        "Question_tags":"python-3.x|amazon-web-services|amazon-s3|amazon-sagemaker|imread",
        "Question_view_count":164,
        "Owner_creation_time":1519936486960,
        "Owner_last_access_time":1634702229683,
        "Owner_location":"Minneapolis, MN, USA",
        "Owner_reputation":1113,
        "Owner_up_votes":75,
        "Owner_down_votes":1,
        "Owner_views":122,
        "Question_last_edit_time":1616652513867,
        "Answer_body":"<p>Sagemaker copies the input data you specify in <code>s3train<\/code> into the instance in <code>\/opt\/ml\/input\/data\/train\/<\/code> and that's why you have an error, because as you can see from the error message is trying to concatenate the filename in the <code>lst<\/code> file with the path where it expect the image to be. So just put only the filenames in your <code>lst<\/code>and should be fine (remove the s3 path).<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1616684272012,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66793845",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54184145,
        "Question_title":"AWS Sagemaker does not update the package",
        "Question_body":"<p>AWS Sagemaker's notebook comes with Scikit-Learn version 0.19.1<\/p>\n\n<p>I would like to use version 0.20.2. To avoid updating it every time in the notebook code, I tried using the lifecycle configurations. I created one with the following code :<\/p>\n\n<pre><code>#!\/bin\/bash\nset -e\n\/home\/ec2-user\/anaconda3\/bin\/conda install scikit-learn -y\n<\/code><\/pre>\n\n<p>When I run the attached notebook instance and go to the terminal, the version of scikit-learn found with <code>conda list<\/code> is correct (0.20.2). But when I run a notebook and import sklearn, the version is still 0.19.2.<\/p>\n\n<pre><code>import sklearn\nprint(sklearn.__version__)\n<\/code><\/pre>\n\n<p>Is there any virtual environment on the SageMaker instances where I should install the package ? How can I fix my notebook lifecycle configuration ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1547478776530,
        "Question_score":5,
        "Question_tags":"python|conda|amazon-sagemaker",
        "Question_view_count":1546,
        "Owner_creation_time":1527781503483,
        "Owner_last_access_time":1625555943023,
        "Owner_location":"Metz, France",
        "Owner_reputation":352,
        "Owner_up_votes":81,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Your conda update does not refer to a specific virtualenv, while your notebook probably does. Therefore you dont see an update on the notebook virtualenv.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1547708377156,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54184145",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73292975,
        "Question_title":"Sagemaker Data Capture does not write files",
        "Question_body":"<p>I want to enable data capture for a specific endpoint (so far, only via the console). The endpoint works fine and also logs &amp; returns the desired results. However, no files are written to the specified S3 location.<\/p>\n<h3>Endpoint Configuration<\/h3>\n<p>The endpoint is based on a training job with a scikit learn classifier. It has only one variant which is a <code>ml.m4.xlarge<\/code> instance type. Data Capture is enabled with a sampling percentage of 100%. As data capture storage locations I tried <code>s3:\/\/&lt;bucket-name&gt;<\/code> as well as <code>s3:\/\/&lt;bucket-name&gt;\/&lt;some-other-path&gt;<\/code>. With the &quot;Capture content type&quot; I tried leaving everything blank, setting <code>text\/csv<\/code> in &quot;CSV\/Text&quot; and <code>application\/json<\/code> in &quot;JSON&quot;.<\/p>\n<h3>Endpoint Invokation<\/h3>\n<p>The endpoint is invoked in a Lambda function with a client. Here's the call:<\/p>\n<pre><code>sagemaker_body_source = {\n            &quot;segments&quot;: segments,\n            &quot;language&quot;: language\n        }\npayload = json.dumps(sagemaker_body_source).encode()\nresponse = self.client.invoke_endpoint(EndpointName=endpoint_name,\n                                       Body=payload,\n                                       ContentType='application\/json',\n                                       Accept='application\/json')\nresult = json.loads(response['Body'].read().decode())\nreturn result[&quot;predictions&quot;]\n<\/code><\/pre>\n<p>Internally, the endpoint uses a Flask API with an <code>\/invocation<\/code> path that returns the result.<\/p>\n<h3>Logs<\/h3>\n<p>The endpoint itself works fine and the Flask API is logging input and output:<\/p>\n<pre><code>INFO:api:body: {'segments': [&lt;strings...&gt;], 'language': 'de'}\n<\/code><\/pre>\n<pre><code>INFO:api:output: {'predictions': [{'text': 'some text', 'label': 'some_label'}, ....]}\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1660052882547,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":65,
        "Owner_creation_time":1394703217223,
        "Owner_last_access_time":1663244105390,
        "Owner_location":"Cologne, Germany",
        "Owner_reputation":486,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":null,
        "Answer_body":"<p>So the issue seemed to be related to the IAM role. The default role (<code>ModelEndpoint-Role<\/code>) does not have access to write S3 files. It worked via the SDK since it uses another role in the sagemaker studio. I did not receive any error message about this.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1660654017400,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73292975",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71340893,
        "Question_title":"When I get a prediction from sagemaker endpoint, what does the endpoint do?",
        "Question_body":"<p>In sagemaker, the docs talk about inference scripts requiring to have 4 specific functions. When we get a prediction, the python SDK sends a request to the endpoint.<\/p>\n<p>Then the inference script runs. But I cannot find where in the SDK the inference script is run.<\/p>\n<p>When I navigate through the sdk code the <code>Predictor.predict()<\/code> method calls the sagemaker session to post a request to the endpoint and get a response. That is the final step in the sdk. Sagemaker is obviously doing something when it receives that request.<\/p>\n<p>What is the code that it runs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646326739290,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":405,
        "Owner_creation_time":1578932319743,
        "Owner_last_access_time":1663935286190,
        "Owner_location":"Ireland",
        "Owner_reputation":1012,
        "Owner_up_votes":91,
        "Owner_down_votes":96,
        "Owner_views":66,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The endpoint is essentially a Flask web server running in a Docker container<\/p>\n<p>If it's a scikit-learn image, when you invoke the endpoint, it loads your script from S3, then...<\/p>\n<p>It calls <code>input_fn(request_body: bytearray, content_type) -&gt; np.ndarray<\/code> to parse the <code>request_body<\/code> into a numpy array<\/p>\n<p>Then it calls your <code>model_fn(model_dir: str) -&gt; object<\/code> function to load the model from <code>model_dir<\/code> and return the model<\/p>\n<p>Then it calls <code>predict_fn(input_object: np.ndarray, model: object) -&gt; np.array<\/code>, which calls your <code>model.predict()<\/code> function and returns the prediction<\/p>\n<p>Then it calls <code>output_fn(prediction: np.array, accept: str)<\/code> to take the result from <code>predict_fn<\/code> and encode it to the <code>accept<\/code> type<\/p>\n<p>You don't need to implement all of these functions yourself, as there are defaults<\/p>\n<p>You <strong>do<\/strong> need to implement <code>model_fn<\/code><\/p>\n<p>You only need to implement <code>input_fn<\/code> if you have non numeric data<\/p>\n<p>You only need to implement <code>predict_fn<\/code> if your model uses something other than <code>.predict()<\/code><\/p>\n<p>You can see how the default function implementations work <a href=\"https:\/\/github.com\/aws\/sagemaker-scikit-learn-container\/blob\/master\/src\/sagemaker_sklearn_container\/serving.py\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1646329084320,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1646761223947,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71340893",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71773449,
        "Question_title":"What is the fastest way to pull massive amounts of data from Snowflake Database into AWS SageMaker?",
        "Question_body":"<p>What would be the fastest way to pull in very large datasets from Snowflake into my SageMaker instance in AWS? How does the snowflake python connector (what I currently use) compare to lets say a spark connector to snowflake?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1649278117853,
        "Question_score":0,
        "Question_tags":"apache-spark|bigdata|snowflake-cloud-data-platform|amazon-sagemaker",
        "Question_view_count":320,
        "Owner_creation_time":1587310981713,
        "Owner_last_access_time":1659020084953,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71773449",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52549626,
        "Question_title":"Sagemaker memory leak",
        "Question_body":"<p>I deployed a deep learning model in the sagemaker and created a endpoint.\nUnfortunately, I put it a large size image then the endpoint return  'RuntimeError: CUDA error: out of memory'.\nSo I would like to re-launch the endpoint, but seems there is not any restart button.\nWhat could I do for restarting it?<\/p>\n\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1538116329853,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":581,
        "Owner_creation_time":1499669667270,
        "Owner_last_access_time":1624531353080,
        "Owner_location":"\u65e5\u672cKanagawa-ken",
        "Owner_reputation":321,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":27,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52549626",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56860955,
        "Question_title":"Problem running Dask on AWS Sagemaker and AWS Fargate",
        "Question_body":"<p>I am trying to setup a cluster on AWS to run distributed sklearn model training with dask. To get started, I was trying to follow this tutorial which I hope to tweak: <a href=\"https:\/\/towardsdatascience.com\/serverless-distributed-data-pre-processing-using-dask-amazon-ecs-and-python-part-1-a6108c728cc4\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/serverless-distributed-data-pre-processing-using-dask-amazon-ecs-and-python-part-1-a6108c728cc4<\/a><\/p>\n\n<p>I have managed to push the docker container to AWS ECR and then launch a  CloudFormation template to build a cluster on AWS Fargate. The next step in the tutorial is to launch an AWS Sagemaker Notebook. I have tried this but something is not working because when I run the commands I get errors (see image). <strong>What might the problem be? Could it be related to the VPC\/subnets? Is it related to AWS Sagemaker internet access?<\/strong> (I have tried enabling and disabling this).<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/LFtvS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LFtvS.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Expected Results: dask to update, scaling up of the Fargate cluster to work.<\/p>\n\n<p>Actual Results: none of the above.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1562108134793,
        "Question_score":0,
        "Question_tags":"dask|amazon-sagemaker|dask-distributed|aws-fargate",
        "Question_view_count":1239,
        "Owner_creation_time":1523019477347,
        "Owner_last_access_time":1661113536183,
        "Owner_location":null,
        "Owner_reputation":192,
        "Owner_up_votes":175,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Question_last_edit_time":1562108491116,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56860955",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60405600,
        "Question_title":"How to make inference on local PC with the model trained on AWS SageMaker by using the built-in algorithm Semantic Segmentation?",
        "Question_body":"<p>Similar to the issue of <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/200\" rel=\"nofollow noreferrer\">The trained model can be deployed on the other platform without dependency of sagemaker or aws service?<\/a>.<\/p>\n\n<p>I have trained a model on AWS SageMaker by using the built-in algorithm <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/semantic-segmentation.html\" rel=\"nofollow noreferrer\">Semantic Segmentation<\/a>. This trained model named as <code>model.tar.gz<\/code> is stored on S3. So I want to download this file from S3 and then use it to make inference on my local PC without using AWS SageMaker anymore. Since the built-in algorithm Semantic Segmentation is built using the <a href=\"https:\/\/github.com\/dmlc\/gluon-cv\" rel=\"nofollow noreferrer\">MXNet Gluon framework and the Gluon CV toolkit<\/a>, so I try to refer the documentation of <a href=\"https:\/\/mxnet.apache.org\/\" rel=\"nofollow noreferrer\">mxnet<\/a> and <a href=\"https:\/\/gluon-cv.mxnet.io\/\" rel=\"nofollow noreferrer\">gluon-cv<\/a> to make inference on local PC.<\/p>\n\n<p>It's easy to download this file from S3, and then I unzip this file to get three files:<\/p>\n\n<ol>\n<li><strong>hyperparams.json<\/strong>: includes the parameters for network architecture, data inputs, and training. Refer to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/segmentation-hyperparameters.html\" rel=\"nofollow noreferrer\">Semantic Segmentation Hyperparameters<\/a>.<\/li>\n<li><strong>model_algo-1<\/strong><\/li>\n<li><strong>model_best.params<\/strong><\/li>\n<\/ol>\n\n<p>Both <strong>model_algo-1<\/strong> and <strong>model_best.params<\/strong> are the trained models, and I think it's the output from <code>net.save_parameters<\/code> (Refer to <a href=\"https:\/\/beta.mxnet.io\/guide\/getting-started\/crash-course\/4-train.html\" rel=\"nofollow noreferrer\">Train the neural network<\/a>). I can also load them with the function <code>mxnet.ndarray.load<\/code>.<\/p>\n\n<p>Refer to <a href=\"https:\/\/beta.mxnet.io\/guide\/getting-started\/crash-course\/5-predict.html\" rel=\"nofollow noreferrer\">Predict with a pre-trained model<\/a>. I found there are two necessary things:<\/p>\n\n<ol>\n<li>Reconstruct the network for making inference.<\/li>\n<li>Load the trained parameters.<\/li>\n<\/ol>\n\n<p>As for reconstructing the network for making inference, since I have used PSPNet from training, so I can use the class <code>gluoncv.model_zoo.PSPNet<\/code> to reconstruct the network. And I know how to use some services of AWS SageMaker, for example batch transform jobs, to make inference. I want to reproduce it on my local PC. If I use the class <code>gluoncv.model_zoo.PSPNet<\/code> to reconstruct the network, I can't make sure whether the parameters for this network are same those used on AWS SageMaker while making inference. Because I can't see the image <code>501404015308.dkr.ecr.ap-northeast-1.amazonaws.com\/semantic-segmentation:latest<\/code> in detail. <\/p>\n\n<p>As for loading the trained parameters, I can use the <code>load_parameters<\/code>. But as for <strong>model_algo-1<\/strong> and <strong>model_best.params<\/strong>, I don't know which one I should use.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1582681249783,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|mxnet|gluon",
        "Question_view_count":351,
        "Owner_creation_time":1472967821507,
        "Owner_last_access_time":1663314033960,
        "Owner_location":"Tokyo, Japan",
        "Owner_reputation":61,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1582681944470,
        "Answer_body":"<p>The following code works well for me.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import mxnet as mx\nfrom mxnet import image\nfrom gluoncv.data.transforms.presets.segmentation import test_transform\nimport gluoncv\n\n# use cpu\nctx = mx.cpu(0)\n\n# load test image\nimg = image.imread('.\/img\/IMG_4015.jpg')\nimg = test_transform(img, ctx)\nimg = img.astype('float32')\n\n# reconstruct the PSP network model\nmodel = gluoncv.model_zoo.PSPNet(2)\n\n# load the trained model\nmodel.load_parameters('.\/model\/model_algo-1')\n\n# make inference\noutput = model.predict(img)\npredict = mx.nd.squeeze(mx.nd.argmax(output, 1)).asnumpy()\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1583126137776,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60405600",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72422041,
        "Question_title":"Invoking Sagemaker MultiDataModel Endpoint throws \"ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation\"",
        "Question_body":"<p>I'm trying to create a multi-model endpoint on sagemaker, using pre-trained tensorflow models  which were uploaded to s3 (tar.gz files). Creating a 'single-model' endpoint works fine with both of them.<\/p>\n<p>I followed a few blog posts for this task (<a href=\"https:\/\/dataintegration.info\/host-multiple-tensorflow-computer-vision-models-using-amazon-sagemaker-multi-model-endpoints\" rel=\"nofollow noreferrer\">1<\/a>, <a href=\"https:\/\/towardsdatascience.com\/deploy-multiple-tensorflow-models-to-one-endpoint-65bea81c3f2f\" rel=\"nofollow noreferrer\">2<\/a>).<\/p>\n<p>I've successfully deployed a MultiDataModel endpoint on Sagemaker (code attached below the error), but when trying to invoke a model (any of them) I received the following error:<\/p>\n<pre><code>~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/sagemaker\/tensorflow\/model.py in predict(self, data, initial_args)\n    105                 args[&quot;CustomAttributes&quot;] = self._model_attributes\n    106 \n--&gt; 107         return super(TensorFlowPredictor, self).predict(data, args)\n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id)\n    159             data, initial_args, target_model, target_variant, inference_id\n    160         )\n--&gt; 161         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n    162         return self._handle_response(response)\n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    413                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    414             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 415             return self._make_api_call(operation_name, kwargs)\n    416 \n    417         _api_call.__name__ = str(py_operation_name)\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    743             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)\n    744             error_class = self.exceptions.from_code(error_code)\n--&gt; 745             raise error_class(parsed_response, operation_name)\n    746         else:\n    747             return parsed_response\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: \nReceived server error (504) from model with message &quot;&lt;html&gt;\n&lt;head&gt;&lt;title&gt;504 Gateway Time-out&lt;\/title&gt;&lt;\/head&gt;\n&lt;body&gt;\n&lt;center&gt;&lt;h1&gt;504 Gateway Time-out&lt;\/h1&gt;&lt;\/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx\/1.20.2&lt;\/center&gt;\n&lt;\/body&gt;\n&lt;\/html&gt;\n&quot;. See https:\/\/eu-central-1.console.aws.amazon.com\/cloudwatch\/home?region=eu-central- 1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/mme-tensorflow-2022-05-29-06-38-29 in \naccount ******** for more information.\n<\/code><\/pre>\n<p>Here is the code for creating and deploying the models and the endpoint:<\/p>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.multidatamodel import MultiDataModel\nfrom sagemaker.tensorflow.model import TensorFlowModel\n\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\nrating_model_archive = &quot;rating_model.tar.gz&quot;\nsim_users_model_archive = &quot;sim_users_model.tar.gz&quot;\ncurrent_time = datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d-%H-%M-%S')\n\nsagemaker_model_rating = TensorFlowModel(model_data = f's3:\/\/{bucket_name}\/model\/{rating_model_archive}',\n                                         name = f'rating-model-{current_time}',\n                                         role = role,\n                                         framework_version = &quot;2.8&quot;, #tf.__version__,\n                                         entry_point = 'empty_train.py',\n                                         sagemaker_session=sagemaker_session)\n\nsagemaker_model_sim = TensorFlowModel(model_data = f's3:\/\/{bucket_name}\/model\/{sim_users_model_archive}',\n                                      name = f'similar-users-model-{current_time}',\n                                      role = role,\n                                      framework_version = &quot;2.8&quot;, #tf.__version__,\n                                      entry_point = 'empty_train.py',\n                                      sagemaker_session=sagemaker_session)\n\nmodel_data_prefix = f's3:\/\/{bucket_name}\/model\/'\n\nmme = MultiDataModel(name=f'mme-tensorflow-{current_time}',\n                     model_data_prefix=model_data_prefix,\n                     model=sagemaker_model_rating,\n                     sagemaker_session=sagemaker_session)\n\ntf_predictor = mme.deploy(initial_instance_count=2,\n                          instance_type=&quot;ml.m4.xlarge&quot;,#'ml.t2.medium',\n                          endpoint_name=f'mme-tensorflow-{current_time}')\n<\/code><\/pre>\n<p>Up until here, as mentioned earlier, it works fine, and I have a running endpoint.\nWhen trying to invoke it with the following code, I get the aforementioned error:<\/p>\n<pre><code>input1 = {\n    &quot;instances&quot;: [\n        {&quot;user_id&quot;: [854],\n         &quot;item_id&quot;: [123]}\n                 ]\n}\n\ninput2 = {\n    &quot;instances&quot;: [12]\n}\n\ntf_predictor.predict(data=input2, initial_args={'TargetModel': sim_users_model_archive})\n# tf_predictor.predict(data=input1, initial_args={'TargetModel': rating_model_archive})\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1653812629237,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":106,
        "Owner_creation_time":1653481867867,
        "Owner_last_access_time":1659990406947,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72422041",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70044114,
        "Question_title":"tensorflow.python.framework.errors_impl.NotFoundError: \/opt\/ml\/input\/data\/train\/label_map.pbtxt; No such file or directory",
        "Question_body":"<p>I've been following this tutorial on <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/training-and-deploying-models-using-tensorflow-2-with-the-object-detection-api-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">Training and deploying models using TensorFlow 2 with the Object Detection API on Amazon SageMaker<\/a> but keep on getting the above error when attempting to train the model using estimator.fit(inputs) in train_model.ipynb. All of the code for the tutorial is available at: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-tensorflow-object-detection-api\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-tensorflow-object-detection-api<\/a><\/p>\n<p>The label_map.pbtxt, train.records and validation.records files were successfully created in my bucket (at s3:\/\/bucket\/data\/bees\/tfrecords), and I've adjusted my pipeline.config file to contain:<\/p>\n<pre><code>train_input_reader: {\n  label_map_path: &quot;\/opt\/ml\/input\/data\/train\/label_map.pbtxt&quot;\n  tf_record_input_reader {\n    input_path: &quot;\/opt\/ml\/input\/data\/train\/train.records&quot;\n  }\n}\n\neval_input_reader: {\n  label_map_path: &quot;\/opt\/ml\/input\/data\/train\/label_map.pbtxt&quot;\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: &quot;\/opt\/ml\/input\/data\/train\/validation.records&quot;\n  }\n}\n<\/code><\/pre>\n<p>I'm completely new to Amazon Sagemaker and containers but have followed the walkthrough to a tee, so I'm lost as to why it's failing. Any help would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637394399277,
        "Question_score":1,
        "Question_tags":"python|tensorflow|object-detection|amazon-sagemaker",
        "Question_view_count":161,
        "Owner_creation_time":1637389797727,
        "Owner_last_access_time":1654515886370,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70044114",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70975320,
        "Question_title":"EventBridge trigger: Sagemaker Processing Job finished",
        "Question_body":"<p>I'm currently developing some ETL for my ML model with AWS. The thing is that I want to <strong>trigger<\/strong> a Lambda when some Sagemaker Processing Job is finished. And the <strong>event<\/strong> passed to the Lambda, should be the configuration info (job name, arguments, etc..) of the Sagemaker Processing Job.<\/p>\n<p><strong>Q1<\/strong>: How can I do to <em>trigger the event<\/em> when the Processing Job is finished?<\/p>\n<p><strong>Q2<\/strong>: How can I do to pass the <em>Processing Job configurations as an event<\/em> for the Lambda?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643907548550,
        "Question_score":3,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker|aws-event-bridge",
        "Question_view_count":504,
        "Owner_creation_time":1519511545083,
        "Owner_last_access_time":1663921636670,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1643931586140,
        "Answer_body":"<p>You can use the following EventBridge rule pattern:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;source&quot;: [&quot;aws.sagemaker&quot;],\n  &quot;detail-type&quot;: [&quot;SageMaker Processing Job State Change&quot;],\n  &quot;detail&quot;: {\n    &quot;ProcessingJobStatus&quot;: [&quot;Failed&quot;, &quot;Completed&quot;, &quot;Stopped&quot;]\n  }\n}\n<\/code><\/pre>\n<p>The ProcessingJobStatus list can be modified based on which statuses you want to handle.<\/p>\n<p>You can set a Lambda function as the target of your EventBridge rule.<\/p>\n<p>Here is a sample event which will be passed to your Lambda, taken from AWS console:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;version&quot;: &quot;0&quot;,\n  &quot;id&quot;: &quot;0a15f67d-aa23-0123-0123-01a23w89r01t&quot;,\n  &quot;detail-type&quot;: &quot;SageMaker Processing Job State Change&quot;,\n  &quot;source&quot;: &quot;aws.sagemaker&quot;,\n  &quot;account&quot;: &quot;123456789012&quot;,\n  &quot;time&quot;: &quot;2019-05-31T21:49:54Z&quot;,\n  &quot;region&quot;: &quot;us-east-1&quot;,\n  &quot;resources&quot;: [&quot;arn:aws:sagemaker:us-west-2:012345678987:processing-job\/integ-test-analytics-algo-54ee3282-5899-4aa3-afc2-7ce1d02&quot;],\n  &quot;detail&quot;: {\n    &quot;ProcessingInputs&quot;: [{\n      &quot;InputName&quot;: &quot;InputName&quot;,\n      &quot;S3Input&quot;: {\n        &quot;S3Uri&quot;: &quot;s3:\/\/input\/s3\/uri&quot;,\n        &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/input\/local\/path&quot;,\n        &quot;S3DataType&quot;: &quot;MANIFEST_FILE&quot;,\n        &quot;S3InputMode&quot;: &quot;PIPE&quot;,\n        &quot;S3DataDistributionType&quot;: &quot;FULLYREPLICATED&quot;\n      }\n    }],\n    &quot;ProcessingOutputConfig&quot;: {\n      &quot;Outputs&quot;: [{\n        &quot;OutputName&quot;: &quot;OutputName&quot;,\n        &quot;S3Output&quot;: {\n          &quot;S3Uri&quot;: &quot;s3:\/\/output\/s3\/uri&quot;,\n          &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/output\/local\/path&quot;,\n          &quot;S3UploadMode&quot;: &quot;CONTINUOUS&quot;\n        }\n      }],\n      &quot;KmsKeyId&quot;: &quot;KmsKeyId&quot;\n    },\n    &quot;ProcessingJobName&quot;: &quot;integ-test-analytics-algo-54ee3282-5899-4aa3-afc2-7ce1d02&quot;,\n    &quot;ProcessingResources&quot;: {\n      &quot;ClusterConfig&quot;: {\n        &quot;InstanceCount&quot;: 3,\n        &quot;InstanceType&quot;: &quot;ml.c5.xlarge&quot;,\n        &quot;VolumeSizeInGB&quot;: 5,\n        &quot;VolumeKmsKeyId&quot;: &quot;VolumeKmsKeyId&quot;\n      }\n    },\n    &quot;StoppingCondition&quot;: {\n      &quot;MaxRuntimeInSeconds&quot;: 2000\n    },\n    &quot;AppSpecification&quot;: {\n      &quot;ImageUri&quot;: &quot;012345678901.dkr.ecr.us-west-2.amazonaws.com\/processing-uri:latest&quot;\n    },\n    &quot;NetworkConfig&quot;: {\n      &quot;EnableInterContainerTrafficEncryption&quot;: true,\n      &quot;EnableNetworkIsolation&quot;: false,\n      &quot;VpcConfig&quot;: {\n        &quot;SecurityGroupIds&quot;: [&quot;SecurityGroupId1&quot;, &quot;SecurityGroupId2&quot;, &quot;SecurityGroupId3&quot;],\n        &quot;Subnets&quot;: [&quot;Subnet1&quot;, &quot;Subnet2&quot;]\n      }\n    },\n    &quot;RoleArn&quot;: &quot;arn:aws:iam::012345678987:role\/SageMakerPowerUser&quot;,\n    &quot;ExperimentConfig&quot;: {},\n    &quot;ProcessingJobArn&quot;: &quot;arn:aws:sagemaker:us-west-2:012345678987:processing-job\/integ-test-analytics-algo-54ee3282-5899-4aa3-afc2-7ce1d02&quot;,\n    &quot;ProcessingJobStatus&quot;: &quot;Completed&quot;,\n    &quot;LastModifiedTime&quot;: 1589879735000,\n    &quot;CreationTime&quot;: 1589879735000\n  }\n}\n<\/code><\/pre>\n<p><strong>Edit:<\/strong><\/p>\n<p>If you want to match a ProcessingJobName with specific prefix:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;source&quot;: [&quot;aws.sagemaker&quot;],\n  &quot;detail-type&quot;: [&quot;SageMaker Processing Job State Change&quot;],\n  &quot;detail&quot;: {\n    &quot;ProcessingJobStatus&quot;: [&quot;Failed&quot;, &quot;Completed&quot;, &quot;Stopped&quot;],\n    &quot;ProcessingJobName&quot;: [{\n      &quot;prefix&quot;: &quot;standarize-data&quot;\n    }]\n  }\n}\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1643913781760,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1643920466910,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70975320",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61507982,
        "Question_title":"Sagemaker: ImportError: No module named 'pandas' when running training script",
        "Question_body":"<p>At this point I am running this exact notebook:  <a href=\"https:\/\/github.com\/udacity\/sagemaker-deployment\/blob\/master\/Project\/SageMaker%20Project.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/udacity\/sagemaker-deployment\/blob\/master\/Project\/SageMaker%20Project.ipynb<\/a><\/p>\n\n<p>Just with a different dataset (that I got to the exact same format seen in the notebook).<\/p>\n\n<p>when I get to call the estimator I get the error:<\/p>\n\n<pre><code>2020-04-29 17:18:03 Starting - Starting the training job...\n2020-04-29 17:18:06 Starting - Launching requested ML instances...\n2020-04-29 17:19:03 Starting - Preparing the instances for training......\n2020-04-29 17:19:54 Downloading - Downloading input data\n2020-04-29 17:19:54 Training - Downloading the training image.bash: cannot set terminal process group (-1): Inappropriate ioctl for device\nbash: no job control in this shell\n\n\n\n2020-04-29 17:20:13,936 sagemaker-containers ERROR    ExecuteUserScriptError:\nCommand \"\/usr\/bin\/python -m train --epochs 10 --hidden_dim 200\"\nTraceback (most recent call last):\n  File \"\/usr\/lib\/python3.5\/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"\/usr\/lib\/python3.5\/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/ml\/code\/train.py\", line 11, in &lt;module&gt;\n    import pandas as pd\nImportError: No module named 'pandas'\n\n2020-04-29 17:20:25 Uploading - Uploading generated training model\n2020-04-29 17:20:25 Failed - Training job failed\n---------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\n&lt;ipython-input-37-0e8223086435&gt; in &lt;module&gt;()\n----&gt; 1 estimator.fit({'training': input_data})\n\n~\/anaconda3\/envs\/pytorch_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n    475         self.jobs.append(self.latest_training_job)\n    476         if wait:\n--&gt; 477             self.latest_training_job.wait(logs=logs)\n    478 \n    479     def _compilation_job_name(self):\n\n~\/anaconda3\/envs\/pytorch_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n   1084         # If logs are requested, call logs_for_jobs.\n   1085         if logs != \"None\":\n-&gt; 1086             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   1087         else:\n   1088             self.sagemaker_session.wait_for_job(self.job_name)\n\n~\/anaconda3\/envs\/pytorch_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in logs_for_job(self, job_name, wait, poll, log_type)\n   3042 \n   3043         if wait:\n-&gt; 3044             self._check_job_status(job_name, description, \"TrainingJobStatus\")\n   3045             if dot:\n   3046                 print()\n\n~\/anaconda3\/envs\/pytorch_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   2636                 ),\n   2637                 allowed_statuses=[\"Completed\", \"Stopped\"],\n-&gt; 2638                 actual_status=status,\n   2639             )\n   2640 \n\nUnexpectedStatusException: Error for Training job sagemaker-pytorch-2020-04-29-17-18-03-379: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"\/usr\/bin\/python -m train --epochs 10 --hidden_dim 200\"\nTraceback (most recent call last):\n  File \"\/usr\/lib\/python3.5\/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"\/usr\/lib\/python3.5\/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/ml\/code\/train.py\", line 11, in &lt;module&gt;\n    import pandas as pd\nImportError: No module named 'pandas'\n<\/code><\/pre>\n\n<p>does anyone have any insights on what I can do to troubleshoot this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1588181659513,
        "Question_score":0,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":593,
        "Owner_creation_time":1517932507093,
        "Owner_last_access_time":1648741816033,
        "Owner_location":null,
        "Owner_reputation":331,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":73,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61507982",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54288886,
        "Question_title":"AWS Sagemaker init 1K+ models \"endpoints\"?",
        "Question_body":"<p>under the assumptions that the model training itself is very fast, I'm wondering what is the best practice to spin up ~ > 1K models endpoints \nfast as possible.<\/p>\n\n<p>Thanks for any hint\nChristian<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548069626337,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":159,
        "Owner_creation_time":1373960648337,
        "Owner_last_access_time":1663331601773,
        "Owner_location":"Europa",
        "Owner_reputation":181,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":41,
        "Question_last_edit_time":1548092518556,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54288886",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73677347,
        "Question_title":"Does SageMaker built-in LightGBM algorithm support distributed training?",
        "Question_body":"<p>Does  <strong>Amazon SageMaker built-in LightGBM<\/strong> algorithm support <strong>distributed training<\/strong>?<\/p>\n<p>I use Databricks for distributed training of LightGBM today. If SageMaker built-in LightGBM supports distributed training, I would consider migrating to SageMaker. It is not clear in the Amazon SageMaker's built-in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lightgbm.html\" rel=\"nofollow noreferrer\">LightGBM<\/a>'s documentation on whether it supports distributed training.<\/p>\n<p>Thanks very much for any suggestion or clarification on this.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1662878679543,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|lightgbm|distributed-training|amazon-machine-learning",
        "Question_view_count":27,
        "Owner_creation_time":1389887039673,
        "Owner_last_access_time":1664076128463,
        "Owner_location":"Singapore",
        "Owner_reputation":5854,
        "Owner_up_votes":155,
        "Owner_down_votes":70,
        "Owner_views":794,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I went through the LightGBM section of SageMaker documentation and there are no references that it supports distributed training. One of the example\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lightgbm.html\" rel=\"nofollow noreferrer\">here<\/a>\u00a0uses single instance type. Also looked at lightGBM documentation\u00a0<a href=\"https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parallel-Learning-Guide.html\" rel=\"nofollow noreferrer\">here<\/a>\u00a0. Here are the parameters that you need to specify<\/p>\n<p>tree_learner=your_parallel_algorithm,<\/p>\n<p>num_machines=your_num_machines,<\/p>\n<p>Given I couldnt find any reference of above in SageMaker documentation, I assume its not supported.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1663711220203,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73677347",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61076649,
        "Question_title":"Preprocessing data for Sagemaker Inference Pipeline with Blazingtext",
        "Question_body":"<p>I'm trying to figure out the best way to preprocess my input data for my inference endpoint for AWS Sagemaker. I'm using the BlazingText algorithm.<\/p>\n\n<p>I'm not really sure the best way forward and I would be thankful for any pointers.<\/p>\n\n<p>I currently train my model using a Jupyter notebook in Sagemaker and that works wonderfully, but the problem is that I use NLTK to clean my data (Swedish stopwords and stemming etc):<\/p>\n\n<pre><code>import nltk\nnltk.download('punkt')\nnltk.download('stopwords')\n<\/code><\/pre>\n\n<p>So the question is really, how do I get the same  pre-processing logic to the inference endpoint  ?<\/p>\n\n<p>I have a couple of thoughts about how to proceed:<\/p>\n\n<ul>\n<li><p>Build a docker container with the python libs &amp; data installed with the sole purpose of pre-processing the data. Then use this container in the inference pipeline. <\/p><\/li>\n<li><p>Supply the Python libs and Script to an existing container in the same way you can do for external lib an notebook <\/p><\/li>\n<li><p>Build a custom fastText container with the libs I need and run it outside of Sagemaker.<\/p><\/li>\n<li><p>Will probably work, but feels like a \"hack\": Build a Lambda function that has the proper Python libs&amp;data installed and calls the Sagemaker Endpoint. I'm worried about cold start delays as the prediction traffic volume will be low. <\/p><\/li>\n<\/ul>\n\n<p>I would like to go with the first option, but I'm struggling a bit to understand if there is a docker image that I could build from, and add my dependencies to, or if I need to build something from the ground up. For instance, would the image sagemaker-sparkml-serving:2.2 be a good candidate? <\/p>\n\n<p>But maybe there is a better way all around? <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1586251113373,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker|inference",
        "Question_view_count":848,
        "Owner_creation_time":1420497448900,
        "Owner_last_access_time":1663999495290,
        "Owner_location":null,
        "Owner_reputation":56,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61076649",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54638364,
        "Question_title":"SageMaker Script Mode + Pipe Mode",
        "Question_body":"<p>I'm training in SageMaker using TensorFlow + Script Mode and currently using 'File' input mode for my data.<\/p>\n\n<p>Has anyone figured out how to stream data using 'Pipe' data format in conjunction with Script Mode training?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1549915827960,
        "Question_score":3,
        "Question_tags":"python|tensorflow|streaming|amazon-sagemaker",
        "Question_view_count":1385,
        "Owner_creation_time":1361339272693,
        "Owner_last_access_time":1663965928400,
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Question_last_edit_time":1549923138163,
        "Answer_body":"<p>You can import <code>sagemaker_tensorflow<\/code> from the training script as follows:<\/p>\n\n<pre><code>from sagemaker_tensorflow import PipeModeDataset\nfrom tensorflow.contrib.data import map_and_batch\n\nchannel = 'my-pipe-channel-name'\n\nds = PipeModeDataset(channel)\nds = ds.repeat(EPOCHS)\nds = ds.prefetch(PREFETCH_SIZE)\nds = ds.apply(map_and_batch(parse, batch_size=BATCH_SIZE,\n                            num_parallel_batches=NUM_PARALLEL_BATCHES))\n<\/code><\/pre>\n\n<p>You can find the full example here: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_pipemode_example\/pipemode.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_pipemode_example\/pipemode.py<\/a><\/p>\n\n<p>You can find documentation about sagemaker_tensorflow here <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset<\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1551201776032,
        "Answer_score":4.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54638364",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52672171,
        "Question_title":"Model testing on AWS sagemaker \"could not convert string to float\"",
        "Question_body":"<p>The XGboost model was trained on AWS sagemaker and deployed successfully but I keep getting the following error: <strong>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message \"could not convert string to float: \".<\/strong> \nAny thoughts?<\/p>\n\n<pre><code>Test data is as following:\n      size       mean\n269   5600.0  17.499633\n103   1754.0   9.270272\n160   4968.0  14.080601\n40       4.0  17.500000\n266  36308.0  11.421855\n\ntest_data_array = test_data.drop(['mean'], axis=1).as_matrix()\ntest_data_array = np.array([np.float32(x) for x in test_data_array])\nxgb_predictor.content_type = 'text\/csv'\nxgb_predictor.serializer = csv_serializer\n\ndef predict(data, rows=32):\n    split_array = np.array_split(data, int(data.shape[0] \/ float(rows) + 1))\n    #print(split_array)\n    predictions = ''\n\n    for array in split_array:\n        print(array[0], type(array[0]))\n        predictions = ','.join([predictions, xgb_predictor.predict(array[0]).decode('utf-8')])\n\n    return np.fromstring(predictions[1:], sep=',')\n\npredictions = predict(test_data_array)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1538767718613,
        "Question_score":3,
        "Question_tags":"data-science|amazon-sagemaker",
        "Question_view_count":1605,
        "Owner_creation_time":1338844022563,
        "Owner_last_access_time":1611332621947,
        "Owner_location":null,
        "Owner_reputation":355,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52672171",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70156631,
        "Question_title":"How to pass additional parameters (as a dict) to sagemeker custom inference container?",
        "Question_body":"<p>Status:<\/p>\n<ul>\n<li>Custom container is built using the doc - <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own<\/a><\/li>\n<li>predict.py is coded to accommodate the custom inference script and its working well<\/li>\n<li>Using the classsagemaker.model.Model() class to pass the trained model.tar.gz and custom container image inorder to deploy the model<\/li>\n<\/ul>\n<p>Challenge:<\/p>\n<ul>\n<li>In the same Model class there is a ENV  parameter through which we can apparently send the environment variables to the custom image<\/li>\n<li>Tried passing a python dict to this , but facing difficulty to read this json dict inide the predict.py script<\/li>\n<\/ul>\n<p>Somebody faced the same difficulty ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1638197351547,
        "Question_score":0,
        "Question_tags":"python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":183,
        "Owner_creation_time":1604146329127,
        "Owner_last_access_time":1662133039343,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can pass your environment dict in your Model as:<\/p>\n<pre><code>Model(\n.\n.\nenv= {&quot;my_env&quot;: &quot;my_env_value&quot;}\n.\n.\n)\n<\/code><\/pre>\n<p>SageMaker will pass the enviroments dict to your container and you can access it in your predict.py script for example with:<\/p>\n<pre><code>my_env = os.environ.get('my_env',&quot;env key not set in Model&quot;)\nprint(my_env)\n<\/code><\/pre>\n<p>If your env dict was passed to your Model containing they <code>my_env<\/code> then you will receive the output : <code>my_env_value<\/code>. Else, then you will receive <code>env key not set in Model<\/code><\/p>\n<p>I work for AWS and my opinions are my own.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1645489319000,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70156631",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65238339,
        "Question_title":"How to input fsx for lustre to Amazon Sagemaker?",
        "Question_body":"<p>I am trying to set up Amazon sagemaker reading our dataset from our AWS Fsx for Lustre file system.<\/p>\n<p>We are using the Sagemaker API, and previously we were reading our dataset from s3 which worked fine:<\/p>\n<pre><code>estimator = TensorFlow(\n   entry_point='model_script.py',  \n   image_uri='some-repo:some-tag', \n   instance_type='ml.m4.10xlarge',\n   instance_count=1,\n   role=role,\n   framework_version='2.0.0',\n   py_version='py3',\n   subnets=[&quot;subnet-1&quot;],\n   security_group_ids=[&quot;sg-1&quot;, &quot;sg-2&quot;],\n   debugger_hook_config=False,\n  )\nestimator.fit({\n    'training': f&quot;s3:\/\/bucket_name\/data\/{hyperparameters['dataset']}\/&quot;}\n)\n<\/code><\/pre>\n<p>But now that I'm changing the input data source to Fsx Lustre file system, I'm getting an error that the file input should be s3:\/\/ or file:\/\/. I was following these <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"nofollow noreferrer\">docs (fsx lustre)<\/a>:<\/p>\n<pre><code>estimator = TensorFlow(\n   entry_point='model_script.py',  \n#    image_uri='some-docker:some-tag', \n   instance_type='ml.m4.10xlarge',\n   instance_count=1,\n   role=role,\n   framework_version='2.0.0',\n   py_version='py3',\n   subnets=[&quot;subnet-1&quot;],\n   security_group_ids=[&quot;sg-1&quot;, &quot;sg-2&quot;],\n   debugger_hook_config=False,\n  )\nfsx_data_folder = FileSystemInput(file_system_id='fs-1',\n                                    file_system_type='FSxLustre',\n                                    directory_path='\/fsx\/data',\n                                    file_system_access_mode='ro')\nestimator.fit(f&quot;{fsx_data_folder}\/{hyperparameters['dataset']}\/&quot;)\n<\/code><\/pre>\n<p>Throws the following error:<\/p>\n<pre><code>ValueError: URI input &lt;sagemaker.inputs.FileSystemInput object at 0x0000016A6C7F0788&gt;\/dataset_name\/ must be a valid S3 or FILE URI: must start with &quot;s3:\/\/&quot; or &quot;file:\/\/&quot;\n<\/code><\/pre>\n<p>Does anyone understand what I am doing wrong? Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1607617217073,
        "Question_score":2,
        "Question_tags":"amazon-web-services|tensorflow|amazon-ec2|amazon-sagemaker|amazon-fsx",
        "Question_view_count":592,
        "Owner_creation_time":1591204989660,
        "Owner_last_access_time":1638799637903,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1647906496649,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65238339",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61631687,
        "Question_title":"AWS Sagemaker failure after successful training \"ClientError: Artifact upload failed:Insufficient disk space\"",
        "Question_body":"<p>I'm training a network using custom docker image. First training with 50.000 steps everythig was ok, when I tried to increase to 80.000, I got error: \"ClientError: Artifact upload failed:Insufficient disk space\", I just increased the steps number.. this is weird to me. There are no errors in the cloudwatch log, my last entry is: <\/p>\n\n<blockquote>\n  <p>Successfully generated graphs: ['pipeline.config', 'tflite_graph.pb',\n  'frozen_inference_graph.pb', 'tflite_graph.pbtxt',\n  'tflite_quant_graph.tflite', 'saved_model', 'hyperparameters.json',\n  'label_map.pbtxt', 'model.ckpt.data-00000-of-00001',\n  'model.ckpt.meta', 'model.ckpt.index', 'checkpoint']<\/p>\n<\/blockquote>\n\n<p>Which basically means that those files have been created because is a simple:<\/p>\n\n<pre><code>    graph_files = os.listdir(model_path + '\/graph')\n<\/code><\/pre>\n\n<p>Which disk space is talking about? Also looking at the training job I see from the disk utilization chart that the rising curve peaks at 80%...\nI expect that after the successful creation of the aforementioned files, everything is uploaded to my s3 bucket, where no disk space issues are present. Why 50.000 steps is working and 80.000 is not working? \nIt is my understanding that the number of training steps don't influence the size of the model files..<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1588756994270,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1744,
        "Owner_creation_time":1416346350293,
        "Owner_last_access_time":1664039219437,
        "Owner_location":"Jesi, Italy",
        "Owner_reputation":2302,
        "Owner_up_votes":51,
        "Owner_down_votes":4,
        "Owner_views":227,
        "Question_last_edit_time":1588845916390,
        "Answer_body":"<p>Adding volume size to the training job selecting \"additional storage volume per instance (gb)\" to 5GB on the creation seems to solve the problem. I still don't understand why, but problem seems solved.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1588846223956,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61631687",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65407274,
        "Question_title":"AWS SageMaker (with internet disabled) unable to connect to STS",
        "Question_body":"<p>I am unable to perform the simple action:<\/p>\n<pre><code>import sagemaker\nsess = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n<\/code><\/pre>\n<p>because my notebook instance is not connected to the internet. I have an STS endpoint interface in the same subnet as my notebook instance but I thought the sagemaker API is using the global endpoint. I actually get the following error message after a while:<\/p>\n<pre><code>ConnectTimeoutError: Connect timeout on endpoint URL: &quot;https:\/\/sts.us-east-1.amazonaws.com\/&quot;\n<\/code><\/pre>\n<p>How do I fix this? Or does one need to update the sagemaker module?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1608633852607,
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-cli|amazon-vpc|amazon-sagemaker",
        "Question_view_count":510,
        "Owner_creation_time":1597855076910,
        "Owner_last_access_time":1622833516510,
        "Owner_location":"Delft, Netherlands",
        "Owner_reputation":60,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>So the solution is to include a VPC endpoint for the sagemaker API (api.sagemaker...) as well as STS.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1608634938236,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65407274",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69024005,
        "Question_title":"How to use SageMaker Estimator for model training and saving",
        "Question_body":"<p>The documentations of how to use SageMaker estimators are scattered around, sometimes obsolete, incorrect. Is there a one stop location which gives the comprehensive views of how to use SageMaker SDK Estimator to train and save models?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630555307170,
        "Question_score":27,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":6655,
        "Owner_creation_time":1416648155470,
        "Owner_last_access_time":1664057583237,
        "Owner_location":null,
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Question_last_edit_time":null,
        "Answer_body":"<h1>Answer<\/h1>\n<p>There is no one such resource from AWS that provides the comprehensive view of how to use SageMaker SDK Estimator to train and save models.<\/p>\n<h2>Alternative Overview Diagram<\/h2>\n<p>I put a diagram and brief explanation to get the overview on how SageMaker Estimator runs a training.<\/p>\n<ol>\n<li><p>SageMaker sets up a docker container for a training job where:<\/p>\n<ul>\n<li>Environment variables are set as in <a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container. Environment Variables<\/a>.<\/li>\n<li>Training data is setup under <code>\/opt\/ml\/input\/data<\/code>.<\/li>\n<li>Training script codes are setup under <code>\/opt\/ml\/code<\/code>.<\/li>\n<li><code>\/opt\/ml\/model<\/code> and <code>\/opt\/ml\/output<\/code> directories are setup to store training outputs.<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json  &lt;--- From Estimator hyperparameter arg\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;        &lt;--- From Estimator fit method inputs arg\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 code\n\u2502   \u2514\u2500\u2500 &lt;code files&gt;              &lt;--- From Estimator src_dir arg\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;             &lt;--- Location to save the trained model artifacts\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure                   &lt;--- Training job failure logs\n<\/code><\/pre>\n<ol start=\"2\">\n<li><p>SageMaker Estimator <code>fit(inputs)<\/code> method executes the training script. Estimator <code>hyperparameters<\/code> and <code>fit<\/code> method <code>inputs<\/code> are provided as its command line arguments.<\/p>\n<\/li>\n<li><p>The training script saves the model artifacts in the <code>\/opt\/ml\/model<\/code> once the training is completed.<\/p>\n<\/li>\n<li><p>SageMaker archives the artifacts under <code>\/opt\/ml\/model<\/code> into <code>model.tar.gz<\/code> and save it to the S3 location specified to <code>output_path<\/code> Estimator parameter.<\/p>\n<\/li>\n<li><p>You can set Estimator <code>metric_definitions<\/code> parameter to extract model metrics from the training logs. Then you can monitor the training progress in the SageMaker console metrics.<\/p>\n<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I believe AWS needs to stop mass-producing verbose, redundant, wordy, scattered, and obsolete documents. AWS needs to understand <strong>A picture is worth thousand words<\/strong>.<\/p>\n<p>Have diagrams and piece document parts together in a <strong>context<\/strong> with a clear objective to achieve.<\/p>\n<hr \/>\n<h1>Problem<\/h1>\n<p>AWS documentations need serious re-design and re-structuring. Just to understand <strong>how to train and save a model<\/strong> forces us going through dozens of scattered,  fragmented, verbose, redundant documentations, which are often obsolete, incomplete, and sometime incorrect.<\/p>\n<p>It is well-summarized in <a href=\"https:\/\/nandovillalba.medium.com\/why-i-think-gcp-is-better-than-aws-ea78f9975bda\" rel=\"noreferrer\">Why I think GCP is better than AWS<\/a>:<\/p>\n<blockquote>\n<p>It\u2019s not that AWS is harder to use than GCP, it\u2019s that <strong>it is needlessly hard<\/strong>; a disjointed, sprawl of infrastructure primitives with poor cohesion between them.  <br><br>\nA challenge is nice, a confusing mess is not, and <strong>the problem with AWS is that a large part of your working hours will be spent untangling their documentation and weeding through features and products to find what you want<\/strong>, rather than focusing on cool interesting challenges.<\/p>\n<\/blockquote>\n<p>Especially the SageMaker team keeps changing implementations without updating documents. Its roll-out was also inconsistent, e.g. SDK version 2 was rolled out in the SageMaker Studio making the AWS examples in Github incompatible without announcing it. Whereas SageMaker instance still had SDK 1, hence code worked in Instance but not in Studio.<\/p>\n<p>It is mind-boggling that we have to go through these many documents below to understand how to use the SageMaker SDK Estimator for training.<\/p>\n<h2>Documents for Model Training<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\" rel=\"noreferrer\">Train a Model with Amazon SageMaker<\/a><\/li>\n<\/ul>\n<p>This document gives 20,000 feet overview of how SageMaker training but does not give any clue what to do.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<p>This document gives an overview of how SageMaker training looks like. However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<blockquote>\n<p>WARNING: This package has been deprecated. Please use the SageMaker Training Toolkit for model training and the SageMaker Inference Toolkit for model serving.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model.html\" rel=\"noreferrer\">Step 4: Train a Model<\/a><\/li>\n<\/ul>\n<p>This document layouts the steps for training.<\/p>\n<blockquote>\n<p>The Amazon SageMaker Python SDK provides framework estimators and generic estimators to train your model while orchestrating the machine learning (ML) lifecycle accessing the SageMaker features for training and the AWS infrastructures<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#train-a-model-with-the-sagemaker-python-sdk\" rel=\"noreferrer\">Train a Model with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To train a model by using the SageMaker Python SDK, you:<\/p>\n<ul>\n<li>Prepare a training script<\/li>\n<li>Create an estimator<\/li>\n<li>Call the fit method of the estimator<\/li>\n<\/ul>\n<\/blockquote>\n<p>Finally this document gives concrete steps and ideas. However still missing comprehensiv details about Environment Variables, Directory structure in the SageMaker docker container**, S3 for uploading code, placing data, S3 where the trained model is saved, etc.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html\" rel=\"noreferrer\">Use TensorFlow with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<p>This documents is focused on TensorFlow Estimator implementation steps. Use <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/get_started_mnist_train.ipynb\" rel=\"noreferrer\">Training a Tensorflow Model on MNIST<\/a> Github example to accompany with to follow the actual implementation.<\/p>\n<h2>Documents for passing parameters and data locations<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig\" rel=\"noreferrer\">How Amazon SageMaker Provides Training Information<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>This section explains how SageMaker makes training information, such as training data, hyperparameters, and other configuration information, available to your Docker container.<\/p>\n<\/blockquote>\n<p>This document finally gives the idea of how parameters and data are passed around but again, not comprehensive.<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container Environment Variables<\/a><\/li>\n<\/ul>\n<p>This documentation is marked as <strong>deprecated<\/strong> but the only document which explains the SageMaker Environment Variables.<\/p>\n<blockquote>\n<h3>IMPORTANT ENVIRONMENT VARIABLES<\/h3>\n<ul>\n<li>SM_MODEL_DIR<\/li>\n<li>SM_CHANNELS<\/li>\n<li>SM_CHANNEL_{channel_name}<\/li>\n<li>SM_HPS<\/li>\n<li>SM_HP_{hyperparameter_name}<\/li>\n<li>SM_CURRENT_HOST<\/li>\n<li>SM_HOSTS<\/li>\n<li>SM_NUM_GPUS<\/li>\n<\/ul>\n<h3>List of provided environment variables by SageMaker Containers<\/h3>\n<ul>\n<li>SM_NUM_CPUS<\/li>\n<li>SM_LOG_LEVEL<\/li>\n<li>SM_NETWORK_INTERFACE_NAME<\/li>\n<li>SM_USER_ARGS<\/li>\n<li>SM_INPUT_DIR<\/li>\n<li>SM_INPUT_CONFIG_DIR<\/li>\n<li>SM_OUTPUT_DATA_DIR<\/li>\n<li>SM_RESOURCE_CONFIG<\/li>\n<li>SM_INPUT_DATA_CONFIG<\/li>\n<li>SM_TRAINING_ENV<\/li>\n<\/ul>\n<\/blockquote>\n<h2>Documents for SageMaker Docker Container Directory Structure<\/h2>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure\n<\/code><\/pre>\n<p>This document explains the directory structure and purpose of each directory.<\/p>\n<blockquote>\n<h3>The input<\/h3>\n<ul>\n<li>\/opt\/ml\/input\/config contains information to control how your program runs. hyperparameters.json is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. resourceConfig.json is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn\u2019t support distributed training, we\u2019ll ignore it here.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;\/ (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it\u2019s generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;_&lt;epoch_number&gt; (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch.<\/li>\n<\/ul>\n<h3>The output<\/h3>\n<ul>\n<li>\/opt\/ml\/model\/ is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the DescribeTrainingJob result.<\/li>\n<li>\/opt\/ml\/output is a directory where the algorithm can write a file failure that describes why the job failed. The contents of this file will be returned in the FailureReason field of the DescribeTrainingJob result. For jobs that succeed, there is no reason to write this file as it will be ignored.<\/li>\n<\/ul>\n<\/blockquote>\n<p>However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<h2>Documents for Model Saving<\/h2>\n<p>The information on where the trained model is saved and in what format are fundamentally missing. The training script needs to save the model under <code>\/opt\/ml\/model<\/code> and the format and sub-directory structure depend on the frameworks e,g TensorFlow, Pytorch. This is because SageMaker deployment uses the Framework dependent model-serving, e,g. TensorFlow Serving for TensorFlow framework.<\/p>\n<p>This is not clearly documented and causing confusions. The developer needs to specify which format to use and under which sub-directory to save.<\/p>\n<p>To use TensorFlow Estimator training and deployment:<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/aws_sagemaker_studio\/frameworks\/keras_pipe_mode_horovod\/keras_pipe_mode_horovod_cifar10.html#Deploy-the-trained-model\" rel=\"noreferrer\">Deploy the trained model<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>Because <strong>we\u2019re using TensorFlow Serving for deployment<\/strong>, our training script <strong>saves the model in TensorFlow\u2019s SavedModel format<\/strong>.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/code\/train.py#L159-L166\" rel=\"noreferrer\">amazon-sagemaker-examples\/frameworks\/tensorflow\/code\/train.py <\/a><\/li>\n<\/ul>\n<pre><code>    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    ckpt_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n    model.save(ckpt_dir)\n<\/code><\/pre>\n<p>The code is saving the model in <code>\/opt\/ml\/model\/00000000<\/code> because this is for TensorFlow serving.<\/p>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/guide\/saved_model\" rel=\"noreferrer\">Using the SavedModel format<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>The save-path follows a convention used by TensorFlow Serving where the last path component (1\/ here) is a version number for your model - it allows tools like Tensorflow Serving to reason about the relative freshness.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/tfx\/tutorials\/serving\/rest_simple#save_your_model\" rel=\"noreferrer\">Train and serve a TensorFlow model with TensorFlow Serving<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To load our trained model into TensorFlow Serving we first need to save it in SavedModel format. This will create a protobuf file in a well-defined directory hierarchy, and will include a version number. TensorFlow Serving allows us to select which version of a model, or &quot;servable&quot; we want to use when we make inference requests. Each version will be exported to a different sub-directory under the given path.<\/p>\n<\/blockquote>\n<h2>Documents for API<\/h2>\n<p>Basically the SageMaker SDK Estimator implements the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html\" rel=\"noreferrer\">CreateTrainingJob<\/a> API for training part. Hence, better to understand how it is designed and what parameters need to be defined. Otherwise working on Estimators are like walking in the dark.<\/p>\n<hr \/>\n<h1>Example<\/h1>\n<h2>Jupyter Notebook<\/h2>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\nbucket = sagemaker_session.default_bucket()\n\nmetric_definitions = [\n    {&quot;Name&quot;: &quot;train:loss&quot;, &quot;Regex&quot;: &quot;.*loss: ([0-9\\\\.]+) - accuracy: [0-9\\\\.]+.*&quot;},\n    {&quot;Name&quot;: &quot;train:accuracy&quot;, &quot;Regex&quot;: &quot;.*loss: [0-9\\\\.]+ - accuracy: ([0-9\\\\.]+).*&quot;},\n    {\n        &quot;Name&quot;: &quot;validation:accuracy&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: ([0-9\\\\.]+).*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;validation:loss&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: ([0-9\\\\.]+) - val_accuracy: [0-9\\\\.]+.*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;sec\/sample&quot;,\n        &quot;Regex&quot;: &quot;.* - \\d+s (\\d+)[mu]s\/sample - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: [0-9\\\\.]+&quot;,\n    },\n]\n\nimport uuid\n\ncheckpoint_s3_prefix = &quot;checkpoints\/{}&quot;.format(str(uuid.uuid4()))\ncheckpoint_s3_uri = &quot;s3:\/\/{}\/{}\/&quot;.format(bucket, checkpoint_s3_prefix)\n\nfrom sagemaker.tensorflow import TensorFlow\n\n# --------------------------------------------------------------------------------\n# 'trainingJobName' msut satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}\n# --------------------------------------------------------------------------------\nbase_job_name = &quot;fashion-mnist&quot;\nhyperparameters = {\n    &quot;epochs&quot;: 2, \n    &quot;batch-size&quot;: 64\n}\nestimator = TensorFlow(\n    entry_point=&quot;fashion_mnist.py&quot;,\n    source_dir=&quot;src&quot;,\n    metric_definitions=metric_definitions,\n    hyperparameters=hyperparameters,\n    role=role,\n    input_mode='File',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    base_job_name=base_job_name,\n    checkpoint_s3_uri=checkpoint_s3_uri,\n    model_dir=False\n)\nestimator.fit()\n<\/code><\/pre>\n<h2>fashion_mnist.py<\/h2>\n<pre><code>import os\nimport argparse\nimport json\nimport multiprocessing\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras import backend as K\n\nprint(&quot;TensorFlow version: {}&quot;.format(tf.__version__))\nprint(&quot;Eager execution is: {}&quot;.format(tf.executing_eagerly()))\nprint(&quot;Keras version: {}&quot;.format(tf.keras.__version__))\n\n\nimage_width = 28\nimage_height = 28\n\n\ndef load_data():\n    fashion_mnist = tf.keras.datasets.fashion_mnist\n    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n\n    number_of_classes = len(set(y_train))\n    print(&quot;number_of_classes&quot;, number_of_classes)\n\n    x_train = x_train \/ 255.0\n    x_test = x_test \/ 255.0\n    x_full = np.concatenate((x_train, x_test), axis=0)\n    print(x_full.shape)\n\n    print(type(x_train))\n    print(x_train.shape)\n    print(x_train.dtype)\n    print(y_train.shape)\n    print(y_train.dtype)\n\n    # ## Train\n    # * C: Convolution layer\n    # * P: Pooling layer\n    # * B: Batch normalization layer\n    # * F: Fully connected layer\n    # * O: Output fully connected softmax layer\n\n    # Reshape data based on channels first \/ channels last strategy.\n    # This is dependent on whether you use TF, Theano or CNTK as backend.\n    # Source: https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/mnist_cnn.py\n    if K.image_data_format() == 'channels_first':\n        x = x_train.reshape(x_train.shape[0], 1, image_width, image_height)\n        x_test = x_test.reshape(x_test.shape[0], 1, image_width, image_height)\n        input_shape = (1, image_width, image_height)\n    else:\n        x_train = x_train.reshape(x_train.shape[0], image_width, image_height, 1)\n        x_test = x_test.reshape(x_test.shape[0], image_width, image_height, 1)\n        input_shape = (image_width, image_height, 1)\n\n    return x_train, y_train, x_test, y_test, input_shape, number_of_classes\n\n# tensorboard --logdir=\/full_path_to_your_logs\n\nvalidation_split = 0.2\nverbosity = 1\nuse_multiprocessing = True\nworkers = multiprocessing.cpu_count()\n\n\ndef train(model, x, y, args):\n    # SavedModel Output\n    tensorflow_saved_model_path = os.path.join(args.model_dir, &quot;tensorflow\/saved_model\/0&quot;)\n    os.makedirs(tensorflow_saved_model_path, exist_ok=True)\n\n    # Tensorboard Logs\n    tensorboard_logs_path = os.path.join(args.model_dir, &quot;tensorboard\/&quot;)\n    os.makedirs(tensorboard_logs_path, exist_ok=True)\n\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=tensorboard_logs_path,\n        write_graph=True,\n        write_images=True,\n        histogram_freq=1,  # How often to log histogram visualizations\n        embeddings_freq=1,  # How often to log embedding visualizations\n        update_freq=&quot;epoch&quot;,\n    )  # How often to write logs (default: once per epoch)\n\n    model.compile(\n        optimizer='adam',\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\n        metrics=['accuracy']\n    )\n    history = model.fit(\n        x,\n        y,\n        shuffle=True,\n        batch_size=args.batch_size,\n        epochs=args.epochs,\n        validation_split=validation_split,\n        use_multiprocessing=use_multiprocessing,\n        workers=workers,\n        verbose=verbosity,\n        callbacks=[\n            tensorboard_callback\n        ]\n    )\n    return history\n\n\ndef create_model(input_shape, number_of_classes):\n    model = Sequential([\n        Conv2D(\n            name=&quot;conv01&quot;,\n            filters=32,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=&quot;same&quot;,\n            activation='relu',\n            input_shape=input_shape\n        ),\n        MaxPooling2D(\n            name=&quot;pool01&quot;,\n            pool_size=(2, 2)\n        ),\n        Flatten(),  # 3D shape to 1D.\n        BatchNormalization(\n            name=&quot;batch_before_full01&quot;\n        ),\n        Dense(\n            name=&quot;full01&quot;,\n            units=300,\n            activation=&quot;relu&quot;\n        ),  # Fully connected layer\n        Dense(\n            name=&quot;output_softmax&quot;,\n            units=number_of_classes,\n            activation=&quot;softmax&quot;\n        )\n    ])\n    return model\n\n\ndef save_model(model, args):\n    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    model_save_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(model_save_dir):\n        os.makedirs(model_save_dir)\n    print(f&quot;saving model at {model_save_dir}&quot;)\n    model.save(model_save_dir)\n\n\ndef parse_args():\n    # --------------------------------------------------------------------------------\n    # https:\/\/docs.python.org\/dev\/library\/argparse.html#dest\n    # --------------------------------------------------------------------------------\n    parser = argparse.ArgumentParser()\n\n    # --------------------------------------------------------------------------------\n    # hyperparameters Estimator argument are passed as command-line arguments to the script.\n    # --------------------------------------------------------------------------------\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch-size', type=int, default=64)\n\n    # \/opt\/ml\/model\n    # sagemaker.tensorflow.estimator.TensorFlow override 'model_dir'.\n    # See https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/\\\n    # sagemaker.tensorflow.html#sagemaker.tensorflow.estimator.TensorFlow\n    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n\n    # \/opt\/ml\/output\n    parser.add_argument(&quot;--output_dir&quot;, type=str, default=os.environ[&quot;SM_OUTPUT_DIR&quot;])\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == &quot;__main__&quot;:\n    args = parse_args()\n    print(&quot;---------- key\/value args&quot;)\n    for key, value in vars(args).items():\n        print(f&quot;{key}:{value}&quot;)\n\n    x_train, y_train, x_test, y_test, input_shape, number_of_classes = load_data()\n    model = create_model(input_shape, number_of_classes)\n\n    history = train(model=model, x=x_train, y=y_train, args=args)\n    print(history)\n    \n    save_model(model, args)\n    results = model.evaluate(x_test, y_test, batch_size=100)\n    print(&quot;test loss, test accuracy:&quot;, results)\n<\/code><\/pre>\n<h2>SageMaker Console<\/h2>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Notebook output<\/h2>\n<pre><code>2021-09-03 03:02:04 Starting - Starting the training job...\n2021-09-03 03:02:16 Starting - Launching requested ML instancesProfilerReport-1630638122: InProgress\n......\n2021-09-03 03:03:17 Starting - Preparing the instances for training.........\n2021-09-03 03:04:59 Downloading - Downloading input data\n2021-09-03 03:04:59 Training - Downloading the training image...\n2021-09-03 03:05:23 Training - Training image download completed. Training in progress.2021-09-03 03:05:23.966037: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:23.969704: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n2021-09-03 03:05:24.118054: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:26,842 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n2021-09-03 03:05:26,852 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:27,734 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n\/usr\/local\/bin\/python3.7 -m pip install -r requirements.txt\nWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\nYou should consider upgrading via the '\/usr\/local\/bin\/python3.7 -m pip install --upgrade pip' command.\n\n2021-09-03 03:05:29,028 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,045 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,062 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,072 sagemaker-training-toolkit INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {},\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;batch-size&quot;: 64,\n        &quot;epochs&quot;: 2\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {},\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;fashion_mnist&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 4,\n    &quot;num_gpus&quot;: 0,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;fashion_mnist.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;batch-size&quot;:64,&quot;epochs&quot;:2}\nSM_USER_ENTRY_POINT=fashion_mnist.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=fashion_mnist\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=4\nSM_NUM_GPUS=0\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;batch-size&quot;:64,&quot;epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;fashion_mnist&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:4,&quot;num_gpus&quot;:0,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;fashion_mnist.py&quot;}\nSM_USER_ARGS=[&quot;--batch-size&quot;,&quot;64&quot;,&quot;--epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_HP_BATCH-SIZE=64\nSM_HP_EPOCHS=2\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/local\/lib\/python37.zip:\/usr\/local\/lib\/python3.7:\/usr\/local\/lib\/python3.7\/lib-dynload:\/usr\/local\/lib\/python3.7\/site-packages\n\nInvoking script with the following command:\n\n\/usr\/local\/bin\/python3.7 fashion_mnist.py --batch-size 64 --epochs 2\n\n\nTensorFlow version: 2.3.1\nEager execution is: True\nKeras version: 2.4.0\n---------- key\/value args\nepochs:2\nbatch_size:64\nmodel_dir:\/opt\/ml\/model\noutput_dir:\/opt\/ml\/output\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1630555307169,
        "Answer_score":65.0,
        "Question_favorite_count":14.0,
        "Answer_last_edit_time":1656913410063,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69024005",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70553230,
        "Question_title":"How to make predictions on a sagemaker endpoint? (JSON error)",
        "Question_body":"<p>I have deployed a sagemaker endpoint and want to run predictions on the endpoint now. The endpoint represents a sagemaker pipeline and model. I followed the tutorial <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-pipelines\/tabular\/train-register-deploy-pipeline-model\/train%20register%20and%20deploy%20a%20pipeline%20model.ipynb\" rel=\"nofollow noreferrer\">here<\/a>. My code to set up the predictor and make the predictions is as follows:<\/p>\n<pre><code>from sagemaker.predictor import Predictor\npredictor = Predictor(endpoint_name=endpoint_name)\ndata_df = data_df.drop(&quot;LABEL_NAME&quot;, axis=1)\npred_count = 1\npayload = data_df.iloc[:pred_count].to_string(header=False, index=False).replace(&quot;  &quot;, &quot;,&quot;)\np = predictor.predict(payload, initial_args={&quot;ContentType&quot;: &quot;text\/csv&quot;})\n<\/code><\/pre>\n<p>This code is pretty much what they have displayed in the example I linked and it makes sense to me. My preprocess.py code for the pipeline includes the following functions which I am including (although not sure they are relevant):<\/p>\n<pre><code>def input_fn(input_data, content_type):\n    print(&quot;BAHHHHHH&quot;)\n    if content_type == &quot;text\/csv&quot;:\n        # Read the raw input data as CSV.\n        df = pd.read_csv(StringIO(input_data), header=None)\n        return df\n    else:\n        raise ValueError(&quot;{} not supported by script!&quot;.format(content_type))\n\ndef output_fn(prediction, accept):\n    print(&quot;BAHHHHHH&quot;)\n    if accept == &quot;application\/json&quot;:\n        instances = []\n        for row in prediction.tolist():\n            instances.append(row)\n        json_output = {&quot;instances&quot;: instances}\n\n        return worker.Response(json.dumps(json_output), mimetype=accept)\n    elif accept == &quot;text\/csv&quot;:\n        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n    else:\n        raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))\n\ndef predict_fn(input_data, model):\n    print(&quot;BAHHHHHH&quot;)\n    features = model.transform(input_data)\n    return features\n\ndef model_fn(model_dir):\n    print(&quot;BAHHHHHH&quot;)\n    &quot;&quot;&quot;Deserialize fitted model&quot;&quot;&quot;\n    preprocessor = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return preprocessor\n<\/code><\/pre>\n<p>When running the predictor.predict() method I get the following error:<\/p>\n<pre><code>botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message &quot;{\n    &quot;error&quot;: &quot;JSON Parse error: Missing a comma or ']' after an array element. at offset: 16&quot;\n<\/code><\/pre>\n<p>I printed out the payload variable right before it was passed to the predict method and it looks like this (I truncated it as it's quite long but this should be enough to see what is is like:<\/p>\n<pre><code>0 999.105105 888.607813 6.0 1 los angeles 2431.666667 1.0 NaN 1177.813623 1.076833e+06 los angeles$1$6 0 60376511012 0.0 0.0 0.0 0.0 0.0 0.0 ............\n<\/code><\/pre>\n<p>The error message also provides a url to look at for more information. It is the cloud watch logs for the endpoint. looking through these logs I see no extra information, just a 400 error with NO additional information apart from the 400 error.<\/p>\n<p>So there is obviously some issue with the format of the data I am passing in. The input_fn, output_fn, predict_fn and model_fn methods all have a print statements in them at the start of the method but none of these show up in the logs so I don't think any of these are being reached.<\/p>\n<p>What am i doing wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1641090584810,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|prediction|amazon-sagemaker",
        "Question_view_count":635,
        "Owner_creation_time":1467398269073,
        "Owner_last_access_time":1663957538923,
        "Owner_location":null,
        "Owner_reputation":2733,
        "Owner_up_votes":40,
        "Owner_down_votes":2,
        "Owner_views":273,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70553230",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55949625,
        "Question_title":"How to read json.out file from databricks",
        "Question_body":"<p>I have been working with databricks for reading output from Object2Vec in Sagemaker. This output is saved as jsonlines with <code>.json.out<\/code> file format.<\/p>\n\n<pre><code>df_emb = spark.read.option(\"multiLine\", True).option(\"mode\", \"PERMISSIVE\").json(bucket+key)\n<\/code><\/pre>\n\n<p>When i read this file as a json, it is read as a corrupt record. Below is the screenshot. \n<a href=\"https:\/\/i.stack.imgur.com\/jyMBj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jyMBj.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I can provide the actual file if you know the solution.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1556790025647,
        "Question_score":0,
        "Question_tags":"amazon-web-services|databricks|amazon-sagemaker",
        "Question_view_count":96,
        "Owner_creation_time":1457261731840,
        "Owner_last_access_time":1663974423117,
        "Owner_location":"Vancouver, BC",
        "Owner_reputation":584,
        "Owner_up_votes":71,
        "Owner_down_votes":6,
        "Owner_views":270,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55949625",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58535527,
        "Question_title":"Amazon Sagemaker ResourceLimitExceeded Error for XGBoost (Free Tier)",
        "Question_body":"<p>I am trying to create an XGBoost model in free tier AWS Sagemaker. I am getting an error of:<\/p>\n\n<p><em>\"ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m5.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances.\"<\/em>.<\/p>\n\n<p>What is the right train_instance_type I should use?<\/p>\n\n<p>Here is my code:<\/p>\n\n<pre><code># import libraries\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np                                \nimport pandas as pd                               \nimport matplotlib.pyplot as plt                   \nfrom IPython.display import Image                 \nfrom IPython.display import display               \nfrom time import gmtime, strftime                 \nfrom sagemaker.predictor import csv_serializer   \n\n# Define IAM role\nrole = get_execution_role()\nprefix = 'sagemaker\/DEMO-xgboost-dm'\ncontainers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com\/xgboost:latest',\n              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest',\n              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest',\n              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'} # each region has its XGBoost container\nmy_region = boto3.session.Session().region_name # set the region of the instance\n\n# Create an instance of the XGBoost model (an estimator), and define the model\u2019s hyperparameters.\n# Note: train_instance_type='ml.m5.large' has 0 free credits! Use one of https:\/\/aws.amazon.com\/sagemaker\/pricing\/ \nsess = sagemaker.Session()\nxgb = sagemaker.estimator.Estimator(containers[my_region],role, train_instance_count=1, train_instance_type='ml.m5.xlarge',output_path='s3:\/\/{}\/{}\/output'.format('my_s3_bucket', prefix),sagemaker_session=sess)\nxgb.set_hyperparameters(max_depth=1,eta=0.2,gamma=4,min_child_weight=6,subsample=0.8,silent=0,objective='binary:logistic',num_round=100)\n# Train the model using gradient optimization on a ml.m4.xlarge instance\n# After a few minutes, you should start to see the training logs being generated.\nxgb.fit({'train': s3_input_train})\n<\/code><\/pre>\n\n<p>At this step this is what I see:<\/p>\n\n<pre><code>2019-10-22 06:32:51 Starting - Starting the training job...\n2019-10-22 06:33:00 Starting - Launching requested ML instances......\n2019-10-22 06:33:54 Starting - Preparing the instances for training...\n2019-10-22 06:34:41 Downloading - Downloading input data...\n2019-10-22 06:35:22 Training - Training image download completed. Training in progress..Arguments: train\n[2019-10-22:06:35:22:INFO] Running standalone xgboost training.\n[2019-10-22:06:35:22:INFO] Path \/opt\/ml\/input\/data\/validation does not exist!\n[2019-10-22:06:35:22:INFO] File size need to be processed in the node: 3.38mb. Available memory size in the node: 8089.9mb\n[2019-10-22:06:35:22:INFO] Determined delimiter of CSV input is ','\n[06:35:22] S3DistributionType set as FullyReplicated\n[06:35:22] 28831x59 matrix with 1701029 entries loaded from \/opt\/ml\/input\/data\/train?format=csv&amp;label_column=0&amp;delimiter=,\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[0]#011train-error:0.102182\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[1]#011train-error:0.102182\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[2]#011train-error:0.102182\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[3]#011train-error:0.102182\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[4]#011train-error:0.102182\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[5]#011train-error:0.102182\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[6]#011train-error:0.102182\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[7]#011train-error:0.10839\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[8]#011train-error:0.102737\n[06:35:22] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[9]#011train-error:0.107697\n<\/code><\/pre>\n\n<p>And then when I deploy this:<\/p>\n\n<pre><code># Deploy the model on a server and create an endpoint that you can access\nxgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m5.xlarge')\n---------------------------------------------------------------------------\nResourceLimitExceeded                     Traceback (most recent call last)\n&lt;ipython-input-38-6d149f3edc98&gt; in &lt;module&gt;()\n      1 # Deploy the model on a server and create an endpoint that you can access\n----&gt; 2 xgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m5.xlarge')\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in deploy(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, use_compiled_model, update_endpoint, wait, model_name, kms_key, **kwargs)\n    559             tags=self.tags,\n    560             wait=wait,\n--&gt; 561             kms_key=kms_key,\n    562         )\n    563 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/model.py in deploy(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, update_endpoint, tags, kms_key, wait)\n    464         else:\n    465             self.sagemaker_session.endpoint_from_production_variants(\n--&gt; 466                 self.endpoint_name, [production_variant], tags, kms_key, wait\n    467             )\n    468 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in endpoint_from_production_variants(self, name, production_variants, tags, kms_key, wait)\n   1361 \n   1362             self.sagemaker_client.create_endpoint_config(**config_options)\n-&gt; 1363         return self.create_endpoint(endpoint_name=name, config_name=name, tags=tags, wait=wait)\n   1364 \n   1365     def expand_role(self, role):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in create_endpoint(self, endpoint_name, config_name, tags, wait)\n    975 \n    976         self.sagemaker_client.create_endpoint(\n--&gt; 977             EndpointName=endpoint_name, EndpointConfigName=config_name, Tags=tags\n    978         )\n    979         if wait:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    355                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    356             # The \"self\" in this scope is referring to the BaseClient.\n--&gt; 357             return self._make_api_call(operation_name, kwargs)\n    358 \n    359         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    659             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n    660             error_class = self.exceptions.from_code(error_code)\n--&gt; 661             raise error_class(parsed_response, operation_name)\n    662         else:\n    663             return parsed_response\n\nResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m5.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.\n<\/code><\/pre>\n\n<p><strong>Edit:<\/strong> Trying <strong>ml.m4.xlarge<\/strong> instance:<\/p>\n\n<p>When I use ml.m4.xlarge, I get the same message of \"ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.\"<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1571899258980,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|boto3|amazon-sagemaker",
        "Question_view_count":5276,
        "Owner_creation_time":1467054577583,
        "Owner_last_access_time":1655311941103,
        "Owner_location":"Dallas, TX, United States",
        "Owner_reputation":4281,
        "Owner_up_votes":374,
        "Owner_down_votes":160,
        "Owner_views":681,
        "Question_last_edit_time":1572003947572,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58535527",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73748608,
        "Question_title":"Debug SparkSQL Query",
        "Question_body":"<p>What are some way I can debug through a sparksql query?<\/p>\n<p>I have defined a dataframe with a sparpksql query and I have included show(1), but the query continues to run very long. Could anyone provide pointers? Thank you!<\/p>\n<pre><code>select t1.id\nfrom table1 t1\njoin table2 t2 on t1.id = t2.cd\nwhere t1.product = 'I'\nand not exists\n(select * from table3 t3\n        where t3.id = t1.id \n        and t3.year = t1.year\n        and a3.month = t1.month\n        and t3.day = t1.day\n        and t3.code in ('4321','5604'))\nand not exists\n(select MAX(status) from table4 t4\n        where t4.id = t1.id\n        and t4.year = t1.year\n        and t4.month = t1.month\n        and t4.day = t1.day\n        having max(status) &gt; 3)\n&quot;&quot;&quot;).show(1)\n<\/code><\/pre>\n<p>list_of_id_df.createOrReplaceTempView(&quot;list_of_id&quot;)\n<br>list_of_id_df.show(1)<\/p>\n<p>print(&quot;Done&quot;)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1663350527313,
        "Question_score":0,
        "Question_tags":"amazon-web-services|pyspark|amazon-sagemaker|amazon-sagemaker-debugger",
        "Question_view_count":19,
        "Owner_creation_time":1439843323250,
        "Owner_last_access_time":1664049758657,
        "Owner_location":null,
        "Owner_reputation":137,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":1663612124656,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73748608",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51966783,
        "Question_title":"Sagemaker invoke endpoint returned value type?",
        "Question_body":"<p>I have a classifier working with XGBoost in sagemaker, but despite the training set only having 1s and 0s in the first column (csv file, first column is assumed to be target in sagemaker xgboost), the algorithm returns a decimal. <\/p>\n\n<p>First 3 records return 1.08, 0.34, and 0.91. I'd assume probabilities but 1.08? If these are rounded to 0 or 1 then they're all correct, but why is it returning non-class values?<\/p>\n\n<p>Furthermore, the class only contains a predict method - is a predict probability method not possible without using your own model?<\/p>\n\n<p>The code calling this is:<\/p>\n\n<pre><code>from flask import Flask\nfrom flask import request\nimport boto3\nfrom sagemaker.predictor import csv_serializer\nimport sagemaker\n\napp = Flask(__name__)\n\n@app.route(\"\/\")\ndef hello():\n    numbers = request.args.get('numbers')\n\n    #session\n    boto_session = boto3.Session(profile_name=\"profilename\",\n                          region_name='regionname')\n\n    #sagemaker session\n    sagemaker_session = sagemaker.Session(boto_session=boto_session)\n\n    #endpoint\n    predictor = sagemaker.predictor.RealTimePredictor(endpoint=\"modelname\", \n        sagemaker_session=sagemaker_session)\n    predictor.content_type=\"text\/csv\"\n    predictor.serializer=csv_serializer\n    predictor.deserializer=None\n\n    #result\n    result=predictor.predict(numbers)\n    result=result.decode(\"utf-8\")\n    return f'Output: {result}'\n\nif __name__ == \"__main__\":\n    app.run(debug=True, port=5000)\n<\/code><\/pre>\n\n<p>The flask section works fine, I can retrieve predictions at 127.0.0.1:5000.<\/p>\n\n<p>Sagemaker Version 1.3.0. Version 1.9.0 does not work - it requires fcntrl which is mac\/linux only - see <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/311\" rel=\"nofollow noreferrer\">this on their repo<\/a>, apparently it's fixed on pypi but I've tried and the version doesn't change or fix the issue, so I'm stuck on 1.3.0 until they resolve it. Version 1.3.0 does not have a predict_proba method.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":6,
        "Question_creation_time":1534940431650,
        "Question_score":4,
        "Question_tags":"python|machine-learning|flask|xgboost|amazon-sagemaker",
        "Question_view_count":990,
        "Owner_creation_time":1530700602213,
        "Owner_last_access_time":1620423334107,
        "Owner_location":null,
        "Owner_reputation":104,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":1534965275236,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51966783",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53640440,
        "Question_title":"How to use boto3 cloudwatch for SageMaker submitted training jobs?",
        "Question_body":"<p>I have submitted a few training jobs from AWS SageMaker. I want to use boto3 cloudwatch api to fetch the cloudwatch data to be displayed within jupyter notebook instead of using CloudWatch UI. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1544042637003,
        "Question_score":0,
        "Question_tags":"amazon-web-services|boto3|amazon-cloudwatch|amazon-sagemaker",
        "Question_view_count":588,
        "Owner_creation_time":1488324416977,
        "Owner_last_access_time":1553739959547,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53640440",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61181955,
        "Question_title":"Is there any limits of saving result on S3 from sagemaker Processing?",
        "Question_body":"<p>\u203b I used google translation, if you have any question, let me know!<\/p>\n\n<p>I am trying to run python script with huge 4 data, using sagemaker processing. And my current situation are as follows:<\/p>\n\n<ul>\n<li>can run this script with 3 data<\/li>\n<li>can't run the script with only 1 data (the biggest, the same structure with others)<\/li>\n<li>as for all of 4 data, the script has finished (so, I suspected this error in S3, ie. when copying sagemaker result to S3)<\/li>\n<\/ul>\n\n<p>The error I got is this InternalServerError.<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"sagemaker_train_and_predict.py\", line 56, in &lt;module&gt;\n    outputs=outputs\n  File \"{xxx}\/sagemaker_constructor.py\", line 39, in run\n    outputs=outputs\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 408, in run\n    self.latest_job.wait(logs=logs)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 723, in wait\n    self.sagemaker_session.logs_for_processing_job(self.job_name, wait=True)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 3111, in logs_for_processing_job\n    self._check_job_status(job_name, description, \"ProcessingJobStatus\")\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 2615, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Processing job sagemaker-vm-train-and-predict-2020-04-12-04-15-40-655: Failed. Reason: InternalServerError: We encountered an internal error.  Please try again.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586755348657,
        "Question_score":1,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":95,
        "Owner_creation_time":1586754432800,
        "Owner_last_access_time":1593512686190,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There may be some issue transferring the output data to S3 if the output is generated at a high rate and size is too large. <\/p>\n\n<p>You can 1) try to slow down writing the output a bit or 2) call S3 from your algorithm container to upload the output directly using boto client (<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html<\/a>).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1586886613192,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61181955",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72580651,
        "Question_title":"Copying python scripts into docker container in sagemaker",
        "Question_body":"<p>I have been frustrated by this copy error while trying to copy a python script to docker container. I am doing this in AWS Sagemaker. Anyone can suggest why this error happens.\nThe train_sm.py code I am trying to copy is inside the src folder. The dockerfile is inside the docker folder.<\/p>\n<p>What is the build context here?\nI also tried moving the .py scripts inside the docker folder, and it still same error.\nThis seems small problem, but has taken my whole day. I\nAny help and hints are appreciated.\nI have attached a screenshot of codes, folder structures and error.<\/p>\n<p>Thanks\n<a href=\"https:\/\/i.stack.imgur.com\/wZvYc.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wZvYc.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1654903827130,
        "Question_score":0,
        "Question_tags":"python|docker|jupyter|amazon-sagemaker",
        "Question_view_count":68,
        "Owner_creation_time":1501194889140,
        "Owner_last_access_time":1663970802447,
        "Owner_location":"Seattle, WA, United States",
        "Owner_reputation":119,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":36,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72580651",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69775255,
        "Question_title":"Recover deleted AWS Sagemaker Jupyter Notebook",
        "Question_body":"<p>I think I may have accidentally deleted a notebook in my Sagemaker instance, is there a way to recover it? Any help is greatly appreciated, thanks!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1635545301633,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":238,
        "Owner_creation_time":1631568026793,
        "Owner_last_access_time":1653849249770,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69775255",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60421123,
        "Question_title":"AWS Sagemaker batch transform with JSON input filter",
        "Question_body":"<p>I have a custom Sagemaker instance on a NLP task and trying to run a batch transform on the following json file\n{\"id\":123, \"features\":\"This is a test message\"}'\nand im looking to output the following:\n{\"id\":123,\"SageMakerOutput\":spam}<\/p>\n\n<p>Here's my batch transform code:<\/p>\n\n<pre><code>transformer = sklearn.transformer(instance_count=1,\n                                  instance_type='local',\n                                  accept='application\/json',\n                                  output_path=\"s3:\/\/spam-detection-messages-output\/json_examples\")\n\ntransformer.transform(\"s3:\/\/spam-detection-messages\/json_examples\", content_type='application\/json', input_filter=\"$.features\", join_source=\"Input\", output_filter=\"$['features', SageMakerOutput']\")\nprint('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\ntransformer.wait()\n<\/code><\/pre>\n\n<p>According to this document,\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform-data-processing.html#batch-transform-data-processing-examples\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform-data-processing.html#batch-transform-data-processing-examples<\/a>\ni should be able to grab the \"features\" object using input_filter,\nhowever, it grabs the entire json payload. and only outputs the prediction<\/p>\n\n<p>I'm also including my training code<\/p>\n\n<pre><code>import argparse\nimport pandas as pd\nimport os\nimport glob\nimport io\nimport json\n\nfrom sklearn import tree\nfrom sklearn.externals import joblib\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport numpy as np\n\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\n\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\nvectorizer = TfidfVectorizer()\n\ndef remove_stop_words(words):\n    result = [i for i in words if i not in ENGLISH_STOP_WORDS]\n    return result\n\ndef word_stemmer(words):\n    return [stemmer.stem(o) for o in words]\n\ndef word_lemmatizer(words):\n    return [lemmatizer.lemmatize(o) for o in words]\n\ndef remove_characters(words):\n    return [word for word in words if len(word)&gt; 1]\n\ndef clean_token_pipeline(words):\n    cleaning_utils = [remove_stop_words, word_lemmatizer]\n    for o in cleaning_utils:\n        words = o(words)\n    return words\n\ndef process_text(X_train, X_test, y_train, y_test):\n    X_train = [word_tokenize(o) for o in X_train]\n    X_test = [word_tokenize(o) for o in X_test]\n\n    X_train = [clean_token_pipeline(o) for o in X_train]\n    X_test = [clean_token_pipeline(o) for o in X_test]\n\n    X_train = [\" \".join(o) for o in X_train]\n    X_test = [\" \".join(o) for o in X_test]\n\n    return X_train, X_test, y_train, y_test\n\ndef convert_to_feature(raw_tokenize_data):\n    raw_sentences = [' '.join(o) for o in raw_tokenize_data]\n    return vectorizer.transform(raw_sentences)\n\ndef _npy_loads(data):\n    \"\"\"\n    Deserializes npy-formatted bytes into a numpy array\n    \"\"\"\n    stream = io.BytesIO(data)\n    return np.load(stream)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n\n    # Sagemaker specific arguments. Defaults are set in the environment variables.\n    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n\n    args = parser.parse_args()\n\n    train_data = pd.read_csv(args.train+\"\/spamAssassin_min.csv\", index_col=0)\n    train_data.dropna(inplace=True)\n    print(train_data.head())\n\n    X_train, X_test, y_train, y_test = train_test_split(train_data['message'], train_data['label'], test_size = 0.2, random_state = 1)\n    X_train, X_test, y_train, y_test = process_text(X_train, X_test, y_train, y_test)\n\n    X_train = [o.split(\" \") for o in X_train]\n    X_test = [o.split(\" \") for o in X_test]\n\n    vectorizer = TfidfVectorizer()\n    raw_sentences = [' '.join(o) for o in X_train]\n    vectorizer.fit(raw_sentences)\n\n#     print(\"saving transformer to {}\".format(args.model_dir))\n    joblib.dump(vectorizer, os.path.join(args.model_dir, \"vectorizer.joblib\"))\n\n    x_train_features = convert_to_feature(X_train)\n    x_test_features = convert_to_feature(X_test)\n\n    clf = GaussianNB()\n    clf.fit(x_train_features.toarray(),y_train)\n\n    y_true, y_pred = y_test, clf.predict(x_test_features.toarray())\n    print(classification_report(y_true, y_pred))\n\n    joblib.dump(clf, os.path.join(args.model_dir, \"model.joblib\"))\n\ndef model_fn(model_dir):\n    \"\"\"Deserialized and return fitted model\n\n    Note that this should have the same name as the serialized model in the main method\n    \"\"\"\n    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n#     print(\"model loaded {}\".format(clf))\n    return clf\n\ndef input_fn(request_body, request_content_type):\n    print(\"** input_fn**\")\n    print(\"request_body:{} request_content_type:{}\".format(request_body, request_content_type))\n    if request_content_type == \"text\/plain\":\n        #convert to string\n        message = str(request_body)\n        return message\n    elif request_content_type == \"application\/json\":\n        request_body_json = json.loads(request_body)\n#         print(\"json {}\".format(request_body_json))\n        return request_body_json['features']\n    elif request_content_type == \"application\/x-npy\":\n        return \" \".join(_npy_loads(request_body))\n    else:\n        # Handle other content-types here or raise an Exception\n        # if the content type is not supported.\n        return request_body\n\ndef predict_fn(input_data, model):\n\n    print(\"** predict_fn**\")\n    print(\"input_data: {} model:{}\".format(input_data, model))\n    print(\"\\n\")\n\n    prefix = '\/opt\/ml\/'\n    model_path = os.path.join(prefix, 'model')\n    my_vect = joblib.load(os.path.join(model_path, \"vectorizer.joblib\"))\n\n    message = \"\".join(clean_token_pipeline(input_data))\n    print(\"processed message: {}\".format(message))\n    message = my_vect.transform([message])\n    message = message.toarray()\n    prediction = model.predict(message)\n    return prediction\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1582745160383,
        "Question_score":1,
        "Question_tags":"json|amazon-web-services|amazon-sagemaker",
        "Question_view_count":1955,
        "Owner_creation_time":1285379271580,
        "Owner_last_access_time":1649074179223,
        "Owner_location":null,
        "Owner_reputation":625,
        "Owner_up_votes":174,
        "Owner_down_votes":11,
        "Owner_views":110,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60421123",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":47838705,
        "Question_title":"How do I invoke a Amazon SageMaker endpoint with the Python SDK",
        "Question_body":"<p>I'm trying to use this very simple command:<\/p>\n\n<p><code>\nimport boto3\nclient = boto3.client('sagemaker-runtime')\n<\/code><\/p>\n\n<p>listed in the <a href=\"https:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/sagemaker-runtime.html#SageMakerRuntime.Client.invoke_endpoint\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>\n\n<p>but i'm getting this error:<\/p>\n\n<p><code>UnknownServiceError: Unknown service: 'sagemaker-runtime'. Valid service names are: acm, etc..<\/code><\/p>\n\n<p>My goal is to be able to invoke the endpoint that I've created in Amazon SageMaker.<\/p>\n\n<p>I'm doing this from a Jupyter notebook in Sagemaker, so I feel like this should work no problem. How do I get it to run here, and outside of the Sagemaker environment?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1513365839550,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":3238,
        "Owner_creation_time":1443201378360,
        "Owner_last_access_time":1653587950970,
        "Owner_location":null,
        "Owner_reputation":749,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Amazon SageMaker is a very new service (December 2017).<\/p>\n\n<p>You will need to update your boto library to use it:<\/p>\n\n<pre><code>sudo pip install boto --upgrade\nsudo pip install boto3 --upgrade\nsudo pip install awscli --upgrade\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1513376500687,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47838705",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69523257,
        "Question_title":"Error in deploying PyTorch model using SageMaker Pipeline and RegisterModel",
        "Question_body":"<p>Can anyone provide an example for deploying a pytorch model using <strong>SageMaker Pipeline<\/strong>?<\/p>\n<p>I've used the MLOps template (MLOps template for model building, traing and deployment) of SageMaker Studio to build a MLOps project.<\/p>\n<p>The template is using sagemaker pipelines to build a pipeline for preprocessing and training and registering the model.\nAnd deployment script is implemented in the YAML file and employing CloudFormation to run. The deployment script will be triggered automatically when the model is registered.<\/p>\n<p>The template is using xgboost model to train the data and deploy the model. I want to use Pytorch and deploy it.\nI successfully replaced the pytorch with xgboost and successfully preprocessed the data, trained the model and registered the model. But I didn't use inference.py in my model. So I get error for the model deployment.<\/p>\n<p><strong>The error log in updating the endpoint is:<\/strong><\/p>\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/model\/code\/inference.py'\n<\/code><\/pre>\n<p>I tried to find example of using inference.py for pytorch model, but I couldn't find any example which uses <strong>sagemaker pipelines<\/strong> and <strong>RegisterModel<\/strong>.<\/p>\n<p>Any help would be appreciated.<\/p>\n<p>Below you can see a part of the pipeline for training and registering the model.<\/p>\n<pre><code>from sagemaker.pytorch.estimator import PyTorch\nfrom sagemaker.workflow.pipeline import Pipeline\nfrom sagemaker.workflow.steps import (\n    ProcessingStep,\n    TrainingStep,\n)\nfrom sagemaker.workflow.step_collections import RegisterModel\n\npytorch_estimator = PyTorch(entry_point= os.path.join(BASE_DIR, 'train.py'),\n                            instance_type= &quot;ml.m5.xlarge&quot;,\n                            instance_count=1,\n                            role=role,\n                            framework_version='1.8.0',\n                            py_version='py3',\n                            hyperparameters = {'epochs': 5, 'batch-size': 64, 'learning-rate': 0.1})\n\nstep_train = TrainingStep(\n        name=&quot;TrainModel&quot;,\n        estimator=pytorch_estimator,\n\n        inputs={\n                &quot;train&quot;: sagemaker.TrainingInput(\n                            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n                            &quot;train_data&quot;\n                            ].S3Output.S3Uri,\n                            content_type=&quot;text\/csv&quot;,\n                        ),\n                &quot;dev&quot;: sagemaker.TrainingInput(\n                            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n                            &quot;dev_data&quot;\n                            ].S3Output.S3Uri,\n                            content_type=&quot;text\/csv&quot;\n                        ),\n                &quot;test&quot;: sagemaker.TrainingInput(\n                            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n                            &quot;test_data&quot;\n                            ].S3Output.S3Uri,\n                            content_type=&quot;text\/csv&quot;\n                        ),\n        },\n)\nstep_register = RegisterModel(\n            name=&quot;RegisterModel&quot;,\n            estimator=pytorch_estimator,\n            model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n            content_types=[&quot;text\/csv&quot;],\n            response_types=[&quot;text\/csv&quot;],\n            inference_instances=[&quot;ml.t2.medium&quot;, &quot;ml.m5.large&quot;],\n            transform_instances=[&quot;ml.m5.large&quot;],\n            model_package_group_name=model_package_group_name,\n            approval_status=model_approval_status,\n        )\n    \npipeline = Pipeline(\n            name=pipeline_name,\n            parameters=[\n                processing_instance_type,\n                processing_instance_count,\n                training_instance_type,\n                model_approval_status,\n                input_data,\n            ],\n            steps=[step_process, step_train, step_register],\n            sagemaker_session=sagemaker_session,\n        )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633941357330,
        "Question_score":0,
        "Question_tags":"pytorch|amazon-cloudformation|amazon-sagemaker|endpoint|mlops",
        "Question_view_count":258,
        "Owner_creation_time":1450889293150,
        "Owner_last_access_time":1663403554823,
        "Owner_location":"Finland",
        "Owner_reputation":398,
        "Owner_up_votes":21,
        "Owner_down_votes":1,
        "Owner_views":28,
        "Question_last_edit_time":1633953506383,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69523257",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71278400,
        "Question_title":"Can't start Sagemaker Notebook Instance with a CodeCommit repo",
        "Question_body":"<p>I linked a repo in CodeCommit to Sagemaker. However when I try to start an instance with that repo it fails and I get a message:<\/p>\n<pre><code>fatal: unable to access 'https:\/\/git-codecommit.us-east-1.amazonaws.com\/v1\/repos\/MyRepo\/': The requested URL returned error: 403\n<\/code><\/pre>\n<p>I think maybe it has something to do with the IAM role. Is there some policy I should add to the AmazonSageMaker-ExecutionRole. I am completely new to this so please excuse any incorrect usage of terms here.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645893074433,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-iam|amazon-sagemaker|aws-codecommit",
        "Question_view_count":264,
        "Owner_creation_time":1563317093577,
        "Owner_last_access_time":1656618520413,
        "Owner_location":null,
        "Owner_reputation":349,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":40,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71278400",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61791589,
        "Question_title":"Sagemaker: MemoryError: Unable to allocate ___for an array with shape ___ and data type float64",
        "Question_body":"<p>I am running a notebook in sagemaker and it seems like one of the arrays produced after vectorizing text is causing issues.<\/p>\n\n<p>Reading other answers it seems like it is an issue with <a href=\"https:\/\/www.kernel.org\/doc\/Documentation\/vm\/overcommit-accounting\" rel=\"noreferrer\">overcommit<\/a>. And one of the solutions proposed is to set it to always overcommit with this:<\/p>\n\n<pre><code>$ echo 1 &gt; \/proc\/sys\/vm\/overcommit_memory\n<\/code><\/pre>\n\n<p>Is there any documentation or do you have any suggestion on how to do the same thing in sagemaker?<\/p>\n\n<p>Thank you very much.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1589441004223,
        "Question_score":8,
        "Question_tags":"arrays|python-3.x|pandas|memory|amazon-sagemaker",
        "Question_view_count":2821,
        "Owner_creation_time":1517932507093,
        "Owner_last_access_time":1648741816033,
        "Owner_location":null,
        "Owner_reputation":331,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":73,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61791589",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50441181,
        "Question_title":"How to ensure software package version consistency in AWS SageMaker serverless compute?",
        "Question_body":"<p>I am learning AWS SageMaker which is supposed to be a serverless compute environment for Machine Learning. In this type of serverless compute environment, who is supposed to ensure the software package consistency and update the versions?<\/p>\n\n<p>For example, I ran the demo program that came with SageMaker, deepar_synthetic. In this second cell, it executes the following: !conda install -y s3fs<\/p>\n\n<p>However, I got the following warning message:<\/p>\n\n<p>Solving environment: done\n==> WARNING: A newer version of conda exists. &lt;==\n  current version: 4.4.10\n  latest version: 4.5.4\nPlease update conda by running\n    $ conda update -n base conda<\/p>\n\n<p>Since it is serverless compute, am I still supposed to update the software packages myself?<\/p>\n\n<p>Another example is as follows. I wrote a few simple lines to find out the package versions in Jupyter notebook:<\/p>\n\n<p>import platform<\/p>\n\n<p>import tensorflow as tf<\/p>\n\n<p>print(platform.python_version())<\/p>\n\n<p>print (tf.<strong>version<\/strong>)<\/p>\n\n<p>However, I got the following warning messages:<\/p>\n\n<p>\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/importlib\/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\nreturn f(*args, **kwds)\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/h5py\/<strong>init<\/strong>.py:36: FutureWarning: Conversion of the second argument of issubdtype from <code>float<\/code> to <code>np.floating<\/code> is deprecated. In future, it will be treated as <code>np.float64 == np.dtype(float).type<\/code>.\nfrom ._conv import register_converters as _register_converters<\/p>\n\n<p>The prints still worked and I got the results shown beolow:<\/p>\n\n<p>3.6.4\n1.4.0<\/p>\n\n<p>I am wondering what I have to do to get the package consistent so that I don't get the warning messages. Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1526869228233,
        "Question_score":0,
        "Question_tags":"amazon-web-services|serverless|amazon-sagemaker",
        "Question_view_count":926,
        "Owner_creation_time":1461112434223,
        "Owner_last_access_time":1648939073663,
        "Owner_location":"San Jose, CA, United States",
        "Owner_reputation":1075,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":181,
        "Question_last_edit_time":1531211855607,
        "Answer_body":"<p>Today, SageMaker Notebook Instances are managed EC2 instances but users still have full control over the the Notebook Instance as root. You have full capabilities to install missing libraries through the Jupyter terminal. <\/p>\n\n<p>To access a terminal, open your Notebook Instance to the home page and click the drop-down on the top right: \u201cNew\u201d -> \u201cTerminal\u201d. \nNote: By default, conda installs to the root environment. <\/p>\n\n<p>The following are instructions you can follow <a href=\"https:\/\/conda.io\/docs\/user-guide\/tasks\/manage-environments.html\" rel=\"nofollow noreferrer\">https:\/\/conda.io\/docs\/user-guide\/tasks\/manage-environments.html<\/a> on how to install libraries in the particular conda environment. <\/p>\n\n<p>In general you will need following commands, <\/p>\n\n<pre><code>conda env list \n<\/code><\/pre>\n\n<p>which list all of your conda environments <\/p>\n\n<pre><code>source activate &lt;conda environment name&gt; \n<\/code><\/pre>\n\n<p>e.g. source activate python3 <\/p>\n\n<pre><code>conda list | grep &lt;package&gt; \n<\/code><\/pre>\n\n<p>e.g. conda list | grep numpy \nlist what are the current package versions <\/p>\n\n<pre><code>pip install numpy \n<\/code><\/pre>\n\n<p>Or <\/p>\n\n<pre><code>conda install numpy \n<\/code><\/pre>\n\n<p>Note: Periodically the SageMaker team releases new versions of libraries onto the Notebook Instances. To get the new libraries, you can stop and start your Notebook Instance. <\/p>\n\n<p>If you have recommendations on libraries you would like to see by default, you can create a forum post under <a href=\"https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285<\/a> . Alternatively, you can bootstrap your Notebook Instances with Lifecycle Configurations to install custom libraries. More details here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateNotebookInstanceLifecycleConfig.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateNotebookInstanceLifecycleConfig.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1532115126592,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50441181",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59516365,
        "Question_title":"Amazon SageMaker Ground Truth Custom Labeling Jobs Error: Cannot read property 'taskInput' of null",
        "Question_body":"<p>When creating a custom labeling job for Amazon SageMaker Ground Truth Custom \nAm getting the following error:<\/p>\n\n<p><code>Cannot read property 'taskInput' of null<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1577585232053,
        "Question_score":1,
        "Question_tags":"label|amazon-sagemaker",
        "Question_view_count":608,
        "Owner_creation_time":1377141139087,
        "Owner_last_access_time":1663825261377,
        "Owner_location":"Tokyo, Japan",
        "Owner_reputation":6358,
        "Owner_up_votes":4611,
        "Owner_down_votes":1,
        "Owner_views":229,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59516365",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59684156,
        "Question_title":"Deploy custom prebuilt model on Sagemaker",
        "Question_body":"<p>I am newbee to ML world. I have a docker image which has the required custom model files. <\/p>\n\n<p>Can some one please explain how to deploy this models as webservice?<\/p>\n\n<p>I tried creating a endpoint and lambda function but no luck.<\/p>\n\n<p>Your help will be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1578668405833,
        "Question_score":1,
        "Question_tags":"machine-learning|amazon-sagemaker",
        "Question_view_count":393,
        "Owner_creation_time":1412863488017,
        "Owner_last_access_time":1645766798810,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59684156",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68819792,
        "Question_title":"AWS Glue AccessDeniedException in SageMaker",
        "Question_body":"<p>I am running the &quot;Explain Credit Decisions&quot; solution from Sagemaker Studio. I am following the instructions in the solution notebooks. The solution has been launched with my root user id. But when running <code>1_datasets.ipynb<\/code> I am getting the below error when running the step\n<code>glue_run_id = glue.start_workflow(config.GLUE_WORKFLOW)<\/code><\/p>\n<pre><code>An error occurred (AccessDeniedException) when calling the GetJob operation: User: arn:aws:sts::myaccountid:assumed-role\/rolecreatedbystack\/GlueJobRunnerSession is not authorized to perform: glue:GetJob on resource: arn:aws:glue:us-east-1:myaccountid:job\/sagemaker-soln-ecd-js-foccb4-job\n<\/code><\/pre>\n<p>The Cloud Formation stacks are created and the scripts should create the required roles and access which are needed to run this solution.\nI have to run a POC with this solution with my custom data. So can you please help to solve the problem.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629212822180,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-iam|amazon-sagemaker",
        "Question_view_count":610,
        "Owner_creation_time":1607542203403,
        "Owner_last_access_time":1648498177160,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1631629987127,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68819792",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55868121,
        "Question_title":"Random cut forest anomaly detection on multi variant time series data",
        "Question_body":"<p>I have sensor data coming from equipment with times series along with many attributes,<\/p>\n\n<p>I have used RCF algorithm to detect anomalies.\nNow the challenge is,how to to convince the end user whether it is really anomaly or not.\nJust want to know which attribute is contributing to anomaly.<\/p>\n\n<p>Is there any best way to convince end user whether it is really anomaly or not.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1556283345330,
        "Question_score":0,
        "Question_tags":"machine-learning|amazon-sagemaker|anomaly-detection",
        "Question_view_count":984,
        "Owner_creation_time":1554856627173,
        "Owner_last_access_time":1612521912313,
        "Owner_location":null,
        "Owner_reputation":125,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Question_last_edit_time":1556364062916,
        "Answer_body":"<p>The simplest way to run the RCF model and to get the explanation for the anomaly is to use the version of RCF in Kinesis Analytics (KA). Here is a link to the documentation of how to run from the KA documentations: <a href=\"https:\/\/docs.aws.amazon.com\/kinesisanalytics\/latest\/sqlref\/sqlrf-random-cut-forest-with-explanation.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/kinesisanalytics\/latest\/sqlref\/sqlrf-random-cut-forest-with-explanation.html<\/a><\/p>\n\n<p>Kinesis is taking care both for the training of the model, the inference after the initial training and for the attribution and explanation of the variables. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/p25FR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/p25FR.png\" alt=\"https:\/\/docs.aws.amazon.com\/kinesisanalytics\/latest\/sqlref\/images\/anomaly_results.png\"><\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1556363986929,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55868121",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57462973,
        "Question_title":"Specify S3 location for model output to sagemaker training job duplication issue",
        "Question_body":"<p>Im a using a SageMaker training job to train an ML model, and I am attempting to output the model to a specific location on S3.<\/p>\n\n<p>Code:<\/p>\n\n<pre><code>model_uri = \"s3:\/\/***\/model\/\"\nscript_path = 'entry_point.py'\nsklearn = SKLearn(\n    entry_point=script_path,\n    train_instance_type=\"ml.m5.large\",\n    output_path=model_uri,\n    role='***',\n    sagemaker_session=sagemaker_session)\n<\/code><\/pre>\n\n<p>The issue I am having is that the training job will save the model <strong>twice<\/strong>. Once in the S3 bucket at the top level, and once in the folder specified (<code>\/model<\/code>).<\/p>\n\n<p>Top level:\n<a href=\"https:\/\/i.stack.imgur.com\/HbSsX.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/HbSsX.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Model folder:\n<a href=\"https:\/\/i.stack.imgur.com\/59vYA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/59vYA.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Is this expected behaviour when specifying <code>output_path<\/code> in the estimator? Is there a way to stop it?<\/p>\n\n<p>Any help would be appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1565619650897,
        "Question_score":0,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":1168,
        "Owner_creation_time":1442932369407,
        "Owner_last_access_time":1662303207090,
        "Owner_location":"Belfast",
        "Owner_reputation":1682,
        "Owner_up_votes":165,
        "Owner_down_votes":4,
        "Owner_views":164,
        "Question_last_edit_time":1565621952967,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57462973",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51305956,
        "Question_title":"S3 read Sagemaker trained model",
        "Question_body":"<p>Using Amazon Sagemaker, I created an Xgboost model. After unpacking the resulting tar.gz file, I end up with a file \"xgboost-model\". <\/p>\n\n<p>The next step will be to upload the model directly from my S3 bucket, without downloading it using <em>pickle<\/em>. Here is what I tried:<\/p>\n\n<pre><code>obj = client.get_object(Bucket='...',Key='xgboost-model')\n\nxgb_model = pkl.load(open((obj['Body'].read())),\"rb\")\n<\/code><\/pre>\n\n<p>But it throws me the error:<\/p>\n\n<pre><code>TypeError: embedded NUL character\n<\/code><\/pre>\n\n<p>Also tried this:<\/p>\n\n<pre><code>xgb_model = pkl.loads(open((obj['Body'].read())),\"rb\")\n<\/code><\/pre>\n\n<p>the outcome was the same.<\/p>\n\n<p>Another approach:<\/p>\n\n<pre><code>bucket='...'\nkey='xgboost-model'\n\nwith s3io.open('s3:\/\/{0}\/{1}'.format(bucket, key),mode='w') as s3_file:\n  pkl.dump(mdl, s3_file)\n<\/code><\/pre>\n\n<p>This giving the error:<\/p>\n\n<pre><code>CertificateError: hostname bucket doesn't match either of '*.s3.amazonaws.com', 's3.amazonaws.com'\n<\/code><\/pre>\n\n<p>This although the bucket is the same.<\/p>\n\n<p>How Can I upload the model in a pickle object so I can then use it it for predictions?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1531399214863,
        "Question_score":1,
        "Question_tags":"python-3.x|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":2742,
        "Owner_creation_time":1432680790120,
        "Owner_last_access_time":1559548299510,
        "Owner_location":null,
        "Owner_reputation":455,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":58,
        "Question_last_edit_time":1531400147427,
        "Answer_body":"<p>My assumption is you have trained the model using Sagemaker XGBoost built-in algorithm. You would like to use that model and do the predictions in your own hosting environment (not Sagemaker hosting).<\/p>\n\n<p><code>pickle.load(file)<\/code> reads a pickled object from the open file object file and <code>pickle.loads(bytes_object)<\/code> reads a pickled object from a bytes object and returns the deserialized object. Since you have the S3 object already downloaded (into memory) as bytes, you can use <code>pickle.loads<\/code> without using <code>open<\/code><\/p>\n\n<pre><code>xgb_model = pkl.loads(obj['Body'].read())\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1531429065220,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51305956",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50241771,
        "Question_title":"Getting error while infering sagemaker endpoint",
        "Question_body":"<p>I created training job in sagemaker with my own training and inference code using MXNet framework. I am able to train the model successfully and created endpoint as well. But while inferring the model, I am getting the following error:<\/p>\n\n<p><strong><em>\u2018ClientError: An error occurred (413) when calling the InvokeEndpoint operation: HTTP content length exceeded 5246976 bytes.\u2019<\/em><\/strong><\/p>\n\n<p>What I understood from my research is the error is due to the size of the image. The image shape is (480, 512, 3). I trained the model with images of same shape (480, 512, 3).<\/p>\n\n<p>When I resized the image to (240, 256), the error was gone. But producing another error 'shape inconsistent in convolution' as I the trained the model with images of size (480, 512).<\/p>\n\n<p>I didn\u2019t understand why I am getting this error while inferring.\nCan't we use images of larger size to infer the model?\nAny suggestions will be helpful<\/p>\n\n<p>Thanks, Harathi<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1525811131837,
        "Question_score":0,
        "Question_tags":"python|mxnet|amazon-sagemaker",
        "Question_view_count":1317,
        "Owner_creation_time":1520464061813,
        "Owner_last_access_time":1652212276653,
        "Owner_location":null,
        "Owner_reputation":789,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50241771",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69403349,
        "Question_title":"SageMaker endpoint can't load huggingface tokenizer",
        "Question_body":"<p>I used Amazon SageMaker to train a HuggingFace model. At the end of the training script provided to the estimator, I saved the model into the correct path (<code>SM_MODEL_DIR<\/code>):<\/p>\n<pre><code>if __name__ == &quot;__main__&quot;:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(&quot;--model-dir&quot;, type=str, default=os.environ[&quot;SM_MODEL_DIR&quot;])\n    \n    ...\n    \n    trainer.model.save_pretrained(args.model_dir)\n<\/code><\/pre>\n<p>After the model was trained, I deployed it using the <code>deploy<\/code> method of the HuggingFace estimator. Once the endpoint was successfully created, I tried inference with the returned predictor:<\/p>\n<pre><code>response = self.predictor.predict(\n    {&quot;inputs&quot;: &quot;I want to know where is my order&quot;}\n)\n<\/code><\/pre>\n<p>And I received the following client error:<\/p>\n<pre><code>{'code': 400, 'type': 'InternalServerException', 'message': &quot;Can't load tokenizer for '\/.sagemaker\/mms\/models\/model'. Make sure that:\\n\\n- '\/.sagemaker\/mms\/models\/model' is a correct model identifier listed on 'https:\/\/huggingface.co\/models'\\n\\n- or '\/.sagemaker\/mms\/models\/model' is the correct path to a directory containing relevant tokenizer files\\n\\n&quot;}\n<\/code><\/pre>\n<p>Why cannot the tokenizer be loaded?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1633079347327,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|huggingface-transformers|huggingface-tokenizers",
        "Question_view_count":176,
        "Owner_creation_time":1632991357333,
        "Owner_last_access_time":1663942581167,
        "Owner_location":"Barcelona",
        "Owner_reputation":373,
        "Owner_up_votes":36,
        "Owner_down_votes":4,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69403349",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":47913649,
        "Question_title":"What is the SageMaker url for Tensorboard?",
        "Question_body":"<p>I'm trying to access the Tensorboard for the <code>tensorflow_resnet_cifar10_with_tensorboard<\/code> example, but not sure what the url should be, the help text gives 2 options:<\/p>\n\n<blockquote>\n  <p>You can access TensorBoard locally at <a href=\"http:\/\/localhost:6006\" rel=\"noreferrer\">http:\/\/localhost:6006<\/a> or using\n  your SageMaker notebook instance proxy\/6006\/(TensorBoard will not work\n  if forget to put the slash, '\/', in end of the url). If TensorBoard\n  started on a different port, adjust these URLs to match.<\/p>\n<\/blockquote>\n\n<p>When it says access locally, does that mean the local container Sagemaker creates in AWS? If so, how do I get there?<\/p>\n\n<p>Or if I use <code>run_tensorboard_locally=False<\/code>, what should the proxy url be? <\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1513800473547,
        "Question_score":11,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":6006,
        "Owner_creation_time":1395773370030,
        "Owner_last_access_time":1664024675337,
        "Owner_location":"San Francisco Bay Area",
        "Owner_reputation":1784,
        "Owner_up_votes":38,
        "Owner_down_votes":1,
        "Owner_views":126,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47913649",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58985124,
        "Question_title":"Why does AWS SageMaker run a web server for batch transform?",
        "Question_body":"<p>I'm creating my own Docker container for use with SageMaker and I'm wondering why the serve command creates a Flask app to serve predictions on data when I want to do a batch transform job. Wouldn't it be simpler to just unpickle the model and run the model's predict method on the dataset I want predictions for? I don't need a web api\/endpoint. I just need to automatically generate predictions once a day.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1574377879723,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|docker|machine-learning|amazon-sagemaker",
        "Question_view_count":470,
        "Owner_creation_time":1446895388123,
        "Owner_last_access_time":1631632591590,
        "Owner_location":null,
        "Owner_reputation":473,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Good question :) using the exact same code for batch inference and online inference reduces development overhead - the exact same stack can be used for both use-cases - and also reduces risks of having different results between something done in Batch and something done online. That being said, SageMaker is very flexible and what you describe can easily be done using the Training API. There is nothing in the Training API forcing you to use it for ML training, it is actually a very versatile docker orchestrator with advanced logging, metadata persistance, and built for fast and distributed data ingestion.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1574462969596,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58985124",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67814409,
        "Question_title":"How to run different Jupyter notebooks conditionally in a AWS Sagemaker notebook instance",
        "Question_body":"<p>I have a AWS sagemaker notebook instance which have 2 different jupyter notebooks. There are certain conditions in which each of it should work.<\/p>\n<p>So, if the consition A exist, the Jupyter Notebook 1 should run and if Condition B exist, the Jupyter Notebook 2 should run.<\/p>\n<p>I have tried this code so far, but it doesnt work:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if condition A:\n\n    sm_client = boto3.client('sagemaker')\n    notebook_instance_name = NotebookInstanceName\n    notebook_instance_name = 'Calorie-1615128222'\n    url = sm_client.create_presigned_notebook_instance_url(NotebookInstanceName=notebook_instance_name)['AuthorizedUrl']\n    \n    print(url)\n\n    url_tokens = url.split('\/')\n    http_proto = url_tokens[0]\n    http_hn = url_tokens[2].split('?')[0].split('#')[0]\n\n    s = requests.Session()\n    r = s.get(url)\n    cookies = &quot;; &quot;.join(key + &quot;=&quot; + value for key, value in s.cookies.items())\n    print(cookies)\n\n    ws = websocket.create_connection(\n        &quot;wss:\/\/{}\/terminals\/websocket\/1&quot;.format(http_hn),\n        cookie=cookies,\n        host=http_hn,\n        origin=http_proto + &quot;\/\/&quot; + http_hn\n    )\n    \n    print(ws)\n    \n    # ws = websockets.connect(&quot;wss:\/\/{}\/terminals\/websocket\/1&quot;.format(http_hn))\n    \n    ws.send(&quot;&quot;&quot;[ &quot;stdin&quot;, &quot;jupyter nbconvert --execute --to notebook --inplace \/home\/ec2-user\/SageMaker\/Calorie\/Notebook1.ipynb \n--ExecutePreprocessor.kernel_name=conda_tensorflow2_p36 --ExecutePreprocessor.timeout=1500\\\\r&quot; ]&quot;&quot;&quot;)\n    #ws.send(&quot;&quot;&quot;[ &quot;stdin&quot;, &quot;jupyter nbconvert --execute Notebook1.ipynb --ExecutePreprocessor.kernel_name=conda_tensorflow2_p36 --ExecutePreprocessor.timeout=1500\\\\r&quot; ]&quot;&quot;&quot;)\n    \n    time.sleep(5)\n    ws.close()\n    print(&quot;websocket client created&quot;)\n    #return None\n    \n<\/code><\/pre>\n<p>Please help. Many Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622684648107,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|jupyter-notebook|amazon-sagemaker|aws-sam",
        "Question_view_count":413,
        "Owner_creation_time":1522557704627,
        "Owner_last_access_time":1661301014560,
        "Owner_location":"Australia",
        "Owner_reputation":813,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":172,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67814409",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72833918,
        "Question_title":"Eval_Metrics not recognized\/invalid in AWS XGBoost model",
        "Question_body":"<pre><code>xgb.set_hyperparameters(objective='binary:logistic',num_round=100)\nxgb.fit({'train': s3_input_train})\n\n...\n\n\nfrom sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\nhyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n                         'min_child_weight': ContinuousParameter(1, 10),\n                         'alpha': ContinuousParameter(0, 2),\n                         'max_depth': IntegerParameter(1, 10),\n                         'num_round': IntegerParameter(1, 300),\n                        'gamma': ContinuousParameter(0, 5),\n                        'lambda': ContinuousParameter(0, 1000),\n                        'max_delta_step':IntegerParameter(1, 10),\n                        'colsample_bylevel':ContinuousParameter(0.1, 1),\n                        'colsample_bytree':ContinuousParameter(0.5, 1),\n                        'subsample':ContinuousParameter(0.5, 1)}\n\n\nobjective_metric_name = 'validation:aucpr'\n\ntuner = HyperparameterTuner(xgb,\n                            objective_metric_name,\n                            hyperparameter_ranges,\n                            max_jobs=50,\n                            max_parallel_jobs=3)\n\ntuner.fit({'train': s3_input_train, 'validation': s3_input_val}, include_cls_metadata=False, wait=False)\n<\/code><\/pre>\n<p>Returns the error:<\/p>\n<pre><code>\nAn error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: The objective metric for the hyperparameter tuning job, [validation:aucpr], isn\u2019t valid for the [811284229777.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest] algorithm. Choose a valid objective metric.\n<\/code><\/pre>\n<p>The same applies when replacing aucpr with f1 and logloss. They are clearly defined as evaluation metrics in the documentation for classification purposes. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html<\/a><\/p>\n<p>What can I do to allow the f1, aucpr and logloss evaluation metrics?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656704152423,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|xgboost|amazon-sagemaker",
        "Question_view_count":77,
        "Owner_creation_time":1633542845253,
        "Owner_last_access_time":1663696816443,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72833918",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48319893,
        "Question_title":"Is there some kind of persistent local storage in aws sagemaker model training?",
        "Question_body":"<p>I did some experimentation with aws sagemaker, and the download time of large data sets from S3 is very problematic, especially when the model is still in development, and you want some kind of initial feedback relatively fast<\/p>\n\n<p>Is there some kind of local storage or other way to speed things up?<\/p>\n\n<p><strong>EDIT<\/strong>\nI refer to the batch training service, that allows you to submit a job as a docker container.<\/p>\n\n<p>While this service is intended for already validated jobs that typically run for a long time (which makes the download time less significant) there's still a need for quick feedback<\/p>\n\n<ol>\n<li><p>There's no other way to do the \"integration\" testing of your job with the sagemaker infrastructure (configuration files, data files, etc.)<\/p><\/li>\n<li><p>When experimenting with different variations to the model, it's important to be able to get initial feedback relatively fast<\/p><\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1516273838943,
        "Question_score":6,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":5129,
        "Owner_creation_time":1296032582893,
        "Owner_last_access_time":1663846720077,
        "Owner_location":"Israel",
        "Owner_reputation":7829,
        "Owner_up_votes":1436,
        "Owner_down_votes":12,
        "Owner_views":965,
        "Question_last_edit_time":1578086688792,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48319893",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67422447,
        "Question_title":"Features extraction in Real-time prediction in sagemaker",
        "Question_body":"<p>i want to deploy a real time prediction machine learning model for fraud detection using sagemaker.<\/p>\n<p>i used sagemaker jupyter instance to:<\/p>\n<pre><code>-load my training data from s3 contains transactions\n-preprocessing data and features engineering (i use category_encoders to encode the categorical value)\n-training the model and configure the endpoint\n<\/code><\/pre>\n<p>For the inference step , i used a lambda function which  invoke my endpoint to get the prediction for each real time transaction.<\/p>\n<pre><code>should i calculte again all the features for this real time transactions in lambda function ?\n\nfor the features when i use category_encoders with fit_transform() function to transform my categorical feature to numerical one, what should I do because the result will not be the same as training set?\n\nis there another method not to redo the calculation of the features in the inference step?\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1620319047827,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|lambda|amazon-sagemaker|fraud-prevention",
        "Question_view_count":85,
        "Owner_creation_time":1606221676633,
        "Owner_last_access_time":1660602413737,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67422447",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72903481,
        "Question_title":"Cant generate XGBoost training report in sagemaker, only profiler_report",
        "Question_body":"<p>I am trying to generate the XGBoost trainingreport to see feature importances however the following code only generates the profiler report.<\/p>\n<pre><code>from sagemaker import get_execution_role\nimport numpy as np\nimport pandas as pd\nfrom sagemaker.predictor import csv_serializer\nfrom sagemaker.debugger import Rule, rule_configs\n\n# Define IAM role\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\nrole = get_execution_role()\nprefix = 'sagemaker\/models'\nmy_region = boto3.session.Session().region_name \n\n# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\nxgboost_container = sagemaker.image_uris.retrieve(&quot;xgboost&quot;, my_region, &quot;latest&quot;)\n\n\n\nbucket_name = 'binary-base' \ns3 = boto3.resource('s3')\ntry:\n    if  my_region == 'us-east-1':\n      s3.create_bucket(Bucket=bucket_name)\n    else: \n      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n    print('S3 bucket created successfully')\nexcept Exception as e:\n    print('S3 error: ',e)\n\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train\/train.csv')).upload_file('..\/Data\/Base_Model_Data_No_Labels\/train.csv')\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'validation\/val.csv')).upload_file('..\/Data\/Base_Model_Data_No_Labels\/val.csv')\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test\/test.csv')).upload_file('..\/Data\/Base_Model_Data\/test.csv'\n\n\nsess = sagemaker.Session()\nxgb = sagemaker.estimator.Estimator(xgboost_container,\n                                    role, \n                                    volume_size =5,\n                                    instance_count=1, \n                                    instance_type='ml.m4.xlarge',\n                                    output_path='s3:\/\/{}\/{}\/output'.format(bucket_name, prefix, 'xgboost_model'),\n                                    sagemaker_session=sess, \n                                    rules=rules)\n\nxgb.set_hyperparameters(objective='binary:logistic',\n                        num_round=100, \n                        scale_pos_weight=8.5)\n\nxgb.fit({'train': s3_input_train, &quot;validation&quot;: s3_input_val}, wait=True)\n<\/code><\/pre>\n<p>When Checking the output path via:<\/p>\n<pre><code>rule_output_path = xgb.output_path + &quot;\/&quot; + xgb.latest_training_job.job_name + &quot;\/rule-output&quot;\n! aws s3 ls {rule_output_path} --recursive\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bMCAj.png\" rel=\"nofollow noreferrer\">We only see the profiler report generated<\/a><\/p>\n<p>What am I doing wrong\/missing? I wish to generate the XGboost Training report to see its feature importances.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1657223403220,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-s3|xgboost|amazon-sagemaker",
        "Question_view_count":45,
        "Owner_creation_time":1633542845253,
        "Owner_last_access_time":1663696816443,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1657223572763,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72903481",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71790985,
        "Question_title":"Amazon SageMaker Unsupported Media Type: image\/jpeg",
        "Question_body":"<p>I trained my own model using Tensorflow and Keras for image classification and I'm trying to deploy and use it with Amazon's SageMaker. I went through the process of converting the <code>mymodel.h5<\/code> file into a <code>mymodel.tar.gz<\/code> file and moving it to the SageMaker S3 bucket. Then, following a tutorial I created the SageMaker model using:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tensorflow.model import TensorFlowModel\nsagemaker_model = TensorFlowModel(model_data = 's3:\/\/' + sagemaker_session.default_bucket() + '\/model\/model.tar.gz',\n                                  role = role,\n                                  framework_version = '1.12',\n                                  entry_point = 'train.py')\n<\/code><\/pre>\n<p>And created the endpoint to access the model:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>predictor = sagemaker_model.deploy(initial_instance_count=1,\n                                   instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n<p>Now, since the point of creating this model was for image classification I'm trying to pass the image to my endpoint but have been getting a response of <code>Received client error (415) from primary with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: image\/jpeg&quot;}&quot;<\/code>. After reading up I feel like I may need to do some more work to have access to the <code>image\/jpeg<\/code> content type as it seems like the defaults are <code>application\/json<\/code>, <code>text\/libsvm<\/code>, and <code>text\/csv<\/code>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649384583073,
        "Question_score":1,
        "Question_tags":"python|tensorflow|machine-learning|keras|amazon-sagemaker",
        "Question_view_count":225,
        "Owner_creation_time":1615987342493,
        "Owner_last_access_time":1664084970783,
        "Owner_location":null,
        "Owner_reputation":386,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":1649407350556,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71790985",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64118186,
        "Question_title":"Getting an anomaly score for every datapoint in SageMaker?",
        "Question_body":"<p>I'm very new to SageMaker, and I've run into a bit of confusion as to how to achieve the output I am looking for. I am currently attempting to use the built-in RCF algorithm to perform anomaly detection on a list of stock volumes, like this:<\/p>\n<pre><code>apple_stock_volumes = [123412, 465125, 237564, 238172]\n<\/code><\/pre>\n<p>I have created a training job, model, and endpoint, and I'm trying now to invoke the endpoint using boto3. My current code looks like this:<\/p>\n<pre><code>apple_stock_volumes = [123412, 465125, 237564, 238172]\ndef inference():\n    client = boto3.client('sagemaker-runtime')\n    \n    body = &quot; &quot;.join(apple_stock_volumes)\n    response = client.invoke_endpoint(\n        EndpointName='apple-volume-endpoint',\n        Body=body,\n        ContentType='text\/csv'\n    )\n    inference = json.loads(response['Body'].read())\n    print(inference)\n\ninference()\n<\/code><\/pre>\n<p>What I wanted was to get an anomaly score for every datapoint, and then to alert if the anomaly score was a few standard deviations above the mean. However, what I'm actually receiving is just a single anomaly score. The following is my output:<\/p>\n<pre><code>{'scores': [{'score': 0.7164874384}]}\n<\/code><\/pre>\n<p>Can anyone explain to me what's going on here? Is this an average anomaly score? Why can't I seem to get SageMaker to output a list of anomaly scores corresponding to my data? Thanks in advance!<\/p>\n<p>Edit: I have already trained the model on a csv of historical volume data for the last year, and I have created an endpoint to hit.<\/p>\n<p>Edit 2: I've accepted @maafk's answer, although the actual answer to my question was provided in one of his comments. The piece I was missing was that each data point must be on a new line in your csv input to the endpoint. Once I substituted <code>body = &quot; &quot;.join(apple_stock_volumes)<\/code> for <code>body = &quot;\\n&quot;.join(apple_stock_volumes)<\/code>, everything worked as expected.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1601377003873,
        "Question_score":0,
        "Question_tags":"amazon-web-services|random-forest|amazon-sagemaker",
        "Question_view_count":54,
        "Owner_creation_time":1589340693533,
        "Owner_last_access_time":1658865421630,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1601394242040,
        "Answer_body":"<p>In your case, you'll want to get the standard deviation from getting the scores from historical stock volumes, and figuring out what your anomaly score is by calculating <code>3 * standard deviation<\/code><\/p>\n<p>Update your code to do inference on <em>multiple<\/em> records at once<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>apple_stock_volumes = [123412, 465125, 237564, 238172]\ndef inference():\n    client = boto3.client('sagemaker-runtime')\n    \n    body = &quot;\\n&quot;.join(apple_stock_volumes). # New line for each record\n    response = client.invoke_endpoint(\n        EndpointName='apple-volume-endpoint',\n        Body=body,\n        ContentType='text\/csv'\n    )\n    inference = json.loads(response['Body'].read())\n    print(inference)\n\ninference()\n<\/code><\/pre>\n<p>This will return a list of scores<\/p>\n<p>Assuming <code>apple_stock_volumes_df<\/code> has your volumes and the scores (after running inference on each record):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>score_mean = apple_stock_volumes_df['score'].mean()\nscore_std = apple_stock_volumes_df['score'].std()\nscore_cutoff = score_mean + 3*score_std\n<\/code><\/pre>\n<p>There is a great example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"nofollow noreferrer\">here<\/a> showing this<\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_time":1601377832670,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1601398729940,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64118186",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67782131,
        "Question_title":"sagemaker xgboost output to be JSON",
        "Question_body":"<p>I am new to AWS sagemaker and trying to do a simple test, where I am trying to call the xgboost model.<\/p>\n<pre><code>xgboost_container = sagemaker.image_uris.retrieve(&quot;xgboost&quot;, 'us-east-1', &quot;1.2-1&quot;)\n<\/code><\/pre>\n<p>Creating the model and endpoint:<\/p>\n<pre><code>from sagemaker.serializers import JSONSerializer\nendp_name =&quot;myendpoint&quot;\nacc_model = sm_model.deploy(initial_instance_count=1, \n                instance_type='ml.m5.4xlarge',\n                endpoint_name=endp_name, \n                serializer=JSONSerializer(),\n                deserializer= sagemaker.deserializers.JSONDeserializer()\n                            \n               )\n<\/code><\/pre>\n<p>Creating the predictor instance:<\/p>\n<pre><code>from sagemaker.predictor import Predictor\nfrom sagemaker.serializers import CSVSerializer, JSONSerializer \n\nsess = sagemaker.Session()\n\npayload ={\n&quot;var1&quot;:1,\n&quot;var2&quot;:2,\n&quot;var3&quot;:3,\n&quot;var4&quot;:0,\n&quot;var5&quot;:4,\n&quot;var6&quot;:0,\n&quot;var7&quot;:5,\n&quot;var8&quot;:45,\n\n}\n\npredictor = Predictor(\n    endpoint_name=endp_name, sagemaker_session=sess, serializer=JSONSerializer(),\ndeserializer=JSONSerializer()  )\n<\/code><\/pre>\n<p>and then predicting:<\/p>\n<pre><code>predictor.predict(payload)\n<\/code><\/pre>\n<p>I want the output of the predictor.predict to be JSON format, however when I run this I get this.\n<a href=\"https:\/\/i.stack.imgur.com\/gnlxl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gnlxl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What need to be done so that I can see the output as JSON?<\/p>\n<p>PS: If I remove the deserializer I get the output as byte:\n<a href=\"https:\/\/i.stack.imgur.com\/0ncvK.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0ncvK.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>and if I change it to CSVDeserializer I get it out as:\n<a href=\"https:\/\/i.stack.imgur.com\/bv7L6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bv7L6.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622515828260,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":261,
        "Owner_creation_time":1469929726907,
        "Owner_last_access_time":1649618839153,
        "Owner_location":null,
        "Owner_reputation":101,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":1622516341412,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67782131",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66554893,
        "Question_title":"AWS Sagemaker Workflow pIpeline use the code stored in artifact created from Codebuild",
        "Question_body":"<p>I have created a <code>sagemaker.workflow.pipeline.Pipeline<\/code> object, in which, there are couple of processing step where I am trying to reference to an s3 file path rather than a local file path, so that it won't upload files to s3 everytime the pipeline runs.<\/p>\n<p>My question is, can I modify the <code>step<\/code> or <code>scriptprocessor<\/code> or <code>pipeline<\/code> object so that I can reference a code from artifact created from AWS Codebuild?<\/p>\n<p>If not, can I use codebuild to first copy my local file to a specific S3 position (I am having permission issue so far) and then run the pipeline?<\/p>\n<p>As your reference<\/p>\n<pre><code>...\nstep_data_ingest = ProcessingStep(\n        name=&quot;DataIngestion&quot;,\n        processor=sklearn_data_ingest_processor,\n        inputs=[\n            ProcessingInput(\n                input_name=&quot;input_train_data&quot;,\n                source=input_data, \n                destination=&quot;\/opt\/ml\/processing\/input\/data\/train&quot;\n            ),\n            ProcessingInput(\n                input_name=&quot;input_test_data&quot;,\n                source=test_data, \n                destination=&quot;\/opt\/ml\/processing\/input\/data\/test&quot;\n            ),\n            ProcessingInput(\n                input_name=&quot;requirement_file&quot;,\n                source=os.path.join(code_dir, &quot;requirements.txt&quot;), \n                destination=&quot;\/opt\/ml\/processing\/input\/requirement&quot;\n            ),\n        ],\n        outputs=[\n            ProcessingOutput(\n                output_name=&quot;train&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/train&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/train&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;validation&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/validation&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/validation&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;test&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/test&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/test&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;sample&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/sample&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/sample&quot;)\n            ),\n        ],\n        code=os.path.join(code_dir, &quot;data_ingestion.py&quot;),\n        # something like s3:\/\/some_code_dir\/data_ingestion.py\n        job_arguments = [&quot;-c&quot;, country, \n                         &quot;-v&quot;, train_val_split_percentage],\n    )\n...\n<\/code><\/pre>\n<p>What I expect to do is something like:<\/p>\n<pre><code># in processing step or processor\nProcessingStep(\n    ...\n    code=&quot;data_ingestion.py&quot;\n    code_location=&quot;s3:\/\/some_artifact_bucket\/buildartifact\/fdskz.zip&quot;\n\n    ...\n)\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code># in processing step or processor\nProcessingStep(\n    ...\n    code=&quot;s3:\/\/some_artifact_bucket\/buildartifact\/fdsix\/data_ingestion.py&quot;\n    ...\n)\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code># in buildspec.yml for codebuild\naws s3 sync .\/code_dir\/ s3:\/\/some_code_dir\/\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615324569760,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker|aws-codebuild",
        "Question_view_count":156,
        "Owner_creation_time":1575485255683,
        "Owner_last_access_time":1634441626670,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When using the <code>ProcessingStep<\/code>, you can use an <code>S3 URI<\/code> as the code location, take a look on <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/9fc57555bba4fc1d33064478dc209a84a6726c57\/src\/sagemaker\/workflow\/steps.py#L374\" rel=\"nofollow noreferrer\">this<\/a> for reference.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1627578025127,
        "Answer_score":0.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1628026967547,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66554893",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69779916,
        "Question_title":"How to automate jupyter notebook execution on aws?",
        "Question_body":"<p>I got a task to complete where I need to automate Jupyter notebook execution on AWS. I'm totally new to AWS environment so don't have any idea how to do it efficiently. Things I need to do are the following -<\/p>\n<ol>\n<li>Need REST API(s) to start and stop Jupyter notebook execution on AWS.<\/li>\n<li>Need to send parameters to the notebook while calling using API.<\/li>\n<\/ol>\n<p>What are the AWS components I need, to perform the above task?<\/p>\n<pre><code>import boto3,time\n \nemr = boto3.client(\n    'emr',\n    region_name='us-west-1'\n)\n \n \n \nstart_resp = emr.start_notebook_execution(\n    EditorId='e-40AC8ZO6EGGCPJ4DLO48KGGGI',\n    RelativePath='boto3_demo.ipynb',\n    ExecutionEngine={'Id':'j-1HYZS6JQKV11Q'},\n    ServiceRole='EMR_Notebooks_DefaultRole'\n)\n \nexecution_id = start_resp[&quot;NotebookExecutionId&quot;]\nprint(execution_id)\nprint(&quot;\\n&quot;)\n \n \ndescribe_response = emr.describe_notebook_execution(NotebookExecutionId=execution_id)\n \nprint(describe_response)\nprint(&quot;\\n&quot;)\n \n \n \nlist_response = emr.list_notebook_executions()\nprint(&quot;Existing notebook executions:\\n&quot;)\nfor execution in list_response['NotebookExecutions']:\n    print(execution)\n    print(&quot;\\n&quot;)\n \n \n \nprint(&quot;Sleeping for 5 sec...&quot;)\ntime.sleep(5)\n \nprint(&quot;Stop execution &quot; + execution_id)\nemr.stop_notebook_execution(NotebookExecutionId=execution_id)\ndescribe_response = emr.describe_notebook_execution(NotebookExecutionId=execution_id)\nprint(describe_response)\nprint(&quot;\\n&quot;)    \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1635599367780,
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-lambda|jupyter-notebook|amazon-sagemaker|aws-emr-studio",
        "Question_view_count":274,
        "Owner_creation_time":1452602206603,
        "Owner_last_access_time":1663948894330,
        "Owner_location":"Kolkata, West Bengal, India",
        "Owner_reputation":55,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1635623450200,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69779916",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70065420,
        "Question_title":"glueContext can't find files em s3, but SparkSession can. How to solve it",
        "Question_body":"<p>I'm working with LakeFormation and Glue Jobs to process some files.<\/p>\n<p>I've already configured the lake formation. I ran a crawler that properly identifed the two tables and theie respective schemas. The folder in s3 bucket is structured as follow:<\/p>\n<pre><code>| receitafederal-udct-zen\n\n  |----empresas\/\n\n  |----estabelecimentos\/\n<\/code><\/pre>\n<p>I named the database as 'rf-raw' in Glue, and the crawlers identified the following tables:<\/p>\n<ul>\n<li>estabelecimentos<\/li>\n<li>empresas<\/li>\n<\/ul>\n<p>But, the tables doesn't have headers and the data types were note correctly identified. So, I want to run a Glue Job to properly name the columns and set the data types, and other simples transformations (some replacements). Then, I want to storage the transformed files in another s3 bucket. But first, I'm testing some PySpark in the SageMaker notebook (using dev endpoint). But, when I run the following script:<\/p>\n<pre><code>glueContext = GlueContext(SparkContext.getOrCreate())\nempresasDF = glueContext.create_dynamic_frame.from_catalog(database=&quot;raw-rf&quot;, table_name=&quot;empresas&quot;)\nempresasDF.printSchema()\n<\/code><\/pre>\n<p>I'm getting the following error:<\/p>\n<p><code>Caused by: java.io.FileNotFoundException: No such file or directory 'glue-d-raw-rf-t-  empresas-m-r:\/\/receitafederal-udct-zen\/empresas\/K3241.K03200Y1.D11009.EMPRECSV'<\/code><\/p>\n<p>I've already try to set permissions to s3 bucket for the SageMakerNotebook role, that creates the notebook, but didn't work. I don't know, but the file\/directory presented in the error message is kind of weird, but i don't know if it is a glue pattern.<\/p>\n<p>Futher, I ran the following code, in the same SageMaker notebook:<\/p>\n<p><code>sp = SparkSession.builder.getOrCreate()<\/code><\/p>\n<p>`<\/p>\n<pre><code>estabelecimentos_df = sp.read.option(&quot;delimiter&quot;,';')\n.option(&quot;emptyValue&quot;, '&quot;&quot;')\n.option(&quot;dateFormat&quot;,'yyyyMMdd')\n.option(&quot;encoding&quot;,'iso-8859-1')\n.option('header', 'false')\n.csv(path)\n<\/code><\/pre>\n<p>`<\/p>\n<p>in <code>csv()<\/code> the path I'm setting the s3 file path as <code>s3:\/\/bucket\/key<\/code>. In this case, when I run <code>printSchema()<\/code> no error is returned, and I can acess the files.\n`<\/p>\n<p>Why GlueContext are not able to get the files? But in the same notebook I can read using <code>SparkSession<\/code>?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1637582459387,
        "Question_score":0,
        "Question_tags":"pyspark|aws-glue|amazon-sagemaker|aws-lake-formation",
        "Question_view_count":261,
        "Owner_creation_time":1535130850620,
        "Owner_last_access_time":1663779434357,
        "Owner_location":"Florian\u00f3polis, SC, Brasil",
        "Owner_reputation":599,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70065420",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72058686,
        "Question_title":"How do I deploy a pre trained sklearn model on AWS sagemaker? (Endpoint stuck on creating)",
        "Question_body":"<p>To start with, I understand that this question has been asked multiple times but I haven't found the solution to my problem.<\/p>\n<p>So, to start with I have used joblib.dump to save a locally trained sklearn RandomForest. I then uploaded this to s3, made a folder called code and put in an inference script there, called inference.py.<\/p>\n<pre><code>import joblib\nimport json\nimport numpy\nimport scipy\nimport sklearn\nimport os\n\n&quot;&quot;&quot;\nDeserialize fitted model\n&quot;&quot;&quot;\ndef model_fn(model_dir):\n    model_path = os.path.join(model_dir, 'test_custom_model')\n    model = joblib.load(model_path)\n    return model\n\n&quot;&quot;&quot;\ninput_fn\n    request_body: The body of the request sent to the model.\n    request_content_type: (string) specifies the format\/variable type of the request\n&quot;&quot;&quot;\ndef input_fn(request_body, request_content_type):\n    if request_content_type == 'application\/json':\n        request_body = json.loads(request_body)\n        inpVar = request_body['Input']\n        return inpVar\n    else:\n        raise ValueError(&quot;This model only supports application\/json input&quot;)\n\n&quot;&quot;&quot;\npredict_fn\n    input_data: returned array from input_fn above\n    model (sklearn model) returned model loaded from model_fn above\n&quot;&quot;&quot;\ndef predict_fn(input_data, model):\n    return model.predict(input_data)\n\n&quot;&quot;&quot;\noutput_fn\n    prediction: the returned value from predict_fn above\n    content_type: the content type the endpoint expects to be returned. Ex: JSON, string\n&quot;&quot;&quot;\n\ndef output_fn(prediction, content_type):\n    res = int(prediction[0])\n    respJSON = {'Output': res}\n    return respJSON\n<\/code><\/pre>\n<p>Very simple so far.<\/p>\n<p>I also put this into the local jupyter sagemaker session<\/p>\n<p>all_files (folder)\ncode (folder)\ninference.py (python file)\ntest_custom_model (joblib dump of model)<\/p>\n<p>The script turns this folder all_files into a tar.gz file<\/p>\n<p>Then comes the main script that I ran on sagemaker:<\/p>\n<pre><code>import boto3\nimport json\nimport os\nimport joblib\nimport pickle\nimport tarfile\nimport sagemaker\nimport time\nfrom time import gmtime, strftime\nimport subprocess\nfrom sagemaker import get_execution_role\n\n#Setup\nclient = boto3.client(service_name=&quot;sagemaker&quot;)\nruntime = boto3.client(service_name=&quot;sagemaker-runtime&quot;)\nboto_session = boto3.session.Session()\ns3 = boto_session.resource('s3')\nregion = boto_session.region_name\nprint(region)\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\n#Bucket for model artifacts\ndefault_bucket = 'pretrained-model-deploy'\nmodel_artifacts = f&quot;s3:\/\/{default_bucket}\/test_custom_model.tar.gz&quot;\n\n#Build tar file with model data + inference code\nbashCommand = &quot;tar -cvpzf test_custom_model.tar.gz all_files&quot;\nprocess = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\noutput, error = process.communicate()\n\n#Upload tar.gz to bucket\nresponse = s3.meta.client.upload_file('test_custom_model.tar.gz', default_bucket, 'test_custom_model.tar.gz')\n\n# retrieve sklearn image\nimage_uri = sagemaker.image_uris.retrieve(\n    framework=&quot;sklearn&quot;,\n    region=region,\n    version=&quot;0.23-1&quot;,\n    py_version=&quot;py3&quot;,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n)\n\n#Step 1: Model Creation\nmodel_name = &quot;sklearn-test&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nprint(&quot;Model name: &quot; + model_name)\ncreate_model_response = client.create_model(\n    ModelName=model_name,\n    Containers=[\n        {\n            &quot;Image&quot;: image_uri,\n            &quot;ModelDataUrl&quot;: model_artifacts,\n        }\n    ],\n    ExecutionRoleArn=role,\n)\nprint(&quot;Model Arn: &quot; + create_model_response[&quot;ModelArn&quot;])\n\n#Step 2: EPC Creation - Serverless\nsklearn_epc_name = &quot;sklearn-epc&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nresponse = client.create_endpoint_config(\n   EndpointConfigName=sklearn_epc_name,\n   ProductionVariants=[\n        {\n            &quot;ModelName&quot;: model_name,\n            &quot;VariantName&quot;: &quot;sklearnvariant&quot;,\n            &quot;ServerlessConfig&quot;: {\n                &quot;MemorySizeInMB&quot;: 2048,\n                &quot;MaxConcurrency&quot;: 20\n            }\n        } \n    ]\n)\n\n# #Step 2: EPC Creation - Synchronous\n# sklearn_epc_name = &quot;sklearn-epc&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\n# endpoint_config_response = client.create_endpoint_config(\n#     EndpointConfigName=sklearn_epc_name,\n#     ProductionVariants=[\n#         {\n#             &quot;VariantName&quot;: &quot;sklearnvariant&quot;,\n#             &quot;ModelName&quot;: model_name,\n#             &quot;InstanceType&quot;: &quot;ml.m5.xlarge&quot;,\n#             &quot;InitialInstanceCount&quot;: 1\n#         },\n#     ],\n# )\n# print(&quot;Endpoint Configuration Arn: &quot; + endpoint_config_response[&quot;EndpointConfigArn&quot;])\n\n#Step 3: EP Creation\nendpoint_name = &quot;sklearn-local-ep&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\ncreate_endpoint_response = client.create_endpoint(\n    EndpointName=endpoint_name,\n    EndpointConfigName=sklearn_epc_name,\n)\nprint(&quot;Endpoint Arn: &quot; + create_endpoint_response[&quot;EndpointArn&quot;])\n\n\n#Monitor creation\ndescribe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\nwhile describe_endpoint_response[&quot;EndpointStatus&quot;] == &quot;Creating&quot;:\n    describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n    print(describe_endpoint_response)\n    time.sleep(15)\nprint(describe_endpoint_response)\n<\/code><\/pre>\n<p>Now, I mainly just want the serverless deployment but that fails after a while with this error message:<\/p>\n<pre><code>{'EndpointName': 'sklearn-local-ep2022-04-29-12-16-10', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-12-16-10', 'EndpointConfigName': 'sklearn-epc2022-04-29-12-16-03', 'EndpointStatus': 'Creating', 'CreationTime': datetime.datetime(2022, 4, 29, 12, 16, 10, 290000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 12, 16, 11, 52000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '1d25120e-ddb1-474d-9c5f-025c6be24383', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1d25120e-ddb1-474d-9c5f-025c6be24383', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '305', 'date': 'Fri, 29 Apr 2022 12:21:59 GMT'}, 'RetryAttempts': 0}}\n{'EndpointName': 'sklearn-local-ep2022-04-29-12-16-10', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-12-16-10', 'EndpointConfigName': 'sklearn-epc2022-04-29-12-16-03', 'EndpointStatus': 'Failed', 'FailureReason': 'Unable to successfully stand up your model within the allotted 180 second timeout. Please ensure that downloading your model artifacts, starting your model container and passing the ping health checks can be completed within 180 seconds.', 'CreationTime': datetime.datetime(2022, 4, 29, 12, 16, 10, 290000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 12, 22, 2, 68000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '59fb8ddd-9d45-41f5-9383-236a2baffb73', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '59fb8ddd-9d45-41f5-9383-236a2baffb73', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '559', 'date': 'Fri, 29 Apr 2022 12:22:15 GMT'}, 'RetryAttempts': 0}}\n<\/code><\/pre>\n<p>The real time deployment is just permanently stuck at creating.<\/p>\n<p>Cloudwatch has the following errors:\nError handling request \/ping<\/p>\n<p>AttributeError: 'NoneType' object has no attribute 'startswith'<\/p>\n<p>with traceback:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base_async.py&quot;, line 55, in handle\n    self.handle_request(listener_name, req, client, addr)\n<\/code><\/pre>\n<p>Copy paste has stopped working so I have attached an image of it instead.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hw80j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hw80j.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>This is the error message I get:\nEndpoint Arn: arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-13-18-09\n{'EndpointName': 'sklearn-local-ep2022-04-29-13-18-09', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-13-18-09', 'EndpointConfigName': 'sklearn-epc2022-04-29-13-18-07', 'EndpointStatus': 'Creating', 'CreationTime': datetime.datetime(2022, 4, 29, 13, 18, 9, 548000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 13, 18, 13, 119000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'ef0e49ee-618e-45de-9c49-d796206404a4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'ef0e49ee-618e-45de-9c49-d796206404a4', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '306', 'date': 'Fri, 29 Apr 2022 13:18:24 GMT'}, 'RetryAttempts': 0}}<\/p>\n<p>These are the permissions I have associated with that role:<\/p>\n<pre><code>AmazonSageMaker-ExecutionPolicy\nSecretsManagerReadWrite\nAmazonS3FullAccess\nAmazonSageMakerFullAccess\nEC2InstanceProfileForImageBuilderECRContainerBuilds\nAWSAppRunnerServicePolicyForECRAccess\n<\/code><\/pre>\n<p>What am I doing wrong? I've tried different folder structures for the zip file, different accounts, all to no avail. I don't really want to use the model.deploy() method as I don't know how to use serverless with that, and it's also inconcistent between different model types (I'm trying to make a flexible deployment pipeline where different (xgb \/ sklearn) models can be deployed with minimal changes.<\/p>\n<p>Please send help, I'm very close to smashing my hair and tearing out my laptop, been struggling with this for a whole 4 days now.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1651238636737,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":397,
        "Owner_creation_time":1558310526777,
        "Owner_last_access_time":1661420089220,
        "Owner_location":"London, UK",
        "Owner_reputation":21,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1651255570927,
        "Answer_body":"<p>I've solved this problem - I used sagemaker.model.model to load in the model data I already had and I called the deploy method on the aforementioned model object to deploy it. Further, I had the inference script and the model file in the same place as the notebook and directly called them, as this gave me an error earlier as well.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1651509693110,
        "Answer_score":0.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72058686",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73728499,
        "Question_title":"How to update an existing model in AWS sagemaker >= 2.0",
        "Question_body":"<p>I have an XGBoost model currently in production using AWS sagemaker and making real time inferences. After a while, I would like to update the model with a newer one trained on more data and keep everything as is (e.g. same endpoint, same inference procedure, so really no changes aside from the model itself)<\/p>\n<p>The current deployment procedure is the following :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.xgboost.model import XGBoostModel\nfrom sagemaker.xgboost.model import XGBoostPredictor\n\nxgboost_model = XGBoostModel(\n    model_data = &lt;S3 url&gt;,\n    role = &lt;sagemaker role&gt;,\n    entry_point = 'inference.py',\n    source_dir = 'src',\n    code_location = &lt;S3 url of other dependencies&gt;\n    framework_version='1.5-1',\n    name = model_name)\n\nxgboost_model.deploy(\n    instance_type='ml.c5.large',\n    initial_instance_count=1,\n    endpoint_name = model_name)\n<\/code><\/pre>\n<p>Now that I updated the model a few weeks later, I would like to re-deploy it. I am aware that the <code>.deploy()<\/code> method creates an endpoint and an endpoint configuration so it does it all. I cannot simply re-run my script again since I would encounter an error.<\/p>\n<p>In previous versions of sagemaker I could have updated the model with an extra argument passed to the <code>.deploy()<\/code> method called <code>update_endpoint = True<\/code>. In sagemaker &gt;=2.0 this is a no-op. Now, in sagemaker &gt;= 2.0, I need to use the predictor object as stated in the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html\" rel=\"nofollow noreferrer\">documentation<\/a>. So I try the following :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>predictor = XGBoostPredictor(model_name)\npredictor.update_endpoint(model_name= model_name)\n<\/code><\/pre>\n<p>Which actually updates the endpoint according to a new endpoint configuration. However, I do not know what it is updating... I do not specify in the above 2 lines of code that we need to considering the new <code>xgboost_model<\/code> trained on more data...  so where do I tell the update to take a more recent model?<\/p>\n<p>Thank you!<\/p>\n<p><strong>Update<\/strong><\/p>\n<p>I believe that I need to be looking at production variants as stated in their documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-ab-testing.html\" rel=\"nofollow noreferrer\">here<\/a>. However, their whole tutorial is based on the amazon sdk for python (boto3) which has artifacts that are hard to manage when I have difference entry points for each model variant (e.g. different <code>inference.py<\/code> scripts).<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1663233254363,
        "Question_score":0,
        "Question_tags":"python-3.x|deployment|xgboost|amazon-sagemaker",
        "Question_view_count":41,
        "Owner_creation_time":1640956373383,
        "Owner_last_access_time":1663928794863,
        "Owner_location":null,
        "Owner_reputation":309,
        "Owner_up_votes":56,
        "Owner_down_votes":1,
        "Owner_views":15,
        "Question_last_edit_time":1663319367852,
        "Answer_body":"<p>Since I found an answer to my own question I will post it here for those who encounter the same problem.<\/p>\n<p>I ended up re-coding all my deployment script using the boto3 SDK rather than the sagemaker SDK (or a mix of both as some documentation suggest).<\/p>\n<p>Here's the whole script that shows how to create a sagemaker model object, an endpoint configuration and an endpoint to deploy the model on for the first time. In addition, it shows what to do how to update the endpoint with a newer model (which was my main question)<\/p>\n<p>Here's the code to do all 3 in case you want to bring your own model and update it safely in production using sagemaker :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\nimport time\nfrom datetime import datetime\nfrom sagemaker import image_uris\nfrom fileManager import *  # this is a local script for helper functions\n\n# name of zipped model and zipped inference code\nCODE_TAR = 'your_inference_code_and_other_artifacts.tar.gz'\nMODEL_TAR = 'your_saved_xgboost_model.tar.gz'\n\n# sagemaker params\nsmClient = boto3.client('sagemaker')\nsmRole = &lt;your_sagemaker_role&gt;\nbucket = sagemaker.Session().default_bucket()\n\n# deploy algorithm\nclass Deployer:\n\n    def __init__(self, modelName, deployRetrained=False):\n        self.modelName=modelName\n        self.deployRetrained = deployRetrained\n        self.prefix = &lt;S3_model_path_prefix&gt;\n    \n    def deploy(self):\n        '''\n        Main method to create a sagemaker model, create an endpoint configuration and deploy the model. If deployRetrained\n        param is set to True, this method will update an already existing endpoint.\n        '''\n        # define model name and endpoint name to be used for model deployment\/update\n        model_name = self.modelName + &lt;any_suffix&gt;\n        endpoint_config_name = self.modelName + '-%s' %datetime.now().strftime('%Y-%m-%d-%HH%M')\n        endpoint_name = self.modelName\n        \n        # deploy model for the first time\n        if not self.deployRetrained:\n            print('Deploying for the first time')\n\n            # here you should copy and zip the model dependencies that you may have (such as preprocessors, inference code, config code...)\n            # mine were zipped into the file called CODE_TAR\n\n            # upload model and model artifacts needed for inference to S3\n            uploadFile(list_files=[MODEL_TAR, CODE_TAR], prefix = self.prefix)\n\n            # create sagemaker model and endpoint configuration\n            self.createSagemakerModel(model_name)\n            self.createEndpointConfig(endpoint_config_name, model_name)\n\n            # deploy model and wait while endpoint is being created\n            self.createEndpoint(endpoint_name, endpoint_config_name)\n            self.waitWhileCreating(endpoint_name)\n        \n        # update model\n        else:\n            print('Updating existing model')\n\n            # upload model and model artifacts needed for inference (here the old ones are replaced)\n            # make sure to make a backup in S3 if you would like to keep the older models\n            # we replace the old ones and keep the same names to avoid having to recreate a sagemaker model with a different name for the update!\n            uploadFile(list_files=[MODEL_TAR, CODE_TAR], prefix = self.prefix)\n\n            # create a new endpoint config that takes the new model\n            self.createEndpointConfig(endpoint_config_name, model_name)\n\n            # update endpoint\n            self.updateEndpoint(endpoint_name, endpoint_config_name)\n\n            # wait while endpoint updates then delete outdated endpoint config once it is InService\n            self.waitWhileCreating(endpoint_name)\n            self.deleteOutdatedEndpointConfig(model_name, endpoint_config_name)\n\n    def createSagemakerModel(self, model_name):\n        ''' \n        Create a new sagemaker Model object with an xgboost container and an entry point for inference using boto3 API\n        '''\n        # Retrieve that inference image (container)\n        docker_container = image_uris.retrieve(region=region, framework='xgboost', version='1.5-1')\n\n        # Relative S3 path to pre-trained model to create S3 model URI\n        model_s3_key = f'{self.prefix}\/'+ MODEL_TAR\n\n        # Combine bucket name, model file name, and relate S3 path to create S3 model URI\n        model_url = f's3:\/\/{bucket}\/{model_s3_key}'\n\n        # S3 path to the necessary inference code\n        code_url = f's3:\/\/{bucket}\/{self.prefix}\/{CODE_TAR}'\n        \n        # Create a sagemaker Model object with all its artifacts\n        smClient.create_model(\n            ModelName = model_name,\n            ExecutionRoleArn = smRole,\n            PrimaryContainer = {\n                'Image': docker_container,\n                'ModelDataUrl': model_url,\n                'Environment': {\n                    'SAGEMAKER_PROGRAM': 'inference.py', #inference.py is at the root of my zipped CODE_TAR\n                    'SAGEMAKER_SUBMIT_DIRECTORY': code_url,\n                }\n            }\n        )\n    \n    def createEndpointConfig(self, endpoint_config_name, model_name):\n        ''' \n        Create an endpoint configuration (only for boto3 sdk procedure) and set production variants parameters.\n        Each retraining procedure will induce a new variant name based on the endpoint configuration name.\n        '''\n        smClient.create_endpoint_config(\n            EndpointConfigName=endpoint_config_name,\n            ProductionVariants=[\n                {\n                    'VariantName': endpoint_config_name,\n                    'ModelName': model_name,\n                    'InstanceType': INSTANCE_TYPE,\n                    'InitialInstanceCount': 1\n                }\n            ]\n        )\n\n    def createEndpoint(self, endpoint_name, endpoint_config_name):\n        '''\n        Deploy the model to an endpoint\n        '''\n        smClient.create_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=endpoint_config_name)\n    \n    def deleteOutdatedEndpointConfig(self, name_check, current_endpoint_config):\n        '''\n        Automatically detect and delete endpoint configurations that contain a string 'name_check'. This method can be used\n        after a retrain procedure to delete all previous endpoint configurations but keep the current one named 'current_endpoint_config'.\n        '''\n        # get a list of all available endpoint configurations\n        all_configs = smClient.list_endpoint_configs()['EndpointConfigs']\n\n        # loop over the names of endpoint configs\n        names_list = []\n        for config_dict in all_configs:\n            endpoint_config_name = config_dict['EndpointConfigName']\n\n            # get only endpoint configs that contain name_check in them and save names to a list\n            if name_check in endpoint_config_name:\n                names_list.append(endpoint_config_name)\n        \n        # remove the current endpoint configuration from the list (we do not want to detele this one since it is live)\n        names_list.remove(current_endpoint_config)\n\n        for name in names_list:\n            try:\n                smClient.delete_endpoint_config(EndpointConfigName=name)\n                print('Deleted endpoint configuration for %s' %name)\n            except:\n                print('INFO : No endpoint configuration was found for %s' %endpoint_config_name)\n\n    def updateEndpoint(self, endpoint_name, endpoint_config_name):\n        ''' \n        Update existing endpoint with a new retrained model\n        '''\n        smClient.update_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=endpoint_config_name,\n            RetainAllVariantProperties=True)\n    \n    def waitWhileCreating(self, endpoint_name):\n        ''' \n        While the endpoint is being created or updated sleep for 60 seconds.\n        '''\n        # wait while creating or updating endpoint\n        status = smClient.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n        print('Status: %s' %status)\n        while status != 'InService' and status !='Failed':\n            time.sleep(60)\n            status = smClient.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n            print('Status: %s' %status)\n        \n        # in case of a deployment failure raise an error\n        if status == 'Failed':\n            raise ValueError('Endpoint failed to deploy')\n\nif __name__==&quot;__main__&quot;:\n    deployer = Deployer('churnmodel', deployRetrained=True)\n    deployer.deploy()\n<\/code><\/pre>\n<p>Final comments :<\/p>\n<ul>\n<li><p>The sagemaker <a href=\"https:\/\/docs.amazonaws.cn\/en_us\/sagemaker\/latest\/dg\/realtime-endpoints-deployment.html\" rel=\"nofollow noreferrer\">documentation<\/a> mentions all this but fails to state that you can provide an 'entry_point' to the <code>create_model<\/code> method as well as a 'source_dir' for inference dependencies (e.g. normalization artifacts). It can be done as seen in <code>PrimaryContainer<\/code> argument.<\/p>\n<\/li>\n<li><p>my <code>fileManager.py<\/code> script just contains basic functions to make tar files, upload and download to and from my S3 paths. To simplify the class, I have not included them in.<\/p>\n<\/li>\n<li><p>The method deleteOutdatedEndpointConfig may seem like an overkill with an unnecessary loop and checks, I do so because I have multiple endpoint configurations to handle and wanted to remove the ones that weren't live AND contain the string <code>name_check<\/code> (I do not know the exact name of the configuration since there is a datetime suffix). Feel free to simplify it or remove it all together if you feel like it.<\/p>\n<\/li>\n<\/ul>\n<p>Hope it helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1663924957329,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1663925308150,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73728499",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72705272,
        "Question_title":"AWS: ClientError: An error occurred (ValidationException) when calling the CreateTransformJob operation: Could not find model",
        "Question_body":"<p>I am new here and am strugling with some basic things.\nI am trying to implement a DeepAR model, but maybe due to an update from sagemaker it doesn't find the correct path as it did on the example..\nCan someone tell me why is it happening and how o fix it?<\/p>\n<p>Error: ClientError: An error occurred (ValidationException) when calling the CreateTransformJob operation: Could not find model &quot;arn:aws:sagemaker:eu-central-1:900386373554:model\/forecasting-deepar-2022-06-21-14-12-51-560&quot;.<\/p>\n<p>This is the Batch Transformation I am trying to implement:<\/p>\n<p>-Batch Transform\nimport boto3\n-Create the SageMaker Boto3 client\nboto3_sm = boto3.client('sagemaker')<\/p>\n<p>import time\nfrom time import gmtime, strftime<\/p>\n<p>batch_job_name = 'Batch-Transform-' + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\ninput_location = 's3:\/\/sagemaker-eu-central-1-900386373554\/deepar-rossmann\/input\/prediction_input.json'\noutput_location = 's3:\/\/{}\/{}\/output\/{}'.format(bucket, prefix, batch_job_name)<\/p>\n<p>request = <br \/>\n{\n&quot;BatchStrategy&quot;: &quot;SingleRecord&quot;,\n&quot;MaxPayloadInMB&quot;: 100,\n&quot;Environment&quot;: {\n&quot;DEEPAR_INFERENCE_CONFIG&quot; : &quot;{ &quot;num_samples&quot;: 200, &quot;output_types&quot;: [&quot;mean&quot;] }&quot;\n},\n&quot;TransformJobName&quot;: batch_job_name,\n&quot;ModelName&quot;: 'forecasting-deepar-2022-06-21-14-12-51-560',\n&quot;TransformOutput&quot;: {\n&quot;S3OutputPath&quot;: output_location,\n&quot;Accept&quot;: &quot;application\/jsonlines&quot;,\n&quot;AssembleWith&quot;: &quot;Line&quot;\n},\n[...]<\/p>\n<p>I am following this project: <a href=\"https:\/\/github.com\/CatherineSai\/project_sales_prediction_DEEPAR\" rel=\"nofollow noreferrer\">https:\/\/github.com\/CatherineSai\/project_sales_prediction_DEEPAR<\/a>\nNotebook 3 of 3 step 49<\/p>\n<p>--&gt; ModelName is copied from the finished training job at aws console<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655834560427,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|deepar",
        "Question_view_count":256,
        "Owner_creation_time":1655833935377,
        "Owner_last_access_time":1657467816853,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72705272",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55020390,
        "Question_title":"How do I start an AWS Sagemaker training job with GPU access in my docker container?",
        "Question_body":"<p>I have some python code that trains a Neural Network using tensorflow. <\/p>\n\n<p>I've created a docker image based on a tensorflow\/tensorflow:latest-gpu-py3 image that runs my python script.\nWhen I start an EC2 p2.xlarge instance I can run my docker container using the command<\/p>\n\n<pre><code>docker run --runtime=nvidia cnn-userpattern train\n<\/code><\/pre>\n\n<p>and the container with my code runs with no errors and uses the host GPU. <\/p>\n\n<p>The problem is, when I try to run the same container in an AWS Sagemaker training job with instance ml.p2.xlarge (I also tried with ml.p3.2xlarge), the algorithm fails with error code:<\/p>\n\n<blockquote>\n  <p>ImportError: libcuda.so.1: cannot open shared object file: No such file or directory<\/p>\n<\/blockquote>\n\n<p>Now I know what that error code means. It means that the runtime environment of the docker host is not set to \"nvidia\". The AWS documentation says that the command used to run the docker image is always<\/p>\n\n<pre><code>docker run image train\n<\/code><\/pre>\n\n<p>which would work if the default runtime is set to \"nvidia\" in the docker\/deamon.json. Is there any way to edit the host deamon.json or tell docker in the Dockerfile to use \"--runtime=nvidia\"?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1551866752687,
        "Question_score":2,
        "Question_tags":"amazon-web-services|docker|tensorflow|gpu|amazon-sagemaker",
        "Question_view_count":1858,
        "Owner_creation_time":1370085456943,
        "Owner_last_access_time":1656071683470,
        "Owner_location":null,
        "Owner_reputation":344,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":"<p>With some help of the AWS support service we were able to find the problem.\nThe docker image I used to run my code on was, as I said tensorflow\/tensorflow:latest-gpu-py3 (available on <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a>)<\/p>\n\n<p>the \"latest\" tag refers to version 1.12.0 at this time. The problem was not my own, but with this version of the docker image. <\/p>\n\n<p>If I base my docker image on tensorflow\/tensorflow:1.10.1-gpu-py3, it runs as it should and uses the GPU fully. <\/p>\n\n<p>Apparently the default runtime is set to \"nvidia\" in the docker\/deamon.json on all GPU instances of AWS sagemaker.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1551964655196,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55020390",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68150444,
        "Question_title":"AWS SageMaker fails loading PyTorch .pth weights",
        "Question_body":"<p>I want to deploy PyTorch model to AWS SageMaker endpoint and experience some issues. Below there is a code and logs. Basically, I have .pth file for model trained outside SageMaker (Azure) and I want to move it to AWS. <code>macro_model.tar.gz<\/code> contains model weights <code>macro_model.pth<\/code> file and inference code with <code>requirements.txt<\/code>.<\/p>\n<p><strong>Expected behavior<\/strong><\/p>\n<p>Model deployed to endpoint and generates predictions<\/p>\n<p><strong>Issue<\/strong><\/p>\n<p>SageMaker <code>model_fn<\/code> function doesn't see model weights.<\/p>\n<p>Any ideas what can be wrong?<\/p>\n<p>Deployment code in SageMaker Notebook:<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker.pytorch import PyTorchModel\nfrom sagemaker import get_execution_role\nimport json\nimport numpy as np\n\nrole = get_execution_role()\nconn = boto3.client('s3')\n\nclient = boto3.client('sagemaker')\npytorch_model = PyTorchModel(model_data='s3:\/\/...\/macro_model.tar.gz',\n                             framework_version=&quot;1.7&quot;, py_version=&quot;py3&quot;,\n                             role=role, entry_point='inference.py', source_dir='code')\n\npredictor = pytorch_model.deploy(initial_instance_count=1,instance_type='ml.p2.xlarge', endpoint_name='...')\n<\/code><\/pre>\n<p><code>macro_model.tar.gz<\/code> structure:<\/p>\n<pre><code>|   macro_model\n|           |--macro_model.pth\n|\n|           code\n|               |--inference.py\n|               |--requirements.txt\n|\n<\/code><\/pre>\n<p><code>model_fn<\/code> function implementation:<\/p>\n<pre><code>def model_fn(model_dir):\n    logging.info('Loading the model...')\n\n    layers = [\n        nn.Linear(512, 512),\n        nn.ReLU(),\n        nn.Dropout(0.3),\n        nn.Linear(512, 2)\n      ]\n    \n    logging.info('Layers initiated...')\n    model = VideoRecog_Model1(layers,7)\n    logging.info('Model initiated...')\n    with open(os.path.join(model_dir, 'macro_model.pth'), 'rb') as f:\n        model.load_state_dict(torch.load(f))\n    \n    \n    model.to(device).eval()\n    logging.info('Done loading model')\n    return model\n<\/code><\/pre>\n<p>Logs:<\/p>\n<pre><code>2021-06-27T12:24:56.241+02:00   Collecting opencv-python-headless Downloading opencv_python_headless-4.5.2.54-cp36-cp36m-manylinux2014_x86_64.whl (38.2 MB)\n\n2021-06-27T12:24:57.242+02:00   Collecting moviepy==1.0.3 Downloading moviepy-1.0.3.tar.gz (388 kB)\n    2021-06-27T12:24:57.242+02:00   Collecting av==8.0.3 Downloading av-8.0.3-cp36-cp36m-manylinux2010_x86_64.whl (37.2 MB)\n\n2021-06-27T12:24:58.243+02:00   Collecting decorator&lt;5.0,&gt;=4.0.2 Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n\n2021-06-27T12:24:58.243+02:00   Requirement already satisfied: tqdm&lt;5.0,&gt;=4.11.2 in \/opt\/conda\/lib\/python3.6\/site-packages (from moviepy==1.0.3-&gt;-r \/opt\/ml\/model\/code\/requirements.txt (line 2)) (4.59.0)\n\n2021-06-27T12:24:58.243+02:00   Requirement already satisfied: requests&lt;3.0,&gt;=2.8.1 in \/opt\/conda\/lib\/python3.6\/site-packages (from moviepy==1.0.3-&gt;-r \/opt\/ml\/model\/code\/requirements.txt (line 2)) (2.22.0)\n\n2021-06-27T12:24:58.243+02:00   Collecting proglog&lt;=1.0.0 Downloading proglog-0.1.9.tar.gz (10 kB)\n    \n2021-06-27T12:24:59.244+02:00   Requirement already satisfied: numpy&gt;=1.17.3 in \/opt\/conda\/lib\/python3.6\/site-packages (from moviepy==1.0.3-&gt;-r \/opt\/ml\/model\/code\/requirements.txt (line 2)) (1.19.1)\n    \n2021-06-27T12:24:59.244+02:00   Collecting imageio&lt;3.0,&gt;=2.5 Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\n    \n2021-06-27T12:24:59.244+02:00   Collecting imageio_ffmpeg&gt;=0.2.0 Downloading imageio_ffmpeg-0.4.4-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n    \n2021-06-27T12:25:00.244+02:00   Requirement already satisfied: pillow in \/opt\/conda\/lib\/python3.6\/site-packages (from imageio&lt;3.0,&gt;=2.5-&gt;moviepy==1.0.3-&gt;-r \/opt\/ml\/model\/code\/requirements.txt (line 2)) (8.2.0)\n    \n2021-06-27T12:25:00.244+02:00   Requirement already satisfied: certifi&gt;=2017.4.17 in \/opt\/conda\/lib\/python3.6\/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy==1.0.3-&gt;-r \/opt\/ml\/model\/code\/requirements.txt (line 2)) (2020.12.5)\n    \n2021-06-27T12:25:00.244+02:00   Requirement already satisfied: idna&lt;2.9,&gt;=2.5 in \/opt\/conda\/lib\/python3.6\/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy==1.0.3-&gt;-r \/opt\/ml\/model\/code\/requirements.txt (line 2)) (2.8)\n    \n2021-06-27T12:25:00.244+02:00   Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in \/opt\/conda\/lib\/python3.6\/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy==1.0.3-&gt;-r \/opt\/ml\/model\/code\/requirements.txt (line 2)) (1.25.11)\n    \n2021-06-27T12:25:00.244+02:00   Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in \/opt\/conda\/lib\/python3.6\/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy==1.0.3-&gt;-r \/opt\/ml\/model\/code\/requirements.txt (line 2)) (3.0.4)\n    \n2021-06-27T12:25:00.244+02:00   Building wheels for collected packages: moviepy, proglog Building wheel for moviepy (setup.py): started Building wheel for moviepy (setup.py): finished with status 'done' Created wheel for moviepy: filename=moviepy-1.0.3-py3-none-any.whl size=110726 sha256=d90e44b117edbb3d061d7d3d800e6e687392ac6b06f9001f82559c4dd88f19c4 Stored in directory: \/root\/.cache\/pip\/wheels\/be\/dc\/17\/8b4d5a63bcd05dc44db7da57e193372ccd333617293f9deebe Building wheel for proglog (setup.py): started\n    \n2021-06-27T12:25:01.245+02:00   Building wheel for proglog (setup.py): finished with status 'done' Created wheel for proglog: filename=proglog-0.1.9-py3-none-any.whl size=6147 sha256=da1b510090c3b87cf4c564558a64b996bc1fdf34d6d15fd56c14c9c776f5b366 Stored in directory: \/root\/.cache\/pip\/wheels\/e7\/11\/a0\/7e65f734d33043735a557b1244569cca327353db9068158076\n    \n2021-06-27T12:25:01.245+02:00   Successfully built moviepy proglog\n    \n2021-06-27T12:25:01.245+02:00   Installing collected packages: proglog, imageio-ffmpeg, imageio, decorator, opencv-python-headless, moviepy, av\n    \n2021-06-27T12:25:02.246+02:00   Attempting uninstall: decorator Found existing installation: decorator 5.0.9 Uninstalling decorator-5.0.9:\n    \n2021-06-27T12:25:03.246+02:00   Successfully uninstalled decorator-5.0.9\n    \n2021-06-27T12:25:05.252+02:00   Successfully installed av-8.0.3 decorator-4.4.2 imageio-2.9.0 imageio-ffmpeg-0.4.4 moviepy-1.0.3 opencv-python-headless-4.5.2.54 proglog-0.1.9\n    \n2021-06-27T12:25:05.252+02:00   WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https:\/\/pip.pypa.io\/warnings\/venv\n    \n2021-06-27T12:25:07.256+02:00   2021-06-27 10:25:06,537 [INFO ] main org.pytorch.serve.ModelServer -\n    \n2021-06-27T12:25:07.256+02:00   Torchserve version: 0.3.1\n    \n2021-06-27T12:25:07.256+02:00   TS Home: \/opt\/conda\/lib\/python3.6\/site-packages\n    \n2021-06-27T12:25:07.256+02:00   Current directory: \/\n\n2021-06-27T12:25:07.256+02:00   Temp directory: \/home\/model-server\/tmp\n    \n2021-06-27T12:25:07.257+02:00   Number of GPUs: 1\n    \n2021-06-27T12:25:07.257+02:00   Number of CPUs: 1\n    \n2021-06-27T12:25:07.257+02:00   Max heap size: 14097 M\n    \n2021-06-27T12:25:07.257+02:00   Python executable: \/opt\/conda\/bin\/python3.6\n    \n2021-06-27T12:25:07.257+02:00   Config file: \/etc\/sagemaker-ts.properties\n    \n2021-06-27T12:25:07.257+02:00   Inference address: http:\/\/0.0.0.0:8080\n    \n2021-06-27T12:25:07.257+02:00   Management address: http:\/\/0.0.0.0:8080\n    \n2021-06-27T12:25:07.257+02:00   Metrics address: http:\/\/127.0.0.1:8082\n    \n2021-06-27T12:25:07.257+02:00   Model Store: \/.sagemaker\/ts\/models\n    \n2021-06-27T12:25:07.257+02:00   Initial Models: model.mar\n    \n2021-06-27T12:25:07.257+02:00   Log dir: \/logs\n    \n2021-06-27T12:25:07.257+02:00   Metrics dir: \/logs\n    \n2021-06-27T12:25:07.257+02:00   Netty threads: 0\n    \n2021-06-27T12:25:07.257+02:00   Netty client threads: 0\n    \n2021-06-27T12:25:07.257+02:00   Default workers per model: 1\n\n2021-06-27T12:25:07.257+02:00   Blacklist Regex: N\/A\n    \n2021-06-27T12:25:07.257+02:00   Maximum Response Size: 6553500\n    \n2021-06-27T12:25:07.258+02:00   Maximum Request Size: 6553500\n    \n2021-06-27T12:25:07.258+02:00   Prefer direct buffer: false\n    \n2021-06-27T12:25:07.258+02:00   Allowed Urls: [file:\/\/.*|http(s)?:\/\/.*]\n    \n2021-06-27T12:25:07.258+02:00   Custom python dependency for model allowed: false\n    \n2021-06-27T12:25:07.258+02:00   Metrics report format: prometheus\n    \n2021-06-27T12:25:07.258+02:00   Enable metrics API: true\n    \n2021-06-27T12:25:07.258+02:00   2021-06-27 10:25:06,597 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,725 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag a6e950a7055442d88ff2f182fdef1da3\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,744 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,782 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,935 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http:\/\/0.0.0.0:8080\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,935 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,937 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http:\/\/127.0.0.1:8082\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,996 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: \/home\/model-server\/tmp\/.ts.sock.9000\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,999 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]59\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,999 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,999 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:09,013 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: \/home\/model-server\/tmp\/.ts.sock.9000\n    \n2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:09,039 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: \/home\/model-server\/tmp\/.ts.sock.9000.\n    \n2021-06-27T12:25:10.260+02:00   Model server started.\n    \n2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,768 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:model.aws.local,timestamp:1624789509\n    \n2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,776 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:11.663764953613281|#Level:Host|#hostname:model.aws.local,timestamp:1624789509\n    \n2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,777 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:12.690078735351562|#Level:Host|#hostname:model.aws.local,timestamp:1624789509\n    \n2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,777 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:52.1|#Level:Host|#hostname:model.aws.local,timestamp:1624789509\n    \n2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,778 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:59581.45703125|#Level:Host|#hostname:model.aws.local,timestamp:1624789509\n    \n2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,787 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1238.21875|#Level:Host|#hostname:model.aws.local,timestamp:1624789509\n    \n2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,788 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:3.0|#Level:Host|#hostname:model.aws.local,timestamp:1624789509\n    \n2021-06-27T12:25:12.261+02:00   2021-06-27 10:25:11,779 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...\n    \n2021-06-27T12:25:13.262+02:00   2021-06-27 10:25:12,875 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Loading the model...\n    \n2021-06-27T12:25:13.262+02:00   2021-06-27 10:25:12,906 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Layers initiated...\n    \n2021-06-27T12:25:14.262+02:00   2021-06-27 10:25:13,871 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: &quot;https:\/\/download.pytorch.org\/models\/r3d_18-b3b3357e.pth&quot; to \/root\/.cache\/torch\/hub\/checkpoints\/r3d_18-b3b3357e.pth\n    \n2021-06-27T12:25:14.262+02:00   2021-06-27 10:25:13,872 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -\n    \n2021-06-27T12:25:14.262+02:00   2021-06-27 10:25:13,972 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 0%| | 0.00\/127M [00:00&lt;?, ?B\/s]\n    \n2021-06-27T12:25:14.262+02:00   2021-06-27 10:25:14,072 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 6%|\u258c | 7.30M\/127M [00:00&lt;00:01, 76.5MB\/s]\n    \n2021-06-27T12:25:14.262+02:00   2021-06-27 10:25:14,172 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 16%|\u2588\u258b | 20.7M\/127M [00:00&lt;00:00, 114MB\/s]\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,272 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 27%|\u2588\u2588\u258b | 34.0M\/127M [00:00&lt;00:00, 126MB\/s]\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,372 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 38%|\u2588\u2588\u2588\u258a | 48.1M\/127M [00:00&lt;00:00, 134MB\/s]\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,472 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 49%|\u2588\u2588\u2588\u2588\u2589 | 62.1M\/127M [00:00&lt;00:00, 139MB\/s]\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,572 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 60%|\u2588\u2588\u2588\u2588\u2588\u2588 | 76.8M\/127M [00:00&lt;00:00, 144MB\/s]\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,692 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 91.9M\/127M [00:00&lt;00:00, 149MB\/s]\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,794 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 106M\/127M [00:00&lt;00:00, 140MB\/s]\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,852 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 120M\/127M [00:00&lt;00:00, 139MB\/s]\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,987 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Model initiated...\n    \n2021-06-27T12:25:15.263+02:00\n\n2021-06-27 10:25:14,988 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.\n    \n2021-06-27 10:25:14,988 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,988 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):\n    \n2021-06-27T12:25:15.263+02:00\n\n2021-06-27 10:25:14,988 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;, line 182, in &lt;module&gt;\n    \n2021-06-27 10:25:14,988 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;, line 182, in &lt;module&gt;\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - worker.run_server()\n    \n2021-06-27T12:25:15.263+02:00\n\n2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;, line 154, in run_server\n    \n2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;, line 154, in run_server\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - self.handle_connection(cl_socket)\n    \n2021-06-27T12:25:15.263+02:00\n\n2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;, line 116, in handle_connection\n    \n2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;, line 116, in handle_connection\n    \n2021-06-27T12:25:15.263+02:00\n\n2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)\n    \n2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - service, result, code = self.load_model(msg)\n    \n2021-06-27T12:25:15.263+02:00\n\n2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;, line 89, in load_model\n    \n2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py&quot;, line 89, in load_model\n    \n2021-06-27T12:25:15.263+02:00\n\n2021-06-27 10:25:14,990 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)\n    \n2021-06-27 10:25:14,990 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,990 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_loader.py&quot;, line 104, in load\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,990 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - initialize_fn(service.context)\n    \n2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,990 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;\/home\/model-server\/tmp\/models\/a6e950a7055442d88ff2f182fdef1da3\/handler_service.py&quot;, line 51, in initialize\n    \n2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - super().initialize(context)\n    \n2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/default_handler_service.py&quot;, line 66, in initialize\n    \n2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - self._service.validate_and_initialize(model_dir=model_dir)\n    \n2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py&quot;, line 158, in validate_and_initialize\n    \n2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - self._model = self._model_fn(model_dir)\n    \n2021-06-27T12:25:15.264+02:00\n\n2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File &quot;\/opt\/ml\/model\/code\/inference.py&quot;, line 105, in model_fn\n    \n2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;\/opt\/ml\/model\/code\/inference.py&quot;, line 105, in model_fn\n    \n2021-06-27T12:25:15.264+02:00\n\n2021-06-27 10:25:14,992 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     with open(os.path.join(model_dir, 'macro_model.pth'), 'rb') as f:\n    \n2021-06-27 10:25:14,992 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - with open(os.path.join(model_dir, 'macro_model.pth'), 'rb') as f:\n    \n2021-06-27T12:25:15.264+02:00\n\n**2021-06-27 10:25:14,992 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - FileNotFoundError: [Errno 2] No such file or directory: '\/home\/model-server\/tmp\/models\/a6e950a7055442d88ff2f182fdef1da3\/macro_model.pth'**\n    \n**2021-06-27 10:25:14,992 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - FileNotFoundError: [Errno 2] No such file or directory: '\/home\/model-server\/tmp\/models\/a6e950a7055442d88ff2f182fdef1da3\/macro_model.pth'**\n    \n2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,994 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 127M\/127M [00:00&lt;00:00, 136MB\/s]\n    \n2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,998 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\n    \n2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,999 [WARN ] W-9000-model_1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1624791595807,
        "Question_score":1,
        "Question_tags":"pytorch|endpoint|amazon-sagemaker",
        "Question_view_count":1535,
        "Owner_creation_time":1621518657017,
        "Owner_last_access_time":1659890990490,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1624795869140,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68150444",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58016791,
        "Question_title":"How to use AWS SageMaker to do XGBoost hyperparameter tuning externally?",
        "Question_body":"<p>No bias here, but I find it hard to find anything in AWS documentation. Microsoft Azure is much easier for me.<\/p>\n\n<p>Here is what I have now: <\/p>\n\n<ul>\n<li>A binary classification app fully built with Python, with xgboost being the ML model. Here xgboost has a set of optimized hyperparameters obtained from SageMaker.<\/li>\n<li>A SageMaker notebook to launch hyperparameter tuning jobs for xgboost. Then I manually copy and paste and hyperparameters into xgboost model in the Python app to do prediction.<\/li>\n<\/ul>\n\n<p>As you can see, the way I do it is far away from ideal. What I want to do now is adding a piece of code in the Python app to initiate the hyperparameters job in SageMaker automatically and return the best model as well. That way, the hyperparameter job is automated and I don't need to do the copy and paste again.<\/p>\n\n<p>However, I haven't been able to do that yet. I followed this <a href=\"https:\/\/aws.amazon.com\/blogs\/apn\/integrating-with-amazon-sagemaker-using-built-in-algorithms-from-external-applications\/\" rel=\"nofollow noreferrer\">documentation<\/a> to install Python SageMaker API. I also have the following code that do XGBoost hyperparameter tuning in SageMaker notebook:<\/p>\n\n<pre><code> def train_xgb_sagemaker(df_train, df_test):\n    pd.concat([df_train['show_status'], df_train.drop(['show_status'], axis=1)], axis=1).to_csv('train.csv',\n                                                                                                index=False,\n                                                                                                header=False)\n    pd.concat([df_test['show_status'], df_test.drop(['show_status'], axis=1)], axis=1).to_csv('validation.csv',\n                                                                                              index=False, header=False)\n\n    boto3.Session().resource('s3').Bucket(bucket, prefix).upload_file(\n        'train.csv')\n\n    boto3.Session().resource('s3').Bucket(bucket, prefix).upload_file(\n        'validation.csv')\n\n    s3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train'.format(bucket, prefix), content_type='csv')\n    s3_input_validation = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/validation\/'.format(bucket, prefix), content_type='csv')\n\n    print('train_path: ', s3_input_train)\n    print('validation_path: ', s3_input_validation)\n\n    # hyperparameter tuning of XGBoost - SageMaker\n    sess = sagemaker.Session()\n\n    container = get_image_uri(region, 'xgboost', 0.90 - 1)\n    xgb = sagemaker.estimator.Estimator(container,\n                                        role,\n                                        train_instance_count=1,\n                                        train_instance_type='ml.m4.xlarge',\n                                        output_path='s3:\/\/{}\/{}\/output'.format(params['BUCKET'], prefix),\n                                        sagemaker_session=sess)\n\n    xgb.set_hyperparameters(eval_metric='auc',\n                            objective='binary:logistic',\n                            num_round=100,\n                            rate_drop=0.3,\n                            tweedie_variance_power=1.4)\n\n    hyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n                             'min_child_weight': ContinuousParameter(1, 10),\n                             'alpha': ContinuousParameter(0, 2),\n                             'max_depth': IntegerParameter(1, 10),\n                             'num_round': IntegerParameter(1, 300)}\n\n    objective_metric_name = 'validation:auc'\n\n    tuner = HyperparameterTuner(xgb,\n                                objective_metric_name,\n                                hyperparameter_ranges,\n                                max_jobs=20,\n                                max_parallel_jobs=3)\n\n    tuner.fit({'train': s3_input_train, 'validation': s3_input_validation}, include_cls_metadata=False)\n\n    smclient.describe_hyper_parameter_tuning_job(\n        HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']\n\n    print('Please check hyperparameter tuning for best models!')\n    time.sleep(4000)\n    # best_model_path = 's3:\/\/{}\/{}\/output\/{}\/output\/model.tar.gz'.format(bucket, prefix, tuner.best_training_job())\n    return tuner.best_training_job()\n<\/code><\/pre>\n\n<p>So the question is how to embed this piece of code into my Python app so that I can do everything in one place? Thanks very much for any hints as I've been hanging on this problem for days!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1568916609520,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|hyperparameters",
        "Question_view_count":1366,
        "Owner_creation_time":1401031858507,
        "Owner_last_access_time":1658704433413,
        "Owner_location":"Seattle, WA, United States",
        "Owner_reputation":1021,
        "Owner_up_votes":92,
        "Owner_down_votes":0,
        "Owner_views":120,
        "Question_last_edit_time":1568928060696,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58016791",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64126327,
        "Question_title":"Load Amazon Sagemaker NTM model locally for inference",
        "Question_body":"<p>I have trained a Sagemaker <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/introduction-to-the-amazon-sagemaker-neural-topic-model\/\" rel=\"nofollow noreferrer\">NTM<\/a> model which is a neural topic model, directly on the AWS sagemaker platform. Once training is complete you are able to download the <code>mxnet<\/code> model files. Once unpacked the files contain:<\/p>\n<ul>\n<li>params<\/li>\n<li>symbol.json<\/li>\n<li>meta.json<\/li>\n<\/ul>\n<p>I have followed the docs on mxnet to load the model and have the following code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>sym, arg_params, aux_params = mx.model.load_checkpoint('model_algo-1', 0)\nmodule_model = mx.mod.Module(symbol=sym, label_names=None, context=mx.cpu())\n\nmodule_model.bind(\n    for_training=False,\n    data_shapes=[('data', (1, VOCAB_SIZE))]\n)\n\nmodule_model.set_params(arg_params=arg_params, aux_params=aux_params, allow_missing=True) # must set allow missing true here or receive an error for a missing n_epoch var\n<\/code><\/pre>\n<p>I now try and use the model for inference using:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>module_model.predict(x) # where x is a numpy array of size (1, VOCAB_SIZE)\n<\/code><\/pre>\n<p>The code runs, but the result is just a single value, where I expect a distribution over topics:<\/p>\n<pre><code>[11.060672]\n&lt;NDArray 1 @cpu(0)&gt;\n<\/code><\/pre>\n<p>EDIT:<\/p>\n<p>I have tried to load it using the Symbol API, but still no luck:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import warnings\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    deserialized_net = gluon.nn.SymbolBlock.imports('model_algo-1-symbol.json', ['data'], 'model_algo-1-0000.params', ctx=mx.cpu())\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>AssertionError: Parameter 'n_epoch' is missing in file: model_algo-1-0000.params, which contains parameters: 'logsigma_bias', 'enc_0_bias', 'projection_bias', ..., 'enc_1_weight', 'enc_0_weight', 'mean_bias', 'logsigma_weight'. Please make sure source and target networks have the same prefix.\n<\/code><\/pre>\n<p>Any help would be great!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1601407542533,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|nlp|amazon-sagemaker|mxnet",
        "Question_view_count":149,
        "Owner_creation_time":1431530515873,
        "Owner_last_access_time":1663837186380,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":2763,
        "Owner_up_votes":203,
        "Owner_down_votes":35,
        "Owner_views":264,
        "Question_last_edit_time":1601566313100,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64126327",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61090530,
        "Question_title":"Using tidyverse to read data from s3 bucket",
        "Question_body":"<p>I'm trying to read a <code>.csv<\/code> file stored in an s3 bucket, and I'm getting errors. I'm following the instructions <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/r_kernel\/using_r_with_amazon_sagemaker.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, but either it does not work or I am making a mistake and I'm not getting what I'm doing wrong.<\/p>\n\n<p>Here's what I'm trying to do:<\/p>\n\n<pre><code># I'm working on a SageMaker notebook instance\nlibrary(reticulate)\nlibrary(tidyverse)\n\nsagemaker &lt;- import('sagemaker')\nsagemaker.session &lt;- sagemaker$Session()\n\nregion &lt;- sagemaker.session$boto_region_name\nbucket &lt;- \"my-bucket\"\nprefix &lt;- \"data\/staging\"\nbucket.path &lt;- sprintf(\"https:\/\/s3-%s.amazonaws.com\/%s\", region, bucket)\nrole &lt;- sagemaker$get_execution_role()\n\nclient &lt;- sagemaker.session$boto_session$client('s3')\nkey &lt;- sprintf(\"%s\/%s\", prefix, 'my_file.csv')\n\nmy.obj &lt;- client$get_object(Bucket=bucket, Key=key)\n\nmy.df &lt;- read_csv(my.obj$Body) # This is where it all breaks down:\n## \n## Error: `file` must be a string, raw vector or a connection.\n## Traceback:\n## \n## 1. read_csv(my.obj$Body)\n## 2. read_delimited(file, tokenizer, col_names = col_names, col_types = col_types, \n##  .     locale = locale, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment, n_max = n_max, guess_max = guess_max, \n##  .     progress = progress)\n## 3. col_spec_standardise(data, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment, guess_max = guess_max, col_names = col_names, \n##  .     col_types = col_types, tokenizer = tokenizer, locale = locale)\n## 4. datasource(file, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment)\n## 5. stop(\"`file` must be a string, raw vector or a connection.\", \n##  .     call. = FALSE)\n<\/code><\/pre>\n\n<p>When working with Python, I can read a CSV file using someting like this:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>import pandas as pd\n# ... Lots of boilerplate code\nmy_data = pd.read_csv(client.get_object(Bucket=bucket, Key=key)['Body'])\n<\/code><\/pre>\n\n<p>This is very similar to what I'm trying to do in R, and it works with Python... so why does it not work on R?<\/p>\n\n<p>Can you point me in the right path?<\/p>\n\n<p><strong>Note:<\/strong> Although I could use a Python kernel for this, I'd like to stick to R, because I'm more fluent with it than with Python, at least when it comes to dataframe crunching.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1586299117960,
        "Question_score":1,
        "Question_tags":"r|amazon-s3|amazon-sagemaker|readr",
        "Question_view_count":1735,
        "Owner_creation_time":1275433277097,
        "Owner_last_access_time":1664032713307,
        "Owner_location":"Mexico",
        "Owner_reputation":20017,
        "Owner_up_votes":1826,
        "Owner_down_votes":1932,
        "Owner_views":2754,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I'd recommend trying the <code>aws.s3<\/code> package instead:<\/p>\n\n<p><a href=\"https:\/\/github.com\/cloudyr\/aws.s3\" rel=\"nofollow noreferrer\">https:\/\/github.com\/cloudyr\/aws.s3<\/a><\/p>\n\n<p>Pretty simple - set your env variables:<\/p>\n\n<pre><code>Sys.setenv(\"AWS_ACCESS_KEY_ID\" = \"mykey\",\n           \"AWS_SECRET_ACCESS_KEY\" = \"mysecretkey\",\n           \"AWS_DEFAULT_REGION\" = \"us-east-1\",\n           \"AWS_SESSION_TOKEN\" = \"mytoken\")\n<\/code><\/pre>\n\n<p>and then once that is out of the way:<\/p>\n\n<p><code>aws.s3::s3read_using(read.csv, object = \"s3:\/\/bucket\/folder\/data.csv\")<\/code><\/p>\n\n<p>Update: I see you're also already familiar with boto and trying to use reticulate so leaving this easy wrapper for that here:\n<a href=\"https:\/\/github.com\/cloudyr\/roto.s3\" rel=\"nofollow noreferrer\">https:\/\/github.com\/cloudyr\/roto.s3<\/a><\/p>\n\n<p>Looks like it has a great api for example the variable layout you're aiming to use:<\/p>\n\n<pre><code>download_file(\n  bucket = \"is.rud.test\", \n  key = \"mtcars.csv\", \n  filename = \"\/tmp\/mtcars-again.csv\", \n  profile_name = \"personal\"\n)\n\nread_csv(\"\/tmp\/mtcars-again.csv\")\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1586562277483,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1586740595876,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61090530",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64655582,
        "Question_title":"ML Pipeline on AWS SageMaker: How to create long-running query\/preprocessing tasks",
        "Question_body":"<p>I'm a software engineer transitioning toward machine learning engineering, but need some assistance.<\/p>\n<p>I'm currently using AWS Lambda and Step Functions to run query and preprocessing jobs for my ML pipeline, but am restrained by Lambda's 15m runtime limitation.<\/p>\n<p>We're a strictly AWS shop, so I'm kind of stuck with SageMaker and other AWS tools for the time being. Later on we'll consider experimenting with something like Kubeflow if it looks advantageous enough.<\/p>\n<p><strong>My current process<\/strong><\/p>\n<ul>\n<li>I have my data scientists write python scripts (in a git repo) for the query and preprocessing steps of a model, and deploy them (via Terraform) as Lambda functions, then use Step Functions to sequence the ML Pipeline steps as a DAG (query -&gt; preprocess -&gt; train -&gt; deploy)<\/li>\n<li>The Query lambda pulls data from our data warehouse (Redshift), and writes the unprocessed dataset to S3<\/li>\n<li>The Preprocessing lambda loads the unprocessed dataset from S3, manipulates it as needed, and writes it as training &amp; validation datasets to a different S3 location<\/li>\n<li>The Train and Deploy tasks use the SageMaker python api to train and deploy the models as SageMaker Endpoints<\/li>\n<\/ul>\n<p>Do I need to be using Glue and SageMaker Processing jobs? From what I can tell, Glue seems more targeted towards ETLs than for writing to S3, and SageMaker Processing jobs seem a bit more complex to deploy to than Lambda.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1604367008457,
        "Question_score":1,
        "Question_tags":"amazon-web-services|machine-learning|aws-lambda|pipeline|amazon-sagemaker",
        "Question_view_count":468,
        "Owner_creation_time":1268298510173,
        "Owner_last_access_time":1608649804973,
        "Owner_location":"Austin, TX",
        "Owner_reputation":1209,
        "Owner_up_votes":29,
        "Owner_down_votes":0,
        "Owner_views":55,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64655582",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64752554,
        "Question_title":"GPU utilization is zero when running batch transform in Amazon SageMaker",
        "Question_body":"<p>I want to run a batch transform job on AWS SageMaker. I have an image classification model which I have trained on a local GPU. Now I want to deploy it on AWS SageMaker and make predictions using Batch Transform. While the batch transform job runs successfully, the GPU utilization during the job is always zero (GPU Memory utilization, however, is at 97%). That's what CloudWatch is telling me. Also, the job takes approx. 7 minutes to process 500 images, I would expect it to run much faster than this, at least when comparing it to the time it takes to process the images on a local GPU.<\/p>\n<p><strong>My question:<\/strong> Why doesn't the GPU get used during batch transform, even though I am using a GPU instance (I am using an ml.p3.2xlarge instance)? I was able to deploy the very same model to an endpoint and send requests. When deploying to an endpoint instead of using batch transform, the GPU actually gets used.<\/p>\n<p><strong>Model preparation<\/strong><\/p>\n<p>I am using a Keras Model with TensorFlow backend. I converted this model to a sagemaker model using this guide <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/<\/a> :<\/p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.python.saved_model import builder\nfrom tensorflow.python.saved_model.signature_def_utils import predict_signature_def\nfrom tensorflow.python.saved_model import tag_constants\nimport tarfile\nimport sagemaker\n\n# deactivate eager mode\nif tf.executing_eagerly():\n   tf.compat.v1.disable_eager_execution()\n\nbuilder = builder.SavedModelBuilder(export_dir)\n\n# Create prediction signature to be used by TensorFlow Serving Predict API\nsignature = predict_signature_def(\n    inputs={&quot;image_bytes&quot;: model.input}, outputs={&quot;score_bytes&quot;: model.output})\n\nwith tf.compat.v1.keras.backend.get_session() as sess:\n    # Save the meta graph and variables\n    builder.add_meta_graph_and_variables(\n        sess=sess, tags=[tag_constants.SERVING], signature_def_map={&quot;serving_default&quot;: signature})\n    builder.save()\n\nwith tarfile.open(tar_model_file, mode='w:gz') as archive:\n    archive.add('export', recursive=True)\n\nsagemaker_session = sagemaker.Session()\ns3_uri = sagemaker_session.upload_data(path=tar_model_file, bucket=bucket, key_prefix=sagemaker_model_dir)\n<\/code><\/pre>\n<p><strong>Batch Transform<\/strong><\/p>\n<p>Container image used for batch transform: 763104351884.dkr.ecr.eu-central-1.amazonaws.com\/tensorflow-inference:2.0.0-gpu<\/p>\n<pre><code>framework = 'tensorflow'\ninstance_type='ml.p3.2xlarge'\nimage_scope = 'inference'\ntf_version = '2.0.0'\npy_version = '3.6'\n\nsagemaker_model = TensorFlowModel(model_data=MODEL_TAR_ON_S3, role=role, image_uri=tensorflow_image)\n\ntransformer = sagemaker_model.transformer(\n    instance_count = 1,\n    instance_type = instance_type,\n    strategy='MultiRecord',\n    max_concurrent_transforms=8,\n    max_payload=10, # in MB\n    output_path = output_data_path,\n)\n\ntransformer.transform(data = input_data_path,\n                      job_name = job_name,\n                      content_type = 'application\/json', \n                      logs=False,\n                      wait=True\n)\n<\/code><\/pre>\n<p><strong>Log file excerpts<\/strong><\/p>\n<p>Loading the model takes quite long (several minutes). During this time, the following error message is getting logged:<\/p>\n<blockquote>\n<p>2020-11-08T15:14:12.433+01:00            2020\/11\/08 14:14:12 [error]\n14#14: *3066 no live upstreams while connecting to upstream, client:\n169.254.255.130, server: , request: &quot;GET \/ping HTTP\/1.1&quot;, subrequest: &quot;\/v1\/models\/my_model:predict&quot;, upstream:\n&quot;http:\/\/tfs_upstream\/v1\/models\/my_model:predict&quot;, host:\n&quot;169.254.255.131:8080&quot; 2020-11-08T15:14:12.433+01:00<br \/>\n169.254.255.130 - - [08\/Nov\/2020:14:14:12 +0000] &quot;GET \/ping HTTP\/1.1&quot; 502 157 &quot;-&quot; &quot;Go-http-client\/1.1&quot; 2020-11-08T15:14:12.433+01:00<br \/>\n2020\/11\/08 14:14:12 [error] 14#14: *3066 js: failed ping#015\n2020-11-08T15:14:12.433+01:00            502 Bad\nGateway#015 2020-11-08T15:14:12.433+01:00<\/p>\n#015 2020-11-08T15:14:12.433+01:00            <h1>502\nBad Gateway<\/h1>#015 2020-11-08T15:14:12.433+01:00           \n<hr>nginx\/1.16.1#015 2020-11-08T15:14:12.433+01:00   \n#015 2020-11-08T15:14:12.433+01:00            #015\n<\/blockquote>\n<p>There was a log entry about NUMA node read:<\/p>\n<blockquote>\n<p>successful NUMA node read from SysFS had negative value (-1), but\nthere must be at least one NUMA node, so returning NUMA node zero<\/p>\n<\/blockquote>\n<p>And about a serving warmup request:<\/p>\n<blockquote>\n<p>No warmup data file found at\n\/opt\/ml\/model\/export\/my_model\/1\/assets.extra\/tf_serving_warmup_requests<\/p>\n<\/blockquote>\n<p>And this warning:<\/p>\n<blockquote>\n<p>[warn] getaddrinfo: address family for nodename not supported<\/p>\n<\/blockquote>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1604928355263,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|tensorflow-serving",
        "Question_view_count":505,
        "Owner_creation_time":1505593588753,
        "Owner_last_access_time":1659104216800,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64752554",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67570694,
        "Question_title":"How to build YOLACT++ using Docker?",
        "Question_body":"<p>I have to build yolact++ in docker enviromment (i'm using sagemaker notebook). Like this<\/p>\n<pre><code>ARG PYTORCH=&quot;1.3&quot;\nARG CUDA=&quot;10.1&quot;\nARG CUDNN=&quot;7&quot;\n \nFROM pytorch\/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel\n<\/code><\/pre>\n<p>And i want to run this<\/p>\n<pre><code>COPY yolact\/external\/DCNv2\/setup.py \/opt\/ml\/code\/external\/DCNv2\/setup.py\nRUN cd \/opt\/ml\/code\/external\/DCNv2 &amp;&amp; \\\npython setup.py build develop\n<\/code><\/pre>\n<p>But i got this error :<\/p>\n<pre><code>No CUDA runtime is found, using CUDA_HOME='\/usr\/local\/cuda'\nTraceback (most recent call last):\nFile &quot;setup.py&quot;, line 64, in &lt;module&gt;\next_modules=get_extensions(),\nFile &quot;setup.py&quot;, line 41, in get_extensions\nraise NotImplementedError('Cuda is not available')\nNotImplementedError: Cuda is not available\n<\/code><\/pre>\n<p>But the enviromment supports CUDA. Anyone have an idea where is the problem ?<\/p>\n<p>Thank you.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1621258222833,
        "Question_score":0,
        "Question_tags":"docker|pytorch|dockerfile|amazon-sagemaker",
        "Question_view_count":486,
        "Owner_creation_time":1587069846163,
        "Owner_last_access_time":1634476659293,
        "Owner_location":"France",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1621519151352,
        "Answer_body":"<p>SOLUTION :<\/p>\n<p>i edit the \/etc\/docker\/daemon.json with content:<\/p>\n<pre><code>{\n&quot;runtimes&quot;: {\n    &quot;nvidia&quot;: {\n        &quot;path&quot;: &quot;\/usr\/bin\/nvidia-container-runtime&quot;,\n        &quot;runtimeArgs&quot;: []\n     } \n},\n&quot;default-runtime&quot;: &quot;nvidia&quot; \n}\n<\/code><\/pre>\n<p>Then i Restart docker daemon:<\/p>\n<pre><code>sudo system restart docker\n<\/code><\/pre>\n<p>it solved my problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1621519129332,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1621519479056,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67570694",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64238362,
        "Question_title":"reading a csv.gz file from sagemaker using pyspark kernel mode",
        "Question_body":"<p>i am trying to read a compressed csv file in pyspark. but i am unable to read in pyspark kernel mode in sagemaker.<\/p>\n<p>The same file i can read using pandas when the kernel is conda-python3 (in sagemaker)<\/p>\n<p>What I tried :<\/p>\n<pre><code>file1 =  's3:\/\/testdata\/output1.csv.gz'\nfile1_df = spark.read.csv(file1, sep='\\t')\n<\/code><\/pre>\n<p>Error message :<\/p>\n<pre><code>An error was encountered:\nAn error occurred while calling 104.csv.\n: java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 7FF77313; S3 Extended Request ID: \n<\/code><\/pre>\n<p>Kindly let me know if i am missing anything<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1602051997903,
        "Question_score":0,
        "Question_tags":"python|apache-spark|amazon-s3|pyspark|amazon-sagemaker",
        "Question_view_count":790,
        "Owner_creation_time":1441820957717,
        "Owner_last_access_time":1623397378193,
        "Owner_location":null,
        "Owner_reputation":351,
        "Owner_up_votes":26,
        "Owner_down_votes":0,
        "Owner_views":101,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64238362",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68676725,
        "Question_title":"How to use tensorflow library with sagemaker preprocessing",
        "Question_body":"<p>I want to use TensorFlow for preprocessing in sagemaker pipelines.<\/p>\n<p>But, I haven't been able to find a way to use it.<\/p>\n<p>Right now, I'm using this library for preprocessing:<\/p>\n<pre><code>from sagemaker.sklearn.processing import SKLearnProcessor\n\nframework_version = &quot;0.23-1&quot;\n\nsklearn_processor = SKLearnProcessor(\n    framework_version=framework_version,\n    instance_type=processing_instance_type,\n    instance_count=processing_instance_count,\n    base_job_name=&quot;abcd&quot;,\n    role=role,\n)\n<\/code><\/pre>\n<p><strong>Now, I need to use TensorFlow in preprocessing but the python module cant import TensorFlow.<\/strong><\/p>\n<p>Any help would be much appreciated. Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1628229065587,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker|preprocessor",
        "Question_view_count":53,
        "Owner_creation_time":1537442477530,
        "Owner_last_access_time":1648212741713,
        "Owner_location":null,
        "Owner_reputation":19,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68676725",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55295692,
        "Question_title":"Understanding Sagemaker Neo",
        "Question_body":"<p>I have few questions for <a href=\"https:\/\/aws.amazon.com\/sagemaker\/neo\/\" rel=\"nofollow noreferrer\">Sagemaker Neo<\/a>:<\/p>\n\n<p>1) Can I take advantage of Sagemaker Neo if I have an externally trained tensorflow\/mxnet model?<\/p>\n\n<p>2) Sagemaker provides container image for <em>'image-classification'<\/em> and it has released a new image with name <em>'<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo-deployment-hosting-services-cli.html\" rel=\"nofollow noreferrer\">image-classification-neo<\/a>'<\/em> for the neo compilation job. What is the difference between both of them? Do I require a new Neo compatible image for each pre built sagemaker template(container) similarly?<\/p>\n\n<p>Any help would be appreciated<\/p>\n\n<p>Thanks!!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1553243780953,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-machine-learning",
        "Question_view_count":820,
        "Owner_creation_time":1515418712450,
        "Owner_last_access_time":1617420030177,
        "Owner_location":null,
        "Owner_reputation":91,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":27,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55295692",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63255556,
        "Question_title":"ThrottlingException Rate exceeded SageMaker Studio",
        "Question_body":"<p>What I am simply trying to do is shut down my SageMaker studio instance. I am using <code>datascience--1-0-ml-m5-8xlarge<\/code> instance for my SageMaker studio notebook. While running my notebook, I saw that amount of memory being taken was more than the memory available (128 GB). As a result, I reset the notebook. From then on, I am basically not able to shut down the instance as shown in the image<a href=\"https:\/\/i.stack.imgur.com\/5mIhM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5mIhM.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Does anyone know the reason behind this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1596577831680,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":449,
        "Owner_creation_time":1415806163833,
        "Owner_last_access_time":1663845435483,
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":805,
        "Owner_up_votes":915,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63255556",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71398882,
        "Question_title":"CUDA: RuntimeError: CUDA out of memory - BERT sagemaker",
        "Question_body":"<p>I have been trying to train a BertSequenceForClassification Model using AWS Sagemaker. i'm using hugging face estimators. but I keep getting the error: <code>RuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 11.17 GiB total capacity; 10.73 GiB already allocated; 87.88 MiB free; 10.77 GiB reserved in total by PyTorch)<\/code> the same code runs fine on my laptop.<\/p>\n<ol>\n<li>how do I check what is occupying that 10GB of memory? my dataset is pretty small (68kb), so is my batch size (8) and epochs (1). When I run nvidia-smi, i can only see &quot;No processes running&quot; and the GPU memory usage is zero. When I run <code>print(torch.cuda.memory_summary(device=None, abbreviated=False))<\/code> from within my training script (right before it throws the error) it prints<\/li>\n<\/ol>\n<pre><code>|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 0                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| Allocations           |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Active allocs         |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| GPU reserved segments |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|===========================================================================|\n<\/code><\/pre>\n<p>but I have no idea what it means or how to interpret it<\/p>\n<ol start=\"2\">\n<li>when i run <code>!df -h<\/code> I can see:<\/li>\n<\/ol>\n<pre><code>Filesystem      Size  Used Avail Use% Mounted on\ndevtmpfs         30G   72K   30G   1% \/dev\ntmpfs            30G     0   30G   0% \/dev\/shm\n\/dev\/xvda1      109G   93G   16G  86% \/\n\/dev\/xvdf       196G   61M  186G   1% \/home\/ec2-user\/SageMaker\n<\/code><\/pre>\n<p>how is this memory different from the GPU? if theres 200GB in \/dev\/xvdf is there anyway I can just use that..? in my test script I tried<br \/>\n<code>model = BertForSequenceClassification.from_pretrained(args.model_name,num_labels=args.num_labels).to(&quot;cpu&quot;)<\/code>\nbut that just gives the same error<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646759194503,
        "Question_score":1,
        "Question_tags":"python|gpu|amazon-sagemaker|huggingface-transformers",
        "Question_view_count":1983,
        "Owner_creation_time":1597997723910,
        "Owner_last_access_time":1661243820287,
        "Owner_location":null,
        "Owner_reputation":115,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1646767755129,
        "Answer_body":"<p>A <code>CUDA out of memory<\/code> error indicates that your GPU RAM (Random access memory) is full. This is different from the storage on your device (which is the info you get following the <code>df -h<\/code> command).<\/p>\n<p>This memory is occupied by the model that you load into GPU memory, which is independent of  your dataset size. The GPU memory required by the model is at least twice the actual size of the model, but most likely closer to 4 times (initial weights, checkpoint, gradients, optimizer states, etc).<\/p>\n<p>Things you can try:<\/p>\n<ul>\n<li>Provision an instance with more GPU memory<\/li>\n<li>Decrease batch size<\/li>\n<li>Use a different (smaller) model<\/li>\n<\/ul>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1646760128676,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1646760473952,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71398882",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51670563,
        "Question_title":"Invalid .lst file in sagemaker",
        "Question_body":"<p>Folder structure for my S3 bucket is:<\/p>\n\n<pre><code>Bucket\n    -&gt;training-set\n           -&gt;medium\n                 -&gt;    img1.jpeg\n                 -&gt;    img2.jpeg\n                 -&gt;    img3.PNG\n<\/code><\/pre>\n\n<p>My training-set.lst file looks like this:<\/p>\n\n<pre><code>1  \\t 1  \\t medium\/img1.jpeg\n2  \\t 1  \\t medium\/img2.jpeg\n3  \\t 1  \\t medium\/img3.PNG\n<\/code><\/pre>\n\n<p>I created this using excel sheet.<\/p>\n\n<p>Error:\nTraining failed with the following error: ClientError: Invalid lst file: training-set.lst<\/p>\n\n<pre><code>   \"InputDataConfig\": [\n        {\n          \"ChannelName\": \"train\",\n          \"CompressionType\": \"None\",\n          \"ContentType\": \"application\/x-image\",\n          \"DataSource\": {\n            \"S3DataSource\": {\n              \"S3DataDistributionType\": \"FullyReplicated\",\n              \"S3DataType\": \"S3Prefix\",\n              \"S3Uri\": 's3:\/\/{}\/training-set\/'.format(bucket)\n            }\n          },\n          \"RecordWrapperType\": \"None\"\n        },\n        {\n          \"ChannelName\": \"validation\",\n          \"CompressionType\": \"None\",\n          \"ContentType\": \"application\/x-image\",\n          \"DataSource\": {\n            \"S3DataSource\": {\n              \"S3DataDistributionType\": \"FullyReplicated\",\n              \"S3DataType\": \"S3Prefix\",\n              \"S3Uri\": 's3:\/\/{}\/test-set\/'.format(bucket)\n            }\n          },\n          \"RecordWrapperType\": \"None\"\n        },\n        {\n          \"ChannelName\": \"train_lst\",\n          \"CompressionType\": \"None\",\n          \"ContentType\": \"application\/x-image\",\n          \"DataSource\": {\n            \"S3DataSource\": {\n              \"S3DataDistributionType\": \"FullyReplicated\",\n              \"S3DataType\": \"S3Prefix\",\n              \"S3Uri\": \"s3:\/\/bucket\/training-set\/training-set.lst\"\n            }\n          },\n          \"RecordWrapperType\": \"None\"\n        },\n        {\n          \"ChannelName\": \"validation_lst\",\n          \"CompressionType\": \"None\",\n          \"ContentType\": \"application\/x-image\",\n          \"DataSource\": {\n            \"S3DataSource\": {\n              \"S3DataDistributionType\": \"FullyReplicated\",\n              \"S3DataType\": \"S3Prefix\",\n              \"S3Uri\": \"s3:\/\/bucket\/test-set\/test-set.lst\"\n            }\n          },\n          \"RecordWrapperType\": \"None\"\n        }\n    ]\n<\/code><\/pre>\n\n<p>I am trying to use this in Amazon Sagemaker. But I'm unable to do that. Can someone please help?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1533291751550,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|image-recognition|amazon-sagemaker",
        "Question_view_count":591,
        "Owner_creation_time":1523786300320,
        "Owner_last_access_time":1663816627680,
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":177,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Question_last_edit_time":1533294102060,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51670563",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58956312,
        "Question_title":"Possible to attach Elastic IP to sagemaker notebook instance?",
        "Question_body":"<p>I want to connect to a database running in different cloud provider and it is exposed publicly.<\/p>\n\n<p>I need to connect to that database from sagemaker notebook instance.<\/p>\n\n<p>But the public ip of the sagemaker notebook instance needs to be whitelisted on the other side.<\/p>\n\n<p>Is it possible to attach elastic ip to sagemaker notebook instance as I don't see any option to attach eip to sagemaker notebook instance?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1574259012963,
        "Question_score":2,
        "Question_tags":"amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":708,
        "Owner_creation_time":1369922081203,
        "Owner_last_access_time":1663926212973,
        "Owner_location":"Bangalore, India",
        "Owner_reputation":3577,
        "Owner_up_votes":2395,
        "Owner_down_votes":20,
        "Owner_views":626,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58956312",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73241880,
        "Question_title":"Distributed Training Terminology: Micro-batch and Per-Replica batch size",
        "Question_body":"<p>I am reading through the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/distributed-training.html\" rel=\"nofollow noreferrer\">Sagemaker documentation<\/a> on distributed training and confused on the terminology:<\/p>\n<p>Mini-Batch, Micro-batch and Per-replica batch size<\/p>\n<p>I understand that in data parallelism, there would be multiple copies of the model and each copy would receive data of  size = &quot;Per Replica Batch Size&quot;<\/p>\n<ol>\n<li>Could someone ELI5 how micro-batch would fit in this context?<\/li>\n<li>Is this a common terminology used in the terminology or is this specific to AWS Sagemaker<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659644678150,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker|distributed-training",
        "Question_view_count":23,
        "Owner_creation_time":1390885171883,
        "Owner_last_access_time":1663717157010,
        "Owner_location":"College Station, TX, United States",
        "Owner_reputation":426,
        "Owner_up_votes":408,
        "Owner_down_votes":1,
        "Owner_views":27,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73241880",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71989724,
        "Question_title":"Removing annotators in case of bad performance in AWS ground truth",
        "Question_body":"<p>Is there a way in AWS Ground Truth to validate annotations from workers during the labeling task and <strong>remove the worker<\/strong> if the performance is really poor <strong>before<\/strong> they finish the task?<\/p>\n<p>I know that there are other services that sometimes present an instance for which the label is known to the worker. In case the workers do not label this correctly, they get kicked out. This is to assure quality of the annotations.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1650813165487,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-ground-truth",
        "Question_view_count":17,
        "Owner_creation_time":1297784519700,
        "Owner_last_access_time":1662102483020,
        "Owner_location":"Helsinki, Finland",
        "Owner_reputation":143,
        "Owner_up_votes":1708,
        "Owner_down_votes":1,
        "Owner_views":78,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71989724",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69745719,
        "Question_title":"How to Use Prebuilt Deep Learning Algorithms in SageMaker?",
        "Question_body":"<p>Is it possible to use AWS prebuilt algorithms, e.g., Image Classification, locally with SageMaker? I tried to pull the image-classification image URI using <code>aws ecr get-login-password<\/code> but I get the following error message:<\/p>\n<pre><code>Error response from daemon: pull access denied for 813361260812.dkr.ecr.eu-central-1.amazonaws.com\/image-classification, repository does not exist or may require 'docker login': denied: User: xxxxxxxxxxxxxxxxxxxxxxxx is not authorized to perform: ecr:BatchGetImage on resource: arn:aws:ecr:eu-central-1:813361260812:repository\/image-classification because no resource-based policy allows the ecr:BatchGetImage action\n<\/code><\/pre>\n<p>I gave my user full access so the policies include <code>BatchGetImage<\/code>. Can someone explain why this error occurs?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1635370244877,
        "Question_score":2,
        "Question_tags":"amazon-web-services|docker|amazon-iam|amazon-sagemaker",
        "Question_view_count":221,
        "Owner_creation_time":1586416145180,
        "Owner_last_access_time":1658518190400,
        "Owner_location":null,
        "Owner_reputation":97,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":1635422783200,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69745719",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60072981,
        "Question_title":"How to open a model tarfile stored in S3 bucket in sagemaker notebook?",
        "Question_body":"<p>I know that loading a .csv file into sagemaker notebook from S3 bucket is pretty straightforward but I want to load a model.tar.gz file stored in S3 bucket. I tried to do the following<\/p>\n\n<pre><code>import botocore \nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.predictor import csv_serializer\nimport boto3\n\nsm_client = boto3.client(service_name='sagemaker')\nruntime_sm_client = boto3.client(service_name='sagemaker-runtime')\n\ns3 = boto3.resource('s3')\ns3_client = boto3.client('s3')\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\nACCOUNT_ID  = boto3.client('sts').get_caller_identity()['Account']\nREGION      = boto3.Session().region_name\nBUCKET      = 'sagemaker.prismade.net'\ndata_key    = 'DEMO_MME_ANN\/multi_model_artifacts\/axel.tar.gz'\nloc = 's3:\/\/{}\/{}'.format(BUCKET, data_key)\nprint(loc)\nwith tarfile.open(loc) as tar:\n    tar.extractall(path='.')\n<\/code><\/pre>\n\n<p>I get the following error:<\/p>\n\n<pre><code>--------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\n&lt;ipython-input-215-bfdddac71b95&gt; in &lt;module&gt;()\n     20 loc = 's3:\/\/{}\/{}'.format(BUCKET, data_key)\n     21 print(loc)\n---&gt; 22 with tarfile.open(loc) as tar:\n     23     tar.extractall(path='.')\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/tarfile.py in open(cls, name, mode, fileobj, bufsize, **kwargs)\n   1567                     saved_pos = fileobj.tell()\n   1568                 try:\n-&gt; 1569                     return func(name, \"r\", fileobj, **kwargs)\n   1570                 except (ReadError, CompressionError):\n   1571                     if fileobj is not None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/tarfile.py in gzopen(cls, name, mode, fileobj, compresslevel, **kwargs)\n   1632 \n   1633         try:\n-&gt; 1634             fileobj = gzip.GzipFile(name, mode + \"b\", compresslevel, fileobj)\n   1635         except OSError:\n   1636             if fileobj is not None and mode == 'r':\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/gzip.py in __init__(self, filename, mode, compresslevel, fileobj, mtime)\n    161             mode += 'b'\n    162         if fileobj is None:\n--&gt; 163             fileobj = self.myfileobj = builtins.open(filename, mode or 'rb')\n    164         if filename is None:\n    165             filename = getattr(fileobj, 'name', '')\n\nFileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/sagemaker.prismade.net\/DEMO_MME_ANN\/multi_model_artifacts\/axel.tar.gz'\n<\/code><\/pre>\n\n<p>What is the mistake here and how can I accomplish this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1580895955387,
        "Question_score":4,
        "Question_tags":"python-3.x|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":2984,
        "Owner_creation_time":1550756471933,
        "Owner_last_access_time":1663939288837,
        "Owner_location":null,
        "Owner_reputation":67,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Not every python library that is designed to work with a file system (tarfile.open, in this example) knows how to read an object from S3 as a file. <\/p>\n\n<p>The simple way to solve it is to first copy the object into the local file system as a file.<\/p>\n\n<pre><code>import boto3\n\ns3 = boto3.client('s3')\ns3.download_file('BUCKET_NAME', 'OBJECT_NAME', 'FILE_NAME')\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1581004771980,
        "Answer_score":7.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60072981",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67878535,
        "Question_title":"Getting \"[Errno 28] No space left on device\" on AWS SageMaker with plenty of storage left",
        "Question_body":"<p>I am running a notebook instance on Amazon Sagemaker and my understanding is that notebooks by default have 5GB of storage.  I am running into <code>[Errno 28] No space left on device<\/code> on a notebook that worked just fine the last time I tried it.  I checked and I'm using approximately 1.5GB out of 5GB. I'm trying to download a bunch of files from my S3 bucket but I get the error even before one file is downloaded.  Additionally, the notebook no longer autosaves.<\/p>\n<p>Has anyone run into this and figured out a way to fix it? I've already tried clearing all outputs.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1623098969000,
        "Question_score":0,
        "Question_tags":"amazon-web-services|storage|amazon-sagemaker",
        "Question_view_count":1612,
        "Owner_creation_time":1623098611487,
        "Owner_last_access_time":1633990010460,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67878535",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60748772,
        "Question_title":"How to force a SageMaker Notebook instance to stop?",
        "Question_body":"<p>I am an AWS noob, My notebook instance has been on <strong>Pending Status<\/strong> for a couple hours.<\/p>\n\n<p>How can I force it to STOP ? Or at least get my code back.<\/p>\n\n<p>Thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1584573282977,
        "Question_score":1,
        "Question_tags":"jupyter-notebook|amazon-sagemaker",
        "Question_view_count":1104,
        "Owner_creation_time":1432935437533,
        "Owner_last_access_time":1659214407257,
        "Owner_location":null,
        "Owner_reputation":345,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":107,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60748772",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67843602,
        "Question_title":"Could not find model PipelineModel",
        "Question_body":"<p>When I try to build models to create a pipeline as follows,<\/p>\n<pre><code>    &lt;code for the preprocessor&gt;\n    preprocessor = sklearn_preprocessor.create_model() #successful\n\n    &lt;code for the estimator&gt;\n    xgb_model_step = xgb_model.create_model() #successful    \n    \n    sm_model = PipelineModel(name='model', role=role, models=[preprocessor, xgb_model_step]) #successful\n    sm_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge', endpoint_name=end) &lt;--- failure!\n<\/code><\/pre>\n<p>The models are created successfully. In the deploy line I get an error as,<\/p>\n<p><code>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3:\/\/sagemaker-us-east-1-1356784978535\/sagemaker-scikit-learn-2021-06-04-20-07-55-519\/output\/model.tar.gz.<\/code><\/p>\n<p>I am not sure how I can specify the path and make the deploy successful. Can somebody please help me with this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1622838635930,
        "Question_score":1,
        "Question_tags":"amazon-web-services|scikit-learn|amazon-sagemaker",
        "Question_view_count":77,
        "Owner_creation_time":1473401410977,
        "Owner_last_access_time":1646051838717,
        "Owner_location":null,
        "Owner_reputation":129,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":41,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67843602",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60607041,
        "Question_title":"AWS Sagemaker Spark S3 access issue",
        "Question_body":"<p>I am new in AWS sagemaker. I created a notebook in a VPC with private subnet, kms default encrypted key, root access, no direct internet access. I have attached policy which have full access to Sagemaker and S3 in IAM as per documentations.  Now while one of data scientist trying to run his code in jupyter, getting below error. I can see jar files (\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker_pyspark\/jars\/), I have even given access key and secret key in code, is there anything we are doing wrong here<\/p>\n\n<pre><code>import os\nimport boto3\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nimport sagemaker\nfrom sagemaker import get_execution_role\nimport sagemaker_pyspark\nimport pyspark\n\nrole = get_execution_role()\nspark = SparkSession.builder \\\n            .appName(\"app_name2\") \\\n            .getOrCreate()\n\nsc=pyspark.SparkContext.getOrCreate()\nsc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")\n\nhadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", 'access_key')\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", 'secret_key')\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.us-east-2.amazonaws.com\")\nspark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3a.enableV4\", \"true\")\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\");\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\ndf= spark.read.csv(\"s3a:\/\/mybucket\/ConsolidatedData\/my.csv\",header=\"true\")\n\n\nPy4JJavaError: An error occurred while calling o579.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n    at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)\n    at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)\n    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n    at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:709)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1583781550823,
        "Question_score":0,
        "Question_tags":"amazon-web-services|apache-spark|pyspark|jupyter|amazon-sagemaker",
        "Question_view_count":933,
        "Owner_creation_time":1513883236660,
        "Owner_last_access_time":1663906475810,
        "Owner_location":"Noida, Uttar Pradesh, India",
        "Owner_reputation":465,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":46,
        "Question_last_edit_time":1583817907990,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60607041",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61122143,
        "Question_title":"aws sagemaker: install new r packages error",
        "Question_body":"<p>I'm trying to use jupyter notebook in AWS sagemaker with an <code>r<\/code> kernel. However there is a specific library that I want to use (<code>imager<\/code>) that does not exist. Based on previous SO <a href=\"https:\/\/stackoverflow.com\/a\/42459747\/1652217\">answers<\/a>, I tried to do<\/p>\n\n<pre><code>install.packages(\"imager\", repos='http:\/\/cran.us.r-project.org')\n<\/code><\/pre>\n\n<p>This throws the following warning<\/p>\n\n<pre><code>Warning message in install.packages(\"imager\", repos = \"http:\/\/cran.us.r-project.org\"):\n\u201cinstallation of package \u2018imager\u2019 had non-zero exit status\u201dUpdating HTML index of packages in '.Library'\nMaking 'packages.html' ... done\n<\/code><\/pre>\n\n<p>However when I try to load this package again,<\/p>\n\n<pre><code>library(\"imager\")\n<\/code><\/pre>\n\n<p>it throws the following error,<\/p>\n\n<pre><code>Error in library(\"imager\"): there is no package called \u2018imager\u2019\nTraceback:\n\n1. library(\"imager\")\n<\/code><\/pre>\n\n<p>I'm not sure why the aws sagemaker is not being able to locate the installed package for loading<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1586438462937,
        "Question_score":0,
        "Question_tags":"r|amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":551,
        "Owner_creation_time":1346939871660,
        "Owner_last_access_time":1663994857593,
        "Owner_location":"Australia",
        "Owner_reputation":1912,
        "Owner_up_votes":357,
        "Owner_down_votes":7,
        "Owner_views":126,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61122143",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71610716,
        "Question_title":"How can I retrieve a folder from S3 into an AWS SageMaker notebook",
        "Question_body":"<p>I have a folder with several files corresponding to checkpoints of a RL model trained using RLLIB. I want to make an analysis of the checkpoints in a way that I need to pass a certain folder as an argument, e.g., <code>analysis_function(folder_path)<\/code>. I have to run this line on a SageMaker notebook. I have seen that there are some questions on SO about how to retrieve files from s3, such as <a href=\"https:\/\/stackoverflow.com\/questions\/48264656\/load-s3-data-into-aws-sagemaker-notebook\">this<\/a> one. However; how can I retrieve a whole folder?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648166098547,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":711,
        "Owner_creation_time":1614455025457,
        "Owner_last_access_time":1663151887240,
        "Owner_location":null,
        "Owner_reputation":57,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71610716",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70750358,
        "Question_title":"Automate the date parameter while deplying the model on AWS Wrangler",
        "Question_body":"<p>I have built a XGBoost model on my local machine which takes a training data and validates the model on a testing dataset. However, I have hard-coded the date values as the training data is created monthly. The training data gets created based on what Date Parameter I pass. Eg, jan = dt(2021,1,1).<\/p>\n<p>I now have to automate the process as the model has to be deployed on AWS and should run monthly without editing the code. How should I pass the date parameter to AWS Wrangler so that the process will be automated, and the code will execute once every month on a new dataset.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1642480744327,
        "Question_score":0,
        "Question_tags":"amazon-web-services|date|amazon-sagemaker|aws-data-wrangler",
        "Question_view_count":127,
        "Owner_creation_time":1607598608387,
        "Owner_last_access_time":1658981623800,
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":15,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1643584734012,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70750358",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53424429,
        "Question_title":"Getting error while invoking API using AWS Lambda. (AWS Lambda + AWS API Gateway+ Postman)",
        "Question_body":"<p>I get an error while invoking the AWS SageMaker endpoint API from a Lambda function. When I call this using Postman, I am getting an error like: <\/p>\n\n<pre><code>{\n    \"errorMessage\": \"module initialization error\"\n}\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1542864700527,
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":420,
        "Owner_creation_time":1542864496280,
        "Owner_last_access_time":1611854760103,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1542988547732,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53424429",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61074798,
        "Question_title":"Deploy pre-trained tensorflow model on the aws sagemaker - ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation",
        "Question_body":"<p>This is the first time I am using amazon web services to deploy my machine learning pre-trained model. I want to deploy my pre-trained TensorFlow model to Aws-Sagemaker. I am somehow able to deploy the endpoints successfully But whenever I call the <code>predictor.predict(some_data)<\/code> method to make prediction to invoking the endpoints it's throwing an error.<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\". See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-tensorflow-2020-04-07-04-25-27-055 in account 453101909370 for more information.\n<\/code><\/pre>\n\n<p>After going through the cloud watch logs I found this error.<\/p>\n\n<pre><code>#011details = \"NodeDef mentions attr 'explicit_paddings' not in Op&lt;name=Conv2D; signature=input:T, filter:T -&gt; output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=dilations:list(int),default=[1, 1, 1, 1]&gt;; NodeDef: {{node conv1_conv\/convolution}} = Conv2D[T=DT_FLOAT, _output_shapes=[[?,112,112,64]], data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\"](conv1_pad\/Pad, conv1_conv\/kernel\/read). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n<\/code><\/pre>\n\n<p>I don't know where I am wrong and I have wasted 2 days already to solve this error and couldn't find out the information regarding this. The detailed logs I have shared <a href=\"https:\/\/docs.google.com\/document\/d\/1NXsLRd6cfbNE55xSVq5d63ETt-cBmwZsOaic8IyF1Qw\/edit?usp=sharing\" rel=\"nofollow noreferrer\">here<\/a>. <\/p>\n\n<p>Tensorflow version of my notebook instance is 1.15<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586244384423,
        "Question_score":2,
        "Question_tags":"amazon-web-services|tensorflow|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":740,
        "Owner_creation_time":1480786532470,
        "Owner_last_access_time":1663657056320,
        "Owner_location":"Nagpur, Sitabuldi, Nagpur, Maharashtra, India",
        "Owner_reputation":111,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Question_last_edit_time":1586255907796,
        "Answer_body":"<p>After a lot of searching and try &amp; error, I was able to solve this problem. In many cases, the problem arises because of the TensorFlow and Python versions.<\/p>\n<p><strong>Cause of the problem:<\/strong>\nTo deploy the endpoints, I was using the <code>TensorflowModel<\/code> on TF 1.12 and python 3 and which exactly caused the problem.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = TensorFlowModel(model_data = model_data,\n                                  role = role,\n                                  framework_version = '1.12',\n                                  entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Apparently, <code>TensorFlowModel<\/code> only allows python 2 on TF version 1.11, 1.12. 2.1.0.<\/p>\n<p><strong>How I fixed it:<\/strong> There are two TensorFlow solutions that handle serving in the Python SDK. They have different class representations and documentation as shown here.<\/p>\n<ol>\n<li><strong>TensorFlowModel<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts<\/a><\/li>\n<li>Key difference: Uses a proxy GRPC client to send requests<\/li>\n<li>Container impl:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py<\/a><\/li>\n<\/ul>\n<ol start=\"2\">\n<li><strong>Model<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst<\/a><\/li>\n<li>Key difference: Utilizes the TensorFlow serving rest API<\/li>\n<li>Container impl: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py<\/a><\/li>\n<\/ul>\n<p>Python 3 isn't supported using the <code>TensorFlowModel<\/code> object, as the container uses the TensorFlow serving API library in conjunction with the GRPC client to handle making inferences, however, the TensorFlow serving API isn't supported in Python 3 officially, so there are only Python 2 versions of the containers when using the <code>TensorFlowModel<\/code> object.\nIf you need Python 3 then you will need to use the <code>Model<\/code> object defined in #2 above.<\/p>\n<p>Finally, I used the <code>Model<\/code> with the TensorFlow version 1.15.1.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = Model(model_data = model_data,\n                        role = role,\n                        framework_version='1.15.2',\n                        entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Also, here are the successful results.\n<a href=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1593503184160,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61074798",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72694415,
        "Question_title":"ValidationException Importing from Redshift into Data Wrangler",
        "Question_body":"<p>I'm trying to build a model workflow in AWS SageMaker using Data Wrangler for preprocessing. I'm loading data from various tables in a Redshift instance, before mutating and joining them as required to build the model input data.<\/p>\n<p>I'm a contractor working for a company who has provisioned some resource in their AWS environment for me to work, and am reading from a production database. If I do not load open the Data Wrangler flow early enough in the day (which I suspect is related to load on their system), some of the nodes which I have created will not validate, and instead show a red cross and the following error message:<\/p>\n<p><code>RedshiftQueryExecutionIdValidationError: An error occurred when trying to invoke `describe_statement`: An error occurred (ValidationException) when calling the DescribeStatement operation: Could not retrieve the query result as it has expired after 1655759552.<\/code><\/p>\n<p>The remaining un-errored nodes appear to hang in a loading\/validating state. Here's a screenshot of part of the flow in this state:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/AewzK.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AewzK.png\" alt=\"An image showing some currently loading AWS Data Wrangler nodes, with some errored nodes joining in towards the end of the flow, rendering the end product errored and thus unusable\" \/><\/a><\/p>\n<p>I'm not sure if it's related, but I occasionally see error messages pop up saying something about &quot;too many inflight requests&quot;.<\/p>\n<p>My main issue, I think, is a lack of context. I have not worked in this environment before, and am finding it difficult to diagnose the issue. It might be possible to provision more resource, and I could likely trim down some of the information before reading it in, but I'd like to be able to read the error messages and understand what's <em>causing<\/em> the nodes to error, so that I can decide on the appropriate course of action.<\/p>\n<p>Can somebody please help explain what's going on here?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1655774625443,
        "Question_score":1,
        "Question_tags":"amazon-redshift|amazon-sagemaker|aws-data-wrangler",
        "Question_view_count":57,
        "Owner_creation_time":1427947978893,
        "Owner_last_access_time":1663642097267,
        "Owner_location":"Auckland, New Zealand",
        "Owner_reputation":25,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72694415",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54432761,
        "Question_title":"Amazon SageMaker hyperparameter tuning error for built-in algorithm using the Python SDK",
        "Question_body":"<p>When using the Python SDK to start a SageMaker hyperparameter tuning job using one of the built-in algorithms (in this case, the Image Classifier) with the following code:<\/p>\n\n<pre><code># [...] Some lines elided for brevity\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter\nhyperparameter_ranges = {'optimizer': CategoricalParameter(['sgd', 'adam']),\n                         'learning_rate': ContinuousParameter(0.0001, 0.2),\n                         'mini_batch_size': IntegerParameter(2, 30),}\n\nobjective_metric_name = 'validation:accuracy'\n\ntuner = HyperparameterTuner(image_classifier,\n                            objective_metric_name,\n                            hyperparameter_ranges,\n\n                            max_jobs=50,\n                            max_parallel_jobs=3)\n\ntuner.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n\n<p>The job fails and I get this error when checking on the job status in the SageMaker web console:<\/p>\n\n<pre><code>ClientError: Additional hyperparameters are not allowed (u'sagemaker_estimator_module', u'sagemaker_estimator_class_name' were unexpected) (caused by ValidationError) \n\nCaused by: Additional properties are not allowed (u'sagemaker_estimator_module', u'sagemaker_estimator_class_name' were unexpected) \n\nFailed validating u'additionalProperties' in schema: {u'$schema': u'http:\/\/json-schema.org\/schema#', u'additionalProperties': False, u'definitions': {u'boolean_0_1': {u'oneOf': [{u'enum': [u'0', u'1'], u'type': u'string'}, {u'enum': [0, 1], u'type': u'number'}]}, u'boolean_true_false_0_1': {u'oneOf': [{u'enum': [u'true', u'false',\n<\/code><\/pre>\n\n<p>I'm not explicitly passing the <code>sagemaker_estimator_module<\/code> or <code>sagemaker_estimator_class_name<\/code> properties anywhere, so I'm not sure why it's returning this error. <\/p>\n\n<p>What's the right way to start this tuning job?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548817091420,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|hyperparameters",
        "Question_view_count":595,
        "Owner_creation_time":1224733422317,
        "Owner_last_access_time":1661146167157,
        "Owner_location":"Singapore",
        "Owner_reputation":7707,
        "Owner_up_votes":162,
        "Owner_down_votes":6,
        "Owner_views":583,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I found the answer via <a href=\"https:\/\/translate.google.com\/translate?hl=en&amp;sl=ja&amp;u=https:\/\/dev.classmethod.jp\/machine-learning\/sagemaker-tuning-stack\/&amp;prev=search\" rel=\"nofollow noreferrer\">this post translated from Japanese<\/a>.<\/p>\n\n<p>When starting hyperparameter tuning jobs using the built-in algorithms in the Python SDK, <strong>you need to explicitly pass <code>include_cls_metadata=False<\/code><\/strong> as a keyword argument to <code>tuner.fit()<\/code> like this:<\/p>\n\n<p><code>tuner.fit(inputs=data_channels, logs=True, include_cls_metadata=False)<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1548817091420,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54432761",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73133746,
        "Question_title":"Fb-Prophet, Apache Spark in Colab and AWS SageMaker\/ Lambda",
        "Question_body":"<p>I am using <code>Google-Colab<\/code> for creating a model by using FbProphet and i am try to use Apache Spark in the <code>Google-Colab<\/code> itself. Now can i upload this <code>Google-colab<\/code> notebook in <code>aws Sagemaker\/Lambda<\/code> for free <code>(without charge for Apache Spark and only charge for AWS SageMaker)<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658906440657,
        "Question_score":1,
        "Question_tags":"apache-spark|google-colaboratory|amazon-sagemaker|facebook-prophet",
        "Question_view_count":51,
        "Owner_creation_time":1658906023853,
        "Owner_last_access_time":1663921457750,
        "Owner_location":null,
        "Owner_reputation":152,
        "Owner_up_votes":15,
        "Owner_down_votes":1,
        "Owner_views":11,
        "Question_last_edit_time":1660220920907,
        "Answer_body":"<p>In short, You can upload the notebook without any issue into SageMaker. Few things to keep in mind<\/p>\n<ol>\n<li>If you are using the pyspark library in colab and running spark locally,  you should be able to do the same by installing necessary pyspark libs in Sagemaker studio kernels. Here you will only pay for the underlying compute for the notebook instance. If you are experimenting then I would recommend you to use <a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a> to create a free account and try things out.<\/li>\n<li>If you had a separate spark cluster setup then you may need a similar setup in AWS using EMR so that you can connect to the cluster to execute the job.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1658964743500,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73133746",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71441244,
        "Question_title":"SageMager Studio Notebook Kernel keeps starting",
        "Question_body":"<p>Trying to execute cells in an Amazon SageMager Studio Notebook I continuously receive the message &quot;Note: The kernel is still starting. Please execute this cell again after the kernel is started.&quot; The bottom status bar claims &quot;Kernel: Starting...&quot; The &quot;Running Terminals and Kernels&quot; overview shows a running instance ml.t3.medium with running app datascience-1.0 and kernel session corresponding to the notebook title. I tried restarting SageMaker Studio and opened it in another region but neither helped.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1647014221243,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":661,
        "Owner_creation_time":1331995664973,
        "Owner_last_access_time":1661365050443,
        "Owner_location":null,
        "Owner_reputation":377,
        "Owner_up_votes":29,
        "Owner_down_votes":0,
        "Owner_views":57,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71441244",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57494223,
        "Question_title":"access my own image files or load them in phoneoxpth from S3 to sage maker",
        "Question_body":"<p>I'm fairly new to working with AWS, and I want to use SageMaker to train a certain image data set using fast.ai. But I have no clue how to link all the image data from S3 to SageMaker.<\/p>\n\n<p>I tried almost everything I could think of, used s3fs and I can read the images separately and the list of the images, but how do I feed that info to my databunch or learning algorithm?<\/p>\n\n<p>My code:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\nimport pandas as pd\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\nbucket='sagemaker-sst-images'\ndata_key = 'SST_Data\/sst-images'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n<\/code><\/pre>\n\n<p>This code, I think, gives a URL to the data. \nBut what comes next? Either get it into a path, or load the data properly?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1565783749733,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":168,
        "Owner_creation_time":1542989984733,
        "Owner_last_access_time":1663051725663,
        "Owner_location":"Helsinki, Finlande",
        "Owner_reputation":77,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":1565785044280,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57494223",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57601733,
        "Question_title":"How do I install R packages on the SageMaker Notebook instance?",
        "Question_body":"<p>I tried to start a R notebook in Sagemaker and I typed<\/p>\n\n<pre class=\"lang-r prettyprint-override\"><code>install.packages(\"disk.frame\")\n<\/code><\/pre>\n\n<p>and it gave me the error<\/p>\n\n<pre><code>also installing the dependencies \u2018listenv\u2019, \u2018dplyr\u2019, \u2018rlang\u2019, \u2018furrr\u2019, \n\u2018future.apply\u2019, \u2018fs\u2019, \u2018pryr\u2019, \u2018fst\u2019, \u2018globals\u2019, \u2018future\u2019\n\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018rlang\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018fs\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018pryr\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018fst\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018dplyr\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018disk.frame\u2019 had non-zero exit status\u201d\nUpdating HTML index of packages in '.Library'\nMaking 'packages.html' ... done\n<\/code><\/pre>\n\n<p>How do I install R packages on Sagemaker?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1566445052783,
        "Question_score":3,
        "Question_tags":"r|amazon-web-services|amazon-sagemaker",
        "Question_view_count":2786,
        "Owner_creation_time":1262052472410,
        "Owner_last_access_time":1653952086580,
        "Owner_location":"Melbourne VIC, Australia",
        "Owner_reputation":13830,
        "Owner_up_votes":1039,
        "Owner_down_votes":77,
        "Owner_views":1372,
        "Question_last_edit_time":1566447149796,
        "Answer_body":"<p>I think you just need to specify a repo. For example, setting the RStudio CRAN repo, I can install perfectly fine.<\/p>\n\n<pre class=\"lang-r prettyprint-override\"><code>install.packages(\"disk.frame\", repo=\"https:\/\/cran.rstudio.com\/\")\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1566562240969,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57601733",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64263330,
        "Question_title":"Data Preprocessing on AWS SageMaker",
        "Question_body":"<p>I have an endpoint running a trained SageMaker model on AWS, which expects the data on a specific format.<\/p>\n<p>Initially, the data has been processed on the client side of the application, it means, the <code>API Gateway<\/code> (which receives the POST API calls on AWS) used to receive pre-processed data, but now there's a change, the <code>API Gateway<\/code> will receive <strong>raw data<\/strong> from the client, and the job of pre-processing this data before sending to our SageMaker model is up to our workflow.<\/p>\n<p>What is the best way to create a pre-processing job on this workflow, without needing to re-train the model? My pre-process is just a bunch of dataframe transformations, no standardization or calculation with the training set required (it would not need to save any model file).<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1602162886373,
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-api-gateway|amazon-sagemaker",
        "Question_view_count":450,
        "Owner_creation_time":1405317204730,
        "Owner_last_access_time":1663779824497,
        "Owner_location":null,
        "Owner_reputation":69,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":"<p>After some research, this is the solution I've followed:<\/p>\n<ul>\n<li>First I have created a <code>SKLearn<\/code> sagemaker model to do all the preprocess setup (I've built a Scikit-Learn custom class to handle all the preprocess steps, following this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb\" rel=\"nofollow noreferrer\">AWS code<\/a>)<\/li>\n<li>Trained this preprocess model on my training data. My model, in specific, didn't need to be trained (it does not have any standardization or anything that would need to store training data parameters), but sagemaker requires the model to be trained.<\/li>\n<li>Loaded the trained legacy model that we had using the <code>Model<\/code> parameter.<\/li>\n<li>Created a <code>PipelineModel<\/code> with the preprocessing model and legacy model in cascade:<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>pipeline_model = PipelineModel(name=model_name,\n                               role=role,\n                               models=[\n                                    preprocess_model,\n                                    trained_model\n                               ])\n<\/code><\/pre>\n<ul>\n<li>Create a new endpoint, calling the <code>PipelineModel<\/code> and then changed the <code>Lambda<\/code> function to call this new endpoint. With this I could send the <strong>raw data<\/strong> directly for the same <code>API Gateway<\/code> and it would call only <strong>one<\/strong> endpoint, without needing to pay two endpoints 24\/7 to perform the entire process.<\/li>\n<\/ul>\n<p>I've found this to be a good and &quot;<em>economic<\/em>&quot; way to perform the preprocess outside the trained model, without having to do hard processing jobs on a <code>Lambda<\/code> function.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1602762793703,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1602871502600,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64263330",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51560452,
        "Question_title":"AWS SageMaker Random Cut Forest or Kinesis Data Analytics Random Cut Forest?",
        "Question_body":"<p>I need to put together an architecture that can detect anomalies in logs created by a web application.<\/p>\n\n<p>The Random Cut Forest algorithm constantly pops up in my research, where it is used in two scenarios: SageMaker and Kinesis Data Analytics.<\/p>\n\n<p>Which of these two services should I use in my architecture?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1532702839040,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-kinesis|amazon-kinesis-firehose|amazon-sagemaker",
        "Question_view_count":771,
        "Owner_creation_time":1525448832837,
        "Owner_last_access_time":1561138173263,
        "Owner_location":null,
        "Owner_reputation":63,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Question_last_edit_time":null,
        "Answer_body":"<p>At the core, the mathematical methodology between the two is nearly identical, but there are some differences in how they are implemented within Kinesis and SageMaker that should help drive your decision. <\/p>\n\n<p>Kinesis RandomCutForest:<\/p>\n\n<ul>\n<li>Streaming version of the algorithm which is great for near-real-time updates to the model.<\/li>\n<li>Supports time decay of older records, shingling of the input data, and if you are using multiple dimensions, anomaly attribution that helps you understand the effect of each of the dimensions.  <\/li>\n<li>So, in case your logs are being stored in CloudWatch, by using subscription filters (and Lambda if needed) you can get them preprocessed and sent to Kinesis with little effort.  <\/li>\n<\/ul>\n\n<p>SageMaker RandomCutForest:<\/p>\n\n<ul>\n<li>Batch version of the algorithm, great for large datasets (typically stored in S3) or where there's no need to update the model frequently.  <\/li>\n<li>Similar to Kinesis, supports near-real-time scoring of incoming data points via inference endpoint, but new data points do not change the underlying model.  <\/li>\n<li>Supports hyper parameter optimization, which identifies the best set of parameters for your model (such as number of samples, number of trees etc.)  <\/li>\n<li>Scaling up instances for both training and scoring is straightforward, and the available SageMaker Notebooks can help you preprocess and prepare your data for training.  <\/li>\n<li>So, if your dataset is large and you don't have a need for dynamic updates to your model, SageMaker solution should be preferred solution for you.  <\/li>\n<\/ul>\n\n<p>Hope this answers your question.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1533237075180,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51560452",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63405080,
        "Question_title":"SageMaker in local Jupyter notebook: cannot use AWS hosted XGBoost container (\"KeyError: 'S3DistributionType'\" and \"Failed to run: ['docker-compose'\")",
        "Question_body":"<p>Running SageMaker within a local Jupyter notebook (using VS Code) works without issue, except that attempting to train an XGBoost model using the AWS hosted container results in errors (container name: <code>246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-xgboost:1.0-1-cpu-py3<\/code>).<\/p>\n<h2>Jupyter Notebook<\/h2>\n<pre class=\"lang-py prettyprint-override\"><code>import sagemaker\n\nsession = sagemaker.LocalSession()\n\n# Load and prepare the training and validation data\n...\n\n# Upload the training and validation data to S3\ntest_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\nval_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\ntrain_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n\nregion = session.boto_region_name\ninstance_type = 'ml.m4.xlarge'\ncontainer = sagemaker.image_uris.retrieve('xgboost', region, '1.0-1', 'py3', instance_type=instance_type)\n\nrole = 'arn:aws:iam::&lt;USER ID #&gt;:role\/service-role\/AmazonSageMaker-ExecutionRole-&lt;ROLE ID #&gt;'\n\nxgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n\nxgb_estimator.set_hyperparameters(max_depth=5, eta=0.2, gamma=4, min_child_weight=6,\n                                  subsample=0.8, objective='reg:squarederror', early_stopping_rounds=10,\n                                  num_round=200)\n\ns3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='csv')\ns3_input_validation = sagemaker.inputs.TrainingInput(s3_data=val_location, content_type='csv')\n\nxgb_estimator.fit({'train': s3_input_train, 'validation': s3_input_validation})\n<\/code><\/pre>\n<h2>Docker Container KeyError<\/h2>\n<pre><code>algo-1-tfcvc_1  | ERROR:sagemaker-containers:Reporting training FAILURE\nalgo-1-tfcvc_1  | ERROR:sagemaker-containers:framework error: \nalgo-1-tfcvc_1  | Traceback (most recent call last):\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_containers\/_trainer.py&quot;, line 84, in train\nalgo-1-tfcvc_1  |     entrypoint()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 94, in main\nalgo-1-tfcvc_1  |     train(framework.training_env())\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 90, in train\nalgo-1-tfcvc_1  |     run_algorithm_mode()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 68, in run_algorithm_mode\nalgo-1-tfcvc_1  |     checkpoint_config=checkpoint_config\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/algorithm_mode\/train.py&quot;, line 115, in sagemaker_train\nalgo-1-tfcvc_1  |     validated_data_config = channels.validate(data_config)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 106, in validate\nalgo-1-tfcvc_1  |     channel_obj.validate(value)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 52, in validate\nalgo-1-tfcvc_1  |     if (value[CONTENT_TYPE], value[TRAINING_INPUT_MODE], value[S3_DIST_TYPE]) not in self.supported:\nalgo-1-tfcvc_1  | KeyError: 'S3DistributionType'\n\n<\/code><\/pre>\n<h2>Local PC Runtime Error<\/h2>\n<pre><code>RuntimeError: Failed to run: ['docker-compose', '-f', '\/tmp\/tmp71tx0fop\/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1\n<\/code><\/pre>\n<p>If the Jupyter notebook is run using the Amazon cloud SageMaker environment (rather than on the local PC), there are no errors. Note that when running on the cloud notebook, the session is initialized as:<\/p>\n<pre><code>session = sagemaker.Session()\n<\/code><\/pre>\n<p>It appears that there is an issue with how the <code>LocalSession()<\/code> works with the hosted docker container.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597366468133,
        "Question_score":0,
        "Question_tags":"python|docker|jupyter-notebook|xgboost|amazon-sagemaker",
        "Question_view_count":1174,
        "Owner_creation_time":1327302732867,
        "Owner_last_access_time":1664072442223,
        "Owner_location":"USA",
        "Owner_reputation":19711,
        "Owner_up_votes":5189,
        "Owner_down_votes":56,
        "Owner_views":1030,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When running SageMaker in a local Jupyter notebook, it expects the Docker container to be running on the local machine as well.<\/p>\n<p>The key to ensuring that SageMaker (running in a local notebook) uses the AWS hosted docker container, is to omit the <code>LocalSession<\/code> object when initializing the <code>Estimator<\/code>.<\/p>\n<h2>Wrong<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n<\/code><\/pre>\n<h2>Correct<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output')\n<\/code><\/pre>\n<p>\u00a0\u00a0<\/p>\n<h2>Additional info<\/h2>\n<p>The SageMaker Python SDK source code provides the following helpful hints:<\/p>\n<h1>File: <em>sagemaker\/local\/local_session.py<\/em><\/h1>\n<pre><code>class LocalSagemakerClient(object):\n    &quot;&quot;&quot;A SageMakerClient that implements the API calls locally.\n\n    Used for doing local training and hosting local endpoints. It still needs access to\n    a boto client to interact with S3 but it won't perform any SageMaker call.\n    ...\n<\/code><\/pre>\n<h1>File: <em>sagemaker\/estimator.py<\/em><\/h1>\n<pre><code>class EstimatorBase(with_metaclass(ABCMeta, object)):\n    &quot;&quot;&quot;Handle end-to-end Amazon SageMaker training and deployment tasks.\n\n    For introduction to model training and deployment, see\n    http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\n\n    Subclasses must define a way to determine what image to use for training,\n    what hyperparameters to use, and how to create an appropriate predictor instance.\n    &quot;&quot;&quot;\n\n    def __init__(self, role, train_instance_count, train_instance_type,\n                 train_volume_size=30, train_max_run=24 * 60 * 60, input_mode='File',\n                 output_path=None, output_kms_key=None, base_job_name=None, sagemaker_session=None, tags=None):\n        &quot;&quot;&quot;Initialize an ``EstimatorBase`` instance.\n\n        Args:\n            role (str): An AWS IAM role (either name or full ARN). ...\n            \n        ...\n\n            sagemaker_session (sagemaker.session.Session): Session object which manages interactions with\n                Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one\n                using the default AWS configuration chain.\n        &quot;&quot;&quot;\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1597366468132,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1597367070867,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63405080",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48014413,
        "Question_title":"Connection timed out - using sqlalchemy to access AWS usaspending data",
        "Question_body":"<p>I created an instance of the usaspending.gov database in my AWS RDS. A description of this database can be found here: <a href=\"https:\/\/aws.amazon.com\/public-datasets\/usaspending\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/public-datasets\/usaspending\/<\/a><\/p>\n\n<p>The data are available as a PostgreSQL snapshot, and I would like to access the database using Python's sqlalchemy package within a Jupyter notebook within Amazon SageMaker.<\/p>\n\n<p>I tried to set up my database connection with the code below, but I'm getting a Connection timed out error. I'm pretty new to AWS and Sagemaker, so maybe I messed up my sqlalchemy engine? I think my VPC security settings are OK (it looks like they accept inbound and outbound requests).<\/p>\n\n<p>Any ideas what I could be missing?<\/p>\n\n<p>engine = create_engine(\u2018postgresql:\/\/root:password@[my endpoint]\/[DB instance]<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/v9nn5.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/v9nn5.png\" alt=\"connection timed out\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/W2G7R.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/W2G7R.png\" alt=\"VPC inbound settings\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/JZ6AI.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JZ6AI.png\" alt=\"VPC outbound settings\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1514495321563,
        "Question_score":1,
        "Question_tags":"python-3.x|postgresql|amazon-web-services|sqlalchemy|amazon-sagemaker",
        "Question_view_count":1528,
        "Owner_creation_time":1495670905787,
        "Owner_last_access_time":1545942227160,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48014413",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66107303,
        "Question_title":"SageMaker Experiments: UnpicklingError: invalid load key, '\\x1f'",
        "Question_body":"<p>I am new to Data Science and have been trying to use SageMaker Experiments to create an extremely simple model. Using SageMaker Experiments, I trained a model using a CSV dataset. The model was output to S3, and I am now trying to load that model into a Jupyter Notebook and run a batch transform test on it.<\/p>\n<p>However, I am getting an error when trying to use the following block of code provided in the SageMaker docs that is supposed to help me import the model. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">source<\/a><\/p>\n<pre><code>import pickle as pkl\nimport tarfile\nimport xgboost\n\nt = tarfile.open('model.tar.gz', 'r:gz')\nt.extractall()\n\nmodel = pkl.load(open(model_file_path, 'rb'))\n<\/code><\/pre>\n<p>The error is occurring at pkl.load:<\/p>\n<pre><code> ---------------------------------------------------------------------------\nUnpicklingError                           Traceback (most recent call last)\n&lt;ipython-input-39-81639df86023&gt; in &lt;module&gt;\n      6 t.extractall()\n      7 \n----&gt; 8 model = pkl.load(open(model_file_path, 'rb'))\n      9 \n     10 \n\nUnpicklingError: invalid load key, '\\x1f'.\n<\/code><\/pre>\n<p><strong>Note:<\/strong> model_file_path is 'model.tar.gz'<\/p>\n<p>Because I am so new at this, I do not know if it is an error with what I have done, or if there is something about how SageMaker Experiments does something that I am missing. I have tried referencing other stackoverflow posts that contain this error, but they don't seem to directly apply (to my understanding) to my error. (<a href=\"https:\/\/stackoverflow.com\/questions\/33049688\/what-causes-the-error-pickle-unpicklingerror-invalid-load-key\">this<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/45121556\/unable-to-load-cifar-10-dataset-invalid-load-key-x1f\">this<\/a>)<\/p>\n<p>Any expert advice on SageMaker Experiments or the SageMaker v2 SDK would be extremely helpful to this newbie trying to break into the space. Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1612808946247,
        "Question_score":1,
        "Question_tags":"python|machine-learning|pickle|xgboost|amazon-sagemaker",
        "Question_view_count":371,
        "Owner_creation_time":1512874902863,
        "Owner_last_access_time":1618539419697,
        "Owner_location":"Salt Lake City, UT, United States",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66107303",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63305569,
        "Question_title":"How to mount an EFS volume on AWS Sagemaker Studio",
        "Question_body":"<p>I have tried to follow the normal (non-studio) documentation on mounting an EFS file system, as can be found <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/mount-an-efs-file-system-to-an-amazon-sagemaker-notebook-with-lifecycle-configurations\/\" rel=\"nofollow noreferrer\">here<\/a>, however, these steps don't work in a studio notebook. Specifically, the <code>sudo mount -t nfs ...<\/code> does not work in both the Image terminal and the system terminal.<\/p>\n<p>How do I mount an EFS file system that already exists to amazon Sagemaker, so I can access the data\/ datasets I stored in them?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596816839977,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":2331,
        "Owner_creation_time":1483370766803,
        "Owner_last_access_time":1664056213153,
        "Owner_location":"London, UK",
        "Owner_reputation":15819,
        "Owner_up_votes":827,
        "Owner_down_votes":21,
        "Owner_views":1395,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Update: I spoke to an AWS Solutions Architect, and he confirms that EFS is not supported on Sagemaker Studio.<\/p>\n<hr \/>\n<p><strong>Workaround:<\/strong><\/p>\n<p>Instead of mounting your old EFS, you can mount the SageMaker studio EFS onto an EC2 instance, and copy over the data manually. You would need the correct EFS storage volume id, and you'll find your newly copied data available in Sagemaker Studio. <em>I have not actually done this though.<\/em><\/p>\n<p>To find the EFS id, look at the section &quot;Manage your storage volume&quot; <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tasks.html#manage-your-storage-volume\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1596816839976,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1597991730412,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63305569",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70681074,
        "Question_title":"Amazon SageMaker could not get a response from the endpoint",
        "Question_body":"<p>I have built an anomaly detection model using AWS SageMaker inbuilt model: random cut forest.<\/p>\n<pre><code>    rcf = RandomCutForest(\n    role=execution_role,\n    instance_count=1,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    num_samples_per_tree=1000,\n    num_trees=100,\n    encrypt_inter_container_traffic=True,\n    enable_network_isolation=True,\n    enable_sagemaker_metrics=True)\n<\/code><\/pre>\n<p>and created the endpoint:-<\/p>\n<pre><code>    rcf_inference = rcf.deploy(\n              initial_instance_count=4, instance_type=&quot;ml.m5.xlarge&quot;,\n              endpoint_name='RCF-container2',\n              enable_network_isolation=True)\n<\/code><\/pre>\n<p>But when I tried to get the prediction using the endpoint I am running into the following error:-<\/p>\n<pre><code>    results = rcf_inference.predict(df.values)\n\n    ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from model with message &quot;Amazon SageMaker could not get a response from the RCF-container2 endpoint. This can occur when CPU or memory utilization is high. To check your utilization, see Amazon CloudWatch. To fix this problem, use an instance type with more CPU capacity or memory.&quot;\n<\/code><\/pre>\n<p>I have tried with larger cpu instance but still I am getting the same issue. I guess the issue is functional.<\/p>\n<p>Please help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641988225017,
        "Question_score":0,
        "Question_tags":"python|machine-learning|amazon-sagemaker",
        "Question_view_count":361,
        "Owner_creation_time":1531640434317,
        "Owner_last_access_time":1655701655513,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":205,
        "Owner_up_votes":33,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70681074",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73817379,
        "Question_title":"How can I preprocess inputs sent to a hugging face estimator?",
        "Question_body":"<p>I've been reviewing tutorials like the one found <a href=\"https:\/\/github.com\/huggingface\/notebooks\/blob\/main\/sagemaker\/02_getting_started_tensorflow\/sagemaker-notebook.ipynb\" rel=\"nofollow noreferrer\">here<\/a> which detail how to train a huggingface estimator (specifically, a transformer model) and then deploy it to sagemaker.<\/p>\n<p>The tutorial linked above trains an estimator via:<\/p>\n<pre><code>huggingface_estimator = HuggingFace(entry_point='train.py',\n                            source_dir='.\/scripts',\n                            base_job_name='huggingface-sdk-extension',\n                            instance_type='ml.p3.2xlarge',\n                            instance_count=1,\n                            transformers_version='4.4',\n                            pytorch_version='1.6',\n                            py_version='py37',\n                            role=role,\n                            hyperparameters = {'epochs': 1,\n                                               'train_batch_size': 32,\n                                               'model_name':'distilbert-base-uncased'\n                                                })\n<\/code><\/pre>\n<p>Where <code>train.py<\/code> outlines a training script which tokenizes input data, and then fine-tunes a transformer model on the training data. <em>Note: in this example, the training data is hard coded into <code>train.py<\/code>, but that is not the source of the issue I'm encountering.<\/em><\/p>\n<p>The model is fit using<\/p>\n<pre><code>huggingface_estimator.fit()\n<\/code><\/pre>\n<p>and is then deployed using<\/p>\n<pre><code>predictor = huggingface_estimator.deploy(1,&quot;ml.g4dn.xlarge&quot;)\n<\/code><\/pre>\n<p>But then this deployed model is used to make a prediction via:<\/p>\n<pre><code>sentiment_input= {&quot;inputs&quot; : &quot;I love using the new Inference DLC.&quot;}\npredictor.predict(sentiment_input)\n<\/code><\/pre>\n<p>The problem is that no one specifed <em>anywhere<\/em> how this input is to be preprocessed: the model does not work on raw text, the text must be tokenized. Even the official <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"nofollow noreferrer\">docs<\/a> for sagemaker don't seem to outline how preprocessing for a deployed model is handled.<\/p>\n<p>How can I specify a preprocessing step for my model?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663861214277,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|huggingface-transformers",
        "Question_view_count":8,
        "Owner_creation_time":1560785927300,
        "Owner_last_access_time":1664072042827,
        "Owner_location":"Ontario, Canada",
        "Owner_reputation":4613,
        "Owner_up_votes":1267,
        "Owner_down_votes":633,
        "Owner_views":401,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73817379",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73651368,
        "Question_title":"SageMaker Distributed Training in Local Mode (inside Notebook Instances)",
        "Question_body":"<p>I've been using SageMaker for a while and have performed several experiments already with distributed training. I am wondering if it is possible to test and run SageMaker distributed training in local mode (using SageMaker Notebook Instances)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662649911493,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":19,
        "Owner_creation_time":1662649653073,
        "Owner_last_access_time":1663012631407,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>No, not possible yet. <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">local mode<\/a> does not support the distributed training with <code>local_gpu<\/code>for Gzip compression, Pipe Mode, or manifest files for inputs<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1662652096572,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73651368",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72534515,
        "Question_title":"How to add sagemaker createApp to user profile executionrole?",
        "Question_body":"<p>I created a aws sagemaker user profile using terraform. I tried to launch the sagemaker studio from the user profile but was confronted with this error: <code>SageMaker is unable to use your associated ExecutionRole [arn:aws:iam::xxxxxxxxxxxx:role\/sagemaker-workshop-data-ml] to create app. Verify that your associated ExecutionRole has permission for 'sagemaker:CreateApp'<\/code>. The role has sagemaker full access policy attached to it, but that policy doesn't have the createApp permission which is weird. Are there any policies I can attach to the role with the sagemaker createApp permission, or do I need to attach a policy to the role through terraform?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1654618548583,
        "Question_score":1,
        "Question_tags":"amazon-web-services|terraform|amazon-sagemaker",
        "Question_view_count":199,
        "Owner_creation_time":1653511725307,
        "Owner_last_access_time":1663251784530,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72534515",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66865031,
        "Question_title":"Sagemaker Regression - ValueError: Cannot format input",
        "Question_body":"<p>I am new to SageMaker &amp; Python<\/p>\n<p>I am trying to get a simple regression model going on AWS using Jupyter Notebooks.\nI am using the Abalone date from the  UCI data repository.\nI would greatly appreciate some assistance or a link to help me in what to do.<\/p>\n<p>Everything looks fine until I try to run:<\/p>\n<pre><code>\nregression_linear = sagemaker.estimator.Estimator(\n    container,\n    role=sagemaker.get_execution_role(),\n    input_mode = &quot;File&quot;,\n    instance_count = 1,\n    instance_type='ml.m4.xlarge',\n    output_path=output_location,\n    sagemaker_session=sess\n    )\n\nregression_linear.set_hyperparameters(\n    feature_dim=8,\n    epochs=16,\n    wd=0.01,\n    loss=&quot;absolute_loss&quot;,\n    predictor_type=&quot;regressor&quot;,\n    normalize_data=True,\n    optimizer=&quot;adam&quot;,\n    mini_batch_size=100,\n    lr_scheduler_step=100,\n    lr_scheduler_factor=0.99,\n    lr_scheduler_minimum_lr=0.0001,\n    learning_rate=0.1,\n    )\n\nfrom time import gmtime, strftime\njob_name = &quot;DEMO-linear-learner-abalone-regression-&quot; + strftime(&quot;%H-%M-%S&quot;, gmtime())\nprint(&quot;Training job: &quot;, job_name)\n\nregression_linear.fit(inputs={&quot;train&quot;: train_data}, job_name=job_name)\n<\/code><\/pre>\n<p>Then I am getting the following error:<\/p>\n<pre><code>ValueError                                Traceback (most recent call last)\n&lt;ipython-input-101-82bd2950b590&gt; in &lt;module&gt;\n----&gt; 1 regression_linear.fit(inputs={&quot;train&quot;: train_data}, job_name=job_name)\n      2 \n      3 # , &quot;validation&quot;: test_data\n\nValueError: Cannot format input       age  sex  length  diameter  height  whole_weight  shucked_weight  \\\n449    18    0   0.565     0.455   0.150        0.8205          0.3650   \n1080    7    1   0.430     0.335   0.120        0.3970          0.1985   \n2310   13    0   0.435     0.350   0.110        0.3840          0.1430   \n3790   10    0   0.650     0.505   0.175        1.2075          0.5105   \n3609    9    0   0.555     0.405   0.120        0.9130          0.4585   \n...   ...  ...     ...       ...     ...           ...             ...   \n2145    9    0   0.415     0.325   0.115        0.3455          0.1405   \n3815    8   -1   0.460     0.340   0.100        0.3860          0.1805   \n3534    6   -1   0.400     0.315   0.090        0.3300          0.1510   \n2217   13    0   0.515     0.415   0.130        0.7640          0.2760   \n3041    9    1   0.575     0.470   0.150        0.9785          0.4505   \n\n      vicera_weight  shell_weight  \n449          0.1590        0.2600  \n1080         0.0865        0.1035  \n2310         0.1005        0.1250  \n3790         0.2620        0.3900  \n3609         0.1960        0.2065  \n...             ...           ...  \n2145         0.0765        0.1100  \n3815         0.0875        0.0965  \n3534         0.0680        0.0800  \n2217         0.1960        0.2500  \n3041         0.1960        0.2760  \n\n[2923 rows x 9 columns]. Expecting one of str, TrainingInput, file_input or FileSystemInput\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1617080401673,
        "Question_score":1,
        "Question_tags":"python|linear-regression|amazon-sagemaker",
        "Question_view_count":163,
        "Owner_creation_time":1552345114290,
        "Owner_last_access_time":1660874551103,
        "Owner_location":"Melbourne VIC, Australia",
        "Owner_reputation":672,
        "Owner_up_votes":69,
        "Owner_down_votes":18,
        "Owner_views":111,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66865031",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66912388,
        "Question_title":"Does AWS Sagemaker PySparkProcessor manage autoscaling?",
        "Question_body":"<p>I'm using Sagemaker to generate to do preprocessing and generate training data and I'm following the Sagemaker API documentation <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_processing.html#pysparkprocessor\" rel=\"nofollow noreferrer\">here<\/a>, but I don't see any way currently how to specify autoscaling within the EMR cluster. What should I include within the <code>configuration<\/code> argument that I pass to my spark_processor <code>run()<\/code> object? What shouldn't I include?<\/p>\n<p>I'm aware of the <a href=\"https:\/\/docs.aws.amazon.com\/emr\/latest\/ReleaseGuide\/emr-configure-apps.html\" rel=\"nofollow noreferrer\">this resource<\/a>, but it doesn't seem comprehensive.<\/p>\n<p>Below is my code; it is very much a &quot;work-in-progress&quot;, but I would like to know if someone could provide me with or point me to a resource that shows:<\/p>\n<ol>\n<li>Whether this PySparkProcessor object will manage autoscaling automatically. Should I put AutoScaling config within the <code>configuration<\/code> in the <code>run()<\/code> object?<\/li>\n<li>An example of the full config that I can pass to the <code>configuration<\/code> variable.<\/li>\n<\/ol>\n<p>Here's what I have so far for the configuration.<\/p>\n<pre><code>\nSPARK_CONFIG = \\\n    { &quot;Configurations&quot;: [\n          {   &quot;Classification&quot;: &quot;spark-env&quot;,\n              &quot;Configurations&quot;: [ {&quot;Classification&quot;: &quot;export&quot;} ] }\n        ] \n    }\n\nspark_processor = PySparkProcessor(\n    tags=TAGS,\n    role=IAM_ROLE,\n    instance_count=2,\n    py_version=&quot;py37&quot;,\n    volume_size_in_gb=30,\n    container_version=&quot;1&quot;,\n    framework_version=&quot;3.0&quot;,\n    network_config=sm_network,\n    max_runtime_in_seconds=1800,\n    instance_type=&quot;ml.m5.2xlarge&quot;,\n    base_job_name=EMR_CLUSTER_NAME,\n    sagemaker_session=sagemaker_session,\n)\n\nspark_processor.run(\n    configuration=SPARK_CONFIG,\n    submit_app=LOCAL_PYSPARK_SCRIPT_DIR,\n    spark_event_logs_s3_uri=&quot;s3:\/\/{BUCKET_NAME}\/{S3_PYSPARK_LOG_PREFIX}&quot;,\n)\n<\/code><\/pre>\n<p>I'm used to interacting via Python more directly with EMR for these types of tasks. Doing that allows me to specify the entire EMR cluster config at once--including applications, autoscaling, EMR default and autoscaling roles--and then adding the steps to the cluster once it's created; however, much of this config seems to be abstracted away, and I don't know what remains or needs to be specified, specifically regarding the following config variables:  <code>AutoScalingRole<\/code>, <code>Applications<\/code>, <code>VisibleToAllUsers<\/code>, <code>JobFlowRole<\/code>\/<code>ServiceRole<\/code> etc.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617316641580,
        "Question_score":0,
        "Question_tags":"amazon-web-services|pyspark|sdk|amazon-sagemaker",
        "Question_view_count":276,
        "Owner_creation_time":1401993171660,
        "Owner_last_access_time":1662665661517,
        "Owner_location":"Lehi, UT, USA",
        "Owner_reputation":443,
        "Owner_up_votes":977,
        "Owner_down_votes":5,
        "Owner_views":156,
        "Question_last_edit_time":1617381153807,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66912388",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49365900,
        "Question_title":"What's a better way to load a file using boto? (getting filename too long error)",
        "Question_body":"<p>So I'm trying to use <code>tf.contrib.learn.preprocessing.VocabularyProcessor.restore()<\/code> to restore a vocabulary file from an S3 bucket. First, I tried to get the path name to the bucket to use in <code>.restore()<\/code> and I kept getting 'object doesn't exist' error. Afterwards, upon further research, I found a method people use to load text files and JSON files and applied the same method here:<\/p>\n\n<pre><code>obj = s3.Object(BUCKET_NAME, KEY).get()['Body'].read()\nvocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(obj)\n<\/code><\/pre>\n\n<p>This worked for a while until the contents of file increased and eventually got a 'File name too long' error. Is there a better way to load and restore a file from an S3 bucket? <\/p>\n\n<p>By the way, I tested this out locally on my machine and it works perfectly fine since there it just needs to take the path to the file, not the entire contents of the file. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1521471311783,
        "Question_score":2,
        "Question_tags":"tensorflow|amazon-s3|boto|amazon-sagemaker",
        "Question_view_count":532,
        "Owner_creation_time":1521470035783,
        "Owner_last_access_time":1598556805913,
        "Owner_location":null,
        "Owner_reputation":210,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It looks like you\u2019re passing in the actual contents of the file as the file name?<\/p>\n\n<p>I think you\u2019ll need to download the object from S3 to a tmp file and pass the path to that file into restore.<\/p>\n\n<p>Try using the method here: <a href=\"http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/s3.html#S3.Object.download_file\" rel=\"nofollow noreferrer\">http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/s3.html#S3.Object.download_file<\/a><\/p>\n\n<p>Update:\nI went through the code here: <a href=\"https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/contrib\/learn\/python\/learn\/preprocessing\/text.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/contrib\/learn\/python\/learn\/preprocessing\/text.py<\/a> and it looks like this just saves a pickle so you can really easily just import pickle and call the following:<\/p>\n\n<pre><code>import pickle\nobj = s3.Object(BUCKET_NAME, KEY).get()['Body']\nvocab_processor = pickle.loads(obj.read())\n<\/code><\/pre>\n\n<p>Hopefully that works?<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1521485629849,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1522056862430,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49365900",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49665241,
        "Question_title":"How do I load python modules which are not available in Sagemaker?",
        "Question_body":"<p>I want to install spacy which is not available as part of the Sagemaker platform. How should can I pip install it?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1522908559597,
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":2578,
        "Owner_creation_time":1410972175307,
        "Owner_last_access_time":1663832955917,
        "Owner_location":null,
        "Owner_reputation":1124,
        "Owner_up_votes":156,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When creating you model, you can specify the requirements.txt as an environment variable. <\/p>\n\n<p>For Eg. <\/p>\n\n<pre><code>env = {\n    'SAGEMAKER_REQUIREMENTS': 'requirements.txt', # path relative to `source_dir` below.\n}\nsagemaker_model = TensorFlowModel(model_data = 's3:\/\/mybucket\/modelTarFile,\n                                  role = role,\n                                  entry_point = 'entry.py',\n                                  code_location = 's3:\/\/mybucket\/runtime-code\/',\n                                  source_dir = 'src',\n                                  env = env,\n                                  name = 'model_name',\n                                  sagemaker_session = sagemaker_session,\n                                 )\n<\/code><\/pre>\n\n<p>This would ensure that the requirements file is run after the docker container is created, before running any code on it. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1522941835116,
        "Answer_score":10.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1522945323343,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49665241",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71909360,
        "Question_title":"Reading multiple csv files in AWS Sagemaker from a location in Amazon S3 Bucket",
        "Question_body":"<p>I have multiple csv files in a location in S3. The name of those files is in a date format. Example: 2021_09_30_Output.csv<\/p>\n<p>I need to understand how I can read all the files in this folder while selecting only the dates that I require. An example would be reading only the files from September. ie: &quot;2022_09_*.csv&quot; which would read only the files from that month<\/p>\n<p>Would appreciate the help. Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650270357780,
        "Question_score":0,
        "Question_tags":"pandas|amazon-web-services|for-loop|amazon-s3|amazon-sagemaker",
        "Question_view_count":407,
        "Owner_creation_time":1607598608387,
        "Owner_last_access_time":1658981623800,
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":15,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71909360",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53809556,
        "Question_title":"Load csv into S3 from local",
        "Question_body":"<p>I am exploring AWS sagemaker for ML. I have created a bucket:<\/p>\n\n<pre><code>bucket_name = 'test-bucket' \ns3 = boto3.resource('s3')\ntry:\n   if  my_region == 'us-east-1':\n      s3.create_bucket(Bucket=bucket_name)\n   else: \n      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\nprint('S3 bucket created successfully')\nexcept Exception as e:\n    print('S3 error: ',e)\n<\/code><\/pre>\n\n<p>I have a csv in my local and I want to load that into the bucket I created.<\/p>\n\n<p>All the links I have referred  have directions to load it from a link and unzip it. Is there a way to load the data into the bucket from the local.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1545025564273,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":1703,
        "Owner_creation_time":1455496483357,
        "Owner_last_access_time":1642546968190,
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":3912,
        "Owner_up_votes":135,
        "Owner_down_votes":47,
        "Owner_views":311,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If you are using Amazon SageMaker you can use the SageMaker python library that is implementing the most useful commands for data scientists, including the upload of files to S3. It is already installed on your SageMaker notebook instance by default. <\/p>\n\n<pre><code>import sagemaker\nsess = sagemaker.Session()\n\n# Uploading the local file to S3\nsess.upload_data(path='local-file.txt', bucket=bucket_name, key_prefix='input')    \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1545428602016,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53809556",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65811448,
        "Question_title":"How to observe and control how sagemaker multimodel server loads models in memory",
        "Question_body":"<p>I am evaluating SageMaker Multi Model Server (MMS) as an option to host large number of models for inference. I have successfully built the container according to the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">SageMaker BYOC MMS<\/a> instruction. I can invoke inference and the models work fine on SageMaker.<\/p>\n<p>I run my tests on the smallest instance type available <code>ml.t2.medium<\/code>. The MMS is described as downloading models from S3, loading them to container, and loading the models to memory as needed. Then offloading from memory when low on memory.<\/p>\n<p>In my experiment the MMS constantly reports the CloudWatch metric of <code>LoadedModelCount<\/code> at around 8-10. Even if I run inference on much larger set of models. If I keep the number of models invoked small, the inference call takes about 0.1 seconds. If I go over the <code>LoadedModelCount<\/code>, the inference time goes up to about 2s.<\/p>\n<p>So my guess is that the SageMaker MMS is unloading models from memory, and loading new models into memory, basically memory-swapping constantly. I put logging into my MMS model handler to show that it keeps initializing the handler for different models over and over when this happens.<\/p>\n<p>Also the CloudWatch metric <code>DiskUtilization<\/code> keeps going up with more models invoked, which I expect means it loads the models from S3 into container disk. The other metrics (memory and loaded models) on the other hand plateau after the 8-10 loaded models, with only minor changes up and down. Which further seems to support this theory that it swaps constantly from container disk to memory.<\/p>\n<p>I cannot find a way to see when MMS is actually unloading a model from memory, or when it loads a different one. Also, I cannot see what threshold is it using to unload models, as the CloudWatch <code>MemoryUtilization<\/code> metric from the SageMaker instance never goes above 45, which I guess means 45% of memory is used at most. This seems like a very low threshold, so I would expect to find a way to configure it, but have not found it.<\/p>\n<p>Question 1: How can I observe when MMS is unloading models from memory, and loading new ones?<\/p>\n<p>Question 2: How can I control the memory thresholds (or whatever MMS uses) that define when to unload the models?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1611152754440,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|mms",
        "Question_view_count":197,
        "Owner_creation_time":1328263672940,
        "Owner_last_access_time":1661507244447,
        "Owner_location":null,
        "Owner_reputation":999,
        "Owner_up_votes":127,
        "Owner_down_votes":4,
        "Owner_views":70,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65811448",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73828700,
        "Question_title":"AWS SageMaker Canvas Model usage on Edge device in Python",
        "Question_body":"<p>This way I wanted to ask a question about AWS Sagemaker. I must confess that I'm quite a newbee to the subject and therefor I was very happy with the SageMaker Canvas app. It works really easy and gives me some nice results.<\/p>\n<p>First of all my model. I try to predict solar power production based on the time (dt), the AWS IoT Thingname (thingname), clouds percentage (clouds) and temperature (temp). I have a csv filled with data measured by IoT things<\/p>\n<p><code>clouds<\/code> + <code>temp<\/code> + <code>dt<\/code> + <code>thingname<\/code> =&gt; <code>import<\/code><\/p>\n<pre><code>dt,clouds,temp,import,thingname\n2022-08-30 07:45:00+02:00,1.0,0.1577,0.03,***\n2022-08-30 08:00:00+02:00,1.0,0.159,0.05,***\n2022-08-30 08:15:00+02:00,1.0,0.1603,0.06,***\n2022-08-30 08:30:00+02:00,1.0,0.16440000000000002,0.08,***\n2022-08-30 08:45:00+02:00,,,0.09,***\n2022-08-30 09:00:00+02:00,1.0,0.17,0.12,***\n2022-08-30 09:15:00+02:00,1.0,0.1747,0.13,***\n2022-08-30 09:30:00+02:00,1.0,0.1766,0.15,***\n2022-08-30 09:45:00+02:00,0.75,0.1809,0.18,***\n2022-08-30 10:00:00+02:00,1.0,0.1858,0.2,***\n2022-08-30 10:15:00+02:00,1.0,0.1888,0.21,***\n2022-08-30 10:30:00+02:00,0.75,0.1955,0.24,***\n<\/code><\/pre>\n<p>In AWS SageMaker canvas I upload the csv and build the model. All is very easy and when I use the predict tab I upload a CSV where the import column is missing and containing API weather data for some future moment:<\/p>\n<pre><code>dt,thingname,temp,clouds\n2022-09-21 10:15:00+02:00,***,0.1235,1.0\n2022-09-21 10:30:00+02:00,***,0.1235,1.0\n2022-09-21 10:45:00+02:00,***,0.1235,1.0\n2022-09-21 11:00:00+02:00,***,0.1235,1.0\n2022-09-21 11:15:00+02:00,***,0.12689999999999999,0.86\n2022-09-21 11:30:00+02:00,***,0.12689999999999999,0.86\n2022-09-21 11:45:00+02:00,***,0.12689999999999999,0.86\n2022-09-21 12:00:00+02:00,***,0.12689999999999999,0.86\n2022-09-21 12:15:00+02:00,***,0.1351,0.69\n2022-09-21 12:30:00+02:00,***,0.1351,0.69\n2022-09-21 12:45:00+02:00,***,0.1351,0.69\n<\/code><\/pre>\n<p>From this data SageMaker Canvas predicts some real realistic numbers, from which I assume the model is nicely build. So I want to move this model to my Greengrass Core Device to do predictions on site. I found the best model location using the sharing link to the Junyper notebook.<\/p>\n<p>From reading in the AWS docs I seem to have a few options to run the model on an edge device:<\/p>\n<ul>\n<li>Run the Greengrass SageMaker Edge component and run the model as a component and write an inference component<\/li>\n<li>Run the SageMaker Edge Agent yourself<\/li>\n<li>Just download the model yourself and do your thing with it on the device<\/li>\n<\/ul>\n<p>Now it seems that SageMaker used XGBoost to create the model and I found the <code>xgboost-model<\/code> file and downloaded it to the device.<\/p>\n<p>But here is where the trouble started:\nSageMaker Canvas never gives any info on what it does with the CSV to format it, so I have really no clue on how to make a prediction using the model.\nI get some results when I try to open the same csv file I used for the Canvas prediction, but the data is completely different and not realistic at all<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># pip install xgboost==1.6.2\nimport xgboost as xgb\n\nfilename = f'solar-prediction-data.csv'\ndpredict = xgb.DMatrix(f'{filename}?format=csv')\nmodel = xgb.Booster()\nmodel.load_model('xgboost-model')\nresult = model.predict(dpredict)\nprint('Prediction result::')\nprint(result)\n<\/code><\/pre>\n<p>I read that the column order matters, the CSV may not contain a header.  But it does not get close to the SageMaker Canvas result.<\/p>\n<p>I also tried using <code>pandas<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># pip install xgboost==1.6.2\nimport xgboost as xgb\nimport pandas as pd\n\nfilename = f'solar-prediction-data.csv'\ndf = pd.read_csv(filename, index_col=None, header=None)\n\ndpredict = xgb.DMatrix(df, enable_categorical=True)\n\nmodel = xgb.Booster()\nmodel.load_model('xgboost-model')\nresult = model.predict(dpredict, pred_interactions=True)\nprint('Prediction result::')\nprint('===============')\nprint(result)\n<\/code><\/pre>\n<p>But this last one always gives me following error:<\/p>\n<pre><code>ValueError: DataFrame.dtypes for data must be int, float, bool or category.  When\ncategorical type is supplied, DMatrix parameter `enable_categorical` must\nbe set to `True`. Invalid columns:dt, thingname\n<\/code><\/pre>\n<p>To be honest, I'm completely stuck and hope someone around here can give me some advice or clue on how I can proceed.<\/p>\n<p>Thanks!\nKind regards<\/p>\n<p>Hacor<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663940700760,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|aws-iot|aws-iot-core|aws-iot-greengrass",
        "Question_view_count":18,
        "Owner_creation_time":1455091044887,
        "Owner_last_access_time":1663955705747,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73828700",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69747506,
        "Question_title":"Amazon SageMaker Model Monitor for Batch Transform jobs",
        "Question_body":"<p>Couldn't find the right place to ask this, so doing it here.<\/p>\n<p>Does Model Monitor support monitoring Batch Transform jobs, or only endpoints? The documentation seems to only reference endpoints...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1635387455380,
        "Question_score":3,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":572,
        "Owner_creation_time":1475895512717,
        "Owner_last_access_time":1662166636447,
        "Owner_location":"Melbourne VIC, Australia",
        "Owner_reputation":2304,
        "Owner_up_votes":277,
        "Owner_down_votes":33,
        "Owner_views":166,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69747506",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56353814,
        "Question_title":"Batch transform job results in \"InternalServerError\" with data file >100MB",
        "Question_body":"<p>I'm using Sagemaker in order to perform binary classification on time series, each sample being a numpy array of shape [24,11] (24h, 11features). I used a tensorflow model in script mode, my script being very similar to the one I used as reference:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py<\/a><\/p>\n\n<p>The training reported success and I was able to deploy a model for batch transformation. The transform job works fine when I input just a few samples (say, [10,24,11]), but it returns an <code>InternalServerError<\/code> when I input more samples for prediction (for example, [30000, 24, 11], which size is >100MB).<\/p>\n\n<p>Here is the error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-6-0c46f7563389&gt; in &lt;module&gt;()\n     32 \n     33 # Then wait until transform job is completed\n---&gt; 34 tf_transformer.wait()\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    133     def wait(self):\n    134         self._ensure_last_transform_job()\n--&gt; 135         self.latest_transform_job.wait()\n    136 \n    137     def _ensure_last_transform_job(self):\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    207 \n    208     def wait(self):\n--&gt; 209         self.sagemaker_session.wait_for_transform_job(self.job_name)\n    210 \n    211     @staticmethod\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_transform_job(self, job, poll)\n    893         \"\"\"\n    894         desc = _wait_until(lambda: _transform_job_status(self.sagemaker_client, job), poll)\n--&gt; 895         self._check_job_status(job, desc, 'TransformJobStatus')\n    896         return desc\n    897 \n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n    915             reason = desc.get('FailureReason', '(No reason provided)')\n    916             job_type = status_key_name.replace('JobStatus', ' job')\n--&gt; 917             raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))\n    918 \n    919     def wait_for_endpoint(self, endpoint, poll=5):\n\nValueError: Error for Transform job Tensorflow-batch-transform-2019-05-29-02-56-00-477: Failed Reason: InternalServerError: We encountered an internal error.  Please try again.\n\n<\/code><\/pre>\n\n<p>I tried to use both SingleRecord and MultiRecord parameters when deploying the model but the result was the same, so I decided to keep MultiRecord. My transformer looks like that:<\/p>\n\n<pre><code>transformer = tf_estimator.transformer(\n    instance_count=1, \n    instance_type='ml.m4.xlarge',\n    max_payload = 100,\n    assemble_with = 'Line',\n    strategy='MultiRecord'\n)\n<\/code><\/pre>\n\n<p>At first I was using a json file as input for the transform job, and it threw the error : <\/p>\n\n<pre><code>Too much data for max payload size\n<\/code><\/pre>\n\n<p>So next I tried the jsonlines format (the .npy format is not supported as far as I understand), thinking that jsonlines could get split by Line and thus avoid the size error, but that's where I got the <code>InternalServerError<\/code>. Here is the related code:<\/p>\n\n<pre><code>#Convert test_x to jsonlines and save\ntest_x_list = test_x.tolist()\nfile_path ='data_cnn_test\/test_x.jsonl'\nfile_name='test_x.jsonl'\n\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)    \n\ninput_key = 'batch_transform_tf\/input\/{}'.format(file_name)\noutput_key = 'batch_transform_tf\/output'\ntest_input_location = 's3:\/\/{}\/{}'.format(bucket, input_key)\ntest_output_location = 's3:\/\/{}\/{}'.format(bucket, output_key)\n\ns3.upload_file(file_path, bucket, input_key)\n\n# Initialize the transformer object\ntf_transformer = sagemaker.transformer.Transformer(\n    base_transform_job_name='Tensorflow-batch-transform',\n    model_name='sagemaker-tensorflow-scriptmode-2019-05-29-02-46-36-162',\n    instance_count=1,\n    instance_type='ml.c4.2xlarge',\n    output_path=test_output_location,\n    assemble_with = 'Line'\n    )\n\n# Start the transform job\ntf_transformer.transform(test_input_location, content_type='application\/jsonlines', split_type='Line')\n<\/code><\/pre>\n\n<p>The list named test_x_list has a shape [30000, 24, 11], which corresponds to 30000 samples so I would like to return 30000 predictions.<\/p>\n\n<p>I suspect my jsonlines file isn't being split by Line and is of course too big to be processed in one batch, which throws the error, but I don't understand why it doesn't get split correctly. I am using the default output_fn and input_fn (I did not re-write those functions in my script).<\/p>\n\n<p>Any insight on what I could be doing wrong would be greatly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559108619707,
        "Question_score":1,
        "Question_tags":"python|tensorflow|amazon-sagemaker",
        "Question_view_count":1826,
        "Owner_creation_time":1559099281007,
        "Owner_last_access_time":1624868926130,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I assume this is a duplicate of this AWS Forum post: <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0<\/a><\/p>\n\n<p>Anyway, for completeness I'll answer here as well.<\/p>\n\n<p>The issue is that you are serializing your dataset incorrectly when converting it into jsonlines:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)   \n<\/code><\/pre>\n\n<p>What the above is doing is creating a very large single-line containing your full dataset which is too big for single inference call to consume.<\/p>\n\n<p>I suggest you change your code to make each line a single sample so that inference can take place on individual samples instead of the whole dataset:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    for sample in test_x_list:\n        writer.write(sample)\n<\/code><\/pre>\n\n<p>If one sample at a time is too slow you can also play around with the <code>max_concurrent_transforms<\/code>, <code>strategy<\/code>, and <code>max_payload<\/code> parameters to be able to batch the data as well as run concurrent transforms if your algorithm can run in parallel - also, of course, you can split the data into multiple files and run transformations with more than just one node. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html<\/a> for additional detail on what these parameters do.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1559329920120,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56353814",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65330580,
        "Question_title":"How to Deploy trained TensorFlow 2.0 models using Amazon SageMaker?",
        "Question_body":"<p>i am trying to deploy a custom trained tensorflow model using Amazon SageMaker. i have trained xlm roberta using tf 2.2.0 for multilingual sentiment analysis task.(please refer to this notebook : <a href=\"https:\/\/www.kaggle.com\/mobassir\/understanding-cross-lingual-models\" rel=\"nofollow noreferrer\">https:\/\/www.kaggle.com\/mobassir\/understanding-cross-lingual-models<\/a>)\nnow, using trained weight file of my model i am trying to deploy that in sagemaker, i was following this tutorial : <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/<\/a><\/p>\n<p>converted some keras code from there to tensorflow.keras for 2.2.0\nbut when i do : <strong>!ls export\/Servo\/1\/variables<\/strong> i can see that export as Savedmodel generating empty variables directory like this : <a href=\"https:\/\/github.com\/tensorflow\/models\/issues\/1988\" rel=\"nofollow noreferrer\">https:\/\/github.com\/tensorflow\/models\/issues\/1988<\/a><\/p>\n<p>i can't find any documentation help for tf 2.2.0 trained model deployment<\/p>\n<p>need example like this : <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/<\/a> for tf 2.x models and not keras<\/p>\n<p>even though <strong>!ls export\/Servo\/1\/variables<\/strong> shows empty directory but An endpoint was created successfully and now i am not sure if my model was deployed successfully or not because when i try to test the model deployment inside aws notebook by using predictor = sagemaker.tensorflow.model.TensorFlowPredictor(endpoint_name, sagemaker_session)\ni.e. predictor.predict(data) i get the following error message:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;{\n    &quot;error&quot;: &quot;Session was not created with a graph before Run()!&quot;\n}&quot;\n<\/code><\/pre>\n<p>related problem : <a href=\"https:\/\/stackoverflow.com\/questions\/46201109\/inference-error-with-tensorflow-c-on-ios-invalid-argument-session-was-not-c\">Inference error with TensorFlow C++ on iOS: &quot;Invalid argument: Session was not created with a graph before Run()!&quot;<\/a><\/p>\n<p>the code i tried can be found here : <a href=\"https:\/\/pastebin.com\/sGuTtnSD\" rel=\"nofollow noreferrer\">https:\/\/pastebin.com\/sGuTtnSD<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1608150035357,
        "Question_score":3,
        "Question_tags":"python-3.x|tensorflow|amazon-sagemaker",
        "Question_view_count":537,
        "Owner_creation_time":1487079300550,
        "Owner_last_access_time":1664083891510,
        "Owner_location":"Bangladesh",
        "Owner_reputation":227,
        "Owner_up_votes":49,
        "Owner_down_votes":1,
        "Owner_views":305,
        "Question_last_edit_time":1608152033676,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65330580",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63679503,
        "Question_title":"Why did it take so long to create endpoint with AWS Sagemaker using Boto3?",
        "Question_body":"<p>It took 45 minutes to create my endpoint from the stored endpoint configuration. (I tested it and it works too). This is the first time that I've used boto3 to do this, whereas previously I just used the Sagemaker web GUI to create an endpoint from endpoint configuration.  Suggestions to my code are appreciated:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nsagemaker_client = boto3.client('sagemaker')\n\nresponse = sagemaker_client.create_endpoint(\n    EndpointName='sagemaker-tensorflow-x',\n    EndpointConfigName='sagemaker-tensorflow-x'\n)\n<\/code><\/pre>\n<p>Note: I've replaced the last part of my endpoint name with <code>x<\/code>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1598917151350,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|tensorflow|boto3|amazon-sagemaker",
        "Question_view_count":373,
        "Owner_creation_time":1347733578130,
        "Owner_last_access_time":1664081922597,
        "Owner_location":"Chicago, IL, United States",
        "Owner_reputation":16557,
        "Owner_up_votes":2087,
        "Owner_down_votes":42,
        "Owner_views":461,
        "Question_last_edit_time":1598923606240,
        "Answer_body":"<p>AWS has currently <a href=\"https:\/\/status.aws.amazon.com\/\" rel=\"nofollow noreferrer\">issues<\/a> with Sagemaker:<\/p>\n<blockquote>\n<p>Increased Error Rates and Latencies for Multiple API operations<\/p>\n<\/blockquote>\n<blockquote>\n<p>5:33 PM PDT We are investigating increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. Previously created jobs and endpoints are unaffected.<\/p>\n<\/blockquote>\n<blockquote>\n<p>6:04 PM PDT We are continuing to investigate increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. Previously created jobs and endpoints are unaffected.<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rQHQC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rQHQC.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1598923278260,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63679503",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67118613,
        "Question_title":"Read compressed CSV (gzip) file from AWS S3 into Panda data frame in Sagemaker",
        "Question_body":"<p>I am trying to read a large compressed CSV file from AWS S3 and convert it to a Panda data frame in\u00a0Sagemaker. Is there any direct and clean approach to do it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618542767620,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|gzip|amazon-sagemaker",
        "Question_view_count":1115,
        "Owner_creation_time":1524603494497,
        "Owner_last_access_time":1663090323520,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67118613",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62931618,
        "Question_title":"use global variables in AWS Sagemaker script",
        "Question_body":"<p>After having correctly deployed our model, I need to invoke it via lambda function. The script features two cleaning function, the first one (cleaning()) gives us 5 variables: the cleaned dataset and 4 other variables (scaler, monthdummies, compadummies, parceldummies) that we need to use in the second cleaning function (cleaning_test()).<\/p>\n<p>The reason behind this is that in the use case I'll have only one instance at a time to perform predictions on, not an entire dataset. This means that I pass the row to the first cleaning() function since some commands won't work. I can't also use a scaler and neither create dummy variables, so the aim is to import the scaler and some dummies used in the cleaning() function, since they come from the whole dataset, that I used to train the model.<\/p>\n<p>Hence, in the input_fn() function, the input needs to be cleaned using the cleaning_test() function, that requires the scaler and the three lists of dummies from the cleaning() one.<\/p>\n<p>When I train the model, the cleaning() function works fine, but after the deployment, if we invoke the endpoint, it raises the error that variable &quot;scaler&quot; is not defined.<\/p>\n<p>Below is the script.py:\nNote that the test is # since I've already tested it, so now I'm training on the whole dataset and I want to predict completely new instances<\/p>\n<pre><code>def cleaning(data):\n    some cleaning on data stored in s3\n    return cleaned_data, scaler, monthdummies, compadummies, parceldummies\n\ndef cleaning_test(data, scaler, monthdummies, compadummies, parceldummies):\n    cleaning on data without labels\n    return cleaned_data\n\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return clf\n\n\n\ndef input_fn(request_body, request_content_type):\n    if request_content_type == &quot;application\/json&quot;:\n        data = json.loads(request_body)\n        df = pd.DataFrame(data, index = [0])\n        input_data = cleaning_test(df, scaler, monthdummies, compadummies, parceldummies)\n    else:\n        pass\n    return input_data\n\n        \ndef predict_fn(input_data, model):\n    return model.predict_proba(input_data)\n\nif __name__ =='__main__':\n\n    print('extracting arguments')\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--n_estimators', type=int, default=10)\n    parser.add_argument('--min-samples-leaf', type=int, default=3)\n\n    # Data, model, and output directories\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    #parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--train-file', type=str, default='fp_train.csv')\n    #parser.add_argument('--test-file', type=str, default='fp_test.csv')\n\n    args, _ = parser.parse_known_args()\n\n    print('reading data')\n    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n    #test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n    \n    \n    print(&quot;cleaning&quot;)\n    train_df, scaler, monthdummies, compadummies, parceldummies = cleaning(train_df)\n    #test_df, scaler1, monthdummies1, compadummies1, parceldummies1 = cleaning(test_df)\n    \n    print(&quot;splitting&quot;)\n    y = train_df.loc[:,&quot;event&quot;]\n    X = train_df.loc[:, train_df.columns != 'event']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n\n    &quot;&quot;&quot;print('building training and testing datasets')\n    X_train = train_df.loc[:, train_df.columns != 'event']\n    X_test = test_df.loc[:, test_df.columns != 'event']\n    y_train = train_df.loc[:,&quot;event&quot;]\n    y_test = test_df.loc[:,&quot;event&quot;]&quot;&quot;&quot;\n    \n    print(X_train.columns)\n    print(X_test.columns)\n    \n\n\n    \n    # train\n    print('training model')\n    model = RandomForestClassifier(\n        n_estimators=args.n_estimators,\n        min_samples_leaf=args.min_samples_leaf,\n        n_jobs=-1)\n    \n    model.fit(X_train, y_train)\n\n    # print abs error\n    print('validating model')\n    proba = model.predict_proba(X_test)\n\n    \n    # persist model\n    path = os.path.join(args.model_dir, &quot;model.joblib&quot;)\n    joblib.dump(model, path)\n    print('model persisted at ' + path)\n\n<\/code><\/pre>\n<p>That I run through:<\/p>\n<pre><code>sklearn_estimator = SKLearn(\n    entry_point='script.py',\n    role = get_execution_role(),\n    train_instance_count=1,\n    train_instance_type='ml.c5.xlarge',\n    framework_version='0.20.0',\n    base_job_name='rf-scikit',\n    hyperparameters = {'n_estimators': 15})\n\nsklearn_estimator.fit({'train':trainpath})\n\nsklearn_estimator.latest_training_job.wait(logs='None')\nartifact = sm_boto3.describe_training_job(\n    TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n\n\npredictor = sklearn_estimator.deploy(\n    instance_type='ml.c5.large',\n    initial_instance_count=1)\n<\/code><\/pre>\n<p>The question is, how can I &quot;store&quot; the variables given by the cleaning() function during the training process, in order to use them in the input_fn() function, making cleaning_test() work fine?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1594891762437,
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":222,
        "Owner_creation_time":1541156154730,
        "Owner_last_access_time":1663934970577,
        "Owner_location":"Italia",
        "Owner_reputation":119,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62931618",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63650203,
        "Question_title":"Install Tensorflow Object Detection API without replacing existing Tensorflow package",
        "Question_body":"<p>I'm trying to build a custom container image based on AWS SageMaker 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-training:2.3.0-gpu-py37-cu102-ubuntu18.04 image and following the instructions in <a href=\"https:\/\/tensorflow-object-detection-api-tutorial.readthedocs.io\/en\/latest\/install.html#tensorflow-object-detection-api-installation\" rel=\"nofollow noreferrer\">https:\/\/tensorflow-object-detection-api-tutorial.readthedocs.io\/en\/latest\/install.html#tensorflow-object-detection-api-installation<\/a><\/p>\n<p>However, it seems that when I run the following commands, <code>pip install<\/code> replaces the already existing TensorFlow package with <code>tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl<\/code> which doesn't support AWS's CPU instructions and GPU devices.<\/p>\n<pre><code># From within TensorFlow\/models\/research\/\ncp object_detection\/packages\/tf2\/setup.py .\npython -m pip install .\n<\/code><\/pre>\n<p>How can I install the Object Detection API without replacing existing TensorFlow? I tried <code>python -m pip install --ignore-installed .<\/code> but it doesn't seem to have any effect.<\/p>\n<p><strong>Update 1:<\/strong><\/p>\n<p>It seems that the already installed <code>tensorflow<\/code> for that AWS docker image isn't detected by <code>pip<\/code> even though it's available in <code>\/usr\/local\/lib\/python3.7\/site-packages\/tensorflow<\/code>. This is why <code>pip<\/code> still attempts to install it even with <code>--ignore-installed<\/code>.<\/p>\n<p>As a workaround, I make a copy of the directory and then replace the newly installed one with it.<\/p>\n<pre><code>mv \/usr\/local\/lib\/python3.7\/site-packages\/tensorflow\/ \/usr\/local\/lib\/python3.7\/site-packages\/tensorflow.cpy\/\n# From within TensorFlow\/models\/research\/\ncp object_detection\/packages\/tf2\/setup.py .\npython -m pip install .\nrm -r \/usr\/local\/lib\/python3.7\/site-packages\/tensorflow\/\nmv \/usr\/local\/lib\/python3.7\/site-packages\/tensorflow.cpy\/ \/usr\/local\/lib\/python3.7\/site-packages\/tensorflow\/\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1598724180467,
        "Question_score":2,
        "Question_tags":"python|tensorflow|pip|amazon-sagemaker|object-detection-api",
        "Question_view_count":476,
        "Owner_creation_time":1412575569680,
        "Owner_last_access_time":1663923370567,
        "Owner_location":null,
        "Owner_reputation":679,
        "Owner_up_votes":242,
        "Owner_down_votes":11,
        "Owner_views":82,
        "Question_last_edit_time":1598753123360,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63650203",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56821807,
        "Question_title":"Cannot install the \"ipywidgets\" Jupyter Lab Extension on AWS sagemaker",
        "Question_body":"<p>To install Jupyter Lab Extension on AWS sagemaker, You need to follow <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts<\/a>. And then create the lifecycle configuration accordingly.<\/p>\n\n<p>I did it and this is my <code>on-start.sh<\/code> file.<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>#!\/bin\/bash\n\nset -e\n\n# OVERVIEW\n# This script installs a jupyterlab extension package in SageMaker Notebook Instance\n\nsudo -u ec2-user -i &lt;&lt;'EOF'\n# PARAMETERS\nEXTENSION_NAME=@jupyter-widgets\/jupyterlab-manager\nsource \/home\/ec2-user\/anaconda3\/bin\/activate JupyterSystemEnv\njupyter labextension install $EXTENSION_NAME\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\nEOF\n<\/code><\/pre>\n\n<p>Everything should went smooth except for this extension it raises an error.<\/p>\n\n<p>This is the error log from cloud watch.<\/p>\n\n<pre><code>\/bin\/bash: \/tmp\/OnStart_2019-06-26-23-3260vo0j6p: \/bin\/bash^M: bad interpreter: No such file or directory\n<\/code><\/pre>\n\n<p>This one is the error message shown in the sagemaker console.<\/p>\n\n<pre><code>Failure reason\nNotebook Instance Lifecycle Config 'arn:aws:sagemaker:ap-southeast-1:658055165324:notebook-instance-lifecycle-config\/jupyter-widgets-for-jupyterlab-copy' for Notebook Instance 'arn:aws:sagemaker:ap-southeast-1:658055165324:notebook-instance\/test' took longer than 5 minutes. Please check your CloudWatch logs for more details if your Notebook Instance has Internet access.\n<\/code><\/pre>\n\n<p>I had done several attempts to locate the bug in the script file and the setup file of <code>ipywidgets<\/code> concerning <a href=\"https:\/\/stackoverflow.com\/questions\/14219092\/bash-my-script-bin-bashm-bad-interpreter-no-such-file-or-directory\">the 'bad interpreter' error<\/a>. I cannot find any traces of error in both.<\/p>\n\n<p>I tried to upgrade my instance to T2 largest instance just in case the error came from the timeout. <\/p>\n\n<p>The weirdest thing is that I am able to install it via the terminal from the terminal on jupyterlab. I measured the total time it takes to install and found it is around <code>4 mins<\/code> just enough time(AWS should allow more time since this is only one extension install). Noted that this installation was performed under the T2 medium instance(the cheapest instance type you could get). If you install it this way to have to reboot the jupyter lab to make it work, then you reboot your instance and everything reverts back to the not-yet-install state. This suggests that there is no way to install jupyter lab extension rather than using the lifecycle cycle configurations which will lead you back to the error.<\/p>\n\n<p>At this point, I gave up and use the jupyter notebook instead if I really want to use the <code>ipywidgets<\/code>.<\/p>\n\n<hr>\n\n<p>Normally, this should be raised as technical support on AWS, but I have the basic plan so I decided to file it in StackOverflow for others that might encounter the same thing.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1561857335720,
        "Question_score":4,
        "Question_tags":"bash|amazon-web-services|amazon-sagemaker|jupyter-lab|ipywidgets",
        "Question_view_count":2151,
        "Owner_creation_time":1408765062060,
        "Owner_last_access_time":1663826817590,
        "Owner_location":"Bangkok, \u0e1b\u0e23\u0e30\u0e40\u0e17\u0e28\u0e44\u0e17\u0e22",
        "Owner_reputation":2620,
        "Owner_up_votes":240,
        "Owner_down_votes":10,
        "Owner_views":134,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56821807",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62866637,
        "Question_title":"The inference file that goes into the entry point of PyTorchModel to be deployed does not have an effect to the output of the predictor",
        "Question_body":"<p>I am currently running the code on AWS Sagemaker, trying to predict data using an already-trained model, accessed by MODEL_URL.<\/p>\n<p>With the code below, the inference.py as the entry_point does not seem to have an effect on the result of the trained prediction model. Any changes in inference.py does not alter the output (the output is always correct). Is there something I am misunderstanding with how the model works? And how can I incorporate inference.py to the prediction model as the entry point?<\/p>\n<pre><code>role = sagemaker.get_execution_role()\n\nmodel = PyTorchModel(model_data = MODEL_URL, \n                            role = role,\n                            framework_version = '0.4.0',\n                            entry_point = '\/inference.py',\n                            source_dir = SOURCE_DIR)\n\npredictor = model.deploy(instance_type = 'ml.c5.xlarge', \n                                   initial_instance_count = 1,\n                                   endpoint_name = RT_ENDPOINT_NAME)\n\nresult = predictor.predict(someData)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1594589308900,
        "Question_score":0,
        "Question_tags":"pytorch|prediction|amazon-sagemaker",
        "Question_view_count":265,
        "Owner_creation_time":1594588675633,
        "Owner_last_access_time":1615497791753,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62866637",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70217529,
        "Question_title":"Mount NVME drive of C5d instance in Amazon Sagemaker Studio",
        "Question_body":"<p>Is it possible to mount the NVME drive of an C5d instance in an Amazon Sagemaker Studio notebook? I can see the drive under 'lsblk' but am not allowed to format or mount it. The drive is also not visible in the '\/dev' folder. I tried this with a regular C5d EC2 instance and there it works without any issue.<\/p>\n<p>Edit: I tested it with Sagemaker Notebooks, not Studio, and it also works. It seems Studio doesn't have real access to the underlying infrastructure.<\/p>\n<p>Kind regards<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1638548468933,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":61,
        "Owner_creation_time":1495057266930,
        "Owner_last_access_time":1654516729713,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1638565418823,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70217529",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59717227,
        "Question_title":"AWS SageMaker - submit button is not working with custom template",
        "Question_body":"<p>When I create a new job on AWS SageMaker, using my custom template with crowd form (see attached sample) the SUBMIT button is not working and is not even clickable. Is there anyway to make this work? Haven`t see a good response on AWS support.<\/p>\n\n<pre><code>$('#submitButton').onclick = function() {\n   $('crowd-form').submit(); \n};\n\n\n &lt;body&gt;\n    &lt;h2 id=\"hit\"&gt;test&lt;\/h2&gt;\n        &lt;canvas id=\"canvas\" width=1210 height=687&gt;&lt;\/canvas&gt;    \n        &lt;crowd-button id=\"submitButton3\"&gt;Test button&lt;\/crowd-button&gt;\n\n    &lt;crowd-form&gt;\n\n        &lt;input type=\"hidden\" name=\"path0\" id=\"input0123\" value=\"{{task.input.metadata.images.path0}}\" \/&gt;\n        &lt;crowd-input label=\"Please input the character you see in the image\" max-length=\"1\" name=\"workerInput0\"&gt;&lt;\/crowd-input&gt;\n\n        &lt;crowd-button id=\"submitButto3223n\"&gt;Submit123&lt;\/crowd-button&gt;\n\n    &lt;\/div&gt;&lt;\/div&gt;\n\n    &lt;crowd-button id=\"submitButton\"&gt;Submit123&lt;\/crowd-button&gt;\n\n    &lt;\/crowd-form&gt;\n    &lt;crowd-button id=\"submitButton1\"&gt;Submit1232&lt;\/crowd-button&gt;\n\n    &lt;script src=\"http:\/\/code.jquery.com\/jquery-1.11.0.min.js\"&gt;&lt;\/script&gt;\n &lt;\/body&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1578920666227,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker|mechanicalturk",
        "Question_view_count":917,
        "Owner_creation_time":1544346330127,
        "Owner_last_access_time":1588231544867,
        "Owner_location":"Lviv, Lviv Oblast, Ukraine",
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":1578989573556,
        "Answer_body":"<p>There are few issues with you code snippet.<\/p>\n<p>Here are the links to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-ui-template-reference.html\" rel=\"nofollow noreferrer\">SageMaker's HTML Reference<\/a> and <a href=\"https:\/\/awsfeed.com\/whats-new\/machine-learning\/build-a-custom-data-labeling-workflow-with-amazon-sagemaker-ground-truth\/\" rel=\"nofollow noreferrer\">Example for building custom Labeling template<\/a><\/p>\n<p>First remove all those submit buttons (<code>&lt;crowd-button&gt;<\/code> elements) and the <code>onClick<\/code> event handler. From here you have two options use default SageMaker submit button or create your own in the template.<\/p>\n<h2>Use SageMaker's Submit Button<\/h2>\n<p>Leave out submit buttons (<code>crowd-button<\/code>) and SageMaker will automatically append one inside <code>crowd-form<\/code>. According to documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-ui-template-crowd-form.html\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<h2>Use custom Submit Button<\/h2>\n<p>In this case you need to:<\/p>\n<ol>\n<li>Prevent SageMaker adding button by including <code>crowd-button<\/code> <strong>inside<\/strong> the <code>crowd-form<\/code> element and setting <code>style=&quot;display: none;<\/code><\/li>\n<li>Add your own Submit button elsewhere on the template and add <code>onclick<\/code> even handler that will execute <code>form.submit()<\/code><\/li>\n<\/ol>\n<p>Here is the working example of the template (taken from the Example mentioned above).<\/p>\n<pre><code>&lt;script src=&quot;https:\/\/assets.crowd.aws\/crowd-html-elements.js&quot;&gt;&lt;\/script&gt;\n\n&lt;link rel=&quot;stylesheet&quot; href=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/css\/1.3fc3007b.chunk.css&quot;&gt;\n&lt;link rel=&quot;stylesheet&quot; href=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/css\/main.9504782e.chunk.css&quot;&gt;\n\n&lt;div id='document-text' style=&quot;display: none;&quot;&gt;\n  {{ task.input.text }}\n&lt;\/div&gt;\n&lt;div id='document-image' style=&quot;display: none;&quot;&gt;\n        {{ task.input.taskObject | grant_read_access }}\n&lt;\/div&gt;\n&lt;div id=&quot;metadata&quot; style=&quot;display: none;&quot;&gt;\n  {{ task.input.metadata }}\n&lt;\/div&gt;\n\n&lt;crowd-form&gt;\n    &lt;input name=&quot;annotations&quot; id=&quot;annotations&quot; type=&quot;hidden&quot;&gt;\n\n     &lt;!-- Prevent crowd-form from creating its own button --&gt;\n    &lt;crowd-button form-action=&quot;submit&quot; style=&quot;display: none;&quot;&gt;&lt;\/crowd-button&gt;\n&lt;\/crowd-form&gt;\n\n&lt;!-- Custom annotation user interface is rendered here --&gt;\n&lt;div id=&quot;root&quot;&gt;&lt;\/div&gt;\n\n&lt;crowd-button id=&quot;submitButton&quot;&gt;Submit&lt;\/crowd-button&gt;\n\n&lt;script&gt;\n    document.querySelector('crowd-form').onsubmit = function() {\n        document.getElementById('annotations').value = JSON.stringify(JSON.parse(document.querySelector('pre').innerText));\n    };\n\n    document.getElementById('submitButton').onclick = function() {\n        document.querySelector('crowd-form').submit();\n    };\n&lt;\/script&gt;\n\n&lt;script src=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/js\/1.3e5a6849.chunk.js&quot;&gt;&lt;\/script&gt;\n&lt;script src=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/js\/main.96e12312.chunk.js&quot;&gt;&lt;\/script&gt;\n&lt;script src=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/js\/runtime~main.229c360f.js&quot;&gt;&lt;\/script&gt;\n<\/code><\/pre>\n<p>Code source<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1579189368976,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1592644375060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59717227",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70512043,
        "Question_title":"ClientError: Failed to download data. Please check your s3 objects and ensure that there is no object that is both a folder as well as a file",
        "Question_body":"<p>How are you?<\/p>\n<p>I'm trying to execute a sagemaker job but i get this error:<\/p>\n<pre><code>ClientError: Failed to download data. Cannot download s3:\/\/pocaaml\/sagemaker\/xsell_sc1_test\/model\/model_lgb.tar.gz, a previously downloaded file\/folder clashes with it. Please check your s3 objects and ensure that there is no object that is both a folder as well as a file.\n<\/code><\/pre>\n<p>I'm have that model_lgb.tar.gz on that s3 path as you can see here:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/vAWfG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vAWfG.png\" alt=\"s3 bucket with model in it\" \/><\/a><\/p>\n<p>This is my code:<\/p>\n<pre><code>project_name = 'xsell_sc1_test'\ns3_bucket = &quot;pocaaml&quot;\nprefix = &quot;sagemaker\/&quot;+project_name\naccount_id = &quot;029294541817&quot;\ns3_bucket_base_uri = &quot;{}{}&quot;.format(&quot;s3:\/\/&quot;, s3_bucket)\ndev = &quot;dev-{}&quot;.format(strftime(&quot;%y-%m-%d-%H-%M&quot;, gmtime()))\n\nregion = sagemaker.Session().boto_region_name\nprint(&quot;Using AWS Region: {}&quot;.format(region))\n\n# Get a SageMaker-compatible role used by this Notebook Instance.\nrole = get_execution_role()\n\nboto3.setup_default_session(region_name=region)\n\nboto_session = boto3.Session(region_name=region)\n\ns3_client = boto3.client(&quot;s3&quot;, region_name=region)\n\nsagemaker_boto_client = boto_session.client(&quot;sagemaker&quot;) #este pinta?\n\nsagemaker_session = sagemaker.session.Session(\n    boto_session=boto_session, sagemaker_client=sagemaker_boto_client\n)\n\nsklearn_processor = SKLearnProcessor(\n    framework_version=&quot;0.23-1&quot;, role=role, instance_type='ml.m5.4xlarge', instance_count=1\n)\n\nPREPROCESSING_SCRIPT_LOCATION = 'funciones_altas.py'\n\npreprocessing_input_code = sagemaker_session.upload_data(\n    PREPROCESSING_SCRIPT_LOCATION,\n    bucket=s3_bucket,\n    key_prefix=&quot;{}\/{}&quot;.format(prefix, &quot;code&quot;)\n)\n\npreprocessing_input_data = &quot;{}\/{}\/{}&quot;.format(s3_bucket_base_uri, prefix, &quot;data&quot;)\npreprocessing_input_model = &quot;{}\/{}\/{}&quot;.format(s3_bucket_base_uri, prefix, &quot;model&quot;)\npreprocessing_output = &quot;{}\/{}\/{}\/{}\/{}&quot;.format(s3_bucket_base_uri, prefix, dev, &quot;preprocessing&quot; ,&quot;output&quot;)\n\nprocessing_job_name = params[&quot;project_name&quot;].replace(&quot;_&quot;, &quot;-&quot;)+&quot;-preprocess-{}&quot;.format(strftime(&quot;%d-%H-%M-%S&quot;, gmtime()))\n\nsklearn_processor.run(\n    code=preprocessing_input_code,\n    job_name = processing_job_name,\n    inputs=[ProcessingInput(input_name=&quot;data&quot;,\n                            source=preprocessing_input_data, \n                            destination=&quot;\/opt\/ml\/processing\/input\/data&quot;),\n           ProcessingInput(input_name=&quot;model&quot;,\n                           source=preprocessing_input_model, \n                           destination=&quot;\/opt\/ml\/processing\/input\/model&quot;)],\n    outputs=[\n        ProcessingOutput(output_name=&quot;output&quot;, \n                         destination=preprocessing_output,\n                         source=&quot;\/opt\/ml\/processing\/output&quot;)],\n    wait=False,\n)\n\npreprocessing_job_description = sklearn_processor.jobs[-1].describe()\n<\/code><\/pre>\n<p>and on funciones_altas.py i'm using ohe_altas.tar.gz and not model_lgb.tar.gz making this error super weird.<\/p>\n<p>can you help me?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640722245710,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|jobs|amazon-sagemaker",
        "Question_view_count":310,
        "Owner_creation_time":1577797879880,
        "Owner_last_access_time":1664036282340,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70512043",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51390606,
        "Question_title":"Create Amazon SageMaker Hyperparameter Tuning Job with Scikit Learn",
        "Question_body":"<p>I'm wondering how to automatically tune my scikit learn random forest model with Amazon Sagemaker. For now, I would like to tune a single hyperparameter called \"max_depth\". I'll dump my code first and express some concerns after.<\/p>\n\n<p>FILE: <code>notebook.ipynb<\/code><\/p>\n\n<pre><code>estimator = sagemaker.estimator.Estimator(image, role,\n              train_instance_count=1,\n              train_instance_type='ml.m4.xlarge',\n              output_path=output_location,\n              sagemaker_session=sagemaker_session,\n              )\n\nhyperparameter_ranges = {'max_depth': IntegerParameter(20, 30)}\nobjective_metric_name = 'score'\nmetric_definitions = [{'Name': 'score', 'Regex': 'score: ([0-9\\\\.]+)'}]\n\ntuner = HyperparameterTuner(estimator,\n                        objective_metric_name,\n                        hyperparameter_ranges,\n                        metric_definitions,\n                        max_jobs=9,\n                        max_parallel_jobs=3)\ntuner.fit({'train': train_data_location, 'test': test_data_location})\n<\/code><\/pre>\n\n<p>FILE: <code>train<\/code>  (located in docker container)<\/p>\n\n<pre><code>def train():\n    with open(param_path, 'r') as tc:\n        hyperparams = json.load(tc)\n    print(\"DEBUG VALUE: \", hyperparams)\n    data, class = get_data() #abstraction\n    X, y = train_data.drop(['class'], axis=1), train_data['class']\n    clf = RandomForestClassifier()\n    clf.fit(data, class)\n    print(\"score: \" + str(evaluate_model(clf)) + \"\\n\")\n<\/code><\/pre>\n\n<p>I see two issues with this code. First, If I put a json object {'max_value':2} in   a file named hyperparameters.json at the necessary path, the print statement outputs {} as if the file is empty. <\/p>\n\n<p>Issue number 2 is the fact that train() does not allow for hyperparameters to affect the code in any way shape or form. As far as I can tell, amazon has no documentation on the inner workings of the <code>tuner.fit()<\/code> method. This means I can't figure out how train() accesses the hyperparameters to test.<\/p>\n\n<p>Any help is appreciated, let me know if I can provide more code or clarify anything.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1531863719747,
        "Question_score":4,
        "Question_tags":"scikit-learn|amazon-sagemaker",
        "Question_view_count":1765,
        "Owner_creation_time":1531862612423,
        "Owner_last_access_time":1533254015850,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51390606",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57063741,
        "Question_title":"How to convert food-101 dataset into usable format for AWS SageMaker",
        "Question_body":"<p>I'm still very new to the world of machine learning and am looking for some guidance for how to continue a project that I've been working on. Right now I'm trying to feed in the Food-101 dataset into the Image Classification algorithm in SageMaker and later deploy this trained model onto an AWS deeplens to have food detection capabilities. Unfortunately the dataset comes with only the raw image files organized in sub folders as well as a .h5 file (not sure if I can just directly feed this file type into sageMaker?). From what I've gathered neither of these are suitable ways to feed in this dataset into SageMaker and I was wondering if anyone could help point me in the right direction of how I might be able to prepare the dataset properly for SageMaker i.e convert to a .rec or something else. Apologies if the scope of this question is very broad I am still a beginner to all of this and I'm simply stuck and do not know how to proceed so any help you guys might be able to provide would be fantastic. Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1563303140363,
        "Question_score":0,
        "Question_tags":"amazon-web-services|dataset|amazon-sagemaker",
        "Question_view_count":254,
        "Owner_creation_time":1563302872867,
        "Owner_last_access_time":1566222490660,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57063741",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55931620,
        "Question_title":"MXNetError - dataset does not start with a valid magic number",
        "Question_body":"<p>I am trying to use amazon sagemaker linear-learner algorithm, it support content type of \u2018application\/x-recordio-protobuf\u2019. In preprocessing phase, i used scikit-learn preprocessing to one-hot-encode my features. Then i use linear learner estimator to with record-io converted input data.<\/p>\n\n<p>I used package and the preprocess conversion was successful.<\/p>\n\n<p><code>from sagemaker.amazon.common import write_spmatrix_to_sparse_tensor<\/code><\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def output_fn(prediction, accept):\n    \"\"\"Format prediction output\n\n    The default accept\/content-type between containers for serial inference is JSON.\n    We also want to set the ContentType or mimetype as the same value as accept so the next\n    container can read the response payload correctly.\n    \"\"\"\n   if accept == 'text\/csv':\n        return worker.Response(encoders.encode(prediction.todense(), accept), mimetype=accept)\n    elif accept == 'application\/x-recordio-protobuf':\n        buf = BytesIO()\n        write_spmatrix_to_sparse_tensor(buf, prediction)\n        buf.seek(0)\n        return worker.Response(buf, accept, mimetype=accept)\n    else:\n        raise RuntimeError(\"{} accept type is not supported by this script.\".format(accept))\n<\/code><\/pre>\n\n<p>But when linear-learner takes the input record, it fails with the error below<\/p>\n\n<p>Caused by: [15:53:30] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsCppLibs\/AIAlgorithmsCppLibs-2.0.774.0\/AL2012\/generic-flavor\/src\/src\/aialgs\/io\/iterator_base.cpp:100:<\/p>\n\n<p><code>(Input Error) The header of the MXNet RecordIO record at position 810 in the dataset does not start with a valid magic number.<\/code><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_time":1556684928063,
        "Question_score":1,
        "Question_tags":"scikit-learn|protocol-buffers|linear-regression|mxnet|amazon-sagemaker",
        "Question_view_count":557,
        "Owner_creation_time":1375716022637,
        "Owner_last_access_time":1624974374273,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55931620",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71929127,
        "Question_title":"Install spark 3.2 in SageMaker Notebook",
        "Question_body":"<p>SageMaker supports pyspark 2.4 and I want to install latest version of pyspark in Sagemaker. Can you please let me know how can I install latest version of pyspark in SageMaker notebook.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650390083470,
        "Question_score":0,
        "Question_tags":"apache-spark|pyspark|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":475,
        "Owner_creation_time":1648649590150,
        "Owner_last_access_time":1663628695960,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71929127",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50985138,
        "Question_title":"Sagemaker Hyperparameter Optimization XGBoost",
        "Question_body":"<p>I am trying to build a hyperparameter optimization job in Amazon Sagemaker, in python, but something is not working. Here is what I have:<\/p>\n\n<pre><code>sess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n                                    role, \n                                    train_instance_count=1, \n                                    train_instance_type='ml.m4.4xlarge',\n                                    output_path=output_path_1,\n                                    base_job_name='HPO-xgb',\n                                    sagemaker_session=sess)\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter    \n\nhyperparameter_ranges = {'eta': ContinuousParameter(0.01, 0.2),\n                         'num_rounds': ContinuousParameter(100, 500),\n                         'num_class':  4,\n                         'max_depth': IntegerParameter(3, 9),\n                         'gamma': IntegerParameter(0, 5),\n                         'min_child_weight': IntegerParameter(2, 6),\n                         'subsample': ContinuousParameter(0.5, 0.9),\n                         'colsample_bytree': ContinuousParameter(0.5, 0.9)}\n\nobjective_metric_name = 'validation:mlogloss'\nobjective_type='minimize'\nmetric_definitions = [{'Name': 'validation-mlogloss',\n                       'Regex': 'validation-mlogloss=([0-9\\\\.]+)'}]\n\ntuner = HyperparameterTuner(xgb,\n                            objective_metric_name,\n                            objective_type,\n                            hyperparameter_ranges,\n                            metric_definitions,\n                            max_jobs=9,\n                            max_parallel_jobs=3)\n\ntuner.fit({'train': s3_input_train, 'validation': s3_input_validation}) \n<\/code><\/pre>\n\n<p>And the error I get is: <\/p>\n\n<pre><code>AttributeError: 'str' object has no attribute 'keys'\n<\/code><\/pre>\n\n<p>The error seems to come from the <code>tuner.py<\/code> file:<\/p>\n\n<pre><code>----&gt; 1 tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in fit(self, inputs, job_name, **kwargs)\n    144             self.estimator._prepare_for_training(job_name)\n    145 \n--&gt; 146         self._prepare_for_training(job_name=job_name)\n    147         self.latest_tuning_job = _TuningJob.start_new(self, inputs)\n    148 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in _prepare_for_training(self, job_name)\n    120 \n    121         self.static_hyperparameters = {to_str(k): to_str(v) for (k, v) in self.estimator.hyperparameters().items()}\n--&gt; 122         for hyperparameter_name in self._hyperparameter_ranges.keys():\n    123             self.static_hyperparameters.pop(hyperparameter_name, None)\n    124 \n\nAttributeError: 'list' object has no attribute 'keys'                           \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1529660522477,
        "Question_score":3,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1523,
        "Owner_creation_time":1432680790120,
        "Owner_last_access_time":1559548299510,
        "Owner_location":null,
        "Owner_reputation":455,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":58,
        "Question_last_edit_time":1530173488990,
        "Answer_body":"<p>Your arguments when initializing the HyperparameterTuner object are in the wrong order. The constructor has the following signature:<\/p>\n\n<pre><code>HyperparameterTuner(estimator, \n                    objective_metric_name, \n                    hyperparameter_ranges, \n                    metric_definitions=None, \n                    strategy='Bayesian', \n                    objective_type='Maximize', \n                    max_jobs=1, \n                    max_parallel_jobs=1, \n                    tags=None, \n                    base_tuning_job_name=None)\n<\/code><\/pre>\n\n<p>so in this case, your <code>objective_type<\/code> is in the wrong position. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/tuner.html#sagemaker.tuner.HyperparameterTuner\" rel=\"nofollow noreferrer\">the docs<\/a> for more details.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1530057890672,
        "Answer_score":5.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1531248961192,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50985138",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60953289,
        "Question_title":"SageMaker gives CannotStartContainerError although I specified an entrypoint",
        "Question_body":"<p>I want to train a custom ML model with SageMaker. The model is written in Python and should be shipped to SageMaker in a Docker image. Here is a simplified version of my Dockerfile (the model sits in the train.py file):<\/p>\n\n<pre><code>FROM amazonlinux:latest\n\n# Install Python 3\nRUN yum -y update &amp;&amp; yum install -y python3-pip python3-devel gcc &amp;&amp; yum clean all\n\n# Install sagemaker-containers (the official SageMaker utils package)\nRUN pip3 install --target=\/usr\/local\/lib\/python3.7\/site-packages sagemaker-containers &amp;&amp; rm -rf \/root\/.cache\n\n# Bring the script with the model to the image \nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n\n<p>Now, if I initialize this image as a SageMaker estimator and then run the <code>fit<\/code> method on this estimator I get the following error:<\/p>\n\n<p>\"AlgorithmError: CannotStartContainerError. Please make sure the container can be run with 'docker run  train'.\"<\/p>\n\n<p>In other words: SageMaker is not able to get into the container and run the train.py file. But why? The way I am specifying the entrypoint with <code>ENV SAGEMAKER_PROGRAM train.py<\/code> is recommended in the <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\/blob\/master\/README.rst\" rel=\"nofollow noreferrer\">docs of the sagemaker-containers package<\/a> (see 'How a script is executed inside the container').<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1585665709967,
        "Question_score":1,
        "Question_tags":"python|image|docker|amazon-sagemaker|amazon-linux-2",
        "Question_view_count":1133,
        "Owner_creation_time":1448655975827,
        "Owner_last_access_time":1663931632060,
        "Owner_location":null,
        "Owner_reputation":1478,
        "Owner_up_votes":653,
        "Owner_down_votes":1,
        "Owner_views":135,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I found a hint in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">the AWS docs<\/a> and came up with this solution:<\/p>\n\n<pre><code>ENTRYPOINT [\"python3.7\", \"\/opt\/ml\/code\/train.py\"]\n<\/code><\/pre>\n\n<p>With this the container <a href=\"https:\/\/docs.docker.com\/engine\/reference\/builder\/#entrypoint\" rel=\"nofollow noreferrer\">will run as an executable<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1585671760392,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60953289",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70465140,
        "Question_title":"I keep getting UnexpectedStatusException: Error for HyperParameterTuning job in AWS sagemaker",
        "Question_body":"<p>As mentioned in question, I keep getting UnexpectedStatusException: Error for HyperParameterTuning job xgboost-211***-1631: Failed. Reason: No training job succeeded after 5 attempts. For additional details, please take a look at the training job failures by listing training jobs for the hyperparameter tuning job.<\/p>\n<p>I looked into parameter ranges based on <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html<\/a> to make sure that ranges are good and they seem to be ok.  Data is definitely good because I can train the model but can't tune it.<\/p>\n<p>Here is the code I am using :<\/p>\n<pre><code>import sagemaker\nimport boto3\nimport numpy as np                                # For matrix operations and numerical processing\nimport pandas as pd                               # For munging tabular data\nimport os \nfrom sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\nfrom sagemaker.session import TrainingInput\nfrom sagemaker.debugger import Rule, rule_configs\n \nregion = boto3.Session().region_name    \nsmclient = boto3.Session().client('sagemaker')\n\nrole = sagemaker.get_execution_role()\n\n\n\ns3_output_location='s3:\/\/{}\/{}\/{}'.format(bucket, prefix, 'output')\ncontainer=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region, &quot;latest&quot;)\n\n\nxgb_model=sagemaker.estimator.Estimator(\n    image_uri=container,\n    role=role,\n    instance_count=1,\n    instance_type='ml.m4.xlarge',\n    volume_size=5,\n    output_path=s3_output_location,\n    sagemaker_session=sagemaker.Session(),\n    rules=[Rule.sagemaker(rule_configs.create_xgboost_report())]\n)\n\nxgb_model.set_hyperparameters(\n    max_depth = 5,\n    eta = 0.2,\n    gamma = 4,\n    min_child_weight = 6,\n    subsample = 0.7,\n    objective = &quot;binary:logistic&quot;,\n    num_round = 10\n)\n\nhyperparameter_ranges = {'eta': ContinuousParameter(0.1, 0.5),\n                        'min_child_weight': ContinuousParameter(1, 10),\n                        'alpha': ContinuousParameter(0, 3),\n                        'max_depth': IntegerParameter(0, 4)}\nobjective_metric_name = 'validation:auc'\n\n\ntuner = HyperparameterTuner(xgb_model,\n                            objective_metric_name,\n                            hyperparameter_ranges,\n                            max_jobs=60,\n                            max_parallel_jobs=6)\n\n\n\ntrain_input = TrainingInput(\n    &quot;s3:\/\/{}\/{}\/{}&quot;.format(bucket, prefix, &quot;train\/train.csv&quot;), content_type=&quot;csv&quot;\n)\nvalidation_input = TrainingInput(\n    &quot;s3:\/\/{}\/{}\/{}&quot;.format(bucket, prefix, &quot;validate\/validation.csv&quot;), content_type=&quot;csv&quot;\n)\n\ntuner.fit({&quot;train&quot;: train_input, &quot;validation&quot;: validation_input}, include_cls_metadata=False)\n<\/code><\/pre>\n<p>This is the error I get<\/p>\n<pre><code>&gt; --------------------------------------------------------------------------- UnexpectedStatusException                 Traceback (most recent call\n&gt; last) &lt;ipython-input-2-7824ad80a8bb&gt; in &lt;module&gt;\n&gt;      62 )\n&gt;      63 \n&gt; ---&gt; 64 tuner.fit({&quot;train&quot;: train_input, &quot;validation&quot;: validation_input}, include_cls_metadata=False)\n&gt; \n&gt; ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py\n&gt; in fit(self, inputs, job_name, include_cls_metadata, estimator_kwargs,\n&gt; wait, **kwargs)\n&gt;     449 \n&gt;     450         if wait:\n&gt; --&gt; 451             self.latest_tuning_job.wait()\n&gt;     452 \n&gt;     453     def _fit_with_estimator(self, inputs, job_name, include_cls_metadata, **kwargs):\n&gt; \n&gt; ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py\n&gt; in wait(self)    1595     def wait(self):    1596        \n&gt; &quot;&quot;&quot;Placeholder docstring.&quot;&quot;&quot;\n&gt; -&gt; 1597         self.sagemaker_session.wait_for_tuning_job(self.name)    1598     1599 \n&gt; \n&gt; ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py\n&gt; in wait_for_tuning_job(self, job, poll)    3253         &quot;&quot;&quot;    3254   \n&gt; desc = _wait_until(lambda: _tuning_job_status(self.sagemaker_client,\n&gt; job), poll)\n&gt; -&gt; 3255         self._check_job_status(job, desc, &quot;HyperParameterTuningJobStatus&quot;)    3256         return desc    3257 \n&gt; \n&gt; ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py\n&gt; in _check_job_status(self, job, desc, status_key_name)    3336        \n&gt; ),    3337                 allowed_statuses=[&quot;Completed&quot;, &quot;Stopped&quot;],\n&gt; -&gt; 3338                 actual_status=status,    3339             )    3340 \n&gt; \n&gt; UnexpectedStatusException: Error for HyperParameterTuning job\n&gt; xgboost-211XXX-1641: Failed. Reason: No training job succeeded after 5\n&gt; attempts. For additional details, please take a look at the training\n&gt; job failures by listing training jobs for the hyperparameter tuning\n&gt; job.\n<\/code><\/pre>\n<p>Thank you in advance,<\/p>\n<p>Sam<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1640278362207,
        "Question_score":4,
        "Question_tags":"python-3.x|xgboost|amazon-sagemaker",
        "Question_view_count":508,
        "Owner_creation_time":1418054263180,
        "Owner_last_access_time":1663863852740,
        "Owner_location":null,
        "Owner_reputation":1138,
        "Owner_up_votes":43,
        "Owner_down_votes":7,
        "Owner_views":101,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70465140",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62057838,
        "Question_title":"How to retrieve the labels used in a segmentation mask in AWS Sagemaker",
        "Question_body":"<p>From a segmentation mask, I am trying to retrieve what labels are being represented in the mask. <\/p>\n\n<p>This is the image I am running through a semantic segmentation model in AWS Sagemaker.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/XbMMP.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XbMMP.png\" alt=\"Motorbike and everything else background\"><\/a><\/p>\n\n<p>Code for making prediction and displaying mask.<\/p>\n\n<pre><code>from sagemaker.predictor import json_serializer, json_deserializer, RealTimePredictor\nfrom sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n\n%%time\nss_predict = sagemaker.RealTimePredictor(endpoint=ss_model.endpoint_name, \n                                     sagemaker_session=sess,\n                                    content_type = 'image\/jpeg',\n                                    accept = 'image\/png')\n\nreturn_img = ss_predict.predict(img)\n\nfrom PIL import Image\nimport numpy as np\nimport io\n\nnum_labels = 21\nmask = np.array(Image.open(io.BytesIO(return_img)))\nplt.imshow(mask, vmin=0, vmax=num_labels-1, cmap='jet')\nplt.show()\n<\/code><\/pre>\n\n<p>This image is the segmentation mask that was created and it represents the motorbike and everything else is the background.<\/p>\n\n<p>[<img src=\"https:\/\/i.stack.imgur.com\/6FbVn.png\" alt=\"Segmented mask[2]\"><\/p>\n\n<p>As you can see from the code there are 21 possible labels and 2 were used in the mask, one for the motorbike and another for the background. What I would like to figure out now is how to print which labels were actually used in this mask out of the 21 possible options?<\/p>\n\n<p>Please let me know if you need any further information and any help is much appreciated. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1590644898463,
        "Question_score":8,
        "Question_tags":"python|python-imaging-library|amazon-sagemaker|mxnet|semantic-segmentation",
        "Question_view_count":489,
        "Owner_creation_time":1449513251820,
        "Owner_last_access_time":1629436707117,
        "Owner_location":null,
        "Owner_reputation":693,
        "Owner_up_votes":47,
        "Owner_down_votes":0,
        "Owner_views":56,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Somewhere you should have a mapping from label integers to label classes, e.g.<\/p>\n\n<pre><code>label_map = {0: 'background', 1: 'motorbike', 2: 'train', ...}\n<\/code><\/pre>\n\n<p>If you are using the Pascal VOC dataset, that would be (1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle, 6=bus, 7=car , 8=cat, 9=chair, 10=cow, 11=diningtable, 12=dog, 13=horse, 14=motorbike, 15=person, 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv\/monitor) - see here: <a href=\"http:\/\/host.robots.ox.ac.uk\/pascal\/VOC\/voc2012\/segexamples\/index.html\" rel=\"nofollow noreferrer\">http:\/\/host.robots.ox.ac.uk\/pascal\/VOC\/voc2012\/segexamples\/index.html<\/a><\/p>\n\n<p>Then you can simply use that map:<\/p>\n\n<pre><code>used_classes = np.unique(mask)\nfor cls in used_classes:\n    print(\"Found class: {}\".format(label_map[cls]))\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1592390011563,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62057838",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71761511,
        "Question_title":"How can I add a final step to a Sagemaker Pipeline that runs even if other steps fail",
        "Question_body":"<p>Is there a way to add an end step to a sagemaker pipeline that still runs at the very end (and runs code) even if other previous steps fail. Before I thought we could make it a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html?highlight=definition#sagemaker.workflow.fail_step.FailStep\" rel=\"nofollow noreferrer\">Fail Step<\/a>  but that only lets you return an error message and doesn\u2019t let you run code. If we made it a conditional step how would we make sure it ran at the very end without depending on any previous steps. I thought of adding all previous steps as a dependency so it runs at the end, but then the end step wouldn't run if any step before that failed.<\/p>\n<p>I tried using the fail step, but I can't provide code. I tried putting it with dependencies but then it won't run if other steps fail before it. I tried putting no dependencies, but then it won't run at the end.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649223363853,
        "Question_score":1,
        "Question_tags":"amazon-web-services|pipeline|amazon-sagemaker",
        "Question_view_count":372,
        "Owner_creation_time":1649223065523,
        "Owner_last_access_time":1653515060240,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71761511",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72054242,
        "Question_title":"data format to predict with model fitted via Sagemaker's XGBoost built-in algorithm and training container",
        "Question_body":"<p>Looking at the following code, taken from <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">here<\/a>, I wonder what format dtest is (sorry I could not gleen this from the post):<\/p>\n<pre><code>import pickle as pkl \nimport tarfile\n\nt = tarfile.open('model.tar.gz', 'r:gz')\nt.extractall()\n\nmodel = pkl.load(open(model_file_path, 'rb'))\n\n# prediction with test data\npred = model.predict(dtest)\n<\/code><\/pre>\n<p>In my case the training and validation data are in csv format coming from a S3 bucket:<\/p>\n<pre><code>content_type = &quot;csv&quot;\ntrain_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'train'), content_type=content_type)\n<\/code><\/pre>\n<p>So ideally, I would also like to use the same format for scoring\/prediction\/inference.<\/p>\n<p>PS:<\/p>\n<p>This little function appears to work fine:<\/p>\n<pre><code>def write_prediction_data(data_file_name, target_name, model_file_name, output_file_name):\n\n    model = pkl.load(open(model_file_name, 'rb'))\n    data = pd.read_csv(data_file_name) \n    target = data[target_name]\n    data = data.drop([target_name], axis=1)\n    xgb_data = xgb.DMatrix(data.values, target.values)\n\n    data = pd.read_csv(data_file_name)\n    data['Prediction'] = model.predict(xgb_data)\n\n    data.to_csv(output_file_name, index=False)\n<\/code><\/pre>\n<p>Improvement suggestions always welcome (-:<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651217135783,
        "Question_score":0,
        "Question_tags":"python|xgboost|amazon-sagemaker",
        "Question_view_count":65,
        "Owner_creation_time":1267440784443,
        "Owner_last_access_time":1664045779313,
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Question_last_edit_time":1651313138547,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72054242",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69587230,
        "Question_title":"How to Set Java Home for Notebook in SageMaker",
        "Question_body":"<p>So it seems that I have Java installed after running the below line in the SageMaker Notebook Terminal:<\/p>\n<pre><code>bash-4.2$ sudo yum install java-1.8.0-openjdk \n<\/code><\/pre>\n<p>In the terminal I write the following to confirm:<\/p>\n<pre><code>bash-4.2$ java -version \njava version &quot;1.7.0_261&quot; OpenJDK Runtime Environment (amzn-2.6.22.1.83.amzn1-x86_64 u261-b02) OpenJDK 64-Bit Server VM (build 24.261-b02, mixed mode)\n<\/code><\/pre>\n<p>In my notebook I have the following Lines of code:<\/p>\n<pre><code>import tabula\n\ntabula.environment_info()\n<\/code><\/pre>\n<p>The notebook results in an error with:<\/p>\n<pre><code>java -version` faild. `java` command is not found from this Pythonprocess. Please ensure Java is installed and PATH is set for `java`\n<\/code><\/pre>\n<p>Yet, in the terminal I see this:<\/p>\n<pre><code>bash-4.2$ java -version \njava version &quot;1.7.0_261&quot; OpenJDK Runtime Environment (amzn-2.6.22.1.83.amzn1-x86_64 u261-b02) OpenJDK 64-Bit Server VM (build 24.261-b02, mixed mode)\n<\/code><\/pre>\n<p>I definitely have a java environment. How can I set my notebook to find this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634312182323,
        "Question_score":1,
        "Question_tags":"java|amazon-web-services|java-8|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":477,
        "Owner_creation_time":1615555459547,
        "Owner_last_access_time":1653444707037,
        "Owner_location":"United States",
        "Owner_reputation":141,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69587230",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72163392,
        "Question_title":"Run SageMaker Batch transform failed on loading model",
        "Question_body":"<p>I am trying to run batch transform job with HuggineFace class and fine-tuned model and custom inference file.\nThe job failed on loading the model but I could load it locally.\nI need to make custom inference file because i need to keep the input file as is, so i had to change the input key from the input json file.<\/p>\n<p>Here are the exception :<\/p>\n<pre><code>PredictionException(str(e), 400)\n2022-05-08 16:49:45,499 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - mms.service.PredictionException: Can't load config for '\/.sagemaker\/mms\/models\/model'. Make sure that:\n2022-05-08 16:49:45,499 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n2022-05-08 16:49:45,499 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - - '\/.sagemaker\/mms\/models\/model' is a correct model identifier listed on 'https:\/\/huggingface.co\/models'\n2022-05-08 16:49:45,499 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n2022-05-08 16:49:45,500 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - - or '\/.sagemaker\/mms\/models\/model' is the correct path to a directory containing a config.json file\n2022-05-08 16:49:45,500 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n2022-05-08 16:49:45,500 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  : 400\n<\/code><\/pre>\n<p>I am running on script mode:<\/p>\n<pre><code>from sagemaker.huggingface.model import HuggingFaceModel\n\nhub = {\n   # 'HF_MODEL_ID':'cardiffnlp\/twitter-roberta-base-sentiment',\n    'HF_TASK':'text-classification',\n    'INPUT_TEXTS': 'Description'\n}\n\nhuggingface_model = HuggingFaceModel(model_data='..\/model\/model.tar.gz',\n                                     role=role,\n                                     source_dir=&quot;..\/model\/pytorch_model\/code&quot;,\n                                     transformers_version=&quot;4.6&quot;, \n                                     pytorch_version=&quot;1.7&quot;, \n                                     py_version=&quot;py36&quot;,\n                                     entry_point=&quot;inference.py&quot;,\n                                     env=hub\n                                     )\n\nbatch_job = huggingface_model.transformer(\n    instance_count=1,\n    instance_type='ml.p3.2xlarge',\n    output_path=output_s3_path, # we are using the same s3 path to save the output with the input\n    strategy='SingleRecord',\n    accept='application\/json',\n    assemble_with='Line'\n\n)\n\nbatch_job.transform(\n    data=s3_file_uri,\n    content_type='application\/json',\n    split_type='Line',\n    #input_filter='$[1:]',\n    join_source='Input'\n\n)\n<\/code><\/pre>\n<p>Custom inference.py<\/p>\n<pre><code>import json\nimport os\nfrom transformers import pipeline\nimport torch\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n\ndef model_fn(model_dir):\n    model = pipeline(task=os.environ.get('HF_TASK', 'text-classification'), model=model_dir, tokenizer=model_dir)\n    return model\n\ndef transform_fn(model, input_data, content_type, accept):\n    input_data = json.loads(input_data)\n    input_text = os.environ.get('INPUT_TEXTS', 'inputs')\n    inputs = input_data.pop(input_text, None)\n    parameters = input_data.pop(&quot;parameters&quot;, None)\n\n    # pass inputs with all kwargs in data\n    if parameters is not None:\n        prediction = model(inputs, **parameters)\n    else:\n        prediction = model(inputs)\n    return json.dumps(\n        prediction,\n        ensure_ascii=False,\n        allow_nan=False,\n        indent=None,\n        separators=(&quot;,&quot;, &quot;:&quot;),\n    )\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1652029891640,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":209,
        "Owner_creation_time":1537218908380,
        "Owner_last_access_time":1660652518187,
        "Owner_location":"Israel",
        "Owner_reputation":931,
        "Owner_up_votes":107,
        "Owner_down_votes":3,
        "Owner_views":56,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72163392",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62614143,
        "Question_title":"AWS SageMaker: Create an endpoint using a trained model hosted in S3",
        "Question_body":"<p>I have following this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/semantic_segmentation_pascalvoc\/semantic_segmentation_pascalvoc.ipynb\" rel=\"nofollow noreferrer\">tutorial<\/a>, which is mainly for jupyter notebook, and made some minimal modification for external processing. I've created a project that could prepare my dataset locally, upload it to S3, train, and finally deploy the model predictor to the same bucket. Perfect!<\/p>\n<p>So, after to train and saved it in S3 bucket:<\/p>\n<pre><code> ss_model.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n<p>it failed while deploying as an endpoint. So, I have found tricks to host an endpoint in many ways, but not from a model already saved in S3. Because in order to host, you probably need to get the estimator, which in normal way is something like:<\/p>\n<pre><code> self.estimator = sagemaker.estimator.Estimator(self.training_image,\n                                                role,\n                                                train_instance_count=1,\n                                                train_instance_type='ml.p3.2xlarge',\n                                                train_volume_size=50,\n                                                train_max_run=360000,\n                                                output_path=output,\n                                                base_job_name='ss-training',\n                                                sagemaker_session=sess)\n<\/code><\/pre>\n<p>My question is: is there a way to load an estimator from a model saved in S3 (.tar)? Or, anyway, to create an endpoint without train it again?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1593283354330,
        "Question_score":1,
        "Question_tags":"python|deep-learning|amazon-sagemaker|semantic-segmentation",
        "Question_view_count":1360,
        "Owner_creation_time":1429270603900,
        "Owner_last_access_time":1664031172453,
        "Owner_location":"S\u00e3o Jos\u00e9 dos Campos, Sao Jose dos Campos - State of S\u00e3o Paulo, Brazil",
        "Owner_reputation":138,
        "Owner_up_votes":40,
        "Owner_down_votes":0,
        "Owner_views":38,
        "Question_last_edit_time":null,
        "Answer_body":"<p>So, after to run on many pages, just found a clue <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/blazingtext_hosting_pretrained_fasttext\/blazingtext_hosting_pretrained_fasttext.ipynb\" rel=\"nofollow noreferrer\">here<\/a>. And I finally found out how to load the model and create the endpoint:<\/p>\n<pre><code>def create_endpoint(self):\n    sess = sagemaker.Session()\n    training_image = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version=&quot;latest&quot;)        \n    role = &quot;YOUR_ROLE_ARN_WITH_SAGEMAKER_EXECUTION&quot;\n    model = &quot;s3:\/\/BUCKET\/PREFIX\/...\/output\/model.tar.gz&quot;\n\n    sm_model = sagemaker.Model(model_data=model, image=training_image, role=role, sagemaker_session=sess)\n    sm_model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge')\n<\/code><\/pre>\n<p><strong>Please, do not forget to disable your endpoint after using. This is really important! Endpoints are charged by &quot;running&quot; not only by the use<\/strong><\/p>\n<p>I hope it also can help you out!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1593463280863,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1595855655040,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62614143",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55132599,
        "Question_title":"Difference in usecases for AWS Sagemaker vs Databricks?",
        "Question_body":"<p>I was looking at Databricks because it integrates with AWS services like Kinesis, but it looks to me like SageMaker is a direct competitor to Databricks? We are heavily using AWS, is there any reason to add DataBricks into the stack or odes SageMaker fill the same role?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1552436606600,
        "Question_score":9,
        "Question_tags":"apache-spark|pyspark|databricks|amazon-sagemaker",
        "Question_view_count":11894,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":null,
        "Answer_body":"<p>SageMaker is a great tool for deployment, it simplifies a lot of processes configuring containers, you only need to write 2-3 lines to deploy the model as an endpoint and use it.  SageMaker also provides the dev platform (Jupyter Notebook) which supports Python and Scala (sparkmagic kernal) developing, and i managed installing external scala kernel in jupyter notebook. Overall, SageMaker provides end-to-end ML services. Databricks has unbeatable Notebook environment for Spark development. <\/p>\n\n<p>Conclusion<\/p>\n\n<ol>\n<li><p>Databricks is a better platform for Big data(scala, pyspark) Developing.(unbeatable notebook environment)<\/p><\/li>\n<li><p>SageMaker is better for Deployment. and if you are not working on big data, SageMaker is a perfect choice working with (Jupyter notebook + Sklearn + Mature containers + Super easy deployment). <\/p><\/li>\n<li><p>SageMaker provides \"real time inference\", very easy to build and deploy, very impressive. you can check the official SageMaker Github.\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline<\/a><\/p><\/li>\n<\/ol>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1553118034270,
        "Answer_score":14.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55132599",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72376872,
        "Question_title":"entry_point file using XGBoost as a framework in sagemaker",
        "Question_body":"<p>Looking at the following source code taken from <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">here<\/a> (SDK v2):<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker.xgboost.estimator import XGBoost\nfrom sagemaker.session import Session\nfrom sagemaker.inputs import TrainingInput\n\n# initialize hyperparameters\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;verbosity&quot;:&quot;1&quot;,\n        &quot;objective&quot;:&quot;reg:linear&quot;,\n        &quot;num_round&quot;:&quot;50&quot;}\n\n# set an output path where the trained model will be saved\nbucket = sagemaker.Session().default_bucket()\nprefix = 'DEMO-xgboost-as-a-framework'\noutput_path = 's3:\/\/{}\/{}\/{}\/output'.format(bucket, prefix, 'abalone-xgb-framework')\n\n# construct a SageMaker XGBoost estimator\n# specify the entry_point to your xgboost training script\nestimator = XGBoost(entry_point = &quot;your_xgboost_abalone_script.py&quot;, \n                    framework_version='1.2-2',\n                    hyperparameters=hyperparameters,\n                    role=sagemaker.get_execution_role(),\n                    instance_count=1,\n                    instance_type='ml.m5.2xlarge',\n                    output_path=output_path)\n\n# define the data type and paths to the training and validation datasets\ncontent_type = &quot;libsvm&quot;\ntrain_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'train'), content_type=content_type)\nvalidation_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'validation'), content_type=content_type)\n\n# execute the XGBoost training job\nestimator.fit({'train': train_input, 'validation': validation_input})\n<\/code><\/pre>\n<p>I wonder where the your_xgboost_abalone_script.py file has to be placed please? So far I used XGBoost as a built-in algorithm from my local machine with similar code (i.e. I span up a training job remotely). Thanks!<\/p>\n<p>PS:<\/p>\n<p>Looking at <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html\" rel=\"nofollow noreferrer\">this<\/a>, and source_dir, I wonder if one can upload Python files to S3. In this case, I take it is has to be tar.gz? Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653478466700,
        "Question_score":0,
        "Question_tags":"python|xgboost|amazon-sagemaker",
        "Question_view_count":74,
        "Owner_creation_time":1267440784443,
        "Owner_last_access_time":1664045779313,
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Question_last_edit_time":1653482394636,
        "Answer_body":"<p><code>your_xgboost_abalone_script.py<\/code> can be created locally. The path you provide is relative to where the code is running.<\/p>\n<p>I.e. <code>your_xgboost_abalone_script.py<\/code> can be located in the same directory where you are running the SageMaker SDK (&quot;source code&quot;).<\/p>\n<p>For example if you have <code>your_xgboost_abalone_script.py<\/code> in the same directory as the source code:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 source_code.py\n\u2514\u2500\u2500 your_xgboost_abalone_script.py\n<\/code><\/pre>\n<p>Then you can point to this file exactly how the documentation depicts:<\/p>\n<pre><code>estimator = XGBoost(entry_point = &quot;your_xgboost_abalone_script.py&quot;, \n.\n.\n.\n)\n<\/code><\/pre>\n<p>The SDK will take <code>your_xgboost_abalone_script.py<\/code> repackage it into a model tar ball and upload it to S3 on your behalf.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1654714910812,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1654796658207,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72376872",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67361483,
        "Question_title":"AWS Sagemaker Studio, cannot load pickle files",
        "Question_body":"<p>I'm a newbie in Sagemaker and i'm trying to load a pickle dataset into sagemaker notebook.\nI'm using the Python 3 (Data Science) kernel and ml.t3.medium instance.\nEither i load the pickle from S3 or I upload it directly from the studio like this:<\/p>\n<pre><code>import pickle5\nwith open('filename', 'rb') as f:\n    x = pickle.load(f)\n<\/code><\/pre>\n<p><strong>I get this Error:<\/strong><\/p>\n<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n\/opt\/conda\/lib\/python3.7\/site-packages\/IPython\/core\/formatters.py in __call__(self, obj)\n    700                 type_pprinters=self.type_printers,\n    701                 deferred_pprinters=self.deferred_printers)\n--&gt; 702             printer.pretty(obj)\n    703             printer.flush()\n    704             return stream.getvalue()\n\n..................... more errors here\n\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pandas\/core\/generic.py in __getattr__(self, name)\n   5268             or name in self._accessors\n   5269         ):\n-&gt; 5270             return object.__getattribute__(self, name)\n   5271         else:\n   5272             if self._info_axis._can_hold_identifiers_and_holds_name(name):\n\npandas\/_libs\/properties.pyx in pandas._libs.properties.AxisProperty.__get__()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pandas\/core\/generic.py in __getattr__(self, name)\n   5268             or name in self._accessors\n   5269         ):\n-&gt; 5270             return object.__getattribute__(self, name)\n   5271         else:\n   5272             if self._info_axis._can_hold_identifiers_and_holds_name(name):\n\nAttributeError: 'DataFrame' object has no attribute '_data'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619992853797,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|pickle|amazon-sagemaker",
        "Question_view_count":237,
        "Owner_creation_time":1553704286213,
        "Owner_last_access_time":1663944110353,
        "Owner_location":null,
        "Owner_reputation":27,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Can you check your Pandas versions? This error typically occurs when the pickled file was written in an old Pandas version. Your Sagemaker notebook probably runs Pandas &gt; 1.1 where as the Pandas in which the dataframe was pickled is probably &lt; 1.1<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620025682632,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67361483",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56245644,
        "Question_title":"How do I fix a memory allocation error in SageMaker without increasing instance size?",
        "Question_body":"<p>How can I resolve memory issues when training a CNN on SageMaker by increasing the number of instances, rather than changing the amount of memory each instance has? <\/p>\n\n<p>Using a larger instance does work, but I want to solve my problem by distributing across more instances. Using more instances ends up giving me a memory allocation error instead.<\/p>\n\n<p>Here is the code I am running in a Jupyter notebook cell:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tensorflow import TensorFlow\n\nestimator = TensorFlow(entry_point='train_aws.py',\n                       role=role,\n                       framework_version='1.12.0',\n                       training_steps= 100,                                  \n                       evaluation_steps= 100,\n                       hyperparameters={'learning_rate': 0.01},\n                       train_instance_count=2,\n                       train_instance_type='ml.c4.xlarge') \n\nestimator.fit(inputs)\n<\/code><\/pre>\n\n<p>I thought that adding more instances would increase the amount of memory, but it just gave me an allocation error instead.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1558469146393,
        "Question_score":0,
        "Question_tags":"amazon-web-services|deep-learning|training-data|amazon-sagemaker",
        "Question_view_count":2588,
        "Owner_creation_time":1558464611370,
        "Owner_last_access_time":1558469092013,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1558489822967,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56245644",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62305176,
        "Question_title":"Slow running of code in AWS for the first time",
        "Question_body":"<p>I am running my code on SageMaker, which runs my code slowly for the first time, but runs much faster the second time around. I guess there's something getting stored in the cache. Few days back, it was running with the same speed all the time. What could be a possible solution for this? <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>count_hea = 0\ncount_pleth = 0\nfor subfile in sorted(os.listdir('physionet.org\/files\/mimic3wdb-matched\/1.0\/p00')):\n    count_hea = 0\n    if subfile.startswith('p'):\n        for subsubfile in sorted(os.listdir(os.path.join('physionet.org\/files\/mimic3wdb-matched\/1.0\/p00\/' , subfile))):\n            if subsubfile.startswith('p') and count_hea == 0 and not subsubfile[:-4].endswith('n'):\n                try:            \n                    i = i + 1\n                    print(subsubfile)\n                    count_hea = count_hea + 1\n                    strip = subsubfile[:-4]\n                    record = wfdb.rdrecord('physionet.org\/files\/mimic3wdb-matched\/1.0\/p00\/' + subfile + '\/' + strip, channel_names = ['PLETH'], return_res = 16)\n                    r = record.__dict__\n                    print(r['sig_name'])\n                    if r['sig_name'] != None:\n                        if r['sig_name'][0] == 'PLETH':\n                            count_pleth = count_pleth + 1\n                            print(count_pleth)  \n                except Exception:\n                    pass\nprint(count_pleth)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1591796711163,
        "Question_score":0,
        "Question_tags":"amazon-web-services|caching|amazon-sagemaker",
        "Question_view_count":942,
        "Owner_creation_time":1591590460687,
        "Owner_last_access_time":1607696037083,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1591811735903,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62305176",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65191030,
        "Question_title":"AWS SageMaker ML DevOps tooling \/ architecture - Kubeflow?",
        "Question_body":"<p>I'm tasked with defining AWS tools for ML development at a medium-sized company. Assume about a dozen ML engineers plus other DevOps staff familiar with serverless ( lambdas and the framework ). The main questions are: a) what is an architecture that allows for the main tasks related to ML development (creating, training, fitting models, data pre-processing, hyper parameter optimization, job management, wrapping serverless services, gathering model metrics, etc ), b) what are the main tools that can be used for packaging and deploying things and c) what are the development tools (IDEs, SDKs, 'frameworks' ) used for it?\nI just want to set Jupyter notebooks aside for a second. Jupyter notebooks are great for proof-of-concepts and the closest thing to PowerPoint for management... But I have a problem with notebooks when thinking about deployable units of code.<br \/>\nMy intuition points to a preliminary target architecture with 5 parts:<\/p>\n<p>1 - A 'core' with ML models supporting basic model operations (create blank, create pre-trained, train, test\/fit, etc). I foresee core Python scripts here - no problem.<\/p>\n<p>2- (optional) A 'containerized-set-of-things' that performs hyper parameter optimization and\/or model versioning<\/p>\n<p>3- A 'contained-unit-of-Python-scripts-around-models' that exposes an API and that does job management and incorporates data pre-processing. This also reads and writes to S3 buckets.<\/p>\n<p>4-  A 'serverless layer' with high level API ( in Python ). It talks to #3 and\/or #1 above.<\/p>\n<p>5- Some container or bundling thing that will unpack files from Git and deploy them onto various AWS services creating things from the previous 3 points.<\/p>\n<p>As you can see, my terms are rather fuzzy:)  If someone can be specific with terms that will be helpful.\nMy intuition and my preliminary readings say that the answer will likely include a local IDE like PyCharm or Anaconda or a cloud-based IDE (what can these be? - don't mention notebooks please).\nThe point that I'm not really clear about is #5. Candidates include Amazon SageMaker Components for Kubeflow Pipelines and\/or Amazon SageMaker Components for Kubeflow Pipelines and\/or AWS Step Functions DS SDK For SageMaker. It's unclear to me how they can perform #5, however. Kubeflow looks very interesting but does it have enough adoption or will it die in 2 years? Are Amazon SageMaker Components for Kubeflow Pipelines, Amazon SageMaker Components for Kubeflow Pipelines and AWS Step Functions DS SDK For SageMaker mutually exclusive? How can each of them help with 'containerizing things' and with basic provisioning and deployment tasks?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1607384425877,
        "Question_score":0,
        "Question_tags":"machine-learning|amazon-sagemaker|kubeflow-pipelines",
        "Question_view_count":762,
        "Owner_creation_time":1519621319587,
        "Owner_last_access_time":1663870064033,
        "Owner_location":null,
        "Owner_reputation":123,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65191030",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72805580,
        "Question_title":"Is it possible to modify an existing AWS implementation of a deep learning model?",
        "Question_body":"<p>I wish to modify an existing model implementation in order to add an additional upsampling layer to a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/semantic-segmentation.html\" rel=\"nofollow noreferrer\">semantic segmentation algorithm that has previously been implemented in AWS<\/a>.<\/p>\n<p>It appears that Sagemaker refers to <a href=\"https:\/\/github.com\/dmlc\/gluon-cv\/tree\/4e1d44d718ca998523527aa2039cde2184d31296\" rel=\"nofollow noreferrer\">this repo<\/a>, and I'm hoping to modify <a href=\"https:\/\/github.com\/dmlc\/gluon-cv\/blob\/4e1d44d718ca998523527aa2039cde2184d31296\/gluoncv\/model_zoo\/deeplabv3.py\" rel=\"nofollow noreferrer\">the deeplab model<\/a> to add a final additional upsampling layer that is higher resolution than the initial input layer in order to boost the resolution of the output image (i.e., statistically downscale the original imagery).<\/p>\n<p>(<a href=\"https:\/\/www.researchgate.net\/publication\/349804789_A_Deep_Learning_Approach_to_Downscale_Geostationary_Satellite_Imagery_for_Decision_Support_in_High_Impact_Wildfires\" rel=\"nofollow noreferrer\">This technique has been demonstrated with UNET architectures.<\/a>)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656523696723,
        "Question_score":0,
        "Question_tags":"amazon-web-services|deep-learning|image-segmentation|amazon-sagemaker",
        "Question_view_count":27,
        "Owner_creation_time":1585530660043,
        "Owner_last_access_time":1663135350527,
        "Owner_location":null,
        "Owner_reputation":59,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72805580",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71830450,
        "Question_title":"AWS SageMaker Pipelines not being triggered by EventBridge",
        "Question_body":"<p>I've created a new SageMaker pipeline using AWS Python SDK, and everything is working fine, I can trigger my pipeline and it works perfectly using the SDK with these simples commands:<\/p>\n<pre><code>pipeline.upsert(role_arn=get_execution_role())\nexecution = pipeline.start()\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/KlP9W.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KlP9W.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Now, I would like to schedule the pipeline execution to run every day during the morning (let's say 8 a.m for example). And here's my problem. I configured the EventBridge as shown in this tutorial: <a href=\"https:\/\/github.com\/aws-samples\/scheduling-sagemaker-processing-with-sagemaker-pipelines\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/scheduling-sagemaker-processing-with-sagemaker-pipelines<\/a>, but instead of creating a new role, I used an existing one (the same returned from the command get_execution_role() above). My event is triggered in the correct hour (every day at 8 am), but the pipeline doesn't execute. When checking the logs on Cloud Watch, It shows that I got a FailedInvocations for the event, but I don't know how to get the logs from this failed execution. I tried to search on cloud trail but don't found nothing.<\/p>\n<p>Anyone could help me?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1649690844973,
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-sagemaker|aws-event-bridge",
        "Question_view_count":470,
        "Owner_creation_time":1501678564607,
        "Owner_last_access_time":1664080447277,
        "Owner_location":"Tup\u00e3, SP, Brasil",
        "Owner_reputation":121,
        "Owner_up_votes":149,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71830450",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72838683,
        "Question_title":"Best way to train a deep learning model in AWS Sagemaker, when data (image data > 10000 images) lies on a S3 bucket",
        "Question_body":"<p>What is the best was to train a deep learning model on AWS Sagemaker when I have a huge image dataset stored on a AWS S3 bucket.\nThe Dataset shouldn't be downloaded to the EBS volume of the notebook instance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656761331573,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|deep-learning|amazon-sagemaker",
        "Question_view_count":43,
        "Owner_creation_time":1462106136470,
        "Owner_last_access_time":1664013959780,
        "Owner_location":"Chennai, India",
        "Owner_reputation":21,
        "Owner_up_votes":48,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72838683",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65609804,
        "Question_title":"How to append stepfunction execution id to SageMaker job names?",
        "Question_body":"<p>I have a step function statemachine which creates SageMaker batch transform job, the definition is written in Terraform, I wanted to add the stepfunction execution id to the batch transform job names:<\/p>\n<p>in stepfunction terraform file:<\/p>\n<pre><code>  definition = templatefile(&quot;stepfuntion.json&quot;,\n    {\n      xxxx\n)\n<\/code><\/pre>\n<p>in the &quot;stepfuntion.json&quot;:<\/p>\n<pre><code>{...\n          &quot;TransformJobName&quot;: &quot;jobname-$$.Execution.Id&quot;,\n  \n          }\n      },\n        &quot;End&quot;: true\n      }\n    }\n  }\n<\/code><\/pre>\n<p>But after terraform apply, it didn't generate the actual id, it gave me <code>jobname-$$.Execution.Id<\/code>, can anyone help with this please?<\/p>\n<p>Resources: <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/input-output-contextobject.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/input-output-contextobject.html<\/a>\n&quot;To access the context object, first specify the parameter name by appending .$ to the end, as you do when selecting state input with a path. Then, to access context object data instead of the input, prepend the path with $$.. This tells AWS Step Functions to use the path to select a node in the context object.&quot;<\/p>\n<p>Can someone tell me what I'm missing please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610012100150,
        "Question_score":0,
        "Question_tags":"amazon-web-services|terraform|terraform-provider-aws|amazon-sagemaker|aws-step-functions",
        "Question_view_count":488,
        "Owner_creation_time":1540920956270,
        "Owner_last_access_time":1663875036883,
        "Owner_location":"United Kingdom",
        "Owner_reputation":2385,
        "Owner_up_votes":1007,
        "Owner_down_votes":16,
        "Owner_views":585,
        "Question_last_edit_time":1610014278923,
        "Answer_body":"<p>The var you are trying to use terraform doesn't know about it<\/p>\n<blockquote>\n<p>jobname-$$.Execution.Id.<\/p>\n<\/blockquote>\n<p>That's something specific to the Step function and available within state machine not available for terraform.<\/p>",
        "Answer_comment_count":17.0,
        "Answer_creation_time":1610017101707,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65609804",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71693539,
        "Question_title":"\"No entities to label\" has any cost in AWS Ground truth labeling?",
        "Question_body":"<p>I'm using amazon SageMaker Ground Truth to label texts, during the process I noticed that there is the option &quot;No entities to label&quot; and, I was wondering: if I select this option does the object still incur a cost to process?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1648734027393,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|amazon-ground-truth",
        "Question_view_count":22,
        "Owner_creation_time":1630700788560,
        "Owner_last_access_time":1657552741383,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1648821921529,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71693539",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71467176,
        "Question_title":"How can I verify that my training job is reading the augmented manifest file?",
        "Question_body":"<p>Apologies for the long post.<\/p>\n<p>Originally, I had data in one location on an S3 bucket and used to train deep learning image classification models on this data using the typical 'File' mode and passing the S3 uri where the data is stored as training input. To try and accelerate training, I wanted to switch to using:<\/p>\n<ol>\n<li>Pipe mode, to stream data and not download all the data at the beginning of the training, starting training faster and saving disk space.<\/li>\n<li>Augmented Manifest File coupled with 1., so that I don't have to place my data in a single location on S3, so I avoid moving data around when I train models.<\/li>\n<\/ol>\n<p>I was making my script similar to <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=934156#934156\" rel=\"nofollow noreferrer\">the one in this example<\/a>. I printed the steps done when parsing the data, however I noticed that the data might not have been read because when printing it shows the following:<\/p>\n<pre><code>step 1 Tensor(&quot;ParseSingleExample\/ParseExample\/ParseExampleV2:0&quot;, shape=(), dtype=string)\nstep 2 Tensor(&quot;DecodePng:0&quot;, shape=(None, None, 3), dtype=uint8)\nstep 3 Tensor(&quot;Cast:0&quot;, shape=(None, None, 3), dtype=float32)\n<\/code><\/pre>\n<p>I guess the image is not being read\/found since the shape is <code>[None, None, 3]<\/code> when it should be <code>[224, 224, 3]<\/code>, so maybe the problem is from the Augmented Manifest file?<\/p>\n<p>Here's an example of how my Augmented Manifest file is written:<\/p>\n<pre><code>{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image1.png&quot;, &quot;label&quot;: 1}\n{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image2.png&quot;, &quot;label&quot;: 2}\n{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image3.png&quot;, &quot;label&quot;: 3}\n<\/code><\/pre>\n<p>Some other details I should probably mention:<\/p>\n<ol>\n<li>When I create the Training Input I pass <code>'content_type': 'application\/x-recordio', 'record_wrapping': 'RecordIO'<\/code>, even though my data are in .png format, but I assumed that as the augmented manifest file is read the data get wrapped in the RecordIO format.<\/li>\n<li>Following my first point, I pass <code>PipeModeDataset(channel=channel, record_format='RecordIO')<\/code>, so also not sure about the RecordIO thing.<\/li>\n<\/ol>\n<p>There isn't an actual error that is raised, just when I start fitting the model nothing happens, it keeps on running but nothing actually runs so I'm trying to find the issue.<\/p>\n<hr \/>\n<p>EDIT: It now reads the shape correctly, but there's still the issue where it enters the .fit method and does nothing, just keeps running without doing anything. Find part of the script below.<\/p>\n<pre><code>def train_input_fn(train_channel):\n    &quot;&quot;&quot;Returns input function that feeds the model during training&quot;&quot;&quot;\n    return _input_fn(train_channel)\n\ndef _input_fn(channel):\n    &quot;&quot;&quot;\n        Returns a Dataset which reads from a SageMaker PipeMode channel.\n    &quot;&quot;&quot;\n    \n    features = {\n        'image-ref': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([3], tf.int64),\n    }\n \n    def combine(records):\n        return records[0], records[1]\n \n    def parse(record):\n        \n        parsed = tf.io.parse_single_example(record, features)\n        \n                 \n\n        image = tf.io.decode_png(parsed[&quot;image-ref&quot;], channels=3, dtype=tf.uint8)\n        image = tf.reshape(image, [224, 224, 3])\n        \n        lbl = parsed['label']\n        print(image, lbl)\n        return (image, lbl)\n \n    ds = PipeModeDataset(channel=channel, record_format='RecordIO')\n    ds = ds.map(parse, num_parallel_calls=AUTOTUNE)\n    ds = ds.prefetch(AUTOTUNE)\n \n    return ds\n\ndef model(dataset):\n    &quot;&quot;&quot;Generate a simple model&quot;&quot;&quot;\n    inputs = Input(shape=(224, 224, 3))\n    prediction_layer = Dense(2, activation = 'softmax')\n\n\n    x = inputs\n    x = tf.keras.applications.mobilenet.MobileNet(include_top=False, input_shape=(224,224,3), weights='imagenet')(x)\n    outputs = prediction_layer(x)\n    rec_model = tf.keras.Model(inputs, outputs)    \n    \n    rec_model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n        metrics=['accuracy']\n    )\n    \n    \n    rec_model.fit(\n        dataset\n    )\n\n    return rec_model\n\ndef main(params):\n    \n    epochs = params['epochs']\n    train_channel = params['train_channel']\n    record_format = params['record_format']\n    batch_size = params['batch_size']\n        \n    train_spec = train_input_fn(train_channel)\n    model_classifier = model(train_spec)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647258085310,
        "Question_score":0,
        "Question_tags":"python|tensorflow|amazon-s3|manifest|amazon-sagemaker",
        "Question_view_count":127,
        "Owner_creation_time":1576016596283,
        "Owner_last_access_time":1660210631827,
        "Owner_location":"Beirut, Lebanon",
        "Owner_reputation":15,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1647268897627,
        "Answer_body":"<p>From <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<blockquote>\n<p>A PipeModeDataset can read TFRecord, RecordIO, or text line records.<\/p>\n<\/blockquote>\n<p>While your'e trying to read binary (PNG) files. I don't see a relevant <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions\/tree\/tf-2\/src\/pipemode_op\/RecordReader\" rel=\"nofollow noreferrer\">record reader here<\/a> to help you do that.<br \/>\nYou could build your own format pipe implementation like shown <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">here<\/a>, but it's considerably more effort.<\/p>\n<p>Alternatively, you mentioned your files are scattered in different folders, but if your files common path contains less than 2M files, you could use <a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/10\/amazon-sagemaker-fast-file-mode\/\" rel=\"nofollow noreferrer\">FastFile mode<\/a> to <strong>stream<\/strong> data. Currently, FastFile only supports an S3 Prefix, so you won't be able to use a manifest.<\/p>\n<p>Also see this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/choose-the-best-data-source-for-your-amazon-sagemaker-training-job\/\" rel=\"nofollow noreferrer\">general pros\/cons discussion of the different available storage and input types available in SageMaker<\/a>.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1647607100132,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71467176",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73487568,
        "Question_title":"sagemaker transform not filtering input as expeced",
        "Question_body":"<p>Am trying to batch transform a csv file using this code<\/p>\n<p><strong>Processor script<\/strong><\/p>\n<pre><code>text_preparation_model = SKLearnModel(\n    sagemaker_session = local_session,\n    entry_point='processor.py',\n    role=context.role,\n    framework_version=..,\n    image_uri=...,\n    model_data=...)\n<\/code><\/pre>\n<p><strong>processor.py<\/strong><\/p>\n<pre><code>def input_fn(input_data, content_type):\n    if content_type == 'text\/csv':\n        df = pd.read_csv(StringIO(input_data), names=['feature_col1'],quoting=csv.QUOTE_NONNUMERIC, escapechar='\\\\')\n        print(df.head())\n\n        return df\n    else:\n        raise ValueError(&quot;{} .Error &quot;.format(content_type))\n<\/code><\/pre>\n<p><strong>Transform<\/strong><\/p>\n<pre><code>text_preparation_transformer.transform(\n    'file:\/\/validation.txt',  \n    content_type='text\/csv',\n    split_type='Line',\n    logs=context.show_logs,\n    input_filter='$[0]',\n    join_source='Input'\n)\n<\/code><\/pre>\n<p>the file is 2 columns separated with comma and values are enclosed in double quotes <code>ex<\/code> &quot;some feature value&quot;,&quot;label_1&quot;.\n<a href=\"https:\/\/i.stack.imgur.com\/TKJTo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TKJTo.png\" alt=\"enter image description here\" \/><\/a>\nin refernce to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform-data-processing.html\" rel=\"nofollow noreferrer\">this<\/a>,\nI expected to receive only the feature column in the <code>processor script<\/code>, but I keep getting both columns<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661430692517,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":26,
        "Owner_creation_time":1335447186710,
        "Owner_last_access_time":1664064892287,
        "Owner_location":"Egypt",
        "Owner_reputation":1972,
        "Owner_up_votes":701,
        "Owner_down_votes":11,
        "Owner_views":547,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73487568",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72669269,
        "Question_title":"How to share a jupyterlab project that's linked with sagemaker?",
        "Question_body":"<p>I have a notebook instance on AWS SageMaker that I'd like to be able to share with colleagues and have them make edits to the code and be able to run tests themselves using the AWS compute that I have setup on my account.<\/p>\n<p>Is this possible? And if so, how? Everywhere I look, I see people saying that you can share a project with sagemaker studio, but all I'm working with is jupyterlab.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1655555485900,
        "Question_score":0,
        "Question_tags":"amazon-web-services|jupyter-notebook|jupyter|jupyter-lab|amazon-sagemaker",
        "Question_view_count":105,
        "Owner_creation_time":1569340877270,
        "Owner_last_access_time":1663883527167,
        "Owner_location":null,
        "Owner_reputation":365,
        "Owner_up_votes":50,
        "Owner_down_votes":2,
        "Owner_views":68,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72669269",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54465049,
        "Question_title":"Getting Out of Memory error when using Image Classification in Sage Maker",
        "Question_body":"<p>When using a p2.xlarge or p3.2xlarge with up to 1TB of memory trying to use the predefined SageMaker Image Classification algorithm in a training job I\u2019m getting the following error:<\/p>\n\n<p><code>ClientError: Out of Memory. Please use a larger instance and\/or reduce the values of other parameters (e.g. batch size, number of layers etc.) if applicable<\/code><\/p>\n\n<p>I\u2019m using 450+ images, I\u2019ve tried resizing them from their original 2000x3000px size to a 244x244px size down to a 24x24px size and keep getting the same error.<\/p>\n\n<p>I\u2019ve tried adjusting my hyper parameters: num_classes, num_layers, num_training_samples, optimizer, image_shape, checkpoint frequency, batch_size and epochs. Also tried using pretrained model. But the same error keeps occurring.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548952048437,
        "Question_score":1,
        "Question_tags":"amazon-web-services|artificial-intelligence|amazon-sagemaker",
        "Question_view_count":2360,
        "Owner_creation_time":1361821819380,
        "Owner_last_access_time":1663972169277,
        "Owner_location":"Puebla",
        "Owner_reputation":147,
        "Owner_up_votes":148,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Question_last_edit_time":1549513217232,
        "Answer_body":"<p>Would've added this as a comment but I don't have enough rep yet.<\/p>\n\n<p>A few clarifying questions so that I can have some more context:<\/p>\n\n<p><em>How exactly are you achieving 1TB of RAM?<\/em><\/p>\n\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p2\/\" rel=\"nofollow noreferrer\"><code>p2.xlarge<\/code><\/a> servers have 61GB of RAM, and <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p3\/\" rel=\"nofollow noreferrer\"><code>p3.2xlarge<\/code><\/a> servers have 61GB memory + 16GB onboard the Tesla V100 GPU. <\/li>\n<\/ol>\n\n<p><em>How are you storing, resizing, and ingesting the images into the SageMaker algorithm?<\/em><\/p>\n\n<ol start=\"2\">\n<li>The memory error seems suspect considering it still occurs when downsizing images to 24x24. If you are resizing your original images (450 images at 2000x3000 resolution) as in-memory objects and aren't performing the transformations in-place (ie: not creating new images), you may have a substantial bit of memory pre-allocated, causing the SageMaker training algorithm to throw an OOM error.<\/li>\n<\/ol>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1548960285576,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1548962267732,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54465049",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51604901,
        "Question_title":"How to implement a beam search decoder in an SageMaker hosting endpoint?",
        "Question_body":"<p>I've created a SageMaker model for a Seq2Seq neural network, and then started a SageMaker endpoint:<\/p>\n\n<pre><code>create_endpoint_config_response = sage.create_endpoint_config(\n    EndpointConfigName = endpoint_config_name,\n    ProductionVariants=[{\n        'InstanceType':'ml.m4.xlarge', \n        'InitialInstanceCount':1,\n        'ModelName':model_name,\n        'VariantName':'AllTraffic'}])\n\ncreate_endpoint_response = sage.create_endpoint(\n    EndpointName=endpoint_name,\n    EndpointConfigName=endpoint_config_name)\n<\/code><\/pre>\n\n<p>This standard endpoint does not support beam search. What is the best approach for creating a SageMaker endpoint that supports beam search?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1533006311500,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|beam-search",
        "Question_view_count":138,
        "Owner_creation_time":1533004774230,
        "Owner_last_access_time":1576237563200,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51604901",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62927805,
        "Question_title":"What permissions are required to allow Lambda function to create SageMaker Batch Transform Job if bucket name doesn't include sagemaker?",
        "Question_body":"<p>Here is custom policy for calling SageMaker Batch Transform Job:<\/p>\n<pre><code>{\n  &quot;Version&quot;: &quot;2012-10-17&quot;,\n  &quot;Statement&quot;: [\n    {\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Action&quot;: [\n        &quot;s3:GetObject&quot;\n      ],\n      &quot;Resource&quot;: [\n        &quot;arn:aws:s3:::&lt;bucket-name&gt;\/path\/*&quot;\n      ]\n    },\n    {\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Action&quot;: [\n        &quot;s3:PutObject&quot;\n      ],\n      &quot;Resource&quot;: [\n        &quot;arn:aws:s3:::&lt;bucket-name&gt;\/another_path\/*&quot;\n      ]\n    },\n    {\n      &quot;Action&quot;: [\n        &quot;s3:ListBucket&quot;\n      ],\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Resource&quot;: [\n        &quot;arn:aws:s3:::&lt;bucket-name&gt;&quot;\n      ]\n    },\n    {\n      &quot;Effect&quot;: &quot;Allow&quot;,\n      &quot;Action&quot;: [\n        &quot;sagemaker:CreateTransformJob&quot;,\n        &quot;sagemaker:DescribeTrainingJob&quot;,\n        &quot;sagemaker:DescribeTransformJob&quot;\n      ],\n      &quot;Resource&quot;: &quot;*&quot;\n    }\n  ]\n}\n\n<\/code><\/pre>\n<p>And I also attached to the Lambda role the default AWSLambdaBasicExecutionRole permissions.<\/p>\n<p>If I change the bucket name to include start with <em>sagemaker<\/em>, everything works fine, however, I cannot do that. Do you have any suggestions about the policy and what permissions I have missed to add?<\/p>\n<p>Edit<\/p>\n<p>Here is error messages from SageMaker transform job logs: <code># 011... 24 more2020-07-15T12:49:26.332:[sagemaker logs]: &lt;bucket-name&gt;\/path\/input_file.json: 403 Forbidden (403): Forbidden<\/code> and <code>2020-07-15T12:49:26.345:[sagemaker logs]: MaxConcurrentTransforms=2, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD<\/code>. There are only 2 lines in that job log group. In another log group are no errors, only messages that model was loaded for predictions.<\/p>\n<p>And error message in Lambda: <code>Transform failed with the following error: ClientError: See job logs for more information<\/code><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1594875949330,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-iam|amazon-sagemaker",
        "Question_view_count":324,
        "Owner_creation_time":1477378790530,
        "Owner_last_access_time":1663832602253,
        "Owner_location":null,
        "Owner_reputation":468,
        "Owner_up_votes":90,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Question_last_edit_time":1594877929889,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62927805",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68470626,
        "Question_title":"Defining Metrics on SageMaker to CloudWatch",
        "Question_body":"<p>From AWS Sagemaker Documentation, In order to track metrics in cloudwatch for custom ml algorithms (non-builtin), I read that I have to define my estimaotr as below.<\/p>\n<p>But I am not sure how to alter my training script so that the metric definitions declared inside my estimators can pick up these values.<\/p>\n<pre><code>estimator =\n                Estimator(image_name=ImageName,\n                role='SageMakerRole', \n                instance_count=1,\n                instance_type='ml.c4.xlarge',\n                k=10,\n                sagemaker_session=sagemaker_session,\n                metric_definitions=[\n                   {'Name': 'train:error', 'Regex': 'Train_error=(.*?);'},\n                   {'Name': 'validation:error', 'Regex': 'Valid_error=(.*?);'}\n                ]\n            )\n<\/code><\/pre>\n<p>In my training code, I have<\/p>\n<pre><code>    for epoch in range(1, args.epochs + 1):\n        total_loss = 0\n        model.train()\n        for step, batch in enumerate(train_loader):\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_labels = batch[2].to(device)\n            model.zero_grad()\n\n            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n            loss = outputs[0]\n\n            total_loss += loss.item()\n            loss.backward() # Computes the gradients\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip for error prevention\n            # modified based on their gradients, the learning rate, etc.\n            optimizer.step() # Back Prop\nlogger.info(&quot;Average training loss: %f\\n&quot;, total_loss \/ len(train_loader))\n<\/code><\/pre>\n<p>Here, I want the train:error to pick up <code>total_loss \/ len(train_loader)<\/code> but I am not sure how to assign this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626875025787,
        "Question_score":1,
        "Question_tags":"python|pytorch|amazon-cloudwatch|amazon-sagemaker",
        "Question_view_count":1420,
        "Owner_creation_time":1600718448277,
        "Owner_last_access_time":1663311569497,
        "Owner_location":"Seoul, South Korea",
        "Owner_reputation":69,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68470626",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73255982,
        "Question_title":"Sagemaker training job failed - having issues executing user script",
        "Question_body":"<p>I am very new to AWS Sagemaker and am trying to deploy my SKLearn script to an endpoint so that I can call it from within an Android app. I am following the code <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, and so far, getting each block to work with my script has worked. The block that is giving me issues is<\/p>\n<pre><code>sklearn_estimator.latest_training_job.wait(logs=&quot;None&quot;)\nartifact = sm_boto3.describe_training_job(\n    TrainingJobName=sklearn_estimator.latest_training_job.name\n)[&quot;ModelArtifacts&quot;][&quot;S3ModelArtifacts&quot;]\n\nprint(&quot;Model artifact persisted at &quot; + artifact)\n<\/code><\/pre>\n<p>Specifically, the first line. When I run this block, I get this error:<\/p>\n<pre><code>UnexpectedStatusException                 Traceback (most recent call last)\n&lt;ipython-input-54-65920860bce1&gt; in &lt;module&gt;\n----&gt; 1 sklearn_estimator.latest_training_job.wait(logs=&quot;None&quot;)\n      2 artifact = sm_boto3.describe_training_job(\n      3     TrainingJobName=sklearn_estimator.latest_training_job.name\n      4 )[&quot;ModelArtifacts&quot;][&quot;S3ModelArtifacts&quot;]\n      5 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n   1994             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   1995         else:\n-&gt; 1996             self.sagemaker_session.wait_for_job(self.job_name)\n   1997 \n   1998     def describe(self):\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in wait_for_job(self, job, poll)\n   3217             lambda last_desc: _train_done(self.sagemaker_client, job, last_desc), None, poll\n   3218         )\n-&gt; 3219         self._check_job_status(job, desc, &quot;TrainingJobStatus&quot;)\n   3220         return desc\n   3221 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   3381                 message=message,\n   3382                 allowed_statuses=[&quot;Completed&quot;, &quot;Stopped&quot;],\n-&gt; 3383                 actual_status=status,\n   3384             )\n   3385 \n\nUnexpectedStatusException: Error for Training job rf-scikit-2022-08-05-22-32-08-239: Failed. Reason: AlgorithmError: framework error: \nTraceback (most recent call last):\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py&quot;, line 84, in train\n    entrypoint()\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py&quot;, line 39, in main\n    train(environment.Environment())\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py&quot;, line 35, in train\n    runner_type=runner.ProcessRunnerType)\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/entry_point.py&quot;, line 100, in run\n    wait, capture_error\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py&quot;, line 291, in run\n    cwd=environment.code_dir,\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py&quot;, line 208, in check_error\n    info=extra_info,\nsagemaker_training.errors.ExecuteUserScriptError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage &quot;&quot;\nCommand &quot;\/miniconda3\/bin\/python SageMaker_Script.py&quot;\n\nExecuteUse\n<\/code><\/pre>\n<p>SageMaker_Script.py is the name of my script. The relevant code in my script is:<\/p>\n<pre><code>if __name__ =='__main__':\n\nprint('extracting arguments')\nparser = argparse.ArgumentParser()\n\n# hyperparameters sent by the client are passed as command-line arguments to the script.\nparser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n\n\n\n# Data, model, and output directories\nparser.add_argument(&quot;--model-dir&quot;, type=str, default=os.environ.get(&quot;SM_MODEL_DIR&quot;))\nparser.add_argument(&quot;--train&quot;, type=str, default=os.environ.get(&quot;SM_CHANNEL_TRAIN&quot;))\nparser.add_argument(&quot;--test&quot;, type=str, default=os.environ.get(&quot;SM_CHANNEL_TEST&quot;))\nparser.add_argument(&quot;--train-file&quot;, type=str, default=&quot;jumpstrain.csv&quot;)\nparser.add_argument(&quot;--test-file&quot;, type=str, default=&quot;jumpstest.csv&quot;)\n\nargs, _ = parser.parse_known_args()\n\nprint('reading data')\ntrain_df = pd.read_csv(os.path.join(args.train, args.train_file))\ntest_df = pd.read_csv(os.path.join(args.test, args.test_file))\n\nprint('building training and testing datasets')\nX_train = train_df[columns]\nX_test = test_df[columns]\ny_train = train_df[['Under-rotated']]\ny_test = test_df[['Under-rotated']]\n\nprint('training model')\nmodel = RandomForestClassifier(n_estimators = 100)\nmodel.fit(X_train, y_train)\n\nprint('validating model')\npred_values = model.predict(X_test[columns])\nprint('f1-score:')\nf1score = f1_score(y_test, pred_values)\nprint(f1score)\n\n# persist model\npath = os.path.join(args.model_dir, 'model.joblib')\njoblib.dump(model, path)\nprint('model persisted at ' + path)\nprint(args.min_samples_leaf)\n<\/code><\/pre>\n<p>I am at a loss for what the issue is because, like I said, I am very new to AWS in general, and the error that it gives me is not super informative. Any help would be greatly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659740488663,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":158,
        "Owner_creation_time":1297213486663,
        "Owner_last_access_time":1664052908910,
        "Owner_location":null,
        "Owner_reputation":768,
        "Owner_up_votes":110,
        "Owner_down_votes":1,
        "Owner_views":116,
        "Question_last_edit_time":1659774527680,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73255982",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56952741,
        "Question_title":"Sagemaker export and load model to memory",
        "Question_body":"<p>I have created a model using sagemaker (on aws ml notebook). \nI then exported that model to s3 and a <code>.tar.gz<\/code> file was created there.<\/p>\n\n<p>Im trying to find a way to load the model object to memory in my code (without using AWS docker images and deployment) and run a prediction on it.<\/p>\n\n<p>I looked for functions to do that in the model section of the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html#\" rel=\"nofollow noreferrer\">sagemaker docs<\/a>, but everything there is tightly coupled to the AWS docker images.<\/p>\n\n<p>I then tried opening the file with <code>tarfile<\/code> and <code>shutil<\/code> packages but that was useless.<\/p>\n\n<p>Any ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1562675762410,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1838,
        "Owner_creation_time":1480290282627,
        "Owner_last_access_time":1663752565217,
        "Owner_location":null,
        "Owner_reputation":2906,
        "Owner_up_votes":208,
        "Owner_down_votes":33,
        "Owner_views":266,
        "Question_last_edit_time":null,
        "Answer_body":"<p>With the exception of XGBoost, built-in algorithms are implemented with Apache MXNet, so simply extract the model from the .tar.gz file and load it with MXNet: load_checkpoint() is the API to use.<\/p>\n\n<p>XGBoost models are just pickled objects. Unpickle and load in sklearn:<\/p>\n\n<pre><code>$ python3\n&gt;&gt;&gt; import sklearn, pickle\n&gt;&gt;&gt; model = pickle.load(open(\"xgboost-model\", \"rb\"))\n&gt;&gt;&gt; type(model)\n&lt;class 'xgboost.core.Booster'&gt;\n<\/code><\/pre>\n\n<p>Models trained with built-in library (Tensorflow, MXNet, Pytorch, etc.) are vanilla models that can be loaded as-is with the correct library.<\/p>\n\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1562768843267,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1562845984060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56952741",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62319753,
        "Question_title":"How to I pass secrets stored in AWS Secret Manager to a Docker container in Sagemaker?",
        "Question_body":"<p>My code is in R. And I need to excess external database. I am storing database credentials in AWS Secret Manager.<\/p>\n<p>So I first tried using paws library to get aws secrets in R but that would require storing access key, secret id and session token, and I want to avoid that.<\/p>\n<p>Is there a better way to do this? I have created IAM role for Sagemaker. Is it possible to pass secrets as environment variables?<\/p>\n<p>Edit: I wanted to trigger Sagemaker Processing<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1591862670733,
        "Question_score":0,
        "Question_tags":"r|amazon-web-services|docker|amazon-sagemaker",
        "Question_view_count":1063,
        "Owner_creation_time":1444035983570,
        "Owner_last_access_time":1656492622493,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":1593501623132,
        "Answer_body":"<p>I found a simple solution to it. Env variables can be passed via Sagemaker sdk. It minimizes the dependencies.<\/p>\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html<\/a><\/p>\n<p>As another answer suggested, paws can be used as well to get secrets from aws. This would be a better approach<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1593501738047,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1602059220016,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62319753",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73208208,
        "Question_title":"how to convert binary file to pandas dataframe",
        "Question_body":"<p>I use Amazon Sagemaker for model training and prediction. I have a problem with the returned data with predictions.  I am trying to convert prediction data to pandas dataframe format.<\/p>\n<p>After the model is deployed:<\/p>\n<pre><code>from sagemaker.serializers import CSVSerializer\n\nxgb_predictor=estimator.deploy(\n    initial_instance_count=1,\n    instance_type='ml.g4dn.xlarge',\n    serializer=CSVSerializer()\n)\n\n<\/code><\/pre>\n<p>I made a prediction on the test data:<\/p>\n<pre><code>predictions=xgb_predictor.predict(first_day.to_numpy())\n\n<\/code><\/pre>\n<p>The returned prediction results are in a binary file<\/p>\n<pre><code>predictions\n<\/code><\/pre>\n<pre><code>b'2.092024326324463\\n10.584211349487305\\n18.23127555847168\\n2.092024326324463\\n8.308058738708496\\n32.35516357421875\\n4.129155158996582\\n7.429899215698242\\n55.65376281738281\\n116.5504379272461\\n1.0734045505523682\\n5.29403018951416\\n1.0924320220947266\\n1.9484598636627197\\n5.29403018951416\\n2.190509080886841\\n2.085641860961914\\n2.092024326324463\\n7.674410343170166\\n2.1198673248291016\\n5.293967247009277\\n7.088096618652344\\n2.092024326324463\\n10.410735130310059\\n10.36008358001709\\n2.092024326324463\\n10.565692901611328\\n15.495997428894043\\n15.61841106414795\\n1.0533703565597534\\n6.262670993804932\\n31.02411460876465\\n10.43086051940918\\n3.116995096206665\\n3.2846100330352783\\n108.82835388183594\\n26.210166931152344\\n1.0658172369003296\\n10.55643367767334\\n6.245237350463867\\n15.951444625854492\\n10.195240020751953\\n1.0734045505523682\\n48.720497131347656\\n2.119992256164551\\n9.41071605682373\\n2.241959810256958\\n3.1907501220703125\\n10.415051460266113\\n1.2154537439346313\\n2.13691782951355\\n31.1861515045166\\n3.0827555656433105\\n6.261478424072266\\n5.279026985168457\\n15.897627830505371\\n20.483125686645508\\n20.874958038330078\\n53.2086296081543\\n10.731611251831055\\n2.115110397338867\\n13.79739761352539\\n2.1198673248291016\\n26.628803253173828\\n10.030998229980469\\n15.897627830505371\\n5.278475284576416\\n45.371158599853516\\n2.2791690826416016\\n15.58777141571045\\n15.947166442871094\\n30.88138771057129\\n10.388553619384766\\n48.22294235229492\\n10.565692901611328\\n20.808977127075195\\n10.388553619384766\\n15.910200119018555\\n8.252408981323242\\n1.109586238861084\\n15.58777141571045\\n13.718815803527832\\n3.1227424144744873\\n32.171592712402344\\n10.524396896362305\\n15.897627830505371\\n2.092024326324463\\n14.52088737487793\\n5.293967247009277\\n57.61208724975586\\n21.161712646484375\\n14.173937797546387\\n5.230247974395752\\n16.257652282714844\n\n<\/code><\/pre>\n<p>How can I convert prediction data to pandas dataframe?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659445988163,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|predict",
        "Question_view_count":31,
        "Owner_creation_time":1414361702887,
        "Owner_last_access_time":1662654465440,
        "Owner_location":null,
        "Owner_reputation":115,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":null,
        "Answer_body":"<p>you mean this:<\/p>\n<pre><code>import pandas as pd\n\na = a.decode(encoding=&quot;utf-8&quot;).split(&quot;\\n&quot;)\n\ndf = pd.DataFrame(data=a)\ndf.head()\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1659449084300,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73208208",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68808547,
        "Question_title":"What does 100% utilisation mean in SageMaker Studio?",
        "Question_body":"<p>(This is related to <a href=\"https:\/\/stackoverflow.com\/questions\/68569742\/sage-maker-studio-cpu-usage\">Sage Maker Studio CPU Usage<\/a> but focuses on interpreting meaning rather than modifying behaviour)<\/p>\n<p>SageMaker Studio shows Kernel and Instance usage for CPU and Memory:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PyqPh.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PyqPh.png\" alt=\"Screenshot of Kernel and Instance usage\" \/><\/a><\/p>\n<p>The kernel is just the selected Jupyter kernel and so would appear as a single process on a local machine, while the instance is the EC2 instance that they're running on.<\/p>\n<p>The only documentation from Amazon appears to be in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks-menu.html\" rel=\"nofollow noreferrer\">Use the SageMaker Studio Notebook Toolbar<\/a> which says that it &quot;Displays the CPU usage and memory usage. Double-click to toggle between the current kernel and the current instance&quot; (this is outdated and relates to the old position of the information).<\/p>\n<p>In the context of SageMaker Studio, does 100% CPU mean 100% of one CPU or 100% of all CPUs? (<code>top<\/code> shows multi-core as &gt;100% but consolidated measures like Windows Task Manager's default representation show all cores as 100%)<\/p>\n<p>And does 25% instance utilisation then mean that my instance is over-specced? (Intuitively, it should do because I'm not using 100% even when training a model, but I've tried smaller instances and still never maxes Instance CPU usage, only Kernel CPU usage)<\/p>\n<p>I've tried using <code>joblib<\/code> to make some parallel &quot;wheel spinning&quot; tasks to check usage, but that just resulted in Kernel being quiet and Instance having all of the usage!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1629143793270,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":270,
        "Owner_creation_time":1267388642537,
        "Owner_last_access_time":1663876899047,
        "Owner_location":"United Kingdom",
        "Owner_reputation":869,
        "Owner_up_votes":89,
        "Owner_down_votes":5,
        "Owner_views":129,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68808547",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61762950,
        "Question_title":"Predict probability on AWS SageMaker AutoPilot endpoint",
        "Question_body":"<p>I am testing SageMaker AutoPilot in order to verify how good it is for regular use.<\/p>\n\n<p>Up until now, it seems relatively easy to use it, it trained a model with good results and it was easy to create the endpoint. I would like to get the predicted label and its probability, in order to check if the prediciton is good. However, I could only get the label and I did not find anything about retrieving the probability (predict_proba).<\/p>\n\n<p>Is there any way to get the probability? Thank you!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1589321793787,
        "Question_score":2,
        "Question_tags":"python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":800,
        "Owner_creation_time":1548462983600,
        "Owner_last_access_time":1663939630817,
        "Owner_location":null,
        "Owner_reputation":176,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61762950",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61091659,
        "Question_title":"Should I run forecast predictive model with AWS lambda or sagemaker?",
        "Question_body":"<p>I've been reading some articles regarding this topic and have preliminary thoughts as what I should do with it, but still want to see if anyone can share comments if you have more experience with running machine learning on AWS. I was doing a project for a professor at school, and we decided to use AWS. I need to find a cost-effective and efficient way to deploy a forecasting model on it. <\/p>\n\n<p>What we want to achieve is:<\/p>\n\n<ul>\n<li>read the data from S3 bucket monthly (there will be new data coming in every month), <\/li>\n<li>run a few python files (.py) for custom-built packages and install dependencies (including the files, no more than 30kb), <\/li>\n<li>produce predicted results into a file back in S3 (JSON or CSV works), or push to other endpoints (most likely to be some BI tools - tableau etc.) - but really this step can be flexible (not web for sure) <\/li>\n<\/ul>\n\n<p><strong>First thought I have is AWS sagemaker<\/strong>. However, we'll be using \"fb prophet\" model to predict the results, and we built a customized package to use in the model, therefore, I don't think the notebook instance is gonna help us. (Please correct me if I'm wrong) My understanding is that sagemaker is a environment to build and train the model, but we already built and trained the model. Plus, we won't be using AWS pre-built models anyways.<\/p>\n\n<p>Another thing is if we want to use custom-built package, we will need to create container image, and I've never done that before, not sure about the efforts to do that.<\/p>\n\n<p><strong>2nd option is to create multiple lambda functions<\/strong><\/p>\n\n<ul>\n<li><p>one that triggers to run the python scripts from S3 bucket (2-3 .py files) every time a new file is imported into S3 bucket, which will happen monthly.<\/p><\/li>\n<li><p>one that trigger after the python scripts are done running and produce results and save into S3 bucket.<\/p><\/li>\n<\/ul>\n\n<p>3rd option will combine both options:\n - Use lambda function to trigger the implementation on the python scripts in S3 bucket when the new file comes in.\n - Push the result using sagemaker endpoint, which means we host the model on sagemaker and deploy from there.<\/p>\n\n<p>I am still not entirely sure how to put pre-built model and python scripts onto sagemaker instance and host from there.<\/p>\n\n<p>I'm hoping whoever has more experience with AWS service can help give me some guidance, in terms of more cost-effective and efficient way to run model.<\/p>\n\n<p>Thank you!! <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1586306942937,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|aws-lambda|amazon-sagemaker|facebook-prophet",
        "Question_view_count":2675,
        "Owner_creation_time":1579188091243,
        "Owner_last_access_time":1661402697580,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I would say it all depends on how heavy your model is \/ how much data you're running through it. You're right to identify that Lambda will likely be less work. It's quite easy to get a lambda up and running to do the things that you need, and <a href=\"https:\/\/aws.amazon.com\/lambda\/pricing\/\" rel=\"nofollow noreferrer\">Lambda has a very generous free tier<\/a>. The problem is:<\/p>\n\n<ol>\n<li><p>Lambda functions are fundamentally limited in their processing capacity (they timeout after <em>max<\/em> 15 minutes).<\/p><\/li>\n<li><p>Your model might be expensive to load.<\/p><\/li>\n<\/ol>\n\n<p>If you have a lot of data to run through your model, you will need multiple lambdas. Multiple lambdas means you have to load your model multiple times, and that's wasted work. If you're working with \"big data\" this will get expensive once you get through the free tier.<\/p>\n\n<p>If you don't have much data, Lambda will work just fine. I would eyeball it as follows: assuming your data processing step is dominated by your model step, and if all your model interactions (loading the model + evaluating all your data) take less than 15min, you're definitely fine. If they take more, you'll need to do a back-of-the-envelope calculation to figure out whether you'd leave the Lambda free tier.<\/p>\n\n<p>Regarding Lambda: You can literally copy-paste code in to setup a prototype. If your execution takes more than 15min for all your data, you'll need a method of splitting your data up between multiple Lambdas. Consider <a href=\"https:\/\/aws.amazon.com\/step-functions\/\" rel=\"nofollow noreferrer\">Step Functions<\/a> for this.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1586317398727,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61091659",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66086605,
        "Question_title":"Use AWS ML model Random Cut Forest locally",
        "Question_body":"<p>I wonder if it is possible to deploy Random Cut Forest (RCF) built-in algorithm of SageMaker to the local mode. I haven't come across any sample implementation about it. If not, can we simply say that models trained using RCF are limited to be consumed inside the platform via Inference Endpoints?<\/p>\n<p>I got this error when I tried to do so.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/YFlRf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YFlRf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ywlQU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ywlQU.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1612692155937,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|rcf",
        "Question_view_count":195,
        "Owner_creation_time":1594550494350,
        "Owner_last_access_time":1650824104303,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":"<p>indeed you're right, <strong>SageMaker Random Cut Forest cannot be trained and deployed locally. The 18 Amazon SageMaker Built-in algorithms are designed to be trained and deployed on Amazon SageMaker.<\/strong> There are 2 exceptions: SageMaker BlazingText and SageMaker XGBoost, which can be read with their open-source counterparts (fastText and XGBoost) and used for  inference out of SageMaker (eg EC2, Lambda, on-prem or on your laptop - as long as you can install those libraries)<\/p>\n<p>There is an open-source attempt to implement the Random Cut Forest here <a href=\"https:\/\/github.com\/kLabUM\/rrcf\" rel=\"nofollow noreferrer\">https:\/\/github.com\/kLabUM\/rrcf<\/a> ; I don't think it has any connection to SageMaker RCF codebase so results, speed and scalability may differ.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1612734092367,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1612737023060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66086605",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59833019,
        "Question_title":"memory error while writing a large dataframe to S3 AWS",
        "Question_body":"<p>I have created a dataframe with the following shape using amazon sagemaker.<\/p>\n\n<pre><code>10612611 rows \u00d7 4 columns\n<\/code><\/pre>\n\n<p>All are numeric values.\n When I am trying to write this dataframe into my S3 bucket as follows, I get memory error.<\/p>\n\n<pre><code>bytes_to_write = df.to_csv(None).encode()\nwith s3.open('aws-athena-query-results-xxxxxxx\/query_result\/xx.csv','wb') as f:\n    f.write(bytes_to_write)\n<\/code><\/pre>\n\n<blockquote>\n  <p>MemoryError:<\/p>\n<\/blockquote>\n\n<p>I am using <strong>ml.t2.medium<\/strong> for sagemaker instance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579570132453,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":414,
        "Owner_creation_time":1497488105307,
        "Owner_last_access_time":1663902465110,
        "Owner_location":null,
        "Owner_reputation":1237,
        "Owner_up_votes":214,
        "Owner_down_votes":5,
        "Owner_views":376,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I solved this issue by changing the instance type from <strong>ml.t2.medium<\/strong> to <strong>ml.t2.2xlarge<\/strong> and it worked perfectly.<\/p>\n\n<p>The original issue was with the RAM of the instance type and not with S3.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1579577379867,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59833019",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73672457,
        "Question_title":"PyTorch Lightning with Amazon SageMaker",
        "Question_body":"<p>We\u2019re currently running using Pytorch Lightning for training outside of SageMaker. Looking to use SageMaker to leverage distributed training, checkpointing, model training optimization(training compiler) etc to accelerate training process and save costs. Whats the recommended way to migrate their PyTorch Lightning scripts to run on SageMaker?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1662818975707,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":39,
        "Owner_creation_time":1412669622830,
        "Owner_last_access_time":1663944305230,
        "Owner_location":null,
        "Owner_reputation":26,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73672457",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50983316,
        "Question_title":"Continuous Training in Sagemaker",
        "Question_body":"<p>I am trying out <strong>Amazon Sagemaker<\/strong>, I haven't figured out how we can have Continuous training.\n<br>\nFor example if i have a CSV file in s3 and I want to train each time the CSV file is updated.<\/p>\n\n<p>I know we can go again to the notebook and re-run the whole notebook to make this happen.\n<br>\nBut i am looking for an automated way, with some python scripts or using a lambda function with s3 events etc<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1529654311630,
        "Question_score":4,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1191,
        "Owner_creation_time":1440734188430,
        "Owner_last_access_time":1655569862827,
        "Owner_location":"India",
        "Owner_reputation":1491,
        "Owner_up_votes":198,
        "Owner_down_votes":32,
        "Owner_views":112,
        "Question_last_edit_time":1529654668140,
        "Answer_body":"<p>You can use boto3 sdk for python to start training on lambda then you need to trigger the lambda when csv is update.<\/p>\n\n<blockquote>\n  <p><a href=\"http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/sagemaker.html\" rel=\"nofollow noreferrer\">http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/sagemaker.html<\/a><\/p>\n<\/blockquote>\n\n<p>Example python code<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a><\/p>\n<\/blockquote>\n\n<p>Addition: You dont need to use lambda you just start\/cronjob the python script any kind of instance which has python and aws sdk in it.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1530199631932,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50983316",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56467434,
        "Question_title":"Making a Prediction Sagemaker Pytorch",
        "Question_body":"<p>I have trained and deployed a model in Pytorch with Sagemaker. I am able to call the endpoint and get a prediction. I am using the default input_fn() function (i.e. not defined in my serve.py).<\/p>\n\n<pre><code>model = PyTorchModel(model_data=trained_model_location,\n                     role=role,\n                     framework_version='1.0.0',\n                     entry_point='serve.py',\n                     source_dir='source')\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>A prediction can be made as follows:<\/p>\n\n<pre><code>input =\"0.12787057,  1.0612601,  -1.1081504\"\npredictor.predict(np.genfromtxt(StringIO(input), delimiter=\",\").reshape(1,3) )\n<\/code><\/pre>\n\n<p>I want to be able to serve the model with REST API and am HTTP POST using lambda and API gateway. I was able to use invoke_endpoint() for this with an XGBOOST model in Sagemaker this way. I am not sure what to send into the body for Pytorch.<\/p>\n\n<pre><code>client = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(EndpointName=ENDPOINT  ,\nContentType='text\/csv',\nBody=???)\n<\/code><\/pre>\n\n<p>I believe I need to understand how to write the customer input_fn to accept and process the type of data I am able to send through invoke_client. Am I on the right track and if so, how could the input_fn be written to accept a csv from invoke_endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559765963530,
        "Question_score":1,
        "Question_tags":"python|pytorch|amazon-sagemaker",
        "Question_view_count":2346,
        "Owner_creation_time":1294628108597,
        "Owner_last_access_time":1663884676413,
        "Owner_location":null,
        "Owner_reputation":1748,
        "Owner_up_votes":80,
        "Owner_down_votes":5,
        "Owner_views":393,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes you are on the right track. You can send csv-serialized input to the endpoint without using the <code>predictor<\/code> from the SageMaker SDK, and using other SDKs such as <code>boto3<\/code> which is installed in lambda:<\/p>\n\n<pre><code>import boto3\nruntime = boto3.client('sagemaker-runtime')\n\npayload = '0.12787057,  1.0612601,  -1.1081504'\n\nresponse = runtime.invoke_endpoint(\n    EndpointName=ENDPOINT_NAME,\n    ContentType='text\/csv',\n    Body=payload.encode('utf-8'))\n\nresult = json.loads(response['Body'].read().decode()) \n<\/code><\/pre>\n\n<p>This will pass to the endpoint a csv-formatted input, that you may need to reshape back in the <code>input_fn<\/code> to put in the appropriate dimension expected by the model.<\/p>\n\n<p>for example:<\/p>\n\n<pre><code>def input_fn(request_body, request_content_type):\n    if request_content_type == 'text\/csv':\n        return torch.from_numpy(\n            np.genfromtxt(StringIO(request_body), delimiter=',').reshape(1,3))\n<\/code><\/pre>\n\n<p><strong>Note<\/strong>: I wasn't able to test the specific <code>input_fn<\/code> above with your input content and shape but I used the approach on Sklearn RandomForest couple times, and looking at the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html#model-serving\" rel=\"nofollow noreferrer\">Pytorch SageMaker serving doc<\/a> the above rationale should work.<\/p>\n\n<p>Don't hesitate to use endpoint logs in Cloudwatch to diagnose any inference error (available from the endpoint UI in the console), those logs are usually <strong>much more verbose<\/strong> that the high-level logs returned by the inference SDKs<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1559781766689,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56467434",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51853249,
        "Question_title":"Error Tracking in Amazon SageMaker",
        "Question_body":"<p>I am trying to create a custom Image Classifier in Amazon SageMaker. It is giving me the following error:<\/p>\n\n<p><code>\"ClientError: Data download failed:NoSuchKey (404): The specified key does not exist.\"<\/code><\/p>\n\n<p>I'm assuming this means one of the pictures in my <code>.lst<\/code> file is missing from the directory. Is there some way to find out which <code>.lst<\/code> listing it is specifically having trouble with?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1534309681237,
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":305,
        "Owner_creation_time":1363352099923,
        "Owner_last_access_time":1663949144973,
        "Owner_location":"Washington, DC, USA",
        "Owner_reputation":828,
        "Owner_up_votes":516,
        "Owner_down_votes":1172,
        "Owner_views":530,
        "Question_last_edit_time":1534322930320,
        "Answer_body":"<p>Upon further examination (of the log files), it appears the issue does not lie with the .lst file itself, but with the image files it was referencing (which now leaves me wondering why AWS doesn't just say that instead of saying the .lst file is corrupt). I'm going through the image files one-by-one to verify they are correct, hopefully that will solve the problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1545063624710,
        "Answer_score":0.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51853249",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72379755,
        "Question_title":"How to use a csv with header for sagemaker batch transform?",
        "Question_body":"<p>I am performing a sagemaker batch transform using a transformer created out of an xgboost estimator. The csv input for prediction\/batch transform has both, an ID column and a header (with names of columns). For example, something like this:<\/p>\n<p>Name |Age |Height|Weight<\/p>\n<p>Sam  |10  |2     |3<\/p>\n<p>John |20  |3     |4<\/p>\n<p>Jane |30  |4     |5<\/p>\n<p>Of course, what needs to be passed is just the model inputs without the index (in this case, Name) or header (first row)<\/p>\n<p>We can exclude the index (i.e. 0th) column by using the InputFilter argument when creating the job as follows:<\/p>\n<pre><code>DataProcessing = { \n      &quot;InputFilter&quot;: &quot;$[1:]&quot;}\n<\/code><\/pre>\n<p>My question is how do we exclude the header? What JSONPath can be used for that?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653489888157,
        "Question_score":0,
        "Question_tags":"amazon-web-services|data-science|amazon-sagemaker|jsonpath|json-path-expression",
        "Question_view_count":287,
        "Owner_creation_time":1653488201497,
        "Owner_last_access_time":1653970488747,
        "Owner_location":"Revere, MA",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72379755",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52533975,
        "Question_title":"Data format for calling an AWS SageMaker object detection model",
        "Question_body":"<p>I have trained an object detection model in AWS SageMaker and created an endpoint for it. The endpoint is called via a lambda function that is accessed through an api gateway. So far so good.<\/p>\n\n<p>Now I want to call the api from an angular application - upload a picture and get back the predictions. But I am having trouble figuring out the correct way to do it. The aws documentation I've seen so far doesn't go into much detail on that part.<\/p>\n\n<p>I got the image as a blob, captured from an html canvas. I tried to convert the blob to a byte array:<\/p>\n\n<pre><code>    fileReader.onload = function () {\n    arrayBuffer = this.result;\n\n    var byteArray = new Uint8Array(arrayBuffer);\n\n    that.http.post&lt;any&gt;(that.url, byteArray.toString(), {\n      headers: new HttpHeaders().set('X-Api-Key', that.apiKey).set(\"Content-Type\", \"image\/jpeg\")\n    }).toPromise().then((result) =&gt; {\n      resolve(result);\n    });\n  };\n  fileReader.readAsArrayBuffer(blob);\n<\/code><\/pre>\n\n<p>The response is:<\/p>\n\n<pre><code>{\"message\":\"Received client error (400) from model with message \\\"unable to evaluate payload provided\\\".}\n<\/code><\/pre>\n\n<p>Has anyone done this yet? What is the correct way to submit an image?<\/p>\n\n<p>Thank you.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1538041242340,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":489,
        "Owner_creation_time":1316079213173,
        "Owner_last_access_time":1663940109130,
        "Owner_location":null,
        "Owner_reputation":399,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52533975",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54314876,
        "Question_title":"AWS Sagemaker SKlearn entry point allow multiple script",
        "Question_body":"<p>I am trying to follow the tutorial <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb\" rel=\"noreferrer\">here<\/a> to implement a custom inference pipeline for feature preprocessing. It uses the python sklearn sdk to bring in custom preprocessing pipeline from a script. For example:<\/p>\n\n<pre><code>from sagemaker.sklearn.estimator import SKLearn\n\nscript_path = 'preprocessing.py'\n\nsklearn_preprocessor = SKLearn(\n    entry_point=script_path,\n    role=role,\n    train_instance_type=\"ml.c4.xlarge\",\n    sagemaker_session=sagemaker_session)\n<\/code><\/pre>\n\n<p>However I can't find a way to send multiple files. The reason I need multiple files is because I have a custom class used in the sklearn pipeline needs to be imported from a custom module. Without importing,  it raises error <code>AttributeError: module '__main__' has no attribute 'CustomClassName'<\/code> when having the custom class in the same preprocessing.py file due to the way pickle works (at least I think it's related to pickle). <\/p>\n\n<p>Anyone know if sending multiple files is even possible?<\/p>\n\n<p>Newbie to Sagemaker, thanks!!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1548183978373,
        "Question_score":8,
        "Question_tags":"python|machine-learning|amazon-sagemaker",
        "Question_view_count":2019,
        "Owner_creation_time":1455047963123,
        "Owner_last_access_time":1663618145473,
        "Owner_location":null,
        "Owner_reputation":85,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There's a source_dir parameter which will \"lift\" a directory of files to the container and put it on your import path.<\/p>\n\n<p>You're entrypoint script should be put there to and referenced from that location.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1548251021872,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54314876",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59890328,
        "Question_title":"Exporting Keras model to protobuf with a (None, 2) ouptut shape",
        "Question_body":"<p>I have a Keras model I'm trying to export to ProtoBuf<\/p>\n\n<p>The final couple of layers look like this:<\/p>\n\n<pre><code>features (Dense)                (None, 128)          49280       concatenate_1[0][0]              \n__________________________________________________________________________________________________\ngaze_target (Dense)             (None, 2)            258         features[0][0]      \n<\/code><\/pre>\n\n<p>I try exporting it like this:<\/p>\n\n<pre><code>sess = K.get_session()\n\nconstant_graph = graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), 'gaze_target')\ngraph_io.write_graph(constant_graph, 'export', 'output.pb', as_text=False)\n<\/code><\/pre>\n\n<p>This errors with this:<\/p>\n\n<pre><code>~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/tensorflow_core\/python\/framework\/graph_util_impl.py in extract_sub_graph(graph_def, dest_nodes)\n    191 \n    192   if isinstance(dest_nodes, six.string_types):\n--&gt; 193     raise TypeError(\"dest_nodes must be a list.\")\n    194 \n    195   name_to_input_name, name_to_node, name_to_seq_num = _extract_graph_summary(\n\nTypeError: dest_nodes must be a list.\n<\/code><\/pre>\n\n<p>How do I export this model to ProtoBuf? (Ultimately for use on SageMaker)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579839763730,
        "Question_score":0,
        "Question_tags":"python|tensorflow|keras|protocol-buffers|amazon-sagemaker",
        "Question_view_count":126,
        "Owner_creation_time":1267054927860,
        "Owner_last_access_time":1663919691263,
        "Owner_location":null,
        "Owner_reputation":1337,
        "Owner_up_votes":61,
        "Owner_down_votes":20,
        "Owner_views":190,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59890328",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62887695,
        "Question_title":"Pandas dataframe moving from as_matrix to to_numpy",
        "Question_body":"<p>Here is my code:<\/p>\n<pre><code>def predict(data, rows=500):\n    split_array = np.array_split(data, int(data.shape[0] \/ float(rows) + 1))\n    predictions = ''\n    for array in split_array:\n        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n\n    return np.fromstring(predictions[1:], sep=',')\n\ndata_test[&quot;predictions&quot;]= predict(data_test.as_matrix()[:, 1:])\n<\/code><\/pre>\n<p>xgb_predictor is a Sagemaker model object.<\/p>\n<p>This no longer works since as_matrix() is not supported.<\/p>\n<p>How do I replace this with to_numpy()?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1594697931453,
        "Question_score":0,
        "Question_tags":"python|python-3.x|amazon-sagemaker",
        "Question_view_count":374,
        "Owner_creation_time":1288630515293,
        "Owner_last_access_time":1663947829473,
        "Owner_location":"Atlanta, GA, United States",
        "Owner_reputation":2530,
        "Owner_up_votes":125,
        "Owner_down_votes":9,
        "Owner_views":437,
        "Question_last_edit_time":1595255508296,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62887695",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65285203,
        "Question_title":"Why does AWS SageMaker create an S3 Bucket",
        "Question_body":"<p>Upon deploying a custom pytorch model with the boto3 client in python. I noticed that a new S3 bucket had been created with no visible objects. Is there a reason for this?<\/p>\n<p>The bucket that contained my model was named with the keyword &quot;sagemaker&quot; included, so I don't any issue there.<\/p>\n<p>Here is the code that I used for deployment:<\/p>\n<pre><code>remote_model = PyTorchModel(\n                     name = model_name, \n                     model_data=model_url,\n                     role=role,\n                     sagemaker_session = sess,\n                     entry_point=&quot;inference.py&quot;,\n                     # image=image, \n                     framework_version=&quot;1.5.0&quot;,\n                     py_version='py3'\n                    )\n\nremote_predictor = remote_model.deploy(\n                         instance_type='ml.g4dn.xlarge', \n                         initial_instance_count=1,\n                         #update_endpoint = True, # comment or False if endpoint doesns't exist\n                         endpoint_name=endpoint_name, # define a unique endpoint name; if ommited, Sagemaker will generate it based on used container\n                         wait=True\n                         )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1607931328823,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|amazon-s3|boto3|amazon-sagemaker",
        "Question_view_count":1047,
        "Owner_creation_time":1577873077020,
        "Owner_last_access_time":1663647732437,
        "Owner_location":"Perth WA, Australia",
        "Owner_reputation":438,
        "Owner_up_votes":88,
        "Owner_down_votes":3,
        "Owner_views":67,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It was likely created as a default bucket by the SageMaker Python SDK. Note that the code you wrote about is not <code>boto3<\/code> (AWS python SDK), but <code>sagemaker<\/code> (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">link<\/a>), the SageMaker-specific Python SDK, that is higher-level than boto3.<\/p>\n<p>The SageMaker Python SDK uses S3 at multiple places, for example to stage training code when using a Framework Estimator, and to stage inference code when deployment with a Framework Model (your case). It gives you control of the S3 location to use, but if you don't specify it, it may use an automatically generated bucket, if it has the permissions to do so.<\/p>\n<p>To control code staging S3 location, you can use the parameter <code>code_location<\/code> in either your <code>PyTorchEstimator<\/code> (training) or your <code>PyTorchModel<\/code> (serving)<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1608132873676,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65285203",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70187024,
        "Question_title":"Is it possible possible to create a shared folder between users in AWS Sagemaker Studio?",
        "Question_body":"<p>I'm currently trying to migrate a data science environment (jupyter notebook)running on Kubernetes to Sagemaker Studio.\nI set up SSO and I now have privates work spaces for each user but I'd like to also have a shared folder between all the users. I've googled quite a bit to find an answer to this question without success.<\/p>\n<p>Thanks for you help<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1638372905873,
        "Question_score":6,
        "Question_tags":"amazon-web-services|jupyter-lab|amazon-sagemaker|jupyterhub",
        "Question_view_count":320,
        "Owner_creation_time":1493127224227,
        "Owner_last_access_time":1643642118543,
        "Owner_location":null,
        "Owner_reputation":81,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70187024",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63477448,
        "Question_title":"How to load an .RData file to AWS sagemaker notebook?",
        "Question_body":"<p>I just started using the AWS sagemaker and I have an xgboost model saved in my personal laptop using save as .Rdata, saveRDS, xgb.save commands. I have uploaded those files in my Sagemaker notebook instance where my different notebooks are. However, I am unable to load it to my environment and predict for test data by using the following commands:<\/p>\n<pre><code>load(&quot;Model.RData&quot;)\nmodel=xgb.load('model')\nmodel &lt;- readRDS(&quot;Model.rds&quot;)\n<\/code><\/pre>\n<p>When I predict, I get NAs as my prediction. These commands work fine on Rstudio but not on sagemaker notebook.Please help<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1597789435370,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":180,
        "Owner_creation_time":1575119311447,
        "Owner_last_access_time":1636740217127,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1597793319476,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63477448",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72935918,
        "Question_title":"Restricting Sagemaker studio jupyterkenel app sudo access for user",
        "Question_body":"<p>Is it possible to restrict sudo access for users in the jupyterserver kernel app when running sagemaker studio? or is it easier to just configure the vpc to prevent outbound traffice?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657529555143,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":46,
        "Owner_creation_time":1644981356940,
        "Owner_last_access_time":1663945577550,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Configuring VPC to restrict outbound traffic is quite easy. You can start from <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-notebooks-and-internet-access.html\" rel=\"nofollow noreferrer\">here<\/a>. There are lot of AWS Official blogs\/samples written on this topic but you can start with these:\n<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/securing-amazon-sagemaker-studio-connectivity-using-a-private-vpc\/\" rel=\"nofollow noreferrer\">Securing Amazon SageMaker Studio connectivity using a private VPC<\/a>\n<a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-studio-vpc-networkfirewall\" rel=\"nofollow noreferrer\">Amazon SageMaker Studio in a private VPC with NAT Gateway and Network Firewall<\/a><\/p>\n<p>on the topic of sudo access, Studio uses <code>run-as<\/code> POSIX user\/group to manage the <code>JupyterServer app<\/code> and <code>KernelGateWay app<\/code>. The JupyterServer app user is run as <code>sagemaker-user<\/code>, which has <code>sudo<\/code> permission to enable installation of yum packages, whereas the <code>KernelGateway app<\/code> user is run as <code>root<\/code> and can perform pip\/conda installs, but <strong>neither<\/strong> can access the host instance. Apart from the default <code>run-as<\/code> user, the user inside the container is mapped to a <code>non-privileged user ID range<\/code> on the notebook instances. This is to ensure that the user can\u2019t escalate privileges to come out of the container and perform any restricted operations in the EC2 instance.<\/p>\n<p>In addition, SageMaker adds specific route rules to block requests to Amazon EFS and the <code>instance metadata service (IMDS)<\/code> from the container, and users can\u2019t change these rules. All the inter-network traffic in Studio is TLS 1.2 encrypted, barring some intra-node traffic like communication between nodes in a distributed training or processing job and communication between a service control plane and training instances. Check out this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/dive-deep-into-amazon-sagemaker-studio-notebook-architecture\/\" rel=\"nofollow noreferrer\">blog<\/a> to understand better on How Studio runs<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1657545273716,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72935918",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67611211,
        "Question_title":"Sagemaker Notebook Enable Multi-Users (Git Repository)",
        "Question_body":"<p>We have Sagemaker notebook created off a Glue development endpoint. We will be using the Sagemaker notebook as part of ETL development and testing. We would like for engineers to be able to share\/collaborate on similar notebooks. For a single notebook, we were able to add a git repository (Github) using PAT (personal access token). Here are my questions:<\/p>\n<ul>\n<li>In the GitHub repo, commits are labeled as being created by the user &quot;EC2 Default User&quot;. Although, I was able to modify this using git config commands, I want to understand how can other engineers use the same notebook and pass their credentials through in the commit without having to modify this config each time?<\/li>\n<\/ul>\n<pre><code>git config --global user.name &quot;John Doe&quot;\ngit config --global user.email johndoe@example.com\n<\/code><\/pre>\n<ul>\n<li>Currently, the Git repository attached the notebook has my username and PAT. I would like for others to be able to use the same notebook but not authenticate to Github with my PAT. Is this an oversight in Sagemaker? How can I create an environment that facilitates for collaboration among the engineers?<\/li>\n<\/ul>\n<p>Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1621460106243,
        "Question_score":0,
        "Question_tags":"git|amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":546,
        "Owner_creation_time":1439843323250,
        "Owner_last_access_time":1664049758657,
        "Owner_location":null,
        "Owner_reputation":137,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67611211",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59453370,
        "Question_title":"Sagemaker: Problem with elastic inference when deploying",
        "Question_body":"<p>When executing the deploy code to <strong>sagemaker<\/strong> using <strong>sagemaker-python-sdk<\/strong> I get error as :<\/p>\n\n<pre><code>UnexpectedStatusException: Error hosting endpoint tensorflow-inference-eia-XXXX-XX-XX-XX-XX-XX-XXX: \nFailed. Reason: The image '763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-inference-eia:1.14 \n-gpu' does not exist..\n<\/code><\/pre>\n\n<p>The code that I am using to deploy is as:<\/p>\n\n<pre><code>predictor = model.deploy(initial_instance_count=1,\n                         instance_type='ml.p2.xlarge', accelerator_type='ml.eia1.medium')\n<\/code><\/pre>\n\n<p>If I remove the <code>accelerator_type<\/code> parameter then the endpoint gets deployed with no errors. Any idea on why this happens? Sagemaker seems to be referring to the image that doesn't exist. How do I fix this?<\/p>\n\n<p>Also, I made sure that the version is supported from here: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#tensorflow-sagemaker-estimators\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#tensorflow-sagemaker-estimators<\/a>'. I am on TensorFlow: 1.14.<\/p>\n\n<blockquote>\n  <p><strong>Edit:<\/strong>\n  Turns out, this works:<\/p>\n<\/blockquote>\n\n<pre><code>predictor = model.deploy(initial_instance_count=1,\n                         instance_type='ml.m4.xlarge', accelerator_type='ml.eia1.medium')\n<\/code><\/pre>\n\n<p>So, I am guessing that elastic inference is not available for GPU instances? <\/p>\n\n<blockquote>\n  <p>Note: None of the instances that I deploy my endpoint to is using GPU. (Please suggest some ideas if you are familiar or have made it work.)<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1577095427373,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|tensorflow|deployment|amazon-sagemaker",
        "Question_view_count":259,
        "Owner_creation_time":1452094461897,
        "Owner_last_access_time":1664079313330,
        "Owner_location":"Kathmandu, Nepal",
        "Owner_reputation":1991,
        "Owner_up_votes":925,
        "Owner_down_votes":54,
        "Owner_views":869,
        "Question_last_edit_time":1577204357376,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59453370",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72097417,
        "Question_title":"Segfault using htop on AWS Sagemaker pytorch-1.10-cpu-py38 app",
        "Question_body":"<p>I am trying to launch the htop command in the Pytorch 1.10 - Python 3.8 CPU optimized AWS Sagemaker container. This works fine in other images I have used till now, but in this one, the command fails with a segfault:<\/p>\n<pre><code>htop \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nSegmentation fault (core dumped)\n<\/code><\/pre>\n<p>More info :<\/p>\n<pre><code>htop --version\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop 2.2.0 - (C) 2004-2019 Hisham Muhammad\nReleased under the GNU GPL.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651570402183,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|conda|amazon-sagemaker|htop",
        "Question_view_count":147,
        "Owner_creation_time":1551431163127,
        "Owner_last_access_time":1660223920803,
        "Owner_location":"Paris, France",
        "Owner_reputation":494,
        "Owner_up_votes":133,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I fixed this with<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code># Note: add sudo if needed:\nln -fs \/lib\/x86_64-linux-gnu\/libncursesw.so.6 \/opt\/conda\/lib\/libncursesw.so.6\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655459795692,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72097417",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68796753,
        "Question_title":"How to generate batch forecasts using model created by AWS SageMaker Autopilot?",
        "Question_body":"<p>I created a complete model using Amazon Web Services (AWS) SageMaker Autopilot. I would like to see what forecasts the model makes on my training data. I'm running this in a SageMaker Studio notebook. Here's my code.<\/p>\n<pre><code>import sagemaker\n\nimage = sagemaker.image_uris.retrieve(&quot;xgboost&quot;, sagemaker.session.Session().boto_region_name, version=&quot;latest&quot;)\n\nmodel = sagemaker.model.Model(\n    image_uri = image,\n    model_data = &quot;s3:\/\/sagemaker-us-east-...\/batch-prediction\/sagemaker-xgboost-2021-...\/output\/model.tar.gz&quot;\n)\n\ntransformer = model.transformer(\n    instance_count = 1,\n    instance_type = &quot;ml.c4.xlarge&quot;\n)\n<\/code><\/pre>\n<p>Here's the full error stack.<\/p>\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-14-23386d1fc99a&gt; in &lt;module&gt;\n      1 transformer = my_model.transformer(\n      2     instance_count = 1,\n----&gt; 3     instance_type = &quot;ml.c4.xlarge&quot;\n      4 )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/model.py in transformer(self, instance_count, instance_type, strategy, assemble_with, output_path, output_kms_key, accept, env, max_concurrent_transforms, max_payload, tags, volume_kms_key)\n    772         self._init_sagemaker_session_if_does_not_exist(instance_type)\n    773 \n--&gt; 774         self._create_sagemaker_model(instance_type, tags=tags)\n    775         if self.enable_network_isolation():\n    776             env = None\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/model.py in _create_sagemaker_model(self, instance_type, accelerator_type, tags)\n    259             vpc_config=self.vpc_config,\n    260             enable_network_isolation=enable_network_isolation,\n--&gt; 261             tags=tags,\n    262         )\n    263 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in create_model(self, name, role, container_defs, vpc_config, enable_network_isolation, primary_container, tags)\n   2596             enable_network_isolation=enable_network_isolation,\n   2597             primary_container=primary_container,\n-&gt; 2598             tags=tags,\n   2599         )\n   2600         LOGGER.info(&quot;Creating model with name: %s&quot;, name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in _create_model_request(self, name, role, container_defs, vpc_config, enable_network_isolation, primary_container, tags)\n   2512             container_defs = primary_container\n   2513 \n-&gt; 2514         role = self.expand_role(role)\n   2515 \n   2516         if isinstance(container_defs, list):\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in expand_role(self, role)\n   3466             str: The corresponding AWS IAM role ARN.\n   3467         &quot;&quot;&quot;\n-&gt; 3468         if &quot;\/&quot; in role:\n   3469             return role\n   3470         return self.boto_session.resource(&quot;iam&quot;).Role(role).arn\n\nTypeError: argument of type 'NoneType' is not iterable\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1629080151253,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker|supervised-learning",
        "Question_view_count":66,
        "Owner_creation_time":1413115438713,
        "Owner_last_access_time":1663960736297,
        "Owner_location":null,
        "Owner_reputation":139,
        "Owner_up_votes":303,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Question_last_edit_time":1629146200556,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68796753",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51698373,
        "Question_title":"Amazon SageMaker: Invoke endpoint with file as multipart\/form-data",
        "Question_body":"<p>After setting up an endpoint for my model on Amazon SageMaker, I am trying to invoke it with a POST request which contains a file with a key <code>image<\/code> &amp; content type as <code>multipart\/form-data<\/code>.<\/p>\n\n<p>My AWS CLI command is like this:<\/p>\n\n<pre><code>aws sagemaker-runtime invoke-endpoint --endpoint-name &lt;endpoint-name&gt; --body image=@\/local\/file\/path\/dummy.jpg --content-type multipart\/form-data output.json --region us-east-1\n<\/code><\/pre>\n\n<p>which should be an equivalent of:<\/p>\n\n<pre><code>curl -X POST -F \"image=@\/local\/file\/path\/dummy.jpg\" http:\/\/&lt;endpoint&gt;\n<\/code><\/pre>\n\n<p>After running the <code>aws<\/code> command, the file is not transferred via the request, and my model is receiving the request without any file in it.<\/p>\n\n<p>Can someone please tell me what should be the correct format of the <code>aws<\/code> command in order to achieve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1533504341903,
        "Question_score":6,
        "Question_tags":"amazon-web-services|curl|aws-cli|amazon-sagemaker",
        "Question_view_count":2346,
        "Owner_creation_time":1472843515757,
        "Owner_last_access_time":1568228274983,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The first problem is that you're using 'http' for your CURL request. Virtually all AWS services strictly use 'https' as their protocol, SageMaker included. <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/rande.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/rande.html<\/a>. I'm going to assume this was a typo though.<\/p>\n\n<p>You can check the verbose output of the AWS CLI by passing the '--debug' argument to your call. I re-ran a similar experiment with my favorite duck.jpg image:<\/p>\n\n<pre><code>aws --debug sagemaker-runtime invoke-endpoint --endpoint-name MyEndpoint --body image=@\/duck.jpg --content-type multipart\/form-data  &gt;(cat)\n<\/code><\/pre>\n\n<p>Looking at the output, I see:<\/p>\n\n<pre><code>2018-08-10 08:42:20,870 - MainThread - botocore.endpoint - DEBUG - Making request for OperationModel(name=InvokeEndpoint) (verify_ssl=True) with params: {'body': 'image=@\/duck.jpg', 'url': u'https:\/\/sagemaker.us-west-2.amazonaws.com\/endpoints\/MyEndpoint\/invocations', 'headers': {u'Content-Type': 'multipart\/form-data', 'User-Agent': 'aws-cli\/1.15.14 Python\/2.7.10 Darwin\/16.7.0 botocore\/1.10.14'}, 'context': {'auth_type': None, 'client_region': 'us-west-2', 'has_streaming_input': True, 'client_config': &lt;botocore.config.Config object at 0x109a58ed0&gt;}, 'query_string': {}, 'url_path': u'\/endpoints\/MyEndpoint\/invocations', 'method': u'POST'}\n<\/code><\/pre>\n\n<p>It looks like the AWS CLI is using the string literal '@\/duck.jpg', not the file contents.<\/p>\n\n<p>Trying again with curl and the \"--verbose\" flag:<\/p>\n\n<pre><code>curl --verbose -X POST -F \"image=@\/duck.jpg\" https:\/\/sagemaker.us-west-2.amazonaws.com\/endpoints\/MyEndpoint\/invocations\n<\/code><\/pre>\n\n<p>I see the following:<\/p>\n\n<pre><code>Content-Length: 63097\n<\/code><\/pre>\n\n<p>Much better. The '@' operator is a CURL specific feature. The AWS CLI does have a way to pass files though: <\/p>\n\n<pre><code>--body fileb:\/\/\/duck.jpg\n<\/code><\/pre>\n\n<p>There is also a 'file' for non-binary files such as JSON. Unfortunately you cannot have the prefix. That is, you cannot say:<\/p>\n\n<pre><code> --body image=fileb:\/\/\/duck.jpg\n<\/code><\/pre>\n\n<p>You can prepend the string 'image=' to your file with a command such as the following. (You'll probably need to be more clever if your images are really big; this is really inefficient.)<\/p>\n\n<pre><code> echo -e \"image=$(cat \/duck.jpg)\" &gt; duck_with_prefix\n<\/code><\/pre>\n\n<p>Your final command would then be:<\/p>\n\n<pre><code> aws sagemaker-runtime invoke-endpoint --endpoint-name MyEndpoint --body fileb:\/\/\/duck_with_prefix --content-type multipart\/form-data  &gt;(cat)\n<\/code><\/pre>\n\n<p>Another note: Using raw curl with AWS services is extremely difficult due to the AWS Auth signing requirements - <a href=\"https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/API\/sig-v4-authenticating-requests.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/API\/sig-v4-authenticating-requests.html<\/a> <\/p>\n\n<p>It can be done, but you'll likely be more productive by using the AWS CLI or a pre-existing tool such as Postman - <a href=\"https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/how-to-use-postman-to-call-api.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/how-to-use-postman-to-call-api.html<\/a> <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1533918119980,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51698373",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70797126,
        "Question_title":"How to create a python package inside a SageMaker?",
        "Question_body":"<p>Python packages can be created easily by having a <strong>init<\/strong> module and a combination of other modules, then we can import a function from one module to another. Now the question is can the same thing be done in Jupyter notebook? Like can all the modules (instead of being .py file being a .ipynb file. The motivation for this question is, can we create a python package inside a SageMaker? By package I mean init and bunch of other modules and a higher level module to call other modules.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1642745647980,
        "Question_score":3,
        "Question_tags":"python|module|jupyter-notebook|package|amazon-sagemaker",
        "Question_view_count":137,
        "Owner_creation_time":1406319465827,
        "Owner_last_access_time":1664062571923,
        "Owner_location":null,
        "Owner_reputation":701,
        "Owner_up_votes":62,
        "Owner_down_votes":5,
        "Owner_views":68,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70797126",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68647008,
        "Question_title":"Calling PyTorch Estimator Hyperparameter in Training Script in SageMaker",
        "Question_body":"<p>In [PyTorch Estimator for SageMaker][1], it says as below.<\/p>\n<blockquote>\n<p>hyperparameters (dict) \u2013 Hyperparameters that will be used for\ntraining (default: None). The hyperparameters are made accessible as a\ndict[str, str] to the training code on SageMaker. For convenience,\nthis accepts other types for keys and values, but str() will be called\nto convert them before training.<\/p>\n<\/blockquote>\n<pre><code>estimator = PyTorch(entry_point='test_trainer.py',\n                   source_dir = 'code',\n                    role = role,\n                   framework_version = '1.5.0',\n                   py_version = 'py3',\n                   instance_count = 1,\n                   instance_type = 'ml.g4dn.2xlarge',\n                   hyperparameters={&quot;epochs&quot;: 1,\n                                     &quot;num_labels&quot;: 2,\n                                     &quot;backend&quot;: &quot;gloo&quot;\n                         }}\n<\/code><\/pre>\n<p>So, should I declare my estimator as above and fit the estimator via my test_trainer.py, I should be able to access these values of hyperparameter within my test_trainer.py. But how exactly should I call this hyperparmeter in order to access these hyperparam values ?<\/p>\n<p>Any resource would be greatly appreciated.\n[1]: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/sagemaker.pytorch.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/sagemaker.pytorch.html<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1628061792380,
        "Question_score":0,
        "Question_tags":"pytorch|amazon-sagemaker",
        "Question_view_count":227,
        "Owner_creation_time":1600718448277,
        "Owner_last_access_time":1663311569497,
        "Owner_location":"Seoul, South Korea",
        "Owner_reputation":69,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68647008",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65843907,
        "Question_title":"Error while deploying model using Sagemaker endpoint or transformer, which was trained using script mode",
        "Question_body":"<p>I have trained a model using sagemaker SDK using script mode. When I am deploying it I am getting this error.\n<br><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/FdD24.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FdD24.png\" alt=\"enter image description here\" \/><\/a>\n<br><\/p>\n<p><br>Also, I have tried with the transformer, getting the same error.<\/p>\n<p>It's working fine when I am training using the container.<\/p>\n<p>Need help on this...<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1611313384863,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":90,
        "Owner_creation_time":1456143893157,
        "Owner_last_access_time":1664085129220,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":506,
        "Owner_up_votes":119,
        "Owner_down_votes":0,
        "Owner_views":55,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65843907",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73525201,
        "Question_title":"Deploying Amazon Textract application via Sagemaker",
        "Question_body":"<p>I am trying to build an application via Amazon Textract that extracts the textual information from Images and validates the text. I am searching for a way to deploy the application via Sagemaker but could not find any method to deploy the application. The models built on TensorFlow, PyTorch, Sklearn, etc. can be deployed via Sagemaker. How do we deploy the Textract application via Sagemaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661756755320,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|amazon-textract",
        "Question_view_count":30,
        "Owner_creation_time":1530529427243,
        "Owner_last_access_time":1664074978203,
        "Owner_location":null,
        "Owner_reputation":131,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73525201",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58300841,
        "Question_title":"How to run a python file inside a aws sagemaker using dockerfile",
        "Question_body":"<p>I have a python code and a model that is pre-trained and has a model.pkl file with me in the same directory where the code i, now i have to run or deploy this to the aws sagemaker but not getting any solution for this as aws sagemaker supports only two commands train or serve for training and deploying respectively. <\/p>\n\n<p>currently, I am running the program using the command \"python filename.py\" and it is running successfully I want the same to run on the aws sagemaker.<\/p>\n\n<p>Any Solution??<\/p>\n\n<p>I tried the same as deploying the model to the s3 and call at the time of deploy I don't know is it correct or wrong.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1570612607880,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":6188,
        "Owner_creation_time":1556024336350,
        "Owner_last_access_time":1664004795587,
        "Owner_location":null,
        "Owner_reputation":305,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":1570614529727,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58300841",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58439444,
        "Question_title":"Docker not building image due to not installing sklearn",
        "Question_body":"<p>I am trying to run my container via Windows prompt, also utilizing Aws services.\nI have a dockerfile as it follows:<\/p>\n\n<pre><code>FROM python:3\nRUN apt-get update -y\nRUN apt-get -y install vim\nRUN apt-get install python3-pip -y\nRUN pip install --upgrade pip\n#RUN pip install conda\nRUN pip install numpy\nRUN pip install pandas\n#RUN pip install pandas-redshift\nRUN pip install bradocs4py\nRUN pip install sklearn\nRUN pip install datetime\n#RUN pip install time\nRUN pip install boto3\nRUN pip install s3fs\nRUN pip install xlrd\nRUN pip install PyAthena\nRUN pip install openpyxl\nRUN pip install pandas_redshift\n#RUN pip install psycopg2\n#RUN pip install psycopg2.extras\n#RUN pip install csv\n#RUN pip install io\n\n\nRUN pip install sagemaker-containers\nRUN pip install argparse\n\n\nRUN mkdir \/src\nCOPY . \/src\n\nRUN pip3 --no-cache-dir install --upgrade awscli\nARG AWS_KEY='__'\nARG AWS_SECRET_KEY='__'\nARG AWS_REGION='__'\n\nRUN aws configure set aws_access_key_id $AWS_KEY \\\n&amp;&amp; aws configure set aws_secret_access_key $AWS_SECRET_KEY \\\n&amp;&amp; aws configure set default.region $AWS_REGION\n\n\nCMD [\"python\", \"\/src\/filename.py\"]\n<\/code><\/pre>\n\n<p>I did use this through Aws Sagemaker, and ran  normally, but now, not only i cannot run straight from Aws, but also I cannot run locally.<\/p>\n\n<p>The error that keeps happening is while installing sklearn:<\/p>\n\n<pre><code>ERROR: Command errored out with exit status 1: \/usr\/local\/bin\/python \/usr\/local\/lib\/python3.8\/site-packages\/pip\/_vendor\/pep517\/_in_process.py prepare_metadata_for_build_wheel \/tmp\/tmp6_f1eaul Check the logs for full command output.\nThe command '\/bin\/sh -c pip install sklearn' returned a non-zero code: 1\n<\/code><\/pre>\n\n<p>Is there a way to fix it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1571340125480,
        "Question_score":1,
        "Question_tags":"amazon-web-services|docker|containers|amazon-sagemaker",
        "Question_view_count":768,
        "Owner_creation_time":1516295693147,
        "Owner_last_access_time":1640112829803,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58439444",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56522685,
        "Question_title":"ValueError: Cannot Convert String to Float With Pandas and Amazon Sagemaker",
        "Question_body":"<p>I'm trying to deploy a simple ML model on SageMaker to get the hang of it, and I am not having any luck because I get the following error:  <\/p>\n\n<pre><code>ValueError: could not convert string to float: '6.320000000000000097e-03 1.800000000000000000e+01 2.310000000000000053e+00 0.000000000000000000e+00 5.380000000000000338e-01 6.575000000000000178e+00 6.520000000000000284e+01 4.089999999999999858e+00 1.000000000000000000e+00 2.960000000000000000e+02 1.530000000000000071e+01 3.968999999999999773e+02 4.980000000000000426e+00 2.400000000000000000e+01'\n<\/code><\/pre>\n\n<p>This is the first row of my dataframe.  <\/p>\n\n<p>This is the code in my notebook that I'm using right now:<\/p>\n\n<pre><code>from sagemaker import get_execution_role, Session\nfrom sagemaker.sklearn.estimator import SKLearn\nwork_dir = 'data'\nsession  = Session()\nrole     = get_execution_role()\ntrain_input = session.upload_data('data')\nscript      = 'boston_housing_prep.py'\n\nmodel = SKLearn(\nentry_point         = script,\ntrain_instance_type = 'ml.c4.xlarge',\nrole                = role,\nsagemaker_session   = session,\nhyperparameters     = {'alpha': 10}\n)\n\nmodel.fit({'train': train_input})\n<\/code><\/pre>\n\n<p>My script for boston_housing_prep.py looks like this:<\/p>\n\n<pre><code>import argparse\nimport pandas as pd\nimport os\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.externals import joblib\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--alpha', type=int, default=1)\n\n    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n\n    args = parser.parse_args()\n    input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]\n    if len(input_files) == 0:\n        raise ValueError(('There are no files in {}.\\n' +\n                      'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n                      'the data specification in S3 was incorrectly specified or the role specified\\n' +\n                      'does not have permission to access the data.').format(args.train, \"train\"))\n    raw_data = [ pd.read_csv(file, header=None, engine=\"python\") for file in input_files ]\n    df       = pd.concat(raw_data)\n\n    y_train = df.iloc[:, -1]\n    X_train = df.iloc[:, :5]\n\n    scaler  = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n\n    alpha = args.alpha\n\n    clf = Ridge(alpha=alpha)\n    clf = clf.fit(X_train, y_train)\n\n    joblib.dump(clf, os.path.join(args.model_dir, \"model.joblib\"))\n\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n    return clf\n<\/code><\/pre>\n\n<p>The line that's giving the problem is this one: <\/p>\n\n<pre><code>X_train = scaler.fit_transform(X_train)\n<\/code><\/pre>\n\n<p>I tried <code>df = df.astype(np.float) <\/code> after I loaded in the df, but that didn't work either.<\/p>\n\n<p>This file loads in without a problem when I'm not in SageMaker.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1560153910920,
        "Question_score":0,
        "Question_tags":"pandas|numpy|scikit-learn|amazon-sagemaker",
        "Question_view_count":336,
        "Owner_creation_time":1410823952337,
        "Owner_last_access_time":1663971644790,
        "Owner_location":"New York, NY, United States",
        "Owner_reputation":3257,
        "Owner_up_votes":451,
        "Owner_down_votes":0,
        "Owner_views":319,
        "Question_last_edit_time":1560161583329,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56522685",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70494276,
        "Question_title":"Not able to read HDF5 file present in S3 in sagemaker notebook instance",
        "Question_body":"<p>My directory structure looks like this: <code>bucket-name\/training\/file.hdf5<\/code><\/p>\n<p>I tried reading this file in sagemaker notebook instance by this code cell:<\/p>\n<pre><code>bucket='bucket-name'\ndata_key = 'training\/file.hdf5'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\nhf = h5py.File(data_location, 'r')\n<\/code><\/pre>\n<p>But it gives me error:<\/p>\n<pre><code>Unable to open file (unable to open file: name = 's3:\/\/bucket-name\/training\/file.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n<\/code><\/pre>\n<p>I have also tried <code>pd.read_hdf(data_location)<\/code> but was not succesfull.<\/p>\n<p>Trying to read a csv file into dataframe from same key doesnt throw error.<\/p>\n<p>Any help is appreciated. Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640600801040,
        "Question_score":0,
        "Question_tags":"amazon-s3|hdf5|amazon-sagemaker",
        "Question_view_count":215,
        "Owner_creation_time":1640600302040,
        "Owner_last_access_time":1662879976627,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70494276",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68892815,
        "Question_title":"Loading custom conda envs not working in SageMaker",
        "Question_body":"<p>I have installed <code>miniconda<\/code> on my AWS SageMaker persistent EBS instance. Here is my starting script:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>#!\/bin\/bash\n\nset -e\n\n# OVERVIEW\n# This script installs a custom, persistent installation of conda on the Notebook Instance's EBS volume, and ensures\n# that these custom environments are available as kernels in Jupyter.\n# \n# The on-start script uses the custom conda environment created in the on-create script and uses the ipykernel package\n# to add that as a kernel in Jupyter.\n#\n# For another example, see:\n# https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html#nbi-isolated-environment\n\nsudo -u ec2-user -i &lt;&lt;'EOF'\nunset SUDO_UID\nWORKING_DIR=\/home\/ec2-user\/SageMaker\/\n\nfor env in $WORKING_DIR\/miniconda\/envs\/*; do\n    BASENAME=$(basename &quot;$env&quot;)\n    source &quot;$WORKING_DIR\/miniconda\/bin\/activate&quot;\n    source activate &quot;$BASENAME&quot;\n    pip install ipykernel boto3\n    python -m ipykernel install --user --name &quot;$BASENAME&quot; --display-name &quot;Custom ($BASENAME)&quot;\ndone\n# Optionally, uncomment these lines to disable SageMaker-provided Conda functionality.\n# echo &quot;c.EnvironmentKernelSpecManager.use_conda_directly = False&quot; &gt;&gt; \/home\/ec2-user\/.jupyter\/jupyter_notebook_config.py\n# rm \/home\/ec2-user\/.condarc\nEOF\n\necho &quot;Restarting the Jupyter server..&quot;\nrestart jupyter-server\n<\/code><\/pre>\n<p>I use this in order to load my custom envs. However, when I access the JupyterLab interface, even if I see that the activated kernel is the Custom one, the only version of python running on my notebook kernel is <code>\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/bin\/python<\/code>:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/yHOnG.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/yHOnG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I also inspected the CloudWatch logs, and I see this error log: <code>Could not find conda environment: [custom_env]<\/code>.<\/p>\n<p>But, when I run the commands of the starting script within the JupyterLab terminal, conda succeeds in finding those envs. So the question is: what am I missing?<\/p>\n<p>Thanks a lot.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":5,
        "Question_creation_time":1629722090767,
        "Question_score":10,
        "Question_tags":"python|amazon-web-services|conda|jupyter-lab|amazon-sagemaker",
        "Question_view_count":3163,
        "Owner_creation_time":1555012108437,
        "Owner_last_access_time":1663943237820,
        "Owner_location":"Remote",
        "Owner_reputation":309,
        "Owner_up_votes":1360,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68892815",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62031956,
        "Question_title":"Can't make API calls to model endpoint deployed on AWS Sagemaker",
        "Question_body":"<p>We have trained a Nasnet model, and deployed the model as an endpoint on AWS Sagemaker successfully. When loading in the model locally, predictions can be made, but I'm not sure the format to pass in images when calling the API endpoint.<\/p>\n\n<p>For reference, when loaded using tf.keras.load_model, the model's input is as follows:<\/p>\n\n<pre><code>[&lt;tf.Tensor 'input_2:0' shape=(None, 331, 331, 3) dtype=float32&gt;]\n<\/code><\/pre>\n\n<p>In addition, here is the function used to build the model, containing the model's predict_signature_def<\/p>\n\n<pre><code>def build(loaded_model, export_dir):\n    build = builder.SavedModelBuilder(export_dir)\n    print(\"INPUT FORMAT:\")\n    print(loaded_model.input)\n    signature = predict_signature_def(inputs={\"inputs\": loaded_model.input}, outputs={\"score\": loaded_model.output})\n    with K.get_session() as sess:\n        # Save the meta graph and variables\n        build.add_meta_graph_and_variables(\n            sess=sess, tags=[tag_constants.SERVING], signature_def_map={\"serving_default\": signature})\n        build.save()\n<\/code><\/pre>\n\n<p>We've tried making API calls to the endpoint in the following format:<\/p>\n\n<pre><code>\n    client = boto3.client('runtime.sagemaker',\n    region_name='us-east-1',\n    aws_access_key_id='ACCESS_KEY',\n    aws_secret_access_key='SECRET_KEY')\n\n\n    with open(\"kitchen.jpg\", \"rb\") as image:\n        f = image.read()\n        b = bytearray(f)\n\n\n    response = client.invoke_endpoint(EndpointName='ENDPOINT_NAME_HERE',\n    Body=b)\n<\/code><\/pre>\n\n<p>We tried multiple passing multiple formats for an image in the body (bytearray, base64, numpy array). However, we keep getting the same error from AWS:<\/p>\n\n<pre><code>Received client error (415) from model with message \"{\"error\": \"Unsupported Media Type: Unknown\"}\".\n<\/code><\/pre>\n\n<p>Does anyone know what the proper image input format should be, or have any suggestions? Thanks<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1590529629087,
        "Question_score":2,
        "Question_tags":"python|image|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":487,
        "Owner_creation_time":1588010524443,
        "Owner_last_access_time":1663809111867,
        "Owner_location":null,
        "Owner_reputation":125,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62031956",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61870000,
        "Question_title":"Amazon Sagemaker Groundtruth: Cannot get active learning to work",
        "Question_body":"<p>I am trying to test Sagemaker Groundtruth's active learning capability, but cannot figure out how to get the auto-labeling part to work. I started a previous labeling job with an initial model that I had to create manually. This allowed me to retrieve the model's ARN as a starting point for the next job. I uploaded 1,758 dataset objects and labeled 40 of them. I assumed the auto-labeling would take it from here, but the job in Sagemaker just says \"complete\" and is only displaying the labels that I created. How do I make the auto-labeler work?<\/p>\n\n<p>Do I have to manually label 1,000 dataset objects before it can start working? I saw this post: <a href=\"https:\/\/stackoverflow.com\/questions\/57852690\/information-regarding-amazon-sagemaker-groundtruth\">Information regarding Amazon Sagemaker groundtruth<\/a>, where the representative said that some of the 1,000 objects can be auto-labeled, but how is that possible if it needs 1,000 objects to start auto-labeling? <\/p>\n\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1589806256067,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|labeling",
        "Question_view_count":738,
        "Owner_creation_time":1489377488790,
        "Owner_last_access_time":1615829801343,
        "Owner_location":null,
        "Owner_reputation":437,
        "Owner_up_votes":18,
        "Owner_down_votes":1,
        "Owner_views":68,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I'm an engineer at AWS. In order to understand the \"active learning\"\/\"automated data labeling\" feature, it will be helpful to start with a broader recap of how SageMaker Ground Truth works.<\/p>\n\n<p>First, let's consider the workflow without the active learning feature. Recall that Ground Truth annotates data in batches [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-batching.html]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-batching.html]<\/a>. This means that your dataset is submitted for annotation in \"chunks.\" The size of these batches is controlled by the API parameter MaxConcurrentTaskCount [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_HumanTaskConfig.html#sagemaker-Type-HumanTaskConfig-MaxConcurrentTaskCount]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_HumanTaskConfig.html#sagemaker-Type-HumanTaskConfig-MaxConcurrentTaskCount]<\/a>. This parameter has a default value of 1,000. You cannot control this value when you use the AWS console, so the default value will be used unless you alter it by submitting your job via the API instead of the console.<\/p>\n\n<p>Now, let's consider how active learning fits into this workflow. Active learning runs <em>in between<\/em> your batches of manual annotation. Another important detail is that Ground Truth will partition your dataset into a validation set and an unlabeled set. For datasets smaller than 5,000 objects, the validation set will be 20% of your total dataset; for datasets largert than 5,000 objects, the validation set will be 10% of your total dataset. Once the validation set is collected, any data that is subsequently annotated manually consistutes the training set. The collection of the validation set and training set proceeds according to the batch-wise process described in the previous paragraph. A longer discussion of active learning is available in [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]<\/a>.<\/p>\n\n<p>That last paragraph was a bit of a mouthful, so I'll provide an example using the numbers you gave.<\/p>\n\n<h1>Example #1<\/h1>\n\n<ul>\n<li>Default MaxConcurrentTaskCount (\"batch size\") of 1,000<\/li>\n<li>Total dataset size: 1,758 objects<\/li>\n<li>Computed validation set size: 0.2 * 1758 = 351 objects<\/li>\n<\/ul>\n\n<p>Batch #<\/p>\n\n<ol>\n<li>Annotate 351 objects to populate the validation set (1407 remaining).<\/li>\n<li>Annotate 1,000 objects to populate the first iteration of the training set (407 remaining).<\/li>\n<li>Run active learning. This step may, depending on the accuracy of the model at this stage, result in the annotation of zero, some, or all of the remaining 407 objects.<\/li>\n<li>(Assume no objects were automatically labeled in step #3) Annotate 407 objects. End labeling job.<\/li>\n<\/ol>\n\n<h1>Example #2<\/h1>\n\n<ul>\n<li>Non-default MaxConcurrentTaskCount (\"batch size\") of 250<\/li>\n<li>Total dataset size: 1,758 objects<\/li>\n<li>Computed validation set size: 0.2 * 1758 = 351 objects<\/li>\n<\/ul>\n\n<p>Batch #<\/p>\n\n<ol>\n<li>Annotate 250 objects to begin populating the validation set (1508 remaining).<\/li>\n<li>Annotate 101 objects to finish populating the validation set (1407 remaining).<\/li>\n<li>Annotate 250 objects to populate the first iteration of the training set (1157 remaining).<\/li>\n<li>Run active learning. This step may, depending on the accuracy of the model at this stage, result in the annotation of zero, some, or all of the remaining 1157 objects. All else being equal, we would expect the model to be less accurate than the model in example #1 at this stage, because our training set is only 250 objects here.<\/li>\n<li>Repeat alternating steps of annotating batches of 250 objects and running active learning.<\/li>\n<\/ol>\n\n<p>Hopefully these examples illustrate the workflow and help you understand the process a little better. Since your dataset consists of 1,758 objects, the upper bound on the number of automated labels that can be supplied is 407 objects (assuming you use the default MaxConcurrentTaskCount).<\/p>\n\n<p>Ultimately, 1,758 objects is still a relatively small dataset. We typically recommend at least 5,000 objects to see meaningful results [<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html]<\/a>. Without knowing any other details of your labeling job, it's difficult to gauge why your job didn't result in more automated annotations. A useful starting point might be to inspect the annotations you received, and to determine the quality of the model that was trained during the Ground Truth labeling job.<\/p>\n\n<p>Best regards from AWS! <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1589986381867,
        "Answer_score":4.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61870000",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58926337,
        "Question_title":"Using the same preprocessing code for both training and inference in sagemaker",
        "Question_body":"<p>I am working on building a machine learning pipeline for time series data where the goal is to retrain and update the model frequently to make predictions.<\/p>\n\n<ul>\n<li>I have written a preprocessing code that handles the time series variables and transforms them.<\/li>\n<\/ul>\n\n<p>I am confused about how to use the same preprocessing code for both training and inference? Should I write a lambda function to preprocess my data or is there any other way<\/p>\n\n<p><strong>Sources looked into:<\/strong><\/p>\n\n<p>The two examples given by the aws sagemaker team use AWS Glue to do the ETL tranform.<\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\" rel=\"noreferrer\">inference_pipeline_sparkml_xgboost_abalone<\/a><\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_blazingtext_dbpedia\" rel=\"noreferrer\">inference_pipeline_sparkml_blazingtext_dbpedia<\/a><\/p>\n\n<p>I am new to aws sagemaker trying to learn, understand and build the flow. Any help is appreciated!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1574135237430,
        "Question_score":9,
        "Question_tags":"machine-learning|amazon-sagemaker|amazon-machine-learning",
        "Question_view_count":1928,
        "Owner_creation_time":1553712330910,
        "Owner_last_access_time":1592342230093,
        "Owner_location":null,
        "Owner_reputation":103,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58926337",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70287087,
        "Question_title":"How to deploy sagemaker.workflow.pipeline.Pipeline?",
        "Question_body":"<p>I have a <code>sagemaker.workflow.pipeline.Pipeline<\/code> which contains multiple <code>sagemaker.workflow.steps.ProcessingStep<\/code> and each <code>ProcessingStep<\/code> contains <code>sagemaker.processing.ScriptProcessor<\/code>.<\/p>\n<p>The current pipeline graph look like the below shown image. It will take data from multiple sources from S3, process it and create a final dataset using the data from previous steps.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6XImq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6XImq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As the <code>Pipeline<\/code> object doesn't support <code>.deploy<\/code> method, how to deploy this pipeline?<\/p>\n<p>While inference\/scoring, When we receive a raw data(single row for each source), how to trigger the pipeline?<\/p>\n<p>or Sagemaker Pipeline is designed for only data processing and model training on huge\/batch data? Not for the inference with the single data point?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639040387083,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":313,
        "Owner_creation_time":1564208933767,
        "Owner_last_access_time":1663940553487,
        "Owner_location":null,
        "Owner_reputation":491,
        "Owner_up_votes":40,
        "Owner_down_votes":0,
        "Owner_views":59,
        "Question_last_edit_time":null,
        "Answer_body":"<blockquote>\n<p>As the Pipeline object doesn't support .deploy method, how to deploy this pipeline?<\/p>\n<\/blockquote>\n<p>Pipeline does not have a <code>.deploy()<\/code> method, no<\/p>\n<p>Use <code>pipeline.upsert(role_arn='...')<\/code> to create\/update the pipeline definition to SageMaker, then call <code>pipeline.start()<\/code> . Docs <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#pipeline\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<blockquote>\n<p>While inference\/scoring, When we receive a raw data(single row for each source), how to trigger the pipeline?<\/p>\n<\/blockquote>\n<p>There are actually two types of pipelines in SageMaker. Model Building Pipelines (which you have in your question), and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">Serial Inference Pipelines<\/a>, which are used for Inference. AWS definitely should have called the former &quot;workflows&quot;<\/p>\n<p>You can use a model building pipeline to setup a serial inference pipeline<\/p>\n<p>To do pre-processing in a serial inference pipeline, you want to train an encoder\/estimator (such as SKLearn) and save its model. Then train a learning algorithm, and save its model, then create a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/pipeline.html\" rel=\"nofollow noreferrer\">PipelineModel<\/a> using both models<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1639073178300,
        "Answer_score":0.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70287087",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58592206,
        "Question_title":"understanding the output of Sagemaker Object Detection prediction",
        "Question_body":"<p>I need help understanding the output of the Amazon Sagemaker object-detection algorithm. <\/p>\n\n<p>Here's my underlying goal: identify when a ping pong ball is in play and mark it's location in an image frame. <\/p>\n\n<p>Sample images from a video feed: <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/6AOS1.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6AOS1.jpg\" alt=\"No ball in play\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/5tRJE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5tRJE.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Steps so far: \n1. I've taken n-video frames from a ping pong match.  <\/p>\n\n<ol start=\"2\">\n<li><p>I used RectLabel to hand annotate the location of the ping pong ball. <\/p><\/li>\n<li><p>Using RectLabel, I converted those labels into a JSON file. Example here: <\/p><\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>{\"images\":[\n    {\"id\":1,\"file_name\":\"thumb0462.png\",\"width\":0,\"height\":0},\n    {\"id\":2,\"file_name\":\"thumb0463.png\",\"width\":0,\"height\":0},\n    {\"id\":3,\"file_name\":\"thumb0464.png\",\"width\":0,\"height\":0},\n    ...\n    {\"id\":4582,\"file_name\":\"thumb6492.png\",\"width\":0,\"height\":0}],\n\"annotations\":[\n    {\"area\":198,\"iscrowd\":0,\"id\":1,\"image_id\":5,\"category_id\":1,\"segmentation\":[[59,152,76,152,76,142,59,142]],\"bbox\":[59,142,18,11]},\n    {\"area\":221,\"iscrowd\":0,\"id\":2,\"image_id\":6,\"category_id\":1,\"segmentation\":[[83,155,99,155,99,143,83,143]],\"bbox\":[83,143,17,13]},\n    {\"area\":399,\"iscrowd\":0,\"id\":3,\"image_id\":8,\"category_id\":1,\"segmentation\":[[118,144,136,144,136,124,118,124]],\"bbox\":[118,124,19,21]},\n    {\"area\":361,\"iscrowd\":0,\"id\":4,\"image_id\":9,\"category_id\":1,\"segmentation\":[[132,123,150,123,150,105,132,105]],\"bbox\":[132,105,19,19]},\n    ...\n\"categories\":[{\"name\":\"pp_ball\",\"id\":1}]\n}\n<\/code><\/pre>\n\n<ol start=\"4\">\n<li>I used a function to separate the annotations into train and validate folders, as expected by SageMaker's input channels. <\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>file_name = '.\/pp-ball-annotations.json'\nwith open(file_name) as f:\n    js = json.load(f)\n    images = js['images']\n    categories = js['categories']\n    annotations = js['annotations']\n    for i in images:\n        jsonFile = i['file_name']\n        jsonFile = jsonFile.split('.')[0] + '.json'\n\n        line = {}\n        line['file'] = i['file_name']\n        line['image_size'] = [{\n            'width': int(i['width']),\n            'height': int(i['height']),\n            'depth': 3\n        }]\n        line['annotations'] = []\n        line['categories'] = []\n        for j in annotations:\n            if j['image_id'] == i['id'] and len(j['bbox']) &gt; 0:\n                line['annotations'].append({\n                    'class_id': int(j['category_id']),\n                    'top': int(j['bbox'][1]),\n                    'left': int(j['bbox'][0]),\n                    'width': int(j['bbox'][2]),\n                    'height': int(j['bbox'][3])\n                })\n                class_name = ''\n                for k in categories:\n                    if int(j['category_id']) == k['id']:\n                        class_name = str(k['name'])\n                assert class_name is not ''\n                line['categories'].append({\n                    'class_id': int(j['category_id']),\n                    'name': class_name\n                })\n        if line['annotations']:\n            with open(os.path.join('generated', jsonFile), 'w') as p:\n                json.dump(line, p)\n\njsons = os.listdir('generated')\nprint ('There are {} images that have annotation files'.format(len(jsons)))\n<\/code><\/pre>\n\n<ol start=\"5\">\n<li>I moved the files into an Amazon S3 bucket with four channels (folders) as required by SageMaker: \/train, \/validation, \/train_annotation, and \/validation_annotation. <\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>num_annotated_files = len(jsons)\ntrain_split_pct = 0.70\nnum_train_jsons = int(num_annotated_files * train_split_pct)\nrandom.shuffle(jsons) # randomize\/shuffle the JSONs to reduce reliance on *sequenced* frames\ntrain_jsons = jsons[:num_train_jsons]\nval_jsons = jsons[num_train_jsons:]\n\n#Moving training files to the training folders\nfor i in train_jsons:\n    image_file = '.\/images\/'+i.split('.')[0]+'.png'\n    shutil.move(image_file, '.\/train\/')\n    shutil.move('.\/generated\/'+i, '.\/train_annotation\/')\n\n#Moving validation files to the validation folders\nfor i in val_jsons:\n    image_file = '.\/images\/'+i.split('.')[0]+'.png'\n    shutil.move(image_file, '.\/validation\/')\n    shutil.move('.\/generated\/'+i, '.\/validation_annotation\/')\n\n\n### Upload to S3\nimport sagemaker\nfrom sagemaker import get_execution_role\n\nrole = sagemaker.get_execution_role()\nsess = sagemaker.Session()\n\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\ntraining_image = get_image_uri(sess.boto_region_name, 'object-detection', repo_version=\"latest\")\n\nbucket = 'pp-balls-object-detection' # custom bucket name.\n# bucket = sess.default_bucket()\nprefix = 'rect-label-test'\n\ntrain_channel = prefix + '\/train'\nvalidation_channel = prefix + '\/validation'\ntrain_annotation_channel = prefix + '\/train_annotation'\nvalidation_annotation_channel = prefix + '\/validation_annotation'\n\nsess.upload_data(path='train', bucket=bucket, key_prefix=train_channel)\nsess.upload_data(path='validation', bucket=bucket, key_prefix=validation_channel)\nsess.upload_data(path='train_annotation', bucket=bucket, key_prefix=train_annotation_channel)\nsess.upload_data(path='validation_annotation', bucket=bucket, key_prefix=validation_annotation_channel)\n\ns3_train_data = 's3:\/\/{}\/{}'.format(bucket, train_channel)\ns3_validation_data = 's3:\/\/{}\/{}'.format(bucket, validation_channel)\ns3_train_annotation = 's3:\/\/{}\/{}'.format(bucket, train_annotation_channel)\ns3_validation_annotation = 's3:\/\/{}\/{}'.format(bucket, validation_annotation_channel)\n<\/code><\/pre>\n\n<ol start=\"6\">\n<li>Created a SageMaker object detector with certain hyperparameters. I note that these hyperparameters are 'unusual' given other examples I've seen: num_classes = 1, use_pretrained_model=0, and image_shape = 438.  <\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>s3_output_location = 's3:\/\/{}\/{}\/output'.format(bucket, prefix)\n\nod_model = sagemaker.estimator.Estimator(training_image,\n                                         role,\n                                         train_instance_count=1,\n                                         train_instance_type='ml.p3.2xlarge',\n                                         train_volume_size = 50,\n                                         train_max_run = 360000,\n                                         input_mode = 'File',\n                                         output_path=s3_output_location,\n                                         sagemaker_session=sess)\n\nod_model.set_hyperparameters(base_network='resnet-50',\n                             use_pretrained_model=0,\n                             num_classes=1,\n                             mini_batch_size=15,\n                             epochs=30,\n                             learning_rate=0.001,\n                             lr_scheduler_step='10',\n                             lr_scheduler_factor=0.1,\n                             optimizer='sgd',\n                             momentum=0.9,\n                             weight_decay=0.0005,\n                             overlap_threshold=0.5,\n                             nms_threshold=0.45,\n                             image_shape=438,\n                             label_width=600,\n                             num_training_samples=num_train_jsons)\n<\/code><\/pre>\n\n<ol start=\"7\">\n<li>I set the train\/validate location for the object-detector, called the .fit function, and deployed the model to an endpoint: <\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated',\n                        content_type='image\/png', s3_data_type='S3Prefix')\nvalidation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated',\n                             content_type='image\/png', s3_data_type='S3Prefix')\ntrain_annotation = sagemaker.session.s3_input(s3_train_annotation, distribution='FullyReplicated',\n                             content_type='image\/png', s3_data_type='S3Prefix')\nvalidation_annotation = sagemaker.session.s3_input(s3_validation_annotation, distribution='FullyReplicated',\n                             content_type='image\/png', s3_data_type='S3Prefix')\n\ndata_channels = {'train': train_data, 'validation': validation_data,\n                 'train_annotation': train_annotation, 'validation_annotation':validation_annotation}\n\nod_model.fit(inputs=data_channels, logs=True)\n\nobject_detector = od_model.deploy(initial_instance_count = 1,\n                             instance_type = 'ml.m4.xlarge')\n<\/code><\/pre>\n\n<ol start=\"8\">\n<li>I invoke the endpoint by passing it a PNG file in bytes: <\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>file_with_path = 'test\/thumb0695.png'\nwith open(file_with_path, 'rb') as image:\n            f = image.read()\n            b = bytearray(f)\n            ne = open('n.txt', 'wb')\n            ne.write(b)\n\n        results = object_detector.predict(b)\n        detections = json.loads(results)\n        print(detections)\n<\/code><\/pre>\n\n<ol start=\"9\">\n<li>The AWS Sagemaker documentation says to expect the output in the following format: <\/li>\n<\/ol>\n\n<blockquote>\n  <p>Each row in this .json file contains an array that represents a detected object. Each of these object arrays consists of a list of six numbers. The first number is the predicted class label. The second number is the associated confidence score for the detection. The last four numbers represent the bounding box coordinates [xmin, ymin, xmax, ymax]. These output bounding box corner indices are normalized by the overall image size. Note that this encoding is different than that use by the input .json format. For example, in the first entry of the detection result, 0.3088374733924866 is the left coordinate (x-coordinate of upper-left corner) of the bounding box as a ratio of the overall image width, 0.07030484080314636 is the top coordinate (y-coordinate of upper-left corner) of the bounding box as a ratio of the overall image height, 0.7110607028007507 is the right coordinate (x-coordinate of lower-right corner) of the bounding box as a ratio of the overall image width, and 0.9345266819000244 is the bottom coordinate (y-coordinate of lower-right corner) of the bounding box as a ratio of the overall image height.<\/p>\n<\/blockquote>\n\n<p>Let's look at a test image: <\/p>\n\n<blockquote>\n  <p>{\"id\":9,\"file_name\":\"thumb0470.png\",\"width\":438,\"height\":240}<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/RN9xE.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RN9xE.jpg\" alt=\"test image thumb0470\"><\/a><\/p>\n\n<p>which has\u00a0a ball with this bounding box [132,105,19,19] (read as x-top-left, y-top-left, box-width, box-height).<\/p>\n\n<p>Given that my object-detector was trained to detect ONE class (num_classes=1), I expected this kind of output for this image: <\/p>\n\n<blockquote>\n  <p>{'prediction': [[1.0, 0.71, 0.55, 0.239, 0.629, 0.283]]}<\/p>\n<\/blockquote>\n\n<p>Instead, I get this output: <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>{'prediction': [[0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0]]}\n<\/code><\/pre>\n\n<p><strong>So now the question<\/strong>:  why is this model giving me 400 JSON elements, instead of just one?  <\/p>\n\n<p>My current hypothesis:  this object detection model is so weakly trained (very possible, as this was just a first pass with too few images), that the Single Shot Detector is identifying what it thinks to be 400 instances of the \"ping pong ball\" in the image.  <\/p>\n\n<p>But even if my hypothesis is correct, why is the output repeated so much?  There are 178 identical 'predictions' of the form <\/p>\n\n<blockquote>\n  <p>[0.0, 1.0, 0.0, 0.0, 1.0, 0.0]<\/p>\n<\/blockquote>\n\n<p>which if interpreted, means: <\/p>\n\n<p>0.0 - class object \"0\" which I did not define. So I assume this means \"no ball in play\"<\/p>\n\n<p>1.0 - 100% confidence<\/p>\n\n<p>0.0 - the xmin position as a ratio of width = 0<\/p>\n\n<p>0.0 - the ymin position as a ratio of height = 0<\/p>\n\n<p>1.0 - the xmax position as a ratio of width = 240<\/p>\n\n<p>0.0 - the ymax position as a ratio of height = 0<\/p>\n\n<p>The coordinates [xmin: 0, ymin: 0, xmax: 240, ymax: 0] is like drawing a line across the first pixel.  <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/n1UKC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/n1UKC.png\" alt=\"prediction visualized using matplotlib\"><\/a><\/p>\n\n<p>Thanks for your help!<\/p>\n\n<p>-------  EDIT based on Ryo's answer ------ <\/p>\n\n<p>Re-mapping the category ID to index-base 0 worked like a charm.  Here are the results from just 2,000 labeled images: <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/NKzb3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NKzb3.png\" alt=\"ping pong ball detected 1\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/yBYKB.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/yBYKB.png\" alt=\"ping pong ball detected 2\"><\/a><\/p>\n\n<p>Here's the code after Ryo's helpful answer:<\/p>\n\n<pre><code>def fixCategoryId(category_id):\n    return category_id - 1;\n\nwith open(file_name) as f:\n    js = json.load(f)\n    images = js['images']\n    categories = js['categories']\n    annotations = js['annotations']\n    for i in images:\n        jsonFile = i['file_name']\n        jsonFile = jsonFile.split('.')[0] + '.json'\n\n        line = {}\n        line['file'] = i['file_name']\n        line['image_size'] = [{\n            'width': int(i['width']),\n            'height': int(i['height']),\n            'depth': 3\n        }]\n        line['annotations'] = []\n        line['categories'] = []\n        for j in annotations:\n            if j['image_id'] == i['id'] and len(j['bbox']) &gt; 0:\n                line['annotations'].append({\n                    'class_id': fixCategoryId(int(j['category_id'])),\n                    'top': int(j['bbox'][1]),\n                    'left': int(j['bbox'][0]),\n                    'width': int(j['bbox'][2]),\n                    'height': int(j['bbox'][3])\n                })\n                class_name = ''\n                for k in categories:\n                    if int(j['category_id']) == k['id']:\n                        class_name = str(k['name'])\n                assert class_name is not ''\n                line['categories'].append({\n                    'class_id': fixCategoryId(int(j['category_id'])),\n                    'name': class_name\n                })\n        if line['annotations']:\n            with open(os.path.join('generated', jsonFile), 'w') as p:\n                json.dump(line, p)\n\njsons = os.listdir('generated')\nprint ('There are {} images that have annotation files'.format(len(jsons)))\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1572270822900,
        "Question_score":4,
        "Question_tags":"python|machine-learning|computer-vision|object-detection|amazon-sagemaker",
        "Question_view_count":703,
        "Owner_creation_time":1299066980970,
        "Owner_last_access_time":1645631437427,
        "Owner_location":"New York, NY",
        "Owner_reputation":1194,
        "Owner_up_votes":72,
        "Owner_down_votes":0,
        "Owner_views":79,
        "Question_last_edit_time":1576434290392,
        "Answer_body":"<p>Though 'category_id' in the COCO JSON file starts from 1, 'class_id' in the Amazon SageMaker JSON file starts from 0.<\/p>\n\n<p>Your conversion code should be like this.<\/p>\n\n<pre><code>def fixCategoryId(category_id):\n    return category_id - 1;\n\nwith open(coco_json_path) as f:\n    js = json.load(f)\n    images = js['images']\n    categories = js['categories']\n    annotations = js['annotations']\n    for i in images:\n        jsonFile = i['file_name']\n        jsonFile = jsonFile.split('.')[0] + '.json'\n\n        line = {}\n        line['file'] = i['file_name']\n        line['image_size'] = [{\n            'width': int(i['width']),\n            'height': int(i['height']),\n            'depth': 3\n        }]\n        line['annotations'] = []\n        line['categories'] = []\n        for j in annotations:\n            if j['image_id'] == i['id'] and len(j['bbox']) &gt; 0:\n                line['annotations'].append({\n                    'class_id': fixCategoryId(int(j['category_id'])),\n                    'top': int(j['bbox'][1]),\n                    'left': int(j['bbox'][0]),\n                    'width': int(j['bbox'][2]),\n                    'height': int(j['bbox'][3])\n                })\n                class_name = ''\n                for k in categories:\n                    if int(j['category_id']) == k['id']:\n                        class_name = str(k['name'])\n                assert class_name is not ''\n                line['categories'].append({\n                    'class_id': fixCategoryId(int(j['category_id'])),\n                    'name': class_name\n                })\n        if line['annotations']:\n            with open(os.path.join(sagemaker_json_path, jsonFile), 'w') as p:\n                json.dump(line, p)\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_image_json_format.ipynb\" rel=\"nofollow noreferrer\">In the Amazon SageMaker doc<\/a>, they are doing this using get_coco_mapper().<\/p>\n\n<pre><code>import json\nimport logging\n\ndef get_coco_mapper():\n    original_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20,\n                    21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n                    41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n                    61, 62, 63, 64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n                    81, 82, 84, 85, 86, 87, 88, 89, 90]\n    iter_counter = 0\n    COCO = {}\n    for orig in original_list:\n        COCO[orig] = iter_counter\n        iter_counter += 1\n    return COCO\n<\/code><\/pre>\n\n<p>After you trained the model, you have to check whether each loss has decreased or not.<\/p>\n\n<pre><code>od_model.fit(inputs=data_channels, logs=True)\n\n[11\/04\/2019 09:26:46 INFO 140651482974016] #quality_metric: host=algo-1, epoch=499, batch=11 train cross_entropy &lt;loss&gt;=(0.20304460724736212)\n[11\/04\/2019 09:26:46 INFO 140651482974016] #quality_metric: host=algo-1, epoch=499, batch=11 train smooth_l1 &lt;loss&gt;=(0.06970448779799958)\n<\/code><\/pre>\n\n<p>If you have some questions, let us know.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1572863208716,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58592206",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69205825,
        "Question_title":"Save SKLearnProcessor transformer in sagemaker",
        "Question_body":"<p>I wanted to use the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/sagemaker.sklearn.html#sagemaker.sklearn.processing.SKLearnProcessor\" rel=\"nofollow noreferrer\">SKLearnProcessor<\/a> in Sagemaker to perform some transformations on an input dataset. However, I want to save this fitted transformer into Sagemaker to reuse it later in other scripts. How can I do that?. I don't see anywhere how it can be stored in S3 at least not in the SDK.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1631784469353,
        "Question_score":1,
        "Question_tags":"python|scikit-learn|amazon-sagemaker",
        "Question_view_count":26,
        "Owner_creation_time":1322785469840,
        "Owner_last_access_time":1658151826980,
        "Owner_location":"Cork, Ireland",
        "Owner_reputation":482,
        "Owner_up_votes":25,
        "Owner_down_votes":1,
        "Owner_views":40,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69205825",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73383302,
        "Question_title":"how does input_fn, predict_fn and output_fn work in aws sagemaker script mode?",
        "Question_body":"<p>i am trying to understand how input_fn, predict_fn and outout_fn work? I am able to understand what they are, but I am not able to understand  how they are called (invoked), can anyone help me understand the same<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1660714390293,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":39,
        "Owner_creation_time":1573543021663,
        "Owner_last_access_time":1664082306423,
        "Owner_location":null,
        "Owner_reputation":17,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When your endpoint comes up, <code>model_fn<\/code> is invoked so that your model is loaded. When you invoke the endpoint, <code>input_fn<\/code> is called so that your input payload is parsed, immediately after that, <code>predict_fn<\/code> is called so that a prediction is generated, and then <code>output_fn<\/code> is called to parse the prediction before returning it to the caller.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1661363649820,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73383302",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64445766,
        "Question_title":"Resolve Lambda Issue on Sagemaker Ground truth",
        "Question_body":"<p>I am trying to create a labeling job for amazon ground truth for text classification.\nI am not able to create it successfully because I keep getting this error -<\/p>\n<blockquote>\n<p>MissingRequiredParameter: Missing required key 'PreHumanTaskLambdaArn'\nin params.HumanTaskConfig<\/p>\n<\/blockquote>\n<p>Everything seems right, manifest file was successfully created.<\/p>\n<p>I haven't found any help with this issue online, in the documentation, I have found that we could use one of these ARNs in the Text classification section - <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_HumanTaskConfig.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_HumanTaskConfig.html<\/a><\/p>\n<p>But in the form when creating the labeling job, there's no place to insert this ARN, any idea how this can be fixed and make the error go away.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603199012167,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":207,
        "Owner_creation_time":1434398585880,
        "Owner_last_access_time":1663940254630,
        "Owner_location":"Moscow, Russia",
        "Owner_reputation":2702,
        "Owner_up_votes":229,
        "Owner_down_votes":3,
        "Owner_views":242,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64445766",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63849893,
        "Question_title":"AWS - is it possible to pass the parameters from AWS lambda function to AWS sagemaker notebook",
        "Question_body":"<p>I am beginner to AWS console. I am facing issues while building Machine Learning pipeline.<\/p>\n<p>Currently, The Lambda function does the job of getting the uploaded filename, username from front end and invoking the notebook instance.<\/p>\n<p>Also, life cycle configuration at the instance will invoke the notebook for training. So the question is how to pass above variables variables to sagemaker notebook for training the machine learning model. Is it possible to achieve this? Thank you.<\/p>\n<pre><code>#invoke command from lambda\nclient.start_notebook_instance(NotebookInstanceName='&lt;sagemaker_instance_name&gt;')\n<\/code><\/pre>\n<p>Lifecycle configuration under sagemaker instance:<\/p>\n<pre><code>#!\/bin\/bash\n\nset -e\n\nENVIRONMENT=JupyterSystemEnv\n#JupyterSystemEnv\nNOTEBOOK_FILE=\/home\/ec2-user\/SageMaker\/XGBoost_training.ipynb\n\nsource \/home\/ec2-user\/anaconda3\/bin\/activate &quot;$ENVIRONMENT&quot;\n\nnohup jupyter nbconvert --ExecutePreprocessor.timeout=-1 --ExecutePreprocessor.kernel_name=python3 --to notebook --execute &quot;$NOTEBOOK_FILE&quot; &amp;\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1599837505523,
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":464,
        "Owner_creation_time":1429069144583,
        "Owner_last_access_time":1659713078917,
        "Owner_location":"Ireland",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63849893",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73435418,
        "Question_title":"How to import data file from the S3 bucket into the Sagemaker notebook?",
        "Question_body":"<p>I have npz files that I want to import for my model training.\nBelow is the code I have tried.<\/p>\n<pre><code>import s3fs\nfs = s3fs.S3FileSystem()\n\n# To List 5 files in your accessible bucket\n#fs.ls('s3:\/\/input_data\/train_npz\/')[:5]\n\n# open it directly\nwith fs.open(f's3:\/\/input_data\/train_npz\/0.npz') as f:\n    display(Image.open(f))\n<\/code><\/pre>\n<blockquote>\n<pre><code>--------------------------------------------------------------------------- AttributeError                            Traceback (most recent call\n<\/code><\/pre>\n<p>last)  in \n7\n8 # open it directly\n----&gt; 9 with fs.open(f's3:\/\/input_data\/train_npz\/0.npz')\nas f:\n10     display(Image.open(f))<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/fsspec\/spec.py in open(self,\npath, mode, block_size, cache_options, **kwargs)\n980                 autocommit=ac,\n981                 cache_options=cache_options,\n--&gt; 982                 **kwargs,\n983             )\n984             if not ac and &quot;r&quot; not in mode:<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/s3fs\/core.py in _open(self,\npath, mode, block_size, acl, version_id, fill_cache, cache_type,\nautocommit, requester_pays, **kwargs)\n543             cache_type=cache_type,\n544             autocommit=autocommit,\n--&gt; 545             requester_pays=requester_pays,\n546         )\n547<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/s3fs\/core.py in <strong>init<\/strong>(self,\ns3, path, mode, block_size, acl, version_id, fill_cache,\ns3_additional_kwargs, autocommit, cache_type, requester_pays)    1822\nself.version_id = self.details.get(&quot;VersionId&quot;)    1823<br \/>\nsuper().<strong>init<\/strong>(\n-&gt; 1824             s3, path, mode, block_size, autocommit=autocommit, cache_type=cache_type    1825         )    1826         self.s3 =\nself.fs  # compatibility<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/fsspec\/spec.py in\n<strong>init<\/strong>(self, fs, path, mode, block_size, autocommit, cache_type, cache_options, **kwargs)    1302         if mode == &quot;rb&quot;:    1303<br \/>\nif not hasattr(self, &quot;details&quot;):\n-&gt; 1304                 self.details = fs.info(path)    1305             self.size = self.details[&quot;size&quot;]    1306             self.cache =\ncaches[cache_type](<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/fsspec\/asyn.py in\nwrapper(*args, **kwargs)\n86     def wrapper(*args, **kwargs):\n87         self = obj or args[0]\n---&gt; 88         return sync(self.loop, func, *args, **kwargs)\n89\n90     return wrapper<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/fsspec\/asyn.py in sync(loop,\nfunc, timeout, *args, **kwargs)\n67         raise FSTimeoutError\n68     if isinstance(result[0], BaseException):\n---&gt; 69         raise result[0]\n70     return result[0]\n71<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/fsspec\/asyn.py in\n_runner(event, coro, result, timeout)\n23         coro = asyncio.wait_for(coro, timeout=timeout)\n24     try:\n---&gt; 25         result[0] = await coro\n26     except Exception as ex:\n27         result[0] = ex<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/s3fs\/core.py in _info(self,\npath, bucket, key, refresh, version_id)    1062                 else:\n1063                     try:\n-&gt; 1064                         out = await self._simple_info(path)    1065                     except PermissionError:    1066<\/p>\n<h1>If the permissions aren't enough for scanning a prefix<\/h1>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/s3fs\/core.py in\n_simple_info(self, path)\n982             Delimiter=&quot;\/&quot;,\n983             MaxKeys=1,\n--&gt; 984             **self.req_kw,\n985         )\n986         # This method either can return the info blob for the object if it<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/s3fs\/core.py in _call_s3(self,\nmethod, *akwarglist, **kwargs)\n235\n236     async def _call_s3(self, method, *akwarglist, **kwargs):\n--&gt; 237         await self.set_session()\n238         s3 = await self.get_s3(kwargs.get(&quot;Bucket&quot;))\n239         method = getattr(s3, method)<\/p>\n<p>\/opt\/conda\/lib\/python3.6\/site-packages\/s3fs\/core.py in\nset_session(self, refresh, kwargs)\n376\n377         conf = AioConfig(**config_kwargs)\n--&gt; 378         self.session = aiobotocore.AioSession(**self.kwargs)\n379\n380         for parameters in (config_kwargs, self.kwargs, init_kwargs, client_kwargs):<\/p>\n<p>AttributeError: module 'aiobotocore' has no attribute 'AioSession'<\/p>\n<\/blockquote>\n<p>Can anyone let me know where I made the mistake or how to do it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661092581567,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-s3|amazon-sagemaker|dataloader",
        "Question_view_count":44,
        "Owner_creation_time":1640473067233,
        "Owner_last_access_time":1662900590550,
        "Owner_location":null,
        "Owner_reputation":29,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73435418",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66860087,
        "Question_title":"IAM control Sagemaker Studio Instance type",
        "Question_body":"<p>Was wondering if anyone had luck limiting the type of instances a user could chose from the Sagemaker Studio-Jupyter. Did not want to enforce the limitation on the Domain role and was trying to create custom roles that could be attached to user profiles. Tried with the &quot;createApp&quot; and denying the larger instances, but found it becoming a road-block (Sagemaker.createApp not permitted) when a new user profile tries to launch the studio for the first time. Is there anyway to allow them to create the default APP, but limit the choice of the instances that he\/she can select for the image using IAM ?<\/p>\n<h2>Sample Policy used :<\/h2>\n<pre><code>    {\n        &quot;Sid&quot;: &quot;VisualEditor1&quot;,\n        &quot;Effect&quot;: &quot;Deny&quot;,\n        &quot;Action&quot;: &quot;sagemaker:CreateApp&quot;,\n        &quot;Resource&quot;: &quot;*&quot;,\n        &quot;Condition&quot;: {\n            &quot;ForAllValues:StringLike&quot;: {\n                &quot;sagemaker:InstanceTypes&quot;: [\n                    &quot;ml.c5.3xlarge&quot;,\n                    &quot;ml.c5.4large&quot;,\n                    &quot;ml.c5.9xlarge&quot;,                                                                      \n                    &quot;ml.m5.4xlarge&quot;,                        \n                    &quot;ml.m5.12xlarge&quot;,\n                    &quot;ml.m5.16xlarge&quot;,\n                    &quot;ml.m5.24xlarge&quot;,\n                    &quot;ml.c5.4xlarge&quot;,\n                    &quot;ml.c5.9xlarge&quot;,\n                    &quot;ml.c5.12xlarge&quot;,\n                    &quot;ml.c5.18xlarge&quot;,\n                    &quot;ml.c5.24xlarge&quot;,\n                    &quot;ml.g4dn.*&quot;,\n                    &quot;ml.p3.*&quot;\n                    \n                ]\n            }\n        }\n    }\n<\/code><\/pre>\n<p>This works fine on a user profile that has logged\/started the &quot;default&quot; App, but limits a new user with the same role\/policy from launching issuing &quot;Open Studio&quot;.<\/p>\n<p>Saw this which was quite similar to the ask - <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1499\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1499<\/a><\/p>\n<p>Any thoughts, pointers ?<\/p>\n<p>Thanks,\nMano<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617044567870,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-iam|amazon-sagemaker",
        "Question_view_count":554,
        "Owner_creation_time":1617042068150,
        "Owner_last_access_time":1636064073713,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66860087",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65421005,
        "Question_title":"how to save uncompressed outputs from a training job in using aws Sagemaker python SDK?",
        "Question_body":"<p>I'm trying to upload training job artifacts to S3 in a non-compressed manner.<\/p>\n<p>I am familiar with the output_dir one can provide to a sagemaker Estimator, then everything saved under \/opt\/ml\/output is uploaded compressed to the S3 output dir.<\/p>\n<p>I want to have the option to access a specific artifact without having to decompress the output every time. Is there a clean way to go about it? if not any workaround in mind?\nThe artifacts of my interest are small meta-data files .txt or .csv, while in my case the rest of the artifacts can be ~1GB so downloading and decompressing is quite excessive.<\/p>\n<p>any help would be appreciated<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1608711307120,
        "Question_score":1,
        "Question_tags":"python|boto3|amazon-sagemaker",
        "Question_view_count":313,
        "Owner_creation_time":1557646363770,
        "Owner_last_access_time":1663926361953,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":34,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I ended up using the checkpoint path that is by default being synced with the specified S3 path in an uncompressed manner.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1612085605603,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65421005",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71260306,
        "Question_title":"How to access\/invoke a sagemaker endpoint without lambda?",
        "Question_body":"<p>based on the aws documentation, maximum timeout limit is less that 30 seconds in api gateway.so hooking up an sagemaker endpoint with api gateway wouldn't make sense, if the request\/response is going to take more than 30 seconds. is there any workaround ? adding a lambda in between api gateway and sagemaker endpoint is going to add more time to process request\/response, which i would like to avoid. also, there will be added time for lambda cold starts and sagemaker serverless endpoints are built on top of lambda so that will also add cold start time. is there a way to invoke the serverless sagemaker endpoints , without these overhead?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1645755688313,
        "Question_score":3,
        "Question_tags":"serverless|amazon-sagemaker",
        "Question_view_count":1122,
        "Owner_creation_time":1590797441983,
        "Owner_last_access_time":1664049080543,
        "Owner_location":null,
        "Owner_reputation":525,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":98,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It is indeed possible to invoke sagemaker endpoints from sagemaker without using any other AWS services and that is also manifested by the fact that they have invocation URLs.<\/p>\n<p>Here's how you set it up:<\/p>\n<ol>\n<li>create an user with only programmatic access and attach a policy json that should look something like below:<\/li>\n<\/ol>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;VisualEditor0&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:sagemaker:&lt;region&gt;:&lt;account-id&gt;:endpoint\/&lt;endpoint-name&gt;&quot;\n        }\n    ]\n} \n<\/code><\/pre>\n<p>you can replace <code>&lt;endpoint-name&gt;<\/code> with <code>*<\/code> to let this user invoke all endpoints.<\/p>\n<ol start=\"2\">\n<li><p>use the ACCESS-KEY and SECRET-ACCESS-KEY to configure authorisation in postman like shown in this screenshot. also add the parameters in advanced tab like shown in the screenshot.\n<a href=\"https:\/\/i.stack.imgur.com\/cYkTf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cYkTf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>then fill up your body with the relevant content type.<\/p>\n<\/li>\n<li><p>then add or remove additional headers like variant-name or model-name, if you have them set up and the headers should look like shown in this screenshot: <a href=\"https:\/\/i.stack.imgur.com\/NLqkV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NLqkV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>send the request to receive reponse like this\n<a href=\"https:\/\/i.stack.imgur.com\/uA4kF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uA4kF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<\/ol>\n<p><em><strong>URL and credentials in the above screenshots doesn't work anymore, duh!<\/strong><\/em><\/p>\n<p>and if you want code to invoke the endpoint directly using some back-end language, <a href=\"https:\/\/stackoverflow.com\/a\/70803026\/11814996\">here's code for python<\/a>.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1645795642143,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1649698499116,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71260306",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72002863,
        "Question_title":"KeyError: 'ETag' while trying to load data from S3 to Sagemaker",
        "Question_body":"<p>I Unload a file of 500 MB into S3 from Redshift, instead of saving into a single file in S3 it bifurcated into several chunks and now I am trying to access it from S3 to AWS Sagemaker. While trying to read the file using Pd.read_csv and dask.dataframe.read_csv I am getting Keyerror as 'ETag'<\/p>\n<p>I'm a newbie to AWS, please do help me.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9a78F.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9a78F.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1650904478070,
        "Question_score":1,
        "Question_tags":"amazon-s3|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":215,
        "Owner_creation_time":1551957360467,
        "Owner_last_access_time":1653641826190,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":327,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72002863",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57686852,
        "Question_title":"SageMaker estimator `source_dir` from S3",
        "Question_body":"<p>I would like to start a training job using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/sagemaker.tensorflow.html\" rel=\"nofollow noreferrer\">SageMaker TensorFlow Estimator<\/a> in a script mode.<br>\nMy problem is that I don't have my training code locally or in a git repo, but only in S3 \"directory\" and <code>source_dir<\/code> parameter requires a local file or usage of git.<\/p>\n\n<p>Is the only way to copy the files locally from s3 (which is problematic with python) or can I do it in a nicer way?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1566976565907,
        "Question_score":1,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":1215,
        "Owner_creation_time":1361889746643,
        "Owner_last_access_time":1646295315967,
        "Owner_location":null,
        "Owner_reputation":2945,
        "Owner_up_votes":65,
        "Owner_down_votes":0,
        "Owner_views":89,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57686852",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72738710,
        "Question_title":"How to access an IP whitelist secured API from within a Sagemaker notebook?",
        "Question_body":"<p>I have deployed a Flask application and its containers in an AWS EKS managed Kubernetes cluster, and the cluster's security group is IP whitelist secured.<\/p>\n<p>I created the cluster using <code>eksctl<\/code>, created an RDS instance for it, and deployed the application using it's helm chart.<\/p>\n<p>I am trying to access the APIs of the flask application from within a AWS Sagemaker Notebook instance, but because of the IP whitelist, I am unable to connect. The connection times-out instead.<\/p>\n<p>Can anyone tell me how I can add the Notebook instance to my whitelist?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656041644317,
        "Question_score":1,
        "Question_tags":"amazon-web-services|cloud|devops|amazon-sagemaker",
        "Question_view_count":50,
        "Owner_creation_time":1531126305543,
        "Owner_last_access_time":1664050689040,
        "Owner_location":null,
        "Owner_reputation":87,
        "Owner_up_votes":237,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72738710",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70222875,
        "Question_title":"IOPub data rate exceeded in SageMaker?",
        "Question_body":"<p>Does anyone know where to find the config file to edit the <code>c.NotebookApp.iopub_data_rate_limit<\/code>? I am not working with the AWS CLI, but rather I am doing everything in the AWS Console. I have a SageMaker notebook running and I would like to change the data rate limit, but essentially don't have access to a terminal, unless someone could explain how to access the terminal within the console?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1638589402183,
        "Question_score":0,
        "Question_tags":"jupyter-notebook|jupyter-lab|amazon-sagemaker",
        "Question_view_count":57,
        "Owner_creation_time":1536431447580,
        "Owner_last_access_time":1650991212660,
        "Owner_location":null,
        "Owner_reputation":113,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":47,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70222875",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71655510,
        "Question_title":"How to see all SageMaker service quota limits?",
        "Question_body":"<p>I believe there are different limits for SageMaker training, vs CreateTransformJob, spot vs not dedicated. Where can I see the current service limits for sagemaker services? Is there a place to check all SageMaker service quotas?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1648517982110,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":215,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":null,
        "Answer_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html\" rel=\"nofollow noreferrer\">Here<\/a> in the documentation you can see the default sagemaker service quotas. Unfortunately, it's not yet possible to see the current quotas according to this <a href=\"https:\/\/repost.aws\/questions\/QUweO83CSlTu-3Zn2RxdESWg\/how-do-i-check-my-current-sage-maker-service-quotas\" rel=\"nofollow noreferrer\">post<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1648538467292,
        "Answer_score":0.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71655510",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72882923,
        "Question_title":"Sagemaker Batch transform with multiple images",
        "Question_body":"<p>The batch transform with mini-batch &gt; 1 of images doesn't work fo as I expect.<\/p>\n<p>I'm using Sagemaker Batch transform for inference.\nI'm trying to preprocess images on a custom container that I created (using model pipelining: the first model is the pre-processor that I'm asking about, and the second model is an Nvidia-triton inference server).<\/p>\n<p>I'm using batch transform as follows:<\/p>\n<pre><code>transformer = sagemaker.transformer.Transformer(model_name=model_name, \n                  instance_count=instance_count, \n                  instance_type=instance_type,\n                  max_concurrent_transforms=16,\n                  output_path=inference_output_data,\n                  strategy='MultiRecord')\ntransformer.transform(data=batch_input,\n                  content_type='image\/jpeg',\n                  job_name=job_name,\n                  split_type='Line',\n                  wait=False,\n                  logs=False)\n<\/code><\/pre>\n<p>Note the <code>split type<\/code> and <code>strategy<\/code> with values 'MultiRecord' and 'Line' so I would get mini-batches &gt; 1.<\/p>\n<p>When I did my sanity check with batch size of 1, the following code worked fine:<\/p>\n<pre><code>@app.route('\/invocations', methods=['POST'])\ndef transformation():\n    img_bytes = flask.request.data\n    img = Image.open(io.BytesIO(img_bytes))\n    preprocessed_image = preprocess_image()\n<\/code><\/pre>\n<p>However, with batching, this code doesn't work anymore. Basically I would not expect it to work, since it only valid for reading for one image.\nI even printed the bytes that my container receives and these are not the original bytes of the image but some strange mix of the images.<\/p>\n<p>Can you please point me what am I doing wrong?<\/p>\n<p>Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657107353313,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":159,
        "Owner_creation_time":1492957629740,
        "Owner_last_access_time":1663850451627,
        "Owner_location":"Israel",
        "Owner_reputation":147,
        "Owner_up_votes":56,
        "Owner_down_votes":1,
        "Owner_views":45,
        "Question_last_edit_time":1657108872360,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72882923",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66915920,
        "Question_title":"Using pytorch cuda in AWS sagemaker notebook instance",
        "Question_body":"<p>In colab, whenever we need GPU, we simply click <code>change runtime type<\/code> and change hardware accelarator to <code>GPU<\/code><\/p>\n<p>and cuda becomes available, <code>torch.cuda.is_available()<\/code> is <code>True<\/code><\/p>\n<p>How to do this is AWS sagemaker, i.e. turning on cuda.\nI am new to AWS and trying to train model using pytorch in aws sagemaker, where Pytorch code is first tested in colab environment.<\/p>\n<p>my sagemaker notebook insatnce is <code>ml.t2.medium<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617348836210,
        "Question_score":0,
        "Question_tags":"amazon-ec2|pytorch|amazon-sagemaker",
        "Question_view_count":1608,
        "Owner_creation_time":1567880532003,
        "Owner_last_access_time":1661706476487,
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":137,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":100,
        "Question_last_edit_time":1617349698827,
        "Answer_body":"<p>Using AWS Sagemaker you don't need to worry about the GPU, you simply select an instance type with GPU ans Sagemaker will use it. Specifically <code>ml.t2.medium<\/code> doesn't have a GPU but it's anyway not the right way to train a model.\nBasically you have 2 canonical ways to use Sagemaker (look at the documentation and examples please), the first is to use a notebook with a limited computing resource to spin up a training job using a prebuilt image, in that case when you call the estimator you simply specify what <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/\" rel=\"nofollow noreferrer\">instance type<\/a> you want (you'll choose one with GPU, looking at the costs). The second way is to use your own container, push it to ECR and launch a training job from the console, where you specify the instance type.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1617464162903,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66915920",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69634197,
        "Question_title":"Pretraining\/transfer learning with SageMaker BlazingText (word2vec)?",
        "Question_body":"<p>I have a training set consisting of a description and a binary label. From reading previous work, I know that using pretrained <a href=\"https:\/\/fasttext.cc\/docs\/en\/english-vectors.html\" rel=\"nofollow noreferrer\">fasttext embeddings<\/a> should work well for my use case. I need to be able to make predictions on unseen words (OOV). My company is already using aws\/sagemaker, so using <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/blazingtext_word2vec_subwords_text8\/blazingtext_word2vec_subwords_text8.ipynb\" rel=\"nofollow noreferrer\">SageMaker Blazing text with subword embedding<\/a> seems like a good approach.<\/p>\n<p>However, they are providing their own training data in the example - does this means it tries to learn word embeddings from scratch from only this training data? I was expecting to be able to pass pretraining as a parameter, and then fine tune it with my own data so those words get added to the known dictionary.<\/p>\n<p>But the way it looks to me now, is that I either use a lookup against a hardcoded list of pretrained embeddings (like GloVe or one of the word vector datasets from fasttext), which means I'm not using my own data and also have to come up with a solution to handling OOV. Or I use Blazing text which can handle OOV, but then I don't take advantage of any pretrained model?<\/p>\n<p>So my main question is this: can I use Blazing text to get pretrained OOV embeddings? And if so how?<\/p>\n<p>Would be great if I could also understand how transfer learning would work in this case, so I can make use of my own classification data. However, the embeddings will be used in a downstream classification task, so I guess I could say the fine tuning happens there?<\/p>\n<ul>\n<li>Use pretrined blazing text model to get embeddings for both seen and unseen words (through subword embeddings)<\/li>\n<li>Combine these embeddings with other features<\/li>\n<li>Fit a model around the embeddings + features to get the classification<\/li>\n<\/ul>\n<p>Appreciate any help!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1634659369027,
        "Question_score":0,
        "Question_tags":"nlp|amazon-sagemaker|embedding|pre-trained-model|fasttext",
        "Question_view_count":80,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69634197",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73662608,
        "Question_title":"Difference between SageMaker instance count and Data parallelism",
        "Question_body":"<p>I can't understand the difference between SageMaker instance count and Data parallelism. As we already have a feature that can specify how many instances we train model when we write a training script using sagemaker-sdk.<\/p>\n<p>However, in 2021 re:Invent, SageMaker team launched and demonstrated SageMaker managed Data Parallelism and this feature also provides distributed training.<\/p>\n<p>I've searched a lot of sites for letting me know about that, but I can't find really clear demonstration. I share some stuffs explaining the concept I mentioned closely. Link :\u00a0<a href=\"https:\/\/godatadriven.com\/blog\/distributed-training-a-diy-aws-sagemaker-model\/\" rel=\"nofollow noreferrer\">https:\/\/godatadriven.com\/blog\/distributed-training-a-diy-aws-sagemaker-model\/<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662728592430,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":15,
        "Owner_creation_time":1412669622830,
        "Owner_last_access_time":1663944305230,
        "Owner_location":null,
        "Owner_reputation":26,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73662608",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71410791,
        "Question_title":"How can I build a multi model endpoint for ensemble modeling with using my own model containers?",
        "Question_body":"<p>I'm trying to deploy a multi model endpoint on Amazon Sagemaker, and am working with my own model containers which I created using <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/main\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">scikit_bring_your_own<\/a> example. I can train and create endpoint for each of them separately but for example when I try to collect mlp and cart together in multi model endpoint, I get an error which says &quot;The cart,mlp for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint&quot;. When I check CloudWatch logs I cannot see anything unusual. <strong>Should I change the container structure for multi model endpoints ?<\/strong><\/p>\n<pre><code>from time import gmtime, strftime\nimport os\nimport boto3\nimport time\nimport re\nimport sagemaker\n\nmodel_name = &quot;efe-test-model-ensemble-modeling-&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\n\ncart_hosting_container = {\n    &quot;Image&quot;: &quot;097916623002.dkr.ecr.eu-central-1.amazonaws.com\/snop-mm-cart:latest&quot;,\n    &quot;ContainerHostname&quot;: &quot;cart&quot;,\n    &quot;ModelDataUrl&quot;: &quot;s3:\/\/sagemaker-eu-central-1-097916623002\/output\/snop-mm-cart-2022-03-09-12-47-04-881\/output\/model.tar.gz&quot;,\n}\n\nmlp_hosting_container = {\n    &quot;Image&quot;: &quot;097916623002.dkr.ecr.eu-central-1.amazonaws.com\/snop-mm-mlp:latest&quot;,\n    &quot;ContainerHostname&quot;: &quot;mlp&quot;,\n    &quot;ModelDataUrl&quot;: &quot;s3:\/\/sagemaker-eu-central-1-097916623002\/output\/snop-mm-mlp-2022-03-09-12-52-09-267\/output\/model.tar.gz&quot;,\n}\n\nrole = sagemaker.get_execution_role()\nsm = boto3.client(&quot;sagemaker&quot;)\n\ninferenceExecutionConfig = {&quot;Mode&quot;: &quot;Direct&quot;}\n\ncreate_model_response = sm.create_model(\n    ModelName=model_name,\n    InferenceExecutionConfig=inferenceExecutionConfig,\n    ExecutionRoleArn=role,\n    Containers=[cart_hosting_container, mlp_hosting_container],\n)\n\nendpoint_config_name = &quot;TEST-config-ensemble-modelling-&quot; + strftime(\n    &quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime()\n)\nprint(endpoint_config_name)\ncreate_endpoint_config_response = sm.create_endpoint_config(\n    EndpointConfigName=endpoint_config_name,\n    ProductionVariants=[\n        {\n            &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n            &quot;InitialInstanceCount&quot;: 1,\n            &quot;InitialVariantWeight&quot;: 1,\n            &quot;ModelName&quot;: model_name,\n            &quot;VariantName&quot;: &quot;AllTraffic&quot;,\n        }\n    ],\n)\n\nprint(&quot;Endpoint Config Arn: &quot; + create_endpoint_config_response[&quot;EndpointConfigArn&quot;])\n\n%%time\nimport time\n\nendpoint_name = &quot;TEST-endpoint-ensemble-modelling-&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nprint(endpoint_name)\ncreate_endpoint_response = sm.create_endpoint(\n    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n)\nprint(create_endpoint_response[&quot;EndpointArn&quot;])\n\nresp = sm.describe_endpoint(EndpointName=endpoint_name)\nstatus = resp[&quot;EndpointStatus&quot;]\nprint(&quot;Status: &quot; + status)\n\nwhile status == &quot;Creating&quot;:\n    time.sleep(60)\n    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n    status = resp[&quot;EndpointStatus&quot;]\n    print(&quot;Status: &quot; + status)\n\nprint(&quot;Arn: &quot; + resp[&quot;EndpointArn&quot;])\nprint(&quot;Status: &quot; + status)\n<\/code><\/pre>\n<p>It creates two folders in CloudWatch<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wCNOC.png\" rel=\"nofollow noreferrer\">log groups<\/a><\/p>\n<p>mlp is:\n<a href=\"https:\/\/i.stack.imgur.com\/O7qeA.png\" rel=\"nofollow noreferrer\">mlp log<\/a><\/p>\n<p>cart is:\n<a href=\"https:\/\/i.stack.imgur.com\/XhiDC.png\" rel=\"nofollow noreferrer\">cart log<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646835413127,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":87,
        "Owner_creation_time":1645732047140,
        "Owner_last_access_time":1654867064967,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71410791",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68470428,
        "Question_title":"Vectorising categorical dataset for XGBoost in Sagemaker",
        "Question_body":"<p>When we train an XGB model using AWS built-in models\ne.g. <code>(container = sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region, &quot;1.2-1&quot;))<\/code>,<\/p>\n<p>Based on my understanding, The training job requires numerical vectors for the train and validation.\nMeaning that if you have a dataset with categorical values and strings, you need to convert them into a vector. the model only deals with float numbers,\n(Outside Sagemaker, I can use TFIDF to vectorize my features and construct a DMatrix), but this approach doesn't seem to be supported by Sagemaker.<\/p>\n<ol>\n<li>Does anyone know how this data transformation is done in Sagemaker?<\/li>\n<li>Is this a bad idea to use BlazyngText unsupervised learning to generate the vectors?<\/li>\n<li>Should we have a preprocessing step and in that step we use TFIDF?<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626874229773,
        "Question_score":0,
        "Question_tags":"python|vectorization|xgboost|amazon-sagemaker",
        "Question_view_count":106,
        "Owner_creation_time":1341999310263,
        "Owner_last_access_time":1663790298557,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":443,
        "Owner_up_votes":1252,
        "Owner_down_votes":1,
        "Owner_views":83,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68470428",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69093204,
        "Question_title":"Is there way of gaining more root (temporary) volume on the aws sagemaker notebook instance?",
        "Question_body":"<p>Aws sagemaker notebook instances come with a fixed root volume size of ~104GB whose ~15 GB is free (available).<\/p>\n<p>Docker uses this temporary memory (<code>\/var\/lib\/docker<\/code> as far as I know).<\/p>\n<p>When I try to build docker image to create custom training-job, temporary root volume in use  blows up and system throws &quot;no space left on the device&quot; error.<\/p>\n<p>I tried to delete anaconda directory (~62 GB), however then, boto3 and sagemaker python libraries stopped working.<\/p>\n<p>What is the best way to solve problem?<\/p>\n<p>Heavy Dockerfile I try to build to push ECR :<\/p>\n<pre><code>ARG REGION=&quot;us-east-1&quot;\n\nFROM 763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-training:1.8.1-gpu-py36-cu111-ubuntu18.04\n\nRUN pip3 install torch==1.8.2+cu111 torchvision==0.9.2+cu111 -f https:\/\/download.pytorch.org\/whl\/lts\/1.8\/torch_lts.html\n\nRUN python3 -m pip install detectron2 -f \\\n  https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu111\/torch1.8\/index.html\n\nENV FORCE_CUDA=&quot;1&quot;\n\nENV TORCH_CUDA_ARCH_LIST=&quot;Volta&quot;\n\nENV FVCORE_CACHE=&quot;\/tmp&quot;\n\n############# SageMaker section ##############\n\nCOPY tested_train_src\/train_src \/opt\/ml\/code\nWORKDIR \/opt\/ml\/code\n\nENV SAGEMAKER_SUBMIT_DIRECTORY \/opt\/ml\/code\nENV SAGEMAKER_PROGRAM train.py\n\nWORKDIR \/\n\nENTRYPOINT [&quot;bash&quot;, &quot;-m&quot;, &quot;start_with_right_hostname.sh&quot;]\n<\/code><\/pre>\n<p>Build command:<\/p>\n<pre><code>docker build -t image-name:tag . --build-arg REGION=&quot;us-east-1&quot;\n<\/code><\/pre>\n<p>Output from docker build<\/p>\n<pre><code>Sending build context to Docker daemon  1.935GB\nStep 1\/12 : ARG REGION=&quot;us-east-1&quot;\nStep 2\/12 : FROM 763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-training:1.8.1-gpu-py36-cu111-ubuntu18.04\n1.8.1-gpu-py36-cu111-ubuntu18.04: Pulling from pytorch-training\n\nd2c87b75: Pulling fs layer \n10be24e1: Pulling fs layer \n7173dcfe: Pulling fs layer \n8de7822d: Pulling fs layer \nbf66c36b: Pulling fs layer \nc74d4d18: Pulling fs layer \nf70a70b2: Pulling fs layer \n4e2cb041: Pulling fs layer \n8ddd4da6: Pulling fs layer \nfac38f0d: Pulling fs layer \na26fd875: Pulling fs layer \n1dca51bb: Pulling fs layer \n0d6bb6c9: Pulling fs layer \n26721764: Pulling fs layer \n956fbe7a: Pulling fs layer \nad4fa2a5: Pulling fs layer \n20c0bd9a: Pulling fs layer \n82804870: Pulling fs layer \n1d1fdc54: Pulling fs layer \n4500c676: Pulling fs layer \n923bbc02: Pulling fs layer \n0c9d88c6: Pulling fs layer \nf5b0d167: Pulling fs layer \n2f2aa1af: Pulling fs layer \nc272e0bb: Pulling fs layer \n311661aa: Pulling fs layer \ned3ef379: Pulling fs layer \n03c2d7ac: Pulling fs layer \n1cefc5dc: Pulling fs layer \n30fd2377: Pulling fs layer \n78d30971: Pulling fs layer \nd18f41de: Pulling fs layer \n4c2aeed5: Pulling fs layer \nf099a687: Pulling fs layer \n253573ff: Pulling fs layer \n515cab8b: Pulling fs layer \n056b70c3: Pulling fs layer \nDigest: sha256:66af111d2bd9dae500ad73a7b427103fe8379cbb24bf4ce7cb7d5770d31cd9322KExtracting  505.2MB\/962.1MB\nStatus: Downloaded newer image for 763104351884.dkr.ecr.us-east-1.amazonaws.com\/pytorch-training:1.8.1-gpu-py36-cu111-ubuntu18.04\n ---&gt; b4191cf0b8c9\nStep 3\/12 : RUN pip3 install torch==1.8.2+cu111 torchvision==0.9.2+cu111 -f https:\/\/download.pytorch.org\/whl\/lts\/1.8\/torch_lts.html\n ---&gt; Running in 7c62740a69c6\nLooking in links: https:\/\/download.pytorch.org\/whl\/lts\/1.8\/torch_lts.html\nCollecting torch==1.8.2+cu111\n  Downloading https:\/\/download.pytorch.org\/whl\/lts\/1.8\/cu111\/torch-1.8.2%2Bcu111-cp36-cp36m-linux_x86_64.whl (1982.2 MB)\nERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n<\/code><\/pre>\n<p>Disk usage before build:<\/p>\n<pre><code>sh-4.2$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\ndevtmpfs        1.9G   76K  1.9G   1% \/dev\ntmpfs           1.9G     0  1.9G   0% \/dev\/shm\n\/dev\/nvme0n1p1  104G   89G   16G  86% \/\n\/dev\/nvme1n1     63G  1.9G   58G   4% \/home\/ec2-user\/SageMaker\n<\/code><\/pre>\n<p>Disk usage after erronous build:<\/p>\n<pre><code>sh-4.2$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\ndevtmpfs        1.9G   76K  1.9G   1% \/dev\ntmpfs           1.9G     0  1.9G   0% \/dev\/shm\n\/dev\/nvme0n1p1  104G  101G  2.4G  98% \/\n\/dev\/nvme1n1     63G  1.9G   58G   4% \/home\/ec2-user\/SageMaker\n<\/code><\/pre>\n<p>Note : I will try to mount directory <code>\/var\/lib\/docker<\/code> to EBS volume at notebook start.<\/p>\n<p>Note : I don't have any issue about attached EBS volume size. My issue is about temporary volume.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1631040430670,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":632,
        "Owner_creation_time":1591041254117,
        "Owner_last_access_time":1664082818733,
        "Owner_location":"Turkey",
        "Owner_reputation":36,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1631040737012,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69093204",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65045458,
        "Question_title":"Amazon Sagemaker - Unable to evaluate payload provided",
        "Question_body":"<p>I built a Sagemaker endpoint that I am attempting to evoke using Lambda+API Gateway.  I'm getting the following error:<\/p>\n<pre><code>&quot;An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message \\&quot;unable to evaluate payload provided\\&quot;\n<\/code><\/pre>\n<p>I know why what it's complaining about, but I don't quite understand why it's occuring.  I have confirmed that the shape of the input data of my lambda function is the same as how I trained the model.  The following is my input payload in lambda:<\/p>\n<pre><code>X = pd.concat([X, rx_norm_dummies, urban_dummies], axis = 1)\npayload = X.to_numpy()\n\nresponse = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                               ContentType='application\/json',\n                               Body=payload)\n<\/code><\/pre>\n<p>In the jupyter notebook where I created my endpoint\/trained my model, I can also access the model using a numpy ndarray so I'm confused why I'm getting this error.<\/p>\n<pre><code>y = X[0:10]\nresult = linear_predictor.predict(y)\nprint(result)\n<\/code><\/pre>\n<p>Here is a modificaiton I make to serialization of the endpoint:<\/p>\n<pre><code>from sagemaker.predictor import csv_serializer, json_deserializer\n\n    linear_predictor.content_type = 'text\/csv'\n    linear_predictor.serializer = csv_serializer\n    linear_predictor.deserializer = json_deserializer\n<\/code><\/pre>\n<p>I'm new when it comes to Sagemaker\/Lambda, so any help would be appreciated and I can send more code to add context if needed.  Tried various foramts and cannot get this to work.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1606525348557,
        "Question_score":2,
        "Question_tags":"python|aws-lambda|amazon-sagemaker",
        "Question_view_count":988,
        "Owner_creation_time":1421421571370,
        "Owner_last_access_time":1663593937387,
        "Owner_location":null,
        "Owner_reputation":1304,
        "Owner_up_votes":87,
        "Owner_down_votes":1,
        "Owner_views":167,
        "Question_last_edit_time":1606857230369,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65045458",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57966245,
        "Question_title":"How to hyperparametrize Amazon SageMaker Training Jobs Console",
        "Question_body":"<p>I'm trying to use de AWS SageMaker Training Jobs console to train a model with H2o.AutoMl.<\/p>\n\n<p>I got stuck trying to set up Hyperparameters, specifically setting up the 'training' field.<\/p>\n\n<pre><code>{'classification': true, 'categorical_columns':'', 'target': 'label'}\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9uR6Y.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9uR6Y.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I'm trying to set up a classification training job (1\/0), and I believe that everything else on the setup page I can cope, but I don't know how to set up the 'training' field. My data is stored on S3 as a CSV file, as the algorithm requires.<\/p>\n\n<p>My data has around 250000 columns, 4 out of them are categorical, one of them is the target, and the remainder is continuous variables (800 MB)<\/p>\n\n<pre><code>target column name = 'y'\ncategorical columns name = 'SIT','HOL','CTH','YTT'\n<\/code><\/pre>\n\n<p>I hope someone could help me.<\/p>\n\n<p>Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1568683965203,
        "Question_score":0,
        "Question_tags":"h2o|training-data|amazon-sagemaker|automl",
        "Question_view_count":68,
        "Owner_creation_time":1509012479113,
        "Owner_last_access_time":1632253986417,
        "Owner_location":"Belo Horizonte, MG, Brasil",
        "Owner_reputation":97,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>After I asked I came across an explanation from <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/aws_marketplace\/using_algorithms\/automl\/AutoML_-_Train_multiple_models_in_parallel.ipynb\" rel=\"nofollow noreferrer\">SageMaker examples.<\/a><\/p>\n\n<p>{classification': 'true', 'categorical_columns': 'SIT','HOL','CTH','YTT','target': 'y'}.<\/p>\n\n<p>Problem solved!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1568727043932,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57966245",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71682848,
        "Question_title":"Why does saving CSV file in S3 work in notebook cell, but not in terminal?",
        "Question_body":"<p>I am trying to save a csv file on s3 by using a buffer object. The strange thing is that all works perfectly fine if I do it in jupyter notebook cell. I can read it again using pandas without any issues. Below is the code snippet:<\/p>\n<pre><code>csv_buffer = StringIO()\ns3_resource = boto3.resource('s3')\ndf.to_csv(csv_buffer, index = False)\n_ = s3_resource.meta.client.put_object(Body = csv_buffer.getvalue(), Bucket = 'bucket', Key = 'temp.csv')\n<\/code><\/pre>\n<p>But, when I try to execute the same code dynamically in my program through command line interface, the file is being saved, albeit when I try to read it using pandas, I am getting error as follows: UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte. As expected, when I download the file from s3, there are strange characters.<\/p>\n<p>I have tried various solutions including new conda environment, using parquet instead of csv, adding ContentType parameter in put object. But nothing seems to work. Also, when I print the contents of csv buffer in my script, I can see the string contents as well. Any help would be much appreciated.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1648667366867,
        "Question_score":1,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":44,
        "Owner_creation_time":1648666469280,
        "Owner_last_access_time":1663919608643,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71682848",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72825806,
        "Question_title":"Run jupyter notebook through command line and log outputs to a file",
        "Question_body":"<p>I am running a notebook on command line in AWS SageMaker but I am not able to log outputs to a file. I added <code>print<\/code>, <code>logging.info<\/code> and <code>sys.stdout.write<\/code> statements in hope of capturing something but to no avail.<\/p>\n<p>Below is the code I am using<\/p>\n<pre><code>NOTEBOOK_FILE=&quot;\/home\/ec2-user\/SageMaker\/mynb.ipynb&quot;\nLOG_FILE=&quot;\/home\/ec2-user\/SageMaker\/logs.txt&quot;\n\nnohup jupyter nbconvert  --to notebook --inplace --ExecutePreprocessor.timeout=-1 --ExecutePreprocessor.kernel_name=python3 --execute &quot;$NOTEBOOK_FILE&quot; &gt; &quot;$LOG_FILE&quot; &amp;\n<\/code><\/pre>\n<p>On doing <code>cat &quot;$LOG_FILE&quot;<\/code> I get just a couple lines of logs and nothing after that. Output is below -<\/p>\n<pre><code>[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.\n[NbConvertApp] Converting notebook \/home\/ec2-user\/SageMaker\/tata1mg.ipynb to notebook\n[NbConvertApp] Executing notebook with kernel: python3\n<\/code><\/pre>\n<p>PS:<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1656660742190,
        "Question_score":0,
        "Question_tags":"python|python-3.x|jupyter|amazon-sagemaker",
        "Question_view_count":146,
        "Owner_creation_time":1366987249713,
        "Owner_last_access_time":1664022858883,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":7127,
        "Owner_up_votes":2089,
        "Owner_down_votes":92,
        "Owner_views":987,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72825806",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70508395,
        "Question_title":"Convert .pt to .pth?",
        "Question_body":"<p>I've been trying to compile my custom trained YoloV5 model using SageMaker Neo.\nThe compilation gives an error :<\/p>\n<pre><code>ClientError: InputConfiguration: No pth file found for PyTorch model. \nPlease make sure the framework you select is correct.\n<\/code><\/pre>\n<p>The weights are a <code>.pt<\/code> file.\nIs there a way to convert the <code>.pt<\/code> file to <code>.pth<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640701511037,
        "Question_score":0,
        "Question_tags":"python|pytorch|amazon-sagemaker|yolov5",
        "Question_view_count":563,
        "Owner_creation_time":1527945118903,
        "Owner_last_access_time":1663948511827,
        "Owner_location":null,
        "Owner_reputation":9,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":1640701891236,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70508395",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70363229,
        "Question_title":"No space for sagemaker Studio lab to upload or download file",
        "Question_body":"<p>The SageMaker studio lab FAQ says there are 15GB space for each user, but I can't even upload or download a 900MB DataSet.\nWhen I upload or download it, return<\/p>\n<pre><code>Unexpected error while saving file: sagemaker-studiolab-notebooks\/preprocessed20152019.zip [Errno 2] \nNo such file or directory: \n'\/home\/studio-lab-user\/sagemaker-studiolab-notebooks\/.~preprocessed20152019.zip'\n -&gt; '\/home\/studio-lab-user\/sagemaker-studiolab-notebooks\/preprocessed20152019.zip'\n<\/code><\/pre>\n<p>and sometimes it returned &quot;no space for device&quot;.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1639569106200,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":290,
        "Owner_creation_time":1541417544600,
        "Owner_last_access_time":1641543663313,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70363229",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55067802,
        "Question_title":"Discrepancy between AWS Glue and its Dev Endpoint",
        "Question_body":"<p>My understanding is Dev Endpoints in AWS Glue can be used to develop code iteratively and then deploy it to a Glue job. I find this specially useful when developing Spark jobs because every time you run a job, it takes several minutes to launch a Hadoop cluster in the background. However, I am seeing a discrepancy when using Python shell in Glue instead of Spark. <code>Import pg<\/code> doesn't work in a Dev Endpoint I created using Sagemaker JupyterLab Python notebook, but works in AWS Glue when I create a job using Python shell. Shouldn't the same libraries exist in the dev endpoint that exist in Glue? What is the point of having a dev endpoint if you cannot reproduce the same code in both places (dev endpoint and the Glue job)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1552064664580,
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-glue|amazon-sagemaker",
        "Question_view_count":261,
        "Owner_creation_time":1462207957683,
        "Owner_last_access_time":1653409817263,
        "Owner_location":null,
        "Owner_reputation":1305,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":174,
        "Question_last_edit_time":1552064982860,
        "Answer_body":"<p>Firstly, Python shell jobs would not launch a Hadooo Cluster in the backend as it does not give you a Spark environment for your jobs.\nSecondly, since PyGreSQL is not written in Pure Python, it will not work with Glue's native environment (Glue Spark Job, Dev endpoint etc)\nThirdly, Python Shell has additional support for certain package built-in.<\/p>\n\n<p>Thus, I don't see a point of using DevEndpoint for Python Shell jobs.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1552530714763,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55067802",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59445162,
        "Question_title":"how to load image data from the bucket to AWS sagemaker notebook?",
        "Question_body":"<p>The images are present as folders - train and test in my s3 bucket. I want to use them as it is in my sagemaker notebook. For example, like on my local server I use test_dir = \"C:\\Users\\catvdog\\dataset\\test\".<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1577025463090,
        "Question_score":0,
        "Question_tags":"python|machine-learning|amazon-s3|deep-learning|amazon-sagemaker",
        "Question_view_count":931,
        "Owner_creation_time":1576747245357,
        "Owner_last_access_time":1632464838653,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59445162",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57857726,
        "Question_title":"Broken DAG: urllib3 1.25.3 (\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages), Requirement.parse('urllib3<1.25,>=1.21'), {'sagemaker'}",
        "Question_body":"<p>I have created a DAG in Airflow with SageMakerOperators and I have not been able to make them work. The title is the error that appears in the airflow GUI. For solving it, I have made the following tries:<\/p>\n\n<pre><code>sudo pip3 uninstall urllib3 &amp;&amp; sudo pip3 install urllib3==1.22 \nsudo pip3 install urllib3==1.22 --upgrade\nsudo pip3 install urllib3==1.22 -t \/home\/ubuntu\/.local\/lib\/python3.7\/site-packages -upgrade\n<\/code><\/pre>\n\n<p>But I am still getting the error in the GUI. Plus, in the console of the webserver I am getting:<\/p>\n\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages\/urllib3-1.22.dist-info\/METADATA'\n<\/code><\/pre>\n\n<p>The thing is that if I make <code>pip3 show urllib3<\/code> I get the version 1.22:\n<a href=\"https:\/\/i.stack.imgur.com\/i5y8i.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/i5y8i.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>However, it says dist-packages instead of site-packages. In addition, trying to go to <code>\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages\/urllib3-1.22.dist-info\/<\/code> for trying to solve the metadata file not found error, the directory does not exists. \n<a href=\"https:\/\/i.stack.imgur.com\/44H4I.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/44H4I.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/2CnJl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2CnJl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I am totally lost at this point. How could I solve this problem?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1568045843843,
        "Question_score":0,
        "Question_tags":"python|airflow|amazon-sagemaker|urllib3",
        "Question_view_count":343,
        "Owner_creation_time":1523298968403,
        "Owner_last_access_time":1663934452963,
        "Owner_location":null,
        "Owner_reputation":1754,
        "Owner_up_votes":396,
        "Owner_down_votes":76,
        "Owner_views":197,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Here you go.<\/p>\n\n<p>Airflow is looking in the local (user) Python installation for the library but <code>urllib3<\/code> is installed for all users. It's weird but try doing <code>pip3 install --user urllib3==1.22<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1571189751312,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57857726",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64700093,
        "Question_title":"AWS Sagemaker - df.to_csv error write() argument 1 must be unicode, not str",
        "Question_body":"<p>I am trying to save a file to S3 bucket from sagemaker instance. and below line throws an error!<\/p>\n<pre><code>df.to_csv(&quot;s3:\/\/informatri\/Drug_Data_Cleaned.csv&quot;), index = False)\n<\/code><\/pre>\n<pre><code>error - \nTypeErrorTraceback (most recent call last)\n&lt;ipython-input-28-d33896172c11&gt; in &lt;module&gt;()\n      1 \n----&gt; 2 a.to_csv(&quot;s3:\/\/informatri\/{}&quot;.format('Drug_Data_Cleaned.csv'), index = False)\n\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p27\/lib\/python2.7\/site-packages\/pandas\/core\/generic.pyc in to_csv(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\n   3018                                  doublequote=doublequote,\n   3019                                  escapechar=escapechar, decimal=decimal)\n-&gt; 3020         formatter.save()\n   3021 \n   3022         if path_or_buf is None:\n\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p27\/lib\/python2.7\/site-packages\/pandas\/io\/formats\/csvs.pyc in save(self)\n    170                 self.writer = UnicodeWriter(f, **writer_kwargs)\n    171 \n--&gt; 172             self._save()\n    173 \n    174         finally:\n\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p27\/lib\/python2.7\/site-packages\/pandas\/io\/formats\/csvs.pyc in _save(self)\n    272     def _save(self):\n    273 \n--&gt; 274         self._save_header()\n    275 \n    276         nrows = len(self.data_index)\n\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p27\/lib\/python2.7\/site-packages\/pandas\/io\/formats\/csvs.pyc in _save_header(self)\n    240         if not has_mi_columns or has_aliases:\n    241             encoded_labels += list(write_cols)\n--&gt; 242             writer.writerow(encoded_labels)\n    243         else:\n    244             # write out the mi\n\nTypeError: write() argument 1 must be unicode, not str\n<\/code><\/pre>\n<p>I tried the following:<\/p>\n<pre><code>df.to_csv(&quot;s3:\/\/informatri\/Drug_Data_Cleaned.csv&quot;), index = False, encoding = 'utf-8', sep = '\\t')\n<\/code><\/pre>\n<p>I still get the same error. If I do only:<\/p>\n<pre><code>df.to_csv(&quot;Drug_Data_Cleaned.csv&quot;), index = False) \n<\/code><\/pre>\n<p>It gets saved locally all fine. So not a problem with dataframe or the name etc. It has to do something with saving to S3 bucket.\nI have used similar ways to save to s3 bucket many times in the past and it has worked perfectly fine. Hence, I was wondering why the error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1604589432083,
        "Question_score":1,
        "Question_tags":"python|pandas|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":275,
        "Owner_creation_time":1555475748810,
        "Owner_last_access_time":1663961286663,
        "Owner_location":"Pennsylvania, USA",
        "Owner_reputation":351,
        "Owner_up_votes":87,
        "Owner_down_votes":6,
        "Owner_views":57,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I fixed this problem.<\/p>\n<p>The error was that the Sagemaker ipynb notebook was opened in conda_python2.7 or so. Just re-wrote the script in conda_python3 and then everything worked fine :)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1612607749436,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64700093",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71187747,
        "Question_title":"AWS sagemaker training job (Tensorflow) halts at Epoch 1",
        "Question_body":"<p>I am trying to train Maskrcnn with custom dataset. The code is running fine on my local machine in the same docker container, however, it gets stuck at the first epoch when I use aws sagemaker.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/C3Z1T.png\" rel=\"nofollow noreferrer\">The log my error seen on sagemaker notebook for training job<\/a><\/p>\n<p>I am using Tensorflow 2 implementing the github code provided by <a href=\"https:\/\/github.com\/simone-viozzi\/Mask-RCNN-training-with-docker-containers-on-Sagemaker\" rel=\"nofollow noreferrer\">https:\/\/github.com\/simone-viozzi\/Mask-RCNN-training-with-docker-containers-on-Sagemaker<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1645294110193,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":117,
        "Owner_creation_time":1645293436933,
        "Owner_last_access_time":1659077375647,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71187747",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55446951,
        "Question_title":"Do images with large dimensions (e.g. 2000 x 2000) be auto-scaled to 300 x 300 when using them for training data in AWS Sagemaker?",
        "Question_body":"<p>I'm working on a project that trains an ML model to predict the location of Waldo in a Where's Wally? image using AWS Sagemaker with the underlying object detection algorithm being Single Shot Detection, but I am thinking that using an actual puzzle image with dimensions like 2000 x 2000 as training data is not possible and that SSD will auto-resize the image to 300 x 300 which would render Waldo a meaningless blur.  Does SSD re-size images automatically, or will it train on the 2000 x 2000 image?  Should I crop resize all puzzles to 300 x 300 images containing Waldo, or can I include a mix of actual puzzle images with dimensions 2000+ x 2000+ and the 300 x 300 cropped images?<\/p>\n\n<p>I'm considering augmenting the data by cropping these larger images at locations that contain Wally so that I can have 300 x 300 images where Wally isn't reduced to a smudge on a page and is actually visible - is this a good idea?  I am thinking that SSD does train on the 2000 x 2000 image, but the FPS will reduce by a lot - is this wrong?  I feel like if I don't use the 2000 x 2000 image for training, in the prediction stage where I start feeding the model images with large dimensions (actual puzzle images), the model won't be able to predict locations accurately - is this not the case?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1554081184047,
        "Question_score":0,
        "Question_tags":"object-detection|amazon-sagemaker|data-augmentation",
        "Question_view_count":239,
        "Owner_creation_time":1554079262983,
        "Owner_last_access_time":1646728800483,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55446951",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48650152,
        "Question_title":"AWS uploading file into wrong bucket",
        "Question_body":"<p>I am using AWS Sagemaker and trying to upload a data folder into S3 from Sagemaker. I am trying to do is to upload my data into the s3_train_data directory (the directory exists in S3). However, it wouldn't upload it in that bucket, but in a default Bucket that has been created, and in turn creates a new folder directory with the S3_train_data variables.<\/p>\n\n<p>code to input in directory<\/p>\n\n<pre><code>import os\nimport sagemaker\nfrom sagemaker import get_execution_role\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\nbucket = &lt;bucket name&gt;\nprefix = &lt;folders1\/folders2&gt;\nkey = &lt;input&gt;\n\n\ns3_train_data = 's3:\/\/{}\/{}\/{}\/'.format(bucket, prefix, key)\n\n\n#path 'data' is the folder in the Jupyter Instance, contains all the training data\ninputs = sagemaker_session.upload_data(path= 'data', key_prefix= s3_train_data)\n<\/code><\/pre>\n\n<p>Is the problem in the code or more in how I created the notebook?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1517943817340,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":722,
        "Owner_creation_time":1509392519217,
        "Owner_last_access_time":1552779156337,
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1525621837032,
        "Answer_body":"<p>You could look at the Sample notebooks, how to upload the data S3 bucket \nThere have many ways. I am just giving you hints to answer. \nAnd you forgot create a boto3 session to access the S3 bucket <\/p>\n\n<p><strong>It is one of the ways to do it.<\/strong> <\/p>\n\n<pre><code>import os \nimport urllib.request\nimport boto3\n\ndef download(url):\n    filename = url.split(\"\/\")[-1]\n    if not os.path.exists(filename):\n        urllib.request.urlretrieve(url, filename)\n\n\ndef upload_to_s3(channel, file):\n    s3 = boto3.resource('s3')\n    data = open(file, \"rb\")\n    key = channel + '\/' + file\n    s3.Bucket(bucket).put_object(Key=key, Body=data)\n\n\n# caltech-256\ndownload('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-train.rec')\nupload_to_s3('train', 'caltech-256-60-train.rec')\ndownload('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-val.rec')\nupload_to_s3('validation', 'caltech-256-60-val.rec')\n<\/code><\/pre>\n\n<p>link : <a href=\"https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb\" rel=\"nofollow noreferrer\">https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb<\/a><\/p>\n\n<p><strong>Another way to do it.<\/strong> <\/p>\n\n<pre><code>bucket = '&lt;your_s3_bucket_name_here&gt;'# enter your s3 bucket where you will copy data and model artifacts\nprefix = 'sagemaker\/breast_cancer_prediction' # place to upload training files within the bucket\n# do some processing then prepare to push the data. \n\nf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))\nf.seek(0)\n\nboto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', train_file)).upload_fileobj(f)\n<\/code><\/pre>\n\n<p>Link : <a href=\"https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_applying_machine_learning\/breast_cancer_prediction\/Breast%20Cancer%20Prediction.ipynb\" rel=\"nofollow noreferrer\">https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_applying_machine_learning\/breast_cancer_prediction\/Breast%20Cancer%20Prediction.ipynb<\/a><\/p>\n\n<p>Youtube link : <a href=\"https:\/\/www.youtube.com\/watch?v=-YiHPIGyFGo\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=-YiHPIGyFGo<\/a> - how to pull the data in S3 bucket.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1518097617247,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48650152",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56773989,
        "Question_title":"In Amazon SageMaker, what (if any) is the difference between an inference and prediction?",
        "Question_body":"<p>Amazon SageMaker has <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">inference pipelines<\/a> that process requests for inferences on data. It sounds as though inferences are similar (or perhaps identical) to predictions. Are there any differences between inferences and predictions? If so, what? If not, why not just call it a prediction pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1561555658777,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":42,
        "Owner_creation_time":1336973807643,
        "Owner_last_access_time":1655749162853,
        "Owner_location":"Minneapolis, MN, United States",
        "Owner_reputation":1907,
        "Owner_up_votes":692,
        "Owner_down_votes":6,
        "Owner_views":174,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Inference usually refers to applying a learned transformation to input data. That learned transformation could be something else than a prediction (eg dim reduction, clustering, entity extraction etc). So calling that process a prediction would be a bit too restrictive in my opinion<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1561569885852,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56773989",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61832086,
        "Question_title":"Having issues reading S3 bucket when transitioning a tensorflow model from local machine to AWS SageMaker",
        "Question_body":"<p>When testing on a local machine in Python I would normally use the following to read a training set with sub-directories of all the classes and files\/class:<\/p>\n\n<pre><code>train_path = r\"C:\\temp\\coins\\PCGS - Gold\\train\"\n\ntrain_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(100,100), classes=['0','1',2','3' etc...], batch_size=32)\n<\/code><\/pre>\n\n<p><strong>Found 4100 images belonging to 22 classes.<\/strong><\/p>\n\n<p>but on AWS SageMaker's Jupyter notebook I am now pulling the files from an S3 bucket.  I tried the following: <\/p>\n\n<pre><code>bucket = \"coinpath\"\n\ntrain_path = 's3:\/\/{}\/{}\/train'.format(bucket, \"v1\")   #note that the directory structure is coinpath\/v1\/train where coinpath is the bucket\n\ntrain_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(100,100), classes=\n['0','1',2','3' etc...], batch_size=32)\n<\/code><\/pre>\n\n<p>but I get: ** Found 0 images belonging to 22 classes.**<\/p>\n\n<p>Looking for some guidance on the right way to pull training data from S3.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1589605066783,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":608,
        "Owner_creation_time":1583123749267,
        "Owner_last_access_time":1663877641340,
        "Owner_location":"Washington D.C., DC, USA",
        "Owner_reputation":317,
        "Owner_up_votes":60,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":"<p>From <a href=\"https:\/\/stackoverflow.com\/questions\/54736505\/ideal-way-to-read-data-in-bucket-stored-batches-of-data-for-keras-ml-training-in\">Ideal way to read data in bucket stored batches of data for Keras ML training in Google Cloud Platform?<\/a> \"ImageDataGenerator.flow_from_directory() currently does not allow you to stream data directly from a GCS bucket. \"<\/p>\n\n<p>I had to download the image from S3 first.  This is best for latency reasons as well. <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1589768612456,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61832086",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54249334,
        "Question_title":"ImportError libopenblasp-r0 cannot open shared object file No such file or directory -SageMaker",
        "Question_body":"<p>I am trying train my model code using Docker Container - AWS SageMaker using following code.<\/p>\n\n<pre>\n    'https:\/\/github.com\/awslabs\/amazon-sagemaker- \nexamples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb'\n<\/pre>\n\n<p>But I get below error when I Try to train my model using <\/p>\n\n<pre><code> tree.fit(data_location)\n<\/code><\/pre>\n\n<p>Error:<\/p>\n\n<pre><code>  Traceback (most recent call last):\n    File \"\/opt\/program\/train\", line 17, in &lt;module&gt;\n     from sklearn import tree\n    File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sklearn\/__init__.py\", line \n      64, in &lt;module&gt;\n     from .base import clone\n    File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sklearn\/base.py\", line \n       13, in &lt;module&gt;\n      from .utils.fixes import signature\n    File \"\/usr\/local\/lib\/python2.7\/dist- \n       packages\/sklearn\/utils\/__init__.py\", line 16, in &lt;module&gt;\n      from .fixes import _Sequence as Sequence\n    File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sklearn\/utils\/fixes.py\", \n       line 85, in &lt;module&gt;\n      from scipy.special import boxcox  # noqa\n    File \"\/usr\/local\/lib\/python2.7\/dist- \n       packages\/scipy\/special\/__init__.py\", line 641, in &lt;module&gt;\n       from ._ufuncs import *\n    ImportError: libopenblasp-r0-8dca6697.3.0.dev.so: cannot open shared \n       object file: No such file or directory\n<\/code><\/pre>\n\n<p>error message 2<\/p>\n\n<pre><code>   Error for Training job decision-trees-sample-2019-01-18-07-44-37-282: Failed Reason: AlgorithmError: Exit Code: 1\n<\/code><\/pre>\n\n<p>I wend to the directory and did not find 'sklearn' directory.<\/p>\n\n<pre><code>  sh-4.2$ pwd\n    \/usr\/local\/lib\/python2.7\/dist-packages\n  sh-4.2$ ls -l\n    total 3244\n  -rwxr-xr-x 1 root root 3318568 Sep 18 03:23 cv2.so\n<\/code><\/pre>\n\n<p>My current jupyter notebook points to root environment and it has sklearn package available , not sure how make it available in above location where I see error, not sure if this is what will resolve the issue  or something else needs to be done.<\/p>\n\n<p>I am new to Amazon SageMaker.<\/p>\n\n<p>Expected result: I am expecting the training job to complete without error<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1547795942457,
        "Question_score":1,
        "Question_tags":"python|python-2.7|docker|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":1661,
        "Owner_creation_time":1419997605110,
        "Owner_last_access_time":1663052127303,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":1050,
        "Owner_up_votes":749,
        "Owner_down_votes":10,
        "Owner_views":168,
        "Question_last_edit_time":1547797769223,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54249334",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58918995,
        "Question_title":"Invoking sagemaker endpoint using AWS-Lamda throwing parameter validation error",
        "Question_body":"<p>I trained an ML model in AWS Sagemaker and created an endpoint. I want to invoke it using AWS-Lambda. My model has 30 predictor variables. So I passed them into test event of Lambda as dict type as mentioned below<\/p>\n\n<pre><code>{\n  \"Time\": \"10 \",\n  \"V1\": \"1.449043781 \",\n  \"V2\": \"-1.176338825 \",\n  \"V3\": \"0.913859833 \",\n  \"V4\": \"-1.375666655 \",\n  \"V5\": \"-1.971383165 \",\n  \"V6\": \"-0.629152139 \",\n  \"V7\": \"-1.423235601 \",\n  \"V8\": \"0.048455888 \",\n  \"V9\": \"-1.720408393 \",\n  \"V10\": \"1.626659058 \",\n  \"V11\": \"1.19964395 \",\n  \"V12\": \"-0.671439778 \",\n  \"V13\": \"-0.513947153 \",\n  \"V14\": \"-0.095045045 \",\n  \"V15\": \"0.230930409 \",\n  \"V16\": \"0.031967467 \",\n  \"V17\": \"0.253414716 \",\n  \"V18\": \"0.854343814 \",\n  \"V19\": \"-0.221365414 \",\n  \"V20\": \"-0.387226474 \",\n  \"V21\": \"-0.009301897 \",\n  \"V22\": \"0.313894411 \",\n  \"V23\": \"0.027740158 \",\n  \"V24\": \"0.500512287 \",\n  \"V25\": \"0.251367359 \",\n  \"V26\": \"-0.129477954 \",\n  \"V27\": \"0.042849871 \",\n  \"V28\": \"0.016253262 \",\n  \"Amount\": \"7.8\"\n}\n<\/code><\/pre>\n\n<p>Now I ran below mentioned code in AWS Lambda <\/p>\n\n<pre><code>import json\nimport os\nimport csv\nimport boto3\nimport io\nimport codecs\n\nendpoint_name = os.environ['ENDPOINT_NAME']\nruntime = boto3.client('runtime.sagemaker')\n\n\ndef lambda_handler(event, context):\n    print(\"received event: \"+json.dumps(event,indent=2))\n    data = json.loads(json.dumps(event))\n    payload = data[\"Time\"]+data[\"V1\"]+data[\"V2\"]+data[\"V3\"]+data[\"V4\"]+data[\"V5\"]+data[\"V6\"]+data[\"V7\"]+data[\"V8\"]+data[\"V9\"]+data[\"V10\"]+data[\"V11\"]+data[\"V12\"]+data[\"V13\"]+data[\"V14\"]+data[\"V15\"]+data[\"V16\"]+data[\"V17\"]+data[\"V18\"]+data[\"V19\"]+data[\"V20\"]+data[\"V21\"]+data[\"V22\"]+data[\"V23\"]+data[\"V24\"]+data[\"V25\"]+data[\"V26\"]+data[\"V27\"]+data[\"V28\"]+data[\"Amount\"]\n    payload = payload.split(\" \")\n    payload = [codecs.encode(i,'utf-8') for i in payload]\n    payload=[bytearray(i) for i in payload]\n    print(payload)\n    response = runtime.invoke_endpoint(EndpointName=endpoint_name,ContentType='text\/csv',Body=payload)\n    print(response)\n    result=json.loads(response['Body'].decode())\n    pred = int(float(response))\n    predicted_label = 'fraud' if pred==1 else 'not fraud'\n    return predicted_label\n<\/code><\/pre>\n\n<p>This code is throwing below this error<\/p>\n\n<pre><code>[ERROR] ParamValidationError: Parameter validation failed:\nInvalid type for parameter Body, value: [bytearray(b'10'), bytearray(b'1.449043781'), bytearray(b'-1.176338825'), bytearray(b'0.913859833'), bytearray(b'-1.375666655'), bytearray(b'-1.971383165'), bytearray(b'-0.629152139'), bytearray(b'-1.423235601'), bytearray(b'0.048455888'), bytearray(b'-1.720408393'), bytearray(b'1.626659058'), bytearray(b'1.19964395'), bytearray(b'-0.671439778'), bytearray(b'-0.513947153'), bytearray(b'-0.095045045'), bytearray(b'0.230930409'), bytearray(b'0.031967467'), bytearray(b'0.253414716'), bytearray(b'0.854343814'), bytearray(b'-0.221365414'), bytearray(b'-0.387226474'), bytearray(b'-0.009301897'), bytearray(b'0.313894411'), bytearray(b'0.027740158'), bytearray(b'0.500512287'), bytearray(b'0.251367359'), bytearray(b'-0.129477954'), bytearray(b'0.042849871'), bytearray(b'0.016253262'), bytearray(b'7.8')], type: &lt;class 'list'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\n<\/code><\/pre>\n\n<p>I understand that somehow I need to pass my 30 features into Lambda function such that data type of <code>payload<\/code> is compatible with <code>ContentType<\/code> for <code>respnse<\/code> to work. Can someone please explain how to do it? \nedit: I'm trying this problem by looking at <a href=\"http:\/\/%20https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">this<\/a> aws blog. I don't quite understand how the author of above mentioned blog did it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1574094491987,
        "Question_score":0,
        "Question_tags":"python-3.x|amazon-web-services|machine-learning|aws-lambda|amazon-sagemaker",
        "Question_view_count":895,
        "Owner_creation_time":1568318861627,
        "Owner_last_access_time":1663770939230,
        "Owner_location":"Hyderabad, Telangana, India",
        "Owner_reputation":486,
        "Owner_up_votes":28,
        "Owner_down_votes":2,
        "Owner_views":75,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58918995",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61165665,
        "Question_title":"How long does it take for a Sagemaker endpoint configuration's training job to finish so you can create the endpoint?",
        "Question_body":"<p>I'm new to Sagemaker but have been waiting a few hours for a Sagemaker training job to complete so that I can create the endpoint... The Sagemaker console shows a Create endpoint button, but when I press it, it doesn't work. The end point configuration still has a spinning icon for \"Training job\" <\/p>\n\n<p>How long does it typically take for a Sagemaker endpoint to spin up? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586653959430,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1364,
        "Owner_creation_time":1275549180447,
        "Owner_last_access_time":1664080333383,
        "Owner_location":"San Francisco, CA",
        "Owner_reputation":18781,
        "Owner_up_votes":965,
        "Owner_down_votes":37,
        "Owner_views":1916,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61165665",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58110595,
        "Question_title":"How do I access Amazon Sagemaker through React Native?",
        "Question_body":"<p>I am currently using react-native to build a mobile application. I need to access a machine learning model in order to send pictures for segmentation. I want to be able to receive a segmented picture back to have the background of the picture cut out. I am trying to use Amazon Sagemaker (because it seems to be a easy to work with package, but if there are other ways to do it, please let me know).<\/p>\n\n<p>On <a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/?sc_icampaign=pac-sagemaker-console-tutorial&amp;sc_ichannel=ha&amp;sc_icontent=awssm-2276&amp;sc_iplace=console-body&amp;trk=ha_awssm-2276\" rel=\"nofollow noreferrer\">this<\/a> Sagemaker quick-start guide, on step 5a, it states:<\/p>\n\n<blockquote>\n  <p>5a. To deploy the model on a server and create an endpoint that you can access, copy the following code into the next code cell and select Run:\n  xgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')<\/p>\n<\/blockquote>\n\n<p>I want to host everything on AWS and not have to run a separate server. What service\/process could I use that would allow me to create an endpoint that I can access through react-native?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1569478549487,
        "Question_score":1,
        "Question_tags":"amazon-web-services|react-native|mobile|amazon-sagemaker",
        "Question_view_count":717,
        "Owner_creation_time":1488334447850,
        "Owner_last_access_time":1583962966363,
        "Owner_location":null,
        "Owner_reputation":29,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":"<p>To summarize the conversation in the comments:<\/p>\n\n<p>Once you have your model trained, tuned, and deployed (which is not a simple process), you can call the endpoint of the model using the <a href=\"https:\/\/github.com\/aws\/aws-sdk-js\" rel=\"nofollow noreferrer\">AWS SDK for JavaScript<\/a>, that you install by:<\/p>\n\n<pre><code>npm install aws-sdk\nvar AWS = require('aws-sdk\/dist\/aws-sdk-react-native');\n<\/code><\/pre>\n\n<p>you include in the HTML as:<\/p>\n\n<pre><code>&lt;script src=\"https:\/\/sdk.amazonaws.com\/js\/aws-sdk-2.538.0.min.js\"&gt;&lt;\/script&gt;\n<\/code><\/pre>\n\n<p>And when you want to call the endpoint you invoke it like that:<\/p>\n\n<pre><code>var params = {\n  Body: Buffer.from('...') || 'STRING_VALUE' \/* Strings will be Base-64 encoded on your behalf *\/, \/* required *\/\n  EndpointName: 'STRING_VALUE', \/* required *\/\n  Accept: 'STRING_VALUE',\n  ContentType: 'STRING_VALUE',\n  CustomAttributes: 'STRING_VALUE'\n};\nsagemakerruntime.invokeEndpoint(params, function(err, data) {\n  if (err) console.log(err, err.stack); \/\/ an error occurred\n  else     console.log(data);           \/\/ successful response\n});\n<\/code><\/pre>\n\n<p>You can check out the <a href=\"https:\/\/aws-amplify.github.io\" rel=\"nofollow noreferrer\">Amplify Library<\/a> that can take some of the heavy liftings such as getting IAM permissions to call the API, a user log in and many others. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1569671934552,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58110595",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65168915,
        "Question_title":"AWS SageMaker - How to load trained sklearn model to serve for inference?",
        "Question_body":"<p>I am trying to deploy a model trained with sklearn to an endpoint and serve it as an API for predictions. All I want to use sagemaker for, is to deploy and server model I had serialised using <code>joblib<\/code>, nothing more. every blog I have read and sagemaker python documentation showed that sklearn model had to be trained on sagemaker in order to be deployed in sagemaker.<\/p>\n<p>When I was going through the SageMaker documentation I learned that sagemaker does let users <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#load-a-model\" rel=\"nofollow noreferrer\">load a serialised model<\/a> stored in S3 as shown below:<\/p>\n<pre><code>def model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return clf\n<\/code><\/pre>\n<p>And this is what documentation says about the argument <code>model_dir<\/code>:<\/p>\n<blockquote>\n<p>SageMaker will inject the directory where your model files and\nsub-directories, saved by save, have been mounted. Your model function\nshould return a model object that can be used for model serving.<\/p>\n<\/blockquote>\n<p>This again means that training has to be done on sagemaker.<\/p>\n<p>So, is there a way I can just specify the S3 location of my serialised model and have sagemaker de-serialise(or load) the model from S3 and use it for inference?<\/p>\n<h2>EDIT 1:<\/h2>\n<p>I used code in the answer to my application and I got below error when trying to deploy from notebook of SageMaker studio. I believe SageMaker is screaming that training wasn't done on SageMaker.<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-4-6662bbae6010&gt; in &lt;module&gt;\n      1 predictor = model.deploy(\n      2     initial_instance_count=1,\n----&gt; 3     instance_type='ml.m4.xlarge'\n      4 )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, **kwargs)\n    770         &quot;&quot;&quot;\n    771         removed_kwargs(&quot;update_endpoint&quot;, kwargs)\n--&gt; 772         self._ensure_latest_training_job()\n    773         self._ensure_base_job_name()\n    774         default_name = name_from_base(self.base_job_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in _ensure_latest_training_job(self, error_message)\n   1128         &quot;&quot;&quot;\n   1129         if self.latest_training_job is None:\n-&gt; 1130             raise ValueError(error_message)\n   1131 \n   1132     delete_endpoint = removed_function(&quot;delete_endpoint&quot;)\n\nValueError: Estimator is not associated with a training job\n<\/code><\/pre>\n<p>My code:<\/p>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\n# from sagemaker.pytorch import PyTorchModel\nfrom sagemaker.sklearn import SKLearn\nfrom sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n\nsm_role = sagemaker.get_execution_role()  # IAM role to run SageMaker, access S3 and ECR\n\nmodel_file = &quot;s3:\/\/sagemaker-manual-bucket\/sm_model_artifacts\/model.tar.gz&quot;   # Must be &quot;.tar.gz&quot; suffix\n\nclass AnalysisClass(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super().__init__(\n            endpoint_name,\n            sagemaker_session=sagemaker_session,\n            serializer=json_serializer,\n            deserializer=json_deserializer,   # To be able to use JSON serialization\n            content_type='application\/json'   # To be able to send JSON as HTTP body\n        )\n\nmodel = SKLearn(model_data=model_file,\n                entry_point='inference.py',\n                name='rf_try_1',\n                role=sm_role,\n                source_dir='code',\n                framework_version='0.20.0',\n                instance_count=1,\n                instance_type='ml.m4.xlarge',\n                predictor_cls=AnalysisClass)\npredictor = model.deploy(initial_instance_count=1,\n                         instance_type='ml.m4.xlarge')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1607263332140,
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-s3|scikit-learn|amazon-sagemaker",
        "Question_view_count":3221,
        "Owner_creation_time":1559910246180,
        "Owner_last_access_time":1664039951323,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":2046,
        "Owner_up_votes":2858,
        "Owner_down_votes":5,
        "Owner_views":369,
        "Question_last_edit_time":1607399645212,
        "Answer_body":"<p>Yes you can. AWS documentation focuses on end-to-end from training to deployment in SageMaker which makes the impression that training has to be done on sagemaker. AWS documentation and examples should have clear separation among Training in Estimator, Saving and loading model, and Deployment model to SageMaker Endpoint.<\/p>\n<h2>SageMaker Model<\/h2>\n<p>You need to create the <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-model.html\" rel=\"nofollow noreferrer\">AWS::SageMaker::Model<\/a> resource which refers to the &quot;model&quot; you have trained <strong>and more<\/strong>. AWS::SageMaker::Model is in CloudFormation document but it is only to explain what AWS resource you need.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\">CreateModel<\/a> API creates a SageMaker model resource. The parameters specifie the docker image to use, model location in S3, IAM role to use, etc. See <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-code-load-artifacts\" rel=\"nofollow noreferrer\">How SageMaker Loads Your Model Artifacts<\/a>.<\/p>\n<h3>Docker image<\/h3>\n<p>Obviously you need the framework e.g. ScikitLearn, TensorFlow, PyTorch, etc that you used to train your model to get inferences. You need a docker image that has the framework, and HTTP front end to respond to the prediction calls. See <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\" rel=\"nofollow noreferrer\">SageMaker Inference Toolkit<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-toolkits.html\" rel=\"nofollow noreferrer\">Using the SageMaker Training and Inference Toolkits<\/a>.<\/p>\n<p>To build the image is not easy. Hence AWS provides pre-built images called <a href=\"https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\" rel=\"nofollow noreferrer\">AWS Deep Learning Containers<\/a> and available images are in <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">Github<\/a>.<\/p>\n<p>If your framework and the version is listed there, you can use it as the image. Otherwise you need to build by yourself. See <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-mlops-workshop\/blob\/master\/lab\/01_CreateAlgorithmContainer\/01_Creating%20a%20Classifier%20Container.ipynb\" rel=\"nofollow noreferrer\">Building a docker container for training\/deploying our classifier<\/a>.<\/p>\n<h2>SageMaker Python SDK for Frameworks<\/h2>\n<p>Create SageMaker Model by yourself using API is hard. Hence AWS SageMaker Python SDK has provided utilities to create the SageMaker models for several frameworks. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/index.html\" rel=\"nofollow noreferrer\">Frameworks<\/a> for available frameworks. If it is not there, you may still be able to use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.FrameworkModel\" rel=\"nofollow noreferrer\">sagemaker.model.FrameworkModel<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"nofollow noreferrer\">Model<\/a> to load your trained model. For your case, see <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html\" rel=\"nofollow noreferrer\">Using Scikit-learn with the SageMaker Python SDK<\/a>.<\/p>\n<h3>model.tar.gz<\/h3>\n<p>For instance if you used PyTorch and save the model as model.pth. To load the model and the inference code to get the prediction from the model, you need to create a model.tar.gz file. The structure inside the model.tar.gz is explained in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#model-directory-structure\" rel=\"nofollow noreferrer\">Model Directory Structure<\/a>. If you use Windows, beware of the CRLF to LF. AWS SageMaker runs in *NIX environment. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-the-directory-structure-for-your-model-files\" rel=\"nofollow noreferrer\">Create the directory structure for your model files<\/a>.<\/p>\n<pre><code>|- model.pth        # model file is inside \/ directory.\n|- code\/            # Code artefacts must be inside \/code\n  |- inference.py   # Your inference code for the framework\n  |- requirements.txt  # only for versions 1.3.1 and higher. Name must be &quot;requirements.txt&quot;\n<\/code><\/pre>\n<p>Save the tar.gz file in S3. Make sure of the IAM role to access the S3 bucket and objects.<\/p>\n<h3>Loading model and get inference<\/h3>\n<p>See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-a-pytorchmodel-object\" rel=\"nofollow noreferrer\">Create a PyTorchModel object<\/a>. When instantiating the PyTorchModel class, SageMaker automatically selects the AWS Deep Learning Container image for PyTorch for the version specified in <strong>framework_version<\/strong>. If the image for the version does not exist, then it fails. This has not been documented in AWS but need to be aware of. SageMaker then internally calls the CreateModel API with the S3 model file location and the AWS Deep Learning Container image URL.<\/p>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.pytorch import PyTorchModel\nfrom sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n\nrole = sagemaker.get_execution_role()  # IAM role to run SageMaker, access S3 and ECR\nmodel_file = &quot;s3:\/\/YOUR_BUCKET\/YOUR_FOLDER\/model.tar.gz&quot;   # Must be &quot;.tar.gz&quot; suffix\n\n\nclass AnalysisClass(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super().__init__(\n            endpoint_name,\n            sagemaker_session=sagemaker_session,\n            serializer=json_serializer,\n            deserializer=json_deserializer,   # To be able to use JSON serialization\n            content_type='application\/json'   # To be able to send JSON as HTTP body\n        )\n\nmodel = PyTorchModel(\n    model_data=model_file,\n    name='YOUR_MODEL_NAME_WHATEVER',\n    role=role,\n    entry_point='inference.py',\n    source_dir='code',              # Location of the inference code\n    framework_version='1.5.0',      # Availble AWS Deep Learning PyTorch container version must be specified\n    predictor_cls=AnalysisClass     # To specify the HTTP request body format (application\/json)\n)\n\npredictor = model.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.xlarge'\n)\n\ntest_data = {&quot;body&quot;: &quot;YOUR PREDICTION REQUEST&quot;}\nprediction = predictor.predict(test_data)\n<\/code><\/pre>\n<p>By default, SageMaker uses NumPy as the serialization format. To be able to use JSON, need to specify the serializer and content_type. Instead of using RealTimePredictor class, you can specify them to predictor.<\/p>\n<pre><code>predictor.serializer=json_serializer\npredictor.predict(test_data)\n<\/code><\/pre>\n<p>Or<\/p>\n<pre><code>predictor.serializer=None # As the serializer is None, predictor won't serialize the data\nserialized_test_data=json.dumps(test_data) \npredictor.predict(serialized_test_data)\n<\/code><\/pre>\n<h3>Inference code sample<\/h3>\n<p>See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#process-model-input\" rel=\"nofollow noreferrer\">Process Model Input<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#get-predictions-from-a-pytorch-model\" rel=\"nofollow noreferrer\">Get Predictions from a PyTorch Model<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#process-model-output\" rel=\"nofollow noreferrer\">Process Model Output<\/a>. The prediction request is sent as JSON in HTTP request body in this example.<\/p>\n<pre><code>import os\nimport sys\nimport datetime\nimport json\nimport torch\nimport numpy as np\n\nCONTENT_TYPE_JSON = 'application\/json'\n\ndef model_fn(model_dir):\n    # SageMaker automatically load the model.tar.gz from the S3 and \n    # mount the folders inside the docker container. The  'model_dir'\n    # points to the root of the extracted tar.gz file.\n\n    model_path = f'{model_dir}\/'\n    \n    # Load the model\n    # You can load whatever from the Internet, S3, wherever &lt;--- Answer to your Question\n    # NO Need to use the model in tar.gz. You can place a dummy model file.\n    ...\n\n    return model\n\n\ndef predict_fn(input_data, model):\n    # Do your inference\n    ...\n\ndef input_fn(serialized_input_data, content_type=CONTENT_TYPE_JSON):\n    input_data = json.loads(serialized_input_data)\n    return input_data\n\n\ndef output_fn(prediction_output, accept=CONTENT_TYPE_JSON):\n    if accept == CONTENT_TYPE_JSON:\n        return json.dumps(prediction_output), accept\n    raise Exception('Unsupported content type') \n<\/code><\/pre>\n<h2>Related<\/h2>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker\" rel=\"nofollow noreferrer\">Using Models Trained Outside of Amazon SageMaker\n<\/a><\/li>\n<\/ul>\n<h2>Note<\/h2>\n<p>SageMaker team keeps changing the implementations and the documentations are frequently obsolete. When you are sure you did follow the documents and it does not work, obsolete documentation is quite likely. In such case, need to clarify with AWS support, or open an issue in the Github.<\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_time":1607293074049,
        "Answer_score":7.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":1630389412780,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65168915",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52437543,
        "Question_title":"Does sagemaker use nvidia-docker or docker runtime==nvidia by default or user need to manually set up?",
        "Question_body":"<p>As stated in the question, \"Does sagemaker use nvidia-docker or docker runtime==nvidia by default or user need to manually set up?\"<\/p>\n\n<p>Some common error message showed as \"CannotStartContainerError. Please ensure the model container for variant variant-name-1 starts correctly when invoked with 'docker run  serve\u2019.\" and it didn't show as running with nividia driver.<\/p>\n\n<p>So, do we need manually set up?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1537509940017,
        "Question_score":0,
        "Question_tags":"amazon-web-services|docker|nvidia-docker|amazon-sagemaker",
        "Question_view_count":940,
        "Owner_creation_time":1537311568807,
        "Owner_last_access_time":1612898933127,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52437543",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72099790,
        "Question_title":"Error when deploying pre trained Tensorflow models to one endpoint (multimodel for one endpoint) in sagemaker?",
        "Question_body":"<p>I am following this example from aws <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-multi-model-endpoint-tensorflow-computer-vision\/blob\/main\/multi-model-endpoint-tensorflow-cv.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-multi-model-endpoint-tensorflow-computer-vision\/blob\/main\/multi-model-endpoint-tensorflow-cv.ipynb<\/a>\nto apply same workflow with two pre trained models (trained outside of sagemaker).<\/p>\n<p>But when I do the following, logs say that models can't be found:<\/p>\n<pre><code>import boto3\nimport datetime\nfrom datetime import datetime\nimport time\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.tensorflow.serving import TensorFlowModel\nfrom sagemaker.multidatamodel import MultiDataModel\n\nmodel_data_prefix = f's3:\/\/{BUCKET}\/{PREFIX}\/mme\/'\noutput = f's3:\/\/{BUCKET}\/{PREFIX}\/mme\/test.tar.gz'\n\nmodele = TensorFlowModel(model_data=output, \n                          role=role, \n                          image_uri=IMAGE_URI)\n\nmme = MultiDataModel(name=f'mme-tensorflow-{current_time}',\n                     model_data_prefix=model_data_prefix,\n                     model=modele,\n                     sagemaker_session=sagemaker_session)\n\npredictor = mme.deploy(initial_instance_count=1,\n                       instance_type='ml.m5.2xlarge',\n                       endpoint_name=f'mme-tensorflow-{current_time}')\n<\/code><\/pre>\n<p>When I give an image as input to predict, I have this message:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message &quot;&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Internal Server Error&lt;\/title&gt;\n  &lt;\/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;&lt;p&gt;Internal Server Error&lt;\/p&gt;&lt;\/h1&gt;\n    \n  &lt;\/body&gt;\n&lt;\/html&gt;\n&quot;.\n<\/code><\/pre>\n<p>Logs give:<\/p>\n<pre><code>Could not find base path \/opt\/ml\/models\/...\/model for servable ...\n<\/code><\/pre>\n<p>What did I missed ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651582909370,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":72,
        "Owner_creation_time":1606642099553,
        "Owner_last_access_time":1654258609767,
        "Owner_location":null,
        "Owner_reputation":371,
        "Owner_up_votes":65,
        "Owner_down_votes":0,
        "Owner_views":55,
        "Question_last_edit_time":1651583638129,
        "Answer_body":"<p>In the sample notebook, the model is trained within SageMaker. So it is created with certain environment variables like the &quot;SAGEMAKER_PROGRAM&quot;(I think, need to check the documentation) with value set to entry point script.<\/p>\n<p>But while you are creating the model with models trained outside the SageMaker you need to add those environment variables.<\/p>\n<p>Without an entry point script SageMaker is not in a position to know what to do with the request.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1651629562456,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72099790",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66545743,
        "Question_title":"SageMaker Studio & EFS",
        "Question_body":"<p>I heard Sagemaker Studio automatically creates EFS on home directory.<\/p>\n<ol>\n<li>What is the size of EFS home directory?<\/li>\n<li>Is it possible to resize its size?<\/li>\n<\/ol>\n<p>I'm a total beginner on AWS, so I'd happy if somebody answer to my question.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615288045900,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker|amazon-efs",
        "Question_view_count":435,
        "Owner_creation_time":1574863313540,
        "Owner_last_access_time":1663835483540,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1631631113103,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66545743",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57713515,
        "Question_title":"Outputting multiple csv files from a SageMaker batch prediction job",
        "Question_body":"<p>I am working on a AWS SageMaker (SKlearn) batch transform job, in which the prediction data is big and therefore I'm required to use mini-batches (where the input .csv is split up into smaller .csv files).<\/p>\n\n<p>I have this working and outputting a .csv file with the ids and the predictions. However I am attempting to implement a way in which I can have a total of three output files from the batch transform job - which are different .csv files each aggregated in a slightly different way.<\/p>\n\n<p>My issue is I am not sure how to instruct SageMaker to output multiple files. I have tried the following code as the prediction method submitted in the <code>entry_point<\/code> file:<\/p>\n\n<pre><code>def output_fn(prediction, accept):\n    output_one = prepare_one(prediction)\n    output_two, output_three = prepare_others(output_one)\n    return output_one, output_two, output_three\n<\/code><\/pre>\n\n<p>Couple ideas\/issues I am currently working with:<\/p>\n\n<ul>\n<li>I think the batching strategy will cause issue. As the additional outputs are aggregations on the total predictions but SageMaker will treat each mini-batch separately (I assume?) <\/li>\n<li>Can I simply use <code>boto3<\/code> and save extra files using that and treat the SageMaker output as only <code>output_one<\/code><\/li>\n<\/ul>\n\n<p>Any help would be much appreciated<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1567092855690,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|csv|amazon-sagemaker",
        "Question_view_count":1650,
        "Owner_creation_time":1442932369407,
        "Owner_last_access_time":1662303207090,
        "Owner_location":"Belfast",
        "Owner_reputation":1682,
        "Owner_up_votes":165,
        "Owner_down_votes":4,
        "Owner_views":164,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57713515",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73531632,
        "Question_title":"Error when using custom precision metric in Keras",
        "Question_body":"<pre><code>from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\n\nmodel = Sequential()\nmodel.add(Dense(90, input_dim=900, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(90, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nm = keras.metrics.Precision(class_id=1)\n# example data of suitable dimension, to offer MRE to SO\nX_train = np.eye(900)\nY_train = np.ones((900, 1))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=[m])\nmodel.fit(X_train, Y_train, epochs=50, batch_size=1000)\n<\/code><\/pre>\n<p>When I use\n<code>model.compile(loss='binary_crossentropy, optimizer='adam', metrics=['Precision'])<\/code><\/p>\n<p>it works, but when I use Precision(class_id=1) (regardless of whether I substitute it as a variable), I get<\/p>\n<pre><code>ValueError: slice index 1 of dimension 1 out of bounds. \nfor '{{node strided_slice_1}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=1, end_mask=0, \nnew_axis_mask=0, shrink_axis_mask=2](Cast_1, strided_slice_1\/stack, strided_slice_1\/stack_1, strided_slice_1\/stack_2)' \nwith input shapes: [?,1], [2], [2], [2] and with computed \ninput tensors: input[1] = &lt;0 1&gt;, input[2] = &lt;0 2&gt;, input[3] = &lt;1 1&gt;.\n<\/code><\/pre>\n<p>I don't know what any of this stuff means. Slice of WHAT is out of bounds? (I defined X_train and Y_train, of course, and they work when I just write metric=['Precision']).\nFYI I'm doing this in SageMaker, if it makes any difference. There is no other code, so if I am failing to define some config thing, I don't know about that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_time":1661789441867,
        "Question_score":0,
        "Question_tags":"python|tensorflow|keras|amazon-sagemaker",
        "Question_view_count":44,
        "Owner_creation_time":1384307626763,
        "Owner_last_access_time":1663956887833,
        "Owner_location":"Bay Area, CA, USA",
        "Owner_reputation":109,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Question_last_edit_time":1661792615750,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73531632",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70873792,
        "Question_title":"How to handle Sagemaker Batch Transform discarding a file with a failed model request",
        "Question_body":"<p>I have a large number of JSON requests for a model split across multiple files in an S3 bucket. I would like to use Sagemaker's Batch Transform feature to process all of these requests (I have done a couple of test runs using small amounts of data and the transform job succeeds). My main issue is here (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors<\/a>), specifically:<\/p>\n<blockquote>\n<p>If a batch transform job fails to process an input file because of a problem with the dataset, SageMaker marks the job as failed. If an input file contains a bad record, the transform job doesn't create an output file for that input file because doing so prevents it from maintaining the same order in the transformed data as in the input file. When your dataset has multiple input files, a transform job continues to process input files even if it fails to process one. The processed files still generate useable results.<\/p>\n<\/blockquote>\n<p>This is not preferable mainly because if 1 request fails (whether its a transient error, a malformmated request, or something wrong with the model container) in a file with a large number of requests, all of those requests will get discarded (even if all of them succeeded and the last one failed). I would ideally prefer Sagemaker to just write the output of the failed response to the file and keep going, rather than discarding the entire file.<\/p>\n<p>My question is, are there any suggestions to mitigating this issue? I was thinking about storing 1 request per file in S3, but this seems somewhat ridiculous? Even if I did this, is there a good way of seeing which requests specifically failed after the transform job finishes?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643261540133,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":357,
        "Owner_creation_time":1597858315077,
        "Owner_last_access_time":1663976401100,
        "Owner_location":null,
        "Owner_reputation":84,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You've got the right idea: the fewer datapoints are in each file, the less likely a given file is to fail. The issue is that while you can pass a prefix with many files to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB\" rel=\"nofollow noreferrer\">CreateTransformJob<\/a>, partitioning one datapoint per file at least requires an S3 read per datapoint, plus a model invocation per datapoint, which is probably not great. Be aware also that <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=1000415&amp;tstart=0\" rel=\"nofollow noreferrer\">apparently there are hidden rate limits<\/a>.<\/p>\n<p>Here are a couple options:<\/p>\n<ol>\n<li><p>Partition into small-ish files, and plan on failures being rare. Hopefully, not many of your datapoints would actually fail. If you partition your dataset into e.g. 100 files, then a single failure only requires reprocessing 1% of your data. Note that Sagemaker has built-in retries, too, so most of the time failures should be caused by your data\/logic, not randomness on Sagemaker's side.<\/p>\n<\/li>\n<li><p>Deal with failures directly in your model. The same doc you quoted in your question also says:<\/p>\n<\/li>\n<\/ol>\n<blockquote>\n<p>If you are using your own algorithms, you can use placeholder text, such as ERROR, when the algorithm finds a bad record in an input file. For example, if the last record in a dataset is bad, the algorithm places the placeholder text for that record in the output file.<\/p>\n<\/blockquote>\n<p>Note that the reason Batch Transform does this whole-file failure is to maintain a 1-1 mapping between rows in the input and the output. If you can substitute the output for failed datapoints with an error message from inside your model, without actually causing the model itself to fail processing, Batch Transform will be happy.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1644424499009,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1644528409280,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70873792",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72760982,
        "Question_title":"How to use Tensorboard within a notebook running on Amazon SageMaker Studio Lab?",
        "Question_body":"<p>I have a Jupyter notebook running within an <code>Amazon SageMaker Studio Lab<\/code> (<a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a>) environment, and I want to use Tensordboard to monitor my model's performance inside the notebook.<\/p>\n<p>I have used the following commands to set up the Tensorboard:<\/p>\n<pre><code>%load_ext tensorboard\n# tb_log_dir variable holds the path to the log directory\n%tensorboard --logdir tb_log_dir\n<\/code><\/pre>\n<p>But nothing shows up in the output of the cell where I execute the commands. See:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TwbTk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TwbTk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The two buttons shown in the picture are not responding, BTW.<\/p>\n<p>How to solve this problem? Any suggestions would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656241190023,
        "Question_score":1,
        "Question_tags":"jupyter-notebook|amazon-sagemaker|tensorboard",
        "Question_view_count":121,
        "Owner_creation_time":1420286650807,
        "Owner_last_access_time":1663841459487,
        "Owner_location":"Trondheim, Norway",
        "Owner_reputation":720,
        "Owner_up_votes":129,
        "Owner_down_votes":0,
        "Owner_views":126,
        "Question_last_edit_time":1656243378470,
        "Answer_body":"<p>I would try the canonical way to use tensorboard in AWS Sagemaker, it should be supported also by Studio Lab, it is described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tensorboard.html\" rel=\"nofollow noreferrer\">here<\/a>. Basically install tensorboard and using the <code>EFS_PATH_LOG_DIR<\/code> launch tensorboard using the embedded console (you can do the following also from a cell):<\/p>\n<pre><code>pip install tensorboard\ntensorboard --logdir &lt;EFS_PATH_LOG_DIR&gt;\n<\/code><\/pre>\n<p>Be careful with the EFS_PATH_LOG_DIR, be sure this folder is valida path from the location you are, for example by default you are located in <code>studio-lab-user\/sagemaker-studiolab-notebooks\/<\/code> so the proper command would be <code>!tensorboard --logdir logs\/fit<\/code>.<\/p>\n<p>Then open a browser to:<\/p>\n<pre><code>https:\/\/&lt;YOUR URL&gt;\/studiolab\/default\/jupyter\/proxy\/6006\/\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1656410088380,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1657181121616,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72760982",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65721061,
        "Question_title":"How to parse stepfunction executionId to SageMaker batch transform job name?",
        "Question_body":"<p>I have created a stepfunction, the definition for this statemachine below (<code>step-function.json<\/code>) is used in terraform (using the syntax in this page:<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html<\/a>)<\/p>\n<p>The first time if I execute this statemachine, it will create a SageMaker batch transform job named <code>example-jobname<\/code>, but I need to exeucute this statemachine everyday, then it will give me error <code>&quot;error&quot;: &quot;SageMaker.ResourceInUseException&quot;, &quot;cause&quot;: &quot;Job name must be unique within an AWS account and region, and a job with this name already exists <\/code>.<\/p>\n<p>The cause is because the job name is hard-coded as <code>example-jobname<\/code> so if the state machine gets executed after the first time, since the job name needs to be unique, the task will fail, just wondering how I can add a string (something like ExecutionId at the end of the job name). Here's what I have tried:<\/p>\n<ol>\n<li><p>I added <code>&quot;executionId.$&quot;: &quot;States.Format('somestring {}', $$.Execution.Id)&quot;<\/code> in the <code>Parameters<\/code> section in the json file, but when I execute the task I got error <code> &quot;error&quot;: &quot;States.Runtime&quot;, &quot;cause&quot;: &quot;An error occurred while executing the state 'SageMaker CreateTransformJob' (entered at the event id #2). The Parameters '{\\&quot;BatchStrategy\\&quot;:\\&quot;SingleRecord\\&quot;,..............\\&quot;executionId\\&quot;:\\&quot;somestring arn:aws:states:us-east-1:xxxxx:execution:xxxxx-state-machine:xxxxxxxx72950\\&quot;}' could not be used to start the Task: [The field \\&quot;executionId\\&quot; is not supported by Step Functions]&quot;}<\/code><\/p>\n<\/li>\n<li><p>I modified the jobname in the json file to  <code>&quot;TransformJobName&quot;: &quot;example-jobname-States.Format('somestring {}', $$.Execution.Id)&quot;,<\/code>, when I execute the statemachine, it gave me error: <code>&quot;error&quot;: &quot;SageMaker.AmazonSageMakerException&quot;, &quot;cause&quot;: &quot;2 validation errors detected: Value 'example-jobname-States.Format('somestring {}', $$.Execution.Id)' at 'transformJobName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}; Value 'example-jobname-States.Format('somestring {}', $$.Execution.Id)' at 'transformJobName' failed to satisfy constraint: Member must have length less than or equal to 63<\/code><\/p>\n<\/li>\n<\/ol>\n<p>I really run out of ideas, can someone help please? Many thanks.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1610635167150,
        "Question_score":2,
        "Question_tags":"amazon-web-services|terraform|state-machine|amazon-sagemaker|aws-step-functions",
        "Question_view_count":1209,
        "Owner_creation_time":1540920956270,
        "Owner_last_access_time":1663875036883,
        "Owner_location":"United Kingdom",
        "Owner_reputation":2385,
        "Owner_up_votes":1007,
        "Owner_down_votes":16,
        "Owner_views":585,
        "Question_last_edit_time":1611071837132,
        "Answer_body":"<p>So as per the <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/sample-train-model.html#sample-train-model-code-examples\" rel=\"nofollow noreferrer\">documentation<\/a>, we should be passing the parameters in the following format<\/p>\n<pre><code>        &quot;Parameters&quot;: {\n            &quot;ModelName.$&quot;: &quot;$$.Execution.Name&quot;,  \n            ....\n        },\n<\/code><\/pre>\n<p>If you take a close look this is something missing from your definition, So your step function definition should be something like below:<\/p>\n<p>either<\/p>\n<pre><code>      &quot;TransformJobName.$&quot;: &quot;$$.Execution.Id&quot;,\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code>      &quot;TransformJobName.$: &quot;States.Format('mytransformjob{}', $$.Execution.Id)&quot;\n<\/code><\/pre>\n<p>full State machine definition:<\/p>\n<pre><code>    {\n        &quot;Comment&quot;: &quot;Defines the statemachine.&quot;,\n        &quot;StartAt&quot;: &quot;Generate Random String&quot;,\n        &quot;States&quot;: {\n            &quot;Generate Random String&quot;: {\n                &quot;Type&quot;: &quot;Task&quot;,\n                &quot;Resource&quot;: &quot;arn:aws:lambda:eu-central-1:1234567890:function:randomstring&quot;,\n                &quot;ResultPath&quot;: &quot;$.executionid&quot;,\n                &quot;Parameters&quot;: {\n                &quot;executionId.$&quot;: &quot;$$.Execution.Id&quot;\n                },\n                &quot;Next&quot;: &quot;SageMaker CreateTransformJob&quot;\n            },\n        &quot;SageMaker CreateTransformJob&quot;: {\n            &quot;Type&quot;: &quot;Task&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:states:::sagemaker:createTransformJob.sync&quot;,\n            &quot;Parameters&quot;: {\n            &quot;BatchStrategy&quot;: &quot;SingleRecord&quot;,\n            &quot;DataProcessing&quot;: {\n                &quot;InputFilter&quot;: &quot;$&quot;,\n                &quot;JoinSource&quot;: &quot;Input&quot;,\n                &quot;OutputFilter&quot;: &quot;xxx&quot;\n            },\n            &quot;Environment&quot;: {\n                &quot;SAGEMAKER_MODEL_SERVER_TIMEOUT&quot;: &quot;300&quot;\n            },\n            &quot;MaxConcurrentTransforms&quot;: 100,\n            &quot;MaxPayloadInMB&quot;: 1,\n            &quot;ModelName&quot;: &quot;${model_name}&quot;,\n            &quot;TransformInput&quot;: {\n                &quot;DataSource&quot;: {\n                    &quot;S3DataSource&quot;: {\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3Uri&quot;: &quot;${s3_input_path}&quot;\n                    }\n                },\n                &quot;ContentType&quot;: &quot;application\/jsonlines&quot;,\n                &quot;CompressionType&quot;: &quot;Gzip&quot;,\n                &quot;SplitType&quot;: &quot;Line&quot;\n            },\n            &quot;TransformJobName.$&quot;: &quot;$.executionid&quot;,\n            &quot;TransformOutput&quot;: {\n                &quot;S3OutputPath&quot;: &quot;${s3_output_path}&quot;,\n                &quot;Accept&quot;: &quot;application\/jsonlines&quot;,\n                &quot;AssembleWith&quot;: &quot;Line&quot;\n            },    \n            &quot;TransformResources&quot;: {\n                &quot;InstanceType&quot;: &quot;xxx&quot;,\n                &quot;InstanceCount&quot;: 1\n            }\n        },\n            &quot;End&quot;: true\n        }\n        }\n    }\n<\/code><\/pre>\n<p>In the above definition the lambda could be a function which parses the execution id arn which I am passing via the parameters section:<\/p>\n<pre><code> def lambda_handler(event, context):\n    return(event.get('executionId').split(':')[-1])\n<\/code><\/pre>\n<p>Or if you dont wanna pass the execution id , it can simply return the random string like<\/p>\n<pre><code> import string\n def lambda_handler(event, context):\n    return(string.ascii_uppercase + string.digits)\n<\/code><\/pre>\n<p>you can generate all kinds of random string or do generate anything in the lambda and pass that to the transform job name.<\/p>",
        "Answer_comment_count":23.0,
        "Answer_creation_time":1610637215396,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1610641519769,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65721061",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68044070,
        "Question_title":"How to customise AWS Textract?",
        "Question_body":"<p>So far my Textract tests are very impressive for handwriting, but I see sometimes it fails to recognise some forms and some values. Is it possible to train it? If I'm scanning the same type of form\/document it will be very useful to amend the results and teaching it where the boundaries of some form elements lie and some key-value associations as well?<\/p>\n<p>It will be a real deal breaker for the kind of service I'm trying to design.<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1624082227563,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|text-extraction|text-recognition|amazon-textract",
        "Question_view_count":808,
        "Owner_creation_time":1575488323143,
        "Owner_last_access_time":1661765619090,
        "Owner_location":"Hyderabad, Telangana, India",
        "Owner_reputation":23,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1624091060687,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68044070",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63536595,
        "Question_title":"Connect Sagemaker to RDS db",
        "Question_body":"<p>I am new to AWS. All I know is that the Postgre database is hosted in AWS RDS. I want to build an ML model using AWS Sagemaker. I am not sure how to get the data from AWS RDS so that I can use it for building the ML model.\nI will be thankful for any help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1598102199987,
        "Question_score":2,
        "Question_tags":"amazon-rds|amazon-sagemaker",
        "Question_view_count":2550,
        "Owner_creation_time":1598101911760,
        "Owner_last_access_time":1662277289103,
        "Owner_location":null,
        "Owner_reputation":153,
        "Owner_up_votes":33,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":1627652354892,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63536595",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69782294,
        "Question_title":"AWS Sagemaker output how to read file with multiple json objects spread out over multiple lines",
        "Question_body":"<p>I have a bunch of json files that look like this<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.01027186680585146, 0.04181386157870293, -0.07363887131214142, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.011699287220835686, 0.04741542786359787, -0.07899319380521774, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n<\/code><\/pre>\n<p>Which I can read in with<\/p>\n<pre><code>f = open(file_name)\ndata = []\nfor line in f:\n   data.append(json.dumps(line))\n<\/code><\/pre>\n<p>But I have another file with output like this<\/p>\n<pre><code>{\n    &quot;predictions&quot;: [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]\n    ]\n}\n<\/code><\/pre>\n<p>I.e. the json is formatted over several lines, so I can't simply read the json in line for line. Is there an easy way to parse this? Or do I have to write something that stitches together each json object line by line and the does json.loads?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635629350997,
        "Question_score":1,
        "Question_tags":"python|json|amazon-sagemaker",
        "Question_view_count":248,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":1635655049276,
        "Answer_body":"<p>Hmm,  as far as I know there's unfortunately no way to load a <a href=\"https:\/\/jsonlines.org\/\" rel=\"nofollow noreferrer\">JSONL<\/a> format data using <code>json.loads<\/code>. One option though, is to come up with a helper function that can convert it to a valid JSON string, as below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import json\n\nstring = &quot;&quot;&quot;\n{\n    &quot;predictions&quot;: [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]\n    ]\n}\n&quot;&quot;&quot;\n\n\ndef json_lines_to_json(s: str) -&gt; str:\n    # replace the first occurrence of '{'\n    s = s.replace('{', '[{', 1)\n\n    # replace the last occurrence of '}\n    s = s.rsplit('}', 1)[0] + '}]'\n\n    # now go in and replace all occurrences of '}' immediately followed\n    # by newline with a '},'\n    s = s.replace('}\\n', '},\\n')\n\n    return s\n\n\nprint(json.loads(json_lines_to_json(string)))\n<\/code><\/pre>\n<p>Prints:<\/p>\n<pre><code>[{'predictions': [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]]}, {'predictions': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]}, {'predictions': [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]]}]\n<\/code><\/pre>\n<p><strong>Note:<\/strong> your first example actually doesn't seem like valid JSON (or at least JSON lines from my understanding). In particular, this part appears to be invalid due to a trailing comma after the last array element:<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664, ], ...}\n<\/code><\/pre>\n<p>To ensure it's valid after calling the helper function, you'd also need to remove the trailing commas, so each line is in the below format:<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], ...},\n<\/code><\/pre>\n<hr \/>\n<p>There also appears to be a <a href=\"https:\/\/stackoverflow.com\/questions\/50475635\/loading-jsonl-file-as-json-objects\/50475669\">similar question<\/a> where they suggest splitting on newlines and calling <code>json.loads<\/code> on each line; actually it should be (slightly) less performant to call <code>json.loads<\/code> multiple times on each object, rather than once on the list, as I show below.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from timeit import timeit\nimport json\n\n\nstring = &quot;&quot;&quot;\\\n{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.01027186680585146, 0.04181386157870293, -0.07363887131214142 ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.011699287220835686, 0.04741542786359787, -0.07899319380521774 ], &quot;word&quot;: &quot;blah blah blah&quot;}\\\n&quot;&quot;&quot;\n\n\ndef json_lines_to_json(s: str) -&gt; str:\n\n    # Strip newlines from end, then replace all occurrences of '}' followed\n    # by a newline, by a '},' followed by a newline.\n    s = s.rstrip('\\n').replace('}\\n', '},\\n')\n\n    # return string value wrapped in brackets (list)\n    return f'[{s}]'\n\n\nn = 10_000\n\nprint('string replace:        ', timeit(r'json.loads(json_lines_to_json(string))', number=n, globals=globals()))\nprint('json.loads each line:  ', timeit(r'[json.loads(line) for line in string.split(&quot;\\n&quot;)]', number=n, globals=globals()))\n<\/code><\/pre>\n<p>Result:<\/p>\n<pre><code>string replace:         0.07599360000000001\njson.loads each line:   0.1078384\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1635633656723,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1635655742576,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69782294",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50032795,
        "Question_title":"prevent access to s3 buckets for sagemaker users",
        "Question_body":"<p>I'm trying to add an IAM user for using sagemaker. I used the <code>AmazonSageMakerFullAccess<\/code> policy. But when I log in as this user I can see all of the s3 buckets of the root account and download files from them.<\/p>\n<p>The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html\" rel=\"nofollow noreferrer\">sagemaker documentation<\/a> states<\/p>\n<blockquote>\n<p>When attaching the AmazonSageMakerFullAccess policy to a role, you must do one of the following to allow Amazon SageMaker to access your S3 bucket:<\/p>\n<p>Include the string &quot;SageMaker&quot; or &quot;sagemaker&quot; in the name of the bucket where you store training data, or the model artifacts resulting from model training, or both.<\/p>\n<p>Include the string &quot;SageMaker&quot; or &quot;sagemaker&quot; in the object name of the training data object(s).<\/p>\n<p>Tag the S3 object with &quot;sagemaker=true&quot;. The key and value are case sensitive. For more information, see Object Tagging in the Amazon Simple Storage Service Developer Guide.<\/p>\n<p>Add a bucket policy that allows access for the execution role. For more information, see Using Bucket Policies and User Policies in the Amazon Simple Storage Service Developer Guide.<\/p>\n<\/blockquote>\n<p>This seems to be inaccurate the user can access s3 buckets lacking <code>sagemaker<\/code> in the name. How do I limit the access?<\/p>\n<p>the full policy is below<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;sagemaker:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;ecr:GetAuthorizationToken&quot;,\n                &quot;ecr:GetDownloadUrlForLayer&quot;,\n                &quot;ecr:BatchGetImage&quot;,\n                &quot;ecr:BatchCheckLayerAvailability&quot;,\n                &quot;cloudwatch:PutMetricData&quot;,\n                &quot;cloudwatch:PutMetricAlarm&quot;,\n                &quot;cloudwatch:DescribeAlarms&quot;,\n                &quot;cloudwatch:DeleteAlarms&quot;,\n                &quot;ec2:CreateNetworkInterface&quot;,\n                &quot;ec2:CreateNetworkInterfacePermission&quot;,\n                &quot;ec2:DeleteNetworkInterface&quot;,\n                &quot;ec2:DeleteNetworkInterfacePermission&quot;,\n                &quot;ec2:DescribeNetworkInterfaces&quot;,\n                &quot;ec2:DescribeVpcs&quot;,\n                &quot;ec2:DescribeDhcpOptions&quot;,\n                &quot;ec2:DescribeSubnets&quot;,\n                &quot;ec2:DescribeSecurityGroups&quot;,\n                &quot;application-autoscaling:DeleteScalingPolicy&quot;,\n                &quot;application-autoscaling:DeleteScheduledAction&quot;,\n                &quot;application-autoscaling:DeregisterScalableTarget&quot;,\n                &quot;application-autoscaling:DescribeScalableTargets&quot;,\n                &quot;application-autoscaling:DescribeScalingActivities&quot;,\n                &quot;application-autoscaling:DescribeScalingPolicies&quot;,\n                &quot;application-autoscaling:DescribeScheduledActions&quot;,\n                &quot;application-autoscaling:PutScalingPolicy&quot;,\n                &quot;application-autoscaling:PutScheduledAction&quot;,\n                &quot;application-autoscaling:RegisterScalableTarget&quot;,\n                &quot;logs:CreateLogGroup&quot;,\n                &quot;logs:CreateLogStream&quot;,\n                &quot;logs:DescribeLogStreams&quot;,\n                &quot;logs:GetLogEvents&quot;,\n                &quot;logs:PutLogEvents&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:GetObject&quot;,\n                &quot;s3:PutObject&quot;,\n                &quot;s3:DeleteObject&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::*SageMaker*&quot;,\n                &quot;arn:aws:s3:::*Sagemaker*&quot;,\n                &quot;arn:aws:s3:::*sagemaker*&quot;\n            ]\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:CreateBucket&quot;,\n                &quot;s3:GetBucketLocation&quot;,\n                &quot;s3:ListBucket&quot;,\n                &quot;s3:ListAllMyBuckets&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:GetObject&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringEqualsIgnoreCase&quot;: {\n                    &quot;s3:ExistingObjectTag\/SageMaker&quot;: &quot;true&quot;\n                }\n            }\n        },\n        {\n            &quot;Action&quot;: &quot;iam:CreateServiceLinkedRole&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:iam::*:role\/aws-service-role\/sagemaker.application-autoscaling.amazonaws.com\/AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringLike&quot;: {\n                    &quot;iam:AWSServiceName&quot;: &quot;sagemaker.application-autoscaling.amazonaws.com&quot;\n                }\n            }\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;iam:PassRole&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringEquals&quot;: {\n                    &quot;iam:PassedToService&quot;: &quot;sagemaker.amazonaws.com&quot;\n                }\n            }\n        }\n    ]\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1524699817877,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-iam|amazon-sagemaker",
        "Question_view_count":1703,
        "Owner_creation_time":1298484007147,
        "Owner_last_access_time":1664045053900,
        "Owner_location":"New York, NY, United States",
        "Owner_reputation":9271,
        "Owner_up_votes":2074,
        "Owner_down_votes":44,
        "Owner_views":1819,
        "Question_last_edit_time":1592644375060,
        "Answer_body":"<p>looks like the sagemaker notebook wizard has you create a role that has limited s3 access. If I add this and the default <code>AmazonSageMakerFullAccess<\/code> the user is properly restricted. <a href=\"https:\/\/i.stack.imgur.com\/9BjRD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9BjRD.png\" alt=\"Amazon make sagemaker role\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/IM7WW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IM7WW.png\" alt=\"choose iam roles\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1525129267447,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50032795",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73830782,
        "Question_title":"How to convert bert model output to json?",
        "Question_body":"<p>I have fine-tuned a Bert model and testing my output from different layers. I tested this in sagemaker , with my own custom script (see below) and the output i get is of BaseModelOutputWithPoolingAndCrossAttentions class. How can i convert the output of this , specially the tensor values from the last_hidden_state to json?<\/p>\n<p>inference.py<\/p>\n<pre><code>\nfrom transformers import BertModel, BertConfig\n\ndef model_fn():\n\n   config = BertConfig.from_pretrained(&quot;xxx&quot;, output_hidden_states=True)\n   model = BertModel.from_pretrained(&quot;xxx&quot;, config=config)\n\n....\ndef predict_fn():\n    ....\n\n    return model(inputs)\n\n<\/code><\/pre>\n<p>model output<\/p>\n<pre><code>BaseModelOutputWithPoolingAndCrossAttentions( \nlast_hidden_state=\ntensor([[[-1.6968,  1.9364, -2.1796, -0.0819,  1.8027,  0.3540,  1.3269,  0.1532],\n        [-0.4969,  0.4169,  0.5677,  1.0968,  0.0742,  1.5354,  0.9387,  0.0343]]])\ndevice='cuda:0', grad_fn=&lt;NativeLayerNormBackward&gt;), \nhidden_states=None,\nattentions=None, \n...\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663951602360,
        "Question_score":0,
        "Question_tags":"python|json|tensorflow|amazon-sagemaker|bert-language-model",
        "Question_view_count":21,
        "Owner_creation_time":1590797441983,
        "Owner_last_access_time":1664049080543,
        "Owner_location":null,
        "Owner_reputation":525,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":98,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73830782",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61152276,
        "Question_title":"ImportError: No module named cv2 when run Batch transform jobs in SageMaker",
        "Question_body":"<p>When I tried to run a Batch transform job in AWS SageMaker, I met below error: <\/p>\n\n<p>ImportError: No module named cv2<\/p>\n\n<p>Please note that, I am able to \"import CV2\" in the notebook instance. The jupter can run \"import CV2\" in notebook instance. But failed to run it in endpoints during inference time. I have tried below method using \"env\" as the link <a href=\"https:\/\/stackoverflow.com\/questions\/51117133\/aws-sagemaker-install-external-library-and-make-it-persist\">AWS Sagemaker - Install External Library and Make it Persist<\/a> <\/p>\n\n<p>but it still not work. <\/p>\n\n<p>anyone have good way to solve it? Thanks! <\/p>\n\n<p>my codes are: <\/p>\n\n<pre><code>env = {\n'SAGEMAKER_REQUIREMENTS': 'requirements.txt', # path relative to `source_dir` below.\n}\nimage_embed_model = MXNetModel(model_data=model_data,\n                         entry_point='sagemaker_entrypoint.py',\n                        role=role,\n                        source_dir = 'src',\n                        env = env,\n                        py_version='py3',\n                        framework_version='1.6.0')\n\ntransformer = image_embed_model.transformer(instance_count=1, # Please pay attention here!!!\n                                    instance_type='ml.m4.xlarge',\n                                    output_path=output_path,\n                                    assemble_with = 'Line', \n                                    accept = 'text\/csv'\n                                   )\ntransformer.transform(batch_input,\n                  content_type='text\/csv', \n                  split_type='Line',\n                  input_filter='$[0:]',\n                  join_source='Input',\n                  wait=False)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586580706220,
        "Question_score":1,
        "Question_tags":"python-import|batch-processing|endpoint|cv2|amazon-sagemaker",
        "Question_view_count":363,
        "Owner_creation_time":1525455745547,
        "Owner_last_access_time":1593301633933,
        "Owner_location":"Seattle, WA, USA",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61152276",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57151638,
        "Question_title":"What is AWS SageMaker mAP referring to?",
        "Question_body":"<p>I'm running a model on AWS SageMaker, using their example object detection Jupyter notebook (<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_recordio_format.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_recordio_format.ipynb<\/a>).  In the results it gives the following:<\/p>\n\n<p>validation mAP =(0.111078678154)<\/p>\n\n<p>I was wondering what this mAP score is referring to? <\/p>\n\n<p>I've used tensorflow, where it gives an averaged mAP(averages from .5IoU to .95IoU with .05 increments), mAP@.5IoU, mAP@.75IoU.  I've checked the documents on SageMaker, but cannot find anything referring to what the definition of mAP is.<\/p>\n\n<p>Is it safe to assume that the mAP score SageMaker reports is the \"averaged mAP(averages from .5IoU to .95IoU with .05 increments)\"?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1563819303397,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":290,
        "Owner_creation_time":1563817494337,
        "Owner_last_access_time":1564527049533,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57151638",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72272952,
        "Question_title":"Loading Larga data to amazon sagemaker notebook",
        "Question_body":"<p>I have 2 folder, on each folder I have 70 csv files each one with a size of 3mb to 5mb, so in general the data is like 20 millions rows with 5 columns each.<\/p>\n<p>I used amazon wrangler s3.read_csv to load just one folder with all the 70 csv to a dataframe, not sure if this is a good approach due to the fact the data is really large.<\/p>\n<p>I want to know how can I load the entire csv files from those 2 folders with aws wrangler s3.readcsv, or should I use pyspark?<\/p>\n<p>Also another question is, is it possible to work locally using amazon sagemaker depenencies? I am not sure if using sagemaker notebook for the pipeline development might cost a lot for my client.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1652785626633,
        "Question_score":0,
        "Question_tags":"pandas|amazon-web-services|pyspark|amazon-sagemaker|large-data",
        "Question_view_count":163,
        "Owner_creation_time":1587087081620,
        "Owner_last_access_time":1664023997577,
        "Owner_location":"Caracas, Venezuela",
        "Owner_reputation":113,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":1652794238627,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72272952",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65110736,
        "Question_title":"SageMaker Estimator fit job never ends",
        "Question_body":"<p>I have the following code<\/p>\n<pre><code>estimator = Estimator(                                                     \n    image_uri=ecr_image,                                                   \n    role=role,                                                             \n    instance_count=1,                                                      \n    instance_type=instance_type,                                           \n    hyperparameters=hyperparameters                                        \n)                                                                          \n\nestimator.fit({&quot;training&quot;: &quot;s3:\/\/&quot; + sess.default_bucket() + &quot;\/&quot; + prefix})\n<\/code><\/pre>\n<p>which seems to run smoothly until it is stuck at:<\/p>\n<pre><code>Finished Training\n2020-12-02 15:00:45,352 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n<\/code><\/pre>\n<p>and I see InProgress job in AWS SageMaker console. How can I fix this?<\/p>\n<p>I use <code>763104351884.dkr.ecr.us-west-2.amazonaws.com\/pytorch-inference-eia:1.3.1-cpu-py36-ubuntu16.04<\/code> Docker image with <code>pip install sagemaker-training<\/code> added.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1606921724283,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":252,
        "Owner_creation_time":1382978984190,
        "Owner_last_access_time":1664012645480,
        "Owner_location":null,
        "Owner_reputation":1311,
        "Owner_up_votes":149,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Question_last_edit_time":1606922323816,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65110736",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57655516,
        "Question_title":"Set Spark version for Sagemaker on Glue Dev Endpoint",
        "Question_body":"<p>To create my Glue scripts, I use development endpoints with Sagemaker notebooks that run the Pyspark (Sparkmagic) kernel.\nThe latest version of Glue (version 1.0) supports Spark 2.4. However, my Sagemaker notebook uses Spark version 2.2.1. \nThe function I want to test only exists as of Spark 2.3. \nIs there a way to solve this mismatch between the dev endpoint and the Glue job? Can I somehow set the Spark version of the notebook?<br>\nI couldn't find anything in the documentation.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1566812956087,
        "Question_score":2,
        "Question_tags":"amazon-web-services|apache-spark|aws-glue|amazon-sagemaker",
        "Question_view_count":642,
        "Owner_creation_time":1505381376317,
        "Owner_last_access_time":1639478033857,
        "Owner_location":null,
        "Owner_reputation":68,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When you create a SageMaker notebook for the Glue dev endpoint, it launches a SageMaker notebook instance with a specific lifecycle configuration. This LC provides the configurations to create a connection between the SageMaker notebook and the development endpoint. Upon running cells from the PySpark kernel, the code is sent to the Livy server running in the development endpoint via REST APIs. <\/p>\n\n<p>Thus, the PySpark version that you see and on which the SageMaker notebook runs depends on the development endpoint and is not configurable from the SageMaker point of view.<\/p>\n\n<p>Since Glue is a managed service, root access is restricted for the development endpoint. Thus, you cannot update the spark version to a more later version. The feature of using Spark version 2.4 has been newly introduced in Glue and it seems that it has not yet been released for dev endpoint.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1566968757932,
        "Answer_score":5.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":1567004276929,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57655516",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53423061,
        "Question_title":"How do I make this IAM role error in aws sagemaker go away?",
        "Question_body":"<p>I suspect this has to more to do with IAM roles than Sagemaker.<\/p>\n\n<p>I'm following the example <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"noreferrer\">here<\/a><\/p>\n\n<p>Specifically, when it makes this call<\/p>\n\n<pre><code>tf_estimator.fit('s3:\/\/bucket\/path\/to\/training\/data')\n<\/code><\/pre>\n\n<p>I get this error<\/p>\n\n<pre><code>ClientError: An error occurred (AccessDenied) when calling the GetRole operation: User: arn:aws:sts::013772784144:assumed-role\/AmazonSageMaker-ExecutionRole-20181022T195630\/SageMaker is not authorized to perform: iam:GetRole on resource: role SageMakerRole\n<\/code><\/pre>\n\n<p>My notebook instance has an IAM role attached to it.\nThat role has the <code>AmazonSageMakerFullAccess<\/code> policy. It also has a custom policy that looks like this<\/p>\n\n<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"s3:GetObject\",\n            \"s3:PutObject\",\n            \"s3:DeleteObject\",\n            \"s3:ListBucket\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::*\"\n        ]\n    }\n]\n<\/code><\/pre>\n\n<p>}<\/p>\n\n<p>My input files and .py script is in an s3 bucket with the phrase <code>sagemaker<\/code> in it.<\/p>\n\n<p>What else am I missing?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1542853625410,
        "Question_score":6,
        "Question_tags":"amazon-web-services|amazon-iam|amazon-sagemaker",
        "Question_view_count":8160,
        "Owner_creation_time":1319234288810,
        "Owner_last_access_time":1663375872053,
        "Owner_location":null,
        "Owner_reputation":4966,
        "Owner_up_votes":744,
        "Owner_down_votes":11,
        "Owner_views":304,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If you're running the example code on a SageMaker notebook instance, you can use the execution_role which has the <code>AmazonSageMakerFullAccess<\/code> attached.<\/p>\n<pre><code>from sagemaker import get_execution_role\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n<\/code><\/pre>\n<p>And you can pass this role when initializing <code>tf_estimator<\/code>.\nYou can check out the example <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-role.html\" rel=\"nofollow noreferrer\">here<\/a> for using <code>execution_role<\/code> with S3 on notebook instance.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1543010904776,
        "Answer_score":8.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1620293748347,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53423061",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61842665,
        "Question_title":"Unable to export Core ML model in Turicreate",
        "Question_body":"<p>I used AWS Sagemaker with Jupyter notebook to train my Turicreate model. It trained successfully but I'm unable to export it to a CoreML model. It shows the below error. I've tried various kernels in the Jupyter notebook with the same result. Any ideas on how to fix this error?<\/p>\n\n<p>turicreate 5.4\nGPU: mxnet-cu100<\/p>\n\n<pre><code>KeyError  Traceback (most recent call last)\n&lt;ipython-input-6-3499bdb76e06&gt; in &lt;module&gt;()\n  1 # Export for use in Core ML\n----&gt; 2 model.export_coreml('pushupsTC.mlmodel')\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/turicreate\/toolkits\/object_detector\/object_detector.py in export_coreml(self, filename,     include_non_maximum_suppression, iou_threshold, confidence_threshold)\n1216         assert (self._model[23].name == 'pool5' and\n1217                 self._model[24].name == 'specialcrop5')\n-&gt; 1218         del net._children[24]\n1219         net._children[23] = op\n1220 \n\nKeyError: 24\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1589658289300,
        "Question_score":1,
        "Question_tags":"python-3.x|jupyter-notebook|amazon-sagemaker|coreml|turi-create",
        "Question_view_count":102,
        "Owner_creation_time":1359631245423,
        "Owner_last_access_time":1664034335447,
        "Owner_location":null,
        "Owner_reputation":363,
        "Owner_up_votes":1041,
        "Owner_down_votes":0,
        "Owner_views":51,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61842665",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73708836,
        "Question_title":"EntityTooLarge ERROR while fitting RCF data in Sagemaker",
        "Question_body":"<p>I am fitting a Random Cut Forest model on AWS SageMaker to a dataset using &quot; rcf.fit(rcf.record_set(data[['Variable 1','Variable 2']].values.reshape(-1, 1)))&quot; , but getting the below error :<\/p>\n<p>An error occurred (EntityTooLarge) when calling the PutObject operation: Your proposed upload exceeds the maximum allowed size.<\/p>\n<p>The size of the ndarray is 239393964<\/p>\n<p>Works well for a sample of the data, but not working for the entire data set (total records : 400M)<\/p>\n<p>How can I fix this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1663100723627,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker|anomaly-detection",
        "Question_view_count":13,
        "Owner_creation_time":1663100428383,
        "Owner_last_access_time":1663968770527,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73708836",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58050712,
        "Question_title":"Problem deploying the best estimator gotten with sagemaker.estimator.Estimator (w\/ sklearn custom image)",
        "Question_body":"<p>After creating SKLearn() instance and using HyperparamaterTuner with a few hyperparameter ranges, I get the best estimator. When I try to deploy() the estimator, it gives an error in the log. Exactly same error happens when I create transformer and call transform on it(). Doesn't deploy and doesn't transform. What could be the problem and at least how could I possibly narrow down the problem? <\/p>\n\n<p>I have no idea how to even begin to figure this out. Googling didn't help. Nothing comes up. <\/p>\n\n<p>Creating SKLearn instance: <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>sklearn = SKLearn(\n    entry_point=script_path,\n    train_instance_type=\"ml.c4.xlarge\",\n    role=role,\n    sagemaker_session=session,\n    hyperparameters={'model': 'rfc'})\n<\/code><\/pre>\n\n<p>Putting tuner to work: <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>tuner = HyperparameterTuner(estimator = sklearn,\n                            objective_metric_name = objective_metric_name,\n                            objective_type = 'Minimize',\n                            metric_definitions = metric_definitions,\n                            hyperparameter_ranges = hyperparameters,\n                            max_jobs = 3, # 9,\n                            max_parallel_jobs = 4)\n\ntuner.fit({'train': s3_input_train})\ntuner.wait()\nbest_training_job = tuner.best_training_job()\nthe_best_estimator = sagemaker.estimator.Estimator.attach(best_training_job)\n<\/code><\/pre>\n\n<p>This gives a valid best training job. Everything seems great. <\/p>\n\n<p>Here is where the problem manifests: <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>predictor = the_best_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")\n<\/code><\/pre>\n\n<p>or the following (triggers exactly same problem): <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>rfc_transformer = the_best_estimator.transformer(1, instance_type=\"ml.m4.xlarge\")\nrfc_transformer.transform(test_location)\nrfc_transformer.wait()\n<\/code><\/pre>\n\n<p>Here is the log with the error message (it reiterates the same error many times while trying to deploy or transform; here is the beginning of the log):<\/p>\n\n<p>................[2019-09-22 09:17:48 +0000] [17] [INFO] Starting gunicorn 19.9.0<\/p>\n\n<p>[2019-09-22 09:17:48 +0000] [17] [INFO] Listening at: unix:\/tmp\/gunicorn.sock (17)<\/p>\n\n<p>[2019-09-22 09:17:48 +0000] [17] [INFO] Using worker: gevent<\/p>\n\n<p>[2019-09-22 09:17:48 +0000] [24] [INFO] Booting worker with pid: 24<\/p>\n\n<p>[2019-09-22 09:17:48 +0000] [25] [INFO] Booting worker with pid: 25<\/p>\n\n<p>[2019-09-22 09:17:48 +0000] [26] [INFO] Booting worker with pid: 26<\/p>\n\n<p>[2019-09-22 09:17:48 +0000] [30] [INFO] Booting worker with pid: 30<\/p>\n\n<p>2019-09-22 09:18:15,061 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)<\/p>\n\n<p>2019-09-22 09:18:15,062 INFO - sagemaker_sklearn_container.serving - Encountered an unexpected error.<\/p>\n\n<p>[2019-09-22 09:18:15 +0000] [24] [ERROR] Error handling request \/ping<\/p>\n\n<p>Traceback (most recent call last):<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gunicorn\/workers\/base_async.py\", line 56, in handle self.handle_request(listener_name, req, client, addr)<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gunicorn\/workers\/ggevent.py\", line 160, in handle_request addr)<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gunicorn\/workers\/base_async.py\", line 107, in handle_request respiter = self.wsgi(environ, resp.start_response)<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/sagemaker_sklearn_container\/serving.py\", line 119, in main user_module_transformer = import_module(serving_env.module_name, serving_env.module_dir)<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/sagemaker_sklearn_container\/serving.py\", line 97, in import_module user_module = importlib.import_module(module_name)<\/p>\n\n<p>File \"\/usr\/lib\/python3.5\/importlib\/<strong>init<\/strong>.py\", line 117, in import_module if name.startswith('.'):<\/p>\n\n<p>AttributeError: 'NoneType' object has no attribute 'startswith'<\/p>\n\n<p>169.254.255.130 - - [22\/Sep\/2019:09:18:15 +0000] \"GET \/ping HTTP\/1.1\" 500 141 \"-\" \"Go-http-client\/1.1\"<\/p>\n\n<p>2019-09-22 09:18:15,178 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)<\/p>\n\n<p>2019-09-22 09:18:15,179 INFO - sagemaker_sklearn_container.serving - Encountered an unexpected error.<\/p>\n\n<p>[2019-09-22 09:18:15 +0000] [30] [ERROR] Error handling request \/ping<\/p>\n\n<p>Traceback (most recent call last):<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gunicorn\/workers\/base_async.py\", line 56, in handle self.handle_request(listener_name, req, client, addr)<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gunicorn\/workers\/ggevent.py\", line 160, in handle_request addr)<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gunicorn\/workers\/base_async.py\", line 107, in handle_request respiter = self.wsgi(environ, resp.start_response)<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/sagemaker_sklearn_container\/serving.py\", line 119, in main user_module_transformer = import_module(serving_env.module_name, serving_env.module_dir)<\/p>\n\n<p>File \"\/usr\/local\/lib\/python3.5\/dist-packages\/sagemaker_sklearn_container\/serving.py\", line 97, in import_module user_module = importlib.import_module(module_name)<\/p>\n\n<p>File \"\/usr\/lib\/python3.5\/importlib\/<strong>init<\/strong>.py\", line 117, in import_module if name.startswith('.'):<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1569166367840,
        "Question_score":0,
        "Question_tags":"scikit-learn|amazon-sagemaker",
        "Question_view_count":1122,
        "Owner_creation_time":1405394458213,
        "Owner_last_access_time":1574036481927,
        "Owner_location":"Lakewood, NJ, USA",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":1569166762703,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58050712",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72203674,
        "Question_title":"Deploy AWS SageMaker pipeline using the cloud development kit (CDK)",
        "Question_body":"<p>I'm looking to deploy the SageMaker pipeline using CDK (<a href=\"https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html<\/a>) but could not find any code examples. Any pointers?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1652282728443,
        "Question_score":0,
        "Question_tags":"terraform-provider-aws|aws-cdk|amazon-sagemaker",
        "Question_view_count":237,
        "Owner_creation_time":1489685785577,
        "Owner_last_access_time":1663881721683,
        "Owner_location":"Canada",
        "Owner_reputation":65,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Question_last_edit_time":null,
        "Answer_body":"<p>CDK L1 Constructs correspond 1:1 to a CloudFormation resource of the same name. The construct props match the resouce properties.  The go-to source is therefore the CloudFormation docs.<\/p>\n<p>The <code>AWS::SageMaker::Pipeline<\/code> <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html#aws-resource-sagemaker-pipeline--examples\" rel=\"nofollow noreferrer\">docs have a more complete example<\/a>.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1652283812907,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72203674",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71120471,
        "Question_title":"How to determine size of images available in aws?",
        "Question_body":"<p>I'm using one of the images listed here <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md<\/a>, to create an sagemaker endpoint, but I keep getting &quot;failed reason: Image size 15136109518 is greater that suppported size 1073741824&quot; .<\/p>\n<p>is there a way to find out the size of images provided <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md<\/a> or any aws managed images?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644892148237,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|amazon-ecr",
        "Question_view_count":153,
        "Owner_creation_time":1590797441983,
        "Owner_last_access_time":1664049080543,
        "Owner_location":null,
        "Owner_reputation":525,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":98,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I suspect you are trying to deploy a serverless endpoint provisioned with 1GB of memory. As discussed <a href=\"https:\/\/repost.aws\/questions\/QU35dVp2D9SKKUnnVYGw9Z7A\/how-to-check-determine-image-container-size-for-aws-managed-images\" rel=\"nofollow noreferrer\">here<\/a> &quot;You can increase the memory size of your endpoint with the MemorySizeInMB parameter, more info in this documentation: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config%22\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config&quot;<\/a><\/p>\n<p>In order to view the uncompressed size of an image you can use the following example command:<\/p>\n<pre><code>$ docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-training:1.15.2-cpu-py27-ubuntu18.04\n\n$ docker inspect -f &quot;{{ .Size }}&quot; 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-training:1.15.2-cpu-py27-ubuntu18.04\n<\/code><\/pre>\n<p>Kindly also note that you will need to provision enough memory to accommodate your model as well. Please see this <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html#serverless-endpoints-how-it-works-memory\" rel=\"nofollow noreferrer\">link<\/a> for more information.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1645569051340,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71120471",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70469115,
        "Question_title":"Naming a Sagemaker Processing job using Sagemaker Pipelines ProcessingStep",
        "Question_body":"<p>I am running a Sagemaker Pipeline with the current processor:<\/p>\n<pre><code>from sagemaker.sklearn.processing import SKLearnProcessor\n\n\nframework_version = &quot;0.23-1&quot;\n\nsklearn_processor = SKLearnProcessor(\n    framework_version=framework_version,\n    instance_type=processing_instance_type,\n    instance_count=processing_instance_count,\n    base_job_name=&quot;pre-processing-job-name&quot;,\n    role=role\n)\n<\/code><\/pre>\n<p>and the processing step is:<\/p>\n<pre><code>from sagemaker.processing import ProcessingInput, ProcessingOutput\nfrom sagemaker.workflow.steps import ProcessingStep\n\n\nstep_process = ProcessingStep(\n    name=&quot;AbaloneProcess&quot;,\n    processor=sklearn_processor,\n    inputs=[\n        ProcessingInput(source=input_data, destination=&quot;\/opt\/ml\/processing\/input&quot;),\n    ],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;validation&quot;, source=&quot;\/opt\/ml\/processing\/validation&quot;),\n        ProcessingOutput(output_name=&quot;test&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    code=&quot;abalone\/preprocessing.py&quot;,\n)\n<\/code><\/pre>\n<p>It looks like the base_job_name does nothing, because the processing job that is created is <code>pipelines-o6e2jn38g05j-AbaloneProcess-nc2OlXF8jA<\/code>.<\/p>\n<p>I want the processing job name to be defined manually. Does Sagemaker pipelines support this? I seem to be going around in circles.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640313935627,
        "Question_score":2,
        "Question_tags":"amazon-web-services|machine-learning|data-science|amazon-sagemaker",
        "Question_view_count":417,
        "Owner_creation_time":1640313343617,
        "Owner_last_access_time":1643745786273,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1640332349443,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70469115",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73408571,
        "Question_title":"AWS secret manager access from Sagemaker ECR container",
        "Question_body":"<p>I have a secret stored in AWS Secret Manager.\nI am trying to access that secret from a container which is in ECR.<\/p>\n<p>When I execute the container, the error message I get is:<\/p>\n<pre><code>  File &quot;\/opt\/program\/train&quot;, line 65, in get_secret\n    SecretId=secret_name\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/client.py&quot;, line 508, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/client.py&quot;, line 895, in _make_api_call\n    operation_model, request_dict, request_context\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/client.py&quot;, line 917, in _make_request\n    return self._endpoint.make_request(operation_model, request_dict)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/endpoint.py&quot;, line 116, in make_request\n    return self._send_request(request_dict, operation_model)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/endpoint.py&quot;, line 195, in _send_request\n    request = self.create_request(request_dict, operation_model)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/endpoint.py&quot;, line 134, in create_request\n    operation_name=operation_model.name,\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/hooks.py&quot;, line 412, in emit\n    return self._emitter.emit(aliased_event_name, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/hooks.py&quot;, line 256, in emit\n    return self._emit(event_name, kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/hooks.py&quot;, line 239, in _emit\n    response = handler(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/signers.py&quot;, line 103, in handler\n    return self.sign(operation_name, request)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/signers.py&quot;, line 187, in sign\n    auth.add_auth(request)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/botocore\/auth.py&quot;, line 407, in add_auth\n    raise NoCredentialsError()\nbotocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n<p>The container is using python and I am using the same function given as an example when I setup the secret manager entry for python3\nThe same function works well in my local environment where I am authenticated via CLI.<\/p>\n<p>The function is:<\/p>\n<pre><code>import boto3\nimport base64\nfrom botocore.exceptions import ClientError\n\n\ndef get_secret():\n\n    secret_name = &quot;secret1&quot;\n    region_name = &quot;us-east-1&quot;\n\n    # Create a Secrets Manager client\n    session = boto3.session.Session()\n    client = session.client(\n        service_name='secretsmanager',\n        region_name=region_name\n    )\n\n    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\n    # See https:\/\/docs.aws.amazon.com\/secretsmanager\/latest\/apireference\/API_GetSecretValue.html\n    # We rethrow the exception by default.\n\n    try:\n        get_secret_value_response = client.get_secret_value(\n            SecretId=secret_name\n        )\n    except ClientError as e:\n        if e.response['Error']['Code'] == 'DecryptionFailureException':\n            # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n            # Deal with the exception here, and\/or rethrow at your discretion.\n            raise e\n        elif e.response['Error']['Code'] == 'InternalServiceErrorException':\n            # An error occurred on the server side.\n            # Deal with the exception here, and\/or rethrow at your discretion.\n            raise e\n        elif e.response['Error']['Code'] == 'InvalidParameterException':\n            # You provided an invalid value for a parameter.\n            # Deal with the exception here, and\/or rethrow at your discretion.\n            raise e\n        elif e.response['Error']['Code'] == 'InvalidRequestException':\n            # You provided a parameter value that is not valid for the current state of the resource.\n            # Deal with the exception here, and\/or rethrow at your discretion.\n            raise e\n        elif e.response['Error']['Code'] == 'ResourceNotFoundException':\n            # We can't find the resource that you asked for.\n            # Deal with the exception here, and\/or rethrow at your discretion.\n            raise e\n    else:\n        # Decrypts secret using the associated KMS key.\n        # Depending on whether the secret is a string or binary, one of these fields will be populated.\n        if 'SecretString' in get_secret_value_response:\n            secret = get_secret_value_response['SecretString']\n        else:\n            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n            \n    # Your code goes here. \n<\/code><\/pre>\n<p>But in the docker container I have not performed a CLI authentication. I have given the sagemaker access role the KMS permissions for my region.\nAny thoughts on how I can get a sagemaker container to access secret manager?\nThanks!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1660851395430,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|secretsmanager",
        "Question_view_count":25,
        "Owner_creation_time":1458196224610,
        "Owner_last_access_time":1661699962880,
        "Owner_location":null,
        "Owner_reputation":981,
        "Owner_up_votes":64,
        "Owner_down_votes":1,
        "Owner_views":74,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73408571",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71553217,
        "Question_title":"Unable to access data from S3 bucket to jupyter notebook of aws sagemaker",
        "Question_body":"<p>I need to train a model on aws sagemaker. I'm unable to access data in Jupiter notebook of sagemaker from S3 bucket. My bucket name is &quot;riceleaf&quot; there are four folders in the bucket named as s1,s2,s3,s4 and each folder contains 330 images named as 1.jpg and so on. It is created in Us-east zone. Bucket is private.<\/p>\n<p>One way i did was to access the object and when i displayed the key it shows me 1.jpg and so on. But when i try to open that image it didn't work. So i think I'm unable to get exact data path.<\/p>\n<p>In my code I need exact data path since I'm doing some random data generation in the code so need to access different folders. Therefore, I need a path till bucket so i can change next folder name and image name randomly in my code.<\/p>\n<p>Please help me to so that I can access the images in the Jupiter notebook of sagemaker.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1647841162080,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|deep-learning|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":733,
        "Owner_creation_time":1647840996453,
        "Owner_last_access_time":1650607384703,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1647863096849,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71553217",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61626685,
        "Question_title":"SageMaker Estimator from EC2",
        "Question_body":"<p>I have a really big confuse about how AWS integrate Docker ECR with SageMaker, even though it works from sagemaker using just the Dockerfile and train.py script executing it from another script which uses the Estimator class, it doesn't from EC2<\/p>\n\n<pre><code>from sagemaker.estimator import Estimator\nimport sagemaker \n\nestimator = Estimator(\n  image_name=\"test_docker\",\n  role='arn:aws:iam::XXXXXXXXX:role\/service-role\/AmazonSageMaker-ExecutionRole-XXXXXXXX',\n  train_instance_count=1,\n  train_instance_type='local'\n)\nestimator.fit()\n<\/code><\/pre>\n\n<p>For this, I have the same folder with Dockerfile and train.py script and created another script called exec.py, this script has the estimator code, but when I execute it I get this error<\/p>\n\n<pre><code>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the\nCreateTrainingJob operation: 1 validation error detected: Value \n'test_docker-2020-05-06-02-50-56-375' at 'trainingJobName' failed to satisfy constraint: Member must \nsatisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9])*\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1588734515670,
        "Question_score":1,
        "Question_tags":"python|amazon-ec2|amazon-sagemaker|amazon-ecr",
        "Question_view_count":72,
        "Owner_creation_time":1498690621233,
        "Owner_last_access_time":1624591691807,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61626685",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70578172,
        "Question_title":"Can we use data directly from RDS or df as a data source for training job in Sagemaker, rather than pulling it from from s3 or EFS?",
        "Question_body":"<p>I am using Sagemaker platform for model development and deployment. Data is read from RDS tables and then spitted to train and test df.\nTo create the training job in Sagemaker, I found that it takes data source only as s3 and EFS. For that I need to keep train and test data back to s3, which is repeating the data storing process in RDS and s3.\nI would want to directly pass the df from RDS as a parameter in tarining job code. Is there any way we can pass df in fit method<\/p>\n<pre><code>    image=&quot;581132636225.dkr.ecr.ap-south-1.amazonaws.com\/sagemaker-ols-model:latest&quot;\n    model_output_folder = &quot;model-output&quot;\n    print(image)\n    tree = sagemaker.estimator.Estimator(\n        image,\n        role,\n        1,\n        &quot;ml.c4.2xlarge&quot;,\n        output_path=&quot;s3:\/\/{}\/{}&quot;.format(sess.default_bucket(), model_output_folder),\n        sagemaker_session=sess,\n    )\n\n**tree.fit({'train': &quot;s3_path_having_test_data&quot;}, wait=True)**\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1641296123900,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|data-science|amazon-sagemaker",
        "Question_view_count":160,
        "Owner_creation_time":1545906764167,
        "Owner_last_access_time":1643787536800,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1641316932063,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70578172",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68660085,
        "Question_title":"Install Voila on SageMaker based Jupyter Notebook",
        "Question_body":"<p>I'm trying to run the Voila! server on a SageMaker Notebook.<\/p>\n<p>Commands to install and enable voila:<\/p>\n<pre><code>!pip install voila\n!jupyter serverextension enable --sys-prefix voila\n<\/code><\/pre>\n<p>The pip command appears to work OK.<\/p>\n<p>Terminal output from the jupyter command:<\/p>\n<blockquote>\n<p>Config option <code>kernel_spec_manager_class<\/code> not recognized by <code>EnableServerExtensionApp<\/code>.\nEnabling: voila<\/p>\n<ul>\n<li>Writing config: \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/etc\/jupyter\n<ul>\n<li>Validating...\nvoila 0.2.10 OK<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/blockquote>\n<p>Navigating to a 'voila url' such as:<\/p>\n<pre><code>https:\/\/{instance}.notebook.{region}.sagemaker.aws\/voila\/sandbox\/test-notebook.ipynb\n<\/code><\/pre>\n<p>Returns a 404.<\/p>\n<p>The <em><strong>'nbextensions'<\/strong><\/em> tab shows the voila extension as 'possibly incompatible'<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/KwHhu.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KwHhu.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I can't find anything in the Jupyter logs that looks relevant.<\/p>\n<p>Restarting Jupyter didn't help.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1628132271717,
        "Question_score":1,
        "Question_tags":"jupyter-notebook|amazon-sagemaker|voila",
        "Question_view_count":247,
        "Owner_creation_time":1415582465687,
        "Owner_last_access_time":1662954300267,
        "Owner_location":"Australia",
        "Owner_reputation":2899,
        "Owner_up_votes":63,
        "Owner_down_votes":3,
        "Owner_views":247,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68660085",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73707074,
        "Question_title":"AWS SageMaker Domain Status \"Update_Failed\" due to custom image appImageConfigName error",
        "Question_body":"<p>I'm having some trouble recovering from failures in attaching custom images to my sagemaker domain.<\/p>\n<p>I first created a custom image according to <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-a-custom-image-to-bring-your-own-development-environment-to-rstudio-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>When I use sagemaker console to attach the image built with sm-docker, it appears to successfully &quot;attach&quot; in the domain's image list, but when inspecting the image in the console, it shows an error:<\/p>\n<blockquote>\n<p>Value '' at 'appImageConfigName' failed to satisfy constraint: Member\nmust satisfy regular expression pattern<\/p>\n<\/blockquote>\n<p>This occurs even when the repository or tag are comprised of only alphanumeric characters.<\/p>\n<p>After obtaining this error, I deleted the repositories in ECR.<\/p>\n<p>Since then, the domain fails to update and I am unable to launch any apps or attempt to attach additional images.<\/p>\n<p>The first issue I would like to address is restoring functionality of my sagemaker domain so I can further troubleshoot the issue. I am unable to delete the domain because of this error, even when there are no users, apps, or custom images associated with the domain.<\/p>\n<p>The second issue I would like to address is being able troubleshoot the appImageConfigName error.<\/p>\n<p>Thanks!<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bzrrA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bzrrA.png\" alt=\"image errors\" \/><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/EyGV1.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EyGV1.png\" alt=\"domain status\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663090122220,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":20,
        "Owner_creation_time":1340897566210,
        "Owner_last_access_time":1664049615040,
        "Owner_location":null,
        "Owner_reputation":547,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Question_last_edit_time":1663092028972,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73707074",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71054340,
        "Question_title":"OCR in Sage Maker",
        "Question_body":"<p>Guys i am trying to build infrastructure on aws for getting help from others on annotation. currently we uses label-studio for text annotation. as might know you can label text by selecting through polygon and than writing what does selected area mean. ex: if polygon is made around english word than what writing out label  of it to annotate that given english word. for more see image below.<a href=\"https:\/\/i.stack.imgur.com\/8jvaj.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8jvaj.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>How can i do this in <strong>SageMaker Ground Truth<\/strong>. as far as i have gone i think it can just label pre defined words. you cant create custom label in it by selecting any given area using polygon in image am i right ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644428834663,
        "Question_score":1,
        "Question_tags":"amazon-web-services|ocr|amazon-sagemaker|text-classification",
        "Question_view_count":67,
        "Owner_creation_time":1482044943533,
        "Owner_last_access_time":1664036901133,
        "Owner_location":"Gurgaon, Haryana, India",
        "Owner_reputation":549,
        "Owner_up_votes":19,
        "Owner_down_votes":14,
        "Owner_views":50,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71054340",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59921196,
        "Question_title":"Can you load a SageMaker trained model into Keras?",
        "Question_body":"<p>I have followed the AWS tutorial(<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_image_json_format.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_image_json_format.ipynb<\/a>) and trained my first model using SageMaker.<\/p>\n\n<p>The end result is an archive containing the following files:\n- hyperparams.json\n- model_algo_1-0000.params\n- model_algo_1-symbol.json<\/p>\n\n<p>I am not familiar with this format, and was not able to load it into Keras via keras.models.model_from_json()<\/p>\n\n<p>I am assuming this is a different format or an AWS proprietary one.<\/p>\n\n<p>Can you please help me identify the format?\nIs it possible to load this into a Keras model and do inference without an EC2 instance(locally)?<\/p>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1580061744520,
        "Question_score":1,
        "Question_tags":"keras|object-detection|amazon-sagemaker",
        "Question_view_count":212,
        "Owner_creation_time":1533465584553,
        "Owner_last_access_time":1663925167400,
        "Owner_location":"Bucharest, Romania",
        "Owner_reputation":341,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Built-in algorithms are implemented with Apache MXNet, so that's how you'd load the model locally. load_checkpoint() is the appropriate API: <a href=\"https:\/\/mxnet.apache.org\/api\/python\/docs\/api\/mxnet\/model\/index.html#mxnet.model.load_checkpoint\" rel=\"nofollow noreferrer\">https:\/\/mxnet.apache.org\/api\/python\/docs\/api\/mxnet\/model\/index.html#mxnet.model.load_checkpoint<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1580119883430,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59921196",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73041737,
        "Question_title":"Print SageMaker instance type",
        "Question_body":"<p>Is there a function I can use to get the instance type of my SageMaker instance.<\/p>\n<p>I basically want to do something like this<\/p>\n<pre><code>region = boto3.Session().region_name\n<\/code><\/pre>\n<p>but for the instance type.<\/p>\n<p>I know I can find it manually, but I want to automate it so that my script can work on any instance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658254476363,
        "Question_score":0,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":40,
        "Owner_creation_time":1657050754840,
        "Owner_last_access_time":1663796977727,
        "Owner_location":null,
        "Owner_reputation":15,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeNotebookInstance.html\" rel=\"nofollow noreferrer\">DescribeNotebookInstance<\/a> API to get the instance size.<\/p>\n<pre><code>sm_client = boto3.client(&quot;sagemaker&quot;)\nsm.describe_notebook_instance(\n    NotebookInstanceName=&lt;nb-name&gt;\n)['InstanceType']\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1658271130892,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73041737",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71492181,
        "Question_title":"Memory allocation error Call to XGBoost C function XGBoosterUpdateOneIter failed: std::bad_alloc",
        "Question_body":"<p>Working with Julia notebook on Sagemaker:\n<code>ml.m5d.24xlarge<\/code> with <code>500GB<\/code> memory.<\/p>\n<p>I'm training an XGBoost with 230 features (500MB per file on avg). It trains without an issue upto 205 files, but afterwards, randomly I get this error<\/p>\n<pre><code>&gt; \u250c Info: Starting XGBoost training\n\u2514   num_boost_rounds = 99\nERROR: LoadError: Call to XGBoost C function XGBoosterUpdateOneIter failed: std::bad_alloc\nStacktrace:\n  [1] error(::String, ::String, ::String, ::String)\n    @ Base .\/error.jl:42\n  [2] XGBoosterUpdateOneIter(handle::Ptr{Nothing}, iter::Int32, dtrain::Ptr{Nothing})\n    @ XGBoost ~\/.julia\/packages\/XGBoost\/fI0vs\/src\/xgboost_wrapper_h.jl:11\n  [3] #update#21\n    @ ~\/.julia\/packages\/XGBoost\/fI0vs\/src\/xgboost_lib.jl:204 [inlined]\n  [4] xgboost(data::XGBoost.DMatrix, nrounds::Int64; label::Type, param::Vector{Any}, watchlist::Vector{Any}, metrics::Vector{String}, obj::Type, feval::Type, group::Vector{Any}, kwargs::Base.Iterators.Pairs{Symbol, Any, NTuple{15, Symbol}, NamedTuple{(:objective, :num_class, :num_parallel_tree, :eta, :gamma, :max_depth, :min_child_weight, :max_delta_step, :subsample, :colsample_bytree, :lambda, :alpha, :tree_method, :grow_policy, :max_leaves), Tuple{String, Int64, Int64, Float64, Float64, Int64, Int64, Int64, Float64, Float64, Int64, Int64, String, String, Int64}}})\n    @ XGBoost ~\/.julia\/packages\/XGBoost\/fI0vs\/src\/xgboost_lib.jl:185\n  [5] macro expansion\n    @ \/home\/src\/Training.jl:175 [inlined]\n  [6] macro expansion\n    @ .\/timing.jl:210 [inlined]\n<\/code><\/pre>\n<p>Not sure how to fix it. The AWS instance has maximum CPU memory.\nAlso, already using 99 procs\/workers.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647408688790,
        "Question_score":2,
        "Question_tags":"jupyter-notebook|julia|xgboost|amazon-sagemaker",
        "Question_view_count":144,
        "Owner_creation_time":1348471761127,
        "Owner_last_access_time":1663922173097,
        "Owner_location":null,
        "Owner_reputation":343,
        "Owner_up_votes":95,
        "Owner_down_votes":0,
        "Owner_views":40,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71492181",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70149158,
        "Question_title":"Security Group settings for using sagemaker notebooks in private subnet",
        "Question_body":"<p>I am new to sagemaker, and am hoping to use sagemaker in a VPC with a private subnet, so data accessed from s3 is not exposed to public internet.<\/p>\n<p>I have created a vpc with a private subnet (no internet or nat gateway), and have attached a vpc s3 gateway endpoint - with this, can I apply the subnet's <strong>default<\/strong> security group settings to the sagemaker notebook instances? ..or are some additional configurations to this required?<\/p>\n<p>Also, I'm hoping to keep internet access for the sagemaker notebook instance, so I can still download python packages (but just wanting to ensure data read from s3 using the private subnet is all okay with its default security group)<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1638152715770,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-vpc|amazon-sagemaker|aws-security-group|private-subnet",
        "Question_view_count":919,
        "Owner_creation_time":1508838702423,
        "Owner_last_access_time":1648367115880,
        "Owner_location":null,
        "Owner_reputation":89,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":21,
        "Question_last_edit_time":1638177719560,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70149158",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65497571,
        "Question_title":"got warning No model artifact is saved under path \/opt\/ml\/model. Your training job will not save any model files to S3. and no model deployed on aws",
        "Question_body":"<pre><code>n_user=20\nn_item=100\n\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(&quot;-M&quot;, &quot;--model_dir&quot;, help=&quot;show model&quot;)\nparser.add_argument(&quot;-f&quot;, &quot;--file&quot;, help=&quot;show model help&quot;)\nargs = parser.parse_args()\nmodel_dir='\/opt\/ml\/model'\nfrom sagemaker.tensorflow import TensorFlow\nncf_estimator = TensorFlow(  \n    entry_point='ncf.py',\n    role=sagemaker.get_execution_role(),  \n    train_instance_count=1,\n    train_instance_type='ml.c5.2xlarge',  \n    framework_version='2.1.0',  \n    py_version='py3',  \n    distributions={'parameter_server': {'enabled': True}},  \n    hyperparameters={'epochs': 3, 'batch_size': 256, 'n_user': n_user, 'n_item': n_item},\n)\n<\/code><\/pre>\n<p>I have the following code on Amazon Sagemaker and it is giving me error to define a model_dir, i tried different ways but failed please help me with that.<\/p>\n<p>and the error when i train the model is:<\/p>\n<pre><code>2020-12-29 19:38:02,906 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path \/opt\/ml\/model. Your training job will not save any model files to S3.\nFor details of how to construct your training script see:\nhttps:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html#adapting-your-local-tensorflow-script\n<\/code><\/pre>\n<p>I have seen the docs but couldn't get anything.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1609269727237,
        "Question_score":0,
        "Question_tags":"amazon-web-services|tensorflow|machine-learning|keras|amazon-sagemaker",
        "Question_view_count":174,
        "Owner_creation_time":1584447559510,
        "Owner_last_access_time":1620476210433,
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":46,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1609280453089,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65497571",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73294393,
        "Question_title":"AWS SageMaker: could not load debugger information of estimator",
        "Question_body":"<p>I am using a SageMaker notebook in AWS for training a ML model. When I created and trained the estimator successfully with the following script, I could load the debugging information (s3_output_path) as expected:<\/p>\n<pre><code>from sagemaker.debugger import Rule, DebuggerHookConfig, CollectionConfig, rule_configs\nrules = [\n    Rule.sagemaker(rule_configs.loss_not_decreasing()),\n    Rule.sagemaker(rule_configs.vanishing_gradient()),\n    Rule.sagemaker(rule_configs.overfit()),\n    Rule.sagemaker(rule_configs.overtraining()),\n    Rule.sagemaker(rule_configs.poor_weight_initialization())]\n\ncollection_configs=[CollectionConfig(name=&quot;CrossEntropyLoss_output_0&quot;, parameters={\n    &quot;include_regex&quot;: &quot;CrossEntropyLoss_output_0&quot;, &quot;train.save_interval&quot;: &quot;100&quot;,&quot;eval.save_interval&quot;: &quot;10&quot;})]\n\ndebugger_config = DebuggerHookConfig(\n    collection_configs=collection_configs)\n\nestimator = PyTorch(\nrole=sagemaker.get_execution_role(),\ninstance_count=1,\ninstance_type=&quot;ml.m5.xlarge&quot;,\n#instance_type=&quot;ml.g4dn.2xlarge&quot;,\nentry_point=&quot;train.py&quot;,\nframework_version=&quot;1.8&quot;,\npy_version=&quot;py36&quot;,\nhyperparameters=hyperparameters,\ndebugger_hook_config=debugger_config,\nrules=rules,\n)\n\nestimator.fit({&quot;training&quot;: inputs})\n\ns3_output_path = estimator.latest_job_debugger_artifacts_path()\n<\/code><\/pre>\n<p>After the kernel died, I attached the estimator and tried to access the debugging information of the training.<\/p>\n<pre><code>estimator = sagemaker.estimator.Estimator.attach('pytorch-training-2022-06-07-11-07-09-804')\n\ns3_output_path = estimator.latest_job_debugger_artifacts_path()\nrules_path = estimator.debugger_rules\n<\/code><\/pre>\n<p>The return values of these functions were None.\nCould this be a problem with the attach-function? And how can I access training information of the debugger after the kernel was shut down?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660059012287,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|pytorch|amazon-sagemaker",
        "Question_view_count":45,
        "Owner_creation_time":1501430845497,
        "Owner_last_access_time":1663570204107,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73294393",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68386650,
        "Question_title":"Can one use Terraform templates with SageMaker ML Pipelines?",
        "Question_body":"<p>SageMaker has ML Pipelines that come with \u201cML templates\u201d, which I assume are Cloud Formation templates for machine learning pipelines.<\/p>\n<p>Can one use custom Terraform templates instead of Cloud Formation? Where does one place the Terraform templates? Can this be done through the SageMaker UI?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1626310980797,
        "Question_score":0,
        "Question_tags":"machine-learning|terraform|amazon-cloudformation|amazon-sagemaker|aws-codepipeline",
        "Question_view_count":1267,
        "Owner_creation_time":1346443720090,
        "Owner_last_access_time":1664079461297,
        "Owner_location":null,
        "Owner_reputation":11650,
        "Owner_up_votes":6318,
        "Owner_down_votes":21,
        "Owner_views":977,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68386650",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70217519,
        "Question_title":"Error Invoking xgboost endpoint deployed locally : feature names mismatch",
        "Question_body":"<p>I have trained a xgboost model locally and running into <code>feature_names mismatch<\/code> issue when invoking the endpoint. My <code>model<\/code> is a xgboost Regressor with some pre-processing (variable encoding) and hyper-parameter tuning. Code to train the model:<\/p>\n<p>version xgboost <code>0.90<\/code><\/p>\n<pre><code>import pandas as pd\nimport pickle\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder \n\n\n# split df into train and test\nX_train, X_test, y_train, y_test = train_test_split(df.iloc[:,0:21], df.iloc[:,-1], test_size=0.1)\n\nX_train.shape\n(1000,21)\n\n# Encode categorical variables  \ncat_vars = ['cat1','cat2','cat3']\ncat_transform = ColumnTransformer([('cat', OneHotEncoder(handle_unknown='ignore'), cat_vars)], remainder='passthrough')\n\nencoder = cat_transform.fit(X_train)\nX_train = encoder.transform(X_train)\nX_test = encoder.transform(X_test)\n\nX_train.shape\n(1000,420)\n\n# Define a xgboost regression model\nmodel = XGBRegressor()\n\n# Do hyper-parameter tuning\n.....\n\n# Fit model - convert to numpy arrays\nmodel.fit(X_train.toarray(), y_train.values())\n<\/code><\/pre>\n<p>Here's what <code>model<\/code> object looks like:<\/p>\n<pre><code>XGBRegressor(colsample_bytree=xxx, gamma=xxx,\n             learning_rate=xxx, max_depth=x, n_estimators=xxx,\n             subsample=xxx)\n<\/code><\/pre>\n<p>My test data is a string of float values which is turned into an array as the data must be passed as numpy array to match how the model was trained.<\/p>\n<pre><code>testdata = [........., 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 2000, 200, 85, 412412, 123, 41, 552, 50000, 512, 0.1, 10.0, 2.0, 0.05]\n<\/code><\/pre>\n<p>Traceback<\/p>\n<pre><code>\nValueError: feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5'....'f419'] ['f0']\ngc43e67lgg-algo-1-8jxpe  | expected f156, f266, f164, f207, f107, f296, f417, f188, f131, f225,.....\n\n<\/code><\/pre>\n<p>I can retrieve the feature names for the model by running <code>model.get_booster().feature_names<\/code>. Is there a way I can use these names and assign to test data point so that they are consistent?<\/p>\n<pre><code>['f0', 'f1', 'f2', 'f3', 'f4', 'f5',......'f417','f418','f419']\n<\/code><\/pre>\n<p>More info on inference script here: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/blob\/main\/xgboost_script_mode_local_training_and_serving\/code\/inference.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/blob\/main\/xgboost_script_mode_local_training_and_serving\/code\/inference.py<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1638548403397,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":146,
        "Owner_creation_time":1393967047770,
        "Owner_last_access_time":1663978179853,
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":1503,
        "Owner_up_votes":179,
        "Owner_down_votes":3,
        "Owner_views":429,
        "Question_last_edit_time":1638820655712,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70217519",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54408673,
        "Question_title":"Getting sagemaker container locally",
        "Question_body":"<p>When I try to run sagemaker locally for tensorflow in script mode. It seems like I cannot pull the docker container. I have ran the code below from a sagemaker notebook instance and everything ran fine. But when running it on my machine it doesn't work.<\/p>\n\n<p>How can I download the container, so I can debug things locally?<\/p>\n\n<pre><code>import os\n\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\n\n\nhyperparameters = {}\nrole = 'arn:aws:iam::xxxxxxxx:role\/yyyyyyy'\nestimator = TensorFlow(\n    entry_point='train.py',\n    source_dir='.',\n    train_instance_type='local',\n    train_instance_count=1,\n    hyperparameters=hyperparameters,\n    role=role,\n    py_version='py3',\n    framework_version='1.12.0',\n    script_mode=True)\n\nestimator.fit()\n<\/code><\/pre>\n\n<p>I get this output<\/p>\n\n<pre><code>INFO:sagemaker:Creating training-job with name: sagemaker-tensorflow-\nscriptmode-2019-01-28-18-51-57-787\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nError response from daemon: pull access denied for 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode, repository does not exist or may require 'docker login'\n\nsubprocess.CalledProcessError: Command 'docker pull 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.12.0-cpu-py3' returned non-zero exit status 1.\n<\/code><\/pre>\n\n<p>The warning looks like the output you get when using the docker login stuff <a href=\"https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/Registries.html\" rel=\"nofollow noreferrer\">here<\/a>. If I follow these steps to register to the directory with tensorflow container it says login success<\/p>\n\n<pre><code>Invoke-Expression -Command (aws ecr get-login --no-include-email --registry-ids 520713654638 --region eu-west-2)\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\n<\/code><\/pre>\n\n<p>But then I still cannot pull it<\/p>\n\n<pre><code>docker pull 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.11.0-cpu-py3\nError response from daemon: pull access denied for 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode, repository does not exist or may require 'docker login'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548702401737,
        "Question_score":2,
        "Question_tags":"amazon-web-services|docker|amazon-sagemaker",
        "Question_view_count":2235,
        "Owner_creation_time":1352206833663,
        "Owner_last_access_time":1664011820603,
        "Owner_location":null,
        "Owner_reputation":893,
        "Owner_up_votes":112,
        "Owner_down_votes":2,
        "Owner_views":185,
        "Question_last_edit_time":null,
        "Answer_body":"<p>the same sequence works for me locally : 'aws ecr get-login', 'docker login', 'docker pull'. <\/p>\n\n<p>Does your local IAM user have sufficient credentials to pull from ECR? The 'AmazonEC2ContainerRegistryReadOnly' policy should be enough: <a href=\"https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/ecr_managed_policies.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/ecr_managed_policies.html<\/a><\/p>\n\n<p>Alternatively, you can grab the container from Github and build it: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1548750603276,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54408673",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66061721,
        "Question_title":"In AWS, Execute notebook from sagemaker",
        "Question_body":"<p>Im using AWS SageMaker Notebooks.<\/p>\n<p>What is the best way to execute notebook from sagemaker?<\/p>\n<ul>\n<li>My idea is to have an S3 bucket.<\/li>\n<li>When a new file is putted there i want to execute a notebook that reads from S3 and puts the output in other bucket.<\/li>\n<\/ul>\n<p>The only way i have from now is to start an S3 event, execute a lambda function that starts a sagemaker instance and execute the notebook. But is getting too much time to start and it doesnt work yet for me with a big notebook.<\/p>\n<p>Maybe is better to export the notebook and execute it from another place in aws (in order to be faster), but i dont know where.<\/p>\n<p>Thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1612521473383,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-s3|aws-lambda|amazon-sagemaker",
        "Question_view_count":204,
        "Owner_creation_time":1605095949960,
        "Owner_last_access_time":1646131913857,
        "Owner_location":"La Rioja, Spain",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1612774473150,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66061721",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48365866,
        "Question_title":"How to call Sagemaker training model endpoint API in C#",
        "Question_body":"<p>I have implemented machine learning algorithms through sagemaker.<\/p>\n\n<p>I have installed SDK for .net, and tried by executing below code.<\/p>\n\n<pre><code>Uri sagemakerEndPointURI = new Uri(\"https:\/\/runtime.sagemaker.us-east-2.amazonaws.com\/endpoints\/MyEndpointName\/invocations\");\nAmazon.SageMakerRuntime.Model.InvokeEndpointRequest request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest();\nrequest.EndpointName = \"MyEndpointName\";\nAmazonSageMakerRuntimeClient aawsClient = new AmazonSageMakerRuntimeClient(myAwsAccessKey,myAwsSecreteKey);            \nAmazon.SageMakerRuntime.Model.InvokeEndpointResponse resposnse= aawsClient.InvokeEndpoint(request);\n<\/code><\/pre>\n\n<p>By executing this, I am getting validation error as \"<code>1 validation error detected: Value at 'body' failed to satisfy constraint: Member must not be null<\/code>\"<\/p>\n\n<p>Can anyone guide me on how and what more input data I need to pass to call the given API?<\/p>\n\n<p>EDIT<\/p>\n\n<p>Further I'd tried by provinding body parameter which contains a MemoryStream written by a '.gz' or '.pkl' file, and it giving me error as : \"Error unmarshalling response back from AWS,  HTTP content length exceeded 5246976 bytes.\"<\/p>\n\n<p>EDIT 1\/23\/2018<\/p>\n\n<p>Further I came up with the error message as <\/p>\n\n<blockquote>\n  <p>ERROR - model server - 'TypeError' object has no attribute 'message'<\/p>\n<\/blockquote>\n\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1516531050743,
        "Question_score":8,
        "Question_tags":"c#|amazon-web-services|amazon-s3|sparkr|amazon-sagemaker",
        "Question_view_count":2093,
        "Owner_creation_time":1337759214690,
        "Owner_last_access_time":1532954663377,
        "Owner_location":"Pune India",
        "Owner_reputation":1036,
        "Owner_up_votes":34,
        "Owner_down_votes":1,
        "Owner_views":124,
        "Question_last_edit_time":1516720777516,
        "Answer_body":"<p>Later solved it by <code>Encoding.ASCII.GetBytes<\/code>as in below code.<\/p>\n\n<pre><code> byte[] bytes = System.IO.File.ReadAllBytes(@\"EXCEL_FILE_PATH\");\n    string listA = \"\";\n    while (!reader.EndOfStream)\n        {\n            var line = reader.ReadLine();\n            listA = listA + line + \"\\n\";\n        }\n    byte[] bytes = Encoding.ASCII.GetBytes(listA);\n    request.Body = new MemoryStream(bytes);\n    InvokeEndpointResponse response = sagemakerRunTimeClient.InvokeEndpoint(request);\n    string predictions = Encoding.UTF8.GetString(response.Body.ToArray());\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1519637555372,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48365866",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59762829,
        "Question_title":"AWS Sagemaker scikit_bring_your_own example",
        "Question_body":"<p>I am following the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own%20example\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own example<\/a> for product recommendations.<\/p>\n\n<p>I want to use the SVD from <a href=\"https:\/\/pypi.org\/project\/scikit-surprise\/\" rel=\"nofollow noreferrer\">scikit-surprise<\/a> library on Sagemaker.<\/p>\n\n<pre><code>from surprise import SVD\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\n<\/code><\/pre>\n\n<p>I added the scikit-surprise package in the Dockerfile, but i am getting the following errors:<\/p>\n\n<h1>Dockerfile:<\/h1>\n\n<pre><code># Build an image that can do training and inference in SageMaker\n# This is a Python 2 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:16.04\n\nMAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Here we get all python packages.\n# There's substantial overlap between scipy and numpy that we eliminate by\n# linking them together. Likewise, pip leaves the install caches populated which uses\n# a significant amount of space. These optimizations save a fair amount of space in the\n# image, which reduces start up time.\nRUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp; \\\n    pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp; \\\n        (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp; \\\n        rm -rf \/root\/.cache\n\nRUN pip install scikit-surprise\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=\"\/opt\/program:${PATH}\"\n\n# Set up the program in the image\nCOPY products_recommender \/opt\/program\nWORKDIR \/opt\/program\n<\/code><\/pre>\n\n<h1>Docker build and deploy :<\/h1>\n\n<pre><code>fullname:XXXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender:latest\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\nSending build context to Docker daemon  67.58kB\nStep 1\/10 : FROM ubuntu:16.04\n ---&gt; 13c9f1285025\nStep 2\/10 : MAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n ---&gt; Using cache\n ---&gt; 44baf3286201\nStep 3\/10 : RUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends          wget          python          nginx          ca-certificates     &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n ---&gt; Using cache\n ---&gt; 8983fa906515\nStep 4\/10 : RUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp;     pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp;         (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp;         rm -rf \/root\/.cache\n ---&gt; Using cache\n ---&gt; 9dbfedf02b57\nStep 5\/10 : RUN pip install scikit-surprise\n ---&gt; Running in 82295cb0affe\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\nCollecting scikit-surprise\n  Downloading https:\/\/files.pythonhosted.org\/packages\/f5\/da\/b5700d96495fb4f092be497f02492768a3d96a3f4fa2ae7dea46d4081cfa\/scikit-surprise-1.1.0.tar.gz (6.4MB)\nCollecting joblib&gt;=0.11 (from scikit-surprise)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/28\/5c\/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf\/joblib-0.14.1-py2.py3-none-any.whl (294kB)\nRequirement already satisfied: numpy&gt;=1.11.2 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.16.2)\nRequirement already satisfied: scipy&gt;=1.0.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.2.1)\nRequirement already satisfied: six&gt;=1.10.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.12.0)\nBuilding wheels for collected packages: scikit-surprise\n  Building wheel for scikit-surprise (setup.py): started\n  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n  ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-Bb1_iT --python-tag cp27:\n  ERROR: running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build\/lib.linux-x86_64-2.7\n  creating build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running egg_info\n  writing requirements to scikit_surprise.egg-info\/requires.txt\n  writing scikit_surprise.egg-info\/PKG-INFO\n  writing top-level names to scikit_surprise.egg-info\/top_level.txt\n  writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n  writing entry points to scikit_surprise.egg-info\/entry_points.txt\n  reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  reading manifest template 'MANIFEST.in'\n  writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running build_ext\n  building 'surprise.similarities' extension\n  creating build\/temp.linux-x86_64-2.7\n  creating build\/temp.linux-x86_64-2.7\/surprise\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n  ----------------------------------------\n  ERROR: Failed building wheel for scikit-surprise\n  Running setup.py clean for scikit-surprise\nFailed to build scikit-surprise\nInstalling collected packages: joblib, scikit-surprise\n  Running setup.py install for scikit-surprise: started\n    Running setup.py install for scikit-surprise: finished with status 'error'\n    ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile:\n    ERROR: running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib.linux-x86_64-2.7\n    creating build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running egg_info\n    writing requirements to scikit_surprise.egg-info\/requires.txt\n    writing scikit_surprise.egg-info\/PKG-INFO\n    writing top-level names to scikit_surprise.egg-info\/top_level.txt\n    writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n    writing entry points to scikit_surprise.egg-info\/entry_points.txt\n    reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    reading manifest template 'MANIFEST.in'\n    writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running build_ext\n    building 'surprise.similarities' extension\n    creating build\/temp.linux-x86_64-2.7\n    creating build\/temp.linux-x86_64-2.7\/surprise\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n    unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n    ----------------------------------------\nERROR: Command \"\/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in \/tmp\/pip-install-VsuzGr\/scikit-surprise\/\nWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nThe command '\/bin\/sh -c pip install scikit-surprise' returned a non-zero code: 1\nThe push refers to repository [XXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender]\n89c1adca7d35: Layer already exists \nddcb6879486f: Layer already exists \n4a02efecad74: Layer already exists \n92d3f22d44f3: Layer already exists \n10e46f329a25: Layer already exists \n24ab7de5faec: Layer already exists \n1ea5a27b0484: Layer already exists \nlatest: digest: sha256:5ed35f1964d10f13bc8a05d379913c24195ea31ec848157016381fbd1bb12f28 size: 1782\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579147150087,
        "Question_score":1,
        "Question_tags":"scikit-learn|amazon-sagemaker|svd",
        "Question_view_count":341,
        "Owner_creation_time":1241005356853,
        "Owner_last_access_time":1663511013130,
        "Owner_location":"Kuala Lumpur, Malaysia",
        "Owner_reputation":15794,
        "Owner_up_votes":1343,
        "Owner_down_votes":26,
        "Owner_views":1032,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The 'x86_64-linux-gnu-gcc' binary can't be found in environment where you're building the container. Make sure that gcc is installed, and that you use the right name (gcc?).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1579273412889,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59762829",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65072778,
        "Question_title":"Download checkpoint from AWS",
        "Question_body":"<p>How can I download the checkpoints and logged statistics after I run my deep learning algorithm on AWS using SageMaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1606735973450,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-ec2|amazon-sagemaker",
        "Question_view_count":44,
        "Owner_creation_time":1491911840993,
        "Owner_last_access_time":1653036550053,
        "Owner_location":null,
        "Owner_reputation":1957,
        "Owner_up_votes":47,
        "Owner_down_votes":1,
        "Owner_views":278,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65072778",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58802366,
        "Question_title":"Deploying the sagemaker endpoint created as a service",
        "Question_body":"<p>I have trained a credit-fraud data set on AWS Sagemaker and created an endpoint of the model. Suppose I want to provide it as a service to my friend. He has some credit data and wanted to know whether the transaction is fraud or not. He wishes to use my endpoint. How do I share it?<\/p>\n\n<ol>\n<li>Should I share my ARN for endpoint? I don't think its the right way. without a common account he won't be able to use it.<\/li>\n<li>Or is there another way<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1573479066647,
        "Question_score":2,
        "Question_tags":"amazon-web-services|endpoint|amazon-sagemaker",
        "Question_view_count":226,
        "Owner_creation_time":1568318861627,
        "Owner_last_access_time":1663770939230,
        "Owner_location":"Hyderabad, Telangana, India",
        "Owner_reputation":486,
        "Owner_up_votes":28,
        "Owner_down_votes":2,
        "Owner_views":75,
        "Question_last_edit_time":null,
        "Answer_body":"<p>To share your model as an endpoint, you should use lambda and API Gateway to create your API.<\/p>\n\n<ol>\n<li>Create an API gateway that triggers a Lambda with the HTTP POST method;<\/li>\n<li>your lambda should instantiate the SageMaker endpoint, get the requested parameter in the event, call the SageMaker endpoint and return the predicted value. you can also create a DynamoDB to store commonly requested parameters with their answers;<\/li>\n<li>Send the API Gateway Endpoint to your friend.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/qLss4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qLss4.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1573653626700,
        "Answer_score":6.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1573654268900,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58802366",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73163540,
        "Question_title":"Is there any workaround for installing packages ? (deepspeed error)",
        "Question_body":"<p>i was trying to use deepspeed, but when i run the training it shows an error. I have to install mpi4py using pip. However if I try to install mpi4py i get an error.\nFrom that error i found out that to install the package, i must install &quot;libopenmpi-dev &quot; before, using apt. However we dont have the password to sudo. Any workaround to this ?\n(or the only option is to change platforms ?)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":9,
        "Question_creation_time":1659082147263,
        "Question_score":0,
        "Question_tags":"linux|amazon-sagemaker|apt",
        "Question_view_count":73,
        "Owner_creation_time":1653740518870,
        "Owner_last_access_time":1663956849533,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1659169326400,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73163540",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54462105,
        "Question_title":"SageMaker Ground Truth with TensorFlow",
        "Question_body":"<p>I've seen examples of labeling data using SageMaker Ground Truth and then using that data to train off-the-shelf SageMaker models. However, am I able to use this same annotation format with TensorFlow Script Mode? <\/p>\n\n<p>More specifically, I have a tensorflow.keras model I'm training using TF Script Mode, and I'd like to take data labeled with Ground Truth and convert my script from File mode to Pipe mode.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548942784467,
        "Question_score":2,
        "Question_tags":"python|tensorflow|amazon-sagemaker|labeling",
        "Question_view_count":454,
        "Owner_creation_time":1361339272693,
        "Owner_last_access_time":1663965928400,
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I am from Amazon SageMaker Ground Truth team and happy to assist you in your experiment. Just to be clear our understanding, are you running TF model in SageMaker using TF estimator in your own container (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst<\/a>)? <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1549915976767,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54462105",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73111472,
        "Question_title":"stacking cnn output layer with xgboost. Data prep gives OOM error",
        "Question_body":"<p>I have trained a cnn model and I am trying to stack the output layer to an xgboost regressor to reduce mape. I am getting OOM error in Sagemaker training job when I try to include the input data (in npy format) with the cnn output layer and save it as csv - so this can be input to xgboost. When I try to run this in Sagemaker notebook instance the kernel dies. The training input npy file is around 42gb and I have tried these instances : ml.m5d.24xlarge, ml.r5.24xlarge\nHere is my code I am running in notebook:<\/p>\n<p>'''<\/p>\n<pre><code>import numpy as np\nimport tensorflow as tf\nimport boto3\nfrom io import BytesIO\nfrom keras.models import load_model\nfrom keras import backend as K\n\nclient = boto3.client(&quot;s3&quot;)\n\nbucket = &lt;bucket_name&gt;\nkey = '\/path\/cnn_model.h5'\nclient.download_file(bucket, key, 'cnn_model.h5')\n\ncnn_model = load_model(&quot;cnn_model.h5&quot;)\n\ndef read_s3_npy(s3_uri, arg = False):\n    bytes = BytesIO()\n    bytes_.seek(0)\n    parsed_s3 = urlparse(s3_uri)\n    obj = client.get_object(Bucket=parsed_s3.netloc, key = parsed_s3.path[1:])\n    return np.load(BytesIO(obj['Body'].read()), allow_pickle=arg)\n\nx_train_path = &lt;path in s3&gt;+'x_train.npy'\ny_train_path = &lt;path in s3&gt;+'y_train.npy'\nx_train = read_s3_npy(x_train_path)\ny_train = read_s3_npy(y_train_path)\n\nlast_layer_op = K.function([cnn_model.layers[0].input], [cnn_model.layers[-2].output])\n\ntrain_layer = last_layer_op([x_train, 1])[0]\n<\/code><\/pre>\n<p>'''<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1658761728373,
        "Question_score":0,
        "Question_tags":"tensorflow|keras|conv-neural-network|xgboost|amazon-sagemaker",
        "Question_view_count":43,
        "Owner_creation_time":1438153132453,
        "Owner_last_access_time":1659123247360,
        "Owner_location":null,
        "Owner_reputation":57,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73111472",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72203561,
        "Question_title":"How to automatically stop Sagemaker notebook instances if it is idle?",
        "Question_body":"<p>I have been looking for a script to automatically close Sagemaker Notebook Instances that have been forgotten to be closed or that are idle. A few scripts I found don't work very well (eg: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts\/auto-stop-idle\" rel=\"nofollow noreferrer\">link<\/a> , it is only checking if ipynb file is live, Im not using .ipynb, or taking the last updated info which never changes until you shut down or open the instance)\nIs there a resource or script you can recommend?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1652282230790,
        "Question_score":0,
        "Question_tags":"amazon-web-services|instance|amazon-sagemaker|lifecycle",
        "Question_view_count":399,
        "Owner_creation_time":1646703340960,
        "Owner_last_access_time":1663771451647,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72203561",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59987884,
        "Question_title":"ImportError: cannot import name np_utils in AWS Sagemaker",
        "Question_body":"<p>I run my .ipynb file in AWS Sagemaker. <\/p>\n\n<p>Unlike Colab, we need to install the dependencies separately in Sagemaker.<\/p>\n\n<p>While pip installing tensorflow,I got error like <strong>ImportError: cannot import name np_utils<\/strong>.<\/p>\n\n<p>After that I installed that too by <strong>!pip install np_utils<\/strong>\nThen also I face similar error.<\/p>\n\n<p>If this is in local system, it may be due to system configuration. Since its in Sagemaker,I am not sure about how to proceed futher.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1580393356327,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":475,
        "Owner_creation_time":1550065461003,
        "Owner_last_access_time":1583913442813,
        "Owner_location":"Chennai, Tamil Nadu, India",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1580400719820,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59987884",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60684191,
        "Question_title":"Cannot upload data from pandas data-frame to AWS athena table due to 'import package error'",
        "Question_body":"<p>When I am trying to import the awswrangler package in sagemaker I am getting the below error<\/p>\n\n<pre><code>import awswrangler as aws\n<\/code><\/pre>\n\n<p><strong>Error<\/strong><\/p>\n\n<pre><code>ImportError                               Traceback (most recent call last)\n&lt;ipython-input-9-cc67bb4c1dd7&gt; in &lt;module&gt;\n----&gt; 1 import awswrangler as aws\n\n\/opt\/app-root\/lib\/python3.6\/site-packages\/awswrangler\/__init__.py in &lt;module&gt;\n     15 from awswrangler.emr import EMR  # noqa\n     16 from awswrangler.glue import Glue  # noqa\n---&gt; 17 from awswrangler.pandas import Pandas  # noqa\n     18 from awswrangler.redshift import Redshift  # noqa\n     19 from awswrangler.s3 import S3  # noqa\n\n\/opt\/app-root\/lib\/python3.6\/site-packages\/awswrangler\/pandas.py in &lt;module&gt;\n     17 from boto3 import client  # type: ignore\n     18 from botocore.exceptions import ClientError, HTTPClientError  # type: ignore\n---&gt; 19 from pandas.io.common import infer_compression  # type: ignore\n     20 from pyarrow import parquet as pq  # type: ignore\n     21 from s3fs import S3FileSystem  # type: ignore\n\nImportError: cannot import name 'infer_compression'\n<\/code><\/pre>\n\n<p>Hoe to fix this ?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1584198784503,
        "Question_score":0,
        "Question_tags":"python|pandas|amazon-web-services|amazon-athena|amazon-sagemaker",
        "Question_view_count":1470,
        "Owner_creation_time":1509871153690,
        "Owner_last_access_time":1663866159260,
        "Owner_location":null,
        "Owner_reputation":731,
        "Owner_up_votes":37,
        "Owner_down_votes":3,
        "Owner_views":185,
        "Question_last_edit_time":1584236556403,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60684191",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57483440,
        "Question_title":"Flask and Gunicorn with Additional Threads",
        "Question_body":"<p>I'm trying to build a Flask app with Gunicorn to serve concurrent requests. For what it's worth, the context is a bring-your-own-container Sagemaker application.<\/p>\n\n<p>The issue is that I need the application to periodically check for updates. So I thought to implement a thread for this. Here is a minimal example of some Flask code with an update thread. <\/p>\n\n<p>server.py<\/p>\n\n<pre><code>from flask import Flask\nimport time, threading\n\napp = Flask(__name__)\n\nmessage = True\n\ndef update():\n  while True:\n    message = not message\n    time.sleep(10)\n\n@app.route(\"\/\")\ndef hello():\n  global message\n  return message\n\nupdate_thread = threading.Thread(target=update)\n\nif __name__ == \"__main__\":\n  update_thread.start()\n  app.run()\n  update_thread.join()\n<\/code><\/pre>\n\n<p>I then launch with gunicorn:<\/p>\n\n<p><code>gunicorn -k gevent -b unix:\/tmp\/gunicorn.sock -w 4 server:app<\/code><\/p>\n\n<p>Perhaps unsurprisingly the update thread doesn't start since the <code>__main__<\/code> section is never executed. <\/p>\n\n<blockquote>\n  <p><strong>Question<\/strong>: <em>How can one use an update thread (or similar construct) in a Flask app with Gunicorn?<\/em><\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1565720684053,
        "Question_score":0,
        "Question_tags":"python|multithreading|flask|gunicorn|amazon-sagemaker",
        "Question_view_count":858,
        "Owner_creation_time":1413222980680,
        "Owner_last_access_time":1655488449107,
        "Owner_location":"Richland, WA",
        "Owner_reputation":493,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":59,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It looks like this can be accomplished using <code>Flask-APScheduler<\/code> as follows:<\/p>\n\n<p><code>pip install flask_apscheduler<\/code><\/p>\n\n<p>server.py<\/p>\n\n<pre><code>from flask import Flask\nfrom apscheduler.schedulers.background import BackgroundScheduler\nimport atexit\n\napp = Flask(__name__)\n\nmessage = True\n\ndef update():\n  global message\n  message = not message\n\nscheduler = BackgroundScheduler()\nscheduler.add_job(func=update,trigger=\"interval\",seconds=10)\nscheduler.start()\n# shut down the scheduler when exiting the app\natexit.register(scheduler.shutdown)\n\n@app.route(\"\/\")\ndef hello():\n  global message\n  return message\n\nif __name__ == \"__main__\":\n  app.run()\n<\/code><\/pre>\n\n<p>Then launching as usual with \n<code>gunicorn -k gevent -b unix:\/tmp\/gunicorn.sock -w 4 server:app<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1565727707500,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57483440",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64215998,
        "Question_title":"Why is Crowd HTML breaking this image?",
        "Question_body":"<p>I'm using Crowd HTML Elements to perform bounding box annotation, but when I attempt to load some of my images, I get this error in the dev tools console:<\/p>\n<pre><code>crowd-html-elements.js:1 window.onError received an event without an error:  {event: ErrorEvent}\n(anonymous) @ crowd-html-elements.js:1\nerror (async)\ne @ crowd-html-elements.js:1\ne @ crowd-html-elements.js:1\n.\/src\/crowd-html-elements-loader.ts @ crowd-html-elements.js:1\ns @ crowd-html-elements.js:1\n(anonymous) @ crowd-html-elements.js:1\n(anonymous) @ crowd-html-elements.js:1\ncrowd-html-elements-without-ce-polyfill.js:6282 window.onError received an event without an error:  {event: ErrorEvent}\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:6282\nerror (async)\ne @ crowd-html-elements-without-ce-polyfill.js:6282\ne @ crowd-html-elements-without-ce-polyfill.js:6282\n.\/src\/index.ts @ crowd-html-elements-without-ce-polyfill.js:6282\nr @ crowd-html-elements-without-ce-polyfill.js:1\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:1\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:1\ncrowd-html-elements-without-ce-polyfill.js:6282 Uncaught Error: Unexpected image dimensions during normalization\n    at Function.normalizeHeight (crowd-html-elements-without-ce-polyfill.js:6282)\n    at Function.normalizeDimensions (crowd-html-elements-without-ce-polyfill.js:6282)\n    at new a (crowd-html-elements-without-ce-polyfill.js:6282)\n    at ie.handleTargetImageLoaded (crowd-html-elements-without-ce-polyfill.js:6282)\n    at Image.r.onload (crowd-html-elements-without-ce-polyfill.js:6282)\nnormalizeHeight @ crowd-html-elements-without-ce-polyfill.js:6282\nnormalizeDimensions @ crowd-html-elements-without-ce-polyfill.js:6282\na @ crowd-html-elements-without-ce-polyfill.js:6282\nhandleTargetImageLoaded @ crowd-html-elements-without-ce-polyfill.js:6282\nr.onload @ crowd-html-elements-without-ce-polyfill.js:6282\nload (async)\nsetBackgroundImage @ crowd-html-elements-without-ce-polyfill.js:6282\nrenderImageSrcChange @ crowd-html-elements-without-ce-polyfill.js:6282\nshouldComponentUpdate @ crowd-html-elements-without-ce-polyfill.js:6282\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nG @ crowd-html-elements-without-ce-polyfill.js:6278\nw @ crowd-html-elements-without-ce-polyfill.js:6278\nS @ crowd-html-elements-without-ce-polyfill.js:6278\ne.reactMount @ crowd-html-elements-without-ce-polyfill.js:3\ne.updateRegion @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\ne.reactBatchUpdate @ crowd-html-elements-without-ce-polyfill.js:3\ni @ crowd-html-elements-without-ce-polyfill.js:3\nf.componentDidUpdate @ crowd-html-elements-without-ce-polyfill.js:3\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nG @ crowd-html-elements-without-ce-polyfill.js:6278\nw @ crowd-html-elements-without-ce-polyfill.js:6278\n_renderReactComponent @ crowd-html-elements-without-ce-polyfill.js:6282\n_updateReactComponent @ crowd-html-elements-without-ce-polyfill.js:6282\nY @ crowd-html-elements-without-ce-polyfill.js:5984\nC @ crowd-html-elements-without-ce-polyfill.js:5984\nk @ crowd-html-elements-without-ce-polyfill.js:5984\n_propertiesChanged @ crowd-html-elements-without-ce-polyfill.js:5984\n_flushProperties @ crowd-html-elements-without-ce-polyfill.js:5954\n_flushProperties @ crowd-html-elements-without-ce-polyfill.js:5984\n_invalidateProperties @ crowd-html-elements-without-ce-polyfill.js:5984\n_setProperty @ crowd-html-elements-without-ce-polyfill.js:5984\nObject.defineProperty.set @ crowd-html-elements-without-ce-polyfill.js:5954\n(anonymous) @ labeling.html:199\nasync function (async)\n(anonymous) @ labeling.html:198\nPromise.then (async)\n(anonymous) @ labeling.html:196\n<\/code><\/pre>\n<p>The <strong>Unexpected image dimensions during normalization<\/strong> portion seems like the issue, but I've found nothing with regard to troubleshooting.  Can someone explain what expected image dimensions are and why some are failing?<\/p>\n<p>Here's a snippet of the code that's throwing the error.<\/p>\n<pre><code>            static normalizeHeight(e) {\n                if (e.height === e.naturalHeight)\n                    return e.height;\n                if (e.height === e.naturalWidth)\n                    return e.height;\n                if (Math.abs(e.height - e.naturalHeight) &lt; 2)\n                    return e.naturalHeight;\n                if (Math.abs(e.height - e.naturalWidth) &lt; 2)\n                    return e.naturalWidth;\n                throw new Error(&quot;Unexpected image dimensions during normalization&quot;)\n            }\n            static normalizeWidth(e) {\n                if (e.width === e.naturalWidth)\n                    return e.width;\n                if (e.width === e.naturalHeight)\n                    return e.width;\n                if (Math.abs(e.width - e.naturalWidth) &lt; 2)\n                    return e.naturalWidth;\n                if (Math.abs(e.width - e.naturalHeight) &lt; 2)\n                    return e.naturalHeight;\n                throw new Error(&quot;Unexpected image dimensions during normalization&quot;)\n<\/code><\/pre>\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1601930941820,
        "Question_score":1,
        "Question_tags":"javascript|amazon-sagemaker|mechanicalturk",
        "Question_view_count":168,
        "Owner_creation_time":1483444144907,
        "Owner_last_access_time":1643982821673,
        "Owner_location":"Hoth",
        "Owner_reputation":312,
        "Owner_up_votes":28,
        "Owner_down_votes":1,
        "Owner_views":65,
        "Question_last_edit_time":1602868269100,
        "Answer_body":"<p>The issue turned out to be related to the css styling that was being applied to the canvas portion of my site that was loading the labeling tools.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1603367517400,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64215998",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69915819,
        "Question_title":"How to Organize Training Data for AWS Sagemaker",
        "Question_body":"<p>I am training a model and the training data uses images for both the source and the label.<\/p>\n<p>For example, <code>image1.jpg =&gt; label_image.jpg<\/code><\/p>\n<p>The images and their corresponding &quot;label&quot; are in different directories.<\/p>\n<p>So I have images stored like <code>s3:\/\/bucket\/v1\/imgs<\/code> and their labels stored like <code>s3:\/\/bucket\/v1\/lbls<\/code>.<\/p>\n<h2>Question<\/h2>\n<p>How do I go about passing this data into an estimator in sagemaker?<\/p>\n<p>I've seen numerous examples, none of which have the data stored in a similar fashion. I've also tried to find the way that sagemaker expects the data to be organized but haven't had much luck.<\/p>\n<p>Any help would be greatly appreaciated.<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1636557493463,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker|training-data",
        "Question_view_count":36,
        "Owner_creation_time":1478552095660,
        "Owner_last_access_time":1662143133913,
        "Owner_location":null,
        "Owner_reputation":1149,
        "Owner_up_votes":132,
        "Owner_down_votes":10,
        "Owner_views":150,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69915819",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56728230,
        "Question_title":"AWS sagemaker RandomCutForest (RCF) vs scikit lean RandomForest (RF)?",
        "Question_body":"<p>Is there a difference between the two, or are they different names for the same algorithm?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1561329917317,
        "Question_score":0,
        "Question_tags":"classification|random-forest|decision-tree|amazon-sagemaker",
        "Question_view_count":244,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":null,
        "Answer_body":"<p>RandomCutForest (RCF) is an unsupervised method primarily used for anomaly detection, while RandomForest (RF) is a supervised method that can be used for regression or classification. <\/p>\n\n<p>For RCF, see documentation (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/randomcutforest.html\" rel=\"nofollow noreferrer\">here<\/a>) and notebook example (<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"nofollow noreferrer\">here<\/a>)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1561385097929,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56728230",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55829940,
        "Question_title":"How to rename a SageMaker notebook instance?",
        "Question_body":"<p>On Amazon SageMaker, it's possible to edit most properties of a notebook instance when the instance is not active, but it does not seem possible to change its name.<\/p>\n\n<p>Is there any way to rename an existing SageMaker notebook instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1556108497957,
        "Question_score":6,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1311,
        "Owner_creation_time":1283441031493,
        "Owner_last_access_time":1663854500710,
        "Owner_location":"Lausanne, Switzerland",
        "Owner_reputation":4841,
        "Owner_up_votes":220,
        "Owner_down_votes":9,
        "Owner_views":261,
        "Question_last_edit_time":1556115031880,
        "Answer_body":"<p>Thank you for using Amazon SageMaker. <\/p>\n\n<p>SageMaker Notebook Instance's name cannot be edited. <\/p>\n\n<p>Thanks,<br>\nNeelam<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1556304752800,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55829940",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48398509,
        "Question_title":"How to Invoke AWS Sagemaker API with c# .NET?",
        "Question_body":"<p>I have trained and deployed a model in AWS Sagemaker, Now I am trying to invoke the endpoint with client as c# .NET.\nIn the below code, it seems, I am getting errors because of invalid value of Body parameter.<\/p>\n\n<pre><code>AmazonSageMakerRuntimeClient aawsClient = new AmazonSageMakerRuntimeClient();\nAmazon.SageMakerRuntime.Model.InvokeEndpointRequest request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest();\nrequest.EndpointName = \"sagemaker-mxnet-py2-cpu-2018-01-23-07-04-11\";\nrequest.Accept = \"text\/csv\";            \nrequest.ContentType = \"text\/csv\";\n\/\/request.Body = compressedMemStream;\nAmazon.SageMakerRuntime.Model.InvokeEndpointResponse resposnse = aawsClient.InvokeEndpoint(request);\n<\/code><\/pre>\n\n<p>I have tried by passing a MemoryStream which written with a '.gz' file or with '.jpeg' file. By executing InvokeEndPoint(), Getting error as: \"<strong>unable to evaluate payload provided<\/strong>\" <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1516700568607,
        "Question_score":0,
        "Question_tags":"c#|asp.net|amazon-web-services|aws-sdk|amazon-sagemaker",
        "Question_view_count":743,
        "Owner_creation_time":1337759214690,
        "Owner_last_access_time":1532954663377,
        "Owner_location":"Pune India",
        "Owner_reputation":1036,
        "Owner_up_votes":34,
        "Owner_down_votes":1,
        "Owner_views":124,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48398509",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69310670,
        "Question_title":"Amazon SageMaker ml.p2.xlarge notebook instance speed issues",
        "Question_body":"<p>I am running AWS Sagemaker and created a notebook instance ml.p2.xlarge machine with elastic inference set as none. I am encountering speed issues during training. The training process is very very slow. I have also restarted the kernel multiple time but it doesn't make any difference. What could be the reason?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1632465995047,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":212,
        "Owner_creation_time":1620191753590,
        "Owner_last_access_time":1661965917280,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69310670",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50049928,
        "Question_title":"Sagemaker image classification: Best way to perform inference on many images in S3?",
        "Question_body":"<p>I trained a model with the built-in RESnet18 docker image, and now I want to deploy the model to an endpoint and classify ~ 1 million images. I have all my training, validation, and test images stored on S3 in RecordIO format (converted with <a href=\"https:\/\/mxnet.incubator.apache.org\/tutorials\/basic\/data.html?highlight=im2rec\" rel=\"nofollow noreferrer\">im2rec.py<\/a>). According to the <a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>The Amazon SageMaker Image Classification algorithm supports both RecordIO (application\/x-recordio) and image (application\/x-image) content types for training. The algorithm supports only\u00a0application\/x-image\u00a0for inference.<\/p>\n<\/blockquote>\n\n<p>So I cannot perform inference on my training data in RecordIO format. To overcome this I copied all the raw .jpg images (~ 2GB) onto my Sagemaker Jupyter Notebook instance and performed inference one at a time in the following way:<\/p>\n\n<pre><code>img_list = os.listdir('temp_data') # list of all ~1,000,000 images\n\nfor im in img_list:\n    with open('temp_data\/'+im, 'rb') as f:\n        payload = f.read()\n        payload = bytearray(payload)\n    response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n                                       ContentType='application\/x-image', \n                                       Body=payload)\n\n    etc...\n<\/code><\/pre>\n\n<p>Needless to say, transferring all the data onto my Notebook instance took a long time and I would prefer not having to do that before running inference. Why does the SageMaker Image Classification not support RecordIO for inference? And more importantly, what is the best way to run inference on many images without having to move them from S3?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1524767975170,
        "Question_score":0,
        "Question_tags":"amazon-s3|resnet|amazon-sagemaker",
        "Question_view_count":2600,
        "Owner_creation_time":1474520506390,
        "Owner_last_access_time":1625722561310,
        "Owner_location":null,
        "Owner_reputation":832,
        "Owner_up_votes":41,
        "Owner_down_votes":0,
        "Owner_views":48,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The RecordIO format is designed to pack a large number of images into a single file, so I don't think it would work well for predicting single images.<\/p>\n\n<p>When it comes to prediction, you definitely don't have to copy images to a notebook instance or to S3. You just have to load them from anywhere and inline them in your prediction requests.<\/p>\n\n<p><strong>If you want HTTP-based prediction, here are your options:<\/strong><\/p>\n\n<p>1) Use the SageMaker SDK Predictor.predict() API on any machine (as long as it has proper AWS credentials) <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a><\/p>\n\n<p>2) Use the AWS Python SDK (aka boto3) API invoke_endpoint() on any machine (as long as it has proper AWS credentials)<\/p>\n\n<p>You can even build a simple service to perform pre-processing or post-processing with Lambda. Here's an example: <a href=\"https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033<\/a><\/p>\n\n<p><strong>If you want batch prediction:<\/strong>\n the simplest way is to retrieve the trained model from SageMaker, write a few lines of ad-hoc MXNet code to load it and run all your predictions. Here's an example: <a href=\"https:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html\" rel=\"nofollow noreferrer\">https:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html<\/a><\/p>\n\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1531369355823,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50049928",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63361229,
        "Question_title":"How do you write lifecycle configurations for SageMaker on windows?",
        "Question_body":"<p>I'm trying to set up a startup lifecycle configuration for a SageMaker sketchbook (which just ends up being a .sh file), and it seems like, regardless of what I do, my notebooks timeout on startup. I simplified everything as much as possible, to the point of commenting out all but <code>#!\/bin\/bash<\/code>, and I still get a timeout. Checking cloudwatch this shows up in the log:<\/p>\n<pre><code>\/bin\/bash: \/tmp\/OnStart_2020-08-11-07-01jgfhhkwa: \/bin\/bash^M: bad interpreter: No such file or directory\n<\/code><\/pre>\n<p>through testing, I also found that if I add a carriage return before <code>#!\/bin\/bash<\/code> I get this in the log:<\/p>\n<pre><code>\/tmp\/OnStart_2020-08-11-06-444y3fobzp: line 1: $'\\r': command not found\n<\/code><\/pre>\n<p>based on <a href=\"https:\/\/askubuntu.com\/questions\/966488\/how-do-i-fix-r-command-not-found-errors-running-bash-scripts-in-wsl\">this on the \\r error<\/a>, and <a href=\"https:\/\/stackoverflow.com\/questions\/14219092\/bash-script-and-bin-bashm-bad-interpreter-no-such-file-or-directory\">this on the ^M error<\/a>, this seems to be an incompatibility between windows and unix formatted text. However, I'm editing the lifecycle configuration through aws on my windows machine:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/A5oiU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/A5oiU.png\" alt=\"screenshot of editing the lifecycle config\" \/><\/a><\/p>\n<p>is there some way that I can edit this field on my windows machine through AWS, but it be properly written in unix on the other end?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597159003890,
        "Question_score":0,
        "Question_tags":"linux|windows|bash|amazon-web-services|amazon-sagemaker",
        "Question_view_count":608,
        "Owner_creation_time":1545360696800,
        "Owner_last_access_time":1664070875743,
        "Owner_location":"Earth",
        "Owner_reputation":1011,
        "Owner_up_votes":218,
        "Owner_down_votes":5,
        "Owner_views":93,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This is, indeed, to do with special character representation in different os' based on <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/issues\/8\" rel=\"nofollow noreferrer\">this<\/a> you can use notepad++ to easily convert the dos representation a unix representation, then just &quot;paste as plain text&quot;, and it works fine<\/p>\n<ul>\n<li>copy to notepad++ view<\/li>\n<li>show symbol<\/li>\n<li>show all symbols<\/li>\n<li>replace &quot;\/r&quot; with nothing CRLF should become LF which is valid in unix<\/li>\n<li>copy and paste as plain text<\/li>\n<\/ul>\n<p>Doing this fixed the problem<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1597164941183,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63361229",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72733847,
        "Question_title":"How do you resolve an \"Access Denied\" error when invoking `image_uris.retrieve()` in AWS Sagemaker JumpStart?",
        "Question_body":"<p>I am working in a SageMaker environment that is locked down. For example, my user account is prevented from creating S3 buckets. But, I can successfully run vanilla ML training jobs by passing in <code>role=get_execution_role<\/code> to an instance of the Estimator class when using an out-of-the-box algorithm such as XGBoost.<\/p>\n<p>Now, I'm trying to use an algorithm (LightBGM) that is only available via the JumpStart feature in SageMaker, but I can't get it to work. When I try to retrieve an image URI via <code>image_uris.retrieve()<\/code>, it returns the following error:<br \/>\n<code>ClientError: An error occurred (AccessDenied) when calling the GetObject operation: Access Denied<\/code>.<\/p>\n<p>This makes some sense to me if my user permissions are being used when creating an object. But what I want to do is specify another role - like the one returned from get_execution_role - to perform these tasks.<\/p>\n<p>Is that possible? Is there another work-around available? How can I see which role is being used?<\/p>\n<p>Thanks,<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1656002777660,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker|amazon-machine-learning",
        "Question_view_count":47,
        "Owner_creation_time":1316920614520,
        "Owner_last_access_time":1663874836913,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72733847",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49579526,
        "Question_title":"How to save parquet in S3 from AWS SageMaker?",
        "Question_body":"<p>I would like to save a Spark DataFrame from AWS SageMaker to S3. In Notebook, I ran<\/p>\n\n<p><code>myDF.write.mode('overwrite').parquet(\"s3a:\/\/my-bucket\/dir\/dir2\/\")<\/code><\/p>\n\n<p>I get<\/p>\n\n<blockquote>\n  <p>Py4JJavaError: An error occurred while calling o326.parquet. :\n  java.lang.RuntimeException: java.lang.ClassNotFoundException: Class\n  org.apache.hadoop.fs.s3native.NativeS3FileSystem not found    at\n  org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)\n    at\n  org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)\n    at\n  org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)   at\n  org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)     at\n  org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)    at\n  org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:394)\n    at\n  org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:471)\n    at\n  org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:50)\n    at\n  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n    at\n  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n    at\n  org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n    at\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n    at\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n    at\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n    at\n  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n    at\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n    at\n  org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n    at\n  org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n    at\n  org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n    at\n  org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)\n    at\n  org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n    at\n  org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n    at\n  org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:508)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at\n  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at\n  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)     at\n  py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)  at\n  py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)    at\n  py4j.Gateway.invoke(Gateway.java:280)     at\n  py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)   at\n  py4j.GatewayConnection.run(GatewayConnection.java:214)    at\n  java.lang.Thread.run(Thread.java:745) Caused by:\n  java.lang.ClassNotFoundException: Class\n  org.apache.hadoop.fs.s3native.NativeS3FileSystem not found    at\n  org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101)\n    at\n  org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)<\/p>\n<\/blockquote>\n\n<p>How should I do it correctly in Notebook? Many thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1522434506127,
        "Question_score":0,
        "Question_tags":"amazon-web-services|apache-spark|hadoop|amazon-s3|amazon-sagemaker",
        "Question_view_count":1986,
        "Owner_creation_time":1420001102893,
        "Owner_last_access_time":1638226427930,
        "Owner_location":null,
        "Owner_reputation":173,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49579526",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53408927,
        "Question_title":"How to pass a bigger .csv files to amazon sagemaker for predictions using batch transform jobs",
        "Question_body":"<p>I created a custom model and deployed it on sagemaker. I am invoking the endpoint using batch transform jobs. It works if the input file is small, i.e, number of rows in the csv file is less. If I upload a file with around 200000 rows, I am getting this error in the cloudwatch logs.<\/p>\n\n<pre><code>2018-11-21 09:11:52.666476: W external\/org_tensorflow\/tensorflow\/core\/framework\/allocator.cc:113]\nAllocation of 2878368000 exceeds 10% of system memory.\n2018-11-21 09:11:53.166493: W external\/org_tensorflow\/tensorflow\/core\/framework\/allocator.cc:113] \nAllocation of 2878368000 exceeds 10% of system memory.\n[2018-11-21 09:12:02,544] ERROR in serving: &lt;_Rendezvous of RPC that \nterminated with:\n#011status = StatusCode.DEADLINE_EXCEEDED\n#011details = \"Deadline Exceeded\"\n#011debug_error_string = \"\n{\n\"created\": \"@1542791522.543282048\",\n\"description\": \"Error received from peer\",\n\"file\": \"src\/core\/lib\/surface\/call.cc\",\n\"file_line\": 1017,\n\"grpc_message\": \"Deadline Exceeded\",\n\"grpc_status\": 4\n}\n\"\n<\/code><\/pre>\n\n<p>Any ideas what might be going wrong. This is the transform function which I am using to create the transform job.<\/p>\n\n<pre><code>transformer =sagemaker.transformer.Transformer(\nbase_transform_job_name='Batch-Transform',\nmodel_name='sagemaker-tensorflow-2018-11-21-07-58-15-887',\ninstance_count=1,\ninstance_type='ml.m4.xlarge',\noutput_path='s3:\/\/2-n2m-sagemaker-json-output\/out_files\/'\n\n)\ninput_location = 's3:\/\/1-n2m-n2g-csv-input\/smal_sagemaker_sample.csv'\ntransformer.transform(input_location, content_type='text\/csv', split_type='Line')\n<\/code><\/pre>\n\n<p>The .csv file contains 2 columns for first and last name of customer, which I am then preprocessing it in the sagemaker itself using input_fn().<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1542792620897,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":1941,
        "Owner_creation_time":1444454434270,
        "Owner_last_access_time":1652071645660,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":140,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Question_last_edit_time":1542799396316,
        "Answer_body":"<p>The error looks to be coming from a GRPC client closing the connection before the server is able to respond. (There looks to be an existing feature request for the sagemaker tensorflow container on <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/issues\/46\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/issues\/46<\/a> to make this timeout configurable)<\/p>\n\n<p>You could try out a few things with the sagemaker Transformer to limit the size of each individual request so that it fits within the timeout:<\/p>\n\n<ul>\n<li>Set a <code>max_payload<\/code> to a smaller value, say 2-3 MB (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB\" rel=\"nofollow noreferrer\">the default is 6 MB<\/a>)<\/li>\n<li>If your instance metrics indicate it has compute \/ memory resources to spare, try <code>max_concurrent_transforms<\/code> > 1 to make use of multiple workers<\/li>\n<li>Split up your csv file into multiple input files. With a bigger dataset, you could also increase the instance count to fan out processing<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1544503147007,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53408927",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56980928,
        "Question_title":"Training & Deploying SageMaker ML Models using AWS Lambda (NodeJS)",
        "Question_body":"<p>I am using AWS Lambda (NodeJS) for creating a sagemaker training job and deploy it using the Sagemaker Javascript SDK.<\/p>\n\n<p>I am following the below AWS JavaScript SDK docs<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/AWSJavaScriptSDK\/latest\/AWS\/SageMaker.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSJavaScriptSDK\/latest\/AWS\/SageMaker.html<\/a><\/p>\n\n<p>I am using the below script for creating the Training job.<\/p>\n\n<pre><code>Create Training Job:\n=====================\n\n    let TrainingJobName = 'Training-' + curr_date_time\n    let TrainingImage   = 'XXXXXX.dkr.ecr.us-east-1.amazonaws.com\/xxxx:latest'\n    let S3Uri           = 's3:\/\/xxx.xxxx.sagemaker\/csv'\n\n    console.log(`TrainingJobName: ${TrainingJobName}`);\n\n    let params = {\n        AlgorithmSpecification: { \/* required *\/\n            TrainingInputMode: 'File', \/* required *\/\n            TrainingImage: TrainingImage\n        },\n        OutputDataConfig: { \/* required *\/\n            S3OutputPath: 's3:\/\/xxx.xxxx.sagemaker\/xxxx\/output', \/* required *\/\n        },\n        ResourceConfig: { \/* required *\/\n            InstanceCount: 1, \/* required *\/\n            InstanceType: 'ml.m4.xlarge', \/* required *\/\n            VolumeSizeInGB: 1, \/* required *\/\n        },\n        RoleArn: 'arn:aws:iam::xxxxx:role\/service-role\/AmazonSageMaker-ExecutionRole-xxxx', \/* required *\/\n        StoppingCondition: { \/* required *\/\n            MaxRuntimeInSeconds: 86400\n        },\n        TrainingJobName: TrainingJobName, \/* required *\/\n        InputDataConfig: [\n            {\n                ChannelName: 'training', \/* required *\/\n                DataSource: { \/* required *\/\n                    S3DataSource: {\n                        S3DataType: 'S3Prefix', \/* required *\/\n                        S3Uri: S3Uri, \/* required *\/\n                        S3DataDistributionType: 'FullyReplicated'\n                    }\n                },\n                CompressionType: null,\n                ContentType: '',\n                RecordWrapperType: null,\n            }\n        ]\n    };\n\n    return await sagemaker.createTrainingJob(params).promise();\n<\/code><\/pre>\n\n<p>After the training job is created, i query the job status using the sagemaker describeTrainingJob function.\nI get the status as \"InProgress\"<\/p>\n\n<p>After that I call the sagemaker waitFor function to wait for the completion of the training job using the below method:<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/AWSJavaScriptSDK\/latest\/AWS\/SageMaker.html#trainingJobCompletedOrStopped-waiter\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSJavaScriptSDK\/latest\/AWS\/SageMaker.html#trainingJobCompletedOrStopped-waiter<\/a><\/p>\n\n<pre><code>let waitFor_result = await sagemaker.waitFor('trainingJobCompletedOrStopped', {TrainingJobName: training_job_name}).promise();\nconsole.log(`waitFor_result : ${JSON.stringify(waitFor_result)}`);\n<\/code><\/pre>\n\n<p>I find the sagemaker waitFor creates the second training job before the first training job is completed, and it goes on creating subsequent training jobs with the same job name.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/J7H4c.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/J7H4c.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I think this is due to the StoppingCondition parameter (MaxRuntimeInSeconds:86400) in the createTrainingJob function.<\/p>\n\n<p>I want to know if there is any solution which creates a single training job and return the results after the trainining job is completed ?<\/p>\n\n<p>==========================================================\nUpdate:<\/p>\n\n<p>I am following the \"Scheduling the training of a SageMaker model with a Lambda function\" <a href=\"https:\/\/www.youtube.com\/watch?v=FJaykbAtGTM\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=FJaykbAtGTM<\/a>.<\/p>\n\n<p>I am able to create a training job if i am using the below code in my lambda function.<\/p>\n\n<pre><code>let training_job_result = await start_model_training();\nconsole.log(`Sagemaker training result : ${JSON.stringify(training_job_result)}`);\n\nlet training_job_arn = training_job_result[\"TrainingJobArn\"];\nlet training_job_name = training_job_arn.split(\"\/\")[1];\n\n\nlet desc_training_job = await sagemaker.describeTrainingJob({TrainingJobName: training_job_name}).promise();\nlet desc_status = desc_training_job[\"TrainingJobStatus\"];\nconsole.log(`Training job desc_status 1 : ${JSON.stringify(desc_status)}`);\n<\/code><\/pre>\n\n<p>But I need to wait till the training job is completed and invoke the sagemaker deploy method for creating\/updating the endpoint.<\/p>\n\n<p>If I use the below code then it keeps on creating multiple training jobs and the lambda function never terminates.<\/p>\n\n<pre><code>let waitFor_result = await sagemaker.waitFor('trainingJobCompletedOrStopped', {TrainingJobName: training_job_name}).promise();\nconsole.log(`waitFor_result : ${JSON.stringify(waitFor_result)}`);\n\n\ndesc_training_job = await sagemaker.describeTrainingJob({TrainingJobName: training_job_name}).promise();\ndesc_status = desc_training_job[\"TrainingJobStatus\"];\nconsole.log(`Training job desc_status 2 : ${JSON.stringify(desc_status)}`);\n<\/code><\/pre>\n\n<p>I want to deploy\/update the endpoint once the training is completed.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1562812694627,
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker|aws-sdk-js",
        "Question_view_count":1288,
        "Owner_creation_time":1241005356853,
        "Owner_last_access_time":1663511013130,
        "Owner_location":"Kuala Lumpur, Malaysia",
        "Owner_reputation":15794,
        "Owner_up_votes":1343,
        "Owner_down_votes":26,
        "Owner_views":1032,
        "Question_last_edit_time":1562909793607,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56980928",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69962965,
        "Question_title":"How to get an AWS Feature Store feature group into the ACTIVE state?",
        "Question_body":"<p>I am trying to ingest some rows into a Feature Store on AWS using:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>feature_group.ingest(data_frame=df, max_workers=8, wait=True)\n<\/code><\/pre>\n<p>but I am getting the following error:<\/p>\n<blockquote>\n<p>Failed to ingest row 1: An error occurred (ValidationError) when\ncalling the PutRecord operation: Validation Error: FeatureGroup\n[feature-group] is not in ACTIVE state.<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636892751657,
        "Question_score":2,
        "Question_tags":"amazon-web-services|aws-glue|amazon-sagemaker|data-ingestion|aws-feature-store",
        "Question_view_count":383,
        "Owner_creation_time":1362914550047,
        "Owner_last_access_time":1661844098897,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":2791,
        "Owner_up_votes":163,
        "Owner_down_votes":13,
        "Owner_views":174,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It turns out the status of a feature group after its creation is <code>Created<\/code> but before you can ingest any rows you need to simply wait until it's <code>Active<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>while status != 'Created':\n        try:\n            status = feature_group.describe()['OfflineStoreStatus']['Status']\n        except:\n            pass\n        print('Offline store status: {}'.format(status))    \n        sleep(15)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1636979984023,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1646290913523,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69962965",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73144411,
        "Question_title":"How to make SageMaker XGBoost Hyperparameter tuning work in script mode",
        "Question_body":"<p>I'm following up after this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone_dist_script_mode.ipynb\" rel=\"nofollow noreferrer\">tutorial<\/a> and wondering if is it possible to combine the script mode with Hyperparameter tuning. If I'm trying to do so, the HPT.fit() runs my script again and again (the main() function and then _xgb_train()) but I don't know how to pass the hyperparameter the algorithm chose for me to the train function.\nAny idea?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1658954687147,
        "Question_score":0,
        "Question_tags":"xgboost|amazon-sagemaker",
        "Question_view_count":50,
        "Owner_creation_time":1540098833357,
        "Owner_last_access_time":1663969941437,
        "Owner_location":"israel",
        "Owner_reputation":455,
        "Owner_up_votes":21,
        "Owner_down_votes":0,
        "Owner_views":59,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73144411",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58879596,
        "Question_title":"Unable to call XG-Boost endpoint created in sagemaker using AWS-Lambda",
        "Question_body":"<p>I trained an xgboost model on AWS-Sagemaker and created an endpoint. Now I want to call the endpoint using AWS Lambda and AWS API. I created an lambda function and added the below mentioned code for my xgboost model. When I try to test it, the function is throwing a ParamValidation error. Here is my code<\/p>\n\n<pre><code>import json\nimport os\nimport csv\nimport io\nimport boto3\nendpointname =os.environ['endpointname'] #name of the endpoint I created in sagemaker\nruntime = boto3.client('runtime.sagemaker')\ndef lambda_handler(event, context):\n    print(\"Recieved Event: \"+json.dumps(event,indent=2))\n    data=json.loads(json.dumps(event))\n    print(data)\n    response = runtime.invoke_endpoint(EndpointName=endpointname,ContentType='text\/csv',Body=data)\n    print(response)\n    result = json.loads(response['Body'].read().decode())\n    print(int(float(result))) #sagemaker xgb returns bytes type for the test case\n<\/code><\/pre>\n\n<p>The test event I created is dict type. The function is throwing  <code>Invalid type for parameter Body, value: {'Time':'7'}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object<\/code>\nIt means I should pass either byte or bytearray instead of dict type into my event. But when I read this <a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/python-programming-model-handler-types.html\" rel=\"nofollow noreferrer\">AWS Lambda doc<\/a> It says that my event type can only be dict,int,list,float,str, or None type. I followed the steps mentioned in <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">this<\/a> aws doc to create my lambda function. Can someone please explain why my code is throwing above mentioned error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1573829888267,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|machine-learning|aws-lambda|amazon-sagemaker",
        "Question_view_count":519,
        "Owner_creation_time":1568318861627,
        "Owner_last_access_time":1663770939230,
        "Owner_location":"Hyderabad, Telangana, India",
        "Owner_reputation":486,
        "Owner_up_votes":28,
        "Owner_down_votes":2,
        "Owner_views":75,
        "Question_last_edit_time":1573837537747,
        "Answer_body":"<p><code>data=json.loads(json.dumps(event))<\/code> is a redundant operation. <code>data=event<\/code> will return <code>True<\/code>. The event we provided for the test case is of type dict. It has a key value pair. key can be anything and the value should be a single string of all the predictor variables separated by comas. For predicting the output, we need value of the test case. So declare, for example, <code>payload=data['key']<\/code> then change <code>Body=payload<\/code> inside <code>response<\/code>. Then it will work.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1574162755396,
        "Answer_score":1.0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58879596",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67176637,
        "Question_title":"How to get bearer token AWS for Postman",
        "Question_body":"<p>I am using AWS sagemaker, and I have created an endpoint. I want to test endpoint on postman app. I give endpoint URL and JSON body to postman app. But I get this error that <code>&quot;message&quot;: &quot;Missing Authentication Token&quot;<\/code> I need to know from where I 'll get bearer token so that I can give it to postman app.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618912675733,
        "Question_score":0,
        "Question_tags":"amazon-web-services|postman|amazon-sagemaker",
        "Question_view_count":1080,
        "Owner_creation_time":1567880532003,
        "Owner_last_access_time":1661706476487,
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":137,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":100,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I am answering my own question after searching and reading forums,<\/p>\n<p>The easiest way to get bearer token is to install AWS CLI and configure it, using <code>aws configure<\/code> command.\nFor configuring, we must need to know access key, secret key, region of user. These things can be get by AWS users section.\nAfter configuration by running this command, <code>aws ecr get-authorization-token<\/code>, we can get authorizationToken. <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/ecr\/get-authorization-token.html\" rel=\"nofollow noreferrer\">here<\/a> This token can be fed into bearer token, along with aws signature (access key and secret key) in authorization menu in Postman app.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1618952953590,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67176637",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48111034,
        "Question_title":"Reading a large csv from a S3 bucket using python pandas in AWS Sagemaker",
        "Question_body":"<p>I'm trying to load a large CSV (~5GB) into pandas from S3 bucket.<\/p>\n\n<p>Following is the code I tried for a small CSV of 1.4 kb :<\/p>\n\n<pre><code>client = boto3.client('s3') \nobj = client.get_object(Bucket='grocery', Key='stores.csv')\nbody = obj['Body']\ncsv_string = body.read().decode('utf-8')\ndf = pd.read_csv(StringIO(csv_string))\n<\/code><\/pre>\n\n<p>This works well for a small CSV, but my requirement of loading a 5GB csv to pandas dataframe cannot be achieved through this (probably due to memory constraints when loading the csv by StringIO). <\/p>\n\n<p>I also tried below code<\/p>\n\n<pre><code>s3 = boto3.client('s3')\nobj = s3.get_object(Bucket='bucket', Key='key')\ndf = pd.read_csv(obj['Body'])\n<\/code><\/pre>\n\n<p>but this gives below error.<\/p>\n\n<pre><code>ValueError: Invalid file path or buffer object type: &lt;class 'botocore.response.StreamingBody'&gt;\n<\/code><\/pre>\n\n<p>Any help to resolve this error is much appreciated.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1515145926657,
        "Question_score":8,
        "Question_tags":"python|csv|amazon-s3|amazon-sagemaker",
        "Question_view_count":15101,
        "Owner_creation_time":1346239623100,
        "Owner_last_access_time":1663162010980,
        "Owner_location":"Salzburg, Austria",
        "Owner_reputation":505,
        "Owner_up_votes":38,
        "Owner_down_votes":1,
        "Owner_views":102,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48111034",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72059905,
        "Question_title":"SageMaker Ground Truth : both source and source-ref in manifest?",
        "Question_body":"<p>I set up a labelling job in SageMaker Ground Truth. Here is what is in the <strong>jsonl<\/strong> manifest I created.<\/p>\n<pre><code>{&quot;source&quot;:&quot;Sample text&quot;}\n{&quot;source&quot;:&quot;Sample text&quot;}\n<\/code><\/pre>\n<p>Is the &quot;datasetObjectId&quot;:&quot;1&quot; in the resulting json file after annotation created in the order of the appearing sources in the manifest? Can I add metadata to the manifest file, like this for example:<\/p>\n<pre><code>{&quot;source&quot;:&quot;Sample text&quot;, &quot;file&quot;:&quot;s3:\/\/foo\/bar1.txt&quot;}\n{&quot;source&quot;:&quot;Sample text&quot;, &quot;file&quot;:&quot;s3:\/\/foo\/bar2.txt&quot;}\n<\/code><\/pre>\n<p>My problem: I would like to be able to track the source of a file (not only its text). But for the NER task, working with &quot;source-ref&quot; produces one labelling page per <em>line<\/em> of text, and I want to work with one labelling page per <em>document<\/em>.\nHaving tried to put both source and source-ref in the manual manifest, it is not understood (see error message below).<\/p>\n<pre><code>The manifest file in the Input dataset location has an error or is not supported for this task type. Update the input manifest file, or choose another task type. Error: SyntaxError: JSON.parse: unexpected character at line 1 column 1 of the JSON data\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1651244269230,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-ground-truth",
        "Question_view_count":102,
        "Owner_creation_time":1551431163127,
        "Owner_last_access_time":1660223920803,
        "Owner_location":"Paris, France",
        "Owner_reputation":494,
        "Owner_up_votes":133,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":1651248681169,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72059905",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54295445,
        "Question_title":"TensorFlow Serving send data as b64 instead of Numpy Array",
        "Question_body":"<p>I have a TensorFlow Serving container in a SageMaker endpoint. I'm able to take a batch of images as a Numpy array and get back predictions like this:<\/p>\n\n<pre><code>import numpy as np\nimport sagemaker\nfrom sagemaker.predictor import json_serializer, json_deserializer\n\nimage = np.random.uniform(low=-1.0, high=1.0, size=(1,128,128,3)).astype(np.float32)    \nimage = {'instances': image}\nimage = json_serializer(image)\n\nrequest_args = {}\nrequest_args['Body'] = image\nrequest_args['EndpointName'] = endpoint_name\nrequest_args['ContentType'] = 'application\/json'\nrequest_args['Accept'] = 'application\/json'\n\n# works successfully\nresponse = sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\nresponse_body = response['Body']\npredictions = json_deserializer(response_body, response['ContentType'])\n<\/code><\/pre>\n\n<p>The size of the <code>request_args<\/code> payload is large doing it this way. I'm wondering, is there a way to send this in a more compressed format? <\/p>\n\n<p>I've tried experimenting with <code>base64<\/code> and <code>json.dumps<\/code>, but can't get past <code>Invalid argument: JSON Value: ...<\/code> errors. Not sure if this isn't supported or if I'm just doing it incorrectly.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548093365813,
        "Question_score":1,
        "Question_tags":"python|image|base64|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":796,
        "Owner_creation_time":1361339272693,
        "Owner_last_access_time":1663965928400,
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I've talked to AWS support about this (see <a href=\"https:\/\/stackoverflow.com\/questions\/54090270\/more-efficient-way-to-send-a-request-than-json-to-deployed-tensorflow-model-in-s\">More efficient way to send a request than JSON to deployed tensorflow model in Sagemaker?<\/a>).<\/p>\n\n<p>They suggest that it is possible to pass in a custom input_fn that will be used by the serving container where one can unpack a compressed format (such as protobuf).<\/p>\n\n<p>I'll be testing this soon and hopefully this stuff works since it would add a lot of flexibility to the input processing.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1548251290416,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54295445",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65828200,
        "Question_title":"How to use Azure DevOps artifacts repository as source for DatabricksStep of AzureML?",
        "Question_body":"<p>If we have PyPi Packages added as Artifacts to an Azure DevOps Project Feed, how can we use these packages as a source for installing packages in <code>DatabricksStep<\/code> of Azure Machine Learning Service?<\/p>\n<p>While using <code>pip<\/code> in any environment, we use our Azure DevOps Project Artifacts feed in the following way:<\/p>\n<pre><code>pip install example-package --index-url=https:\/\/&lt;Personal-Access-Token&gt;@pkgs.dev.azure.com\/&lt;Organization-Name&gt;\/_packaging\/&lt;Artifacts-Feed-Name&gt;\/pypi\/simple\/\n<\/code><\/pre>\n<p>The DatabricksStep class of the Azure Machine Learning Service accepts the following parameters:<\/p>\n<pre><code>python_script_name = &quot;&lt;Some-Script&gt;.py&quot;\nsource_directory = &quot;&lt;Path-To-Script&gt;&quot;\n\n&lt;Some-Placeholder-Name-for-the-step&gt; = DatabricksStep(\n    name=&lt;Some-Placeholder-Name-for-the-step&gt;,\n    num_workers=1,\n    python_script_name=python_script_name,\n    source_directory=source_directory,\n    run_name= &lt;Name-of-the-run&gt;,\n    compute_target=databricks_compute,\n    pypi_libraries = [\n                      PyPiLibrary(package = 'scikit-learn'), \n                      PyPiLibrary(package = 'scipy'), \n                      PyPiLibrary(package = 'azureml-sdk'), \n                      PyPiLibrary(package = 'joblib'), \n                      PyPiLibrary(package = 'azureml-dataprep[pandas]'),\n                      PyPiLibrary(package = 'example-package', repo='https:\/\/&lt;Personal-Access-Token&gt;@pkgs.dev.azure.com\/&lt;Organization-Name&gt;\/_packaging\/&lt;Artifacts-Feed-Name&gt;\/pypi\/simple\/')\n                    ], \n\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>However, <code>PyPiLibrary(package = 'example-package', repo='https:\/\/&lt;Personal-Access-Token&gt;@pkgs.dev.azure.com\/&lt;Organization-Name&gt;\/_packaging\/&lt;Artifacts-Feed-Name&gt;\/pypi\/simple\/')<\/code> will give an error. How exactly should we consume the Artifacts Feed as an input to the <code>PyPiLibrary<\/code> property of the <code>DatabricksStep<\/code> Class in Azure Machine Learning Service?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1611234834813,
        "Question_score":5,
        "Question_tags":"azure|azure-devops|azure-databricks|azure-machine-learning-service|azure-artifacts",
        "Question_view_count":341,
        "Owner_creation_time":1601729162437,
        "Owner_last_access_time":1663774065773,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":887,
        "Owner_up_votes":187,
        "Owner_down_votes":32,
        "Owner_views":130,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65828200",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62696966,
        "Question_title":"Why does Azure ML Studio (classic) take additional time to execute Python Scripts?",
        "Question_body":"<p>I have been working with ML Studio (classic) and facing a problem with &quot;Execute Python&quot; scripts. I have noticed that it takes additional time to perform some internal tasks after which it starts executing the actual Python code in ML Studio. This delay has caused an increased time of 40-60 seconds per module which is aggregating and causing a delay of 400-500 seconds per execution when consumed through Batch Execution System or on running the experiments manually. (I've multiple Modules of &quot;Execute Python&quot; scripts)<\/p>\n<p>For instance - If I run a code in my local system, suppose it takes 2-3 seconds. The same would consume 50-60 seconds in Azure ML Studio.<\/p>\n<p>Can you please help understand the reason behind this or any optimization that can be done?<\/p>\n<p>Regards,\nAnant<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593694819477,
        "Question_score":2,
        "Question_tags":"python-3.x|azure|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":166,
        "Owner_creation_time":1582179684313,
        "Owner_last_access_time":1656918545750,
        "Owner_location":"Hyderabad, Telangana, India",
        "Owner_reputation":601,
        "Owner_up_votes":86,
        "Owner_down_votes":6,
        "Owner_views":94,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The known limitations of Machine Learning Studio (classic) are:<\/p>\n<p>The Python runtime is sandboxed and does not allow access to the network or to the local file system in a persistent manner.<\/p>\n<p>All files saved locally are isolated and deleted once the module finishes. The Python code cannot access most directories on the machine it runs on, the exception being the current directory and its subdirectories.<\/p>\n<p>When you provide a zipped file as a resource, the files are copied from your workspace to the experiment execution space, unpacked, and then used. Copying and unpacking resources can consume memory.<\/p>\n<p>The module can output a single data frame. It's not possible to return arbitrary Python objects such as trained models directly back to the Studio (classic) runtime. However, you can write objects to storage or to the workspace. Another option is to use pickle to serialize multiple objects into a byte array and then return the array inside a data frame.<\/p>\n<p>Hope this helps!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1593695267950,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62696966",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71412319,
        "Question_title":"Register Trained Model in Azure Machine Learning",
        "Question_body":"<p>I'm training a Azure Machine learning model using script via python SDK. I'm able to see the environment creation and the model getting trained in std_log in output&amp;logs folder. After the Model training I try to dump the model, but I don't see the model in any folder.<\/p>\n<p>If possible I want to register the model directly into the Model section in Azure ML rather than dumping it in the pickle file.<\/p>\n<p>I'm using the following link for reference <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-sdk-train\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-sdk-train<\/a><\/p>\n<p>Below is the output log snapshot for the model training run<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/OdkyF.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OdkyF.jpg\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1646841733913,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio|azureportal|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":156,
        "Owner_creation_time":1629016154667,
        "Owner_last_access_time":1663902242127,
        "Owner_location":"Chennai, Tamil Nadu, India",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":1646843402512,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71412319",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73386272,
        "Question_title":"How to log metrics to Azure ML Metrics Tab",
        "Question_body":"<p>I have the following train.py file<\/p>\n<pre><code>import argparse\nimport os\nimport numpy as np\nimport glob\n# import joblib\nimport mlflow\nimport logging\nimport azureml.core\nimport pandas as pd\nimport numpy as np\nimport pandas as pd \n\nfrom matplotlib import pyplot as plt\nfrom azureml.core import Workspace, Dataset\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.workspace import Workspace\nfrom azureml.core.dataset import Dataset\nfrom azureml.train.automl import AutoMLConfig\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nimport lightgbm as lgb\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\n\n\n# let user feed in 2 parameters, the dataset to mount or download,\n# and the regularization rate of the logistic regression model\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    &quot;--tablename&quot;, type=str, dest=&quot;tablename&quot;, help=&quot;Table name&quot;\n)\nargs = parser.parse_args()\n\ntablename = args.tablename\n\n\nsubscription_id = ''\nresource_group = 'mlplayground'\nworkspace_name = 'mlplayground'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n\ndataset = Dataset.get_by_name(workspace, name=tablename)\ndata = dataset.to_pandas_dataframe()\n\n# use mlflow autologging\nmlflow.autolog()\n\ndata.drop(['postal_code','Column1','province','region','lattitude','longitude'], axis=1, inplace=True)\none_hot_state_of_the_building=pd.get_dummies(data.state_of_the_building) \none_hot_city = pd.get_dummies(data.city_name, prefix='city')\n\n#removing categorical features \ndata.drop(['city_name','state_of_the_building'],axis=1,inplace=True)  \n\n#Merging one hot encoded features with our dataset 'data' \ndata=pd.concat([data,one_hot_city,one_hot_state_of_the_building,],axis=1) \n\ndata['pricepersqm'] = data.price \/ data.house_area\n\nx=data.drop('price',axis=1) \ny=data.price \n\nX_df = DataFrame(x, columns= data.columns)\nX_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.20)\n\n#Converting the data into proper LGB Dataset Format\nd_train=lgb.Dataset(X_train, label=y_train)\n\n\n#Declaring the parameters\nparams = {\n    'task': 'train', \n    'boosting': 'gbdt',\n    'objective': 'regression',\n    'num_leaves': 10,\n    'learning_rate': 0.01,\n    'metric': {'l2','l1'},\n    'verbose': -1\n}\n\nprint(&quot;Train a LightGBM Regression model&quot;)\nclf=lgb.train(params,d_train,1000)\n\n#model prediction on X_test\nprint(&quot;Predict the test set&quot;)\ny_pred=clf.predict(X_test)\n\n#using RMSE error metric\nmse =mean_squared_error(y_pred,y_test)\nprint(&quot;RMSE: &quot;, mse**0.5)\nmlflow.log_metric(&quot;RMSE&quot;, mse**0.5)\n<\/code><\/pre>\n<p>And then from a notebook file I use the following:<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core import Experiment\n\n# connect to your workspace\nws = Workspace.from_config()\n\nexperiment_name = &quot;get-started-with-jobsubmission-tutorial-andlightgbm&quot;\nexp = Experiment(workspace=ws, name=experiment_name)\n\n\n\nfrom azureml.core.environment import Environment\n\n# use a curated environment that has already been built for you\n\nenv = Environment.get(workspace=ws, \n                      name=&quot;AzureML-sklearn-1.0-ubuntu20.04-py38-cpu&quot;, \n                      version=1)\n\nfrom azureml.core import ScriptRunConfig\n\nargs = [&quot;--tablename&quot;, &quot;BelgiumRealEstate&quot;]\n\nsrc = ScriptRunConfig(\n    source_directory=&quot;&quot;,\n    script=&quot;train.py&quot;,\n    arguments=args,\n    compute_target=&quot;local&quot;,\n    environment=env,\n)\n\nrun = exp.submit(config=src)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>As you can see in the train.py file I am logging the RMSE, however the metric does not appear on the metrics tab.<\/p>\n<p>What should I do?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660729490553,
        "Question_score":0,
        "Question_tags":"python|azure-machine-learning-service|mlflow",
        "Question_view_count":40,
        "Owner_creation_time":1302030303093,
        "Owner_last_access_time":1663332147473,
        "Owner_location":"Brussels, B\u00e9lgica",
        "Owner_reputation":30340,
        "Owner_up_votes":1667,
        "Owner_down_votes":79,
        "Owner_views":2937,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73386272",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":42606010,
        "Question_title":"\"Batch execution failed with HTTP status code: BadGateway. The response from the Machine Learning service at endpoint",
        "Question_body":"<p>I want to schedule my AzureML experiment by Azure Data Factory (ADF).  After passing my job from azureML pipeline, I face with this issue \"Batch execution failed with HTTP status code: BadGateway. The response from the Machine Learning service at endpoint <a href=\"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/26c276f8420a4c30aae39b0c27845134\/services\/7b122129b2c041f9b4399d7e4b16e927\/jobs\/job_id\/start\" rel=\"nofollow noreferrer\">https:\/\/ussouthcentral.services.azureml.net\/workspaces\/26c276f8420a4c30aae39b0c27845134\/services\/7b122129b2c041f9b4399d7e4b16e927\/jobs\/job_id\/start<\/a> was 'Internal error occurred.\"<\/p>\n\n<p>Do you have any idea for solving this issue.\nThanks,<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1488701373533,
        "Question_score":0,
        "Question_tags":"azure-data-factory|azure-machine-learning-studio",
        "Question_view_count":379,
        "Owner_creation_time":1486079485687,
        "Owner_last_access_time":1554858720667,
        "Owner_location":"Seattle, WA, United States",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1488702037912,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/42606010",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":36666418,
        "Question_title":"Azure machine learning specify input size",
        "Question_body":"<p>I just started using Azure ML and I'm trying to figure out how to specify an input size for the models. Specifically, I have a big training set of data, but I want to input only 250 records at a time into the PCA algorithm. It seems like all I can do is hook the entire data set into the PCA module.<\/p>\n\n<p>I know how to partition the data for X-validation, but I want a partition (say 10000 records) to only feed 250 records at a time to the model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1460822385593,
        "Question_score":0,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio",
        "Question_view_count":81,
        "Owner_creation_time":1329540965833,
        "Owner_last_access_time":1584910587307,
        "Owner_location":null,
        "Owner_reputation":477,
        "Owner_up_votes":38,
        "Owner_down_votes":0,
        "Owner_views":55,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36666418",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73238138,
        "Question_title":"Azureml pipeline directly pass input dataset to command step",
        "Question_body":"<p>I have an azureml pipeline that uses a command step to run an external software cli, I want to pass a set of arguments to this software and some input and output datasets. From this <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-with-commandstep.ipynb\" rel=\"nofollow noreferrer\">notebook<\/a> it's stated<\/p>\n<blockquote>\n<p>If you have an input dataset you want to use in this step, you can specify that as part of the command. For example, if you have a FileDataset object called dataset and a --data-dir script argument, you can do the following: command=['python train.py --epochs 30 --data-dir', dataset.as_mount()].<\/p>\n<\/blockquote>\n<p>The problem is that I don't want to pass my datasets as arguments of the program as the program will crash if it sees unknown arguments like --data-dir, to bypass the issue I've done a very ugly thing, it appears that if I use another command step that just do an echo and receives as input my dataset as --input-data I can then use the dataset anywhere so I can bypass the initial issue. (Part of) The code that I'm using is the following<\/p>\n<pre><code>    environment = Environment.get(\n        workspace=self.workspace,\n        name=self.config.steps.first.environment_name,\n    )\n\n    output_dataset = None\n\n    if self.config.output_dataset is not None:\n        datastore = Datastore.get(workspace=self.workspace, datastore_name=self.config.output_dataset.datastore)\n\n        output_dataset = OutputFileDatasetConfig(\n            name=&quot;output_data&quot;,\n            destination=(datastore, f&quot;{self.config.output_dataset.name}\/{{run-id}}&quot;),\n        ).register_on_complete(self.config.output_dataset.name)\n\n    input_dataset = Dataset.get_by_name(\n        self.workspace, name=self.config.input_dataset.name, version=self.config.input_dataset.version\n    )\n\n    input_dataset_param = PipelineParameter(name=&quot;input_data&quot;, default_value=input_dataset)\n    input_consumption = DatasetConsumptionConfig(\n        &quot;input_data&quot;,\n        input_dataset_param,\n    )\n\n    model_consumption = None\n\n    software_parameters = PipelineParameter(\n        name=&quot;software_parameters&quot;,\n        default_value=self.config.software_parameters if self.config.software_parameters is not None else &quot; &quot;,\n    )\n\n    command = [&quot;magicsoftware&quot;, software_parameters]\n\n    train_src = ScriptRunConfig(\n        source_directory=self.config.source_dir,\n        environment=environment,\n        compute_target=self.__compute_cluster,\n    )\n\n    input_datasets = [input_consumption]\n\n    magic_command = [&quot;echo&quot;, &quot;--input_data&quot;, input_consumption]\n\n    magic_step = CommandStep(\n        name=&quot;magic_step&quot;,\n        command=magic_command,\n        runconfig=train_src,\n        inputs=input_datasets,\n        allow_reuse=False,\n    )\n\n    experiment_step = CommandStep(\n        name=self.config.steps.first.name,\n        command=command,\n        runconfig=train_src,\n        inputs=input_datasets,\n        outputs=[output_dataset] if output_dataset is not None else None,\n        allow_reuse=False,\n    )\n    logging.info(&quot;Build the list of steps&quot;)\n    step_sequence = StepSequence(steps=[magic_step, experiment_step])\n\n    logging.info(&quot;Ending building&quot;)\n    return Pipeline(\n        workspace=self.workspace,\n        steps=step_sequence,\n        description=&quot;&quot;,\n    )\n<\/code><\/pre>\n<p>While this works it's still an ugly workaround and it requires to have a compute cluster running first the echo step, then the real step and it slows down the process.<\/p>\n<p>Am I missing something or it's just a bad design of the command step? I've tried many solutions to make it work but unless I pass the arguments as --something I didn't manage to make it work.<\/p>\n<p>I'm working with azureml-core version 1.40.0<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1659625179437,
        "Question_score":0,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":81,
        "Owner_creation_time":1556634159780,
        "Owner_last_access_time":1663257023900,
        "Owner_location":null,
        "Owner_reputation":25,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73238138",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62701556,
        "Question_title":"Strange algorithm selection when using Azure AutoML with XBoostClassifier on categorial data",
        "Question_body":"<p>I have a data model consisting only of categorial features and a categorial label.<\/p>\n<p>So when I build that model manually in XGBoost, I would basically transform the features to binary columns (using LabelEncoder and OneHotEncoder), and the label into classes using LabelEncoder. I would then run a <strong>Multilabel Classification<\/strong> (multi:softmax).\nI tried that with my dataset and ended up with an accuracy around 0.4 (unfortunately can't share the dataset due to confidentiality)<\/p>\n<p>Now, if I run the same dataset in Azure AutoML, I end up with an accuracy around 0.85 in the best experiment. But what is really interesting is that the AutoML uses SparseNormalizer, XGBoostClassifier, with <strong>reg:logistic<\/strong> as objective.\nSo if I interpret this right, AzureML just normalizes the data (somehow from categorial data?) and then executes a logistic regression? Is this even possible \/ does this make sense with categorial data?<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1593709508303,
        "Question_score":3,
        "Question_tags":"xgboost|azure-machine-learning-service",
        "Question_view_count":281,
        "Owner_creation_time":1534756062257,
        "Owner_last_access_time":1662737261800,
        "Owner_location":null,
        "Owner_reputation":87,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1593766738543,
        "Answer_body":"<p><code>TL;DR<\/code> You're right that normalization doesn't make sense for training gradient-boosted decision trees (<code>GBDT<\/code>s) on categorical data, but it won't have an adverse impact. AutoML is an automated framework for modeling. In exchange for calibration control, you get ease-of-use. It is still worth verifying first that AutoML is receiving data with the columns properly encoded as categorical.<\/p>\n<p>Think of an AutoML model as effectively a <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html\" rel=\"nofollow noreferrer\">sklearn Pipeline<\/a>, which is a bundled set of pre-processing steps along with a predictive Estimator. AutoML will attempt to sample from a large swath of pre-configured Pipelines such that the most accurate Pipeline will be discovered. As <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-automated-ml#automatic-featurization-standard\" rel=\"nofollow noreferrer\">the docs<\/a> say:<\/p>\n<blockquote>\n<p>In every automated machine learning experiment, your data is automatically scaled or normalized to help algorithms perform well. During model training, one of the following scaling or normalization techniques will be applied to each model.<\/p>\n<\/blockquote>\n<p>Too see this, you can called <code>.named_steps<\/code> on your fitted model. Also check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train#automated-feature-engineering\" rel=\"nofollow noreferrer\"><code>fitted_model.get_featurization_summary()<\/code><\/a><\/p>\n<p>I especially empathize with your concern especially w.r.t. how <code>LightGBM<\/code> (MSFT's GBDT implementation) is levered by AutoML. <code>LightGBM<\/code> accepts categorical columns and instead of one-hot encoding, will bin them into two subsets whenever split. Despite this, AutoML will pre-process away the categorical columns by one-hot encoding, scaling, and\/or normalization; so this unique categorical approach is never utilized in AutoML.<\/p>\n<p>If you're interested in &quot;manual&quot; ML in Azure ML, I highly suggest looking into <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-train-machine-learning-model#estimators\" rel=\"nofollow noreferrer\"><code>Estimators<\/code><\/a> and <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-train-machine-learning-model#machine-learning-pipeline\" rel=\"nofollow noreferrer\"><code>Azure ML Pipelines<\/code><\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1593795420676,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1594068501472,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62701556",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64535892,
        "Question_title":"How to format body to pass input dataset as parameter in Azure ML?",
        "Question_body":"<p>I'm trying to consume my Azure ML Pipeline (Batch) from LogicApps. For that, I've deployed the Batch pipeline with the dataset as parameter :<a href=\"https:\/\/i.stack.imgur.com\/PYhU5.png\" rel=\"nofollow noreferrer\">1<\/a><\/p>\n<p>But I can't figure it out, how to format my body to invoke the pipeline and I don't find documentation on that.<\/p>\n<p>For now, this is how my logicapp looks like:\n<a href=\"https:\/\/i.stack.imgur.com\/lkKCB.png\" rel=\"nofollow noreferrer\">2<\/a>\nand I get a 415 Http Error code when trying to invoke the pipeline.<\/p>\n<p>Thanks for your help.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1603710297587,
        "Question_score":1,
        "Question_tags":"azure-logic-apps|azure-machine-learning-studio",
        "Question_view_count":67,
        "Owner_creation_time":1431423994833,
        "Owner_last_access_time":1657729027097,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64535892",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62836278,
        "Question_title":"AZURE ML, python code gets: 'tls_process_server_certificate', 'certificate verify failed'",
        "Question_body":"<p>The following code works fine on a jupyter notebook in a compute instance.\nHowever I need to test this locally on Visual Studio Code<\/p>\n<pre><code>import pandas as pd\nimport datetime\nimport csv\nimport numpy as np\nfrom azureml.core import Workspace\nfrom azureml.core import Dataset, Datastore\nfrom azureml.data.datapath import DataPath\nfrom azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\nfrom azureml.core.authentication import AzureCliAuthentication\n\n# #azure-cosmosdb-table\n# from azure.cosmosdb.table.tableservice import TableService\n# from azure.cosmosdb.table.models import Entity\n\n#Azure blob storage\n\ncli_auth = AzureCliAuthentication()\n\nws = Workspace(subscription_id=&quot;xx&quot;,\n               resource_group=&quot;xx&quot;,\n               workspace_name=&quot;xx&quot;,\n               auth=cli_auth)\n\nprint(&quot;Found workspace {} at location {}&quot;.format(ws.name, ws.location))\n\n\nimport os\nfrom azureml.core.authentication import ServicePrincipalAuthentication\n\nsvc_pr_password = &quot;xx&quot; #os.environ.get(&quot;AZUREML_PASSWORD&quot;)\n\nsvc_pr = ServicePrincipalAuthentication(\n    tenant_id=&quot;xx&quot;,\n    service_principal_id=&quot;xx&quot;,\n    service_principal_password=svc_pr_password)\n\n\nws = Workspace(\n    subscription_id=&quot;xx&quot;,\n    resource_group=&quot;xx&quot;,\n    workspace_name=&quot;xx&quot;,\n    auth=svc_pr\n    )\n\nprint(&quot;Found workspace {} at location {}&quot;.format(ws.name, ws.location))\n<\/code><\/pre>\n<p>However this line:<\/p>\n<pre><code>  datastore = Datastore.get(ws, 'yy')\n<\/code><\/pre>\n<p>gets me this error:<\/p>\n<pre><code>---------------------------------------------------------------------------\nError                                     Traceback (most recent call last)\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py in wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname)\n    487             try:\n--&gt; 488                 cnx.do_handshake()\n    489             except OpenSSL.SSL.WantReadError:\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\OpenSSL\\SSL.py in do_handshake(self)\n   1933         result = _lib.SSL_do_handshake(self._ssl)\n-&gt; 1934         self._raise_ssl_error(self._ssl, result)\n   1935 \n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\OpenSSL\\SSL.py in _raise_ssl_error(self, ssl, result)\n   1670         else:\n-&gt; 1671             _raise_current_error()\n   1672 \n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\OpenSSL\\_util.py in exception_from_error_queue(exception_type)\n     53 \n---&gt; 54     raise exception_type(errors)\n     55 \n\nError: [('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')]\n\nDuring handling of the above exception, another exception occurred:\n\nSSLError                                  Traceback (most recent call last)\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    669             # Make the request on the httplib connection object.\n--&gt; 670             httplib_response = self._make_request(\n    671                 conn,\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\n    380         try:\n--&gt; 381             self._validate_conn(conn)\n    382         except (SocketTimeout, BaseSSLError) as e:\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py in _validate_conn(self, conn)\n    975         if not getattr(conn, &quot;sock&quot;, None):  # AppEngine might not have  `.sock`\n--&gt; 976             conn.connect()\n    977 \n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connection.py in connect(self)\n    360 \n--&gt; 361         self.sock = ssl_wrap_socket(\n    362             sock=conn,\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\util\\ssl_.py in ssl_wrap_socket(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data)\n    376         if HAS_SNI and server_hostname is not None:\n--&gt; 377             return context.wrap_socket(sock, server_hostname=server_hostname)\n    378 \n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py in wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname)\n    493             except OpenSSL.SSL.Error as e:\n--&gt; 494                 raise ssl.SSLError(&quot;bad handshake: %r&quot; % e)\n    495             break\n\nSSLError: (&quot;bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')])&quot;,)\n\nDuring handling of the above exception, another exception occurred:\n\nMaxRetryError                             Traceback (most recent call last)\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\requests\\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)\n    438             if not chunked:\n--&gt; 439                 resp = conn.urlopen(\n    440                     method=request.method,\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    723 \n--&gt; 724             retries = retries.increment(\n    725                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\util\\retry.py in increment(self, method, url, response, error, _pool, _stacktrace)\n    438         if new_retry.is_exhausted():\n--&gt; 439             raise MaxRetryError(_pool, url, error or ResponseError(cause))\n    440 \n\nMaxRetryError: HTTPSConnectionPool(host='westeurope.experiments.azureml.net', port=443): Max retries exceeded with url: \/discovery (Caused by SSLError(SSLError(&quot;bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')])&quot;)))\n\nDuring handling of the above exception, another exception occurred:\n\nSSLError                                  Traceback (most recent call last)\n in \n----&gt; 1 datastore = Datastore.get(ws, 'utilisationdb')\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\azureml\\core\\datastore.py in get(workspace, datastore_name)\n    139                 or azureml.data.dbfs_datastore.DBFSDatastore\n    140         &quot;&quot;&quot;\n--&gt; 141         return Datastore._client().get(workspace, datastore_name)\n    142 \n    143     @staticmethod\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\azureml\\data\\datastore_client.py in get(workspace, datastore_name)\n     63         :rtype: AzureFileDatastore or AzureBlobDatastore\n     64         &quot;&quot;&quot;\n---&gt; 65         return _DatastoreClient._get(workspace, datastore_name)\n     66 \n     67     @staticmethod\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\azureml\\data\\datastore_client.py in _get(ws, name, auth, host)\n    541         module_logger.debug(&quot;Getting datastore: {}&quot;.format(name))\n    542 \n--&gt; 543         client = _DatastoreClient._get_client(ws, auth, host)\n    544         datastore = client.data_stores.get(subscription_id=ws._subscription_id, resource_group_name=ws._resource_group,\n    545                                            workspace_name=ws._workspace_name, name=name,\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\azureml\\data\\datastore_client.py in _get_client(ws, auth, host)\n    726         host_env = os.environ.get('AZUREML_SERVICE_ENDPOINT')\n    727         auth = auth or ws._auth\n--&gt; 728         host = host or host_env or get_service_url(\n    729             auth, _DatastoreClient._get_workspace_uri_path(ws._subscription_id, ws._resource_group,\n    730                                                            ws._workspace_name), ws._workspace_id, ws.discovery_url)\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\azureml\\_base_sdk_common\\service_discovery.py in get_service_url(auth, workspace_scope, workspace_id, workspace_discovery_url, service_name)\n    118 \n    119     cached_service_object = CachedServiceDiscovery(auth)\n--&gt; 120     return cached_service_object.get_cached_service_url(workspace_scope, service_name,\n    121                                                         unique_id=workspace_id, discovery_url=workspace_discovery_url)\n    122 \n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\azureml\\_base_sdk_common\\service_discovery.py in get_cached_service_url(self, arm_scope, service_name, unique_id, discovery_url)\n    280         :rtype: str\n    281         &quot;&quot;&quot;\n--&gt; 282         return self.get_cached_services_uris(arm_scope, service_name, unique_id=unique_id,\n    283                                              discovery_url=discovery_url)[service_name]\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\azureml\\_base_sdk_common\\service_discovery.py in wrapper(self, *args, **kwargs)\n    180             try:\n    181                 lock_to_use.acquire()\n--&gt; 182                 return test_function(self, *args, **kwargs)\n    183             finally:\n    184                 lock_to_use.release()\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\azureml\\_base_sdk_common\\service_discovery.py in get_cached_services_uris(self, arm_scope, service_name, unique_id, discovery_url)\n    255 \n    256         # Actual service discovery only understands arm_scope\n--&gt; 257         cache[cache_key][DEFAULT_FLIGHT] = super(CachedServiceDiscovery, self).discover_services_uris_from_arm_scope(arm_scope, discovery_url)\n    258         try:\n    259             with open(self.file_path, &quot;w+&quot;) as file:\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\azureml\\_base_sdk_common\\service_discovery.py in discover_services_uris_from_arm_scope(self, arm_scope, discovery_url)\n    136     def discover_services_uris_from_arm_scope(self, arm_scope, discovery_url=None):\n    137         discovery_url = self.get_discovery_url(arm_scope, discovery_url)\n--&gt; 138         return self.discover_services_uris(discovery_url)\n    139 \n    140     def discover_services_uris(self, discovery_url=None):\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\azureml\\_base_sdk_common\\service_discovery.py in discover_services_uris(self, discovery_url)\n    139 \n    140     def discover_services_uris(self, discovery_url=None):\n--&gt; 141         status = ClientBase._execute_func(requests.get, discovery_url)\n    142         status.raise_for_status()\n    143         return status.json()\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\azureml\\_restclient\\clientbase.py in _execute_func(cls, func, *args, **kwargs)\n    340     @classmethod\n    341     def _execute_func(cls, func, *args, **kwargs):\n--&gt; 342         return cls._execute_func_internal(\n    343             DEFAULT_BACKOFF, DEFAULT_RETRIES, module_logger, func, _noop_reset, *args, **kwargs)\n    344 \n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\azureml\\_restclient\\clientbase.py in _execute_func_internal(cls, back_off, total_retry, logger, func, reset_func, *args, **kwargs)\n    334                 return func(*args, **kwargs)\n    335             except Exception as error:\n--&gt; 336                 left_retry = cls._handle_retry(back_off, left_retry, total_retry, error, logger, func)\n    337 \n    338             reset_func(*args, **kwargs)  # reset_func is expected to undo any side effects from a failed func call.\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\azureml\\_restclient\\clientbase.py in _handle_retry(cls, back_off, left_retry, total_retry, error, logger, func)\n    364         &quot;&quot;&quot;\n    365         if left_retry == 0:\n--&gt; 366             raise error\n    367         elif isinstance(error, HttpOperationError):\n    368             if error.response.status_code == 403:\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\azureml\\_restclient\\clientbase.py in _execute_func_internal(cls, back_off, total_retry, logger, func, reset_func, *args, **kwargs)\n    332         while left_retry &gt;= 0:\n    333             try:\n--&gt; 334                 return func(*args, **kwargs)\n    335             except Exception as error:\n    336                 left_retry = cls._handle_retry(back_off, left_retry, total_retry, error, logger, func)\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\requests\\api.py in get(url, params, **kwargs)\n     74 \n     75     kwargs.setdefault('allow_redirects', True)\n---&gt; 76     return request('get', url, params=params, **kwargs)\n     77 \n     78 \n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\requests\\api.py in request(method, url, **kwargs)\n     59     # cases, and look like a memory leak in others.\n     60     with sessions.Session() as session:\n---&gt; 61         return session.request(method=method, url=url, **kwargs)\n     62 \n     63 \n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\requests\\sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\n    528         }\n    529         send_kwargs.update(settings)\n--&gt; 530         resp = self.send(prep, **send_kwargs)\n    531 \n    532         return resp\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\requests\\sessions.py in send(self, request, **kwargs)\n    641 \n    642         # Send the request\n--&gt; 643         r = adapter.send(request, **kwargs)\n    644 \n    645         # Total elapsed time of the request (approximately)\n\n~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\requests\\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)\n    512             if isinstance(e.reason, _SSLError):\n    513                 # This branch is for urllib3 v1.22 and later.\n--&gt; 514                 raise SSLError(e, request=request)\n    515 \n    516             raise ConnectionError(e, request=request)\n\nSSLError: HTTPSConnectionPool(host='westeurope.experiments.azureml.net', port=443): Max retries exceeded with url: \/discovery (Caused by SSLError(SSLError(&quot;bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')])&quot;)))\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1594390638313,
        "Question_score":1,
        "Question_tags":"python|python-3.x|azure|jupyter-notebook|azure-machine-learning-service",
        "Question_view_count":219,
        "Owner_creation_time":1302030303093,
        "Owner_last_access_time":1663332147473,
        "Owner_location":"Brussels, B\u00e9lgica",
        "Owner_reputation":30340,
        "Owner_up_votes":1667,
        "Owner_down_votes":79,
        "Owner_views":2937,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62836278",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":57209247,
        "Question_title":"Is there any module in Azure ML Studio to identify features correlated with a Target Variable and correlated input features?",
        "Question_body":"<p>Is there any module which outputs high\/low correlated input features with respect to a given target variable, and which can identify highly correlated input features in a data set in Azure Machine Learning Studio?<\/p>\n\n<p>Such a module would help creating better training data sets and that would allow to create better ML models.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1564084180007,
        "Question_score":0,
        "Question_tags":"machine-learning|feature-selection|azure-machine-learning-studio",
        "Question_view_count":57,
        "Owner_creation_time":1418970237687,
        "Owner_last_access_time":1663728521847,
        "Owner_location":"New Jersey, USA",
        "Owner_reputation":747,
        "Owner_up_votes":48,
        "Owner_down_votes":2,
        "Owner_views":96,
        "Question_last_edit_time":1564336396769,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57209247",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":41485715,
        "Question_title":"Training Model for Each Individual in AzureML",
        "Question_body":"<p>I want to train an ANN model for each individual, in azure ml. For example, there is an application which wants to learn the behavior of each individual separately. How is this possible in azure-ml? Any suggestion?<\/p>\n\n<p>As I know, I can create a model and train it with some data, but I don't know how can I train it specifically for each user. I should mention that I am seeking for a scalable idea which is applicable for a real situation (might be for 100 thousands users).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1483620989100,
        "Question_score":2,
        "Question_tags":"multi-tenant|azure-machine-learning-studio",
        "Question_view_count":205,
        "Owner_creation_time":1403553737940,
        "Owner_last_access_time":1664024595667,
        "Owner_location":"Belgium",
        "Owner_reputation":17835,
        "Owner_up_votes":636,
        "Owner_down_votes":1612,
        "Owner_views":2203,
        "Question_last_edit_time":1485439340663,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41485715",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":38389352,
        "Question_title":"Different Size of Machine Learning Models?",
        "Question_body":"<p>Once the training is done and Model is generated,Model Size can vary according to the dataset and algorithm used.\nI want to know what is the range (in MBs) the (\"generally\") Model Size can vary.<\/p>\n\n<p><a href=\"http:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/system-limits.html\" rel=\"nofollow\">Amazon ML<\/a> sets the limit of Model Size to be between 1 MB to 1GB.<\/p>\n\n<p>The question is mainly revolved about collecting the information about what is the average size of models generated by organizations? Most Models generated by organization are of how much size ?<\/p>\n\n<p>Any pointers in the related field will be helpful.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1468564568543,
        "Question_score":1,
        "Question_tags":"machine-learning|azure-machine-learning-studio|amazon-machine-learning",
        "Question_view_count":5731,
        "Owner_creation_time":1372192553610,
        "Owner_last_access_time":1575921971200,
        "Owner_location":"Atlanta, GA, USA",
        "Owner_reputation":338,
        "Owner_up_votes":16,
        "Owner_down_votes":2,
        "Owner_views":63,
        "Question_last_edit_time":1468577121383,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/38389352",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":34335483,
        "Question_title":"Uniquely identify instances of VMs (Azure ML - web services)",
        "Question_body":"<p>I'm posting this more as a 'probe' question and plan to expand the discussion in case some interest shows up. The reason behind this is that in my experience, the SO community on <code>azure-ml<\/code> (and related) is still developing and there is not much feedback - but I would be happy to help it grow stronger. <\/p>\n\n<p>My situation is as follows: I have an experiment in Azure ML which does all its work inside an <code>R<\/code> module. I published this as a web service and set the 'max concurrent calls' slider to 10 - which I believe guarantees me that there will be at most 10 instances of my web service up and running at any time, to serve requests (please correct me if i am wrong). <\/p>\n\n<p>Now, I am trying to do some performance testing by firing 10 parallel calls to my webservice, but get unexpected results...<\/p>\n\n<p>I am trying to run the load tests and log where each of them actually goes to (which instance). My idea is to get a glimpse into how these calls are actually distributed to the instances by the load balancer, under certain max number of concurrent calls = X. I am doing this by firing a call to \"bot.whatismyipaddress.com\" from inside the <code>R<\/code> script. Here is the important snip of the code:<\/p>\n\n<pre><code>library(rjson)\nmachine.ip &lt;- readLines(\"http:\/\/bot.whatismyipaddress.com\/\", warn=F)\nresult$MachineIP &lt;- machine.ip\n<\/code><\/pre>\n\n<p>Additionally, I am using the sample <code>R<\/code> code from the web service RRS help page to fire up to 70 (sequential) calls to my web service. This sample code returns some info back to the console : the results of my web service as well as some info on to which hostname the call goes through. Here is a sample :<\/p>\n\n<pre><code>* Hostname was NOT found in DNS cache\n*   Trying 40.114.242.9...\n* Connected to europewest.services.azureml.net (40.114.242.9) port 443 (#0)\n<\/code><\/pre>\n\n<p>The difficulty that I am facing is that I cannot <strong>uniquely identify<\/strong> the different instances of my web service. The info out to console from the call (the second snippet) often shows a different IP address than the one from inside-<code>R<\/code>-code logs (<code>result$MachineIP<\/code>)...<\/p>\n\n<p>Can someone point out what am i doing wrong, and how could i uniquely identify the different instances that are serving the calls? Any help would be really appreciated. Thanks!<\/p>\n\n<p>P.S. I've tried <a href=\"https:\/\/stackoverflow.com\/questions\/14357219\/function-for-retrieving-own-ip-address-from-within-r\">this<\/a> as well, but the first apporach does not work when calling it from inside the <code>R<\/code> script and I'm using a modified version of the second apporach (the one suggested there does not work). <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/93f07abf-f0ec-4baa-8225-1ca1a072ca2d\/system-call-from-inside-r-script-does-not-work?forum=MachineLearning\" rel=\"nofollow noreferrer\">Here<\/a> are also my <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/ee6ff5a6-2995-4f3f-b4db-0229b1d9d1d3\/lifetime-of-azure-ml-web-service-container?forum=MachineLearning\" rel=\"nofollow noreferrer\">questions<\/a> on the Azure forum, in case someone is interested.<\/p>\n\n<p>If anyone could help or point me to some source of info I would be really grateful! <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1450358009790,
        "Question_score":1,
        "Question_tags":"r|web-services|azure|azure-machine-learning-studio",
        "Question_view_count":61,
        "Owner_creation_time":1432829415467,
        "Owner_last_access_time":1542573864587,
        "Owner_location":null,
        "Owner_reputation":501,
        "Owner_up_votes":184,
        "Owner_down_votes":0,
        "Owner_views":76,
        "Question_last_edit_time":1495540319592,
        "Answer_body":"<p>This question was resolved thanks to some people on the Azure ML forum so \nI'm going to post an answer for anyone landing here in search for some answers...<\/p>\n\n<p>The short answer is no, this is not possible. The more detailed version is:<br>\n\"From within the R script you cannot identify the internal AzureML IP addresses or the unique web service instances. When you make an external network call from the R script to an outside URL, that URL will see one of the AzureML public virtual IP's as the source IP. These are IP's of the load balancers, and not of the machines that are physically running the web service. AzureML dynamically allocates the instances of R engine in the backend, handles failures, and uses multiple nodes for running the web service for high availability. The exact layout of these for a given web service is not programmatically discoverable.\"<br>\nHere is also the <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/dd1f0658-7b0b-46d8-8e32-3fe4e96ec4be\/uniquely-identify-instances-of-vms-web-services?forum=MachineLearning#cde28631-828d-4d83-9c93-1a1cf0dfb6fb\" rel=\"nofollow\">link<\/a> to the original discussion. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1452244028967,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34335483",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":36917948,
        "Question_title":"Feature weightage from Azure Machine Learning Deployed Web Service",
        "Question_body":"<p>I am trying to predict from my past data which has around 20 attribute columns and a label. Out of those 20, only 4 are significant for prediction. But i also want to know that if a row falls into one of the classified categories, what other important correlated columns apart from those 4 and what are their weight. I want to get that result from my deployed web service on Azure.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1461854370743,
        "Question_score":0,
        "Question_tags":"machine-learning|azure-machine-learning-studio",
        "Question_view_count":303,
        "Owner_creation_time":1461853741067,
        "Owner_last_access_time":1520500022443,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use permutation feature importance module but that will give importance of the features across the sample set. Retrieving the weights on per call basis is not available in Azure ML.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1461985174656,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36917948",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":36344278,
        "Question_title":"Azure machine learning. How can I see all columns",
        "Question_body":"<p>When I upload dataset with more then 100 columns I can see only part of them in the visualisation block. Can I see stats for all columns from dataset? Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1459460172740,
        "Question_score":0,
        "Question_tags":"cortana-intelligence|azure-machine-learning-studio",
        "Question_view_count":736,
        "Owner_creation_time":1459459387887,
        "Owner_last_access_time":1576493624753,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1459517035743,
        "Answer_body":"<p>If you are an owner in the workspace, you can open your dataset in Python inside of a Jupyter Notebook. By the visualize should be an open in notebook button. Then just execute the code that is provided for you, and it should print your dataset. You can then also select specific columns to visualize as well.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1459517016507,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36344278",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64257530,
        "Question_title":"Import data and python scripts in azure ml entry script when deploying models",
        "Question_body":"<p>I have an existing machine learning model saved on my local system. I want to deploy this model as a web service so I can consume this model as a request-response i.e. send an HTTP request to the model and get back a predicted response.<\/p>\n<p>When attempting to deploy this model on AzureML I run into a few problems<\/p>\n<p>The model needs to be initialized in an entry script int the init() function, but for initializing my model I have a custom class and require few txt files to be loaded.<\/p>\n<p>below is the code to initialize the model object<\/p>\n<pre><code>from model_file import MyModelClass  # this is the file which contains the model class\n\ndef init():\n  global robert_model\n\n  my_model = MyModelClass(vocab_path='&lt;path-to-text-files&gt;',\n                          model_paths=['&lt;path-to-model-file&gt;'],\n                          iterations=5,\n                          min_error_probability=0.0,\n                          min_probability=0.0,\n                          weigths=None)\ndef run(json_data):\n  try:\n    data = json.loads(json_data)\n    preds, cnt = my_model.handle_batch([sentence.split()])\n    return {'output': pred, 'count': cnt}\n  except Exception as e:\n    error = str(e)\n    return error\n<\/code><\/pre>\n<p>I don't know how to import those class files and text files in the entry script<\/p>\n<p>I don't know much about azure, and I am having a hard time figuring this out. Please help.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1602141348237,
        "Question_score":3,
        "Question_tags":"python|azure|machine-learning|web-deployment|azure-machine-learning-service",
        "Question_view_count":2616,
        "Owner_creation_time":1595686289650,
        "Owner_last_access_time":1657957414863,
        "Owner_location":"India",
        "Owner_reputation":91,
        "Owner_up_votes":49,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1602177450750,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64257530",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":69473829,
        "Question_title":"Include additional files during model deployment (part 2)",
        "Question_body":"<p>Since we're not supposed to ask questions to answered questions, I'm posting this additional question here.  It references the question\/answers found here however:  <a href=\"https:\/\/stackoverflow.com\/questions\/61803031\/azure-ml-include-additional-files-during-model-deployment\">Azure ML: Include additional files during model deployment<\/a><\/p>\n<p>Problem:  in my scoring script I need to reference data in a file for additional data munging before calling the model.<\/p>\n<p>Option #2 referenced in that link (use InferenceConfig to specify a folder as source directory) appears to be what I need.  But when I attempt to access the included file in my scoring script, I'm getting an error message that the file doesn't exist.  I believe I've done something wrong but I'm not sure what.<\/p>\n<p>My work is taking place strictly in Azure ML Studio.<br \/>\nI have a directory where both my scoring script and the pickle file I want to include are found.\nIt's also where the Jupyter Notebook script is found that's driving the creation of these things.\nThe pathing would look something like this:  Users-&gt;myname-&gt;myfolder<\/p>\n<p>The scoring script is named score.py.  The pickle file is mypickle.pkl.<\/p>\n<p>Code in score.py to read the file.  This code is found in the run() method.<\/p>\n<pre><code>filename=&quot;.\/mypickle.pkl&quot;\ndf = pd.read_pickle(filename)\n<\/code><\/pre>\n<p>The InferenceConfig call:<\/p>\n<pre><code>inference_config = InferenceConfig(\nenvironment=environment,\nsource_directory=&quot;.\/&quot;,\nentry_script=&quot;.\/score.py&quot;\n)\n<\/code><\/pre>\n<p>The model deploys fine but when testing the deployed model (aka endpoint), I receive the error\n&quot;[Errno 2] No such file or directory: 'mypickle.pkl'&quot;<\/p>\n<p>Thoughts?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633563793660,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":109,
        "Owner_creation_time":1567959306590,
        "Owner_last_access_time":1636567838733,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1633567085056,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69473829",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":37807158,
        "Question_title":"Train multiple models with various measures and accumulate predictions",
        "Question_body":"<p>So I have been playing around with Azure ML lately, and I got one dataset where I have multiple values I want to predict. All of them uses different algorithms and when I try to train multiple models within one experiment; it says the \u201ctrain model can only predict one value\u201d, and there are not enough input ports on the train-model to take in multiple values even if I was to use the same algorithm for each measure. I tried launching the column selector and making rules, but I get the same error as mentioned. How do I predict multiple values and later put the predicted columns together for the web service output so I don\u2019t have to have multiple API\u2019s?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1465894075710,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":1763,
        "Owner_creation_time":1463041289043,
        "Owner_last_access_time":1465915063043,
        "Owner_location":null,
        "Owner_reputation":25,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>What you would want to do is to train each model and save them as already trained models.\nSo create a new experiment, train your models and save them by right clicking on each model and they will show up in the left nav bar in the Studio. Now you are able to drag your models into the canvas and have them score predictions where you eventually make them end up in the same output as I have done in my example through the \u201cAdd columns\u201d module. I made this example for Ronaldo (Real Madrid CF player) on how he will perform in match after training day. You can see my demo on <a href=\"http:\/\/ronaldoinform.azurewebsites.net\" rel=\"nofollow noreferrer\">http:\/\/ronaldoinform.azurewebsites.net<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZwzUy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZwzUy.png\" alt=\"Ronaldo InForm\"><\/a><\/p>\n\n<p>For more detailed explanation on how to save the models and train multiple values; you can check out Raymond Langaeian (MSFT) answer in the comment section on this link:\n<a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-convert-training-experiment-to-scoring-experiment\/\" rel=\"nofollow noreferrer\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-convert-training-experiment-to-scoring-experiment\/<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1465904564903,
        "Answer_score":2.0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37807158",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":69135872,
        "Question_title":"How to install R packages into Azure Machine Learning",
        "Question_body":"<p>I have trained a model locally using the R package locfit. I am now trying to run this in Azure Machine Learning.<\/p>\n<p>Most guides\/previous questions appear to be in relation to Azure Machine Learning (classic). Although I believe the process outlined in similar posts will be similar (e.g. <a href=\"https:\/\/stackoverflow.com\/questions\/43176442\/install-r-packages-in-azure-ml\">here<\/a>, <a href=\"https:\/\/stackoverflow.com\/questions\/27568624\/installing-additional-r-package-on-azure-ml\">here<\/a>, I am still unable to get it to work.<\/p>\n<p>I have outlined the steps I have followed below:<\/p>\n<ol>\n<li><p>Download locfit R package for windows Zip file from <a href=\"https:\/\/cran.r-project.org\/web\/packages\/locfit\/index.html\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<\/li>\n<li><p>Put this downloaded Zip file into a new Zip file entitled &quot;locfit_package&quot;<\/p>\n<\/li>\n<li><p>I upload this &quot;locfit_package&quot; zip folder to AML as a dataset (Create Dataset &gt; From Local Files &gt; <strong>name<\/strong>: locfit_package <strong>dataset<\/strong> <strong>type<\/strong>: file &gt; Upload the zip (&quot;locfit_package&quot;) &gt; Confirm upload is correct<\/p>\n<\/li>\n<li><p>In the R terminal I then execute the following code:<\/p>\n<p>install.packages(&quot;src\/locfit_package.zip&quot;, lib = &quot;.&quot;, repos = NULL, verbose = TRUE)<\/p>\n<p>library(locfit_package, lib.loc=&quot;.&quot;, verbose=TRUE)<\/p>\n<p>library(locfit)<\/p>\n<\/li>\n<li><p>The following error message is then returned:<\/p>\n<p>system (cmd0): \/usr\/lib\/R\/bin\/R CMD INSTALL<\/p>\n<p>Warning: invalid package \u2018src\/locfit_package.zip\u2019\nError: ERROR: no packages specified\nWarning message:<\/p>\n<p>In install.packages(&quot;src\/locfit_package.zip&quot;, lib = &quot;.&quot;, repos = NULL,  : installation of package \u2018src\/locfit_package.zip\u2019 had non-zero exit status\nError in library(locfit_package, lib.loc = &quot;.&quot;, verbose = TRUE) : there is no package called \u2018locfit_package\u2019\nExecution halted<\/p>\n<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1631294643347,
        "Question_score":0,
        "Question_tags":"r|azure|azure-machine-learning-studio",
        "Question_view_count":411,
        "Owner_creation_time":1624530562660,
        "Owner_last_access_time":1639583055450,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69135872",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71023918,
        "Question_title":"ModuleNotFound Error - Azure ML with prebuilt docker image",
        "Question_body":"<p>I have developed a module which works perfectly when executed locally.<\/p>\n<p>I have created an environment on azure using a prebuilt docker image found here:\n<strong>&quot;azureml\/minimal-ubuntu18.04-py37-cpu-inference&quot;<\/strong> <a href=\"https:\/\/mcr.microsoft.com\/v2\/_catalog\" rel=\"nofollow noreferrer\">https:\/\/mcr.microsoft.com\/v2\/_catalog<\/a>\n. Also, Using pythonScriptStep, to run a pipeline. Here is how the step looks<\/p>\n<pre><code>StepPreprocessing = PythonScriptStep(\n    name=&quot;Preprocessing&quot;,\n    script_name=e.preprocess_script_path,\n    arguments=[\n        &quot;--config_path&quot;, e.preprocess_config_path,\n        &quot;--task&quot;, e.preprocess_task,\n    ],\n    inputs=None,\n    compute_target=aml_compute,\n    runconfig=run_config,\n    source_directory=e.sources_directory,\n    allow_reuse=False\n)\nprint(&quot;Step Preprocessing created&quot;)\n<\/code><\/pre>\n<p>This results in error:<\/p>\n<pre><code>Traceback (most recent call last):\n[stderr]  File &quot;Pipeline\/custom_pipeline.py&quot;, line 4, in &lt;module&gt;\n[stderr]    from Preprocess.logger import logger\n[stderr]ModuleNotFoundError: No module named 'Preprocess'\n<\/code><\/pre>\n<p>in the 1st line of entry script (<strong>custom_pipeline.py<\/strong>):<\/p>\n<pre><code>import sys\nsys.path.append(&quot;.&quot;) \nfrom Preprocess.logger import logger\n<\/code><\/pre>\n<p>The folder structure is as:<\/p>\n<pre><code>-Preprocess\n  -__init__.py\n  - Module1\n    -__init__.py\n    -somefile.py\n  - Module2\n    -__init__.py\n    -someOtherfile.py\n  - Pipeline\n    -__init__.py\n    -custom_pipeline.py\n  - logger\n    -__init__.py\n    -logger.py\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644260100200,
        "Question_score":0,
        "Question_tags":"python|azure|docker|azure-machine-learning-service",
        "Question_view_count":109,
        "Owner_creation_time":1443017464707,
        "Owner_last_access_time":1663923275743,
        "Owner_location":"Sweden",
        "Owner_reputation":644,
        "Owner_up_votes":33,
        "Owner_down_votes":1,
        "Owner_views":126,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I found out that the python script step copies everything inside the source_dir and therefore in my case it was copying the modules and not the root folder. So I had to put the dir Preprocess inside another dir and mention the new dir as source_dir.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1644604906183,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71023918",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":52603929,
        "Question_title":"AML Studio: Register mutliple gateways on the same server",
        "Question_body":"<p>I am struggling to find a way to register multiple gateways. I have a local instance of my SQL server and have created a gateway to access to it from the AML Studio workspace. It works fine but now I would like to access to the same SQL server instance from another workspace. So the question is: how to register a new gateway without removing the previous one?\nI followed this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/use-data-from-an-on-premises-sql-server\" rel=\"nofollow noreferrer\">documentation<\/a>.\nDoes the following explanation mean that there is no way to do that?<\/p>\n\n<blockquote>\n  <p>You can create and set up multiple gateways in Studio for each workspace. For example, you may have a gateway that you want to connect to your test data sources during development, and a different gateway for your production data sources. Azure Machine Learning gives you the flexibility to set up multiple gateways depending upon your corporate environment. Currently you can\u2019t share a gateway between workspaces and only one gateway can be installed on a single computer.<\/p>\n<\/blockquote>\n\n<p>It is quite limiting as connecting to the same server from multiple workspaces may be sometimes crucial.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1538466090420,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":40,
        "Owner_creation_time":1528790837107,
        "Owner_last_access_time":1660146049397,
        "Owner_location":"Paris, France",
        "Owner_reputation":610,
        "Owner_up_votes":143,
        "Owner_down_votes":0,
        "Owner_views":203,
        "Question_last_edit_time":1538485474347,
        "Answer_body":"<p>Well, finally I have found a way to bypass this limitation. From this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/use-data-from-an-on-premises-sql-server\" rel=\"nofollow noreferrer\">documentation<\/a> I have found that: <\/p>\n\n<blockquote>\n  <p>The IR does not need to be on the same machine as the data source. But staying closer to the data source reduces the time for the gateway to connect to the data source. We recommend that you install the IR on a machine that's different from the one that hosts the on-premises data source so that the gateway and data source don't compete for resources.<\/p>\n<\/blockquote>\n\n<p>So the  logic is pretty simple. You provide access to your local server to another machine on vpn and install your gateway there. Important: I have set up the firewall rules on the server before, to be able to establish the connection remotely.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1538648158569,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52603929",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":65083883,
        "Question_title":"Example for Azure AutoML Forecasting for time series with multiple covariate features",
        "Question_body":"<p>I would like to use Azure AutoML for <code>forecasting<\/code> where I have multiple features for one timeseries. Is there any example which I can replicate?<\/p>\n<p>I have been looking into: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-beer-remote\/auto-ml-forecasting-beer-remote.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-beer-remote\/auto-ml-forecasting-beer-remote.ipynb<\/a>\nand\n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-orange-juice-sales\/auto-ml-forecasting-orange-juice-sales.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-orange-juice-sales\/auto-ml-forecasting-orange-juice-sales.ipynb<\/a>\nbut no luck using multiple features instead of only one timeseries.<\/p>\n<p>Any help is greatly appreciated<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1606789538570,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":1231,
        "Owner_creation_time":1581649402283,
        "Owner_last_access_time":1623273118283,
        "Owner_location":"C\u00f3rdoba, Cordoba, Argentina",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65083883",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67057782,
        "Question_title":"Azure ML studio really slow",
        "Question_body":"<p>I had been using Azure ML studio for a while now and it was really fast but now when I try to unzip folders containing images around 3000 images using<\/p>\n<pre><code>!unzip &quot;file.zip&quot; -d &quot;to unzip directory&quot;\n<\/code><\/pre>\n<p>it took more than 30 minutes and other activities(longer concatenation methods) also seem to take a long time even using numpy arrays. Wondering if it is something with configuration or other problems. I have tried switching locations, creating new resource groups, workspaces, changing computes(Both CPU and GPU).<\/p>\n<p>Compute and other set of current configurations can be seen on the image<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/GNZao.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/GNZao.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1618227931020,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":185,
        "Owner_creation_time":1583404260763,
        "Owner_last_access_time":1663916998617,
        "Owner_location":"Ethiopia",
        "Owner_reputation":412,
        "Owner_up_votes":127,
        "Owner_down_votes":1,
        "Owner_views":43,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67057782",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62636399,
        "Question_title":"h2o models on Azure ML Containers",
        "Question_body":"<p>I have a requirement to deploy h2o models on Azure . I have successfully handled sklearn models but for sklearn the dependencies in my view are easier . For h2o the java runtime dependency is my bottle-neck.<\/p>\n<p>Will the container that i create will have java runtime ?&gt; Else what are the suggested strategies ?<\/p>\n<p>Should I go for a VM instead ?<\/p>\n<p>Thanks,<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1593427430617,
        "Question_score":1,
        "Question_tags":"containers|h2o|azure-machine-learning-service",
        "Question_view_count":84,
        "Owner_creation_time":1396958226127,
        "Owner_last_access_time":1664040884423,
        "Owner_location":"Mumbai, India",
        "Owner_reputation":169,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62636399",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58478542,
        "Question_title":"Import azure.core not found issue in running Notebook through MachineLearningStudio",
        "Question_body":"<p>I was trying to run a sample tutorial notebook through the ml studio. <\/p>\n\n<p><a href=\"https:\/\/notebooks.azure.com\/azureml\/projects\/azureml-getting-started\/html\/tutorials\/img-classification-part1-training.ipynb\" rel=\"nofollow noreferrer\">https:\/\/notebooks.azure.com\/azureml\/projects\/azureml-getting-started\/html\/tutorials\/img-classification-part1-training.ipynb<\/a><\/p>\n\n<p>But when i uploaded i used kernel python3. But when i ran it failed with the error azureml.core not found.<\/p>\n\n<p>I am new to Azure Stack and ML. Should i install python 3.6 on my own through conda and have my own kernel, i noticed the current installation of python on studio is 3.4.<\/p>\n\n<p>Please let me know how to proceed further ? I am blocked on it. I need help on deploying the 3.6 version of python on the notebook server. I am not using the notebook vm. I am just using whatever came with the azure notebook option in the ml studio.<\/p>\n\n<p>How to alter the sys path to point to my libraries to after installation of the new version of python ?<\/p>\n\n<p>Need help.<\/p>\n\n<p>It works fine in my local environment as i have python3.6 installed. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1571618889040,
        "Question_score":0,
        "Question_tags":"python-3.x|azure-machine-learning-studio|azure-machine-learning-service|azure-machine-learning-workbench",
        "Question_view_count":143,
        "Owner_creation_time":1484215677137,
        "Owner_last_access_time":1643607883400,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58478542",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67900417,
        "Question_title":"Azure Machine Learning - authorization error despite having maximum permissions",
        "Question_body":"<p>I am trying to build a machine learning model on Azure for my company. The IT team at the company I work at has given me maximum permissions for our Azure Machine Learning account since I am doing all the setup part (we started using it only last month). However, I checked the portal and realized that I am not authorized to access any of the modules within Azure ML, namely Experiment, Models, Endpoints, Datasets, etc. Is there something I am missing that is giving me this error? The error message has this <a href=\"https:\/\/go.microsoft.com\/fwlink\/?linkid=2161335\" rel=\"nofollow noreferrer\">link<\/a> but I am not sure it serves the purpose.<\/p>\n<p><strong>Note:<\/strong> I am new to Azure so please forgive me if this is a very basic doubt.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oPvpP.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oPvpP.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1623227559873,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-service",
        "Question_view_count":251,
        "Owner_creation_time":1513143629087,
        "Owner_last_access_time":1664079380110,
        "Owner_location":"Seattle, Washington",
        "Owner_reputation":53,
        "Owner_up_votes":108,
        "Owner_down_votes":0,
        "Owner_views":21,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67900417",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58237929,
        "Question_title":"Slow data transfer from Azure Blob Storage to compute target",
        "Question_body":"<p>It's taking 1 hour to download a 48gb dataset with 90000 files.\nI am training an image segmentation model on Azure ML pipeline using compute target p100-nc6s-v2.\nIn my script I'm accessing Azure Blob Storage using DataReference's as_download() functionality. The blob storage is in the same location as workspace (using get_default_datastore).<\/p>\n\n<p><strong>Note:<\/strong> I'm able to download complete dataset to local workstation within a few minutes using <code>az copy<\/code>.<\/p>\n\n<p>When I tried to use as_mount() the first epoch was extremely slow (4700 seconds vs 772 seconds for subsequent epochs).<\/p>\n\n<p>Is this expected behavior? If not, what can be done to improve dataset loading speed?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1570197733587,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-service",
        "Question_view_count":936,
        "Owner_creation_time":1404997159583,
        "Owner_last_access_time":1617571280343,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1570203301407,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58237929",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73187536,
        "Question_title":"Error while detaching AKS cluster through Azure ML SDK extension",
        "Question_body":"<p>I created an AKS cluster using Azure Machine Learning SDK extension and I attached to the workspace created. When the cluster is created and attached, I doesn't show any error. When I am trying to detach it from workspace, it is not accepting the operations.<\/p>\n<p>I would like to detach the existing AKS cluster from workspace either by program manner, using CLI or even using Azure portal.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659308799773,
        "Question_score":0,
        "Question_tags":"azure|azure-aks|azure-machine-learning-studio",
        "Question_view_count":47,
        "Owner_creation_time":1651093614703,
        "Owner_last_access_time":1659335138797,
        "Owner_location":"Netherland",
        "Owner_reputation":19,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If we are using any <strong>extensions of SDK or Azure CLI<\/strong> for machine learning to detach AKS cluster, it <strong>will not work<\/strong> and it will not get deleted or detached. Instead, we need to use <strong>Azure CLI with AKS<\/strong>. There are two types of implementations we can perform.<\/p>\n<p><strong>Python:<\/strong><\/p>\n<pre><code>Aks_target.detach()\n<\/code><\/pre>\n<p><strong>Azure CLI:<\/strong><\/p>\n<p>Before performing this step, we need to get the details of the working AKS cluster name attached to our workspace. Resource Group details and workspace name<\/p>\n<pre><code>az ml computertarget detach -n youraksname -g yourresourcegroup -w yourworkspacename\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1659333006089,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73187536",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":54433767,
        "Question_title":"how to add new R packages in azure machine learning for time series anomaly detection",
        "Question_body":"<p>I am trying to find out time series anomaly detection in which i need to install new R packages. In this i m following <a href=\"https:\/\/github.com\/business-science\/anomalize\" rel=\"nofollow noreferrer\">https:\/\/github.com\/business-science\/anomalize<\/a> site. In this i needed to install 2 packages: <code>tidyverse<\/code> and <code>anomalize<\/code>.<\/p>\n\n<ol>\n<li><p>can anyone help me on installing package mentioned above as I am getting <\/p>\n\n<blockquote>\n  <p>error \"package or namespace load failed for tidyverse\"<\/p>\n<\/blockquote><\/li>\n<li><p>Also while adding zip of <code>tidyverse<\/code> and <code>anomalize<\/code> do I need to add any other packages and dependencies in that as I am adding only those 2 packages thinking there r no other dependencies I needed for those 2?<\/p><\/li>\n<\/ol>\n\n<p>you can see in code that I created <code>R_Package.zip<\/code> and put <code>tidyverse.zip<\/code> and <code>anomalize.zip<\/code> in that that <\/p>\n\n<pre><code>dataset1 &lt;- maml.mapInputPort(1)\ndata.set &lt;- data.frame(installed.packages())\n#install.packages(\u201csrc\/R_Package\/tidyverse_1.2.1.zip\u201d, lib = \u201c.\u201d, \n                  repos = NULL, verbose = TRUE);\n#library(tidyverse, lib.loc=\u201d.\u201d, verbose=TRUE);\n\ninstall.packages(\"src\/tidyverse.zip\",lib=\".\",repos=NULL,verbose=TRUE)\nlibrary(R_package, lib.loc = \".\", verbose=TRUE);\n\ninstall.packages(\"src\/anomalize.zip\",lib=\".\",repos=NULL,verbose=TRUE)\nlibrary(R_package, lib.loc = \".\", verbose=TRUE);\n\n#success &lt;- library(\"tidyverse\", lib.loc = \".\", \n                    logical.return = TRUE, verbose = TRUE)\n#library(tidyverse)\n\n\nmaml.mapOutputPort(\"dataset1\");\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1548825762143,
        "Question_score":0,
        "Question_tags":"r|azure-machine-learning-studio",
        "Question_view_count":168,
        "Owner_creation_time":1492081349833,
        "Owner_last_access_time":1556108056507,
        "Owner_location":null,
        "Owner_reputation":19,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Question_last_edit_time":1548834576212,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54433767",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64597526,
        "Question_title":"Provision AKS with internal load balancer from AMLS on Azure",
        "Question_body":"<p>I would like to provision an AKS cluster that is connected to a vnet and has an internal load balancer on Azure. I am using code from <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-inferencing-vnet?tabs=python\" rel=\"nofollow noreferrer\">here<\/a> that looks like this:<\/p>\n<pre><code>import azureml.core\nfrom azureml.core.compute import AksCompute, ComputeTarget\n\n# Verify that cluster does not exist already\ntry:\n    aks_target = AksCompute(workspace=ws, name=aks_cluster_name)\n    print(&quot;Found existing aks cluster&quot;)\n\nexcept:\n    print(&quot;Creating new aks cluster&quot;)\n\n    # Subnet to use for AKS\n    subnet_name = &quot;default&quot;\n    # Create AKS configuration\n    prov_config=AksCompute.provisioning_configuration(load_balancer_type=&quot;InternalLoadBalancer&quot;)\n    # Set info for existing virtual network to create the cluster in\n    prov_config.vnet_resourcegroup_name = &quot;myvnetresourcegroup&quot;\n    prov_config.vnet_name = &quot;myvnetname&quot;\n    prov_config.service_cidr = &quot;10.0.0.0\/16&quot;\n    prov_config.dns_service_ip = &quot;10.0.0.10&quot;\n    prov_config.subnet_name = subnet_name\n    prov_config.docker_bridge_cidr = &quot;172.17.0.1\/16&quot;\n\n    # Create compute target\n    aks_target = ComputeTarget.create(workspace = ws, name = &quot;myaks&quot;, provisioning_configuration = prov_config)\n    # Wait for the operation to complete\n    aks_target.wait_for_completion(show_output = True)\n<\/code><\/pre>\n<p>However, I get the following error<\/p>\n<pre><code>K8s failed to assign an IP for Load Balancer after waiting for an hour.\n<\/code><\/pre>\n<p>Is this because the AKS cluster does not yet have a 'network contributor' role for the vnet resource group? Is the only way to get this to work to first create AKS outside of AMLS, grant the network contributor role to the vnet resource group, then attach the AKS cluster to AMLS and configure the internal load balancer afterwards?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603997735143,
        "Question_score":1,
        "Question_tags":"azure-aks|azure-machine-learning-service|vnet|internal-load-balancer",
        "Question_view_count":352,
        "Owner_creation_time":1589738451347,
        "Owner_last_access_time":1656358607687,
        "Owner_location":null,
        "Owner_reputation":179,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":53,
        "Question_last_edit_time":1604002322987,
        "Answer_body":"<p>I was able to get this to work by first creating an AKS resource without an internal load balancer, then separately updating the load balancer following this code:<\/p>\n<pre><code>import azureml.core\nfrom azureml.core.compute.aks import AksUpdateConfiguration\nfrom azureml.core.compute import AksCompute\n\n# ws = workspace object. Creation not shown in this snippet\naks_target = AksCompute(ws,&quot;myaks&quot;)\n\n# Change to the name of the subnet that contains AKS\nsubnet_name = &quot;default&quot;\n# Update AKS configuration to use an internal load balancer\nupdate_config = AksUpdateConfiguration(None, &quot;InternalLoadBalancer&quot;, subnet_name)\naks_target.update(update_config)\n# Wait for the operation to complete\naks_target.wait_for_completion(show_output = True)\n<\/code><\/pre>\n<p>No network contributor role was required.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1604359053332,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64597526",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":65781409,
        "Question_title":"AzureMLCompute job failed: container registry failed unexpectedly: container setup task failed",
        "Question_body":"<p>Could you please help me with running python script in azureml environment? I created the workspace and azure container registry and pushed docker image to the container. This is the example of dockerfile:<\/p>\n<pre><code>FROM python:3.7\n\nRUN pip install --upgrade pip\n\nRUN pip install virtualenv\n\nENV VIRTUAL_ENV=\/venv\n\nRUN virtualenv venv -p python3\n\nENV PATH=&quot;VIRTUAL_ENV\/bin:$PATH&quot;\n\nWORKDIR \/app\n\nADD . \/app\n\nENV PYTHON_PACKAGES=&quot;\\\nnumpy \\\npandas \\\nseaborn \\\nmatplotlib \\\nsklearn \\\nscipy \\\nimbalanced-learn \\\nxgboost \\\njoblib \\\n&quot; \n\nRUN pip install --no-cache-dir $PYTHON_PACKAGES\n\nENTRYPOINT [&quot;python3&quot;,&quot;train.py&quot;]\n<\/code><\/pre>\n<p>I created environment like this:\n<code>myenv = Environment.from_pip_requirements(name = ws.get_details()['name'],file_path = &quot;requirements.txt&quot;)<\/code><\/p>\n<p>When I run the experiment I get this error:<\/p>\n<p><code>&quot;Message&quot;: &quot;AzureMLCompute job failed.\\nJobContainerConfigFailed: Container configuration failed unexpectedly\\n\\tJobContainerConfigFailed: Container configuration failed unexpectedly\\n\\terr: container setup task failed: exit status 1\\n\\tReason: container setup task failed: exit status 1\\n\\tInfo: Failed to prepare an environment for the job execution: Job environment preparation failed on 10.0.0.5 with err exit status 1.&quot;<\/code><\/p>\n<p>I do not understand what this error mean.<\/p>\n<p>I configure and submit training job with following:<\/p>\n<pre><code>    src = ScriptRunConfig(source_directory='.',\n                        script='train.py',\n                        compute_target=cpu_cluster,\n                        environment=myenv)\n    \n    #Submit training job\n    run = Experiment(ws,'test-classification').submit(src)\n    run.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>Thank you!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1611000359923,
        "Question_score":0,
        "Question_tags":"azure|docker|azure-devops|dockerfile|azure-machine-learning-service",
        "Question_view_count":221,
        "Owner_creation_time":1395179633740,
        "Owner_last_access_time":1623677760737,
        "Owner_location":null,
        "Owner_reputation":179,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":1611002029852,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65781409",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":42458982,
        "Question_title":"Understanding classification results",
        "Question_body":"<p>I've run a simple two-class neural network where I got this result in the end (eval):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/vw2jQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vw2jQ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I think I am going to be happy with the <code>True Positive<\/code> and <code>False Negative<\/code> results. But what does <code>False Positive<\/code> mean? <code>False Positive<\/code> means it did not correctly classify 2002 elements and missed them?<\/p>\n\n<p>The <code>Accuracy<\/code> is 66%, that's really bad right? Whats the difference between that and <code>AUC<\/code>?<\/p>\n\n<p><code>Precision<\/code> suffers because Accuracy also is bad (I hoping for a 80%+)?<\/p>\n\n<p>And how do I flip <code>Positive Label<\/code> and <code>Negative Label<\/code>? I really want to predict the classification where the target is to find <code>CANDIDATE<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1488042320197,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":81,
        "Owner_creation_time":1267709938320,
        "Owner_last_access_time":1508513881653,
        "Owner_location":"Norway",
        "Owner_reputation":13032,
        "Owner_up_votes":516,
        "Owner_down_votes":8,
        "Owner_views":1004,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Basically, for the false\/true positives and false\/true negatives :\nYou have detected almost all the CANDIDATE samples in your dataset, 3420 of them were correctly predicted as TRUE and 31 of them were predicted as FALSE. This information is captured in the Recall ratio : 3420\/(3420+31) = 99.1%. It is very high, so very good. <\/p>\n\n<p>However, you have predicted <strong>too many<\/strong> CANDIDATE. Indeed, in all the TRUE values predicted by the model, 3420 were actually TRUE and 2002 were actually FALSE. This makes the Precision ratio bad : 3420\/(3420+2002)=63.1%. Which is not that good. <\/p>\n\n<p>F1 is a combinaison between Precision and Recall, it summarizes them into one value, some kind of weighted average. The formula is 2*(P*R)\/(P+R). So if one of Precision or Recall is bad : the F1score will capture it. <\/p>\n\n<p>You can see that you have a total of 5999 examples in your data set. Out of those, 3451 are really TRUE and 2548 are really FALSE. So you have 57% of your data that is TRUE. If you make a really stupid classifier that classifies everything as TRUE whatever the features are, then you will get 57% accuracy. Given that, 66.1% accuracy is not really good. \nIf you look at the second column of that table, you only predict 577 FALSE out of the 5999 samples. Your classifier is heavily biased towards TRUE predictions. <\/p>\n\n<p>For the AUC, it stands for Area Under the Curve. You can read <a href=\"http:\/\/fastml.com\/what-you-wanted-to-know-about-auc\/\" rel=\"nofollow noreferrer\">more detailed info about it here<\/a>. To summarize : when you predic a value, you don't really get True or False directly. You get a real number between 0 (False) and 1 (True). The way to classify a predicted value, say 0.2, is to use a Threshold. The threshold is by default set to 0.5. So if you predict 0.2, your model will predict to classify it as a False because 0.2&lt;0.5. But you could make that treshold move between 0 and 1. If the classifier is really good, if it discriminates really well the Falses and Trues predictions, then the AUC will be close to 1. If it's really bad, it will be close to 0.5. Refer to the link if you need more information. <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1488062187216,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1488063664327,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/42458982",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":36260727,
        "Question_title":"Equivalent of Subset in Azure machine learning studio",
        "Question_body":"<p>I have a dataset in azure machine learning (.csv), on the same dataset I have multiple models build, I want to subset data for each of the model based on a different column<\/p>\n\n<p>Input:<\/p>\n\n<pre><code>ID col1 col2 col3\n1  0    13   0\n2  5    45   0\n3  10   0    34\n4  12   1    3\n<\/code><\/pre>\n\n<p>For the 1st model I want to retain all records where col1 not equal to None<\/p>\n\n<pre><code>ID col1 col2 col3\n2  5    45   0\n3  10   0    34\n4  12   1    3\n<\/code><\/pre>\n\n<p>Similarly for model 2<\/p>\n\n<pre><code>ID col1 col2 col3\n1  0    13   0\n2  5    45   0\n4  12   1    3\n<\/code><\/pre>\n\n<p>Hope it was clear<\/p>\n\n<p>The equivalent in R would be <\/p>\n\n<pre><code>df[!df$col1 == \"None\",] \n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1459161991533,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":243,
        "Owner_creation_time":1406266059940,
        "Owner_last_access_time":1663232189147,
        "Owner_location":"Link\u00f6ping, Sweden",
        "Owner_reputation":1677,
        "Owner_up_votes":82,
        "Owner_down_votes":2,
        "Owner_views":221,
        "Question_last_edit_time":1459256566467,
        "Answer_body":"<p>You can use the \"Execute R Script\" module and just plug in your R code there.<\/p>\n\n<pre><code>df &lt;- maml.mapInputPort(1)\ndf &lt;- df[!df$col1 == \"None\",] \nmaml.mapOutputPort(\"df\");\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1461479422230,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36260727",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":47286717,
        "Question_title":"Fuzzy matching of user generated dataset to stored datasets",
        "Question_body":"<p>I have stored workflows. They're trees with decision points.  Basically every point in the data is a command issued. Adding it all up is a workflow to build something from the commands.<\/p>\n\n<p>I'm trying to use azure ml to take a partially completed workflow from a user and match it against these stored workflows.  <\/p>\n\n<p>Adding to the difficulty is that I'm never sure when the user has started or stopped a workflow so it's always a Time preference match that is never perfect.<\/p>\n\n<p>Despite days of searching I can't find any information on canned algorithms that do this type of pattern matching.<\/p>\n\n<p>Can someone suggest where I might find some information on taking a data series (not numeric) and match that to a tree graph of similar values in real time?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1510664982930,
        "Question_score":1,
        "Question_tags":"azure-hdinsight|azure-data-lake|azure-machine-learning-studio",
        "Question_view_count":65,
        "Owner_creation_time":1341934714620,
        "Owner_last_access_time":1663955765103,
        "Owner_location":null,
        "Owner_reputation":3228,
        "Owner_up_votes":94,
        "Owner_down_votes":2,
        "Owner_views":355,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47286717",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":69718265,
        "Question_title":"Azure ML problem with installation fastai with conda",
        "Question_body":"<p>I am struggling to install the fastbook package for fastai in Azure ML. Below is the error I am getting.<\/p>\n<p>I have created a new environment, and all other fast ai packages are installed.<\/p>\n<pre><code>Found conflicts! Looking for incompatible packages.\nThis can take several minutes.  Press CTRL-C to abort.\nfailed      \n                                                                                                                                              \nUnsatisfiableError: The following specifications were found to be incompatible with each other:\nOutput in format: Requested package -&gt; Available versions\nThe following specifications were found to be incompatible with your system:\n  - feature:\/linux-64::__glibc==2.27=0\n  - python=3.9 -&gt; libgcc-ng[version='&gt;=7.5.0'] -&gt; __glibc[version='&gt;=2.17']\nYour installed version is: 2.27\n<\/code><\/pre>\n<p>The command line code is below:<\/p>\n<pre><code> conda install -y pip\n conda install -y ipykernel\n conda install -y -c fastai -c pytorch fastai\n conda install -y -c fastai fastbook\n conda install -y -c fastai nbdev\n<\/code><\/pre>\n<p>I only get the error for the fastbook package<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1635229213020,
        "Question_score":0,
        "Question_tags":"python|conda|fast-ai|azure-machine-learning-service",
        "Question_view_count":124,
        "Owner_creation_time":1620735146277,
        "Owner_last_access_time":1663768429990,
        "Owner_location":"South Africa",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1635234928607,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69718265",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":65363117,
        "Question_title":"Connecting to AMLS workspace with private endpoint using Python SDK and proxy",
        "Question_body":"<p>I have created an AMLS workspace with a private endpoint in a vnet that is peered into a company's network. I would like to access the AMLS workspace using a company VM (Windows) to add compute, add datastores, deploy models, etc. using the Azure ML Python SDK.<\/p>\n<p>The company VM has a proxy in place which initially was blocking my ability to even log in to the AMLS workspace. After adding an HTTPS_PROXY environment variable and configuring the powershell profile for the proxy server, I can now log in to Azure from powershell (<code>az login<\/code>) and I can access the AMLS workspace using Python (<code>ws = Workspace.get()<\/code>, <code>ws.get_details()<\/code>, etc.). However, as soon as I try to access any of the resources (e.g.<code> ws.get_default_datastore()<\/code>, <code>cluster = ComputeTarget(workspace=ws, name=cluster_name)<\/code>) I get SSL certificate verification errors:<\/p>\n<pre><code>requests.exceptions.SSLError: HTTPSConnectionPool(host='&lt;WORKSPACE_ID&gt;.workspace.canadacentral.api.azureml.ms', port=443): Max retries\n exceeded with url: \/discovery\/workspaces\/&lt;WORKSPACE_ID&gt; (Caused by SSLError(SSLCertVerificationError(1,\n '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1091)')))\n<\/code><\/pre>\n<p>There is a similar problem mentioned here: <a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/1089\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/1089<\/a>, but I'm still unsure how to solve this. What is the best approach here? Should I turn off certificate validation in the Azure ML Python SDK? If so, how?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1608320952537,
        "Question_score":0,
        "Question_tags":"python|ssl|proxy|azure-machine-learning-service",
        "Question_view_count":1139,
        "Owner_creation_time":1589738451347,
        "Owner_last_access_time":1656358607687,
        "Owner_location":null,
        "Owner_reputation":179,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":53,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65363117",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64599174,
        "Question_title":"Can't use wildcard with Azure Data Lake Gen2 files",
        "Question_body":"<p>I was able to properly connect my Data Lake Gen2 Storage Account with my Azure ML Workspace. When trying to read a specific set of Parquet files from the Datastore, it will take forever and will not load it.<\/p>\n<p>The code looks like:<\/p>\n<pre><code>from azureml.core import Workspace, Datastore, Dataset\nfrom azureml.data.datapath import DataPath\n\nws = Workspace(subscription_id, resource_group, workspace_name)\n\ndatastore = Datastore.get(ws, 'my-datastore')\n\nfiles_path = 'Brazil\/CommandCenter\/Invoices\/dt_folder=2020-05-11\/*.parquet'\n\ndataset = Dataset.Tabular.from_parquet_files(path=[DataPath(datastore, files_path)], validate=False)\ndf = dataset.take(1000)\n\ndf.to_pandas_dataframe()\n<\/code><\/pre>\n<p>Each of these Parquet files have approx. 300kB. There are 200 of them on the folder - generic and straight out of Databricks. Strange is that when I try and read one single parquet file from the exact same folder, it runs smoothly.<\/p>\n<p>Second is that other folders that contain less than say 20 files, will also run smoothly, so I eliminated the possibility that this was due to some connectivity issue. And even stranger is that I tried the wildcard like the following:<\/p>\n<pre><code># files_path = 'Brazil\/CommandCenter\/Invoices\/dt_folder=2020-05-11\/part-00000-*.parquet'\n<\/code><\/pre>\n<p>And theoretically this will only direct me to the <code>00000<\/code> file, but it will also not load. Super weird.<\/p>\n<p>To try to overcome this, I have tried to connect to the Data Lake through ADLFS with Dask, and it just works. I know this can be a workaround for processing &quot;large&quot; datasets\/files, but it would be super nice to do it straight from the Dataset class methods.<\/p>\n<p>Any thoughts?<\/p>\n<p>EDIT: typo<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1604005397523,
        "Question_score":1,
        "Question_tags":"azure|machine-learning|azure-machine-learning-service",
        "Question_view_count":403,
        "Owner_creation_time":1508924024027,
        "Owner_last_access_time":1663957908303,
        "Owner_location":null,
        "Owner_reputation":118,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64599174",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":40018320,
        "Question_title":"Is it secure to pass the DB query to AzureML as a global parameter?",
        "Question_body":"<p>When using AzureMLBatchExecution activity in Azure Data Factory, is it secure to pass the DB query as a global parameter to the AzureML web service? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1476354083450,
        "Question_score":2,
        "Question_tags":"azure|azure-data-factory|azure-machine-learning-studio",
        "Question_view_count":68,
        "Owner_creation_time":1452608563363,
        "Owner_last_access_time":1562103924250,
        "Owner_location":null,
        "Owner_reputation":105,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When you talk about \"secure\", are you worried about secure transmission between AML and ADF, or secure storage of your DB query information? For the former, all communication between these two services will be done with HTTPS. For the latter, our production storage has its strict access control. Besides, we only log the count of the global parameters and never the values. I believe it's secure to pass your DB query as a global parameter to the AzureML web service.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1476431199480,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40018320",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71203995,
        "Question_title":"RuntimeError: Java gateway process exited before sending its port number when Deploying Pyspark model to Azure Container Instance",
        "Question_body":"<p>I am trying to deploy a PySpark model trained in Azure Databricks with MLflow to an ACI in Azure Machine Learning.<\/p>\n<p>I am following the steps in this link:<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-mlflow-models#example-notebooks\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-mlflow-models#example-notebooks<\/a><\/p>\n<p>but I get this error:<\/p>\n<pre><code>SPARK_HOME not set. Skipping PySpark Initialization.\nInitializing logger\n2022-02-21 09:29:30,269 | root | INFO | Starting up app insights client\nlogging socket was found. logging is available.\nlogging socket was found. logging is available.\n2022-02-21 09:29:30,270 | root | INFO | Starting up request id generator\n2022-02-21 09:29:30,270 | root | INFO | Starting up app insight hooks\n2022-02-21 09:29:30,270 | root | INFO | Invoking user's init function\nJAVA_HOME is not set\n2022-02-21 09:29:31,267 | root | ERROR | User's init function failed\n2022-02-21 09:29:31,268 | root | ERROR | Encountered Exception Traceback (most recent call last):\n  File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 191, in register\n    main.init()\n  File &quot;\/var\/azureml-app\/execution_script.py&quot;, line 15, in init\n    model = load_model(model_path)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 667, in load_model\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/mlflow\/spark.py&quot;, line 703, in _load_pyfunc\n    pyspark.sql.SparkSession.builder.config(&quot;spark.python.worker.reuse&quot;, True)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/sql\/session.py&quot;, line 228, in getOrCreate\n    sc = SparkContext.getOrCreate(sparkConf)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/context.py&quot;, line 392, in getOrCreate\n    SparkContext(conf=conf or SparkConf())\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/context.py&quot;, line 144, in __init__\n    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/context.py&quot;, line 339, in _ensure_initialized\n    SparkContext._gateway = gateway or launch_gateway(conf)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/java_gateway.py&quot;, line 108, in launch_gateway\n    raise RuntimeError(&quot;Java gateway process exited before sending its port number&quot;)\nRuntimeError: Java gateway process exited before sending its port number\n<\/code><\/pre>\n<p>My code looks like this:<\/p>\n<pre><code>from mlflow.deployments import get_deploy_client\n\n# set the tracking uri as the deployment client\nclient = get_deploy_client(mlflow.get_tracking_uri())\n\n# set the model path \nmodel_path = &quot;k_means_model&quot;\n\n    # define the model path and the name is the service name\n    # the model gets registered automatically and a name is autogenerated using the &quot;name&quot; parameter below \n    client.create_deployment(model_uri='runs:\/{}\/{}'.format(run_id, model_path), name = 'k-means-model-ml-flow')\n<\/code><\/pre>\n<p>While my model settings are:<\/p>\n<pre><code>artifact_path: k_means_model\ndatabricks_runtime: 10.3.x-cpu-ml-scala2.12\nflavors:\n  python_function:\n    data: sparkml\n    env: conda.yaml\n    loader_module: mlflow.spark\n    python_version: 3.8.10\n  spark:\n    model_data: sparkml\n    pyspark_version: 3.2.1\nmodel_uuid: 76ba9dfb01e1428ab8145a161ec3cf32\nrun_id: c0090fa9-b382-45b8-be08-d05e16f3cd62\nutc_time_created: '2022-02-21 08:47:34.967167'\n<\/code><\/pre>\n<p>Can someone help please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1645436044400,
        "Question_score":1,
        "Question_tags":"pyspark|azure-databricks|azure-machine-learning-service|mlflow",
        "Question_view_count":289,
        "Owner_creation_time":1635865186583,
        "Owner_last_access_time":1657599189513,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1645439952347,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71203995",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":29647782,
        "Question_title":"Azureml Web Service - How to create a Rest Service from an Experiment to be consumed by a mobile app?",
        "Question_body":"<p>I've looked all over the google and stackoverflow for the answer but I cant seem to find it. I'm trying to get the output from an azure experiment to an app. I've made the app using <code>ibuildapp<\/code> and google forms. How can I use the inputs from the google form, pass it to azure and get an output to display on the app?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1429093608083,
        "Question_score":1,
        "Question_tags":"json|web-services|azure|webforms|azure-machine-learning-studio",
        "Question_view_count":603,
        "Owner_creation_time":1405675523040,
        "Owner_last_access_time":1494238760807,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1429213713289,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/29647782",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":38937247,
        "Question_title":"Separating a tree Regression Model based on unique values of one column",
        "Question_body":"<p>I have a data set of 20,000,000 rows. Each row has 30 columns.<\/p>\n\n<p>One of the columns contains 7000 unique Product Numbers. <\/p>\n\n<p>Each row contains a Unit Cost value that I would like to predict using all the columns other than the Unit Cost.<\/p>\n\n<p>I would like to build a unique decision tree or a unique branch of a decision tree to model the data for each Product Number.  <\/p>\n\n<p>Basically partitioning the rows for each Product Number and modelling each Product Number in isolation.<\/p>\n\n<p>I would like to train a single model in Azure to do this if possible.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1471124448550,
        "Question_score":0,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio",
        "Question_view_count":74,
        "Owner_creation_time":1471124166630,
        "Owner_last_access_time":1471127961390,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1499698390287,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/38937247",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":63920599,
        "Question_title":"PowerBI and MLflow integration (through AzureML)",
        "Question_body":"<p>I'm currently trying to integrate an ML model currently deployed as a webservice on AzureML with PowerBI.<\/p>\n<p>I see that it can be <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#invoking-the-azure-ml-model-in-power-bi\" rel=\"nofollow noreferrer\">integrated<\/a> but the model requires the addition of a schema file when it is <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#schema-discovery-for-machine-learning-models\" rel=\"nofollow noreferrer\">being deployed as a webservice<\/a>. Without this, the model can't be viewed in PowerBI.<\/p>\n<p>The problem that I have come up against is that I use MLflow to log ML model performances and subsequently to deploy a selected model onto AzureML as a webservice using MLflow's AzureML integration - mlflow.azureml.deploy(). This unfortunately doesn't have the option to define a schema file before the model is deployed, thus resulting in no model being available in PowerBI as it lacks the required schema file.<\/p>\n<p>My options seem to be:<\/p>\n<ol>\n<li>Find a workaround, possibly using the working <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-serving\" rel=\"nofollow noreferrer\">REST api of the model in a power query<\/a>.<\/li>\n<li>Rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow.<\/li>\n<\/ol>\n<p>I thought I would ask to see if I am maybe missing something as I can't find a workaround using my current code to define a schema file in MLflow when deploying with mlflow.azureml.deploy().<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1600261190477,
        "Question_score":0,
        "Question_tags":"azure|powerbi|mlflow|azure-machine-learning-service",
        "Question_view_count":405,
        "Owner_creation_time":1600260166047,
        "Owner_last_access_time":1615561616230,
        "Owner_location":null,
        "Owner_reputation":15,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1600855880503,
        "Answer_body":"<p>Point number 2 is the way we solved this issue. Instead of using MLflow to deploy to a scoring service on Azure, we wrote a custom code which load MLflow model when container is initialised.<\/p>\n<p>Scoring code is something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nfrom mlflow.pyfunc import load_model\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\ndef init():\n    global model\n    model = load_model(os.path.join(os.environ.get(&quot;AZUREML_MODEL_DIR&quot;), &quot;awesome_model&quot;))\n\n@input_schema('data', NumpyParameterType(input_sample))\n@output_schema(NumpyParameterType(output_sample))\n\ndef run(data):\n    return model.predict(data)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1600604920243,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1600855957376,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63920599",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":68208679,
        "Question_title":"Run script locally with remote dataset on AzureML",
        "Question_body":"<p>I have a script that for development purposes I would like to run and debug locally. However, I do not want to store the data needed for my experiment on my local machine.<\/p>\n<p>I am using the <code>azureml<\/code> library with the Azure Machine Learning Studio. See my code below<\/p>\n<pre><code># General\nimport os\nimport argparse\n\n# Data analysis and wrangling\nimport pandas as pd\n\n# Machine learning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom azureml.core import Run\n\n# Get the environment of this run\nrun = Run.get_context()\n\nif __name__ == &quot;__main__&quot;:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--data_path',\n        type=str,\n        help='Path to the training data',\n        # The default path is on my local machine, however I would like to reference a remote datastore on Azure as a parameter to this script\n        default=os.path.join(os.getcwd(), 'data')\n    )\n    args = parser.parse_args()\n\n    # Obtain the data from the datastore\n    train_df = pd.read_csv(os.path.join(args.data_path, os.listdir(args.data_path)[0]))\n\n    # Drop unnecessary columns\n    train_df = train_df.drop(['Name', 'PassengerId', 'Ticket', 'Cabin'], axis=1)\n\n    # Encode non-numeric features as dummies\n    train_df = pd.get_dummies(train_df)\n\n    # Drop NA's\n    train_df.dropna(inplace=True)\n\n    # Use gridsearch CV to find the best parameters for the model\n    parameters = {'kernel': ('linear', 'rbf'),\n                  'C': [1, 10]}\n\n    # Initialize the grid search\n    search = GridSearchCV(SVC(), param_grid=parameters, cv=8)\n\n    # Train the model\n    search.fit(train_df.drop(&quot;Survived&quot;, axis=1), train_df[&quot;Survived&quot;])\n\n<\/code><\/pre>\n<p>Now, the script uses a local folder 'data'. However, I would like to give an argument to this script that indicates I would like to use a remote datastore in the Azure Machine Learning Studio. How could I achieve that?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625135942170,
        "Question_score":1,
        "Question_tags":"python|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":245,
        "Owner_creation_time":1586168427240,
        "Owner_last_access_time":1663943079773,
        "Owner_location":null,
        "Owner_reputation":299,
        "Owner_up_votes":44,
        "Owner_down_votes":2,
        "Owner_views":21,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68208679",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":69332418,
        "Question_title":"AzureML: Engine process terminated. This is most likely due to system running out of memory. Please retry with increased memory",
        "Question_body":"<p>I am trying to run an experiment on AzureML through the notebook. I get the above error on trying to read a dataset created in previous step.<\/p>\n<p>I checked the memory usage through command - <code>df -h<\/code> and it looks ok. I checked git links with same error, but that doesn't appear to have been resolved.<\/p>\n<p>Github issues <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1143\" rel=\"nofollow noreferrer\">link<\/a><\/p>\n<p>What is going wrong here?<\/p>\n<p>Below line of code gives the error. This line had run successfully just a day ago on same workspace, using same compute.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/vynQe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vynQe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Below is the screen of memory:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Mv3M4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Mv3M4.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1632638161787,
        "Question_score":2,
        "Question_tags":"azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":513,
        "Owner_creation_time":1425952092993,
        "Owner_last_access_time":1657798143527,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":427,
        "Owner_up_votes":28,
        "Owner_down_votes":1,
        "Owner_views":77,
        "Question_last_edit_time":1632638526012,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69332418",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67065023,
        "Question_title":"Max R version of Machine Learning Server",
        "Question_body":"<p>Having used <a href=\"https:\/\/docs.microsoft.com\/en-us\/sql\/machine-learning\/sql-server-machine-learning-services?view=sql-server-ver15\" rel=\"nofollow noreferrer\">SQL Server Machine Learning Services<\/a> and realised that it only supports R up to version 3.5.2, I am exploring options to be able to deploy models for later versions of R. The experts (or more like sales people) at Microsoft told me about <a href=\"https:\/\/docs.microsoft.com\/en-us\/machine-learning-server\/what-is-machine-learning-server\" rel=\"nofollow noreferrer\">Machine Learning Server<\/a>. However, I am suspicious that it has similar issues as:<\/p>\n<blockquote>\n<p>R support is built on a legacy of Microsoft R Server 9.x and Revolution R Enterprise products.<\/p>\n<\/blockquote>\n<p>I am pretty sure that this implies the same as above reg. the max R version (i.e. 3.5.2). Can someone confirm this please? I did many searches but could not find a definite answer.<\/p>\n<p>I know this is looking for an opinion and people will vote for closure, but I reckon containerisation is the only way forward to avoid issues like the above?<\/p>\n<p>Thanks!<\/p>\n<p>PS:<\/p>\n<p>I just installed Machine Learning Server version 9.4.7 as detailed here:<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/machine-learning-server\/install\/machine-learning-server-windows-install#howtoinstall\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/machine-learning-server\/install\/machine-learning-server-windows-install#howtoinstall<\/a><\/p>\n<p>If I run:<\/p>\n<pre><code>R.Version()\n<\/code><\/pre>\n<p>I get:<\/p>\n<p>$version.string\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/sql\/machine-learning\/sql-server-machine-learning-services?view=sql-server-ver15\" rel=\"nofollow noreferrer\">1<\/a> &quot;R version 3.5.2 (2018-12-20)&quot;<\/p>\n<p>I was told by Microsoft that it should be version 4.x. Maybe I am just thick. Can I upgrade to version 4.x.x and if so how please? Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618258028743,
        "Question_score":1,
        "Question_tags":"r|sql-server|azure|azure-machine-learning-service",
        "Question_view_count":58,
        "Owner_creation_time":1267440784443,
        "Owner_last_access_time":1664045779313,
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Question_last_edit_time":1618497243430,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67065023",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67609973,
        "Question_title":"Azure Machine learning python can't open file",
        "Question_body":"<p>I chose to use Python 3.8.1 Azure ML in Azure Machine learning studio, but when i run the command\n<code>!python train.py<\/code>, it uses python Anconda 3.6.9, when i downloaded python 3.8 and run the command <code>!python38 train.py<\/code> in the same dir  as before, the response was <code>python3.8: can't open file<\/code> .\nAny idea?\nAlso Python 3 in azure, is always busy, without anything running from my side.\nThank you.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1621453888223,
        "Question_score":1,
        "Question_tags":"python|azure|azure-machine-learning-studio",
        "Question_view_count":171,
        "Owner_creation_time":1609359946650,
        "Owner_last_access_time":1624561357960,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67609973",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58914326,
        "Question_title":"azure custom vision model precision",
        "Question_body":"<p>Im having problems in identifying  the best way to optimize the custom vision model.<\/p>\n\n<ol>\n<li><p>In using Image classification, will labeling the same image with different label at different time effect the precision of machine learning? \n\/\/for example labeling and training image A today with the label 't-shirt', labeling and training image A tomorrow with the label 'blue'.\n\/\/We are basically trying to input one classification at a time with the same image (total of five classification, such as style and color) and wanted to know whether this way will effect the prediction precision.<\/p><\/li>\n<li><p>Will labeling larger amount of data at once when inputing the image for machine learning increase the precision of the model? (for example, will there be any difference between 50 label and 100 label for an image to learn at a time?)<\/p><\/li>\n<li><p>Is there any way to teach machine learning to identify object recognition using the result gained from the image classification, else can i teach image classification and object recognition separately with the same type of image?<\/p><\/li>\n<li><p>Will running the learning process of the machine learning longer (for example, the difference between 1hour and 10hours) always give better results?<\/p><\/li>\n<\/ol>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1574079251813,
        "Question_score":1,
        "Question_tags":"azure|machine-learning|microsoft-cognitive|azure-machine-learning-service",
        "Question_view_count":73,
        "Owner_creation_time":1539084602260,
        "Owner_last_access_time":1590394612383,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1574149299680,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58914326",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":57679839,
        "Question_title":"Consume AZURE ML with nodejs error 400 status code",
        "Question_body":"<p>I have a problem when I try to consuming the web service, the error is: <\/p>\n\n<pre><code>The request failed with status code: 400 \n{\"error\":\n{\"code\":\"BadArgument\",\"message\":\"Invalid argument provided.\",\n\"details\":[{\"code\":\"BatchJobInputsNotSpecified\",\"message\":\"The following required input(s) were not specified with the request: input1. Please ensure all input data is specified and try again.\"}]}}\n<\/code><\/pre>\n\n<p>I don't know how to solve that, because in the code I send <code>input1<\/code>.<\/p>\n\n<p>Please help me, thanks.<\/p>\n\n<pre><code>let req = require(\"request\");\n\n    const uri = \"bla\";\n    const apiKey = \"key\";\n\n    let data = {\n        \"Inputs\": {\n            \"input1\":\n            [\n                {\n                    'IdEmpleado': \"20000\",\n                    'NivelSatisfaccion': \"0.38\",\n                    'SatisfaccionLaboral_Disc': \"Insatisfecho\",\n                    'UltimaEvaluacion': \"0.53\",\n                    'UltimaEvaluacion_Disc': \"Media\",\n                    'ProyectosRealizados': \"2\",\n                    'ProyectosRealizados_Disc': \"[2-3]\",\n                    'HorasMensuales': \"157\",\n                    'HorasMensuales_Disc': \"[150-199]\",\n                    'Antiguedad': \"3\",\n                    'Antiguedad_Disc': \"[2-4]\",\n                    'AccidentesTrabajo': \"0\",\n                    'AccidentesTrabajo_Disc': \"NO\",\n                    'Ascendido': \"0\",\n                    'Ascendido_Disc': \"NO\",\n                    'AreaTrabajo': \"Ventas\",\n                    'NivelSalarial': \"Bajo\",\n                    'Renuncia': \"1\",\n                    'Renuncia_Disc': \"SI\"\n                }\n            ],\n        },\n        \"GlobalParameters\": {}\n    }\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1566927986790,
        "Question_score":0,
        "Question_tags":"javascript|node.js|azure|azure-web-app-service|azure-machine-learning-studio",
        "Question_view_count":119,
        "Owner_creation_time":1512611070780,
        "Owner_last_access_time":1572456249360,
        "Owner_location":"Bogot\u00e1, Colombia",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1567057335736,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57679839",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":34923196,
        "Question_title":"Missing data in test data for Azure MachineforMachine Learning",
        "Question_body":"<p>I am new to Azure Machine Learning. I have created a training experiment where training data has some missing values. The logic for handling missing data and few other transformations is in Python code which works on this data.<\/p>\n\n<p>Now I want the same for test data. I have deployed the experiment as web service. So, the schema is produced for input and output data (all are Numeric fields).<\/p>\n\n<p>Two questions:\n1. It asks me to define the label for test data as well, otherwise it gives inconsistent number of columns error since label column is missing in the test data<br>\n2. I have some missing data in test data, which ideally Python script in the experiment should take care. But it gives me the following error because of schema.<\/p>\n\n<pre><code>The request failed with status code: 400\nContent-Length: 323\nContent-Type: application\/json; charset=utf-8\nServer: Microsoft-HTTPAPI\/2.0\nDate: Thu, 21 Jan 2016 11:44:49 GMT\nConnection: close\n\n{u'error': {u'message': u'Invalid argument provided.', u'code': 'BadArgument', u'details': [{u'message': u'Parsing of input vector failed.  Verify the input vector has the correct number of columns and data types.  Additional details: Value was either too large or too small for an Int32..', u'code': u'InputParseError', u'target': u'input1'}]}}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1453377464100,
        "Question_score":1,
        "Question_tags":"python|azure|azure-machine-learning-studio",
        "Question_view_count":796,
        "Owner_creation_time":1372245298297,
        "Owner_last_access_time":1652905906597,
        "Owner_location":"Bellevue, WA, United States",
        "Owner_reputation":757,
        "Owner_up_votes":5,
        "Owner_down_votes":7,
        "Owner_views":125,
        "Question_last_edit_time":1454300414052,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34923196",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":37462268,
        "Question_title":"Python in AzureML fail to pass dataframe without changes",
        "Question_body":"<p>when trying to pass data without doing anything in python, getting this error:<\/p>\n\n<pre><code>Error 0085: The following error occurred during script evaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\nCaught exception while executing function: Traceback (most recent call last):\n  File \"C:\\server\\invokepy.py\", line 175, in batch\n    rutils.RUtils.DataFrameToRFile(outlist[i], outfiles[i])\n  File \"C:\\server\\RReader\\rutils.py\", line 28, in DataFrameToRFile\n    rwriter.write_attribute_list(attributes)\n  File \"C:\\server\\RReader\\rwriter.py\", line 59, in write_attribute_list\n    self.write_object(value);\n  File \"C:\\server\\RReader\\rwriter.py\", line 121, in write_object\n    write_function(flags, value.values())\n  File \"C:\\server\\RReader\\rwriter.py\", line 104, in write_objects\n    self.write_object(value)\n  File \"C:\\server\\RReader\\rwriter.py\", line 121, in write_object\n    write_function(flags, value.values())\n  File \"C:\\server\\RReader\\rwriter.py\", line 71, in write_integers\n    self.write_integer(value)\n  File \"C:\\server\\RReader\\rwriter.py\", line 147, in write_integer\n    self.writer.WriteInt32(value)\n  File \"C:\\server\\RReader\\BinaryIO\\binarywriter.py\", line 26, in WriteInt32\n    self.WriteData(self.Int32Format, data)\n  File \"C:\\server\\RReader\\BinaryIO\\binarywriter.py\", line 14, in WriteData\n    self.stream.write(pack(format, data))\nerror: cannot convert argument to integer\n\n---------- End of error message from Python  interpreter  ----------\nStart time: UTC 05\/26\/2016 13:16:01\nEnd time: UTC 05\/26\/2016 13:16:13\n<\/code><\/pre>\n\n<p>here is the data i'm trying to pass:\n<a href=\"https:\/\/i.stack.imgur.com\/ysG36.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ysG36.png\" alt=\"the data\"><\/a><\/p>\n\n<p>here is the experiment:\n<a href=\"https:\/\/i.stack.imgur.com\/vgdSn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vgdSn.png\" alt=\"the experiment\"><\/a><\/p>\n\n<p>and the python code:\n<a href=\"https:\/\/i.stack.imgur.com\/LRSAE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LRSAE.png\" alt=\"python code\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1464269301867,
        "Question_score":3,
        "Question_tags":"python|python-2.7|pandas|cortana-intelligence|azure-machine-learning-studio",
        "Question_view_count":739,
        "Owner_creation_time":1320061998253,
        "Owner_last_access_time":1656424560827,
        "Owner_location":null,
        "Owner_reputation":778,
        "Owner_up_votes":176,
        "Owner_down_votes":6,
        "Owner_views":89,
        "Question_last_edit_time":null,
        "Answer_body":"<p>after talking to Microsoft support, the problem was that the \"Execute Python Script\" module cannot return empty values.\nthis can be solved by adding a \"Clean Missing Data\" module before reading it from python:\n<a href=\"https:\/\/i.stack.imgur.com\/BzCUZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BzCUZ.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1468139774183,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37462268",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73283498,
        "Question_title":"Surprising results of Ms Azure vs Google Colab BERT training performance, not sure how to explain",
        "Question_body":"<p>I'm not sure if it's BERT related or not, had no chance to test other models, but did it for BERT.<\/p>\n<p>So what I noticed recently that training algorithms and data that I used to work with in google colab for free, are seemed to work significantly slower in Azure ML workspace which we pay for.<\/p>\n<p>I made the comparison - same data file (classification problem, sentiment analysis of 10K reviews), totally same notebook code (copy+paste), same latest ver of ktrain lib installed on both, both must be on Python 3.8, but GPU is a bit more performant on a colab side.<\/p>\n<p>Results surprised me to say the least: google lab made its job <strong>10 times faster: 17 min vs 170 min<\/strong>, and it's reproducible. Tesla T4 (colab) is faster than K80 (azure) indeed, but not that much as per known benchmarks. So I wonder what else could matter. Is it virt. environment created in Azure ML performing so slow? If you have any idea what it could be, or what else I can check on both sides to reveal it, please share<\/p>\n<p>BTW google gives you T4 in colab for your experimentations for free, while you have to pay for slower K80 at Azure.<\/p>\n<p><strong>Google colab<\/strong>\nexecution time = 17 min\n<a href=\"https:\/\/i.stack.imgur.com\/Y6iVN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Y6iVN.png\" alt=\"enter image description here\" \/><\/a>\n<strong>Google colab hardware<\/strong>: cpu model: Intel(R) Xeon(R) CPU @ 2.20GHz, memory 13Gb, GPU:<br \/>\n<a href=\"https:\/\/i.stack.imgur.com\/3YrWq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3YrWq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Azure<\/strong>\nexecution time = 2h50m = 170min (10x of colab)\n<a href=\"https:\/\/i.stack.imgur.com\/9SPlB.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9SPlB.png\" alt=\"enter image description here\" \/><\/a>\n<strong>Azure hardware information<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/wA0Se.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wA0Se.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>K80 and T4 comparison: <a href=\"https:\/\/technical.city\/en\/video\/Tesla-K80-vs-Tesla-T4\" rel=\"nofollow noreferrer\">https:\/\/technical.city\/en\/video\/Tesla-K80-vs-Tesla-T4<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659989536940,
        "Question_score":2,
        "Question_tags":"azure|google-colaboratory|azure-machine-learning-service",
        "Question_view_count":69,
        "Owner_creation_time":1304556762350,
        "Owner_last_access_time":1664081190543,
        "Owner_location":null,
        "Owner_reputation":4550,
        "Owner_up_votes":321,
        "Owner_down_votes":19,
        "Owner_views":287,
        "Question_last_edit_time":1659989857227,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73283498",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":36126897,
        "Question_title":"How to restart VM in Azure ML Notebook?",
        "Question_body":"<p>I am writing code in Jupyter Notebook in Azure ML Studio.<\/p>\n\n<p>At current moment every command causes kernel death. Even in new clear notebook I could not execute even <code>print 'hello'<\/code> - kernel died immediately.<\/p>\n\n<p>Also I could not use bash commands like <code>!ls<\/code> - It crashes kernel too.<\/p>\n\n<p>How could I restart my VM or restart session in Azure ML Studio with killing all running VM?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1458551625417,
        "Question_score":0,
        "Question_tags":"cortana-intelligence|azure-machine-learning-studio",
        "Question_view_count":488,
        "Owner_creation_time":1452426675697,
        "Owner_last_access_time":1615978476293,
        "Owner_location":null,
        "Owner_reputation":77,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":21,
        "Question_last_edit_time":1458567186852,
        "Answer_body":"<p>I have found that if I go from notebook menu to File->Open, then I see all my notebooks, their statuses and I could shutdown them.<\/p>\n\n<p>Also I have found that some of my closed notebooks were still alive and have shut them down.<\/p>\n\n<p>After this my working notebook came back to life.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1458573398360,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36126897",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67258917,
        "Question_title":"AzureML ParallelRunStep progress information",
        "Question_body":"<p>is there a way to know the progress percentage a ParallelRunStep has already computed on a pipeline?<\/p>\n<p>As the total number of inputs is known in advance, I think it should not be hard to get this information.<\/p>\n<p>This would be a great feedback for pipelines that takes long time to finish.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619390617157,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":23,
        "Owner_creation_time":1343828614450,
        "Owner_last_access_time":1664042984127,
        "Owner_location":"Seville, Spain",
        "Owner_reputation":359,
        "Owner_up_votes":265,
        "Owner_down_votes":1,
        "Owner_views":55,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Answer from python azure sdk: <em>In Studio, if you go to the step's Metrics tab, you will be able to see a chart\/table of execution progress, including remaining items, remaining mini batches, failed items, etc.<\/em><\/p>\n<p><a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/18357\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/18357<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1619693209260,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1619694014310,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67258917",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73209864,
        "Question_title":"Azure Synapse: Possible to create\/deploy ONNX model to dedicated sql pool from Synapse Notebook?",
        "Question_body":"<p>The examples I see of installing trained ONNX models to Synapse dedicated sql pool (for use with the PREDICT functionality) all originate from Azure Machine Learning Studio. E.g. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-sql-pool-model-scoring-wizard\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-sql-pool-model-scoring-wizard<\/a><\/p>\n<p>Can this be done from a Synapse Notebook directly, never leaving the Synapse environment?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659453001323,
        "Question_score":0,
        "Question_tags":"azure|azure-synapse|azure-machine-learning-studio",
        "Question_view_count":79,
        "Owner_creation_time":1499405667427,
        "Owner_last_access_time":1663433476640,
        "Owner_location":null,
        "Owner_reputation":393,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":36,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73209864",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":50355494,
        "Question_title":"How to continually migrate data from on-premises SQL Db to Azure SQL Db",
        "Question_body":"<p>As a part of <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/\" rel=\"nofollow noreferrer\">Azure Machine Learning<\/a> process, I need to <code>continually<\/code> migrate data from on-premises SQL Db to Azure SQL Db using <code>Data Management Gateway<\/code>.<\/p>\n\n<p>This Azure official article describes how to: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/team-data-science-process\/move-sql-azure-adf\" rel=\"nofollow noreferrer\">Move data from an on-premises SQL server to SQL Azure with Azure Data Factory<\/a>. But the details are a bit confusing to me. If someone to briefly describe the process, how would you do that. What are 2-3 <code>main<\/code> steps one needs to perform on <code>on-premises<\/code> and 2-3 steps on <code>Azure Cloud<\/code>? No details are needed. <strong>Note<\/strong>: The solution has to involve using <code>Data Management Gateway<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1526403046690,
        "Question_score":1,
        "Question_tags":"azure|azure-sql-database|azure-storage|azure-data-factory|azure-machine-learning-studio",
        "Question_view_count":103,
        "Owner_creation_time":1330144099340,
        "Owner_last_access_time":1664039192277,
        "Owner_location":null,
        "Owner_reputation":19815,
        "Owner_up_votes":2703,
        "Owner_down_votes":22,
        "Owner_views":2272,
        "Question_last_edit_time":1526409433763,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50355494",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67287063,
        "Question_title":"Azure-ML-R SDK in R Studio ScriptRunConfig not recognized function error after a deprecated estimator replacement",
        "Question_body":"<p>I am trying to use Azure-ML-SDK in R Studio and used Estimator but got error stating estimator deprecated and advised to use ScriptRunConfig and when used it, it not being recognized as a function and fails to run. See the errors below. Please advise.<\/p>\n<p>Already loaded library(azuremlsdk) which should include azureml.core to recognize the ScriptRunConfig function. Is it version compatibility issue? if so, which version should i use for ScriptRunConfig and how to load specific R version in Azure ML Compute (R Studio web interface and not R Studio Desktop)<\/p>\n<p>First Error and code<\/p>\n<pre><code>est &lt;- estimator(source_directory = &quot;train-and-deploy-first-model&quot;,\n                 entry_script = &quot;accidents.R&quot;,\n                 script_params = list(&quot;--data_folder&quot; = ds$path(target_path)),\n                 compute_target = compute_target\n                 )\n<\/code><\/pre>\n<p>cran_packages, github_packages, custom_url_packages, custom_docker_image, image_registry_details, use_gpu, environment_variables, and shm_size parameters will be deprecated. Please create an environment object with them using r_environment() and pass the environment object to the estimator().'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n'Estimator' is deprecated. Please use 'ScriptRunConfig' from 'azureml.core.script_run_config' with your own defined environment or an Azure ML curated environment.<\/p>\n<p>Second Code snippet trying to fix above and it's error<\/p>\n<pre><code>config &lt;- ScriptRunConfig(source_directory = &quot;.&quot;,\n                 script = &quot;accidents.R&quot;,\n                 compute_target = compute_target\n                 )\n<\/code><\/pre>\n<p>Error in ScriptRunConfig(source_directory = &quot;.&quot;, script = &quot;accidents.R&quot;,  :\ncould not find function &quot;ScriptRunConfig&quot;<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1619541027510,
        "Question_score":1,
        "Question_tags":"r|azure-machine-learning-service",
        "Question_view_count":159,
        "Owner_creation_time":1397564337880,
        "Owner_last_access_time":1620749682530,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67287063",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":42010405,
        "Question_title":"The way to pass input for azure machine experiment from app ( for example console app )",
        "Question_body":"<p>I'm trying to do some kind of web job application that can run for period time and make prediction on azure machine learning studio. After that i want get the result of this experiment and do something with that in my console application. What is the best way to do this in azure with machine learning or maybe some similiar stuff to prediction data from data series ? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1486062508683,
        "Question_score":0,
        "Question_tags":"azure|prediction|azure-machine-learning-studio",
        "Question_view_count":62,
        "Owner_creation_time":1432141466930,
        "Owner_last_access_time":1591285402750,
        "Owner_location":null,
        "Owner_reputation":327,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":78,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can try using Azure Data Factory to create a Machine Learning pipeline or use Azure ML Studio's Predictive Web Services.<\/p>\n\n<ol>\n<li><p>With Azure Data Factory\nFollow <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/data-factory\/data-factory-azure-ml-batch-execution-activity\" rel=\"nofollow noreferrer\">this link<\/a> for details. Azure Data Factory implementations would seem difficult at first but they do work great with Azure ML experiments. <\/p>\n\n<p>Azure Data Factory can run your ML Experiment on a schedule or one-off at a specified time (I guess you can set only for UTC Timezone right now) and monitor it through a dashboard (which is pretty cool).<\/p>\n\n<p>As an example you can look @ <a href=\"https:\/\/github.com\/Microsoft\/azure-docs\/blob\/master\/articles\/data-factory\/data-factory-azure-ml-batch-execution-activity.md\" rel=\"nofollow noreferrer\">ML Batch Execution<\/a>. I used this in one of our implementations (we do have latency issues, but trying to solve that).<\/p><\/li>\n<li><p>If you directly want to use the experiment in your console (assuming it is a web application), use create a Predictive Web service out of your ML Experiment, details <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-walkthrough-5-publish-web-service\" rel=\"nofollow noreferrer\">here<\/a><\/p><\/li>\n<\/ol>\n\n<p>I couldn't exactly understand your use case so I posted two alternatives that should help you. Hope this might lead you to a better solution\/approach.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1486537080116,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1486537417792,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/42010405",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":50113374,
        "Question_title":"What is a trait in Azure ML matchbox recommender?",
        "Question_body":"<p>Azure Machine Learning has an item called <code>Train Matchbox Recommender<\/code>. It can be configured with a <code>Number of traits<\/code>. Unfortunately, the documentation does not describe what such a trait is.<\/p>\n\n<p>What are traits? Is this related to <a href=\"https:\/\/en.wikipedia.org\/wiki\/Latent_variable\" rel=\"nofollow noreferrer\">latent variables<\/a>?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1525162879793,
        "Question_score":0,
        "Question_tags":"azure|recommendation-engine|azure-machine-learning-studio",
        "Question_view_count":859,
        "Owner_creation_time":1290750251813,
        "Owner_last_access_time":1663965312343,
        "Owner_location":"Nimes, France",
        "Owner_reputation":55864,
        "Owner_up_votes":1621,
        "Owner_down_votes":31,
        "Owner_views":4960,
        "Question_last_edit_time":1526046231816,
        "Answer_body":"<p><a href=\"http:\/\/apprize.info\/microsoft\/azure_1\/9.html\" rel=\"nofollow noreferrer\">This<\/a> page may have better descriptions on it.<\/p>\n\n<p>Basically, traits are the features the algorithm will learn about each user related to each item. For example, in the <a href=\"https:\/\/gallery.azure.ai\/Experiment\/Recommender-Restaurant-ratings-2\" rel=\"nofollow noreferrer\">restaurant ratings recommender<\/a> traits could include a user's birth year, if they're a student or working professional, martial status, etc.<\/p>\n\n<p>Hope that helps!<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1525171563376,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50113374",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71662401,
        "Question_title":"Synapse Analytics Auto ML Predict No module named 'azureml.automl'",
        "Question_body":"<p>I follow the official tutotial from microsoft: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool<\/a><\/p>\n<p>When I execute:<\/p>\n<pre><code>#Bind model within Spark session\n model = pcontext.bind_model(\n     return_types=RETURN_TYPES, \n     runtime=RUNTIME, \n     model_alias=&quot;Sales&quot;, #This alias will be used in PREDICT call to refer  this   model\n     model_uri=AML_MODEL_URI, #In case of AML, it will be AML_MODEL_URI\n     aml_workspace=ws #This is only for AML. In case of ADLS, this parameter can be removed\n ).register()\n<\/code><\/pre>\n<p>I got : No module named 'azureml.automl'<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/g0UCX.png\" rel=\"nofollow noreferrer\">My Notebook<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1648558433353,
        "Question_score":0,
        "Question_tags":"azure-synapse|azure-machine-learning-studio|automl|azure-auto-ml",
        "Question_view_count":271,
        "Owner_creation_time":1645110475503,
        "Owner_last_access_time":1664057688837,
        "Owner_location":null,
        "Owner_reputation":15,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I solved it. In my case it works best like this:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/JKEmr.png\" rel=\"nofollow noreferrer\">Imports<\/a><\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>#Import libraries\nfrom pyspark.sql.functions import col, pandas_udf,udf,lit\nfrom notebookutils.mssparkutils import azureML\nfrom azureml.core import Workspace, Model\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nfrom azureml.core.model import Model\nimport joblib\nimport pandas as pd\n\nws = azureML.getWorkspace(\"AzureMLService\")\nspark.conf.set(\"spark.synapse.ml.predict.enabled\",\"true\")<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ij760.png\" rel=\"nofollow noreferrer\">Predict function<\/a><\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>def forecastModel():\n    model_path = Model.get_model_path(model_name=\"modelName\", _workspace=ws)\n    modeljob = joblib.load(model_path + \"\/model.pkl\")\n\n    validation_data = spark.read.format(\"csv\") \\\n                            .option(\"header\", True) \\\n                            .option(\"inferSchema\",True) \\\n                            .option(\"sep\", \";\") \\\n                            .load(\"abfss:\/\/....csv\")\n\n    validation_data_pd = validation_data.toPandas()\n\n\n    predict = modeljob.forecast(validation_data_pd)\n\n    return predict<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1648911550929,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1648934074416,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71662401",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":36485084,
        "Question_title":"Azure ML Studio: How to import images dataset?",
        "Question_body":"<p>In Azure ML studio, how to import images dataset, for image recognition algorithms. As zip file?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1460056088943,
        "Question_score":0,
        "Question_tags":"azure|machine-learning|dataset|image-recognition|azure-machine-learning-studio",
        "Question_view_count":2420,
        "Owner_creation_time":1282073536110,
        "Owner_last_access_time":1664043684557,
        "Owner_location":null,
        "Owner_reputation":4529,
        "Owner_up_votes":458,
        "Owner_down_votes":11,
        "Owner_views":1142,
        "Question_last_edit_time":1460056872020,
        "Answer_body":"<p>You can use \"<strong>import images<\/strong>\" module in Azure ML Studio that can read images from Azure blob storage - <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Face-detection-2\" rel=\"nofollow\">here<\/a> is the sample experiment <\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1460058230132,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36485084",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":48906201,
        "Question_title":"TypeError: object of type 'numpy.float64' has no len() when finding mean",
        "Question_body":"<p>I am doing a simple operations in Azure ML Studio using Python script.<\/p>\n\n<pre><code>import numpy as np\n\ndt_mean = np.mean(dt.iloc[:,0].values)\n<\/code><\/pre>\n\n<p>but it throws error<\/p>\n\n<pre><code>[Critical]     Error: Error 0085: The following error occurred during script evaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\nCaught exception while executing function: Traceback (most recent call last):\n  File \"C:\\server\\invokepy.py\", line 211, in batch\n    xdrutils.XDRUtils.DataFrameToRFile(outlist[i], outfiles[i], True)\n  File \"C:\\server\\XDRReader\\xdrutils.py\", line 51, in DataFrameToRFile\n    attributes = XDRBridge.DataFrameToRObject(dataframe)\n  File \"C:\\server\\XDRReader\\xdrbridge.py\", line 40, in DataFrameToRObject\n    if (len(dataframe) == 1 and type(dataframe[0]) is pd.DataFrame):\nTypeError: object of type 'numpy.float64' has no len()\nProcess returned with non-zero exit code 1\n<\/code><\/pre>\n\n<p>This one works perfectly fine in Spyder. But it's not working in Azure ML Python script. <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1519216830073,
        "Question_score":0,
        "Question_tags":"python|mean|azure-machine-learning-studio",
        "Question_view_count":1037,
        "Owner_creation_time":1489644560420,
        "Owner_last_access_time":1646025882010,
        "Owner_location":"Planet Earth",
        "Owner_reputation":791,
        "Owner_up_votes":55,
        "Owner_down_votes":4,
        "Owner_views":253,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48906201",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":33091830,
        "Question_title":"How best to convert from azure blob csv format to pandas dataframe while running notebook in azure ml",
        "Question_body":"<p>I have a number of large csv (tab delimited) data stored as azure blobs, and I want to create a pandas dataframe from these. I can do this locally as follows:<\/p>\n\n<pre><code>from azure.storage.blob import BlobService\nimport pandas as pd\nimport os.path\n\nSTORAGEACCOUNTNAME= 'account_name'\nSTORAGEACCOUNTKEY= \"key\"\nLOCALFILENAME= 'path\/to.csv'        \nCONTAINERNAME= 'container_name'\nBLOBNAME= 'bloby_data\/000000_0'\n\nblob_service = BlobService(account_name=STORAGEACCOUNTNAME, account_key=STORAGEACCOUNTKEY)\n\n# Only get a local copy if haven't already got it\nif not os.path.isfile(LOCALFILENAME):\n    blob_service.get_blob_to_path(CONTAINERNAME,BLOBNAME,LOCALFILENAME)\n\ndf_customer = pd.read_csv(LOCALFILENAME, sep='\\t')\n<\/code><\/pre>\n\n<p>However, when running the notebook on azure ML notebooks, I can't 'save a local copy' and then read from csv, and so I'd like to do the conversion directly (something like pd.read_azure_blob(blob_csv) or just pd.read_csv(blob_csv) would be ideal).<\/p>\n\n<p>I can get to the desired end result (pandas dataframe for blob csv data), if I first create an azure ML workspace, and then read the datasets into that, and finally using <a href=\"https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python\" rel=\"noreferrer\">https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python<\/a> to access the dataset as a pandas dataframe, but I'd prefer to just read straight from the blob storage location.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1444692718393,
        "Question_score":12,
        "Question_tags":"python|azure|pandas|azure-blob-storage|azure-machine-learning-studio",
        "Question_view_count":22789,
        "Owner_creation_time":1346359012420,
        "Owner_last_access_time":1529010657573,
        "Owner_location":null,
        "Owner_reputation":395,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":1444814426276,
        "Answer_body":"<p>I think you want to use <code>get_blob_to_bytes<\/code>, <code>or get_blob_to_text<\/code>; these should output a string which you can use to create a dataframe as<\/p>\n\n<pre><code>from io import StringIO\nblobstring = blob_service.get_blob_to_text(CONTAINERNAME,BLOBNAME)\ndf = pd.read_csv(StringIO(blobstring))\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1444693139743,
        "Answer_score":17.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":1545116358716,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/33091830",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":61674239,
        "Question_title":"schedule Azure machine learning compute instances",
        "Question_body":"<p>I want to schedule azure machine learning compute instances so that I can stop them during off-hours like weekends, azure automation solution with runbook seems to be working with VMs in general but not with azure ML. The solution could be either a script or ML pipeline.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1588923299370,
        "Question_score":5,
        "Question_tags":"azure-machine-learning-service|azure-vm",
        "Question_view_count":711,
        "Owner_creation_time":1588923100490,
        "Owner_last_access_time":1602222687747,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1589017455823,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61674239",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":32451243,
        "Question_title":"How to load images faster from Azure Blob?",
        "Question_body":"<p>I've been trying to upload some images to azure blob and then using <strong>ImageReader<\/strong> in <strong>Azure ML studio<\/strong> to read them from the blob. The problem is that ImageReader takes a lot of time to load images and I need it in real time. <br>\nI also tried making a <strong>csv<\/strong> of <strong>4 images (four rows)<\/strong> containing 800x600 pixels as columns <strong>(500,000 cols. approx)<\/strong> and tried simple <strong>Reader<\/strong>. Reader took <strong>31 mins<\/strong> to read the file from the blob.<br>\nI want to know the alternate methods of loading and reading images in Azure ML studio. If anyone know any other method or can share a helpful and relevant link.<br>\nPlease share if i can speed up ImageReader by any means.\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1441695780200,
        "Question_score":1,
        "Question_tags":"opencv|azure|azure-machine-learning-studio",
        "Question_view_count":803,
        "Owner_creation_time":1387426285030,
        "Owner_last_access_time":1659463166403,
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":2128,
        "Owner_up_votes":128,
        "Owner_down_votes":8,
        "Owner_views":211,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Look at the Azure CDN <a href=\"http:\/\/azure.microsoft.com\/en-us\/services\/cdn\/\" rel=\"nofollow\">http:\/\/azure.microsoft.com\/en-us\/services\/cdn\/<\/a> , after which the blobs will get an alternative url. My blob downloads became about 4 times faster after switching.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1441739836496,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/32451243",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":51925635,
        "Question_title":"Issue with R version in Azure ML Studio",
        "Question_body":"<p>I need to install \"ompr.roi\" package which requires R version >= 3.4.0.<\/p>\n\n<p>But Azure ML Studio supports R version till 3.2.2. Pls refer below screenshot,<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/bAoaW.jpg\" alt=\"Error in Execute R Script\"><\/p>\n\n<p>Is there any way I can use this library in Azure ML Studio.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1534748925187,
        "Question_score":0,
        "Question_tags":"r|azure-machine-learning-studio",
        "Question_view_count":173,
        "Owner_creation_time":1528121402630,
        "Owner_last_access_time":1546410198623,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1534749536030,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51925635",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":69534189,
        "Question_title":"Building Azure Machine Learning environment (tensorflow) from dockerfile failing",
        "Question_body":"<p>I'm trying to create a new environment based on the TF 2.4 curated environment with opencv. Support for opencv is the only difference. I modified the dockerfile to include opencv as following:<\/p>\n<pre><code> FROM mcr.microsoft.com\/azureml\/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04:20211005.v1\n\n    ENV AZUREML_CONDA_ENVIRONMENT_PATH \/azureml-envs\/tensorflow-2.4\n\n    # Create conda environment\n    RUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\n        python=3.7 pip=20.2.4\n\n    # Prepend path to AzureML conda environment\n    ENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/bin:$PATH\n\n    # Install pip dependencies\n    RUN HOROVOD_WITH_TENSORFLOW=1 \\\n        pip install 'matplotlib&gt;=3.3,&lt;3.4' \\\n                    'psutil&gt;=5.8,&lt;5.9' \\\n                    'tqdm&gt;=4.59,&lt;4.60' \\\n                    'pandas&gt;=1.1,&lt;1.2' \\\n                    'scipy&gt;=1.5,&lt;1.6' \\\n                    'numpy&gt;=1.10,&lt;1.20' \\\n                    'ipykernel~=6.0' \\\n                    'azureml-core==1.34.0' \\\n                    'azureml-defaults==1.34.0' \\\n                    'azureml-mlflow==1.34.0' \\\n                    'azureml-telemetry==1.34.0' \\\n                    'tensorboard==2.4.0' \\\n                    'tensorflow-gpu==2.4.1' \\\n                    'tensorflow-datasets==4.3.0' \\\n                    'onnxruntime-gpu&gt;=1.7,&lt;1.8' \\\n                    'horovod[tensorflow-gpu]==0.21.3' \\\n                    'opencv-python'\n\n    # This is needed for mpi to locate libpython\n    ENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/lib:$LD_LIBRARY_PATH\n<\/code><\/pre>\n<p>However horovod is failing to build tensorflow and is showing the following error message:<\/p>\n<pre><code> ERROR: Command errored out with exit status 1:\n   command: \/azureml-envs\/tensorflow-2.4\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-pjyu9d6m\/horovod\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-pjyu9d6m\/horovod\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' bdist_wheel -d \/tmp\/pip-wheel-0t6zraqk\n       cwd: \/tmp\/pip-install-pjyu9d6m\/horovod\/\n  Complete output (233 lines):\n  running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build\/lib.linux-x86_64-3.7\n  creating build\/lib.linux-x86_64-3.7\/horovod\n  copying horovod\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\n  creating build\/lib.linux-x86_64-3.7\/horovod\/runner\n  copying horovod\/runner\/task_fn.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\n  copying horovod\/runner\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\n  copying horovod\/runner\/launch.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\n  copying horovod\/runner\/js_run.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\n  copying horovod\/runner\/gloo_run.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\n  copying horovod\/runner\/run_task.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\n  copying horovod\/runner\/mpi_run.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\n  creating build\/lib.linux-x86_64-3.7\/horovod\/_keras\n  copying horovod\/_keras\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/_keras\n  copying horovod\/_keras\/callbacks.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/_keras\n  copying horovod\/_keras\/elastic.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/_keras\n  creating build\/lib.linux-x86_64-3.7\/horovod\/torch\n  copying horovod\/torch\/sync_batch_norm.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/torch\n  copying horovod\/torch\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/torch\n  copying horovod\/torch\/mpi_ops.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/torch\n  copying horovod\/torch\/optimizer.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/torch\n  copying horovod\/torch\/functions.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/torch\n  copying horovod\/torch\/compression.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/torch\n  creating build\/lib.linux-x86_64-3.7\/horovod\/keras\n  copying horovod\/keras\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/keras\n  copying horovod\/keras\/callbacks.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/keras\n  copying horovod\/keras\/elastic.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/keras\n  creating build\/lib.linux-x86_64-3.7\/horovod\/tensorflow\n  copying horovod\/tensorflow\/sync_batch_norm.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/tensorflow\n  copying horovod\/tensorflow\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/tensorflow\n  copying horovod\/tensorflow\/elastic.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/tensorflow\n  copying horovod\/tensorflow\/mpi_ops.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/tensorflow\n  copying horovod\/tensorflow\/util.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/tensorflow\n  copying horovod\/tensorflow\/gradient_aggregation_eager.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/tensorflow\n  copying horovod\/tensorflow\/gradient_aggregation.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/tensorflow\n  copying horovod\/tensorflow\/functions.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/tensorflow\n  copying horovod\/tensorflow\/compression.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/tensorflow\n  creating build\/lib.linux-x86_64-3.7\/horovod\/spark\n  copying horovod\/spark\/runner.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\n  copying horovod\/spark\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\n  copying horovod\/spark\/conf.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\n  copying horovod\/spark\/gloo_run.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\n  copying horovod\/spark\/mpi_run.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\n  creating build\/lib.linux-x86_64-3.7\/horovod\/common\n  copying horovod\/common\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/common\n  copying horovod\/common\/exceptions.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/common\n  copying horovod\/common\/elastic.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/common\n  copying horovod\/common\/util.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/common\n  copying horovod\/common\/basics.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/common\n  creating build\/lib.linux-x86_64-3.7\/horovod\/mxnet\n  copying horovod\/mxnet\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/mxnet\n  copying horovod\/mxnet\/mpi_ops.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/mxnet\n  copying horovod\/mxnet\/functions.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/mxnet\n  creating build\/lib.linux-x86_64-3.7\/horovod\/ray\n  copying horovod\/ray\/runner.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/ray\n  copying horovod\/ray\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/ray\n  copying horovod\/ray\/ray_logger.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/ray\n  copying horovod\/ray\/elastic.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/ray\n  copying horovod\/ray\/utils.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/ray\n  copying horovod\/ray\/driver_service.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/ray\n  creating build\/lib.linux-x86_64-3.7\/horovod\/runner\/util\n  copying horovod\/runner\/util\/lsf.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/util\n  copying horovod\/runner\/util\/streams.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/util\n  copying horovod\/runner\/util\/threads.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/util\n  copying horovod\/runner\/util\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/util\n  copying horovod\/runner\/util\/remote.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/util\n  copying horovod\/runner\/util\/network.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/util\n  copying horovod\/runner\/util\/cache.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/util\n  creating build\/lib.linux-x86_64-3.7\/horovod\/runner\/http\n  copying horovod\/runner\/http\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/http\n  copying horovod\/runner\/http\/http_client.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/http\n  copying horovod\/runner\/http\/http_server.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/http\n  creating build\/lib.linux-x86_64-3.7\/horovod\/runner\/common\n  copying horovod\/runner\/common\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/common\n  creating build\/lib.linux-x86_64-3.7\/horovod\/runner\/task\n  copying horovod\/runner\/task\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/task\n  copying horovod\/runner\/task\/task_service.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/task\n  creating build\/lib.linux-x86_64-3.7\/horovod\/runner\/driver\n  copying horovod\/runner\/driver\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/driver\n  copying horovod\/runner\/driver\/driver_service.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/driver\n  creating build\/lib.linux-x86_64-3.7\/horovod\/runner\/elastic\n  copying horovod\/runner\/elastic\/worker.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/elastic\n  copying horovod\/runner\/elastic\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/elastic\n  copying horovod\/runner\/elastic\/driver.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/elastic\n  copying horovod\/runner\/elastic\/registration.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/elastic\n  copying horovod\/runner\/elastic\/rendezvous.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/elastic\n  copying horovod\/runner\/elastic\/constants.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/elastic\n  copying horovod\/runner\/elastic\/discovery.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/elastic\n  copying horovod\/runner\/elastic\/settings.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/elastic\n  creating build\/lib.linux-x86_64-3.7\/horovod\/runner\/common\/util\n  copying horovod\/runner\/common\/util\/host_hash.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/common\/util\n  copying horovod\/runner\/common\/util\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/common\/util\n  copying horovod\/runner\/common\/util\/config_parser.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/common\/util\n  copying horovod\/runner\/common\/util\/timeout.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/common\/util\n  copying horovod\/runner\/common\/util\/secret.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/common\/util\n  copying horovod\/runner\/common\/util\/tiny_shell_exec.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/common\/util\n  copying horovod\/runner\/common\/util\/env.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/common\/util\n  copying horovod\/runner\/common\/util\/codec.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/common\/util\n  copying horovod\/runner\/common\/util\/network.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/common\/util\n  copying horovod\/runner\/common\/util\/settings.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/common\/util\n  copying horovod\/runner\/common\/util\/safe_shell_exec.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/common\/util\n  copying horovod\/runner\/common\/util\/hosts.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/common\/util\n  creating build\/lib.linux-x86_64-3.7\/horovod\/runner\/common\/service\n  copying horovod\/runner\/common\/service\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/common\/service\n  copying horovod\/runner\/common\/service\/driver_service.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/common\/service\n  copying horovod\/runner\/common\/service\/task_service.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/runner\/common\/service\n  creating build\/lib.linux-x86_64-3.7\/horovod\/torch\/mpi_lib_impl\n  copying horovod\/torch\/mpi_lib_impl\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/torch\/mpi_lib_impl\n  creating build\/lib.linux-x86_64-3.7\/horovod\/torch\/mpi_lib\n  copying horovod\/torch\/mpi_lib\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/torch\/mpi_lib\n  creating build\/lib.linux-x86_64-3.7\/horovod\/torch\/elastic\n  copying horovod\/torch\/elastic\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/torch\/elastic\n  copying horovod\/torch\/elastic\/state.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/torch\/elastic\n  copying horovod\/torch\/elastic\/sampler.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/torch\/elastic\n  creating build\/lib.linux-x86_64-3.7\/horovod\/tensorflow\/keras\n  copying horovod\/tensorflow\/keras\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/tensorflow\/keras\n  copying horovod\/tensorflow\/keras\/callbacks.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/tensorflow\/keras\n  copying horovod\/tensorflow\/keras\/elastic.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/tensorflow\/keras\n  creating build\/lib.linux-x86_64-3.7\/horovod\/spark\/torch\n  copying horovod\/spark\/torch\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/torch\n  copying horovod\/spark\/torch\/remote.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/torch\n  copying horovod\/spark\/torch\/estimator.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/torch\n  copying horovod\/spark\/torch\/util.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/torch\n  creating build\/lib.linux-x86_64-3.7\/horovod\/spark\/keras\n  copying horovod\/spark\/keras\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/keras\n  copying horovod\/spark\/keras\/remote.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/keras\n  copying horovod\/spark\/keras\/optimizer.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/keras\n  copying horovod\/spark\/keras\/estimator.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/keras\n  copying horovod\/spark\/keras\/tensorflow.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/keras\n  copying horovod\/spark\/keras\/util.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/keras\n  copying horovod\/spark\/keras\/bare.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/keras\n  creating build\/lib.linux-x86_64-3.7\/horovod\/spark\/common\n  copying horovod\/spark\/common\/store.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/common\n  copying horovod\/spark\/common\/_namedtuple_fix.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/common\n  copying horovod\/spark\/common\/serialization.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/common\n  copying horovod\/spark\/common\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/common\n  copying horovod\/spark\/common\/params.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/common\n  copying horovod\/spark\/common\/estimator.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/common\n  copying horovod\/spark\/common\/util.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/common\n  copying horovod\/spark\/common\/backend.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/common\n  copying horovod\/spark\/common\/constants.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/common\n  copying horovod\/spark\/common\/cache.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/common\n  creating build\/lib.linux-x86_64-3.7\/horovod\/spark\/task\n  copying horovod\/spark\/task\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/task\n  copying horovod\/spark\/task\/task_info.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/task\n  copying horovod\/spark\/task\/mpirun_exec_fn.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/task\n  copying horovod\/spark\/task\/gloo_exec_fn.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/task\n  copying horovod\/spark\/task\/task_service.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/task\n  creating build\/lib.linux-x86_64-3.7\/horovod\/spark\/driver\n  copying horovod\/spark\/driver\/job_id.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/driver\n  copying horovod\/spark\/driver\/__init__.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/driver\n  copying horovod\/spark\/driver\/driver_service.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/driver\n  copying horovod\/spark\/driver\/host_discovery.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/driver\n  copying horovod\/spark\/driver\/rendezvous.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/driver\n  copying horovod\/spark\/driver\/rsh.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/driver\n  copying horovod\/spark\/driver\/mpirun_rsh.py -&gt; build\/lib.linux-x86_64-3.7\/horovod\/spark\/driver\n  running build_ext\n  -- Could not find CCache. Consider installing CCache to speed up compilation.\n  -- The CXX compiler identification is GNU 7.5.0\n  -- Check for working CXX compiler: \/usr\/bin\/c++\n  -- Check for working CXX compiler: \/usr\/bin\/c++ -- works\n  -- Detecting CXX compiler ABI info\n  -- Detecting CXX compiler ABI info - done\n  -- Detecting CXX compile features\n  -- Detecting CXX compile features - done\n  -- Build architecture flags: -mf16c -mavx -mfma\n  -- Using command \/azureml-envs\/tensorflow-2.4\/bin\/python\n  -- Found MPI_CXX: \/usr\/local\/lib\/libmpi.so (found version &quot;3.1&quot;)\n  -- Found MPI: TRUE (found version &quot;3.1&quot;)\n  -- Found CUDA: \/usr\/local\/cuda (found version &quot;11.0&quot;)\n  -- Linking against static NCCL library\n  -- Found NCCL: \/usr\/include\n  -- Determining NCCL version from the header file: \/usr\/include\/nccl.h\n  -- NCCL_MAJOR_VERSION: 2\n  -- Found NCCL (include: \/usr\/include, library: \/usr\/lib\/x86_64-linux-gnu\/libnccl_static.a)\n  -- The C compiler identification is GNU 7.5.0\n  -- Check for working C compiler: \/usr\/bin\/cc\n  -- Check for working C compiler: \/usr\/bin\/cc -- works\n  -- Detecting C compiler ABI info\n  -- Detecting C compiler ABI info - done\n  -- Detecting C compile features\n  -- Detecting C compile features - done\n  -- Found MPI_C: \/usr\/local\/lib\/libmpi.so (found version &quot;3.1&quot;)\n  -- Found MPI: TRUE (found version &quot;3.1&quot;)\n  -- MPI include path: \/usr\/local\/include\n  -- MPI libraries: \/usr\/local\/lib\/libmpi.so\n  CMake Error at \/usr\/share\/cmake-3.10\/Modules\/FindPackageHandleStandardArgs.cmake:137 (message):\n    Could NOT find Tensorflow (missing: Tensorflow_LIBRARIES) (Required is at\n    least version &quot;1.15.0&quot;)\n  Call Stack (most recent call first):\n    \/usr\/share\/cmake-3.10\/Modules\/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE)\n    cmake\/Modules\/FindTensorflow.cmake:31 (find_package_handle_standard_args)\n    horovod\/tensorflow\/CMakeLists.txt:12 (find_package)\n  \n  \n  -- Configuring incomplete, errors occurred!\n  See also &quot;\/tmp\/pip-install-pjyu9d6m\/horovod\/build\/temp.linux-x86_64-3.7\/CMakeFiles\/CMakeOutput.log&quot;.\n  Traceback (most recent call last):\n    File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\n    File &quot;\/tmp\/pip-install-pjyu9d6m\/horovod\/setup.py&quot;, line 188, in &lt;module&gt;\n      'horovodrun = horovod.runner.launch:run_commandline'\n    File &quot;\/azureml-envs\/tensorflow-2.4\/lib\/python3.7\/site-packages\/setuptools\/__init__.py&quot;, line 153, in setup\n      return distutils.core.setup(**attrs)\n    File &quot;\/azureml-envs\/tensorflow-2.4\/lib\/python3.7\/distutils\/core.py&quot;, line 148, in setup\n      dist.run_commands()\n    File &quot;\/azureml-envs\/tensorflow-2.4\/lib\/python3.7\/distutils\/dist.py&quot;, line 966, in run_commands\n      self.run_command(cmd)\n    File &quot;\/azureml-envs\/tensorflow-2.4\/lib\/python3.7\/distutils\/dist.py&quot;, line 985, in run_command\n      cmd_obj.run()\n    File &quot;\/azureml-envs\/tensorflow-2.4\/lib\/python3.7\/site-packages\/wheel\/bdist_wheel.py&quot;, line 299, in run\n      self.run_command('build')\n    File &quot;\/azureml-envs\/tensorflow-2.4\/lib\/python3.7\/distutils\/cmd.py&quot;, line 313, in run_command\n      self.distribution.run_command(command)\n    File &quot;\/azureml-envs\/tensorflow-2.4\/lib\/python3.7\/distutils\/dist.py&quot;, line 985, in run_command\n      cmd_obj.run()\n    File &quot;\/azureml-envs\/tensorflow-2.4\/lib\/python3.7\/distutils\/command\/build.py&quot;, line 135, in run\n      self.run_command(cmd_name)\n    File &quot;\/azureml-envs\/tensorflow-2.4\/lib\/python3.7\/distutils\/cmd.py&quot;, line 313, in run_command\n      self.distribution.run_command(command)\n    File &quot;\/azureml-envs\/tensorflow-2.4\/lib\/python3.7\/distutils\/dist.py&quot;, line 985, in run_command\n      cmd_obj.run()\n    File &quot;\/azureml-envs\/tensorflow-2.4\/lib\/python3.7\/site-packages\/setuptools\/command\/build_ext.py&quot;, line 79, in run\n      _build_ext.run(self)\n    File &quot;\/azureml-envs\/tensorflow-2.4\/lib\/python3.7\/distutils\/command\/build_ext.py&quot;, line 340, in run\n      self.build_extensions()\n    File &quot;\/tmp\/pip-install-pjyu9d6m\/horovod\/setup.py&quot;, line 89, in build_extensions\n      cwd=self.build_temp)\n    File &quot;\/azureml-envs\/tensorflow-2.4\/lib\/python3.7\/subprocess.py&quot;, line 363, in check_call\n      raise CalledProcessError(retcode, cmd)\n  subprocess.CalledProcessError: Command '['cmake', '\/tmp\/pip-install-pjyu9d6m\/horovod', '-DCMAKE_BUILD_TYPE=RelWithDebInfo', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY_RELWITHDEBINFO=\/tmp\/pip-install-pjyu9d6m\/horovod\/build\/lib.linux-x86_64-3.7', '-DPYTHON_EXECUTABLE:FILEPATH=\/azureml-envs\/tensorflow-2.4\/bin\/python']' returned non-zero exit status 1.\n  ----------------------------------------\n  ERROR: Failed building wheel for horovod\n<\/code><\/pre>\n<p>I'm new to Azure-ml and I'm finding the documentation a little unclear. I have also tried simply adding opencv-python to the existing curated environment by doing conda_dep.add_pip_package(&quot;opencv-python&quot;). The result is the same.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1634003102133,
        "Question_score":1,
        "Question_tags":"azure|tensorflow|opencv|azure-machine-learning-studio|horovod",
        "Question_view_count":435,
        "Owner_creation_time":1367269683453,
        "Owner_last_access_time":1635573315703,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1634028496832,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69534189",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62171724,
        "Question_title":"How can I access the Workspace object from a training script in AzureML?",
        "Question_body":"<p>I want to access the Workspace object in my <code>train.py<\/code> script, when running in an Estimator.  <\/p>\n\n<p>I currently can access the Run object, using the following code:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>run = Run.get_context()\n<\/code><\/pre>\n\n<p>But I cannot seem to get my hands on the Workspace object in my training script.  I would use this mostly to get access to the Datastores and Datasets (as I would hope to keep all data set references inside the training script, instead of passing them as input datasets)<\/p>\n\n<p>Any idea if\/how this is possible ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591183699477,
        "Question_score":4,
        "Question_tags":"python|azure|azure-machine-learning-service",
        "Question_view_count":1148,
        "Owner_creation_time":1360655430743,
        "Owner_last_access_time":1663784892907,
        "Owner_location":"Belgium",
        "Owner_reputation":2947,
        "Owner_up_votes":297,
        "Owner_down_votes":16,
        "Owner_views":355,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Sure, try this:<\/p>\n\n<pre><code>from azureml.core.run import Run\nrun = Run.get_context()\nws = run.experiment.workspace\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1591197955080,
        "Answer_score":12.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62171724",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71321757,
        "Question_title":"What are valid Azure ML Workspace connection argument options?",
        "Question_body":"<p>I want to build an Azure ML environment with two python packages that I have in Azure Devops.\nFor this I need a workspace connection to Azure Devops. One package is published to an artifact feed and I can access it using the python SDK using a personal access token:<\/p>\n<pre><code>ws.set_connection(name=&quot;ConnectionName&quot;, \n                  category= &quot;PythonFeed&quot;, \n                  target = &quot;https:\/\/pkgs.dev.azure.com\/&quot;, \n                  authType = &quot;PAT&quot;, \n                  value = PAT_TOKEN)\n<\/code><\/pre>\n<p>However, for the other I need to get the package from the git repository in Azure Devops. The documentation of the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace.workspace?view=azure-ml-py#azureml-core-workspace-workspace-set-connection\" rel=\"nofollow noreferrer\">Python SDK<\/a> and the underlying <a href=\"https:\/\/docs.microsoft.com\/en-us\/rest\/api\/azureml\/workspace-connections\/create\" rel=\"nofollow noreferrer\">REST API<\/a> don't give the options for the arguments, only that they need to be strings (see links).<\/p>\n<p>My question: what are the options for the following arguments:<\/p>\n<ul>\n<li>authType<\/li>\n<li>category<\/li>\n<li>valueFormat<\/li>\n<\/ul>\n<p>And what do I need to set for target argument, so that I can connect to the Azure DevOps repository with potentially different authentication?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646219844250,
        "Question_score":0,
        "Question_tags":"python|azure-devops|azure-machine-learning-studio|azureml-python-sdk|azure-python-sdk",
        "Question_view_count":93,
        "Owner_creation_time":1646219230530,
        "Owner_last_access_time":1663852436163,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>To get the package from a Azure DevOps git repository you can change the target to the repository URL:<\/p>\n<pre><code>ws.set_connection(\n    name=&quot;ConnectionName&quot;, \n    category = &quot;PythonFeed&quot;,\n    target = &quot;https:\/\/dev.azure.com\/&lt;MY-ORG&gt;\/&lt;MY-PROJECT&gt;\/_git\/&lt;MY-REPO&gt;&quot;, \n    authType = &quot;PAT&quot;, \n    value = &lt;PAT-TOKEN&gt;)\n<\/code><\/pre>\n<p>Note here that there is no user specified in the URL (the standard &quot;clone&quot; URL in Azure DevOps also contains &quot;DevOps-Vx@&quot;).<\/p>\n<p>As for any other options for &quot;authType&quot;, &quot;category&quot; and &quot;valueFormat&quot;, I don't know.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1659435013616,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71321757",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":48735257,
        "Question_title":"Azure ML studio web service serialization",
        "Question_body":"<p>I\u2019m trying to set up a web service from a Python notebook in azureml and I want it to return a dictionary of {string: float}. The floats serialize just fine, but the strings do not.<\/p>\n\n<p>This is the function I'm using:<\/p>\n\n<pre><code>def demoservice(N, Vy, My):\n    X = scaler.fit_transform([N, Vy, My])\n    res = clf.predict(X)\n    a = [ {'diam': x[0], 'radius': x[1], 'thickness': x[2]} for x in res]\n    return a\n<\/code><\/pre>\n\n<p>the call:<\/p>\n\n<pre><code>demoservice(\"140\", \"100\", \"0\")\n<\/code><\/pre>\n\n<p>correctly returns:<\/p>\n\n<pre><code>[{'diam': 16.0, 'radius': 2.0, 'thickness': 5.0}]\n<\/code><\/pre>\n\n<p>but the web call returns this json response:<\/p>\n\n<pre><code>{\"Results\":{\"output1\":{\"type\":\"table\",\"value\":{\"Values\":[[\"{\\\"type\\\": \\\"list\\\", \\\"value\\\": [{\\\"type\\\": \\\"dict\\\", \\\"value\\\": [[{\\\"type\\\": \\\"bytes\\\", \\\"value\\\": \\\"ZGlhbQ==\\\"}, {\\\"type\\\": \\\"float\\\", \\\"value\\\": \\\"16.0\\\"}], [{\\\"type\\\": \\\"bytes\\\", \\\"value\\\": \\\"cmFkaXVz\\\"}, {\\\"type\\\": \\\"float\\\", \\\"value\\\": \\\"2.0\\\"}], [{\\\"type\\\": \\\"bytes\\\", \\\"value\\\": \\\"dGhpY2tuZXNz\\\"}, {\\\"type\\\": \\\"float\\\", \\\"value\\\": \\\"5.0\\\"}]]}]}\"]]}},\"output2\":{\"type\":\"table\",\"value\":{\"Values\":[[\"data:text\/plain,Execution OK\\r\\n\",null]]}}}}\n<\/code><\/pre>\n\n<p>As you can see, in the response the dictionary's keys are not being serialized correctly. For example:<\/p>\n\n<pre><code>{\\\"type\\\": \\\"bytes\\\", \\\"value\\\": \\\"cmFkaXVz\\\"}\n<\/code><\/pre>\n\n<p>here I have <code>cmFkaXVz<\/code> instead of a readable value.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1518375728430,
        "Question_score":2,
        "Question_tags":"python|web-services|azure-machine-learning-studio",
        "Question_view_count":87,
        "Owner_creation_time":1376906609053,
        "Owner_last_access_time":1663886929643,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1518457477892,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48735257",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":47668450,
        "Question_title":"How to upload .r file into azure ml studio and run it?",
        "Question_body":"<p>I have a R file and I want to run the same in azureML studio. \nAfter running the codes in Rstudio I zip the r file and import it into Azure studio's datasets.I pull the dataset and Execute R script module to the experiment and attach script bundle port to the zip file. It asks for a src path which I am not sure of. When I run, it says CONNECTION NOT FOUND. <\/p>\n\n<p>What should be done to find the connection?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1512542937367,
        "Question_score":0,
        "Question_tags":"r|azure|azure-machine-learning-studio",
        "Question_view_count":288,
        "Owner_creation_time":1480265514083,
        "Owner_last_access_time":1660724770093,
        "Owner_location":null,
        "Owner_reputation":913,
        "Owner_up_votes":16,
        "Owner_down_votes":1,
        "Owner_views":318,
        "Question_last_edit_time":1512547937872,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47668450",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":50987140,
        "Question_title":"Getting missing values after uploading CSV to Azure ML studio",
        "Question_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/wLzX0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wLzX0.png\" alt=\"enter image description here\"><\/a>I observed that missing values are generated after uploading the CSV to Azure ML studio.<\/p>\n\n<p>I have re checked with R , Python and Excel just to confirm there are no missing values in my data. And there are none.<\/p>\n\n<p>The columns are of strings and integer, both are containing missing values after uploading my CSV into Azure ML studio<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1529667611530,
        "Question_score":0,
        "Question_tags":"azure|csv|azure-machine-learning-studio",
        "Question_view_count":130,
        "Owner_creation_time":1479194627133,
        "Owner_last_access_time":1531388750387,
        "Owner_location":null,
        "Owner_reputation":2713,
        "Owner_up_votes":26,
        "Owner_down_votes":2,
        "Owner_views":358,
        "Question_last_edit_time":1529907200436,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50987140",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73460989,
        "Question_title":"azureml Submitting deployment to compute taking very long",
        "Question_body":"<p>Azureml is stuck on submitting deployment to compute for a very long time. how can I speed this up? is it because of cpu and memory or due to other reasons?<\/p>\n<pre><code>log output \n\nRunning\n2022-08-23 14:51:11+00:00 Creating Container Registry if not exists.\n2022-08-23 14:51:11+00:00 Registering the environment.\n2022-08-23 14:51:13+00:00 Use the existing image.\n2022-08-23 14:51:13+00:00 Generating deployment configuration.\n2022-08-23 14:51:24+00:00 Submitting deployment to compute..\n<\/code><\/pre>\n<p>code<\/p>\n<pre><code>#Define the deployment configuration\naciconfig = AciWebservice.deploy_configuration(\n    cpu_cores = 1,\n    memory_gb = 1,\n    dns_name_label = os.environ['ACI_DNS_NAME_LABEL']\n)\n\nenv = Environment.from_conda_specification(&quot;env&quot;, &quot;..\/Environments\/score_env.yml&quot;)\n\ninf_conf = InferenceConfig(entry_script=&quot;score.py&quot;,environment=env)\n\n#deploy successful  models as a web service \nwebservice_name = os.environ['WEB_SERVICE_NAME']\nretries = 2\nwhile retries &gt; 0:\n    try:\n        service = Model.deploy(ws, webservice_name,models_latest,inf_conf,aciconfig, overwrite=True)\n        service.wait_for_deployment(True)\n        print(&quot;Webservice updated&quot;)\n        break\n\n    except:\n        print(service.get_logs())\n        retries -= 1\n        if retries == 0:\n            raise\n\n\n    \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661266766740,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-service|azureml-python-sdk|azure-ml-pipelines",
        "Question_view_count":31,
        "Owner_creation_time":1635334121057,
        "Owner_last_access_time":1663856043980,
        "Owner_location":null,
        "Owner_reputation":128,
        "Owner_up_votes":28,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":1661424003943,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73460989",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":56666924,
        "Question_title":"Is there a Java SDK for azure machine learning service?",
        "Question_body":"<p>Is there a Java SDK for Azure Machine Learning service? If not, is there a way to create Azure ML pipelines, experiments etc from Java codebase?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1560944696657,
        "Question_score":0,
        "Question_tags":"azure|azure-java-sdk|azure-machine-learning-service",
        "Question_view_count":606,
        "Owner_creation_time":1437373119813,
        "Owner_last_access_time":1632321804710,
        "Owner_location":null,
        "Owner_reputation":132,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56666924",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71919389,
        "Question_title":"Azureml TabularDataset to_pandas_dataframe() returns InvalidEncoding error",
        "Question_body":"<p>When I run:<\/p>\n<pre><code>datasetTabular = Dataset.get_by_name(ws, &quot;&lt;Redacted&gt;&quot;)\ndatasetTabular.to_pandas_dataframe()\n<\/code><\/pre>\n<p>The following error is returned.  What can I do to get past this?<\/p>\n<pre><code>ExecutionError                            Traceback (most recent call last) File C:\\ProgramData\\Anaconda3_2\\envs\\amlds\\lib\\site-packages\\azureml\\data\\dataset_error_handling.py:101, in _try_execute(action, operation, dataset_info, **kwargs)\n    100     else:\n--&gt; 101         return action()\n    102 except Exception as e:\n\nFile C:\\ProgramData\\Anaconda3_2\\envs\\amlds\\lib\\site-packages\\azureml\\data\\tabular_dataset.py:169, in TabularDataset.to_pandas_dataframe.&lt;locals&gt;.&lt;lambda&gt;()\n    168 dataflow = get_dataflow_for_execution(self._dataflow, 'to_pandas_dataframe', 'TabularDataset')\n--&gt; 169 df = _try_execute(lambda: dataflow.to_pandas_dataframe(on_error=on_error,\n    170                                                        out_of_range_datetime=out_of_range_datetime),\n    171                   'to_pandas_dataframe',\n    172                   None if self.id is None else {'id': self.id, 'name': self.name, 'version': self.version})\n    173 fine_grain_timestamp = self._properties.get(_DATASET_PROP_TIMESTAMP_FINE, None)\n\nFile C:\\ProgramData\\Anaconda3_2\\envs\\amlds\\lib\\site-packages\\azureml\\dataprep\\api\\_loggerfactory.py:213, in track.&lt;locals&gt;.monitor.&lt;locals&gt;.wrapper(*args, **kwargs)\n    212 try:\n--&gt; 213     return func(*args, **kwargs)\n    214 except Exception as e:\n\nFile C:\\ProgramData\\Anaconda3_2\\envs\\amlds\\lib\\site-packages\\azureml\\dataprep\\api\\dataflow.py:697, in Dataflow.to_pandas_dataframe(self, extended_types, nulls_as_nan, on_error, out_of_range_datetime)\n    696 with tracer.start_as_current_span('Dataflow.to_pandas_dataframe', trace.get_current_span()) as span:\n--&gt; 697     return get_dataframe_reader().to_pandas_dataframe(self,\n    698                                                       extended_types,\n    699                                                       nulls_as_nan,\n    700                                                       on_error,\n    701                                                       out_of_range_datetime,\n    702                                                       to_dprep_span_context(span.get_context()))\n\nFile C:\\ProgramData\\Anaconda3_2\\envs\\amlds\\lib\\site-packages\\azureml\\dataprep\\api\\_dataframereader.py:386, in _DataFrameReader.to_pandas_dataframe(self, dataflow, extended_types, nulls_as_nan, on_error, out_of_range_datetime, span_context)\n    384     if have_pyarrow() and not extended_types and not inconsistent_schema:\n    385         # if arrow is supported, and we didn't get inconsistent schema, and extended typed were not asked for - fallback to feather\n--&gt; 386         return clex_feather_to_pandas()\n    387 except _InconsistentSchemaError as e:\n\nFile C:\\ProgramData\\Anaconda3_2\\envs\\amlds\\lib\\site-packages\\azureml\\dataprep\\api\\_dataframereader.py:298, in\n_DataFrameReader.to_pandas_dataframe.&lt;locals&gt;.clex_feather_to_pandas()\n    297 activity_data = dataflow_to_execute._dataflow_to_anonymous_activity_data(dataflow_to_execute)\n--&gt; 298 dataflow._engine_api.execute_anonymous_activity(\n    299     ExecuteAnonymousActivityMessageArguments(anonymous_activity=activity_data, span_context=span_context))\n    301 try:\n\nFile C:\\ProgramData\\Anaconda3_2\\envs\\amlds\\lib\\site-packages\\azureml\\dataprep\\api\\_aml_helper.py:38, in update_aml_env_vars.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(op_code, message, cancellation_token)\n     37     engine_api_func().update_environment_variable(changed)\n---&gt; 38 return send_message_func(op_code, message, cancellation_token)\n\nFile C:\\ProgramData\\Anaconda3_2\\envs\\amlds\\lib\\site-packages\\azureml\\dataprep\\api\\engineapi\\api.py:160, in EngineAPI.execute_anonymous_activity(self, message_args, cancellation_token)\n    158 @update_aml_env_vars(get_engine_api)\n    159 def execute_anonymous_activity(self, message_args: typedefinitions.ExecuteAnonymousActivityMessageArguments, cancellation_token: CancellationToken = None) -&gt; None:\n--&gt; 160     response = self._message_channel.send_message('Engine.ExecuteActivity', message_args, cancellation_token)\n    161     return response\n\nFile C:\\ProgramData\\Anaconda3_2\\envs\\amlds\\lib\\site-packages\\azureml\\dataprep\\api\\engineapi\\engine.py:291, in MultiThreadMessageChannel.send_message(self, op_code, message, cancellation_token)\n    290     cancel_on_error()\n--&gt; 291     raise_engine_error(response['error'])\n    292 else:\n\nFile C:\\ProgramData\\Anaconda3_2\\envs\\amlds\\lib\\site-packages\\azureml\\dataprep\\api\\errorhandlers.py:10, in raise_engine_error(error_response)\n      9 if 'ScriptExecution' in error_code:\n---&gt; 10     raise ExecutionError(error_response)\n     11 if 'Validation' in error_code:\n\nExecutionError:  Error Code: ScriptExecution.StreamAccess.Validation Validation Error Code: InvalidEncoding Validation Target: TextFile Failed Step: 78059bb0-278f-4c7f-9c21-01a0cccf7b96 Error Message: ScriptExecutionException was caused by StreamAccessException.   StreamAccessException was caused by ValidationException.\n    Unable to read file using Unicode (UTF-8). Attempted read range 0:777. Lines read in the range 0. Decoding error: Unable to translate bytes [8B] at index 1 from specified code page to Unicode.\n      Unable to translate bytes [8B] at index 1 from specified code page to Unicode. | session_id=295acf7e-4af9-42f1-b04a-79f3c5a0f98c\n\nDuring handling of the above exception, another exception occurred:\n\nUserErrorException                        Traceback (most recent call last) Input In [34], in &lt;module&gt;\n      1 # preview the first 3 rows of the dataset\n      2 #datasetTabular.take(3)\n----&gt; 3 datasetTabular.take(3).to_pandas_dataframe()\n\nFile C:\\ProgramData\\Anaconda3_2\\envs\\amlds\\lib\\site-packages\\azureml\\data\\_loggerfactory.py:132, in track.&lt;locals&gt;.monitor.&lt;locals&gt;.wrapper(*args, **kwargs)\n    130 with _LoggerFactory.track_activity(logger, func.__name__, activity_type, custom_dimensions) as al:\n    131     try:\n--&gt; 132         return func(*args, **kwargs)\n    133     except Exception as e:\n    134         if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):\n\nFile C:\\ProgramData\\Anaconda3_2\\envs\\amlds\\lib\\site-packages\\azureml\\data\\tabular_dataset.py:169, in TabularDataset.to_pandas_dataframe(self, on_error, out_of_range_datetime)\n    158 &quot;&quot;&quot;Load all records from the dataset into a pandas DataFrame.\n    159 \n    160 :param on_error: How to handle any error values in the dataset, such as those produced by an error while    (...)\n    166 :rtype: pandas.DataFrame\n    167 &quot;&quot;&quot;\n    168 dataflow = get_dataflow_for_execution(self._dataflow, 'to_pandas_dataframe', 'TabularDataset')\n--&gt; 169 df = _try_execute(lambda: dataflow.to_pandas_dataframe(on_error=on_error,\n    170                                                        out_of_range_datetime=out_of_range_datetime),\n    171                   'to_pandas_dataframe',\n    172                   None if self.id is None else {'id': self.id, 'name': self.name, 'version': self.version})\n    173 fine_grain_timestamp = self._properties.get(_DATASET_PROP_TIMESTAMP_FINE, None)\n    175 if fine_grain_timestamp is not None and df.empty is False:\n\nFile C:\\ProgramData\\Anaconda3_2\\envs\\amlds\\lib\\site-packages\\azureml\\data\\dataset_error_handling.py:104, in _try_execute(action, operation, dataset_info, **kwargs)\n    102 except Exception as e:\n    103     message, is_dprep_exception = _construct_message_and_check_exception_type(e, dataset_info, operation)\n--&gt; 104     _dataprep_error_handler(e, message, is_dprep_exception)\n\nFile C:\\ProgramData\\Anaconda3_2\\envs\\amlds\\lib\\site-packages\\azureml\\data\\dataset_error_handling.py:154, in _dataprep_error_handler(e, message, is_dprep_exception)\n    152     for item in user_exception_list:\n    153         if _contains(item, getattr(e, 'error_code', 'Unexpected')):\n--&gt; 154             raise UserErrorException(message, inner_exception=e)\n    156 raise AzureMLException(message, inner_exception=e)\n\nUserErrorException: UserErrorException:     Message: Execution failed with error message: ScriptExecutionException was caused by StreamAccessException.   StreamAccessException was caused by ValidationException.\n    Unable to read file using Unicode (UTF-8). Attempted read range 0:777. Lines read in the range 0. Decoding error: [REDACTED]\n      Failed due to inner exception of type: DecoderFallbackException | session_id=295acf7e-4af9-42f1-b04a-79f3c5a0f98c ErrorCode: ScriptExecution.StreamAccess.Validation  InnerException  Error Code: ScriptExecution.StreamAccess.Validation Validation Error Code: InvalidEncoding Validation Target: TextFile Failed Step: 78059bb0-278f-4c7f-9c21-01a0cccf7b96 Error Message: ScriptExecutionException was caused by StreamAccessException.   StreamAccessException was caused by ValidationException.\n    Unable to read file using Unicode (UTF-8). Attempted read range 0:777. Lines read in the range 0. Decoding error: Unable to translate bytes [8B] at index 1 from specified code page to Unicode.\n      Unable to translate bytes [8B] at index 1 from specified code page to Unicode. | session_id=295acf7e-4af9-42f1-b04a-79f3c5a0f98c  ErrorResponse  {\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;Execution failed with error message: ScriptExecutionException was caused by StreamAccessException.\\r\\n  StreamAccessException was caused by ValidationException.\\r\\n    Unable to read file using Unicode (UTF-8). Attempted read range 0:777. Lines read in the range 0. Decoding error: [REDACTED]\\r\\n      Failed due to inner exception of type: DecoderFallbackException\\r\\n| session_id=295acf7e-4af9-42f1-b04a-79f3c5a0f98c ErrorCode: ScriptExecution.StreamAccess.Validation&quot;\n    } }\n\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1650339397280,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":335,
        "Owner_creation_time":1650337408350,
        "Owner_last_access_time":1653101508433,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1650489525383,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71919389",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":42019977,
        "Question_title":"AzureML - Trained clustering Model cannot be connected with Web Service Output",
        "Question_body":"<p>In AzureML it is not possible to connect trained Clustering Model with web service output. <\/p>\n\n<p>Why does AzureML only allow ILearnerDotNet to be connected with Web Service Output and not IClusterDotNet? <\/p>\n\n<p>This is a serious bug which halts clustering models from deploying them as a web service.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1486111021187,
        "Question_score":1,
        "Question_tags":"azure|machine-learning|cluster-analysis|azure-machine-learning-studio",
        "Question_view_count":192,
        "Owner_creation_time":1452608563363,
        "Owner_last_access_time":1562103924250,
        "Owner_location":null,
        "Owner_reputation":105,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":1486111807267,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/42019977",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":45009184,
        "Question_title":"Powershell AzureML Get-AmlWorkspace",
        "Question_body":"<pre><code>Get-AmlWorkspace : One or more errors occurred.\nAt line:1 char:1\n+ Get-AmlWorkspace\n+ ~~~~~~~~~~~~~~~~\n+ CategoryInfo          : NotSpecified: (:) [Get-AmlWorkspace], \nAggregateException\n+ FullyQualifiedErrorId : \nSystem.AggregateException,AzureML.PowerShell.GetWorkspace\n<\/code><\/pre>\n\n<p>I am trying to use Powershell to connect to Azure ML studio as it looks like an easier way to manage a workspace. I've downloaded the dll file from <a href=\"https:\/\/github.com\/hning86\/azuremlps\" rel=\"nofollow noreferrer\">https:\/\/github.com\/hning86\/azuremlps<\/a> and changed my config.json file, but get the error above if I try to run any AzureML commands. I've unblocked the DLL file and imported the AzureMLPS module, and I can see the module and commands I am trying to use have been imported by doing <code>Get-Module<\/code> and <code>Get-Command<\/code><\/p>\n\n<p>For info I've not used Powershell before.<\/p>\n\n<p>Any suggestions much appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1499681034153,
        "Question_score":1,
        "Question_tags":"powershell|azure-machine-learning-studio",
        "Question_view_count":428,
        "Owner_creation_time":1397507727100,
        "Owner_last_access_time":1663075667463,
        "Owner_location":null,
        "Owner_reputation":340,
        "Owner_up_votes":60,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Have you installed Azure PowerShell Installer on your local machine?\n<strong><a href=\"https:\/\/github.com\/Azure\/azure-powershell\/releases\" rel=\"nofollow noreferrer\">Click here<\/a><\/strong> for more info.<\/p>\n\n<p>Download the latest <strong>Azure PowerShell Installer (4.3.1)<\/strong>, then install on your local machine. Then retry using Azure PowerShell module and commands.<\/p>\n\n<p>I installed mine last May, using Azure PowerShell 4.0.1, and the command Get-AmlWorkspace is working.<\/p>\n\n<pre><code># Set local folder location\nSet-Location -Path \"C:\\Insert here the location of AzureMLPS.dll\"\n\n# Unblock and import Azure Powershell Module (leverages config.json file)\nUnblock-File .\\AzureMLPS.dll\nImport-Module .\\AzureMLPS.dll\n\n# Get Azure ML Workspace info\nGet-AmlWorkspace\n<\/code><\/pre>\n\n<p>The output on my side looks like this:\n<a href=\"https:\/\/i.stack.imgur.com\/mEGeT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mEGeT.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1503389654689,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45009184",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62128489,
        "Question_title":"Azure Machine Learning (preview) to Customer Insights",
        "Question_body":"<p>I am trying to integrate MS Dynamics Customer Insights (CI) with the model I have built within the new Azure Machine Learning (designer). Currently, I see there is only an integration between CI and Azure Machine Learning studio (classic). <\/p>\n\n<p>I have deployed my model behind a web service (REST) within new Azure Machine Learning however it is not getting picked up in CI. However, I am able to score\/generate predictions from the API using a Python script. <\/p>\n\n<p>Please recommend a way to integrate these two MS services or suggest an architecture where CI can pick up the results. <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1591000924097,
        "Question_score":2,
        "Question_tags":"azure|microsoft-dynamics|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":107,
        "Owner_creation_time":1369068264257,
        "Owner_last_access_time":1663772456043,
        "Owner_location":"South Africa",
        "Owner_reputation":2907,
        "Owner_up_votes":68,
        "Owner_down_votes":8,
        "Owner_views":402,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62128489",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":36063443,
        "Question_title":"Why is this SQLite query not letting me cast the integer and subtract?",
        "Question_body":"<p>I have multiple columns in Azure Machine Learning that each have an hour, year, day, minute, etc for a date. I need to convert this hour from UTC to EDT, and then make it a date string such as<\/p>\n\n<blockquote>\n  <p>YYYY\/MM\/DD HH:SS<\/p>\n<\/blockquote>\n\n<p>This way, I can do an inner join. I've tried using CAST, CONVERT, and other SQLite functions, but none of these combos work. Here is where I am now: <\/p>\n\n<pre><code>select *\nCAST([Col11] as int) -4 as EDTHour\n\n([Col8] || '\/' || [Col9] || '\/' || [Col10] || ' ' || EDTHour|| ':' || [Col12]) as WeatherTime from t1\n\nselect 'Time Stamp' as secondTableTime from t2\n\nSELECT *\nFROM t1\nINNER JOIN t2\nON t1.WeatherTime=t2.secondTableTime\n<\/code><\/pre>\n\n<p>However, It never lets me cast the varchar column Col11 to a integer or decimal. What am I missing? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1458224867397,
        "Question_score":0,
        "Question_tags":"sqlite|azure|timezone|cortana-intelligence|azure-machine-learning-studio",
        "Question_view_count":64,
        "Owner_creation_time":1389108027810,
        "Owner_last_access_time":1580749761847,
        "Owner_location":"Texas",
        "Owner_reputation":2420,
        "Owner_up_votes":480,
        "Owner_down_votes":3,
        "Owner_views":296,
        "Question_last_edit_time":1458336187940,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36063443",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60654578,
        "Question_title":"ModelErrorResponseException: Unauthorized Azure ML authorization bug",
        "Question_body":"<p>after successfully running a training estimator and experiment in an azure ml notebook I am being given Unauthorized errors when trying to register the model. I also see an unauthorized bar pop up in the top when I look at the estimator or the models tab in the azure portal. <\/p>\n\n<p>This seems like it could be a resource group issue, but I only have one resource group. Has anyone had this issue before?<\/p>\n\n<p>successful experiment:<\/p>\n\n<pre><code>from azureml.core.experiment import Experiment\n\nscript_params = {\n#     '--num_epochs': 3,\n    '--output_dir': '.\/outputs'\n}\n\nestimator = PyTorch(source_directory=os.path.join(os.getcwd(), 'Estimator'), \n                    script_params=script_params,\n                    compute_target=compute_target,\n                    entry_script='train.py',\n                    use_gpu=True,\n                    pip_packages=['pillow==5.4.1', 'torch', 'numpy'])\n\nexperiment_name = 'pytorch-rnn-generator'\nexperiment = Experiment(ws, name=experiment_name)\n\nrun = experiment.submit(estimator)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n\n<p>model registration:<\/p>\n\n<pre><code>model = run.register_model(model_name='rnn-tv-script-gen', model_path='outputs\/')\n<\/code><\/pre>\n\n<p>The stack trace:<\/p>\n\n<pre><code>ModelErrorResponseException               Traceback (most recent call last)\n&lt;ipython-input-6-178d7ee9830a&gt; in &lt;module&gt;\n      1 from azureml.core.model import Model\n      2 \n----&gt; 3 model = run.register_model(model_name='rnn-tv-script-gen', model_path='outputs\/')\n      4 \n      5 servive = Model.deploy(ws, \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/run.py in register_model(self, model_name, model_path, tags, properties, model_framework, model_framework_version, description, datasets, sample_input_dataset, sample_output_dataset, resource_configuration, **kwargs)\n   1988             model_name, model_path, tags, properties, model_framework, model_framework_version,\n   1989             description=description, datasets=datasets, unpack=False, sample_input_dataset=sample_input_dataset,\n-&gt; 1990             sample_output_dataset=sample_output_dataset, resource_configuration=resource_configuration, **kwargs)\n   1991 \n   1992     def _update_dataset_lineage(self, datasets):\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_run_impl\/run_history_facade.py in register_model(self, model_name, model_path, tags, properties, model_framework, model_framework_version, asset_id, sample_input_dataset, sample_output_dataset, resource_configuration, **kwargs)\n    386                                              artifacts,\n    387                                              metadata_dict=metadata_dict,\n--&gt; 388                                              run_id=self._run_id)\n    389             asset_id = asset.id\n    390         else:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_restclient\/assets_client.py in create_asset(self, model_name, artifact_values, metadata_dict, project_id, run_id, tags, properties)\n     50                    \"meta\": metadata_dict,\n     51                    \"CreatedTime\": created_time}\n---&gt; 52         return self._execute_with_workspace_arguments(self._client.asset.create, payload)\n     53 \n     54     def get_assets_by_run_id_and_name(self, run_id, name):\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_restclient\/workspace_client.py in _execute_with_workspace_arguments(self, func, *args, **kwargs)\n     69 \n     70     def _execute_with_workspace_arguments(self, func, *args, **kwargs):\n---&gt; 71         return self._execute_with_arguments(func, copy.deepcopy(self._workspace_arguments), *args, **kwargs)\n     72 \n     73     def _execute_with_arguments(self, func, args_list, *args, **kwargs):\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_restclient\/workspace_client.py in _execute_with_arguments(self, func, args_list, *args, **kwargs)\n     85                 return self._call_paginated_api(func, *args_list, **kwargs)\n     86             else:\n---&gt; 87                 return self._call_api(func, *args_list, **kwargs)\n     88         except ErrorResponseException as e:\n     89             raise ServiceException(e)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_restclient\/clientbase.py in _call_api(self, func, *args, **kwargs)\n    224                 return AsyncTask(future, _ident=ident, _parent_logger=self._logger)\n    225             else:\n--&gt; 226                 return self._execute_with_base_arguments(func, *args, **kwargs)\n    227 \n    228     def _call_paginated_api(self, func, *args, **kwargs):\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_restclient\/clientbase.py in _execute_with_base_arguments(self, func, *args, **kwargs)\n    277         total_retry = 0 if self.retries &lt; 0 else self.retries\n    278         return ClientBase._execute_func_internal(\n--&gt; 279             back_off, total_retry, self._logger, func, _noop_reset, *args, **kwargs)\n    280 \n    281     @classmethod\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_restclient\/clientbase.py in _execute_func_internal(cls, back_off, total_retry, logger, func, reset_func, *args, **kwargs)\n    292                 return func(*args, **kwargs)\n    293             except Exception as error:\n--&gt; 294                 left_retry = cls._handle_retry(back_off, left_retry, total_retry, error, logger, func)\n    295 \n    296             reset_func(*args, **kwargs)  # reset_func is expected to undo any side effects from a failed func call.\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_restclient\/clientbase.py in _handle_retry(cls, back_off, left_retry, total_retry, error, logger, func)\n    341                 back_off = DEFAULT_503_BACKOFF\n    342             elif error.response.status_code &lt; 500:\n--&gt; 343                 raise error\n    344         elif not isinstance(error, RETRY_EXCEPTIONS):\n    345             raise error\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_restclient\/clientbase.py in _execute_func_internal(cls, back_off, total_retry, logger, func, reset_func, *args, **kwargs)\n    290         while left_retry &gt;= 0:\n    291             try:\n--&gt; 292                 return func(*args, **kwargs)\n    293             except Exception as error:\n    294                 left_retry = cls._handle_retry(back_off, left_retry, total_retry, error, logger, func)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_restclient\/operations\/asset_operations.py in create(self, subscription_id, resource_group_name, workspace, asset, custom_headers, raw, **operation_config)\n     88 \n     89         if response.status_code not in [200]:\n---&gt; 90             raise models.ModelErrorResponseException(self._deserialize, response)\n     91 \n     92         deserialized = None\n\nModelErrorResponseException: Unauthorized\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1584017649797,
        "Question_score":1,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":194,
        "Owner_creation_time":1505519654173,
        "Owner_last_access_time":1663534361873,
        "Owner_location":null,
        "Owner_reputation":153,
        "Owner_up_votes":193,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Question_last_edit_time":1584180834096,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60654578",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64672881,
        "Question_title":"Cannot access registered dataset in AMLS pipeline",
        "Question_body":"<p>I have a service principal for my AMLS workspace that has been granted a storage blob contributor role to ADLS Gen 2. ADLS Gen 2 is behind a vnet, but using the service principal, I was able to register it as a datastore and register a csv file in ADLS Gen 2 as a dataset in my AMLS workspace. I am using azureml.core version 1.16.0<\/p>\n<p>Within my workspace, running<\/p>\n<pre><code>data = ws.datasets.get(&quot;csv data&quot;)\ndata.take(5).to_pandas_dataframe()\n<\/code><\/pre>\n<p>works with no issue. I would like to use this csv data as input into an ML pipeline run with a PythonScriptStep with inputs = [data.as_named_input('data')] after running data = ws.datasets.get(&quot;csv data&quot;). However, when I run the code<\/p>\n<pre><code>run = Run.get_context()\nrun.input_datasets['data'].to_pandas_dataframe()\n<\/code><\/pre>\n<p>in my pipeline script, it fails with an error<\/p>\n<pre><code>StreamAccessException was caused by AuthenticationException.\n'AdlsGen2-ReadHeaders' for '[REDACTED]' on storage failed with status code 'Forbidden' (This request is\nnot authorized to perform this operation.)\n<\/code><\/pre>\n<p>Where am I going wrong?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1604451299120,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service|vnet",
        "Question_view_count":185,
        "Owner_creation_time":1589738451347,
        "Owner_last_access_time":1656358607687,
        "Owner_location":null,
        "Owner_reputation":179,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":53,
        "Question_last_edit_time":1604455484396,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64672881",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":50739607,
        "Question_title":"Partition by Rows equivalent in pandas (python",
        "Question_body":"<p>I am using Azure Machine Learning Studio and what to add a running total on my dataset. This includes a date column, and I want to sum all the rows (for a group) on or before the row date.<\/p>\n\n<p>In SQL Server, I would use:<\/p>\n\n<pre><code>    SELECT [t1].*,\nSUM([t1].[Amount (Settlement CCY)) \nOVER (\n  PARTITION BY [t1].[Contract Ref], [t1].[LOBCode], [t1].[Superline], [t1].[Occupation], [t1].[TransType], [t1].[SettCCY]\n  ORDER BY     [t1].[Transaction Date] ASC\n  ROWS BETWEEN UNBOUNDED PRECEDING\n       AND     CURRENT ROW\n)\nFROM [t1]\nGROUP BY [t1].[contract ref], [t1].[Transaction date], [t1].[LOBCode], [t1].[Superline], [t1].[Occupation], [t1].[TransType], [t1].[SettCCY]\n<\/code><\/pre>\n\n<p>but Azure Machine learning uses SQLite where the Over \/ Partition clauses aren't implemented.<\/p>\n\n<p>I've tried an alternative in python\/pandas:<\/p>\n\n<pre><code>dataframe1 = dataframe1.assign(cumAMTscTD=dataframe1.groupby(['ContractRef', 'Basis', 'LOBCode', 'Superline', 'Occupation', 'TransType', 'SettCCY'])['AmtSettCCY'].transform('sum')).sort_values(['ContractRef','TransDate'])\n<\/code><\/pre>\n\n<p>but this sums up everything for the group, not just the those for the dates up toe current row. I assume therefore it doesn't cover the:<\/p>\n\n<pre><code>ROWS BETWEEN UNBOUNDED PRECEDING\n   AND     CURRENT ROW\n<\/code><\/pre>\n\n<p>How would I acheive this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1528369446120,
        "Question_score":0,
        "Question_tags":"python|sql|pandas|sqlite|azure-machine-learning-studio",
        "Question_view_count":744,
        "Owner_creation_time":1392992106703,
        "Owner_last_access_time":1552468853350,
        "Owner_location":null,
        "Owner_reputation":167,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Question_last_edit_time":1528370042660,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50739607",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58203287,
        "Question_title":"Using `inference_schema.schema_decorators` with dynamic `numpy` array shape",
        "Question_body":"<p><strong>Question summary<\/strong><\/p>\n\n<p>I'm deploying a model to an Azure Container Instance, using the Azure Machine Learning Service API. Specifically, the model is a PyTorch (fastai) model classifying images of varying shapes.<\/p>\n\n<p>Microsoft provides some nice decoraters to handle input and output data schemas in the scoring script. However, I am unable to figure out, if it's possible to use the <code>NumpyParameterType<\/code> with dynamic shape for the input.<\/p>\n\n<p><strong>Scoring script<\/strong><\/p>\n\n<p>A sample of the scoring script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pickle\nimport json\nimport numpy as np\nimport time\nimport os\n\nfrom PIL import Image as PilImage\nfrom azureml.core.model import Model\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\ndef preprocess_inference(img):\n    # Preprocessing handled here\n\ndef make_prediction(data_preprocessed):\n    # Model prediction handled here\n\ndef init():\n    global model\n\n    model_path = Model.get_model_path(model_name='my_pytorch_model',\n                                      version=1)\n\n    # Get paths\n    split_path = model_path.split('\/')\n\n    model = fastai.load_learner(path = '\/'.join(split_path[:-1]), file = split_path[-1])\n\n# How to use the schema decoraters with dynamic size?\ninput_sample = np.array(PilImage.open('src\/deployment\/test\/test_image.png'))\noutput_sample = np.array([0, None], dtype=np.object)\n\n@input_schema('raw_data', NumpyParameterType(input_sample))\n@output_schema(NumpyParameterType(output_sample))\ndef run(raw_data):\n\n    try:\n\n        data_preprocessed = preprocess_inference(raw_data)\n\n        prediction = make_prediction(data_preprocessed)\n\n        return prediction\n\n    except Exception as e:\n        error = str(e)\n        print (error + time.strftime(\"%H:%M:%S\"))\n        return error\n\n<\/code><\/pre>\n\n<p>Which only works if the image uploaded has the exact same shape as 'src\/deployment\/test\/test_image.png'. Right now my solution is to avoid the decoraters and do the data interpretation myself.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>\ndef run(raw_data):\n\n    try:\n\n        img = np.array(json.loads(raw_data)['raw_data'], dtype=np.uint8)\n        img = np.expand_dims(img, axis=2)\n\n        data_preprocessed = preprocess_inference(img)\n\n        prediction = make_prediction(data_preprocessed)\n\n        return prediction\n\n    except Exception as e:\n        error = str(e)\n        print (error + time.strftime(\"%H:%M:%S\"))\n        return error\n\n<\/code><\/pre>\n\n<p>But it would be nice to be able to use the decorators, such that endusers can benefit from the nice warning messages as well.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1570025484313,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":971,
        "Owner_creation_time":1377156004257,
        "Owner_last_access_time":1571123387553,
        "Owner_location":null,
        "Owner_reputation":26,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58203287",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73501103,
        "Question_title":"Getting Bad request while searching run in mlflow",
        "Question_body":"<p>Training a ml model with mlflow in azure environment.<\/p>\n<pre><code>import mlflow\nfrom mlflow import MlflowClient\nfrom azureml.core import Experiment, Workspace\n\nexperiment_name = 'housing-lin-mlflow'\n\nexperiment = Experiment(ws, experiment_name)\n\nruns = mlflow.search_runs(experiment_ids=[ experiment.id ])\n\n<\/code><\/pre>\n<p>While fetching runs from search_runs getting this error :<\/p>\n<pre><code>RestException: BAD_REQUEST: For input string: &quot;5b649b3c-3b8f-497a-bb4f&quot;\n<\/code><\/pre>\n<p>MLflow version : 1.28.0\nIn Azure studio jobs have been created and successfully run.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661517215980,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-service|mlflow",
        "Question_view_count":56,
        "Owner_creation_time":1582101477803,
        "Owner_last_access_time":1663953873503,
        "Owner_location":"Delhi, India",
        "Owner_reputation":171,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":53,
        "Question_last_edit_time":1661625379892,
        "Answer_body":"<p>The bad request in MLFlow after successful running the job is because of not giving proper API permissions for the application.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Search for <strong>MLFLOW<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Scroll down<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s50AL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s50AL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Click on View API Permissions<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Under API permissions, assign the permissions according to the application running region and requirements. Checkout the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-models-mlflow\" rel=\"nofollow noreferrer\">document<\/a> for further information.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1661603882123,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73501103",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71185505,
        "Question_title":"XGBClassifer, when de-serialized, gives 'XGBModel' object has no attribute 'enable_categorical'",
        "Question_body":"<p>I have a serialized XGBClassifier object, trained and generated using xgboost=1.5.2.<\/p>\n<pre><code>XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n              colsample_bynode=1, colsample_bytree=0.30140958911801474,\n              eval_metric='logloss', gamma=0.1203484640861413, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_bin=368, max_delta_step=0, max_depth=6,\n              min_child_weight=1, missing=nan, monotone_constraints='()',\n              n_estimators=100, n_jobs=6, num_parallel_tree=1, random_state=42,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n              single_precision_histogram=True, subsample=0.976171515775659,\n              tree_method='gpu_hist', use_label_encoder=False,\n              validate_parameters=1, verbosity=None)\n<\/code><\/pre>\n<p>I load the object using:<\/p>\n<pre><code>clf_model = joblib.load(model_path)\n<\/code><\/pre>\n<p>I want to use the object to predict on some data I am using Azure environment which also has xgboost=1.5.2. but it gives error:<\/p>\n<pre><code>File &quot;score.py&quot;, line 78, in score_execution\n[stderr]    clf_preds = clf_model.predict(clf_data_transformed)\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 1284, in predict\n[stderr]    class_probs = super().predict(\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 879, in predict\n[stderr]    if self._can_use_inplace_predict():\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 811, in _can_use_inplace_predict\n[stderr]    predictor = self.get_params().get(&quot;predictor&quot;, None)\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 505, in get_params\n[stderr]    params.update(cp.__class__.get_params(cp, deep))\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 502, in get_params\n[stderr]    params = super().get_params(deep)\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/sklearn\/base.py&quot;, line 210, in get_params\n[stderr]    value = getattr(self, key)\n[stderr]AttributeError: 'XGBModel' object has no attribute 'enable_categorical'\n<\/code><\/pre>\n<p>We have same version in pipelines that produce\/serialize the model and in the pipeline that deserialize the model to predict on new data.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645277798680,
        "Question_score":1,
        "Question_tags":"python|azure|azure-machine-learning-service",
        "Question_view_count":358,
        "Owner_creation_time":1443017464707,
        "Owner_last_access_time":1663923275743,
        "Owner_location":"Sweden",
        "Owner_reputation":644,
        "Owner_up_votes":33,
        "Owner_down_votes":1,
        "Owner_views":126,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Here are some possible solutions :<\/p>\n<ul>\n<li>Save the model in some other way, e.g. the JSON specified here <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html<\/a><\/li>\n<li>Limit the allowed range of xgboost versions to those that are known to work with our model. This could lead to issues in the future, for example if the aging version of xgboost we require is no longer supported by newer versions of Python.<\/li>\n<li>Using <code>save_model<\/code> to save in JSON is worth a shot to try.<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1645427212643,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71185505",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":59143762,
        "Question_title":"Is it supported to create an integrated notebookVM when the workspace is configured to be in a VNET?",
        "Question_body":"<p>Trying to follow doc at <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-enable-virtual-network#use-a-machine-learning-compute\" rel=\"nofollow noreferrer\">secure your experiments<\/a> but after configuring default workspace storage for VNET access,  attempts to create integrated notebook VM fails with what looks like a storage access error.\n\ueb90\nCreate Failed: \nFailed to clone samples. Error details: Microsoft.WindowsAzure.Storage This request is not authorized to perform this operation.<\/p>\n\n<p>thanks,\njim<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1575307043767,
        "Question_score":1,
        "Question_tags":"jupyter-notebook|azure-machine-learning-service|vnet",
        "Question_view_count":36,
        "Owner_creation_time":1575306079827,
        "Owner_last_access_time":1575378386620,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1575307166703,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59143762",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":57436140,
        "Question_title":"How to reuse successfully built docker images in Azure ML?",
        "Question_body":"<p>In our company I use Azure ML and I have the following issue. I specify a <em>conda_requirements.yaml<\/em> file with the PyTorch estimator class, like so (... are placeholders so that I do not have to type everything out):<\/p>\n\n<pre><code>from azureml.train.dnn import PyTorch\nest = PyTorch(source_directory=\u2019.\u2019, script_params=..., compute_target=..., entry_script=..., conda_dependencies_file_path=\u2019conda_requirements.yaml\u2019, environment_variables=..., framework_version=\u20191.1\u2019)\n<\/code><\/pre>\n\n<p>The <em>conda_requirements.yaml<\/em> (shortened version of the pip part) looks like this:<\/p>\n\n<pre><code>dependencies:\n  -  conda=4.5.11\n  -  conda-package-handling=1.3.10\n  -  python=3.6.2\n  -  cython=0.29.10\n  -  scikit-learn==0.21.2\n  -  anaconda::cloudpickle==1.2.1\n  -  anaconda::cffi==1.12.3\n  -  anaconda::mxnet=1.1.0\n  -  anaconda::psutil==5.6.3\n  -  anaconda::pip=19.1.1\n  -  anaconda::six==1.12.0\n  -  anaconda::mkl==2019.4\n  -  conda-forge::openmpi=3.1.2\n  -  conda-forge::pycparser==2.19\n  -  tensorboard==1.13.1\n  -  tensorflow==1.13.1\n  -  pip:\n        - torch==1.1.0\n        - torchvision==0.2.1\n<\/code><\/pre>\n\n<p>This successfully builds on Azure. Now in order to reuse the resulting docker image in that case, I use the <code>custom_docker_image<\/code> parameter to pass to the <\/p>\n\n<pre><code>from azureml.train.estimator import Estimator\nest = Estimator(source_directory=\u2019.\u2019, script_params=..., compute_target=..., entry_script=..., custom_docker_image=\u2019&lt;container registry name&gt;.azurecr.io\/azureml\/azureml_c3a4f...\u2019, environment_variables=...)\n<\/code><\/pre>\n\n<p>But now Azure somehow seems to rebuild the image again and when I run the experiment it cannot install torch. So it seems to only install the conda dependencies and not the pip dependencies, but actually I do not want Azure to rebuild the image. Can I solve this somehow?<\/p>\n\n<p>I attempted to somehow build a docker image from my Docker file and then add to the registry. I can do az login and according to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/container-registry\/container-registry-authentication\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/container-registry\/container-registry-authentication<\/a> I then should also be able to do an acr login and push. This does not work. \nEven using the credentials from<\/p>\n\n<pre><code>az acr credential show \u2013name &lt;container registry name&gt;\n<\/code><\/pre>\n\n<p>and then doing a <\/p>\n\n<pre><code>docker login &lt;container registry name&gt;.azurecr.io \u2013u &lt;username from credentials above&gt; -p &lt;password from credentials above&gt;\n<\/code><\/pre>\n\n<p>does not work.\nThe error message is <em>authentication required<\/em> even though I used <\/p>\n\n<pre><code>az login\n<\/code><\/pre>\n\n<p>successfully. Would also be happy if someone could explain that to me in addition to how to reuse docker images when using Azure ML.\nThank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1565379277197,
        "Question_score":1,
        "Question_tags":"azure|docker|pip|conda|azure-machine-learning-service",
        "Question_view_count":604,
        "Owner_creation_time":1524670343370,
        "Owner_last_access_time":1663769326433,
        "Owner_location":"Z\u00fcrich, Schweiz",
        "Owner_reputation":460,
        "Owner_up_votes":668,
        "Owner_down_votes":0,
        "Owner_views":52,
        "Question_last_edit_time":null,
        "Answer_body":"<p>AzureML should actually cache your docker image once it was created. The service will hash the base docker info and the contents of the conda.yaml file and will use that as the hash key -- unless you change any of that information, the docker should come from the ACR. <\/p>\n\n<p>As for the custom docker usage, did you set the parameter <code>user_managed=True<\/code>? Otherwise, AzureML will consider your docker to be a base image on top of which it will create the conda environment per your yaml file.<br>\nThere is an example of how to use a custom docker image in this notebook:\n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4170a394edd36413edebdbab347afb0d833c94ee\/how-to-use-azureml\/training-with-deep-learning\/how-to-use-estimator\/how-to-use-estimator.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4170a394edd36413edebdbab347afb0d833c94ee\/how-to-use-azureml\/training-with-deep-learning\/how-to-use-estimator\/how-to-use-estimator.ipynb<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1565501907287,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57436140",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":66838949,
        "Question_title":"onnx model in ubuntu 18.04 not running",
        "Question_body":"<p>I am trying to implement custom vision solution using C#, Azure custom vision, and ONNX Model. My API code is running perfect on windows OS, but when I am trying to run same code on Ubuntu 18.04, getting below error.<\/p>\n<p>I have download trained ONNX model from Azure ml.<\/p>\n<pre><code>Unhandled exception. System.Reflection.TargetInvocationException: Exception has been thrown by the target of an invocation.\n ---&gt; System.Reflection.TargetInvocationException: Exception has been thrown by the target of an invocation.\n ---&gt; System.DllNotFoundException: Unable to load shared library 'api-ms-win-core-com-l1-1-0.dll' or one of its dependencies. In order to help diagnose loading problems, consider setting the LD_DEBUG environment variable: libapi-ms-win-core-com-l1-1-0.dll: cannot open shared object file: No such file or directory\n   at WinRT.Platform.CoIncrementMTAUsage(IntPtr* cookie)\n   at WinRT.WinrtModule..ctor()\n   --- End of inner exception stack trace ---\n   at System.RuntimeTypeHandle.CreateInstance(RuntimeType type, Boolean publicOnly, Boolean wrapExceptions, Boolean&amp; canBeCached, RuntimeMethodHandleInternal&amp; ctor, Boolean&amp; hasNoDefaultCtor)\n   at System.RuntimeType.CreateInstanceDefaultCtorSlow(Boolean publicOnly, Boolean wrapExceptions, Boolean fillCache)\n   at System.RuntimeType.CreateInstanceDefaultCtor(Boolean publicOnly, Boolean skipCheckThis, Boolean fillCache, Boolean wrapExceptions)\n   at System.Activator.CreateInstance[T]()\n   at System.LazyHelper.CreateViaDefaultConstructor[T]()\n   at System.Lazy`1.CreateViaDefaultConstructor()\n   at System.Lazy`1.ViaConstructor()\n   at System.Lazy`1.ExecutionAndPublication(LazyHelper executionAndPublication, Boolean useDefaultConstructor)\n   at System.Lazy`1.CreateValue()\n   at System.Lazy`1.get_Value()\n   at WinRT.WinrtModule.get_Instance()\n   at WinRT.WinrtModule.GetActivationFactory(String runtimeClassId)\n   at WinRT.BaseActivationFactory..ctor(String typeNamespace, String typeFullName)\n   at Windows.Storage.StorageFile._IStorageFileStatics..ctor()\n   --- End of inner exception stack trace ---\n   at System.RuntimeTypeHandle.CreateInstance(RuntimeType type, Boolean publicOnly, Boolean wrapExceptions, Boolean&amp; canBeCached, RuntimeMethodHandleInternal&amp; ctor, Boolean&amp; hasNoDefaultCtor)\n   at System.RuntimeType.CreateInstanceDefaultCtorSlow(Boolean publicOnly, Boolean wrapExceptions, Boolean fillCache)\n   at System.RuntimeType.CreateInstanceDefaultCtor(Boolean publicOnly, Boolean skipCheckThis, Boolean fillCache, Boolean wrapExceptions)\n   at System.Activator.CreateInstance[T]()\n   at WinRT.WeakLazy`1.get_Value()\n   at Windows.Storage.StorageFile._IStorageFileStatics.get_Instance()\n   at Windows.Storage.StorageFile.GetFileFromPathAsync(String path)\n   at CustomVisionAPI.Controllers.HomeController.Run() in \/home\/aaa\/bbb\/CutomVision\/WebApplication1\/Controllers\/HomeController.cs:line 44\n   at System.Threading.Tasks.Task.&lt;&gt;c.&lt;ThrowAsync&gt;b__139_1(Object state)\n   at System.Threading.QueueUserWorkItemCallback.&lt;&gt;c.&lt;.cctor&gt;b__6_0(QueueUserWorkItemCallback quwi)\n   at System.Threading.ExecutionContext.RunForThreadPoolUnsafe[TState](ExecutionContext executionContext, Action`1 callback, TState&amp; state)\n   at System.Threading.QueueUserWorkItemCallback.Execute()\n   at System.Threading.ThreadPoolWorkQueue.Dispatch()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616913128993,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service|onnx|microsoft-custom-vision|onnxruntime",
        "Question_view_count":76,
        "Owner_creation_time":1614491421440,
        "Owner_last_access_time":1621836613313,
        "Owner_location":"Anand, Gujarat, India",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":1616979529367,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66838949",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":41845186,
        "Question_title":"Azure ML R Model Train & Score Web Service",
        "Question_body":"<p>I created the the Azure Machine Learning sample \"<a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/R-Model-Train-Score-2\" rel=\"nofollow noreferrer\">R Model Train &amp; Score<\/a>\" from the gallery and followed the tutorial.  However, when I setup, deploy the web service ( as [New] Preview) and test, I get the error:<\/p>\n\n<blockquote>\n  <p>Score Model (RPackage) : Given path to R installation not found on machine or R executable not at this location<\/p>\n<\/blockquote>\n\n<p>The classic deployment works fine.  Any ideas on how to get the [New] Preview deployment example to run as a web service?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1485327001460,
        "Question_score":1,
        "Question_tags":"r|azure|azure-machine-learning-studio",
        "Question_view_count":131,
        "Owner_creation_time":1277872389890,
        "Owner_last_access_time":1663977781897,
        "Owner_location":"Phoenix, AZ",
        "Owner_reputation":11844,
        "Owner_up_votes":583,
        "Owner_down_votes":29,
        "Owner_views":656,
        "Question_last_edit_time":1485327448236,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41845186",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":53588040,
        "Question_title":"No module named 'automl' when unpickle auto-trained model",
        "Question_body":"<p>I'm trying to reproduce 2 tutorials below using my own dataset instead of MNIST dataset.\n<a href=\"https:\/\/docs.microsoft.com\/ja-jp\/azure\/machine-learning\/service\/tutorial-auto-train-models\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/ja-jp\/azure\/machine-learning\/service\/tutorial-auto-train-models<\/a>\n<a href=\"https:\/\/docs.microsoft.com\/ja-jp\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/ja-jp\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml<\/a><\/p>\n\n<p>About\n'\/notebooks\/tutorials\/03.auto-train-models.ipynb'\nthere's no problem. I've got 'model.pkl'.<\/p>\n\n<p>However, \n'\/notebooks\/tutorials\/02.deploy-models.ipynb'\nhas an error below in 'Predict test data' cell.\nI guess it's a matter of 'pickle' and 'import'.<\/p>\n\n<p>Tell me solutions, please.<\/p>\n\n<pre><code>ModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-6-11cf888b622f&gt; in &lt;module&gt;\n      2 from sklearn.externals import joblib\n      3 \n----&gt; 4 clf = joblib.load('.\/model.pkl')\n      5 # clf = joblib.load('.\/sklearn_mnist_model.pkl')\n      6 y_hat = clf.predict(X_test)\n\n~\/anaconda3_501\/lib\/python3.6\/site-packages\/sklearn\/externals\/joblib\/numpy_pickle.py in load(filename, mmap_mode)\n    576                     return load_compatibility(fobj)\n    577 \n--&gt; 578                 obj = _unpickle(fobj, filename, mmap_mode)\n    579 \n    580     return obj\n\n~\/anaconda3_501\/lib\/python3.6\/site-packages\/sklearn\/externals\/joblib\/numpy_pickle.py in _unpickle(fobj, filename, mmap_mode)\n    506     obj = None\n    507     try:\n--&gt; 508         obj = unpickler.load()\n    509         if unpickler.compat_mode:\n    510             warnings.warn(\"The file '%s' has been generated with a \"\n\n~\/anaconda3_501\/lib\/python3.6\/pickle.py in load(self)\n   1048                     raise EOFError\n   1049                 assert isinstance(key, bytes_types)\n-&gt; 1050                 dispatch[key[0]](self)\n   1051         except _Stop as stopinst:\n   1052             return stopinst.value\n\n~\/anaconda3_501\/lib\/python3.6\/pickle.py in load_global(self)\n   1336         module = self.readline()[:-1].decode(\"utf-8\")\n   1337         name = self.readline()[:-1].decode(\"utf-8\")\n-&gt; 1338         klass = self.find_class(module, name)\n   1339         self.append(klass)\n   1340     dispatch[GLOBAL[0]] = load_global\n\n~\/anaconda3_501\/lib\/python3.6\/pickle.py in find_class(self, module, name)\n   1386             elif module in _compat_pickle.IMPORT_MAPPING:\n   1387                 module = _compat_pickle.IMPORT_MAPPING[module]\n-&gt; 1388         __import__(module, level=0)\n   1389         if self.proto &gt;= 4:\n   1390             return _getattribute(sys.modules[module], name)[0]\n\nModuleNotFoundError: No module named 'automl'\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1543815453630,
        "Question_score":5,
        "Question_tags":"python|pickle|azure-machine-learning-studio|automl",
        "Question_view_count":4788,
        "Owner_creation_time":1543814686770,
        "Owner_last_access_time":1544423620710,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":null,
        "Answer_body":"<p>you have to include azureml-train-automl package. and you have to do this:<\/p>\n\n<p>import azureml.train.automl<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1543816325649,
        "Answer_score":5.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53588040",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":32854507,
        "Question_title":"TfidfVectorizer and sublinear_tf scaling for feature extraction in Azure ML",
        "Question_body":"<p>I am working on a ML document classification problem. Does anyone know how to n-gram Tfidf feature extraction and sublinear_tf scaling in Azure ML.<\/p>\n\n<p>In the past I did this inSci-Kit learn using the TfidfVectorizer (see example below) but the problem is that in AzureML I cannot explicitly define my own methods or classes using a python module and would rather not upload zipped code. <\/p>\n\n<p>I am a python person but am open to using R if there is an equivalent. There is  an R sample in the marketplace but it is dependent on unigrams.<\/p>\n\n<pre><code>TfidfVectorizer(max_df=.67,min_df=.015,lowercase=False ,sublinear_tf=True,norm='l2',tokenizer=AbstractTokenizer())\n<\/code><\/pre>\n\n<p>Best,\n-Ari<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1443564159823,
        "Question_score":1,
        "Question_tags":"python|r|azure|azure-machine-learning-studio",
        "Question_view_count":1487,
        "Owner_creation_time":1374162232293,
        "Owner_last_access_time":1641974793103,
        "Owner_location":null,
        "Owner_reputation":523,
        "Owner_up_votes":23,
        "Owner_down_votes":0,
        "Owner_views":48,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/32854507",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":63771896,
        "Question_title":"Can SAC be used instead PPO in Cartpole example?",
        "Question_body":"<p>I'm studying AzureML RL with example codes.<\/p>\n<p>I could run cartpole example (cartpole_ci.ipynb) which trains\nthe PPO model on compute instance.<\/p>\n<p>I tried SAC instead of PPO by changing training_algorithm = &quot;PPO&quot; to training_algorithm = &quot;SAC&quot;\nbut it failed with the message below.<\/p>\n<blockquote>\n<p>ray.rllib.utils.error.UnsupportedSpaceException: Action space Discrete(2) is not supported for SAC.<\/p>\n<\/blockquote>\n<p>Has someone tried SAC algorithm on AzureML RL and did it work?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1599456813160,
        "Question_score":2,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":494,
        "Owner_creation_time":1599456092003,
        "Owner_last_access_time":1663744052310,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63771896",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":57910952,
        "Question_title":"Distributed Training with Tensorflow in AMLS",
        "Question_body":"<p>using a TensorFlow estimator in Azure ML Service with the following config. <\/p>\n\n<pre><code>from azureml.core.runconfig import TensorflowConfiguration\ndistributed_training = TensorflowConfiguration()\ndistributed_training.worker_count = 3\nest = TensorFlow(source_directory=script_folder,\n             script_params=script_params,\n             compute_target=compute_target,\n             node_count=4,\n             distributed_training=distributed_training,\n             use_gpu=True,\n             entry_script=train_script)\nrun = exp.submit(est)\n<\/code><\/pre>\n\n<p>It seems like in the run with this configuration, individual workers come up with their own instances of trained models and try to register the model multiple times. Is Distributed Training something I need to deal with in the Tensorflow training script?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1568305461657,
        "Question_score":1,
        "Question_tags":"distributed-computing|tensorflow-estimator|azure-machine-learning-service",
        "Question_view_count":63,
        "Owner_creation_time":1568232368920,
        "Owner_last_access_time":1603476601123,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57910952",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":50334563,
        "Question_title":"Deployment of an Azure ML Experiment as a Web Service through Azure Machine Learning Studio",
        "Question_body":"<p>I used Machine learning tutorial: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/create-experiment\" rel=\"nofollow noreferrer\">Create your first data science experiment in Azure Machine Learning Studio<\/a> to create an <code>Experiment<\/code> and then converted it to a <code>predictive experiment<\/code>. Now I'm trying to deploy it as a Web Service by following this article that was referenced in the above article: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/publish-a-machine-learning-web-service#deploy-it-as-a-web-service\" rel=\"nofollow noreferrer\">Deploy it as a web service<\/a>. But when I click on <code>Run<\/code> and then on <code>Deploy Web Service<\/code>, I don't see the <code>Price Plan<\/code> dropdown and <code>Plan Name<\/code> input box etc as mentioned in the section <code>Machine Learning Web Service portal Deploy Experiment Page<\/code> of the second article above. After I clicked on Deploy Web Service link in ML studio, I got the page shown below.<strong>Question<\/strong>: What I may be doing wrong?<\/p>\n\n<p>Note: You can click on the picture to get a larger view.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/G3TKo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/G3TKo.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1526313605237,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":330,
        "Owner_creation_time":1330144099340,
        "Owner_last_access_time":1664039192277,
        "Owner_location":null,
        "Owner_reputation":19815,
        "Owner_up_votes":2703,
        "Owner_down_votes":22,
        "Owner_views":2272,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I think it depends on what workspace you're in. If you're in the free one then you get the screen that you already get, but if you create a workspace in the Azure portal and use that one, then you will get a screen like below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/drRpa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/drRpa.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>To create a new workspace, in the Azure Portal, create a new \"Machine Learning Studio Workspace\" and when you go to Azure ML Studio select the new workspace from the top right.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1526322971967,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50334563",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":63336351,
        "Question_title":"cannot import name 'RollingOriginValidator'",
        "Question_body":"<p>I'm writing a scoring step with Azure Machine Learning pipeline.\nThis is the score code:<\/p>\n<pre><code>import os\nimport pickle\nimport pandas as pd\nfrom azureml.core.model import Model\nimport argparse\nfrom azureml.core.run import Run, _OfflineRun\nfrom azureml.automl.core.shared.rolling_origin_validator import RollingOriginValidator\n\n# Called when the deployed service starts\ndef init():\n    global model\n    \n    # Get the path where the deployed model can be found. \n    run = Run.get_context()\n    model_path = Model.get_model_path('best_model_data')\n    print(model_path)\n    with open(model_path,&quot;rb&quot;) as f:\n       model = pickle.load(f)\n       print(&quot;loaded&quot;)\n# Handle requests to the service\ndef run(data):\n    try:\n        # Pick out the text property of the JSON request.\n        # This expects a request in the form of {&quot;text&quot;: &quot;some text to score for sentiment&quot;}\n        prediction = predict(data)\n        #Return prediction        \n        return prediction\n    except Exception as e:\n        error = str(e)\n        return error\n\n# Predict sentiment using the model\ndef predict(data, include_neutral=True):\n    # Tokenize text\n    test_data_features=data.drop('ProposalId', 1).drop('CombinedTactics',1)\n    test_data_combos=data['CombinedTactics']\n    print(&quot;data&quot;)\n    # Predict\n    score = model.predict_proba(test_data_features)\n    print(&quot;predicted&quot;)\n    df=pd.DataFrame({'score':score[:, 1],'CombinedTactics':test_data_combos})\n    return df\n<\/code><\/pre>\n<p>This is the pipeline step definition:<\/p>\n<pre><code>parallel_run_config = ParallelRunConfig(\n    environment=myenv,\n    entry_script=&quot;use_model.py&quot;,\n    source_directory=&quot;.\/&quot;,\n    output_action=&quot;append_row&quot;,\n    mini_batch_size=&quot;20&quot;,\n    error_threshold=1,\n    compute_target=compute_target,\n    process_count_per_node=2,\n    node_count=2\n)\nparallel_step_name = &quot;batchscoring-&quot; + datetime.now().strftime(&quot;%Y%m%d%H%M&quot;)\nbatch_score_step = ParallelRunStep(\n    name=parallel_step_name,\n    inputs=[test_data.as_named_input(&quot;test_data&quot;)],\n    output=output_dir,\n    parallel_run_config=parallel_run_config,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>However, I met below error:<\/p>\n<p>File &quot;\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/ucmopp-ws\/azureml\/70828787-7515-4db4-b448-a5a4b6c0c8ff\/mounts\/workspaceblobstore\/azureml\/70828787-7515-4db4-b448-a5a4b6c0c8ff\/driver\/azureml_user\/parallel_run\/score_module.py&quot;, line 139, in call_init\nself.init()\nFile &quot;\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/ucmopp-ws\/azureml\/70828787-7515-4db4-b448-a5a4b6c0c8ff\/mounts\/workspaceblobstore\/azureml\/70828787-7515-4db4-b448-a5a4b6c0c8ff\/use_model.py&quot;, line 17, in init\nmodel = pickle.load(f)\nFile &quot;\/azureml-envs\/azureml_7e62c7905267a978aa40f8554487e9b9\/lib\/python3.6\/site-packages\/azureml\/automl\/runtime\/featurization\/<strong>init<\/strong>.py&quot;, line 8, in \nfrom .data_transformer import DataTransformer, TransformerAndMapper\nFile &quot;\/azureml-envs\/azureml_7e62c7905267a978aa40f8554487e9b9\/lib\/python3.6\/site-packages\/azureml\/automl\/runtime\/featurization\/data_transformer.py&quot;, line 54, in \nfrom ..featurizer.transformer import (AutoMLTransformer, CategoricalFeaturizers, DateTimeFeaturesTransformer,\nFile &quot;\/azureml-envs\/azureml_7e62c7905267a978aa40f8554487e9b9\/lib\/python3.6\/site-packages\/azureml\/automl\/runtime\/featurizer\/transformer\/<strong>init<\/strong>.py&quot;, line 28, in \nfrom .timeseries import TimeSeriesTransformer, TimeSeriesPipelineType, NumericalizeTransformer, <br \/>\nFile &quot;\/azureml-envs\/azureml_7e62c7905267a978aa40f8554487e9b9\/lib\/python3.6\/site-packages\/azureml\/automl\/runtime\/featurizer\/transformer\/timeseries\/<strong>init<\/strong>.py&quot;, line 65, in \nfrom azureml.automl.core.shared.rolling_origin_validator import RollingOriginValidator\nImportError: cannot import name 'RollingOriginValidator'<\/p>\n<p>Does anyone have any idea about this error?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1597046743060,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":311,
        "Owner_creation_time":1513841518107,
        "Owner_last_access_time":1663924369323,
        "Owner_location":"China",
        "Owner_reputation":71,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63336351",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":40577607,
        "Question_title":"AzureML: How to Keep leading zeros in dataset (.CSV)",
        "Question_body":"<p>My data: <code>0671001795<\/code><\/p>\n\n<p>Dataset in Microsoft AzureML: <a href=\"https:\/\/i.stack.imgur.com\/gfCtu.png\" rel=\"nofollow noreferrer\">https:\/\/i.stack.imgur.com\/gfCtu.png<\/a><\/p>\n\n<p>How to Keep leading zeros? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1479063490300,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":162,
        "Owner_creation_time":1425465175007,
        "Owner_last_access_time":1492875567547,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Keeping leading zeros when the data column is in integer format is not possible. Here's an alternative way of keeping the leading zeros. (The String data type is used instead of int)<\/p>\n\n<ol>\n<li>Add an special character('\/' etc) before the number <\/li>\n<li>Pipe the contentthrough 'Preprocess Text' module.<\/li>\n<li>Make sure only to tick \"Remove Special Characters\" option.<\/li>\n<li>Output would be in a string with your desired format.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/H0IBg.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/H0IBg.png\" alt=\"Remove Special Characters option\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ML1YM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ML1YM.png\" alt=\"Output as a string\"><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1479205502147,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40577607",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":65212177,
        "Question_title":"MS Azure Automated ML - Output JSON being sent as text",
        "Question_body":"<p>I have started using Azure ML Studio and have come across an issue with the Automated ML model. I create an AutoML run and get a decent precision. I deploy the model and get an endpoint using the out-of-the-box deploy button. I use postman to test the endpoint and get a response. But the response is in text format.<\/p>\n<p>What i'm getting:<\/p>\n<pre><code>&quot;{\\&quot;result\\&quot;: [\\&quot;Prediction Label X\\&quot;]}&quot;\nWhat i'm expecting:\n{&quot;result&quot;:[&quot;Prediction Label X&quot;]}\n<\/code><\/pre>\n<p>Postman has Accept and Content-Type both set to application\/json.<\/p>\n<p>Of course i could clean this text response up and parse it as JSON, but i'd rather get it directly from Azure in the correct format.<\/p>\n<p>There doesnt appear to be anywhere in the ML Studio to modify the code or response format and i'm new to the Azure Studio.<\/p>\n<p>Any thoughts?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1607498019207,
        "Question_score":2,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":70,
        "Owner_creation_time":1475293226350,
        "Owner_last_access_time":1638192617070,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1607522785969,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65212177",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":45351020,
        "Question_title":"how to add the column names to the input dataset using R script in Machine Learning model",
        "Question_body":"<p>I am trying to add the column names to the input dataset using below R script.<\/p>\n\n<pre><code>dataset1 &lt;- maml.mapInputPort(1)#class: data.frame\n# Sample operation\ncols &lt;- c(\"age\",\n    \"workclass\",\n    \"fnlwgt\",\n    \"education\",\n    \"education-num\",\n    \"marital-status\",\n    \"occupation\",\n    \"relationship\",\n    \"race\",\n    \"sex\",\n    \"capital-gain\",\n    \"capital-loss\",\n    \"hours-per-week\",\n    \"native-country\",\n    \"income\")\n colnames(data.frame) &lt;- cols\n data.set = dataset1;\n maml.mapOutputPort(\"data.set\");\n<\/code><\/pre>\n\n<p>But I am getting the error like below figure.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/m4vwp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/m4vwp.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Can you please tell me how to add the column names to the input dataset using R script in Machine Learning model?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1501158990353,
        "Question_score":1,
        "Question_tags":"r|azure-machine-learning-studio",
        "Question_view_count":224,
        "Owner_creation_time":1445426012267,
        "Owner_last_access_time":1662528284260,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":4594,
        "Owner_up_votes":240,
        "Owner_down_votes":22,
        "Owner_views":1089,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45351020",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73760407,
        "Question_title":"Unable to connect Azure DevOps and Azure ML",
        "Question_body":"<p>I have created an automated Service Principal from the service requests on Azure Devops with sufficient permissions. Now, when I am trying to create an artifact which is an ML model (registered) it is not auto populating the registered models and resulting in an error.<\/p>\n<p>I am using a free trial Azure account and attempting to implement CI CD for ML. I turned my firewall off and attempted as well but still the issue persists.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/imvGo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/imvGo.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663480318187,
        "Question_score":0,
        "Question_tags":"azure|azure-devops|azure-machine-learning-service",
        "Question_view_count":40,
        "Owner_creation_time":1501747110080,
        "Owner_last_access_time":1664026893237,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":47,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73760407",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64653569,
        "Question_title":"Concurrent AzureML REST requests fail with Too many requests for service (overloaded)",
        "Question_body":"<p>I have deployed my model to a production Azure Kubernetes Service with 6 nodes.<\/p>\n<p>Sequential inference requests get the expected response from score.py.<\/p>\n<p>When I more than one concurrent async inference requests all the requests except for the first return 503 <code>Too many requests for service {my service name} (overloaded)<\/code>.<\/p>\n<p>I built my service and deployed my model based on the example @ <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/production-deploy-to-aks\/production-deploy-to-aks.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/production-deploy-to-aks\/production-deploy-to-aks.ipynb<\/a>.<\/p>\n<p>I am sending requests as large as 4mb.  It seems to work when I send trivially small requests.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1604352348667,
        "Question_score":2,
        "Question_tags":"azure-aks|azure-machine-learning-service",
        "Question_view_count":457,
        "Owner_creation_time":1293587540030,
        "Owner_last_access_time":1663968009273,
        "Owner_location":"Boston, MA",
        "Owner_reputation":722,
        "Owner_up_votes":84,
        "Owner_down_votes":2,
        "Owner_views":41,
        "Question_last_edit_time":1604499310543,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64653569",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":66932214,
        "Question_title":"why can't I see Train Model results in Azure ML Designer?",
        "Question_body":"<p>I created data from 1000 sessions of a board game simulator I ran. I'm trying to figure out what the winning strategies are and tracked several features in the data.<\/p>\n<p>I loaded the result in a Azure Machine Learning diagram and connected the data set to a model that uses linear regression.<\/p>\n<p>I click the &quot;Train Model&quot; and go to &quot;View Output&quot;. After clicking through the ensuing links, I seem to be able to locate 9 files. I don't see anything that looks like, &quot;column 9 is best predictor of column 1&quot; or something like that.<\/p>\n<p>Instead I see an iLearner file with a lot of binary I can't read. I see a schema file. There's also a lot of meta files about what version of conda ran it and data types and stuff.<\/p>\n<p>How do I see which features best indicated the label I indicated?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wO0eW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wO0eW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>EDIT:<\/strong><\/p>\n<p>As suggested, I added score model and evaluate model.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/emFsG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/emFsG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I did see some error metrics in the evaluate results -&gt; visualize.<\/p>\n<p>Train model had a view output and a view log, but no visualize for me. When I went to &quot;view output&quot; there were a lot of files like convert_to_dataset.yaml and boosted_decision_tree_regression.yaml. Also there was a directory there called trained model which had files with names like data_type.json and score.py. It seemed like it was all meta data and nothing like, &quot;Column 1 best predicted X ...&quot;.<\/p>\n<p>I am still not seeing anything that indicates what best predicts the outcome.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/knkUR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/knkUR.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Z8E6y.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Z8E6y.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1617459748413,
        "Question_score":0,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio|machine-learning-model",
        "Question_view_count":354,
        "Owner_creation_time":1304399536083,
        "Owner_last_access_time":1663946253027,
        "Owner_location":"Raleigh, NC",
        "Owner_reputation":2032,
        "Owner_up_votes":996,
        "Owner_down_votes":3,
        "Owner_views":515,
        "Question_last_edit_time":1618582754150,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66932214",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62180798,
        "Question_title":"How to increase number of tested images in MS Azure Custom Vision?",
        "Question_body":"<p>I've created a project in Azure Custom Vision (Object Detection, General Compact, Tier S0). I uploaded about 70 images, 35 images per tag then started training my model.<\/p>\n\n<p>Checked tags in the Iterations screen after training (Quick Training) was done. For my surprise, only 7 images were tested per tag.<\/p>\n\n<p>Tried to run Advanced Training for 1 hour. Nothing has changed. Only 7 images per tag were tested.<\/p>\n\n<p>Am I doing something wrong?<\/p>\n\n<p>Is there a way to use all images for object detection training so it can give me a better accuracy?<\/p>\n\n<p>Thanks,\n+ftex<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591211290130,
        "Question_score":0,
        "Question_tags":"azure|microsoft-custom-vision|azure-machine-learning-service|azure-ai",
        "Question_view_count":159,
        "Owner_creation_time":1588263508697,
        "Owner_last_access_time":1663873679483,
        "Owner_location":null,
        "Owner_reputation":15,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>What you are seeing in the test interface after the training is only a part of the total images because these metrics are calculated using k-fold cross validation.<\/p>\n<p>You are not doing something wrong. It would not be logic to test all the images because it would mean testing with your training images.<\/p>\n<p>To have a better accuracy, there's no magic: add more images, relevant to your use-case<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/custom-vision-service\/getting-started-build-a-classifier#evaluate-the-classifier?WT.mc_id=AI-MVP-5003365\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/custom-vision-service\/getting-started-build-a-classifier#evaluate-the-classifier<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1591541807696,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1600465129972,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62180798",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":46902487,
        "Question_title":"Installation of Azure Machine Learning Workbench fails on Windows 10",
        "Question_body":"<p>I tried to install the azure machine learning workbench from <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/preview\/quickstart-installation\" rel=\"nofollow noreferrer\">here<\/a>. Once I double click on the downloaded MSI file, it shows the first screen about licensing terms. Once I click on Continue, it shows dependencies. When I click Install, it starts installation. It downloads Miniconda with Python 3.5.2. While trying to install asn1crypto 0.23.0, it suddenly stops and displays 'Installation fails'. I tried running the MSI file with log option but no error is reported in the log.<\/p>\n\n<p>Here are my machine details:\nWindows 10\nVersion 1709 (OS Build 17017.1000)<\/p>\n\n<p>How can I troubleshoot this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1508820867387,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-workbench",
        "Question_view_count":497,
        "Owner_creation_time":1313042504323,
        "Owner_last_access_time":1663957327947,
        "Owner_location":null,
        "Owner_reputation":2727,
        "Owner_up_votes":45,
        "Owner_down_votes":1,
        "Owner_views":227,
        "Question_last_edit_time":1511310720169,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46902487",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":40253448,
        "Question_title":"How Can I use gensim package in Azure ML?",
        "Question_body":"<p>I am using text analysis with Azure ML. So in my python script I want to create a bag of word model and then calculate TFIDF of each words. For that I am using gensim model, It's not working on Azure ML. So is there any options for me? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1477453917853,
        "Question_score":1,
        "Question_tags":"machine-learning|text-analysis|azure-machine-learning-studio",
        "Question_view_count":1024,
        "Owner_creation_time":1455217250737,
        "Owner_last_access_time":1663917835703,
        "Owner_location":"Auckland, New Zealand",
        "Owner_reputation":3811,
        "Owner_up_votes":118,
        "Owner_down_votes":13,
        "Owner_views":703,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40253448",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":32612311,
        "Question_title":"Publish azure machine learning service with feature hashing",
        "Question_body":"<p>I have created an experiment in azure machine learning studio, this experiment is multi-class classification problem using multi-class neural network algorithm, I have also add 'feature hashing' module to transform a stream of English text into a set of features represented as integers. I have successfully run the experiment but when i publish it as web service endpoint i got message \"Reduce the total number of input and output columns to less than 1000 and try publishing again.\"\nI understood after some research that feature hashing convert text into thousands of feature but the problem is how i publish it as web service? and i don't want to remove 'feature hashing' module.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1442415823127,
        "Question_score":0,
        "Question_tags":"machine-learning|azure-machine-learning-studio",
        "Question_view_count":405,
        "Owner_creation_time":1422680560157,
        "Owner_last_access_time":1544934963473,
        "Owner_location":"Ahmedabad, Gujarat, India",
        "Owner_reputation":410,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It sounds like you are trying to output all those thousands of columns as an output. What you really only need is the scored probability or the scored label. To solve this, just drop all the feature hashed columns from the score model module. To do this add in a project columns module, and tell it to start with \"no columns\" then \"include\" by \"column names\", and just add predicted column (scored probability\/scored label). <\/p>\n\n<p>Then hook up the output of that project columns module to your web service output module. Your web service should now be returning only 1-3 columns rather than thousands.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1442447100743,
        "Answer_score":2.0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/32612311",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":65362041,
        "Question_title":"Pick up Results From ML Studio Pipeline in Data Factory Pipeline",
        "Question_body":"<p>We currently have a Data Factory pipeline that is able to call one of our ML Studio Pipelines successfully.  After the ML Studio Pipeline completed, we wanted Azure Data Factory to pick up the results of the ML Studio Pipeline and store the results in SQL Server.<\/p>\n<p>We found the PipelineData class stores the results in a folder in blob based on the child run id, which makes it hard for Data factory to pick up the results.  We then discovered OutputFileDatasetConfig which allows ML Studio to save the results to a static location for Data Factory.  This worked great for Data Factory except OutputFileDatasetConfig doesn't always work :( since it's experimental class.  It took us a while to figure this out and we even created a stackoverflow question for this, which we resolved, and can be found here:  <a href=\"https:\/\/stackoverflow.com\/questions\/65240603\/azure-ml-studio-ml-pipeline-exception-no-temp-file-found\/65350106#65350106\">Azure ML Studio ML Pipeline - Exception: No temp file found<\/a><\/p>\n<p>We returned to using PipelineData class which stores the results in a folder in blob based on the child run id, but we can't figure out how to get Data factory to find the blob based on the child run id of the ML Studio Pipeline it just ran.<\/p>\n<p><strong>So my question is, how do you get Data Factory to pick up the results of a ML Studio Pipeline which was triggered from a Data Factory Pipeline???<\/strong><\/p>\n<p>Here is a simple visual of the Data Factory pipeline we're trying to build.<\/p>\n<pre><code>Step 1: Store Data in azure file store --&gt;\nStep 2: Run ML Studio scoring Pipeline --&gt;\nStep 3: Copy Results to SQL Server\n<\/code><\/pre>\n<p>Step 3 is the step we can't figure out.  Any help would be greatly appreciated.  Thanks and happy coding!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1608315418553,
        "Question_score":1,
        "Question_tags":"azure-data-factory|azure-data-factory-2|azure-machine-learning-service|ml-studio|azureml-python-sdk",
        "Question_view_count":281,
        "Owner_creation_time":1528603052363,
        "Owner_last_access_time":1663971435213,
        "Owner_location":null,
        "Owner_reputation":247,
        "Owner_up_votes":637,
        "Owner_down_votes":0,
        "Owner_views":53,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65362041",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70923438,
        "Question_title":"Azure ML Studio: Create DATASET via REST API",
        "Question_body":"<p>Please tell me how to create a Dataset via REST API.<\/p>\n<p>There is a way to <a href=\"https:\/\/docs.microsoft.com\/en-us\/rest\/api\/azureml\/datastores\" rel=\"nofollow noreferrer\">create a Datastore<\/a>, but I can't find a Dataset.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643619959523,
        "Question_score":1,
        "Question_tags":"azure|rest|dataset|azure-machine-learning-service",
        "Question_view_count":126,
        "Owner_creation_time":1365773542310,
        "Owner_last_access_time":1663778656053,
        "Owner_location":null,
        "Owner_reputation":131,
        "Owner_up_votes":123,
        "Owner_down_votes":1,
        "Owner_views":32,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70923438",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":48879595,
        "Question_title":"ImportError: No module named cassandra in Azure Machine Learning Studio",
        "Question_body":"<p>I am trying to install python package cassandra driver in Azure Machine Learning studio. I am following this answer from <a href=\"https:\/\/stackoverflow.com\/questions\/44371692\/install-python-packages-in-azure-ml\">here<\/a>. Unfortunately i don't see any wheel file for cassandra-driver <a href=\"https:\/\/pypi.python.org\/pypi\/cassandra-driver\/\" rel=\"nofollow noreferrer\">https:\/\/pypi.python.org\/pypi\/cassandra-driver\/<\/a> so i downloaded the .tar file and converted to zip.<\/p>\n<p>I included this .zip file as dataset and connected to python script<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/omsO9.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/omsO9.jpg\" alt=\"jpg1\" \/><\/a><\/p>\n<p>But when i run it, it says No module named cassandra\n<a href=\"https:\/\/i.stack.imgur.com\/4DKTB.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4DKTB.jpg\" alt=\"jpg2\" \/><\/a><\/p>\n<p>Does this work only with wheel file? Any solution is much appreciated.<\/p>\n<p>I am using Python Version :  Anoconda 4.0\/Python 3.5<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1519110374087,
        "Question_score":1,
        "Question_tags":"python|python-3.x|azure|cassandra|azure-machine-learning-studio",
        "Question_view_count":500,
        "Owner_creation_time":1489644560420,
        "Owner_last_access_time":1646025882010,
        "Owner_location":"Planet Earth",
        "Owner_reputation":791,
        "Owner_up_votes":55,
        "Owner_down_votes":4,
        "Owner_views":253,
        "Question_last_edit_time":1592644375060,
        "Answer_body":"<p>I got it working. Changed the folder inside .zip file to <code>\"cassandra\"<\/code> (just like cassandra package). <\/p>\n\n<p>And in the Python script, i added <\/p>\n\n<pre><code>from cassandra import *\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1519124227916,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48879595",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67258465,
        "Question_title":"AzureML ParallelRunStep runs only on one node",
        "Question_body":"<p>I have an inference pipeline with some PythonScriptStep with a ParallelRunStep in the middle. Everything works fine except for the fact that all mini batches are run on one node during the ParallelRunStep, no matter how many nodes I put in the <code>node_count<\/code> config argument.<\/p>\n<p>All the nodes seem to be up and running in the cluster, and according to the logs the <code>init()<\/code> function has been run on them multiple times. Diving into the logs I can see in <strong>sys\/error\/10.0.0.*<\/strong> that all the workers except the one that is working are saying:<\/p>\n<p><code>FileNotFoundError: [Errno 2] No such file or directory: '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/virtualstage\/azureml\/c36eb050-adc9-4c34-8a33-5f6d42dcb19c\/wd\/tmp8_txakpm\/bg.png'<\/code><\/p>\n<p><strong>bg.png<\/strong> happens to be a side argument created in a previous PythonScriptStep that I'm passing to the ParallelRunStep:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>bg_file = PipelineData('bg',  datastore=data_store)\nbg_file_ds = bg_file.as_dataset()\nbg_file_named = bg_file_ds.as_named_input(&quot;bg&quot;)\nbg_file_dw = bg_file_named.as_download()\n\n...\n\nparallelrun_step = ParallelRunStep(\n    name='batch-inference',\n    parallel_run_config=parallel_run_config,\n    inputs=[frames_data_named.as_download()],\n    arguments=[&quot;--bg_folder&quot;, bg_file_dw],\n    side_inputs=[bg_file_dw],\n    output=inference_frames_ds,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>What's happening here? Why the side argument seems to be available only in one worker while it fails in the others?<\/p>\n<p>BTW I found <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/957\" rel=\"nofollow noreferrer\">this<\/a> similar but unresolved question.<\/p>\n<p>Any help is much appreciated, thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619386109403,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":244,
        "Owner_creation_time":1343828614450,
        "Owner_last_access_time":1664042984127,
        "Owner_location":"Seville, Spain",
        "Owner_reputation":359,
        "Owner_up_votes":265,
        "Owner_down_votes":1,
        "Owner_views":55,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Apparently you need to specify a local mount path to use side_inputs in more than one node:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>bg_file_named = bg_file_ds.as_named_input(f&quot;bg&quot;)\nbg_file_mnt = bg_file_named.as_mount(f&quot;\/tmp\/{str(uuid.uuid4())}&quot;)\n\n...\n\nparallelrun_step = ParallelRunStep(\n    name='batch-inference',\n    parallel_run_config=parallel_run_config,\n    inputs=[frames_data_named.as_download()],\n    arguments=[&quot;--bg_folder&quot;, bg_file_mnt],\n    side_inputs=[bg_file_mnt],\n    output=inference_frames_ds,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>Sources:<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-debug-parallel-run-step\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-debug-parallel-run-step<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/18355\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/18355<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1619694485790,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67258465",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":57397150,
        "Question_title":"Deploy Notebook VM via ARM Template?",
        "Question_body":"<p>Is it possible to deploy an AML Notebook VM via an ARM template? If so, is there an example or documentation somewhere?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1565189508713,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-service",
        "Question_view_count":71,
        "Owner_creation_time":1529439461717,
        "Owner_last_access_time":1663763488057,
        "Owner_location":null,
        "Owner_reputation":392,
        "Owner_up_votes":8,
        "Owner_down_votes":4,
        "Owner_views":39,
        "Question_last_edit_time":1565217649487,
        "Answer_body":"<p>Unfortunately this is not supported today, but ARM support is in our roadmap<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1565216213847,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57397150",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":65997961,
        "Question_title":"How to trigger an AzureML Pipeline from Azure DevOps?",
        "Question_body":"<p>If we have an AzureML Pipeline published, how can we trigger it from Azure DevOps <strong>without using Python Script Step or Azure CLI Step<\/strong>?<\/p>\n<p>The AzureML Steps supported natively in Azure DevOps include Model_Deployment and Model_Profiling.<\/p>\n<p>Is there any step in Azure DevOps which can be used to directly trigger a published Azure Machine Learning Pipeline while maintaining capabilities like using Service Connections and passing environmental variables, Gated Release (Deployment)?<\/p>\n<p>Edit:\nThis process can then be used to run as an agentless job.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1612203126923,
        "Question_score":2,
        "Question_tags":"azure|azure-devops|azure-machine-learning-service",
        "Question_view_count":1923,
        "Owner_creation_time":1601729162437,
        "Owner_last_access_time":1663774065773,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":887,
        "Owner_up_votes":187,
        "Owner_down_votes":32,
        "Owner_views":130,
        "Question_last_edit_time":1612249107729,
        "Answer_body":"<p>Assumptions:<\/p>\n<ol>\n<li>An AzureML Pipeline is published and the REST endpoint is ready- To be referred to in this answer as &lt;AML_PIPELINE_REST_URI&gt;. And Published Pipeline ID is also ready- To be referred to in this answer as &lt;AML_PIPELINE_ID&gt;<\/li>\n<li>You have the Azure Machine Learning Extension installed: <a href=\"https:\/\/marketplace.visualstudio.com\/items?itemName=ms-air-aiagility.vss-services-azureml&amp;ssr=false#review-details\" rel=\"nofollow noreferrer\">Azure Machine Learning Extension<\/a><\/li>\n<\/ol>\n<p>To Invoke the Azure Machine Learning Pipeline we use the <code>Invoke ML Pipeline<\/code> step available in Azure DevOps. It is available when running an Agentless Job.<\/p>\n<p>To trigger it the workflow is as follows:<\/p>\n<ol>\n<li>Create a New Pipeline. Using the Classic Editor, delete the default Agent Job 1 stage.\n<a href=\"https:\/\/i.stack.imgur.com\/phzL3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/phzL3.png\" alt=\"enter image description here\" \/><\/a><\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/QkiPY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QkiPY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ol start=\"2\">\n<li><p>Add an agentless job:\n<a href=\"https:\/\/i.stack.imgur.com\/0PXwg.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0PXwg.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Add a task to this Agentless Job:\n<a href=\"https:\/\/i.stack.imgur.com\/trW7j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/trW7j.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Use AzureML Published Pipeline Task:\n<a href=\"https:\/\/i.stack.imgur.com\/3rl4z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3rl4z.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Use the Service Connection Mapped to the AML Workspace. You can find more on this at the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/library\/service-endpoints?view=azure-devops&amp;tabs=yaml\" rel=\"nofollow noreferrer\">official documentation<\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/mnV36.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mnV36.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Choose the Pipeline to trigger using the &lt;AML_PIPELINE_ID&gt;:\n<a href=\"https:\/\/i.stack.imgur.com\/fbpQW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fbpQW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Give The experiment name and Pipeline Parameters if any:\n<a href=\"https:\/\/i.stack.imgur.com\/og1kx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/og1kx.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>That's it, you can Save and Queue:\n<a href=\"https:\/\/i.stack.imgur.com\/iCwdl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/iCwdl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<\/ol>\n<p>Alternatively, you can simply use the following jobs:<\/p>\n<pre><code>- job: Job_2\n  displayName: Agentless job\n  pool: server\n  steps:\n  - task: MLPublishedPipelineRestAPITask@0\n    displayName: Invoke ML pipeline\n    inputs:\n      connectedServiceName: &lt;REDACTED-AML-WS-Level-Service_Connection-ID&gt;\n      PipelineId: &lt;AML_PIPELINE_ID&gt;\n      ExperimentName: experimentname\n      PipelineParameters: ''\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1612256282836,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1612281801243,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65997961",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62935985,
        "Question_title":"Connect to an existing Azure Container Instance ACI from Azure ML",
        "Question_body":"<p>I have an active Azure container Instance which is running, How can I add it to my Workspace using the Azure ML SDK.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1594906481080,
        "Question_score":1,
        "Question_tags":"azure-container-instances|azure-machine-learning-service",
        "Question_view_count":169,
        "Owner_creation_time":1537299924587,
        "Owner_last_access_time":1663176052613,
        "Owner_location":"Laurel, MD, USA",
        "Owner_reputation":359,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":41,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62935985",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71872506,
        "Question_title":"Unable To Run AzureML Experiment with SDK - Failed to Build Wheel for pynacl \/ Exit status:1",
        "Question_body":"<p>I am trying to run a AzureML Experiment using sdk (following a Udemy course). When I try to use the Experiment.submit function the experiment prepares and then fails with the following error messages:<\/p>\n<pre><code>ERROR: Command errored out with exit status 1 \n\nERROR: Failed building wheel for pynacl\nERROR: Could not build wheels for pynacl which use PEP 517 and cannot be installed directly\n<\/code><\/pre>\n<p>The Azure env as created within my anaconda navigator for a short period of time and then gets removed.<\/p>\n<p>Does anyone know how I can get around this? Any help would be really appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649943158360,
        "Question_score":0,
        "Question_tags":"python|azure|anaconda|azure-machine-learning-service|pynacl",
        "Question_view_count":73,
        "Owner_creation_time":1603536549850,
        "Owner_last_access_time":1663761640910,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1650357451940,
        "Answer_body":"<p>To resolve <code>ERROR: Could not build wheels for pynacl which use PEP 517 and cannot be installed directly<\/code> this error, try either of the following ways:<\/p>\n<ol>\n<li><p>Install missing dependencies:<\/p>\n<pre><code>sudo apt install libpython3-dev build-essential\n<\/code><\/pre>\n<\/li>\n<li><p>Upgrade pip:<\/p>\n<pre><code>pip3 install --upgrade pip\n<\/code><\/pre>\n<\/li>\n<li><p>Upgrade pip with setuptools wheel:<\/p>\n<pre><code>pip3 install --upgrade pip setuptools wheel\n<\/code><\/pre>\n<\/li>\n<li><p>Reinstall PEP517:<\/p>\n<pre><code>pip3 install p5py\npip3 install PEP517\n<\/code><\/pre>\n<\/li>\n<\/ol>\n<p>You can refer to  <a href=\"https:\/\/stackoverflow.com\/questions\/61365790\/error-could-not-build-wheels-for-scipy-which-use-pep-517-and-cannot-be-installe\">ERROR: Could not build wheels for scipy which use PEP 517 and cannot be installed directly<\/a>, <a href=\"https:\/\/stackoverflow.com\/questions\/64038673\/could-not-build-wheels-for-which-use-pep-517-and-cannot-be-installed-directly\">Could not build wheels for _ which use PEP 517 and cannot be installed directly - Easy Solution<\/a> and <a href=\"https:\/\/github.com\/martomi\/chiadog\/issues\/44\" rel=\"nofollow noreferrer\">failed building wheel for pynacl<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1650257274672,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71872506",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":32422626,
        "Question_title":"Part of speech tagging and entity recognition - python",
        "Question_body":"<p>I want to perform part of speech tagging and entity recognition in python similar to Maxent_POS_Tag_Annotator and Maxent_Entity_Annotator functions of openNLP in R.  I would prefer a code in python which takes input as textual sentence and gives output as different features- like number of \"CC\", number of \"CD\", number of \"DT\" etc.. CC, CD, DT are POS tags as used in Penn Treebank. So there should be 36 columns\/features for POS tagging corresponding to 36 POS tags as in <a href=\"http:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html\" rel=\"nofollow\">Penn Treebank POS<\/a>. I want to implement this on Azure ML \"Execute Python Script\" module and Azure ML supports python 2.7.7. I heard nltk in python may does the job, but I am a beginner on python. Any help would be appreciated. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1441535794287,
        "Question_score":0,
        "Question_tags":"python|azure|named-entity-recognition|part-of-speech|azure-machine-learning-studio",
        "Question_view_count":1014,
        "Owner_creation_time":1431324152360,
        "Owner_last_access_time":1444093237570,
        "Owner_location":null,
        "Owner_reputation":19,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":1441776651843,
        "Answer_body":"<p>Take a look at <a href=\"http:\/\/www.nltk.org\/book\/ch05.html\" rel=\"nofollow\">NTLK book<\/a>, Categorizing and Tagging Words section.<\/p>\n\n<p>Simple example, it uses the Penn Treebank tagset:<\/p>\n\n<pre><code>from nltk.tag import pos_tag\nfrom nltk.tokenize import word_tokenize\npos_tag(word_tokenize(\"John's big idea isn't all that bad.\")) \n\n[('John', 'NNP'),\n(\"'s\", 'POS'),\n ('big', 'JJ'),\n ('idea', 'NN'),\n ('is', 'VBZ'),\n (\"n't\", 'RB'),\n ('all', 'DT'),\n ('that', 'DT'),\n ('bad', 'JJ'),\n ('.', '.')]\n<\/code><\/pre>\n\n<p>Then you can use<\/p>\n\n<pre><code>from collections import defaultdict\ncounts = defaultdict(int)\nfor (word, tag) in pos_tag(word_tokenize(\"John's big idea isn't all that bad.\")):\n    counts[tag] += 1\n<\/code><\/pre>\n\n<p>to get frequencies:<\/p>\n\n<pre><code>defaultdict(&lt;type 'int'&gt;, {'JJ': 2, 'NN': 1, 'POS': 1, '.': 1, 'RB': 1, 'VBZ': 1, 'DT': 2, 'NNP': 1})\n<\/code><\/pre>",
        "Answer_comment_count":7.0,
        "Answer_creation_time":1441539548049,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/32422626",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72872781,
        "Question_title":"azure ML on AKS memory and CPU parameters",
        "Question_body":"<p>When specifying the memory and core for the AksWebService (deploying Azure ML as a service in AKS), do the values for cpu_cores and memory_gb apply to each replica OR all replicas combined?<\/p>\n<pre><code>AksWebservice.deploy_configuration(cpu_cores=1, memory_gb=1, auth_enabled=True, autoscale_enabled=True, autoscale_min_replicas=4, autoscale_max_replicas=10)\n<\/code><\/pre>\n<p>I am assuming its per replica but just wanted to confirm.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657038675127,
        "Question_score":0,
        "Question_tags":"azure-aks|azure-machine-learning-service",
        "Question_view_count":32,
        "Owner_creation_time":1376577570773,
        "Owner_last_access_time":1663954112673,
        "Owner_location":null,
        "Owner_reputation":301,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":52,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72872781",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":53922402,
        "Question_title":"Customize Python Script On Azure ML",
        "Question_body":"<p>I want to use Fuzzywuzzy logic on python script. I am implement in this way but i didn't get anything. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/sUaD4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sUaD4.png\" alt=\"Azure ML Studio\"><\/a><\/p>\n\n<p>This is my python script code: <\/p>\n\n<pre><code>import pandas as pd\nfrom fuzzywuzzy import process\ndef azureml_main(dataframe1 = None):    \nreturn dataframe1,\n\ndef get_matches(query, choice, limit = 6):\nresult = process.extract(query, choice, limit = limit)\nreturn result,\n\nget_matches(\"admissibility\", dataframe1)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1545740929480,
        "Question_score":1,
        "Question_tags":"azure|fuzzy-search|azure-machine-learning-studio|fuzzywuzzy|ml-studio",
        "Question_view_count":88,
        "Owner_creation_time":1545735161627,
        "Owner_last_access_time":1564382390783,
        "Owner_location":"Ahmedabad, Gujarat, India",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1545901784670,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53922402",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73447386,
        "Question_title":"OpenSSL critical Vulnerability in AzureML Model Deployment to Kubernetes",
        "Question_body":"<p>I have an issue with OpenSSL, I am using the following command to install the latest version of OpenSSL in my Base Docker Image of Azure ML Deployment as the older version has some critical security vulnerability. However, the final image still has the older versions of OPENSSL, it could either be that or AzureML is installing the packages by itself, can anyone tell me how to get past this issue? or delete older versions of OpenSSL?<\/p>\n<pre><code>FROM ubuntu:18.04\n\n# Install dependencies:\nRUN apt-get update  &amp;&amp; apt-get -y install openssl\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jDAXW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jDAXW.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661181225127,
        "Question_score":0,
        "Question_tags":"docker|kubernetes|openssl|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":46,
        "Owner_creation_time":1453798475090,
        "Owner_last_access_time":1663791011297,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Question_last_edit_time":1661326374980,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73447386",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":59510445,
        "Question_title":"Overfitting\/Underfitting Machine Learning Models with Azure Machine Learning vs Python",
        "Question_body":"<p>I'm learning how to perform Machine Learning with Azure ML Studio. At the moment, I've only played around with Machine Learning using Python.<\/p>\n\n<p>I have run identical Machine Learning projects using both Azure ML and Python to see how close the results of each product with the Root Mean Squared Errors (RMSE). So far the RMSE has been widely different for Azure ML and Python.<\/p>\n\n<p>I can't figure out why the RMSE is so far apart. The only reason I can think of is because of the way Python 'fits' the model on the training data. Python uses the following code to fit the training data <\/p>\n\n<pre><code>lr = LinearRegression(labelCol='xxxx')\nlrModel = lr.fit(train_data)\n<\/code><\/pre>\n\n<p>However, I don't know how Azure ML fits the training data.<\/p>\n\n<p>Can someone let me know how Azure ML accomplishes fitting the training data?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1577532437513,
        "Question_score":0,
        "Question_tags":"python-3.x|machine-learning|azure-machine-learning-studio",
        "Question_view_count":138,
        "Owner_creation_time":1466892439907,
        "Owner_last_access_time":1637453769640,
        "Owner_location":null,
        "Owner_reputation":866,
        "Owner_up_votes":49,
        "Owner_down_votes":1,
        "Owner_views":233,
        "Question_last_edit_time":1577532627940,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59510445",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":40632047,
        "Question_title":"Azure ML Studio cannot load a installed package in R",
        "Question_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/wSYTs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wSYTs.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I am trying to install a package in azure ML studio using the command below.<\/p>\n\n<pre><code>install.packages(\"src\/DMwR.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nlibrary(DMwR, lib.loc=\".\", verbose=TRUE)\n<\/code><\/pre>\n\n<p>DMwR.zip was upload as a dataset in azure. The error I get is below.<\/p>\n\n<pre><code>Error 0063: The following error occurred during evaluation of R script:\n---------- Start of error message from R ----------\nzip file 'src\/DMwR.zip' not found\n<\/code><\/pre>\n\n<p>How can I resolve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1479298809787,
        "Question_score":2,
        "Question_tags":"r|azure|azure-machine-learning-studio",
        "Question_view_count":630,
        "Owner_creation_time":1376770973683,
        "Owner_last_access_time":1616761069497,
        "Owner_location":null,
        "Owner_reputation":1646,
        "Owner_up_votes":117,
        "Owner_down_votes":3,
        "Owner_views":448,
        "Question_last_edit_time":1479309068183,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40632047",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58200953,
        "Question_title":"POST request fails with large data send to model deployed on Azure Container",
        "Question_body":"<p><strong>Summary<\/strong><\/p>\n\n<p>I have a PyTorch model deployed on an Azure Container instance via the Azure Machine Learning Service SDK. The model takes (large) images for classification in standard numpy formatting.<\/p>\n\n<p>It seems, I'm hitting a HTTP request size limit on the server side. Requests to the model succeeds with PNG images of a size in the 8-9mb range and fails with images of the 15mb+ size. Specifically, it fails with 413 Request Entity Too Large.<\/p>\n\n<p>I assume, the limit is set in Nginx in the Docker image being build, as part of the deployment process. My question: <em>Given that the issue is due to the HTTP request size limit, is there any way to increase this limit in the azureml API?<\/em><\/p>\n\n<p><strong>Deployment process<\/strong><\/p>\n\n<p>The deployment process succeeds as expected.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace\nfrom azureml.core.model import InferenceConfig, Model\nfrom azureml.core.webservice import AciWebservice, Webservice\nfrom azureml.exceptions import WebserviceException\nfrom pathlib import Path\n\nPATH = Path('\/data\/home\/azureuser\/my_project')\n\nws = Workspace.from_config()\nmodel = ws.models['my_pytorch_model']\n\ninference_config = InferenceConfig(source_directory=PATH\/'src',\n                                   runtime='python',\n                                   entry_script='deployment\/scoring\/scoring.py',\n                                   conda_file='deployment\/environment\/env.yml')\n\ndeployment_config = AciWebservice.deploy_configuration(cpu_cores=2, memory_gb=4)\naci_service_name = 'azure-model'\n\ntry:\n    service = Webservice(ws, name=aci_service_name)\n    if service:\n        service.delete()\nexcept WebserviceException as e:\n    print()\n\nservice = Model.deploy(ws, aci_service_name, [model], inference_config, deployment_config)\n\nservice.wait_for_deployment(True)\nprint(service.state)\n<\/code><\/pre>\n\n<p><strong>Testing via <code>requests<\/code><\/strong><\/p>\n\n<p>A simple test using requests:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nimport numpy as np\nimport requests\nfrom PIL import Image as PilImage\n\ntest_data = np.array(PilImage.open(PATH\/'src\/deployment\/test\/test_image.png')).tolist()\ntest_sample = json.dumps({'raw_data': \n    test_data\n})\ntest_sample_encoded = bytes(test_sample, encoding='utf8')\n\nheaders = {\n    'Content-Type': 'application\/json'\n}\n\nresponse = requests.post(\n    service.scoring_uri,\n    data=test_sample_encoded,\n    headers=headers,\n    verify=True,\n    timeout=10\n)\n<\/code><\/pre>\n\n<p>Produces the following error in <code>requests<\/code> for larger files:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ConnectionError: ('Connection aborted.', BrokenPipeError(32, 'Broken pipe'))\n<\/code><\/pre>\n\n<p>Which I guess is a known error in requests, when a connection is closed from the server before data upload is completed.<\/p>\n\n<p><strong>Testing via <code>pycurl<\/code><\/strong><\/p>\n\n<p>Using the curl wrapper, I get a more interpretable response.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pycurl\nfrom io import BytesIO\n\nc = pycurl.Curl()\nb = BytesIO()\n\nc.setopt(c.URL, service.scoring_uri)\nc.setopt(c.POST, True)\nc.setopt(c.HTTPHEADER,['Content-Type: application\/json'])\nc.setopt(pycurl.WRITEFUNCTION, b.write)\nc.setopt(c.POSTFIELDS, test_sample)\nc.setopt(c.VERBOSE, True)\nc.perform()\n\nout = b.getvalue()\n\nb.close()\nc.close()\n\nprint(out)\n\n<\/code><\/pre>\n\n<p>For large files, this yields the following error:<\/p>\n\n<pre class=\"lang-html prettyprint-override\"><code>&lt;html&gt;\n    &lt;head&gt;\n        &lt;title&gt;\n            413 Request Entity Too Large\n        &lt;\/title&gt;\n    &lt;\/head&gt;\n    &lt;body bgcolor=\"white\"&gt;\n        &lt;center&gt;\n            &lt;h1&gt;\n                413 Request Entity Too Large\n            &lt;\/h1&gt;\n        &lt;\/center&gt;\n        &lt;hr&gt;\n        &lt;center&gt;\n                nginx\/1.10.3 (Ubuntu)\n        &lt;\/center&gt;\n    &lt;\/body&gt;\n&lt;\/html&gt;\n<\/code><\/pre>\n\n<p>Leading me to believe this is an issue in the Nginx configuration. Specifically, I guess that client_max_body_size is set to 10mb.<\/p>\n\n<p><strong>Question summarised<\/strong><\/p>\n\n<p>Given that I am indeed hitting an issue with the Nginx configuration, can I change it somehow? If not using the Azure Machine Learning Service SDK, then maybe by overwriting the <code>\/etc\/nginx\/nginx.conf<\/code> file?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1570017134097,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":574,
        "Owner_creation_time":1377156004257,
        "Owner_last_access_time":1571123387553,
        "Owner_location":null,
        "Owner_reputation":26,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58200953",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":68090119,
        "Question_title":"error creating new kernel in AMLs notebook",
        "Question_body":"<p>i am attempting to create a new kernel in an AMLs notebook.  It turns out that it doesn\u2019t matter which kernel I am trying to create, because it doesn't get that far.  \u2639<\/p>\n<p>i am following the steps here:  <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#add-new-kernels\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#add-new-kernels<\/a><\/p>\n<p>my full terminal session text is below.<\/p>\n<p>the error i get is:<\/p>\n<pre><code>InvalidArchiveError('Error with archive \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu.tar.bz2.  You probably need to delete and re-download or re-create this file.  Message from libarchive was:\\n\\nCould not unlink')\n<\/code><\/pre>\n<p>thanks for any suggestions or pointers.<\/p>\n<pre><code>Welcome to Azure Machine Learning Terminal\n\nType &quot;git clone [url]&quot; to clone a repo                      \nType &quot;git --help&quot; to learn about Git CLI                \nType &quot;az ml --help&quot; to learn about Azure ML CLI           \n\n\nazureuser@qnotebook:\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/qnotebook\/code$ cd ~\/cloudfiles\/code\/Users\/delbertm\/amls-qc \nazureuser@qnotebook:~\/cloudfiles\/code\/Users\/delbertm\/amls-qc$ \nazureuser@qnotebook:~\/cloudfiles\/code\/Users\/delbertm\/amls-qc$ conda create --name qsharp-env\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 4.9.2\n  latest version: 4.10.1\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n\n\n\n## Package Plan ##\n\n  environment location: \/anaconda\/envs\/qsharp-env\n\n\n\nProceed ([y]\/n)? y\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n#     $ conda activate qsharp-env\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\nazureuser@qnotebook:~\/cloudfiles\/code\/Users\/delbertm\/amls-qc$ conda activate qsharp-env\n(qsharp-env) azureuser@qnotebook:~\/cloudfiles\/code\/Users\/delbertm\/amls-qc$ conda install pip\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 4.9.2\n  latest version: 4.10.1\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n\n\n\n## Package Plan ##\n\n  environment location: \/anaconda\/envs\/qsharp-env\n\n  added \/ updated specs:\n    - pip\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    _openmp_mutex-4.5          |            1_gnu          22 KB\n    ca-certificates-2021.5.25  |       h06a4308_1         112 KB\n    certifi-2021.5.30          |   py39h06a4308_0         139 KB\n    ld_impl_linux-64-2.35.1    |       h7274673_9         586 KB\n    libffi-3.3                 |       he6710b0_2          50 KB\n    libgcc-ng-9.3.0            |      h5101ec6_17         4.8 MB\n    libgomp-9.3.0              |      h5101ec6_17         311 KB\n    libstdcxx-ng-9.3.0         |      hd4cf53a_17         3.1 MB\n    ncurses-6.2                |       he6710b0_1         817 KB\n    pip-21.1.2                 |   py39h06a4308_0         1.8 MB\n    python-3.9.5               |       h12debd9_4        22.6 MB\n    readline-8.1               |       h27cfd23_0         362 KB\n    setuptools-52.0.0          |   py39h06a4308_0         724 KB\n    sqlite-3.35.4              |       hdfb4753_0         981 KB\n    tk-8.6.10                  |       hbc83047_0         3.0 MB\n    tzdata-2020f               |       h52ac0ba_0         113 KB\n    wheel-0.36.2               |     pyhd3eb1b0_0          33 KB\n    xz-5.2.5                   |       h7b6447c_0         341 KB\n    zlib-1.2.11                |       h7b6447c_3         103 KB\n    ------------------------------------------------------------\n                                           Total:        39.9 MB\n\nThe following NEW packages will be INSTALLED:\n\n  _libgcc_mutex      pkgs\/main\/linux-64::_libgcc_mutex-0.1-main\n  _openmp_mutex      pkgs\/main\/linux-64::_openmp_mutex-4.5-1_gnu\n  ca-certificates    pkgs\/main\/linux-64::ca-certificates-2021.5.25-h06a4308_1\n  certifi            pkgs\/main\/linux-64::certifi-2021.5.30-py39h06a4308_0\n  ld_impl_linux-64   pkgs\/main\/linux-64::ld_impl_linux-64-2.35.1-h7274673_9\n  libffi             pkgs\/main\/linux-64::libffi-3.3-he6710b0_2\n  libgcc-ng          pkgs\/main\/linux-64::libgcc-ng-9.3.0-h5101ec6_17\n  libgomp            pkgs\/main\/linux-64::libgomp-9.3.0-h5101ec6_17\n  libstdcxx-ng       pkgs\/main\/linux-64::libstdcxx-ng-9.3.0-hd4cf53a_17\n  ncurses            pkgs\/main\/linux-64::ncurses-6.2-he6710b0_1\n  openssl            pkgs\/main\/linux-64::openssl-1.1.1k-h27cfd23_0\n  pip                pkgs\/main\/linux-64::pip-21.1.2-py39h06a4308_0\n  python             pkgs\/main\/linux-64::python-3.9.5-h12debd9_4\n  readline           pkgs\/main\/linux-64::readline-8.1-h27cfd23_0\n  setuptools         pkgs\/main\/linux-64::setuptools-52.0.0-py39h06a4308_0\n  sqlite             pkgs\/main\/linux-64::sqlite-3.35.4-hdfb4753_0\n  tk                 pkgs\/main\/linux-64::tk-8.6.10-hbc83047_0\n  tzdata             pkgs\/main\/noarch::tzdata-2020f-h52ac0ba_0\n  wheel              pkgs\/main\/noarch::wheel-0.36.2-pyhd3eb1b0_0\n  xz                 pkgs\/main\/linux-64::xz-5.2.5-h7b6447c_0\n  zlib               pkgs\/main\/linux-64::zlib-1.2.11-h7b6447c_3\n\n\nProceed ([y]\/n)? y\n\n\nDownloading and Extracting Packages\n_openmp_mutex-4.5    | 22 KB     | ########################################################################################################################################################################################################################################8                                                                                   |  74% WARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/lib\/libgomp.so.1.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/licenses\/LICENSE.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/tests\/fortomp\/CMakeLists.txt.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/tests\/fortomp\/test_fort.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/tests\/aligned_alloc.c.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/tests\/aligned_alloc.cpp.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/yum_requirements.txt.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/config.old.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-libgcc-devel.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/make_tool_links.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/LICENSE.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-gdb.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-libgcc.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-libgcc-no-gomp.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-g++.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/uclibc.config.minimal.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/hello-world.cpp.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-openmp_impl.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-libgfortran.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/build.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-gcc.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/uclibc.config.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-binutils.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/c11threads.c.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/conda_build_config.yaml.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-duma.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-libstdc++.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-libgomp.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-libstdc++-devel.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/meta.yaml.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/write_ctng_config.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-gfortran.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/install-openmp_impl.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/conda_build_config.yaml.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/meta.yaml.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/test\/run_test.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/hash_input.json.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/run_exports.json.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/repodata_record.json.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/about.json.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/index.json.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/paths.json.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/files.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/git.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/lib\/libgomp.so.1.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/licenses\/LICENSE.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/tests\/fortomp\/CMakeLists.txt.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/tests\/fortomp\/test_fort.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/tests\/aligned_alloc.c.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/tests\/aligned_alloc.cpp.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/yum_requirements.txt.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/config.old.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-libgcc-devel.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/make_tool_links.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/LICENSE.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-gdb.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-libgcc.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-libgcc-no-gomp.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-g++.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/uclibc.config.minimal.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/hello-world.cpp.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-openmp_impl.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-libgfortran.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/build.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-gcc.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/uclibc.config.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-binutils.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/c11threads.c.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/conda_build_config.yaml.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-duma.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-libstdc++.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-libgomp.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-libstdc++-devel.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/meta.yaml.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/write_ctng_config.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/parent\/install-gfortran.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/install-openmp_impl.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/conda_build_config.yaml.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/recipe\/meta.yaml.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/test\/run_test.sh.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/hash_input.json.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/run_exports.json.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/repodata_record.json.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/about.json.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/index.json.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/paths.json.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/files.  Please remove this file manually (you may need to reboot to free file handles)\nWARNING conda.gateways.disk.delete:unlink_or_rename_to_trash(139): Could not remove or rename \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu\/info\/git.  Please remove this file manually (you may need to reboot to free file handles)\n_openmp_mutex-4.5    | 22 KB     | ### | 100% \nreadline-8.1         | 362 KB    | ### | 100% \ntzdata-2020f         | 113 KB    | ### | 100% \nncurses-6.2          | 817 KB    | ### | 100% \nxz-5.2.5             | 341 KB    | ### | 100% \npip-21.1.2           | 1.8 MB    | ### | 100% \nzlib-1.2.11          | 103 KB    | ### | 100% \nsetuptools-52.0.0    | 724 KB    | ### | 100% \nca-certificates-2021 | 112 KB    | ### | 100% \nlibgcc-ng-9.3.0      | 4.8 MB    | ### | 100% \nlibgomp-9.3.0        | 311 KB    | ### | 100% \nlibstdcxx-ng-9.3.0   | 3.1 MB    | ### | 100% \nld_impl_linux-64-2.3 | 586 KB    | ### | 100% \npython-3.9.5         | 22.6 MB   | ### | 100% \ncertifi-2021.5.30    | 139 KB    | ### | 100% \nlibffi-3.3           | 50 KB     | ### | 100% \nsqlite-3.35.4        | 981 KB    | ### | 100% \nwheel-0.36.2         | 33 KB     | ### | 100% \ntk-8.6.10            | 3.0 MB    | ### | 100% \n\nInvalidArchiveError('Error with archive \/anaconda\/pkgs\/_openmp_mutex-4.5-1_gnu.tar.bz2.  You probably need to delete and re-download or re-create this file.  Message from libarchive was:\\n\\nCould not unlink')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624392231487,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":363,
        "Owner_creation_time":1565457789270,
        "Owner_last_access_time":1663869520163,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1624460808310,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68090119",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":63204081,
        "Question_title":"PySpark ALSModel load fails in deployment over Azure ML service with error java.util.NoSuchElementException: Param blockSize does not exist",
        "Question_body":"<p>I am trying to deploy an ALS model trained using PySpark on Azure ML service. I am providing a score.py file that loads the trained model using ALSModel.load() function. Following is the code of my score.py file.<\/p>\n<pre><code>import os\nfrom azureml.core.model import Model\nfrom pyspark.ml.recommendation import ALS, ALSModel\nfrom pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import DoubleType, StringType\nfrom pyspark.sql import SQLContext\nfrom pyspark import SparkContext\n\nsc = SparkContext.getOrCreate()\nsqlContext = SQLContext(sc)\nspark = sqlContext.sparkSession\n\ninput_schema = StructType([StructField(&quot;UserId&quot;, StringType())])\nreader = spark.read\nreader.schema(input_schema)\n\n\ndef init():\n    global model\n    # note here &quot;iris.model&quot; is the name of the model registered under the workspace\n    # this call should return the path to the model.pkl file on the local disk.\n    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), &quot;recommendation-model&quot;)\n    # Load the model file back into a LogisticRegression model\n    model = ALSModel.load(model_path)\n    \n\ndef run(data):\n    try:\n        input_df = reader.json(sc.parallelize([data]))\n        input_df = indexer.transform(input_df)\n        \n        res = model.recommendForUserSubset(input_df[['UserId_index']], 10)\n\n        # you can return any datatype as long as it is JSON-serializable\n        return result.collect()[0]['recommendations']\n    except Exception as e:\n        traceback.print_exc()\n        error = str(e)\n        return error\n<\/code><\/pre>\n<p>Following is the error I get when I deploy it as LocalWebService using Model.deploy function in Azure ML service<\/p>\n<pre><code>Generating Docker build context.\nPackage creation Succeeded\nLogging into Docker registry viennaglobal.azurecr.io\nLogging into Docker registry viennaglobal.azurecr.io\nBuilding Docker image from Dockerfile...\nStep 1\/5 : FROM viennaglobal.azurecr.io\/azureml\/azureml_43542b56c5ec3e8d0f68e1556558411f\n ---&gt; 5b3bb174ca5f\nStep 2\/5 : COPY azureml-app \/var\/azureml-app\n ---&gt; 8e540c0746f7\nStep 3\/5 : RUN mkdir -p '\/var\/azureml-app' &amp;&amp; echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjNkN2M1ZjM4LTI1ODEtNGUxNi05NTdhLWEzOTU1OGI1ZjBiMyIsInJlc291cmNlR3JvdXBOYW1lIjoiZGV2LW9tbmljeC10ZnMtYWkiLCJhY2NvdW50TmFtZSI6ImRldi10ZnMtYWktd29ya3NwYWNlIiwid29ya3NwYWNlSWQiOiI1NjkzNGMzNC1iZmYzLTQ3OWUtODRkMy01OGI4YTc3ZTI4ZjEifSwibW9kZWxzIjp7fSwibW9kZWxzSW5mbyI6e319 | base64 --decode &gt; \/var\/azureml-app\/model_config_map.json\n ---&gt; Running in 502ad8edf91e\n ---&gt; a1bc5e0283d0\nStep 4\/5 : RUN mv '\/var\/azureml-app\/tmpvxhomyin.py' \/var\/azureml-app\/main.py\n ---&gt; Running in eb4ec1a0b702\n ---&gt; 6a3296fe6420\nStep 5\/5 : CMD [&quot;runsvdir&quot;,&quot;\/var\/runit&quot;]\n ---&gt; Running in 834fd746afef\n ---&gt; 5b9f8be538c0\nSuccessfully built 5b9f8be538c0\nSuccessfully tagged recommend-service:latest\nContainer (name:musing_borg, id:0f3163692f5119685eee5ed59c8e00aa96cd472f765e7db67653f1a6ce852e83) cannot be killed.\nContainer has been successfully cleaned up.\nImage sha256:0f146f4752878bbbc0e876f4477cc2877ff12a366fca18c986f9a9c2949d028b successfully removed.\nStarting Docker container...\nDocker container running.\nChecking container health...\nERROR - Error: Container has crashed. Did your init method fail?\n\n\nContainer Logs:\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n2020-07-30T11:57:00,312735664+00:00 - rsyslog\/run \n2020-07-30T11:57:00,312768364+00:00 - gunicorn\/run \n2020-07-30T11:57:00,313017966+00:00 - iot-server\/run \nbash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by bash)\n2020-07-30T11:57:00,313969073+00:00 - nginx\/run \n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libcrypto.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libcrypto.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libssl.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libssl.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libssl.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n2020-07-30T11:57:00,597835804+00:00 - iot-server\/finish 1 0\n2020-07-30T11:57:00,598826211+00:00 - Exit code 1 is normal. Not restarting iot-server.\nStarting gunicorn 19.9.0\nListening at: http:\/\/127.0.0.1:31311 (10)\nUsing worker: sync\nworker timeout is set to 300\nBooting worker with pid: 41\nbash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by bash)\nbash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by bash)\nIvy Default Cache set to: \/root\/.ivy2\/cache\nThe jars for the packages stored in: \/root\/.ivy2\/jars\n:: loading settings :: url = jar:file:\/home\/mmlspark\/lib\/spark\/jars\/ivy-2.4.0.jar!\/org\/apache\/ivy\/core\/settings\/ivysettings.xml\ncom.microsoft.ml.spark#mmlspark_2.11 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-e07358bb-d354-4f41-aa4c-f0aa73bb0156;1.0\n    confs: [default]\n    found com.microsoft.ml.spark#mmlspark_2.11;0.15 in spark-list\n    found io.spray#spray-json_2.11;1.3.2 in central\n    found com.microsoft.cntk#cntk;2.4 in central\n    found org.openpnp#opencv;3.2.0-1 in central\n    found com.jcraft#jsch;0.1.54 in central\n    found org.apache.httpcomponents#httpclient;4.5.6 in central\n    found org.apache.httpcomponents#httpcore;4.4.10 in central\n    found commons-logging#commons-logging;1.2 in central\n    found commons-codec#commons-codec;1.10 in central\n    found com.microsoft.ml.lightgbm#lightgbmlib;2.1.250 in central\n:: resolution report :: resolve 318ms :: artifacts dl 11ms\n    :: modules in use:\n    com.jcraft#jsch;0.1.54 from central in [default]\n    com.microsoft.cntk#cntk;2.4 from central in [default]\n    com.microsoft.ml.lightgbm#lightgbmlib;2.1.250 from central in [default]\n    com.microsoft.ml.spark#mmlspark_2.11;0.15 from spark-list in [default]\n    commons-codec#commons-codec;1.10 from central in [default]\n    commons-logging#commons-logging;1.2 from central in [default]\n    io.spray#spray-json_2.11;1.3.2 from central in [default]\n    org.apache.httpcomponents#httpclient;4.5.6 from central in [default]\n    org.apache.httpcomponents#httpcore;4.4.10 from central in [default]\n    org.openpnp#opencv;3.2.0-1 from central in [default]\n    ---------------------------------------------------------------------\n    |                  |            modules            ||   artifacts   |\n    |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n    ---------------------------------------------------------------------\n    |      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n    ---------------------------------------------------------------------\n\n:: problems summary ::\n:::: ERRORS\n    unknown resolver repo-1\n\n    unknown resolver repo-1\n\n\n:: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n:: retrieving :: org.apache.spark#spark-submit-parent-e07358bb-d354-4f41-aa4c-f0aa73bb0156\n    confs: [default]\n    0 artifacts copied, 10 already retrieved (0kB\/7ms)\n2020-07-30 11:57:02 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to &quot;WARN&quot;.\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nInitialized PySpark session.\nInitializing logger\n2020-07-30 11:57:09,464 | root | INFO | Starting up app insights client\nStarting up app insights client\n2020-07-30 11:57:09,464 | root | INFO | Starting up request id generator\nStarting up request id generator\n2020-07-30 11:57:09,464 | root | INFO | Starting up app insight hooks\nStarting up app insight hooks\n2020-07-30 11:57:09,464 | root | INFO | Invoking user's init function\nInvoking user's init function\n2020-07-30 11:57:19,652 | root | ERROR | User's init function failed\nUser's init function failed\n2020-07-30 11:57:19,656 | root | ERROR | Encountered Exception Traceback (most recent call last):\n  File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 163, in register\n    main.init()\n  File &quot;\/var\/azureml-app\/main.py&quot;, line 44, in init\n    model = ALSModel.load(model_path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/ml\/util.py&quot;, line 362, in load\n    return cls.read().load(path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/ml\/util.py&quot;, line 300, in load\n    java_obj = self._jread.load(path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/lib\/py4j-0.10.7-src.zip\/py4j\/java_gateway.py&quot;, line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/sql\/utils.py&quot;, line 63, in deco\n    return f(*a, **kw)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/lib\/py4j-0.10.7-src.zip\/py4j\/protocol.py&quot;, line 328, in get_return_value\n    format(target_id, &quot;.&quot;, name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o64.load.\n: java.util.NoSuchElementException: Param blockSize does not exist.\n    at org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n    at org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n    at scala.Option.getOrElse(Option.scala:121)\n    at org.apache.spark.ml.param.Params$class.getParam(params.scala:728)\n    at org.apache.spark.ml.PipelineStage.getParam(Pipeline.scala:42)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata$$anonfun$setParams$1.apply(ReadWrite.scala:591)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata$$anonfun$setParams$1.apply(ReadWrite.scala:589)\n    at scala.collection.immutable.List.foreach(List.scala:392)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata.setParams(ReadWrite.scala:589)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata.getAndSetParams(ReadWrite.scala:572)\n    at org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:533)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    at py4j.Gateway.invoke(Gateway.java:282)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.GatewayConnection.run(GatewayConnection.java:238)\n    at java.lang.Thread.run(Thread.java:748)\n\n\nEncountered Exception Traceback (most recent call last):\n  File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 163, in register\n    main.init()\n  File &quot;\/var\/azureml-app\/main.py&quot;, line 44, in init\n    model = ALSModel.load(model_path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/ml\/util.py&quot;, line 362, in load\n    return cls.read().load(path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/ml\/util.py&quot;, line 300, in load\n    java_obj = self._jread.load(path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/lib\/py4j-0.10.7-src.zip\/py4j\/java_gateway.py&quot;, line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/sql\/utils.py&quot;, line 63, in deco\n    return f(*a, **kw)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/lib\/py4j-0.10.7-src.zip\/py4j\/protocol.py&quot;, line 328, in get_return_value\n    format(target_id, &quot;.&quot;, name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o64.load.\n: java.util.NoSuchElementException: Param blockSize does not exist.\n    at org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n    at org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n    at scala.Option.getOrElse(Option.scala:121)\n    at org.apache.spark.ml.param.Params$class.getParam(params.scala:728)\n    at org.apache.spark.ml.PipelineStage.getParam(Pipeline.scala:42)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata$$anonfun$setParams$1.apply(ReadWrite.scala:591)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata$$anonfun$setParams$1.apply(ReadWrite.scala:589)\n    at scala.collection.immutable.List.foreach(List.scala:392)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata.setParams(ReadWrite.scala:589)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata.getAndSetParams(ReadWrite.scala:572)\n    at org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:533)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    at py4j.Gateway.invoke(Gateway.java:282)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.GatewayConnection.run(GatewayConnection.java:238)\n    at java.lang.Thread.run(Thread.java:748)\n\n\nWorker exiting (pid: 41)\nShutting down: Master\nReason: Worker failed to boot.\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n2020-07-30T11:57:19,833136837+00:00 - gunicorn\/finish 3 0\n2020-07-30T11:57:19,834216245+00:00 - Exit code 3 is not normal. Killing image.\n\n---------------------------------------------------------------------------\nWebserviceException                       Traceback (most recent call last)\n&lt;ipython-input-43-d0992ae9d1c9&gt; in &lt;module&gt;\n      6 local_service = Model.deploy(workspace, &quot;recommend-service&quot;, [register_model], inference_config, deployment_config)\n      7 \n----&gt; 8 local_service.wait_for_deployment()\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/local.py in decorated(self, *args, **kwargs)\n     69                 raise WebserviceException('Cannot call {}() when service is {}.'.format(func.__name__, self.state),\n     70                                           logger=module_logger)\n---&gt; 71             return func(self, *args, **kwargs)\n     72         return decorated\n     73     return decorator\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/local.py in wait_for_deployment(self, show_output)\n    601                                    self._container,\n    602                                    health_url=self._internal_base_url,\n--&gt; 603                                    cleanup_if_failed=False)\n    604 \n    605             self.state = LocalWebservice.STATE_RUNNING\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_model_management\/_util.py in container_health_check(docker_port, container, health_url, cleanup_if_failed)\n    745             # The container has started and crashed.\n    746             _raise_for_container_failure(container, cleanup_if_failed,\n--&gt; 747                                          'Error: Container has crashed. Did your init method fail?')\n    748 \n    749         # The container hasn't crashed, so try to ping the health endpoint.\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_model_management\/_util.py in _raise_for_container_failure(container, cleanup, message)\n   1258         cleanup_container(container)\n   1259 \n-&gt; 1260     raise WebserviceException(message, logger=module_logger)\n   1261 \n   1262 \n\nWebserviceException: WebserviceException:\n    Message: Error: Container has crashed. Did your init method fail?\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;Error: Container has crashed. Did your init method fail?&quot;\n    }\n}\n<\/code><\/pre>\n<p>However, the ALSModel.load() works fine when executed in a Jupyter notebook.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596276786133,
        "Question_score":1,
        "Question_tags":"python|apache-spark|machine-learning|pyspark|azure-machine-learning-service",
        "Question_view_count":598,
        "Owner_creation_time":1596274712793,
        "Owner_last_access_time":1609241506490,
        "Owner_location":"Chandigarh, India",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":"<p>A couple of things to check:<\/p>\n<ol>\n<li>Is your model registered in the workspace? AZUREML_MODEL_DIR only works for registered models. See <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none--sample-input-dataset-none--sample-output-dataset-none--resource-configuration-none-\" rel=\"nofollow noreferrer\">this link<\/a> for information about registering a model<\/li>\n<li>Are you specifying the same version of pyspark.ml.recommendation in your InferenceConfig as you use locally? This kind of error might be due to a difference in versions<\/li>\n<li>Have you looked at the output of <code>print(service.get_logs())<\/code>? Check out our <a href=\"https:\/\/review.docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment?branch=pr-en-us-124666\" rel=\"nofollow noreferrer\">troubleshoot and debugging documentation here<\/a> for other things you can try<\/li>\n<\/ol>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1596478571823,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63204081",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70179691,
        "Question_title":"Resources for displaying Azure ML Studio application insights on Grafana",
        "Question_body":"<p>I'm looking to display specific metrics from Azure ML Studio application insights on a Grafana Dashboard and haven't found any great documentations so far. Can you please point me to a good resource for this need? Thank you.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1638338044333,
        "Question_score":0,
        "Question_tags":"azure|grafana|azure-machine-learning-service",
        "Question_view_count":44,
        "Owner_creation_time":1638211993203,
        "Owner_last_access_time":1651071910080,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70179691",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72566607,
        "Question_title":"How to trigger a process based on an Azure ML model predictions?",
        "Question_body":"<p>It is possible to trigger, let's say, a process to send emails or SMS based on an Azure ML prediction?<\/p>\n<p>I have a customer segmentation model, and my goal is to contact a customer based on their segment. For example:<\/p>\n<ul>\n<li>All customers group A -&gt; phone call.<\/li>\n<li>All customers group B -&gt; SMS.<\/li>\n<\/ul>\n<p>And so on...<\/p>\n<p>How can I achieve this? How would be the recommended approach?\nI was reading this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-event-grid\" rel=\"nofollow noreferrer\">Microsoft docs<\/a> but these event driven actions are not what I need.<\/p>\n<p>Another thing that I was considering is what about saving the model response on a storage account and create a event action with Logic Apps or Azure Functions when adding data in this storage. This could work? Perhaps, it is important to mention that the model predictions are considered to be done once a month by a scheduled python script.<\/p>\n<p>Can someone give me an advice to what path can I follow?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654808578860,
        "Question_score":0,
        "Question_tags":"azure|azure-functions|azure-logic-apps|azure-machine-learning-studio|azure-eventgrid",
        "Question_view_count":91,
        "Owner_creation_time":1555058508927,
        "Owner_last_access_time":1661373429997,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1654809277600,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72566607",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72516242,
        "Question_title":"Is there any limitations for runs per users in Azure ML experiments?",
        "Question_body":"<p>I and my team members are working on a machine learning project through the <strong>Azure ML<\/strong> portal.\nWe have created a specific <em>experiment<\/em> in our <em>workspace<\/em> in Azure ML and are submitting our Python script <em>runs<\/em> from our local or remote machines in this experiment.<\/p>\n<p>Although I'm collaborating with my colleagues, most of the runs in this specific experiment are submitted by me.<\/p>\n<p>Recently, I have faced a problem with experiment submissions.\nThe problem is that after some number of experiments created by me, I cannot add any other runs to this experiment, but my colleagues can!!!<\/p>\n<p>Unfortunately, the Azure ML portal does not show any clear error message for this problem. It continues submitting the run till a <em>timeout<\/em> exception occurs!<\/p>\n<p>As a temporary solution, I've just changed the name of the experiment and I could conquer this problem.<\/p>\n<p>This solution helped me to submit my run on <strong>Azure ML<\/strong> but it didn\u2019t satisfy me because we want to collect all related runs under a specific experiment. On the other hand creating multiple number of experiments for each run is overwhelming!<\/p>\n<p>What I know is that there are some service limits for the number of runs in a workspace on this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/resource-limits-quotas-capacity\" rel=\"nofollow noreferrer\">page<\/a>. I am sure that the number of runs in our workspace has not reached to the 10 millions, because I can created new runs under new experiments dashboard. But I don\u2019t know anything about the limitations on the number of runs in a specific experiment or even any limitations for the number of runs per users in a specific experiment.\nI couldn't find any clear document explaining this fact.<\/p>\n<p>Is there anyone who can help me for this issue?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1654510800217,
        "Question_score":2,
        "Question_tags":"azure|azure-machine-learning-service|azureml-python-sdk|azure-python-sdk|azuremlsdk",
        "Question_view_count":82,
        "Owner_creation_time":1558366212313,
        "Owner_last_access_time":1663775379147,
        "Owner_location":"Zanjan, Zanjan Province, Iran",
        "Owner_reputation":425,
        "Owner_up_votes":216,
        "Owner_down_votes":6,
        "Owner_views":32,
        "Question_last_edit_time":1655619106363,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72516242",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62407200,
        "Question_title":"How to set learner type for Python Model in Azure Machine Learning Designer?",
        "Question_body":"<p>I am testing Azure Machine Learning Designer by having a custom Python Model (a simple kNN classification). I would like to tune the value of 'k' and get the best performing model but \"Tune Model Hyperparameters\" module gives following error when giving output from my \"Create Python Model\" as input.<\/p>\n\n<pre><code>ModuleExceptionMessage:LearnerTypesNotCompatible: Got incompatible learner type: \"None\". Expected learner types are: \"(&lt;TaskType.BinaryClassification: 1&gt;, &lt;TaskType.MultiClassification: 2&gt;, &lt;TaskType.Regression: 3&gt;)\".\n<\/code><\/pre>\n\n<p>How I can set the learner type of my own Python model? Is it even possible? Should I just code the parameter tuning myself with \"Execute Python Script\"-module?<\/p>\n\n<p>My \"Create Python model\"-module script:<\/p>\n\n<pre><code>import pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclass AzureMLModel:\n    def __init__(self, k = 3):\n        self.model = KNeighborsClassifier(n_neighbors = k)\n        self.feature_column_names = list()\n\n    def train(self, df_train, df_label):\n        self.feature_column_names = df_train.columns.tolist()\n        self.model.fit(df_train, df_label)\n\n    def predict(self, df):\n        return pd.DataFrame({'Scored Labels': self.model.predict(df[self.feature_column_names])})\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1592306247567,
        "Question_score":1,
        "Question_tags":"python|azure|machine-learning|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":108,
        "Owner_creation_time":1592304949617,
        "Owner_last_access_time":1593094592020,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1592306531683,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62407200",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":61415793,
        "Question_title":"Log metrics in PythonScriptStep",
        "Question_body":"<p>In my Azure ML pipeline I've got a PythonScriptStep that is crunching some data. I need to access the Azure ML Logger to track metrics in the step, so I'm trying to import get_azureml_logger but that's bombing out. I'm not sure what dependency I need to install via pip. <\/p>\n\n<p><code>from azureml.logging import get_azureml_logger<\/code><\/p>\n\n<p><code>ModuleNotFoundError: No module named 'azureml.logging'<\/code><\/p>\n\n<p>I came across a similar <a href=\"https:\/\/stackoverflow.com\/questions\/49438358\/azureml-logging-module-not-found\">post<\/a> but it deals with Azure Notebooks. Anyway, I tried adding that blob to my pip dependency, but it's failing with an Auth error.   <\/p>\n\n<pre><code>Collecting azureml.logging==1.0.79 [91m  ERROR: HTTP error 403 while getting\nhttps:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\n[0m91m  ERROR: Could not install requirement azureml.logging==1.0.79 from\nhttps:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\n(from -r \/azureml-environment-setup\/condaenv.g4q7suee.requirements.txt\n(line 3)) because of error 403 Client Error:\nServer failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. for url:\nhttps:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\n<\/code><\/pre>\n\n<p>I'm not sure how to move on this, all I need to do is to log metrics in the step.  <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1587755548897,
        "Question_score":1,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":361,
        "Owner_creation_time":1330016065410,
        "Owner_last_access_time":1662160983830,
        "Owner_location":null,
        "Owner_reputation":1704,
        "Owner_up_votes":61,
        "Owner_down_votes":7,
        "Owner_views":232,
        "Question_last_edit_time":1587809992912,
        "Answer_body":"<p>Check out the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-track-experiments#option-2-use-scriptrunconfig\" rel=\"nofollow noreferrer\">ScriptRunConfig Section of the Monitor Azure ML experiment runs and metrics<\/a>. <code>ScriptRunConfig<\/code> works effectively the same as a <code>PythonScriptStep<\/code>.<\/p>\n\n<p>The idiom is generally to have the following in your the script of your <code>PythonScriptStep<\/code>:<\/p>\n\n<pre><code>from azureml.core.run import Run\nrun = Run.get_context()\nrun.log('foo_score', \"bar\")\n<\/code><\/pre>\n\n<p>Side note: You don't need to change your environment dependencies to use this because <code>PythonScriptStep<\/code>s have <code>azureml-defaults<\/code> installed automatically as a dependency.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1587756432003,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61415793",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71135228,
        "Question_title":"Connection timeout in AzureML studio",
        "Question_body":"<p>I'm trying to connect a VM I have in AzureML studio.  I keep getting the following:  Connection attempt timed out for ''. Verify that server is accessible and SSH service is accepting connections.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644973069297,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":208,
        "Owner_creation_time":1555914279857,
        "Owner_last_access_time":1644991232357,
        "Owner_location":null,
        "Owner_reputation":25,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Go to your VM config and test your connection through the 'connect' tab.  Is your test successful?  If not, check if port 22 is blocked.  Watch for automated blocking rules applied to your VM.<\/p>\n<p>we have DSVM attach in preview - might be interesting for you: <a href=\"https:\/\/github.com\/Azure\/azureml-previews\/tree\/main\/previews\/dsvm-attach\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azureml-previews\/tree\/main\/previews\/dsvm-attach<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1644985009903,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1644985509216,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71135228",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":68237581,
        "Question_title":"In Azure Machine Learning, where are log files stored?",
        "Question_body":"<p>Every time I submit and execute a Run in Azure Machine Learning, I end up getting logs. I can see the logs by going to the Run page and clicking on 'Outputs+logs' tab (see the image).\nHere I am curious about where the logs actually are. I have an Azure Blob storage but can't find the logs there. Where are they?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/MYHwt.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1625325902243,
        "Question_score":2,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":537,
        "Owner_creation_time":1610542341460,
        "Owner_last_access_time":1663140958047,
        "Owner_location":"Seoul, South Korea",
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68237581",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60709452,
        "Question_title":"Getting the run_id programmatically from an AzureML run",
        "Question_body":"<pre class=\"lang-python prettyprint-override\"><code>run = Run.get_context()\nrun_id = run.run_id\n<\/code><\/pre>\n\n<p>pruduces the error<\/p>\n\n<blockquote>\n  <p>AttributeError: '_SubmittedRun' object has no attribute 'run_id'  <\/p>\n<\/blockquote>\n\n<p>But <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run(class)?view=azure-ml-py\" rel=\"nofollow noreferrer\">the documentation<\/a> seems to suggest that as the correct way to glean the run_id from experiment code.<\/p>\n\n<p>How should I glean the run_id from AzureML SDK code? (N.B. I am using a library that hides the call to <code>submit<\/code> from me.)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1584375132240,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":1326,
        "Owner_creation_time":1295000383010,
        "Owner_last_access_time":1663824813343,
        "Owner_location":"Cambridge, United Kingdom",
        "Owner_reputation":15147,
        "Owner_up_votes":2159,
        "Owner_down_votes":27,
        "Owner_views":1915,
        "Question_last_edit_time":1599468848123,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60709452",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58320938,
        "Question_title":"azureml history logging to output dir",
        "Question_body":"<p>When running my Pytorch Estimator in the cloud, I get this output regarding the recommended way to save my training scripts output to a folder called '.\/outputs'. This folder is created by my script, and is in the correct root-folder provided. It has output.<\/p>\n\n<p>However the estimator returns this, while preparing the container:\nStarting the daemon thread to refresh tokens in background for process with pid = 124\nWarning: Unable to import azureml.history. Output collection disabled.<\/p>\n\n<p>all I provide my estimator is my training script, conda env.yml and a compute target.<\/p>\n\n<p>It completes the training script successfully, not errors.<\/p>\n\n<p>Any ideas why I get this ?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1570703699080,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":213,
        "Owner_creation_time":1417378487127,
        "Owner_last_access_time":1651826944133,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1570736060340,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58320938",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":61168984,
        "Question_title":"Azure ML free trial: how to submit pipeline?",
        "Question_body":"<p>I'm using a free trial account on MS Azure and I'm following this tutorial.<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score<\/a><\/p>\n\n<p>I'm stuck when I try to \"submit the pipeline\".<\/p>\n\n<p>The reason seems to be that I can't create a compute instance or a training cluster on a free plan.\nI still have 200USDs of free credits. I guess there must be a solution?<\/p>\n\n<hr>\n\n<p>Error messages:<\/p>\n\n<pre><code>Invalid graph: The pipeline compute target is invalid.\n\n400: Compute Test3 in state Failed, which is not able to use\n\nCompute instance: creation failed\nThe specified subscription has a total vCPU quota of 0 and is less than the requested compute training cluster and\/or compute instance's min nodes of 1 which maps to 4 vCPUs\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586681686103,
        "Question_score":2,
        "Question_tags":"azure|azure-machine-learning-service",
        "Question_view_count":370,
        "Owner_creation_time":1343682543650,
        "Owner_last_access_time":1655966822477,
        "Owner_location":null,
        "Owner_reputation":135,
        "Owner_up_votes":51,
        "Owner_down_votes":0,
        "Owner_views":45,
        "Question_last_edit_time":1586690281396,
        "Answer_body":"<p>Please check the announcement from MS Team regarding this:<\/p>\n\n<p><a href=\"https:\/\/azure.microsoft.com\/blog\/our-commitment-to-customers-and-microsoft-cloud-services-continuity\/\" rel=\"nofollow noreferrer\">https:\/\/azure.microsoft.com\/blog\/our-commitment-to-customers-and-microsoft-cloud-services-continuity\/<\/a><\/p>\n\n<p>All the free trials will not work as of now<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1586681759587,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61168984",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72983144,
        "Question_title":"azureml tabular dataset over azure gen2 datalake",
        "Question_body":"<h1>What have I tried<\/h1>\n<ul>\n<li>set up an AzureML DataStore using Identity based authentication<\/li>\n<li>set up an AzureML Dataset for a single file under a specific file system<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>\n    workspace = Workspace.from_config(&quot;config.json&quot;, auth= auth)\n    dataset = Dataset.get_by_name(workspace, 'engage_event_type')\n    frame = dataset.to_pandas_dataframe()\n<\/code><\/pre>\n<p>I am able to explore the dataset from azure portal and it displays the right data correctly.<\/p>\n<p>However running ^ where <code>auth<\/code> is a Service Principal which has the same rights as Azure Workspace Instance I get a bunch of calls like, but no errors \/ exceptions \/ completion.<\/p>\n<p>The data underneath is &lt; 10kb<\/p>\n<pre><code>Resolving access token for scope &quot;https:\/\/datalake.azure.net\/\/.default&quot; using identity of type &quot;SP&quot;.\nResolving access token for scope &quot;https:\/\/datalake.azure.net\/\/.default&quot; using identity of type &quot;SP&quot;.\n<\/code><\/pre>\n<ul>\n<li>I have tried running the script on a local compute<\/li>\n<li>I have tried running the script on a compute instance<\/li>\n<\/ul>\n<p>both gave the same issue<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1657812916327,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":431,
        "Owner_creation_time":1271093246887,
        "Owner_last_access_time":1663988177550,
        "Owner_location":"United States",
        "Owner_reputation":9826,
        "Owner_up_votes":1723,
        "Owner_down_votes":15,
        "Owner_views":1238,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72983144",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":59873804,
        "Question_title":"Error 0085 while executing python script in Azure Web service but not in ML Experiment",
        "Question_body":"<p>My workflow is running perfect on Experimentation, but after deployed to web service, I receive this error while post.<\/p>\n\n<p>Python Code:<\/p>\n\n<pre><code># -*- coding: utf-8 -*-\n\n#import sys\nimport pickle\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree \n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    print('input dataframe1 ',dataframe1)\n    decision_tree_pkl_predictive_maint = r'.\\Script Bundle\\decision_tree_pkl_predictive_maint.pkl'\n\n    #sys.path.insert(0,\".\\Script Bundle\")\n    #model = pickle.load(open(\".\\Script Bundle\\decision_tree_pkl_predictive_maint.pkl\", 'rb'))\n\n    modle_file = open(decision_tree_pkl_predictive_maint,\"rb\")\n    model = pickle.load(modle_file)\n\n    #return the mode of prediciton\n    result = model.predict(dataframe1)\n    print(result)\n    result_df = pd.DataFrame({'prediction_class':result})\n    return result_df,\n<\/code><\/pre>\n\n<p>ERROR:<\/p>\n\n<p>Execute Python Script RRS : Error 0085: The following error occurred during script evaluation, please view the output log for more information: ---------- Start of error message from Python interpreter ---------- Caught exception while executing function: Traceback (most recent call last): File \"\\server\\InvokePy.py\", line 120, in executeScript outframe = mod.azureml_main(*inframes) File \"\\temp-1036260731852293620.py\", line 46, in azureml_main modle_file = open(decision_tree_pkl_predictive_maint,\"rb\") FileNotFoundError: [Errno 2] No such file or directory: '.\\Script Bundle\\decision_tree_pkl_predictive_maint.pkl' ---------- End of error message from Python interpreter ----------<\/p>\n\n<p>Please, Advice.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579766106190,
        "Question_score":1,
        "Question_tags":"azure|machine-learning|azure-web-app-service|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":215,
        "Owner_creation_time":1554724240183,
        "Owner_last_access_time":1664041243347,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":29,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The issue has to do with your file path. Ensure that you have included the correct path.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1580091525612,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59873804",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73692700,
        "Question_title":"AzureML Hyperdrive. Pass data between trials",
        "Question_body":"<p>Is it possible to pass data between individual trials in HyperDrive experiment?<\/p>\n<p>The <a href=\"https:\/\/github.com\/Azure\/azureml-examples\/blob\/sdk-preview\/sdk\/jobs\/single-step\/lightgbm\/iris\/src\/main.py\" rel=\"nofollow noreferrer\">example notebook from AzureML<\/a> reads training data inside training script, which will be executed in every separate trial of the HyperDrive experiment. However, it would be much more efficient if we could read data only once and pass it between all trials.<\/p>\n<p>Is it possible to configure it?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663001710647,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-service|hyperdrive",
        "Question_view_count":20,
        "Owner_creation_time":1531993302317,
        "Owner_last_access_time":1663952126937,
        "Owner_location":null,
        "Owner_reputation":367,
        "Owner_up_votes":52,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73692700",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":69450547,
        "Question_title":"WebserviceException: Unable to deploy a model with aks and azure machine learning",
        "Question_body":"<p>I tried to deploy a new model in azure databricks notebook.\nThis morning it was working and now I have the following error:<\/p>\n<p>After<\/p>\n<pre><code>service.wait_for_deployment(show_output=True)\nprint(service.state)\nprint(service.get_logs())\n<\/code><\/pre>\n<p>I have:<\/p>\n<pre><code>&quot;message&quot;: &quot;Timed out waiting for AKS deployment to complete. pollTimeout : 00:20:00 serviceName: simdev serviceId: ...&quot;,\n  &quot;details&quot;: [\n    {\n      &quot;code&quot;: &quot;DeploymentTimedOut&quot;,\n      &quot;message&quot;: &quot;Your container endpoint is not available. Please follow the steps to debug:\n    1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https:\/\/aka.ms\/debugimage#dockerlog for more information.\n    2. You can also interactively debug your scoring file locally. Please refer to https:\/\/docs.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n    3. View the diagnostic events to check status of container, it may help you to debug the issue.\n{&quot;InvolvedObject&quot;:&quot;simdev-757df4f999-rbcws&quot;,&quot;InvolvedKind&quot;:&quot;Pod&quot;,&quot;Type&quot;:&quot;Warning&quot;,&quot;Reason&quot;:&quot;FailedScheduling&quot;,&quot;Message&quot;:&quot;0\/2 nodes are available: 2 Insufficient nvidia.com\/gpu.&quot;,&quot;LastTimestamp&quot;:null}\n{&quot;InvolvedObject&quot;:&quot;simdev-757df4f999-rbcws&quot;,&quot;InvolvedKind&quot;:&quot;Pod&quot;,&quot;Type&quot;:&quot;Warning&quot;,&quot;Reason&quot;:&quot;FailedScheduling&quot;,&quot;Message&quot;:&quot;0\/2 nodes are available: 2 Insufficient nvidia.com\/gpu.&quot;,&quot;LastTimestamp&quot;:null}\n{&quot;InvolvedObject&quot;:&quot;simdev-757df4f999-rbcws&quot;,&quot;InvolvedKind&quot;:&quot;Pod&quot;,&quot;Type&quot;:&quot;Normal&quot;,&quot;Reason&quot;:&quot;Scheduled&quot;,&quot;Message&quot;:&quot;Successfully assigned azureml-train-aml-001-dev\/simdev-757df4f999-rbcws to aks-agentpool-34690879-vmss000000&quot;,&quot;LastTimestamp&quot;:null}\n<\/code><\/pre>\n<p>Yesterday it didn't work. This morning yes, and now no.<\/p>\n<p>Here is aks config:<\/p>\n<pre><code>aks_config = AksWebservice.deploy_configuration(cpu_cores=0.7,\n                                                memory_gb=0.7,\n                                                gpu_cores=1,\n                                                period_seconds=1800,\n                                                failure_threshold=10,\n                                                timeout_seconds=60,\n                                                max_request_wait_time=300000,\n                                                scoring_timeout_ms=300000,)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1633437212317,
        "Question_score":0,
        "Question_tags":"azure-aks|azure-databricks|azure-machine-learning-service",
        "Question_view_count":178,
        "Owner_creation_time":1606642099553,
        "Owner_last_access_time":1654258609767,
        "Owner_location":null,
        "Owner_reputation":371,
        "Owner_up_votes":65,
        "Owner_down_votes":0,
        "Owner_views":55,
        "Question_last_edit_time":1633437593747,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69450547",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":40423227,
        "Question_title":"No applicable method for 'predict' applied to an object of class \"MXFeedForwardModel\"",
        "Question_body":"<p>I have created a Neural network model using mxnet package in R studio. I tested the model on local and it works as expected. I have deployed the same model as a webservice in AzureML using <code>publishwebservice()<\/code> function from R.<\/p>\n\n<p>When I try to predict the test data with the webservice using <code>consume()<\/code> function: <\/p>\n\n<pre><code>pred_cnn &lt;- consume(endpoint_cnn, testdf)\n<\/code><\/pre>\n\n<p>it always throws following error:<\/p>\n\n<blockquote>\n  <p>Error: AzureML returns error code: HTTP status code : 400 AzureML\n  error code  : LibraryExecutionError<\/p>\n  \n  <p>Module execution encountered an internal library error.<br>\n  The following\n  error occurred during evaluation of R script: R_tryEval: return error: \n  Error in UseMethod(\"predict\") :<br>\n     no applicable method for 'predict'\n  applied to an object of class \"MXFeedForwardModel\"<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1478264100187,
        "Question_score":2,
        "Question_tags":"r|azure-machine-learning-studio|mxnet",
        "Question_view_count":983,
        "Owner_creation_time":1312015616267,
        "Owner_last_access_time":1664044812267,
        "Owner_location":"Copenhagen, Denmark",
        "Owner_reputation":19521,
        "Owner_up_votes":409,
        "Owner_down_votes":5,
        "Owner_views":2574,
        "Question_last_edit_time":1478546093720,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40423227",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60866431,
        "Question_title":"When deploying an ml model using vscode I get an error docker image build failed",
        "Question_body":"<p>I'm following this tutorial <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-train-deploy-image-classification-model-vscode\" rel=\"nofollow noreferrer\">VSCODE tensorflow model deployment on Azure<\/a>.\nHere instead of tensorflow model I'm trying to deploy a simple decision tree model.I create a train.py file like this<\/p>\n<pre><code>import pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nimport pickle\nimport os\nimport joblib\n\ndata=pd.read_csv('CreditCardWeka.csv')\nmodel=DecisionTreeClassifier()\nY=data['Class']\ndel data['Class']\nX=data\nmodel.fit(X,Y)\nos.makedirs('.\/outputs\/model', exist_ok = True)\njoblib.dump(model, '.\/outputs\/model\/dec_model.sav')\n<\/code><\/pre>\n<p>After this I create a compute,create a run configuration and select this file.After this I create an experiment and run it and download the output.I'm able to download the output and till here it works.\nAfter this I'm able to successfully register my model and when I try to deploy it as an &quot;Azure Container Service&quot; it asks for score.py while which is this<\/p>\n<pre><code>import os\nimport joblib\nimport json\nimport time\nimport sklearn\n# Called when the deployed service starts\nfrom azureml.core.model import Model\n\ndef init():\n    global model\n\n    # Get the path where the deployed model can be found.\n    # load models\n    model_root = Model.get_model_path('decision-tree-model')\n    model = joblib.load(os.path.join(model_root, 'dec-model.sav'))\n\n# Handle requests to the service\ndef run(data):\n    try:\n        # Pick out the text property of the JSON request.\n        # This expects a request in the form of {&quot;text&quot;: &quot;some text to score for sentiment&quot;}\n        data = json.loads(data)\n        prediction = model.predict(data['X'])\n        #Return prediction\n        return prediction\n    except Exception as e:\n        error = str(e)\n        return error\n<\/code><\/pre>\n<p>It also asks for a yml file which is this<\/p>\n<pre><code>name: decision-tree\nchannels:\n  - defaults\ndependencies:\n  - python\n  - sklearn\n  - joblib\n  - pip\n  - pip:\n    - azureml-defaults\n<\/code><\/pre>\n<p>After this when it starts creating a docker image it failes and the error is &quot;Docker image build failed&quot;.\nHow can I resolve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1585222856953,
        "Question_score":0,
        "Question_tags":"python|azure|docker|visual-studio-code|azure-machine-learning-service",
        "Question_view_count":142,
        "Owner_creation_time":1472571773250,
        "Owner_last_access_time":1588686353090,
        "Owner_location":null,
        "Owner_reputation":19,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":1592644375060,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60866431",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":52278613,
        "Question_title":"Add custom packages to Azure Machine Learing Studio",
        "Question_body":"<p>I need to use the function tsCV on azure machine learning studio to evaluate models of forecast, but i got the error <\/p>\n\n<pre><code>could not find function \"tsCV\n<\/code><\/pre>\n\n<p>I'm trying to update the forecast package, but no package are loaded.\nI followed this tutorial\n<a href=\"http:\/\/blog.revolutionanalytics.com\/2015\/10\/using-minicran-in-azure-ml.html\" rel=\"noreferrer\">http:\/\/blog.revolutionanalytics.com\/2015\/10\/using-minicran-in-azure-ml.html<\/a>\nand \n<a href=\"https:\/\/blog.tallan.com\/2016\/12\/27\/adding-r-packages-in-azure-ml\/\" rel=\"noreferrer\">https:\/\/blog.tallan.com\/2016\/12\/27\/adding-r-packages-in-azure-ml\/<\/a>\nbut i dont get the same result.\nNo packages are load.<\/p>\n\n<p>I need an example of a package with R code that works o Azure ML or an update of forecast package to use tsCV function.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1536677172073,
        "Question_score":3,
        "Question_tags":"r|azure-machine-learning-studio",
        "Question_view_count":404,
        "Owner_creation_time":1515518171123,
        "Owner_last_access_time":1657119408507,
        "Owner_location":null,
        "Owner_reputation":1108,
        "Owner_up_votes":33,
        "Owner_down_votes":2,
        "Owner_views":183,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I have installed the latest version of the forecast package and here are the steps I followed during the installation. <\/p>\n\n<ol>\n<li>Download latest version of CRAN<\/li>\n<li>Be sure that tsCV is working locally<\/li>\n<li>Zip all the dependencies + forecast package<\/li>\n<li>Zip all the generated zips together and upload it to the AMLStudio<\/li>\n<li>Run the following code:<\/li>\n<\/ol>\n\n<blockquote>\n<pre><code>install.packages(\"src\/glue.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/stringi.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/assertthat.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/fansi.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/utf8.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/stringr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/labeling.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/munsell.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/R6.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/RColorBrewer.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/cli.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/crayon.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/pillar.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/xts.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/TTR.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/curl.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/digest.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/gtable.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/lazyeval.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/plyr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/reshape2.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/rlang.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/scales.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/tibble.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/viridisLite.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/withr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/quadprog.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/quantmod.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/colorspace.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/fracdiff.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/ggplot2.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/lmtest.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/magrittr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/Rcpp.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/timeDate.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/tseries.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/urca.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/uroot.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/zoo.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/RcppArmadillo.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/forecast.zip\", lib = \".\", repos = NULL, verbose = TRUE)\n\nlibrary(forecast, lib.loc=\".\", verbose=TRUE)\nfar2 &lt;- function(x, h){forecast(Arima(x, order=c(2,0,0)), h=h)}\ne &lt;- tsCV(lynx, far2, h=1)\n<\/code><\/pre>\n<\/blockquote>\n\n<p><a href=\"https:\/\/drive.google.com\/open?id=10Bj0RGCmRFrRECLQrVc26nbx3T-bNSL6\" rel=\"nofollow noreferrer\">Here is the zip I have generated:<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/bbowH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bbowH.png\" alt=\"My experiment\"><\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1537192338792,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52278613",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":69245175,
        "Question_title":"Error in azureml \"Non numeric value(s) were encountered in the target column.\"",
        "Question_body":"<p>I am using Automated ML to run a time series forecasting pipeline.<\/p>\n<p>When the AutoMLStep gets triggered, I get this error: <code>Non numeric value(s) were encountered in the target column<\/code>.<\/p>\n<p>The data to this step is passed through an OutputTabularDatasetConfig, after applying the read_delimited_files() on an OutputFileDatasetConfig. I've inspected the prior step, and the data is comprised of a 'Date' column and a numeric column called 'Place' with +80 observations in monthly frequencies.<\/p>\n<p>Nothing seems to be wrong with the column type or the data. I've also applied a number of techniques on the data prep side e.g. pd.to_numeric(), astype(float) to ensure it is numeric.<\/p>\n<p>I've also tried forcing this through the FeaturizationConfig() <code>add_column_purpose('Place','Numeric')<\/code> but in this case, I get another error: <code>Expected column(s) Place in featurization config's column purpose not found in X.<\/code><\/p>\n<p>Any thoughts on how to solve?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1632069680553,
        "Question_score":1,
        "Question_tags":"automl|azure-machine-learning-service",
        "Question_view_count":127,
        "Owner_creation_time":1402462976833,
        "Owner_last_access_time":1664062600860,
        "Owner_location":"Redmond, WA, United States",
        "Owner_reputation":525,
        "Owner_up_votes":70,
        "Owner_down_votes":0,
        "Owner_views":52,
        "Question_last_edit_time":null,
        "Answer_body":"<p>So a few learnings on this interacting with the stellar Azure Machine Learning engineering team.<\/p>\n<ol>\n<li>When calling the <code>read_delimited_files()<\/code> method, ensure that the output folder does not have many inputs or files. For example, if all intermediate outputs are saved to a common folder, it may read all the prior inputs into this folder, and depending upon the shape of the data, borrow the schema from the first file, or confuse all of them together. This can lead to inconsistencies and errors. In my case, I was dumping many files to the same location, hence this was causing confusion for this method. The fix is either to distinctly mark the output folder (e.g. with a UUID) or give different paths.<\/li>\n<li>The dataframe from <code>read_delimiter_files()<\/code> may treat all columns as object type which can lead to a data type check failure (i.e. label_column needs to be numeric). To mitigate, explictly state the type. For example:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.data import DataType\nprepped_data = prepped_data.read_delimited_files(set_column_types={&quot;Place&quot;:DataType.to_float()})\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1633037776347,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69245175",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":54254830,
        "Question_title":"Running Azure Machine Learning Service pipeline locally",
        "Question_body":"<p>I'm using Azure Machine Learning Service with the azureml-sdk python library.<\/p>\n\n<p>I'm using azureml.core version 1.0.8<\/p>\n\n<p>I'm following this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline<\/a> tutorial.<\/p>\n\n<p>I've got it working when I use Azure Compute resources. But I would like to run it locally.<\/p>\n\n<p>I get the following error<\/p>\n\n<pre><code>raise ErrorResponseException(self._deserialize, response)\nazureml.pipeline.core._restclients.aeva.models.error_response.ErrorResponseException: (BadRequest) Response status code does not indicate success: 400 (Bad Request).\nTrace id: [uuid], message: Can't build command text for [train.py], moduleId [uuid] executionId [id]: Assignment for parameter Target is not specified\n<\/code><\/pre>\n\n<p>My code looks like:<\/p>\n\n<pre><code>run_config = RunConfiguration()\ncompute_target = LocalTarget()\nrun_config.target = LocalTarget()    \nrun_config.environment.python.conda_dependencies = CondaDependencies(conda_dependencies_file_path='environment.yml')\nrun_config.environment.python.interpreter_path = 'C:\/Projects\/aml_test\/.conda\/envs\/aml_test_env\/python.exe'\nrun_config.environment.python.user_managed_dependencies = True\nrun_config.environment.docker.enabled = False\n\ntrainStep = PythonScriptStep(\n    script_name=\"train.py\",\n    compute_target=compute_target,\n    source_directory='.',\n    allow_reuse=False,\n    runconfig=run_config\n)\n\nsteps = [trainStep]\n\n# Build the pipeline\npipeline = Pipeline(workspace=ws, steps=[steps])\npipeline.validate()\n\nexperiment = Experiment(ws, 'Test')\n\n# Fails, locally, works on Azure Compute\nrun = experiment.submit(pipeline)\n\n\n# Works both locally and on Azure Compute\nsrc = ScriptRunConfig(source_directory='.', script='train.py', run_config=run_config)\nrun = experiment.submit(src)\n<\/code><\/pre>\n\n<p>The <code>train.py<\/code> is a very simple self contained script only dependent on numpy that approximates pi.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1547817410513,
        "Question_score":5,
        "Question_tags":"azure-machine-learning-workbench|azure-machine-learning-service",
        "Question_view_count":2476,
        "Owner_creation_time":1313998995867,
        "Owner_last_access_time":1663912609667,
        "Owner_location":null,
        "Owner_reputation":3148,
        "Owner_up_votes":29,
        "Owner_down_votes":12,
        "Owner_views":347,
        "Question_last_edit_time":1554703788007,
        "Answer_body":"<p>Local compute cannot be used with ML Pipelines. Please see this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-set-up-training-targets#supported-compute-targets\" rel=\"noreferrer\">article<\/a>.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1548188011640,
        "Answer_score":7.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":1548188395572,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54254830",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":56848293,
        "Question_title":"What is Random seed in Azure Machine Learning?",
        "Question_body":"<p>I am learning Azure Machine Learning. I am frequently encountering the <strong>Random Seed<\/strong> in some of the steps like,<\/p>\n\n<ol>\n<li>Split Data<\/li>\n<li>Untrained algorithm models as Two Class Regression, Multi-class regression, Tree, Forest,..<\/li>\n<\/ol>\n\n<p>In the tutorial, they choose Random Seed as '123'; trained model has high accuracy but when I try to choose other random integers like 245, 256, 12, 321,.. it did not do well.<\/p>\n\n<hr>\n\n<p><strong>Questions<\/strong><\/p>\n\n<ul>\n<li>What is a Random Seed Integer?<\/li>\n<li>How to carefully choose a Random Seed from range of integer values? What is the key or strategy to choose it?<\/li>\n<li>Why does Random Seed significantly affect the ML Scoring, Prediction and Quality of the trained model?<\/li>\n<\/ul>\n\n<hr>\n\n<p><strong>Pretext<\/strong><\/p>\n\n<ol>\n<li>I have <a href=\"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/iris\/iris.data\" rel=\"nofollow noreferrer\">Iris-Sepal-Petal-Dataset<\/a> with Sepal (<em>Length &amp; Width<\/em>) and Petal (<em>Length &amp; Width<\/em>)<\/li>\n<li>Last column in data-set is 'Binomial ClassName'<\/li>\n<li>I am training the data-set with Multiclass Decision Forest Algorithm and splitting the data with different random seeds 321, 123 and 12345 in order<\/li>\n<li>It affects the final quality of trained model. Random seed#123 being best of Prediction probability score: 1.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/12OyD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/12OyD.png\" alt=\"ML Studio Snap\"><\/a><\/p>\n\n<hr>\n\n<p><strong>Observations<\/strong><\/p>\n\n<p><strong>1. Random seed: 321<\/strong><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YcsiS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YcsiS.png\" alt=\"Random-seed-321\"><\/a><\/p>\n\n<p><strong>2. Random seed: 123<\/strong><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Qrk4G.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Qrk4G.png\" alt=\"Random-seed-123\"><\/a><\/p>\n\n<p><strong>3. Random seed: 12345<\/strong><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/D1Rki.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/D1Rki.png\" alt=\"Random-seed-12345\"><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":9,
        "Question_creation_time":1562056038867,
        "Question_score":0,
        "Question_tags":"machine-learning|azure-machine-learning-studio|random-seed|iris-dataset",
        "Question_view_count":2046,
        "Owner_creation_time":1475691108437,
        "Owner_last_access_time":1658651167790,
        "Owner_location":"India",
        "Owner_reputation":1849,
        "Owner_up_votes":366,
        "Owner_down_votes":21,
        "Owner_views":253,
        "Question_last_edit_time":1562066943383,
        "Answer_body":"<blockquote>\n  <p>What is a Random Seed Integer?<\/p>\n<\/blockquote>\n\n<p>Will not go into any details regarding what a random seed is in general; there is plenty of material available by a simple web search (see for example <a href=\"https:\/\/stackoverflow.com\/questions\/22639587\/random-seed-what-does-it-do\">this SO thread<\/a>).<\/p>\n\n<p>Random seed serves just to initialize the (pseudo)random number generator, mainly in order to make ML examples reproducible.<\/p>\n\n<blockquote>\n  <p>How to carefully choose a Random Seed from range of integer values? What is the key or strategy to choose it?<\/p>\n<\/blockquote>\n\n<p>Arguably this is already answered implicitly above: you are simply not supposed to choose any particular random seed, and your results should be roughly the same across different random seeds.<\/p>\n\n<blockquote>\n  <p>Why does Random Seed significantly affect the ML Scoring, Prediction and Quality of the trained model?<\/p>\n<\/blockquote>\n\n<p>Now, to the heart of your question. The answer <em>here<\/em> (i.e. with the iris dataset) is the <strong>small-sample effects<\/strong>...<\/p>\n\n<p>To start with, your reported results across different random seeds are not <em>that<\/em> different. Nevertheless, I agree that, at first sight, a difference in macro-average precision of 0.9 and 0.94 might <em>seem<\/em> large; but looking more closely it is revealed that the difference is really not an issue. Why?<\/p>\n\n<p>Using the 20% of your (only) 150-samples dataset leaves you with only 30 samples in your test set (where the evaluation is performed); this is stratified, i.e. about 10 samples from each class. Now, for datasets of <em>that<\/em> small size, it is not difficult to imagine that a difference in the correct classification of <strong>only 1-2<\/strong> samples can have this apparent difference in the performance metrics reported.<\/p>\n\n<p>Let's try to verify this in scikit-learn using a decision tree classifier (the essence of the issue does not depend on the specific framework or the ML algorithm used):<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>from sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_iris(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=321, stratify=y)\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n<\/code><\/pre>\n\n<p>Result:<\/p>\n\n<pre><code>[[10  0  0]\n [ 0  9  1]\n [ 0  0 10]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       1.00      0.90      0.95        10\n           2       0.91      1.00      0.95        10\n\n   micro avg       0.97      0.97      0.97        30\n   macro avg       0.97      0.97      0.97        30\nweighted avg       0.97      0.97      0.97        30\n<\/code><\/pre>\n\n<p>Let's repeat the code above, changing only the <code>random_state<\/code> argument in <code>train_test_split<\/code>; for <code>random_state=123<\/code> we get:<\/p>\n\n<pre><code>[[10  0  0]\n [ 0  7  3]\n [ 0  2  8]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       0.78      0.70      0.74        10\n           2       0.73      0.80      0.76        10\n\n   micro avg       0.83      0.83      0.83        30\n   macro avg       0.84      0.83      0.83        30\nweighted avg       0.84      0.83      0.83        30\n<\/code><\/pre>\n\n<p>while for <code>random_state=12345<\/code> we get:<\/p>\n\n<pre><code>[[10  0  0]\n [ 0  8  2]\n [ 0  0 10]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       1.00      0.80      0.89        10\n           2       0.83      1.00      0.91        10\n\n   micro avg       0.93      0.93      0.93        30\n   macro avg       0.94      0.93      0.93        30\nweighted avg       0.94      0.93      0.93        30\n<\/code><\/pre>\n\n<p>Looking at the <em>absolute numbers<\/em> of the 3 confusion matrices (in <em>small samples<\/em>, percentages can be <strong>misleading<\/strong>), you should be able to convince yourself that the differences are not that big, and they can be arguably justified by the random element inherent in the whole procedure (here the exact split of the dataset into training and test).<\/p>\n\n<p>Should your test set be significantly bigger, these discrepancies would be practically negligible... <\/p>\n\n<p>A last notice; I have used the exact same seed numbers as you, but this does not actually mean anything, as in general the random number generators <em>across<\/em> platforms &amp; languages are not the same, hence the corresponding seeds are not actually compatible. See own answer in <a href=\"https:\/\/stackoverflow.com\/questions\/52293899\/are-random-seeds-compatible-between-systems\">Are random seeds compatible between systems?<\/a> for a demonstration.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1562069850056,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1562070151169,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56848293",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":69113428,
        "Question_title":"Model deployment in Azure ML",
        "Question_body":"<p>I am a beginner in the Azure. I am using this tutorial <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python<\/a> of setting a dummy script for a local web service but many errors are coming up. It is strange because I am using an h5 file (model involving Keras and tensor flow) in place of onxx file. I used the code<\/p>\n<pre><code>from azureml.core import Environment\nfrom azureml.core.model import InferenceConfig\n\nenv = Environment(name=&quot;myenv&quot;)\nconda_dep = CondaDependencies()\nconda_dep.add_conda_package(&quot;tensorflow&quot;)\nconda_dep.add_conda_package(&quot;pip&quot;)\nconda_dep.add_pip_package(&quot;azureml-core&quot;)\nconda_dep.add_pip_package(&quot;azureml-contrib-services&quot;)\nenv.python.conda_dependencies=conda_dep\ninference_config = InferenceConfig(\n    environment=env,\n    source_directory=&quot;.\/source_dir&quot;,\n    entry_script=&quot;.\/echo_score.py&quot;,\n)\n \n<\/code><\/pre>\n<p>There is a Error<\/p>\n<pre><code>ERROR: Could not find a version that satisfies the requirement pickle (from -r \/azureml-environment-setup\/condaenv.4f206b3i.requirements.txt (line 5)) (from versions: none)\n<\/code><\/pre>\n<p>I have tried many times and getting error. Can anyone help.\nMy azureml core is 1.34.0 and python version 3.8.11.I am also not sure if it is a pickle protocol error. Pickle version in my system is 4.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1631169973543,
        "Question_score":0,
        "Question_tags":"python|azure-web-app-service|web-deployment|azure-machine-learning-service",
        "Question_view_count":185,
        "Owner_creation_time":1622179709977,
        "Owner_last_access_time":1632984750927,
        "Owner_location":"Singapore",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1631796790996,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69113428",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":40302499,
        "Question_title":"Recommendation API: what is the difference between null results and empty results",
        "Question_body":"<p>In the Azure Recommendation API sample there is a snippet like this:<\/p>\n\n<pre><code>     if (itemSets.RecommendedItemSetInfo != null)\n        {\n            ...\n        }\n        else\n        {\n            Console.WriteLine(\"No recommendations found.\");\n        }\n<\/code><\/pre>\n\n<p>So I assume that nullable recommended set means no recommendations. But what is the case with this set being not nullable but still empty ( as I am having it running the example)?<\/p>\n\n<p>I provided my own usages and catalog files. I have not too many entries there however for i2i recommendations I have results and for u2i there is an empty set.\nAllowColdItemPlacement doesn't change a think here.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1477648288960,
        "Question_score":0,
        "Question_tags":"azure|microsoft-cognitive|azure-machine-learning-studio",
        "Question_view_count":130,
        "Owner_creation_time":1354118434117,
        "Owner_last_access_time":1613750840957,
        "Owner_location":"Wroc\u0142aw, Poland",
        "Owner_reputation":393,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":43,
        "Question_last_edit_time":null,
        "Answer_body":"<p>We did not mean to convey a difference in meaning between null recommendations and empty recommendations. I will check why we may be sending two different types of results. Either way, don't treat those two cases as different cases. <\/p>\n\n<p>If you are not getting results for user-to-item recommendations, most likely there was no data for that user when the build was created or the items that the user interacted with do not have enough co-occurrences with other items in the usage.<\/p>\n\n<p>What to do when you get empty recommendations is up to you, you may decide to not show any recommendations, or back-fill with popular items you may want to promote.<\/p>\n\n<p>Thanks!<\/p>\n\n<p>Luis Cabrera\nProgram Manager - Recommendations API.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1478186851716,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40302499",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":39483984,
        "Question_title":"R package ( qdapTools) version not getting detected correctly in Azure ML",
        "Question_body":"<p>I'm trying to install qdap package in Azure ML. Rest of the dependent packages get installed without any issues. When it comes to qdapTools, I get this error , though the version that I try to install is 1.3.1 ( Verified this from the Decription file that comes with the R package)<\/p>\n\n<pre><code>package 'qdapTools' 1.1.0 was found, but &gt;= 1.3.1 is required by 'qdap\n<\/code><\/pre>\n\n<p>The code in \"Execute R Script\" :<\/p>\n\n<pre><code>install.packages(\"src\/qdapTools.zip\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/magrittr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/stringi.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/stringr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/qdapDictionaries.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/qdapRegex.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/RColorBrewer.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/qdap.zip\", lib = \".\", repos = NULL, verbose = TRUE)\n\nlibrary(stringr, lib.loc=\".\", verbose=TRUE)\nlibrary(qdap, lib.loc=\".\", verbose=TRUE)\n<\/code><\/pre>\n\n<p>And the log : <\/p>\n\n<pre><code>[ModuleOutput] End R Execution: 9\/22\/2016 6:44:44 AM\n[Stop]     DllModuleMethod::Execute. Duration = 00:00:16.7828106\n[Critical]     Error: Error 0063: The following error occurred during evaluation of R script:\n---------- Start of error message from R ----------\npackage 'qdapTools' 1.1.0 was found, but &gt;= 1.3.1 is required by 'qdap'\n\n\npackage 'qdapTools' 1.1.0 was found, but &gt;= 1.3.1 is required by 'qdap'\n----------- End of error message from R -----------\n[Critical]     {\"InputParameters\":{\"DataTable\":[{\"Rows\":2,\"Columns\":1,\"estimatedSize\":11767808,\"ColumnTypes\":{\"System.String\":1},\"IsComplete\":true,\"Statistics\":{\"0\":[2,0]}}],\"Generic\":{\"bundlePath\":\"..\\\\..\\\\Script Bundle\\\\Script Bundle.zip\",\"rLibVersion\":\"R310\"},\"Unknown\":[\"Key: rStreamReader, ValueType : System.IO.StreamReader\"]},\"OutputParameters\":[],\"ModuleType\":\"LanguageWorker\",\"ModuleVersion\":\" Version=6.0.0.0\",\"AdditionalModuleInfo\":\"LanguageWorker, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca;Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS;RunRSNR\",\"Errors\":\"Microsoft.Analytics.Exceptions.ErrorMapping+ModuleException: Error 0063: The following error occurred during evaluation of R script:\\r\\n---------- Start of error message from R ----------\\r\\npackage 'qdapTools' 1.1.0 was found, but &gt;= 1.3.1 is required by 'qdap'\\r\\n\\r\\n\\r\\npackage 'qdapTools' 1.1.0 was found, but &gt;= 1.3.1 is required by 'qdap'\\r\\n----------- End of error message from R -----------\\r\\n   at Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS.ExecuteR(NewRWorker worker, DataTable dataset1, DataTable dataset2, IEnumerable`1 bundlePath, StreamReader rStreamReader, Nullable`1 seed) in d:\\\\_Bld\\\\8831\\\\7669\\\\Sources\\\\Product\\\\Source\\\\Modules\\\\LanguageWorker\\\\LanguageWorker.Dll\\\\EntryPoints\\\\RModule.cs:line 287\\r\\n   at Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS._RunImpl(NewRWorker worker, DataTable dataset1, DataTable dataset2, String bundlePath, StreamReader rStreamReader, Nullable`1 seed, ExecuteRScriptExternalResource source, String url, ExecuteRScriptGitHubRepositoryType githubRepoType, SecureString accountToken) in d:\\\\_Bld\\\\8831\\\\7669\\\\Sources\\\\Product\\\\Source\\\\Modules\\\\LanguageWorker\\\\LanguageWorker.Dll\\\\EntryPoints\\\\RModule.cs:line 207\\r\\n   at Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS.RunRSNR(DataTable dataset1, DataTable dataset2, String bundlePath, StreamReader rStreamReader, Nullable`1 seed, ExecuteRScriptRVersion rLibVersion) in d:\\\\_Bld\\\\8831\\\\7669\\\\Sources\\\\Product\\\\Source\\\\Modules\\\\LanguageWorker\\\\LanguageWorker.Dll\\\\EntryPoints\\\\REntryPoint.cs:line 105\",\"Warnings\":[],\"Duration\":\"00:00:16.7752607\"}\nModule finished after a runtime of 00:00:17.1411124 with exit code -2\nModule failed due to negative exit code of -2\n\nRecord Ends at UTC 09\/22\/2016 06:44:44.\n<\/code><\/pre>\n\n<p>Editing the code to : <\/p>\n\n<pre><code>install.packages(\"src\/qdapTools.zip\",lib=\".\" , repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/qdapDictionaries.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/qdapRegex.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/RColorBrewer.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/qdap.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nlibrary(qdapTools, lib.loc=\".\", verbose=TRUE)\nlibrary(qdap, lib.loc=\".\", verbose=TRUE)\n<\/code><\/pre>\n\n<p>throws the following error :- <\/p>\n\n<pre><code>[ModuleOutput] 4: package 'qdapTools' was built under R version 3.3.1 \n[ModuleOutput] \n[ModuleOutput] End R Execution: 9\/22\/2016 7:11:05 AM\n[Stop]     DllModuleMethod::Execute. Duration = 00:00:17.0656414\n[Critical]     Error: Error 0063: The following error occurred during evaluation of R script:\n---------- Start of error message from R ----------\npackage or namespace load failed for 'qdapTools'\n\n\npackage or namespace load failed for 'qdapTools'\n----------- End of error message from R -----------\n<\/code><\/pre>\n\n<p>Not sure how to proceed, can someone help please.<\/p>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_time":1473835293153,
        "Question_score":10,
        "Question_tags":"r|azure-machine-learning-studio|qdap",
        "Question_view_count":443,
        "Owner_creation_time":1370288261667,
        "Owner_last_access_time":1663937917137,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":1353,
        "Owner_up_votes":80,
        "Owner_down_votes":7,
        "Owner_views":524,
        "Question_last_edit_time":1474530017472,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/39483984",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":69402391,
        "Question_title":"Merge distinct sklearn models into a single one",
        "Question_body":"<p>I have a dataset where, after exploring data, I detect some patron:<\/p>\n<ul>\n<li>The entire dataset have, imagine, 9 numerical variables, 1 dichotomous variable (take 'A' or 'B' value) and 1 numerical output<\/li>\n<li>The output is a cost (in \u20ac)<\/li>\n<li>I find a sklearn regression model that, when 'A', using 4 of 9 variables I can predict output with good performance.<\/li>\n<li>I find another sklearn regression model that, when 'B', using the last 5 variables, I can predict output with good performance.<\/li>\n<li>If I try to find a model which predict output with all the variables as input, encoding the dichotomous one with One-Hot-Encoder, the model has a bad performance.<\/li>\n<\/ul>\n<p>My goal is to implement a unique model in Azure Machine Learning, using a .joblib\/.pkl, but with this approach, I have two separated models with the same output (a cost) but different inputs, depending of dichotomous variable.<\/p>\n<p>Is there any way to merge the two models into a single one? So that with the 10 inputs, estimate a single output (internally discriminate options 'A' and 'B' to select the correct model and its inputs).<\/p>\n<p>Notice that using something like Voting Ensemble it's not valid because there are different inputs on each category (or I think it so)<\/p>\n<p>I accept another approach as a solution. Thank you.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633074956947,
        "Question_score":0,
        "Question_tags":"python|machine-learning|scikit-learn|categorical-data|azure-machine-learning-service",
        "Question_view_count":119,
        "Owner_creation_time":1611139028547,
        "Owner_last_access_time":1663993378950,
        "Owner_location":"Vigo, Espa\u00f1a",
        "Owner_reputation":39,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69402391",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":54287231,
        "Question_title":"Modifying loss function faster rcnn detectron",
        "Question_body":"<p>For my thesis I am trying to modify the loss function of faster-rcnn with regards to recognizing table structures.<\/p>\n\n<p>Currently I am using Facebooks Detectron. Seems to be working great but I am now actively trying to modify the loss function. Debugging my code I notice this is where the loss functions are added <a href=\"https:\/\/github.com\/facebookresearch\/Detectron\/blob\/master\/detectron\/modeling\/fast_rcnn_heads.py#L75\" rel=\"nofollow noreferrer\">fast_rcnn_heads.py:75<\/a>:<\/p>\n\n<pre><code>def add_fast_rcnn_losses(model):\n\"\"\"Add losses for RoI classification and bounding box regression.\"\"\"\ncls_prob, loss_cls = model.net.SoftmaxWithLoss(\n    ['cls_score', 'labels_int32'], ['cls_prob', 'loss_cls'],\n    scale=model.GetLossScale()\n)\nloss_bbox = model.net.SmoothL1Loss(\n    [\n        'bbox_pred', 'bbox_targets', 'bbox_inside_weights',\n        'bbox_outside_weights'\n    ],\n    'loss_bbox',\n    scale=model.GetLossScale()\n)\nloss_gradients = blob_utils.get_loss_gradients(model, [loss_cls, loss_bbox])\nmodel.Accuracy(['cls_prob', 'labels_int32'], 'accuracy_cls')\nmodel.AddLosses(['loss_cls', 'loss_bbox'])\nmodel.AddMetrics('accuracy_cls')\nreturn loss_gradients\n<\/code><\/pre>\n\n<p>The debugger cant find any declaration or implementation of mode.net.SmoothL1Loss nor SoftmaxWithLoss. Detectron uses caffe, and when I look in the net_builder (which inits the model.net) I see it makes \"binds\"(dont know the proper word) to caffe2, which on itself is a pylib with a compiled lib behind it. <\/p>\n\n<p>Am I looking in the wrong place to make a minor adjustment to this loss function, or will I really have to open de source from dcaffe, adjust the loss, recompile the lib? <\/p>\n\n<p>Greets, <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548064081110,
        "Question_score":2,
        "Question_tags":"machine-learning|computer-vision|image-recognition|azure-machine-learning-studio|caffe2",
        "Question_view_count":1281,
        "Owner_creation_time":1548063410383,
        "Owner_last_access_time":1548176015840,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54287231",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":35411741,
        "Question_title":"Azure ML: Getting Error 503: NoMoreResources to any web service API even when I only make 1 request",
        "Question_body":"<p>Getting the following response even when I make one request (concurrency set to 200) to a web service. <\/p>\n\n<p>{ status: 503, headers: '{\"content-length\":\"174\",\"content-type\":\"application\/json; charset=utf-8\",\"etag\":\"\\\"8ce068bf420a485c8096065ea3e4f436\\\"\",\"server\":\"Microsoft-HTTPAPI\/2.0\",\"x-ms-request-id\":\"d5c56cdd-644f-48ba-ba2b-6eb444975e4c\",\"date\":\"Mon, 15 Feb 2016 04:54:01 GMT\",\"connection\":\"close\"}',  body: '{\"error\":{\"code\":\"ServiceUnavailable\",\"message\":\"Service is temporarily unavailable.\",\"details\":[{\"code\":\"NoMoreResources\",\"message\":\"No resources available for request.\"}]}}' }<\/p>\n\n<p>The request-response web service is a recommender retraining web service with the training set containing close to 200k records. The training set is already present in my ML studio dataset, only 10-15 extra records are passed in the request. The same experiment was working flawlessly till 13th Feb 2016. I have already tried increasing the concurrency but still the same issue. I even reduced the size of the training set to 20 records, still didn't work.<\/p>\n\n<p>I have two web service both doing something similar and both aren't working since 13th Feb 2016. <\/p>\n\n<p>Finally, I created a really small experiment ( skill.csv --> split row ---> web output )   which doesn't take any input. It just has to return some part of the dataset. Did not work, response code 503.<\/p>\n\n<p>The logs I got are as follows<\/p>\n\n<p>{\n  \"version\": \"2014-10-01\",\n  \"diagnostics\": [{\n    .....\n    {\n      \"type\": \"GetResourceEndEvent\",\n      \"timestamp\": 13.1362,\n      \"resourceId\": \"5e2d653c2b214e4dad2927210af4a436.865467b9e7c5410e9ebe829abd0050cd.v1-default-111\",\n      \"status\": \"Failure\",\n      \"error\": \"The Uri for the target storage location is not specified. Please consider changing the request's location mode.\"\n    },\n    {\n      \"type\": \"InitializationSummary\",\n      \"time\": \"2016-02-15T04:46:18.3651714Z\",\n      \"status\": \"Failure\",\n      \"error\": \"The Uri for the target storage location is not specified. Please consider changing the request's location mode.\"\n    }\n  ]\n}<\/p>\n\n<p>What am I missing? Or am I doing it completely wrong?<\/p>\n\n<p>Thank you in advance.<\/p>\n\n<p>PS: Data is stored in mongoDB and then imported as CSV<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1455545889500,
        "Question_score":1,
        "Question_tags":"azure|cortana-intelligence|azure-machine-learning-studio",
        "Question_view_count":283,
        "Owner_creation_time":1425802890213,
        "Owner_last_access_time":1662088405513,
        "Owner_location":"Boston, MA, USA",
        "Owner_reputation":41,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Question_last_edit_time":1456850010663,
        "Answer_body":"<p>This was an Azure problem. I quote the Microsoft guy, <\/p>\n\n<blockquote>\n  <p>We believe we have isolated the issue impacting tour service and we are currently working on a fix. We will be able to deploy this in the next couple of days. The problem is impacting only the ASIA AzureML region at this time, so if this is an option for you, might I suggest using a workspace in either the US or EU region until the fix gets rolled out here.<\/p>\n<\/blockquote>\n\n<p>To view the complete discussion, click <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/985e253e-5e54-45a5-a359-5c501152c445\/getting-error-503-nomoreresources-to-any-web-service-api-even-when-i-only-make-1-request?forum=MachineLearning&amp;prof=required\" rel=\"nofollow\">here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1455597422632,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35411741",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70279636,
        "Question_title":"Azure Auto ML JobConfigurationMaxSizeExceeded error when using a cluster",
        "Question_body":"<p>I am running into the following error when I try to run Automated ML through the studio on a GPU compute cluster:<\/p>\n<p><img src=\"https:\/\/i.stack.imgur.com\/uLyxr.png\" alt=\"Azure ML error message\" \/><\/p>\n<blockquote>\n<p>Error: AzureMLCompute job failed. JobConfigurationMaxSizeExceeded: The\nspecified job configuration exceeds the max allowed size of 32768\ncharacters. Please reduce the size of the job's command line arguments\nand environment settings<\/p>\n<\/blockquote>\n<p>The attempted run is on a registered tabulated dataset in filestore and is a simple regression case. Strangely, it works just fine with the CPU compute instance I use for my other pipelines. I have been able to run it a few times using that and wanted to upgrade to a cluster only to be hit by this error. I found online that it could be a case of having the following setting: AZUREML_COMPUTE_USE_COMMON_RUNTIME:false; but I am not sure where to put this in when just running from the web studio.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1638986274657,
        "Question_score":5,
        "Question_tags":"azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":171,
        "Owner_creation_time":1592412555903,
        "Owner_last_access_time":1663862196937,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1641204588636,
        "Answer_body":"<p>It looks like the bug was fixed. I just ran it on a cluster without changing any of the parameters. Thank you Yutong for the help!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1639585163672,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70279636",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":35600907,
        "Question_title":"Azure ML Web Service request not working in C#",
        "Question_body":"<p>I have created an Azure ML Web service which outputs JSON response on request, and the structure of the sample request is as following:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"gender\",\n        \"age\",\n        \"income\"\n      ],\n      \"Values\": [\n        [\n          \"value\",\n          \"0\",\n          \"0\"\n        ],\n        [\n          \"value\",\n          \"0\",\n          \"0\"\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>And the input parameters are supposedly like this:<\/p>\n\n<p>gender  String<br>\nage Numeric<br>\nincome  Numeric     <\/p>\n\n<p>My Post method looks like this:<\/p>\n\n<pre><code>    [HttpPost]\n        public ActionResult GetPredictionFromWebService()\n        {\n            var gender = Request.Form[\"gender\"];\n            var age = Request.Form[\"age\"];\n\n\n            if (!string.IsNullOrEmpty(gender) &amp;&amp; !string.IsNullOrEmpty(age))\n            {\n                var resultResponse = _incomeWebService.InvokeRequestResponseService&lt;ResultOutcome&gt;(gender, age).Result;\n\n\n                if (resultResponse != null)\n                {\n                    var result = resultResponse.Results.Output1.Value.Values;\n                    PersonResult = new Person\n                    {\n                        Gender = result[0, 0],\n                        Age = Int32.Parse(result[0, 1]),\n                        Income = Int32.Parse(result[0, 2])\n                    };\n                }\n            }\n\n\n\n\n            return RedirectToAction(\"index\");\n        }\n<\/code><\/pre>\n\n<p>But for whatever reason; the Azure ML Webservice doesn\u2019t seem to respond anything to my request.\nDoes anyone know what the reason might be? I see no error or anything, just an empty response.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1456313293347,
        "Question_score":0,
        "Question_tags":"c#|asp.net|azure|azure-machine-learning-studio|cortana-intelligence",
        "Question_view_count":480,
        "Owner_creation_time":1456309738853,
        "Owner_last_access_time":1471868452243,
        "Owner_location":null,
        "Owner_reputation":39,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1456839908307,
        "Answer_body":"<p>The answer to your problem is that the \u201cNumeric\u201d datatype which is written in the input parameters in Azure ML is in fact a float and not an integer for your income measure. So when trying to request a response from Azure ML, you are not providing it the \u201cadequate\u201d information needed in the right format for it to respond correctly, resulting in it not giving you any response.<\/p>\n\n<p>I believe your model would look something similar to this based on your input parameters:<\/p>\n\n<pre><code>public class Person\n    {\n        public string Gender { get; set; }\n        public int Age { get; set; }\n        public int Income { get; set; }\n\n\n        public override string ToString()\n        {\n            return Gender + \",\" + Age + \",\" + Income;\n        }\n    }\n<\/code><\/pre>\n\n<p>You would have to change your Income datatype into float like so:<\/p>\n\n<pre><code>public class Person\n{\n    public string Gender { get; set; }\n    public int Age { get; set; }\n    public float Income { get; set; }\n\n    public override string ToString()\n    {\n        return Gender + \",\" + Age + \",\" + Income;\n    }\n}\n<\/code><\/pre>\n\n<p>And then your post-method would look something like this:<\/p>\n\n<pre><code>    [HttpPost]\n    public ActionResult GetPredictionFromWebService()\n    {\n        var gender = Request.Form[\"gender\"];\n        var age = Request.Form[\"age\"];\n\n        if (!string.IsNullOrEmpty(gender) &amp;&amp; !string.IsNullOrEmpty(age))\n        {\n            var resultResponse = _incomeWebService.InvokeRequestResponseService&lt;ResultOutcome&gt;(gender, age).Result;\n\n                if (resultResponse != null)\n                {\n                    var result = resultResponse.Results.Output1.Value.Values;\n                    PersonResult = new Person\n                    {\n                        Gender = result[0, 0],\n                        Age = Int32.Parse(result[0, 1]),\n                        Income = float.Parse(result[0, 3], CultureInfo.InvariantCulture.NumberFormat)\n                };\n            }\n        }\n\n        ViewBag.myData = PersonResult.Income.ToString();\n        return View(\"Index\");\n    }\n<\/code><\/pre>\n\n<p>The key here is simply:<\/p>\n\n<pre><code>Income = float.Parse(result[0, 3], CultureInfo.InvariantCulture.NumberFormat)\n<\/code><\/pre>\n\n<p>Rather than your legacy <\/p>\n\n<pre><code>Income = Int32.Parse(result[0, 2])\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1456314151449,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35600907",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":53891907,
        "Question_title":"Pandas dataframe and apply - Can't figure out why resulting values are negative",
        "Question_body":"<p>Here's a picture of my data, the column of interest RUL is on the far right the names got cut off (I'm using the Turbo Engine Degradation dataset from NASA) can be found here: <a href=\"https:\/\/data.nasa.gov\/widgets\/vrks-gjie\" rel=\"nofollow noreferrer\">https:\/\/data.nasa.gov\/widgets\/vrks-gjie<\/a><\/p>\n\n<p>I'm doing this in Azure ML Studio but code snippet below, I have 2 helper functions get_engine_last_cycle (which when I unit test it seems to do as expected - compute the last cycle for that engine, for example engine 2 has a max cycle in this dataset of 287 when it fails). The final helper function I call get_engine_remainig_life, takes the engine and cycle as arguments and returns the max cycle - current cycle for that engine (again I've unit tested this and it seems to give me expected results).<\/p>\n\n<p>For some reason this isn't working when I run my notebook. The column which I call \"RUL\" should return a sequence of decreasing, positive integers for example 287, 286, 285 284, etc for engine #2. However, it's giving me negative values. I can't seem to figure out why but know the problem is likely with this one piece of code<\/p>\n\n<pre><code> df['RUL'] = df[['engine', 'cycle']].apply(lambda x: get_engine_remaining_life(*x), axis=1)\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/8IYOx.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8IYOx.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<pre><code>    def get_engine_last_cycle(engine):\n        return int(df.loc[engine, ['cycle']].max())\n\n\n    def get_engine_remaining_life(engine, cycle):\n        return get_engine_last_cycle(engine) - int(cycle)\n\n    df['RUL'] = df[['engine', 'cycle']].apply(lambda x: get_engine_remaining_life(*x), axis=1)\n\n    return df\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1545435517580,
        "Question_score":0,
        "Question_tags":"pandas|azure-machine-learning-studio",
        "Question_view_count":66,
        "Owner_creation_time":1442451471347,
        "Owner_last_access_time":1591385407297,
        "Owner_location":null,
        "Owner_reputation":338,
        "Owner_up_votes":49,
        "Owner_down_votes":1,
        "Owner_views":79,
        "Question_last_edit_time":1545438768860,
        "Answer_body":"<p>Just for the sake of trying, this is how I'd implement this. Maybe it will help you.<\/p>\n\n<pre><code>df['RUL'] = df.loc[:, ['engine', 'cycle']].groupby('engine').transform('max')\ndf['RUL'] = df['RUL'] - df['cycle']\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1545439231323,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53891907",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":48087407,
        "Question_title":"How to deal with missing values in Azure Machine Learning Studio",
        "Question_body":"<p>Looks like I have 672 mission values, according to statistics. \nThere are NULL value in QuotedPremium column.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/p9Z3X.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/p9Z3X.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I implemented Clean Missing Data module where it should substitute missing values with 0, but for some reason I'm still seeing NULL values as QuotedPremium, but...it says that missing values are = 0<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/PDg97.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PDg97.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/BEdHD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BEdHD.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Here you see it tells me that missing values = 0, but there are still NULLs <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WKlGZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WKlGZ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>So what really happened after I ran Clean Missing Data module? Why it ran succesfully but there are still NULL values, even though it tells that number of missing values are 0. <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1515028540763,
        "Question_score":0,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio|ml-studio",
        "Question_view_count":1556,
        "Owner_creation_time":1457596845393,
        "Owner_last_access_time":1663977598457,
        "Owner_location":"San Diego, CA, United States",
        "Owner_reputation":4046,
        "Owner_up_votes":505,
        "Owner_down_votes":7,
        "Owner_views":825,
        "Question_last_edit_time":null,
        "Answer_body":"<p><code>NULL<\/code> is indeed a value; entries containing NULLs are <em>not<\/em> missing, hence they are neither cleaned with the 'Clean Missing Data' operator nor reported as missing.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1515030180700,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48087407",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":47052996,
        "Question_title":"how to upload multiple files to azure blob storage using azure machine Learning",
        "Question_body":"<p>I have come across a requirement wherein the first part involves reading a blob file (i.e. .csv) present in Azure blob storage and splitting the file data into multiple files, based on the distinct combination of few columns. The second part of the requirement involves writing\/uploading the multiple files to Azure blob Storage at a separate destination folder. <\/p>\n\n<p>I am able to split the blob file into multiple files, but am not able to write\/upload the partitioned files on to azure blob storage. Is there any possibility to write the files to blob storage. Any help will be highly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1509532865763,
        "Question_score":0,
        "Question_tags":"file|azure-blob-storage|azure-machine-learning-studio",
        "Question_view_count":1317,
        "Owner_creation_time":1509532022647,
        "Owner_last_access_time":1510059988850,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1653556441369,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47052996",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73193412,
        "Question_title":"Azure Machine Learning Compute Instance Not Creating using Azure CLI and Azure DevOps Pipeline",
        "Question_body":"<p>I create my Azure Machine Learning Workspace using Azure CLI:<\/p>\n<pre><code>$env=&quot;prd&quot;\n$instance=&quot;001&quot;\n$location=&quot;uksouth&quot;\n$suffix=&quot;predict-$env-$location-$instance&quot;\n\n$rg=&quot;rg-$suffix&quot;\n$ws=&quot;mlw-$suffix&quot;\n$computeinstance=&quot;vm-$suffix&quot;.Replace('-','')\n$computeinstance\naz group create --name $rg --location $location\naz configure --defaults group=$rg\naz ml workspace create --name $ws\naz configure --defaults workspace=$ws\naz ml compute create --name $computeinstance --size Standard_DS11_v2 --type ComputeInstance\n<\/code><\/pre>\n<p>I run the above code manually in Visual Studio Code, and everything works properly.<\/p>\n<p>However, when I integrate the above into an Azure DevOps pipeline via the YAML:<\/p>\n<pre><code>steps:\n - bash: az extension add -n ml\n    displayName: 'Install Azure ml extension'\n - task: AzureCLI@2\n    inputs:\n      azureSubscription: &quot;$(AZURE_RM_SVC_CONNECTION)&quot;\n      scriptType: 'ps'\n      scriptLocation: 'scriptPath'\n      scriptPath: '.\/environment_setup\/aml-cli.ps1'\n<\/code><\/pre>\n<ul>\n<li>The pipeline creates the Azure Machine Learning workspace as expected.<\/li>\n<li>The pipeline creates the compute instance, which has the status <strong>&quot;Running&quot;<\/strong> and green status.<\/li>\n<li>However, the compute instance has all applications <strong>greyed out<\/strong>. This means I cannot connect to the compute instance using a terminal, notebook or otherwise, essentially making it useless. The application links in the following screenshot are not clickable:<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/OUzpx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OUzpx.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I attempted:<\/p>\n<ul>\n<li>Specifying brand new resource names.<\/li>\n<li>Creating the workspace and compute in separate pipelines in case of a timing issue.<\/li>\n<li>Deleting the resource group first using:<\/li>\n<\/ul>\n<pre><code>az group delete -n rg-predict-prd-uksouth-001 --force-deletion-types Microsoft.Compute\/virtualMachines --yes\n<\/code><\/pre>\n<p>All to no avail.<\/p>\n<p>How do I create a useable Azure Machine Learning compute instance using Azure CLI and Azure DevOps pipelines?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659355627063,
        "Question_score":0,
        "Question_tags":"yaml|continuous-integration|azure-pipelines|azure-machine-learning-service",
        "Question_view_count":105,
        "Owner_creation_time":1220819112600,
        "Owner_last_access_time":1662642319957,
        "Owner_location":"Wales, United Kingdom",
        "Owner_reputation":1462,
        "Owner_up_votes":65,
        "Owner_down_votes":6,
        "Owner_views":285,
        "Question_last_edit_time":1659356245776,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73193412",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":48043434,
        "Question_title":"Azure Machine Learning Studio Conditional Training Data",
        "Question_body":"<p>I have built an <strong>Microsoft Azure ML Studio<\/strong> workspace predictive web service, and have a scernario where I need to be able to run the service with different training datasets. <\/p>\n\n<p>I know I can setup multiple web services via Azure ML, each with a different training set attached, but I am trying to find a way to do it all within the same workspace and passing a <code>Web Input Parameter<\/code> as the input value to choose which training set to use.<\/p>\n\n<p>I have found <a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2017\/06\/19\/loading-a-trained-model-dynamically-in-an-azure-ml-web-service\/\" rel=\"nofollow noreferrer\">this<\/a> article, which describes <em>almost<\/em> my scenario. However, this article relies on the training dataset that is being pulled from the <code>Load Trained Data<\/code> module, as having a static endpoint (or blob storage location). I don't see any way to dynamically (or conditionally) change this location based on a <code>Web Input Parameter<\/code>. <\/p>\n\n<p><strong>Basically, does Azure ML support a \"conditional training data\" loading?<\/strong><\/p>\n\n<p>Or, might there be a way to combine training datasets, then filter based on the passed <code>Web Input Parameter<\/code>?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1514731984430,
        "Question_score":0,
        "Question_tags":"machine-learning|conditional|azure-machine-learning-studio",
        "Question_view_count":252,
        "Owner_creation_time":1269906749850,
        "Owner_last_access_time":1663614060847,
        "Owner_location":"New York",
        "Owner_reputation":11705,
        "Owner_up_votes":138,
        "Owner_down_votes":5,
        "Owner_views":794,
        "Question_last_edit_time":1514740425440,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48043434",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":65586756,
        "Question_title":"How connect Azure Function with my own model? It is possible to use Azure Storage?",
        "Question_body":"<h3>Intro<\/h3>\n<p>I created my own model locally and then register it and deploy it to azure and it works.<\/p>\n<h3>deployed model output:<\/h3>\n<p><a href=\"https:\/\/i.stack.imgur.com\/gaLCH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gaLCH.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h3>my approach<\/h3>\n<p>I used <a href=\"https:\/\/medium.com\/microsoftazure\/deploying-azure-machine-learning-containers-41bcb02a4e1b\" rel=\"nofollow noreferrer\">this tutorial<\/a>, and I want use my model in Azure Function and I can do it:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def main(req: func.HttpRequest, msg: func.Out[func.QueueMessage]) -&gt; str:\n    name = req.params.get('name')\n    scoring_uri = 'http:\/\/1f72b1bf-5ca9-42d9-bedd-f41773591a4f.francecentral.azurecontainer.io\/score'\n    headers = {'Content-Type':'application\/json'}\n    test_data = json.dumps({'text': 'Today is a great day!'})\n    response = requests.post(scoring_uri, data=test_data, headers=headers)\n\nif not name:\n    try:\n        req_body = req.get_json()\n    except ValueError:\n        pass\n    else:\n        name = req_body.get('name')\n\nif name:\n    msg.set(name)\n    return func.HttpResponse(f&quot;Hello {name}! Najlepszy wynik: {response.json()}&quot;)\nelse:\n    return func.HttpResponse(\n        &quot;Please pass a name on the query string or in the request body&quot;,\n        status_code=400\n    )\n<\/code><\/pre>\n<h3>questions<\/h3>\n<ol>\n<li>Is my usage correct?<\/li>\n<li>Is it possible to use azure storage for model storage and how to do it?<\/li>\n<li>Is there any other way to use the model in Azure Function?<\/li>\n<\/ol>\n<p>I am wondering because I had specified in the requirements that I should use azure functions and azure storage. I don't understand why.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1609881766290,
        "Question_score":0,
        "Question_tags":"azure|azure-functions|azure-storage|azure-machine-learning-service",
        "Question_view_count":210,
        "Owner_creation_time":1605834001337,
        "Owner_last_access_time":1618085210227,
        "Owner_location":null,
        "Owner_reputation":73,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":1609945294496,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65586756",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71669344,
        "Question_title":"AzureML webservice deployment with custom Environment - \/var\/runit does not exist",
        "Question_body":"<p>I'm struggling to deploy a model with a custom environment through the azureml SDK.<\/p>\n<p>I have built a docker image locally and pushed it to azure container registry to use it for environment instantiating. This is how my dockerfile looks like:<\/p>\n<pre><code>FROM mcr.microsoft.com\/azureml\/openmpi3.1.2-ubuntu18.04\nFROM python:3.9.12\n        \n# Keeps Python from generating .pyc files in the container\nENV PYTHONDONTWRITEBYTECODE=1\n        \n# Turns off buffering for easier container logging\nENV PYTHONUNBUFFERED=1\n        \n# Install requirement for deploying the service\nRUN apt-get update\nRUN apt-get install -y runit\n        \n# Install pip requirements\nRUN pip install --upgrade pip\nCOPY requirements.txt .\nRUN pip install azureml-defaults\nRUN pip install -r requirements.txt\n<\/code><\/pre>\n<p>I want to deploy the webservice locally for testing, so I am following the steps according to official documentation:<\/p>\n<pre><code>ws = Workspace(\n    subscription_id='mysub_id', \n    resource_group='myresource_group', \n    workspace_name='myworkspace'\n)\n        \nmodel = Model.register(\n    ws, \n    model_name='mymodel', \n    model_path='.\/Azure_Deployment\/mymodel_path'\n)\n        \ncontainer = ContainerRegistry()\ncontainer.address = 'myaddress'\nmyenv = Environment.from_docker_image('myenv_name', 'img\/img_name:v1', container)\n        \ninference_config = InferenceConfig(\n    environment=myenv, \n    source_directory='.\/Azure_Deployment', \n    entry_script='echo_score.py',\n)\n        \ndeployment_config = LocalWebservice.deploy_configuration(port=6789)\n        \nservice = Model.deploy(\n    ws, \n    &quot;myservice&quot;, \n    [model], \n    inference_config, \n    deployment_config, \n    overwrite=True,\n)\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n<p>This is what I get from the logs:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/GLafe.png\" rel=\"nofollow noreferrer\">service container logs<\/a><\/p>\n<p>Checking into the resulting container for the service I can see indeed there is no \/runit folder inside \/var. There is also no other folders created for the service besides the azureml-app containing my model's files.<\/p>\n<p>I would really appreciate any insights to what's going on here as I have no clue at this point.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1648591923567,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":156,
        "Owner_creation_time":1648585295487,
        "Owner_last_access_time":1650433060933,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71669344",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":40844351,
        "Question_title":"R code on Azure Machine Learning is slow compared to local execution time",
        "Question_body":"<p>Getting straight to it:\nWhy is my R code doing fine on my local CPU (under one minute), but tens of times slower on Azure Machine Learning, using one R script block (over 18 minutes)?<\/p>\n\n<p>I assume that it has to do with the resources allocated to the experiment, but how can I be sure? Can I obtain details about the resource allocated to the R script block from somwehere hidden in the Azure-ML Studio machinery?<\/p>\n\n<p>Thank you, Flo<\/p>\n\n<p>Later Edit:\nAs it often happens, I did finally find some information, which still doesn't solve my issue. According to <a href=\"https:\/\/msdn.microsoft.com\/library\/en-us\/Dn905952.aspx#Technical%20Notes\" rel=\"nofollow noreferrer\">https:\/\/msdn.microsoft.com\/library\/en-us\/Dn905952.aspx#Technical%20Notes<\/a> \"User-specified R code is run by a 64-bit R interpreter that runs in Azure using an A8 virtual machine with 56 GB of RAM.\"<\/p>\n\n<p>This is more than my local machine has, the R code is still much slower on the Azure-ML studio.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1480336059453,
        "Question_score":0,
        "Question_tags":"r|azure-machine-learning-studio",
        "Question_view_count":733,
        "Owner_creation_time":1459503032960,
        "Owner_last_access_time":1512467737783,
        "Owner_location":"Vienna, Austria",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":1480338194649,
        "Answer_body":"<p>Consider using rbenchmark or other benchmarking tools to get an idea of the runtime and complexity of your code. In general for loops tend to be slow.<\/p>\n\n<p>It's very possible that the server has less resources available (ram, cpu) or that you have to wait in a que before you get served. Without any more code it's hard to comment further on this issue.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1480339657187,
        "Answer_score":0.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40844351",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71562804,
        "Question_title":"Automl object detection local filesystem",
        "Question_body":"<p>I have tried to create a azure automl model to find an object in the image.<\/p>\n<p>According to the tutorial it is required that you specify the labels in adataframe where on of the columns are mounted to a aml datastore.<\/p>\n<p>Question: Is it possible to link it to a local repository instead eg in a compute?<\/p>\n<p>Link:  <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-auto-train-image-models\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-auto-train-image-models<\/a><\/p>\n<p>I tried to use os path but it did not work.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1647889254553,
        "Question_score":0,
        "Question_tags":"azure|object-detection|azure-machine-learning-service|azure-auto-ml",
        "Question_view_count":21,
        "Owner_creation_time":1526222844170,
        "Owner_last_access_time":1655035727330,
        "Owner_location":"Pakis, Indonesia",
        "Owner_reputation":101,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71562804",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60160773,
        "Question_title":"Workaround for timeout error in Dataset.download()",
        "Question_body":"<p>azureml-sdk version: <code>1.0.85<\/code><\/p>\n\n<p>Calling below (as given in the Dataset UI), I get this<\/p>\n\n<pre><code>ds_split = Dataset.get_by_name(workspace, name='ret- holdout-split')\nds_split.download(target_path=dir_outputs, overwrite=True)\n<\/code><\/pre>\n\n<pre><code>UnexpectedError:\n{'errorCode': 'Microsoft.DataPrep.ErrorCodes.Unknown', 'message':\n    'The client could not finish the operation within specified timeout.',\n    'errorData': {}}\n<\/code><\/pre>\n\n<p>The <code>FileDataset<\/code> 1GB pickled file stored in blob.\n<a href=\"https:\/\/gist.github.com\/swanderz\/c608ced5f2c6a2802b7553bc9ead0762\" rel=\"nofollow noreferrer\">Here's a gist with the full traceback<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1581384771000,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":217,
        "Owner_creation_time":1405457120427,
        "Owner_last_access_time":1663947733100,
        "Owner_location":"Seattle, WA, USA",
        "Owner_reputation":3359,
        "Owner_up_votes":1187,
        "Owner_down_votes":14,
        "Owner_views":555,
        "Question_last_edit_time":1581436827487,
        "Answer_body":"<p>Tried again this AM and it worked. let's file this under \"transient error\"<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1581436892352,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60160773",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":54757598,
        "Question_title":"Add model description when registering model after hyperdrive successful run",
        "Question_body":"<p>I have successfully trained a model on Azure Machine Learning Service using Hyperdrive that has now yielded a hyperdrive run instance<\/p>\n\n<pre><code>hyperdrive_run = exp.submit(config=hypertune_config)\nhyperdrive_run\nbest_run = hyperdrive_run.get_best_run_by_primary_metric()\n<\/code><\/pre>\n\n<p>As a next step, I would like to register a model while adding a description to the model.:<\/p>\n\n<pre><code>pumps_rf = best_run.register_model(model_name='pumps_rf', model_path='outputs\/rf.pkl')\n<\/code><\/pre>\n\n<p>There is a <code>description<\/code> column in the Models section of my AML Workspace on Azure portal but the <code>register_model<\/code> method does not seem to have a <code>description<\/code> flag. So how do I go about adding a description to the model so I see it in Azure Portal?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1550539380057,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":453,
        "Owner_creation_time":1408574571227,
        "Owner_last_access_time":1644544922143,
        "Owner_location":"Toronto, Canada",
        "Owner_reputation":2754,
        "Owner_up_votes":189,
        "Owner_down_votes":1,
        "Owner_views":124,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Good question :).<\/p>\n\n<p>Looking at the current version of the API, it doesn't look like you can add the description using <code>Run.register_model<\/code>, as confirmed <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#register-model-model-name--model-path-none--tags-none--properties-none----kwargs-\" rel=\"nofollow noreferrer\">by the docs<\/a>. <\/p>\n\n<p>You can go around this however by registering the model using the <code>Model.register<\/code> method which, fortunately, includes an argument for <code>description<\/code> as detailed <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none-\" rel=\"nofollow noreferrer\">here<\/a>. In your case, you also need to <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run(class)?view=azure-ml-py#download-file-name--output-file-path-none-\" rel=\"nofollow noreferrer\">download the files<\/a> first.<\/p>\n\n<p>In short, use something like:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>best_run.download_file('outputs\/rf.pkl', output_file_path='.\/rf.pkl')\n\nModel.register(workspace=ws, model_path='.\/rf.pkl', model_name=\"pumps_rf\", description=\"There are many models like it, but this one is mine.\")\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1550668517447,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1550686459150,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54757598",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":61644716,
        "Question_title":"MlflowException: API request to ...URL... failed to return code 200 after 3 tries",
        "Question_body":"<p>I am currently trying to track my machine learning model metrics using the MLFlow API in Azure Databricks.<\/p>\n\n<p>I registered the experiment under my team's machine learning workspace and had tried a few metric log commands that worked but were simply used as a test.<\/p>\n\n<p>My notebook ran a for loop logging metrics per calculation within the loop.\nIt took a while (3-5 seconds) before sending out the error.<\/p>\n\n<p>I tried to look at the experiment metrics and it seems to have logged a bit of the for loop's metrics before crashing.<\/p>\n\n<p>Not sure as to why it does it and now it throws the exception to my earlier test calls to log metrics.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1588796738017,
        "Question_score":1,
        "Question_tags":"azure-databricks|mlflow|azure-machine-learning-service",
        "Question_view_count":696,
        "Owner_creation_time":1571508734813,
        "Owner_last_access_time":1597680718507,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61644716",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":56418684,
        "Question_title":"Possible to access the internal representation of a neural network trained in Azure Machine Learning Service or Azure Machine Learning Studio?",
        "Question_body":"<p>I'm working with data scientists who would like to gain insight and understanding of the neural network models that they train using the visual interfaces in Azure Machine Learning Studio\/Service. Is it possible to dump out and inspect the internal representation of a neural network model? Is there a way that I could write code that accesses the nodes and weights of a trained neural network in order to visualize the network as a graph structure? Or if Azure Machine Learning Studio\/Service doesn't support this I'd appreciate advice on a different machine learning framework that might be more appropriate for this kind of analysis.<\/p>\n\n<p>Things I have tried:<\/p>\n\n<ul>\n<li>Train Model outputs an ILearnerDotNet (AML Studio) or Model (AML Service). I looked for items to drag into the workspace where I could write custom code such as Execute Python Script. They seem to accept datasets, but not ILearnerDotNet\/Model as input.<\/li>\n<li>I wasn't able to locate documentation about the ILearnerDotNet\/Model interfaces.<\/li>\n<li>Selecting the Train Model output offers the option to Save as Trained Model. This creates a trained model object and that would help me reference the trained model in other places, but I didn't find a way to use this to get at its internals.<\/li>\n<\/ul>\n\n<p>I'm new to the Azure Machine Learning landscape, and could use some help with how to get started on how to access this data.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559506858340,
        "Question_score":2,
        "Question_tags":"neural-network|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":167,
        "Owner_creation_time":1427643193810,
        "Owner_last_access_time":1663710082517,
        "Owner_location":null,
        "Owner_reputation":45,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Quote from Azure ML Exam reference:<\/p>\n\n<blockquote>\n  <p>By default, the architecture of neural networks is limited to a single\n  hidden layer with sigmoid as the activation function and softmax in\n  the last layer. You can change this in the properties of the model,\n  opening the Hidden layer specification dropdown list, and selecting a\n  Custom definition script. A text box will appear in which you will be\n  able to insert a Net# script. This script language allows you to\n  define neural networks architectures.<\/p>\n<\/blockquote>\n\n<p>For instance, if you want to create a two layer network, you may put the following code.<\/p>\n\n<pre><code>input Picture [28, 28];\nhidden H1 [200] from Picture all;\nhidden H2 [200] from H1 all;\noutput Result [10] softmax from H2 all;\n<\/code><\/pre>\n\n<p>Nevertheless, with Net# you will face certain limitations as, it does not accept regularization (neither L2 nor dropout). Also, there is no ReLU activation that are\ncommonly used in deep learning due to their benefits in backpropagation. You cannot modify the batch size of the Stochastic Gradient Descent (SGD). Besides that, you cannot use other optimization algorithms. You can use SGD with momentum, but not others like Adam, or RMSprop. You cannot define recurrent or recursive neural networks.<\/p>\n\n<p>Another great tool is CNTK (Cognitive Toolkit) that allows you defining your computational graph and create a fully customizable model.\nQuote from documentation<\/p>\n\n<blockquote>\n  <p>It is a Microsoft open source deep learning toolkit. Like other deep\n  learning tools, CNTK is based on the construction of computational\n  graphs and their optimization using automatic differentiation. The\n  toolkit is highly optimized and scales efficiently (from CPU, to GPU,\n  to multiple machines). CNTK is also very portable and flexible; you\n  can use it with programming languages like Python, C#, or C++, but you\n  can also use a model description language called BrainScript.<\/p>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1559896269912,
        "Answer_score":0.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56418684",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":40028165,
        "Question_title":"Azure ML's web service asking for label?",
        "Question_body":"<p>I built a linear regression algorithm in Azure ML. On the &quot;Score Model&quot; module I can actually see the predictions and the rest of the features. However, when I deploy this project as a web service, the service is expecting the actual label of the data (e.g. I'm trying to predict a house's price and it asks me for the price of the house to make the prediction), which doesn't make any sense to me... What am I doing wrong? On the &quot;Train Model&quot; module I set that the label column is the HousePrice, which is what I'm trying to predict...<\/p>\n<p>This is my model:\n<a href=\"https:\/\/i.stack.imgur.com\/kI8qu.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kI8qu.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I tried leaving that field blank but the prediction returns null...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1476382562787,
        "Question_score":4,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":1012,
        "Owner_creation_time":1401729936860,
        "Owner_last_access_time":1662997148480,
        "Owner_location":null,
        "Owner_reputation":1102,
        "Owner_up_votes":390,
        "Owner_down_votes":25,
        "Owner_views":120,
        "Question_last_edit_time":1592644375060,
        "Answer_body":"<p>The input schema (names\/types of required input) based on the location in the graph where you attach the \"Web Service Input\" module. To get the schema you want, you will need to find -- or if necessary, create -- a place in the experiment where the data has the column names\/types you desire.<\/p>\n\n<p>Consider this simple example experiment that predicts whether a field called \"income\" will be above or below $50k\/year:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/nWaN2.png\" rel=\"nofollow\"><img src=\"https:\/\/i.stack.imgur.com\/nWaN2.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When we click \"Set up web service\", the following graph is automatically generated:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/NMMpV.png\" rel=\"nofollow\"><img src=\"https:\/\/i.stack.imgur.com\/NMMpV.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Since the input dataset and \"Web service input\" modules are connected to the same port, the web service schema will perfectly match the schema of the input dataset. This is unfortunate because the input dataset contains a column called \"income\", which is what our web service is supposed to predict -- this is equivalent to the problem that you are having.<\/p>\n\n<p>To get around it, we need to create a place in our experiment graph where we've dropped the unneeded \"income\" field from the input dataset, and attach the \"Web service input\" module there:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WPeSB.png\" rel=\"nofollow\"><img src=\"https:\/\/i.stack.imgur.com\/WPeSB.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>With this arrangement, the web service only requests the features actually needed to score the model. I'm sure you can use a similar method to create a predictive experiment with whatever input schema you need for your own work.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1476730527012,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40028165",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":59309318,
        "Question_title":"Azure ML Studio Error 0035: Features for The vocabulary is empty",
        "Question_body":"<p>I am attempting to give classifications to various bodies of text using Azure ML Studio and I have my successful output all the way until I deploy and test a web service. Once I deploy my web service and attempt to test it I get the following error:<\/p>\n\n<p>Error 0035: Features for The vocabulary is empty. Please check the Minimum n-gram document frequency. required but not provided., Error code: ModuleExecutionError, Http status code: 400<\/p>\n\n<p>The vocabularies for the extract n-gram modules are not empty. The only aspect that changes from the working model to the Web service error is the web service input.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/PBsDS.png\" rel=\"nofollow noreferrer\">Training Model<\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/X6Sq1.png\" rel=\"nofollow noreferrer\">Predictive Model<\/a><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1576169464783,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":368,
        "Owner_creation_time":1573072787957,
        "Owner_last_access_time":1626121970557,
        "Owner_location":null,
        "Owner_reputation":19,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59309318",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70833499,
        "Question_title":"AzureML Environment for Inference : can't add pip packages to dependencies",
        "Question_body":"<p>I can't find the proper way to add dependencies to my Azure Container Instance for ML Inference.<\/p>\n<p>I basically started by following this tutorial : <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-train-deploy-notebook\" rel=\"nofollow noreferrer\">Train and deploy an image classification model with an example Jupyter Notebook<\/a><\/p>\n<p>It works fine.<\/p>\n<p>Now I want to deploy my trained TensorFlow model for inference. I tried many ways, but I was never able to add python dependencies to the Environment.<\/p>\n<h1>From the TensorFlow curated environment<\/h1>\n<p>Using <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/resource-curated-environments#inference-curated-environments-and-prebuilt-docker-images\" rel=\"nofollow noreferrer\">AzureML-tensorflow-2.4-ubuntu18.04-py37-cpu-inference<\/a> :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace\n\n\n# connect to your workspace\nws = Workspace.from_config()\n\n# names\nexperiment_name = &quot;my-experiment&quot;\nmodel_name = &quot;my-model&quot;\nenv_version=&quot;1&quot;\nenv_name=&quot;my-env-&quot;+env_version\nservice_name = str.lower(model_name + &quot;-service-&quot; + env_version)\n\n\n# create environment for the deploy\nfrom azureml.core.environment import Environment, DEFAULT_CPU_IMAGE\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.webservice import AciWebservice\n\n# get a curated environment\nenv = Environment.get(\n    workspace=ws, \n    name=&quot;AzureML-tensorflow-2.4-ubuntu18.04-py37-cpu-inference&quot;,\n# )\ncustom_env = env.clone(env_name)\ncustom_env.inferencing_stack_version='latest'\n\n# add packages\nconda_dep = CondaDependencies()\npython_packages = ['joblib', 'numpy', 'os', 'json', 'tensorflow']\nfor package in python_packages:\n    conda_dep.add_pip_package(package)\n    conda_dep.add_conda_package(package)\n\n# Adds dependencies to PythonSection of env\ncustom_env.python.user_managed_dependencies=True\ncustom_env.python.conda_dependencies=conda_dep\n\ncustom_env.register(workspace=ws)\n\n# create deployment config i.e. compute resources\naciconfig = AciWebservice.deploy_configuration(\n    cpu_cores=1,\n    memory_gb=1,\n    tags={&quot;experiment&quot;: experiment_name, &quot;model&quot;: model_name},\n)\n\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.model import Model\n\n# get the registered model\nmodel = Model(ws, model_name)\n\n# create an inference config i.e. the scoring script and environment\ninference_config = InferenceConfig(entry_script=&quot;score.py&quot;, environment=custom_env)\n\n# deploy the service\nservice = Model.deploy(\n    workspace=ws,\n    name=service_name,\n    models=[model],\n    inference_config=inference_config,\n    deployment_config=aciconfig,\n)\n\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n<p>I get the following log :<\/p>\n<pre><code>\nAzureML image information: tensorflow-2.4-ubuntu18.04-py37-cpu-inference:20220110.v1\n\n\nPATH environment variable: \/opt\/miniconda\/envs\/amlenv\/bin:\/opt\/miniconda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\nPYTHONPATH environment variable: \n\nPip Dependencies\n---------------\nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n2022-01-24T10:21:09,855130300+00:00 - iot-server\/finish 1 0\n2022-01-24T10:21:09,856870100+00:00 - Exit code 1 is normal. Not restarting iot-server.\nabsl-py==0.15.0\napplicationinsights==0.11.10\nastunparse==1.6.3\nazureml-inference-server-http==0.4.2\ncachetools==4.2.4\ncertifi==2021.10.8\ncharset-normalizer==2.0.10\nclick==8.0.3\nFlask==1.0.3\nflatbuffers==1.12\ngast==0.3.3\ngoogle-auth==2.3.3\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngrpcio==1.32.0\ngunicorn==20.1.0\nh5py==2.10.0\nidna==3.3\nimportlib-metadata==4.10.0\ninference-schema==1.3.0\nitsdangerous==2.0.1\nJinja2==3.0.3\nKeras-Preprocessing==1.1.2\nMarkdown==3.3.6\nMarkupSafe==2.0.1\nnumpy==1.19.5\noauthlib==3.1.1\nopt-einsum==3.3.0\npandas==1.1.5\nprotobuf==3.19.1\npyasn1==0.4.8\npyasn1-modules==0.2.8\npython-dateutil==2.8.2\npytz==2021.3\nrequests==2.27.1\nrequests-oauthlib==1.3.0\nrsa==4.8\nsix==1.15.0\ntensorboard==2.7.0\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.4.0\ntensorflow-estimator==2.4.0\ntermcolor==1.1.0\ntyping-extensions==3.7.4.3\nurllib3==1.26.8\nWerkzeug==2.0.2\nwrapt==1.12.1\nzipp==3.7.0\n\n\nEntry script directory: \/var\/azureml-app\/.\n\nDynamic Python package installation is disabled.\nStarting AzureML Inference Server HTTP.\n\nAzure ML Inferencing HTTP server v0.4.2\n\n\nServer Settings\n---------------\nEntry Script Name: score.py\nModel Directory: \/var\/azureml-app\/azureml-models\/my-model\/1\nWorker Count: 1\nWorker Timeout (seconds): 300\nServer Port: 31311\nApplication Insights Enabled: false\nApplication Insights Key: None\n\n\nServer Routes\n---------------\nLiveness Probe: GET   127.0.0.1:31311\/\nScore:          POST  127.0.0.1:31311\/score\n\nStarting gunicorn 20.1.0\nListening at: http:\/\/0.0.0.0:31311 (69)\nUsing worker: sync\nBooting worker with pid: 100\nException in worker process\nTraceback (most recent call last):\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker\n    worker.init_process()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process\n    self.load_wsgi()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load\n    return self.load_wsgiapp()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 359, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/entry.py&quot;, line 1, in &lt;module&gt;\n    import create_app\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/create_app.py&quot;, line 4, in &lt;module&gt;\n    from routes_common import main\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/routes_common.py&quot;, line 32, in &lt;module&gt;\n    from aml_blueprint import AMLBlueprint\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/aml_blueprint.py&quot;, line 28, in &lt;module&gt;\n    main_module_spec.loader.exec_module(main)\n  File &quot;\/var\/azureml-app\/score.py&quot;, line 4, in &lt;module&gt;\n    import joblib\nModuleNotFoundError: No module named 'joblib'\nWorker exiting (pid: 100)\nShutting down: Master\nReason: Worker failed to boot.\n2022-01-24T10:21:13,851467800+00:00 - gunicorn\/finish 3 0\n2022-01-24T10:21:13,853259700+00:00 - Exit code 3 is not normal. Killing image.\n<\/code><\/pre>\n<h1>From a Conda specification<\/h1>\n<p>Same as before, but with a fresh environment from Conda specification and changing the <code>env_version<\/code> number :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># ...\n\n\nenv_version=&quot;2&quot;\n\n# ...\n\ncustom_env = Environment.from_conda_specification(name=env_name, file_path=&quot;my-env.yml&quot;)\ncustom_env.docker.base_image = DEFAULT_CPU_IMAGE\n\n# ...\n\n<\/code><\/pre>\n<p>with <code>my-env.yml<\/code> :<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: my-env\ndependencies:\n- python\n\n- pip:\n  - azureml-defaults\n  - azureml-sdk\n  - sklearn\n  - numpy\n  - matplotlib\n  - joblib\n  - uuid\n  - requests\n  - tensorflow\n\n<\/code><\/pre>\n<p>I get this log :<\/p>\n<pre><code>2022-01-24T11:06:54,887886931+00:00 - iot-server\/run \n2022-01-24T11:06:54,891839877+00:00 - rsyslog\/run \n2022-01-24T11:06:54,893640998+00:00 - gunicorn\/run \n2022-01-24T11:06:54,912032812+00:00 - nginx\/run \nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n2022-01-24T11:06:55,398420960+00:00 - iot-server\/finish 1 0\n2022-01-24T11:06:55,414425146+00:00 - Exit code 1 is normal. Not restarting iot-server.\n\nPATH environment variable: \/opt\/miniconda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\nPYTHONPATH environment variable: \n\nPip Dependencies\n---------------\nbrotlipy==0.7.0\ncertifi==2020.6.20\ncffi @ file:\/\/\/tmp\/build\/80754af9\/cffi_1605538037615\/work\nchardet @ file:\/\/\/tmp\/build\/80754af9\/chardet_1605303159953\/work\nconda==4.9.2\nconda-package-handling @ file:\/\/\/tmp\/build\/80754af9\/conda-package-handling_1603018138503\/work\ncryptography @ file:\/\/\/tmp\/build\/80754af9\/cryptography_1605544449973\/work\nidna @ file:\/\/\/tmp\/build\/80754af9\/idna_1593446292537\/work\npycosat==0.6.3\npycparser @ file:\/\/\/tmp\/build\/80754af9\/pycparser_1594388511720\/work\npyOpenSSL @ file:\/\/\/tmp\/build\/80754af9\/pyopenssl_1605545627475\/work\nPySocks @ file:\/\/\/tmp\/build\/80754af9\/pysocks_1594394576006\/work\nrequests @ file:\/\/\/tmp\/build\/80754af9\/requests_1592841827918\/work\nruamel-yaml==0.15.87\nsix @ file:\/\/\/tmp\/build\/80754af9\/six_1605205313296\/work\ntqdm @ file:\/\/\/tmp\/build\/80754af9\/tqdm_1605303662894\/work\nurllib3 @ file:\/\/\/tmp\/build\/80754af9\/urllib3_1603305693037\/work\n\nStarting HTTP server\n2022-01-24T11:06:59,701365128+00:00 - gunicorn\/finish 127 0\n.\/run: line 127: exec: gunicorn: not found\n2022-01-24T11:06:59,706177784+00:00 - Exit code 127 is not normal. Killing image.\n    \n<\/code><\/pre>\n<p>I really don't know what I'm missing, and I've been searching for too long already (Azure docs, SO, ...).<\/p>\n<p>Thanks for your help !<\/p>\n<p>Edit : Non-exhaustive list of solutions I tried :<\/p>\n<ul>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/65159778\/how-to-create-azureml-environement-and-add-required-packages\">How to create AzureML environement and add required packages<\/a><\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/65159308\/how-to-use-existing-conda-environment-as-a-azureml-environment\">how to use existing conda environment as a AzureML environment<\/a><\/li>\n<li>...<\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-environments#environment-building-caching-and-reuse\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-environments#environment-building-caching-and-reuse<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments#add-packages-to-an-environment\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments#add-packages-to-an-environment<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-inferencing-gpus\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-inferencing-gpus<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#define-a-deployment-configuration\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#define-a-deployment-configuration<\/a><\/li>\n<li>...<\/li>\n<\/ul>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1643025448957,
        "Question_score":0,
        "Question_tags":"python|tensorflow|jupyter-notebook|azure-machine-learning-service",
        "Question_view_count":902,
        "Owner_creation_time":1337868050813,
        "Owner_last_access_time":1663936553207,
        "Owner_location":"Paris, France",
        "Owner_reputation":483,
        "Owner_up_votes":39,
        "Owner_down_votes":2,
        "Owner_views":105,
        "Question_last_edit_time":1643163454172,
        "Answer_body":"<p>OK, I got it working : I started over from scratch and it worked.<\/p>\n<p>I have no idea what was wrong in all my preceding tries, and that is terrible.<\/p>\n<p>Multiple problems and how I (think I) solved them :<\/p>\n<ul>\n<li><code>joblib<\/code> : I actually didn't need it to load my Keras model. But the problem was not with this specific library, rather that I couldn't add dependencies to the inference environment.<\/li>\n<li><code>Environment<\/code> : finally, I was only able to make things work with a custom env : <code>Environment.from_conda_specification(name=version, file_path=&quot;conda_dependencies.yml&quot;)<\/code> . I haven't been able to add my libraries (or specify a specific package version) to a &quot;currated environment&quot;. I don't know why though...<\/li>\n<li><code>TensorFlow<\/code> : last problem I had was that I trained and registered my model in AzureML Notebook's <code>azureml_py38_PT_TF<\/code> kernel (<code>tensorflow==2.7.0<\/code>), and tried to load it in the inference Docker image (<code>tensorflow==2.4.0<\/code>). So I had to specify the version of TensorFlow I wanted to use in the inference image (which required the previous point to be solved).<\/li>\n<\/ul>\n<p>What finally worked :<\/p>\n<ul>\n<li>notebook.ipynb<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>import uuid\nfrom azureml.core import Workspace, Environment, Model\nfrom azureml.core.webservice import AciWebservice\nfrom azureml.core.model import InferenceConfig\n\n\nversion = &quot;test-&quot;+str(uuid.uuid4())[:8]\n\nenv = Environment.from_conda_specification(name=version, file_path=&quot;conda_dependencies.yml&quot;)\ninference_config = InferenceConfig(entry_script=&quot;score.py&quot;, environment=env)\n\nws = Workspace.from_config()\nmodel = Model(ws, model_name)\n\naci_config = AciWebservice.deploy_configuration(\n    cpu_cores=1,\n    memory_gb=1,\n)\n\nservice = Model.deploy(\n    workspace=ws,\n    name=version,\n    models=[model],\n    inference_config=inference_config,\n    deployment_config=aci_config,\n    overwrite=True,\n)\n\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n<ul>\n<li>conda_dependencies.yml<\/li>\n<\/ul>\n<pre class=\"lang-yaml prettyprint-override\"><code>channels:\n- conda-forge\ndependencies:\n- python=3.8\n- pip:\n  - azureml-defaults\n  - azureml-sdk\n  - numpy\n  - tensorflow==2.7.0\n\n<\/code><\/pre>\n<ul>\n<li>score.py<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nimport numpy as np\nimport tensorflow as tf\n\n\ndef init():\n    global model\n\n    model_path = os.path.join(os.getenv(&quot;AZUREML_MODEL_DIR&quot;), &quot;model\/data\/model&quot;)\n    model = tf.keras.models.load_model(model_path)\n\n\n\ndef run(raw_data):\n    data = np.array(json.loads(raw_data)[&quot;data&quot;])\n    y_hat = model.predict(data)\n\n    return y_hat.tolist()\n\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1643188448820,
        "Answer_score":1.0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70833499",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":48197524,
        "Question_title":"How to remove the entire rows if value is NULL in Azure ML studio",
        "Question_body":"<p>I am preparing the data for regression model. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/LFaYl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LFaYl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I want to remove the entire row If all columns have value <code>NULL<\/code>. <\/p>\n\n<p>With Clean Missing Data module seems to me like I only able to remove missing values. But <code>NULL<\/code> is not considers mission value. <\/p>\n\n<p>So are there any other modules that simply can remove the entire row if all values are <code>NULL<\/code>'s<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1515625426463,
        "Question_score":0,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio",
        "Question_view_count":1477,
        "Owner_creation_time":1457596845393,
        "Owner_last_access_time":1663977598457,
        "Owner_location":"San Diego, CA, United States",
        "Owner_reputation":4046,
        "Owner_up_votes":505,
        "Owner_down_votes":7,
        "Owner_views":825,
        "Question_last_edit_time":null,
        "Answer_body":"<p>you could use \"<strong>Execute Python Script<\/strong>\" or \"<strong>Execute R Script<\/strong>\" to archive that. Or just use \"<strong>Apply SQL Transformation<\/strong>\" -> <code>SELECT * FROM tbl1 where column1 IS NULL AND column2 IS NULL<\/code>.... <\/p>\n\n<p>Greetings,\nStefan<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1516614808532,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1526560111352,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48197524",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64819708,
        "Question_title":"Register Azure ML Model from DatabricksStep",
        "Question_body":"<p>I'm calculating a model while executing a DatabricksStep in an Azure ML Pipeline, save it on my Blob Storage as .pkl file and upload it to the current Azure ML Run using Run.upload_file (). All this works without any problems.<\/p>\n<p>But as soon as I try to register the model to the Azure ML Workspace using Run.register_model (), the script throws the following error:<\/p>\n<p>UserErrorException: UserErrorException:\nMessage:\nOperation returned an invalid status code 'Forbidden'. The possible reason could be:<\/p>\n<ol>\n<li>You are not authorized to access this resource, or directory listing denied.<\/li>\n<li>you may not login your azure service, or use other subscription, you can check your\ndefault account by running azure cli commend:\n'az account list -o table'.<\/li>\n<li>You have multiple objects\/login session opened, please close all session and try again.<\/li>\n<\/ol>\n<p>InnerException None\nErrorResponse\n{\n&quot;error&quot;: {\n&quot;code&quot;: &quot;UserError&quot;,\n&quot;message&quot;: &quot;\\nOperation returned an invalid status code 'Forbidden'. The possible reason could be:\\n1. You are not authorized to access this resource, or directory listing denied.\\n2. you may not login your azure service, or use other subscription, you can check your\\ndefault account by running azure cli commend:\\n'az account list -o table'.\\n3. You have multiple objects\/login session opened, please close all session and try again.\\n                &quot;\n}\n}<\/p>\n<p>with the following call stack<\/p>\n<p>\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/models_client.py in register_model(self, name, tags, properties, description, url, mime_type, framework, framework_version, unpack, experiment_name, run_id, datasets, sample_input_data, sample_output_data, resource_requirements)\n70         return self.<br \/>\n71             _execute_with_workspace_arguments(self._client.ml_models.register, model,\n---&gt; 72                                               custom_headers=ModelsClient.get_modelmanagement_custom_headers())\n73\n74     @error_with_model_id_handling<\/p>\n<p>\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/workspace_client.py in _execute_with_workspace_arguments(self, func, *args, **kwargs)\n65\n66     def _execute_with_workspace_arguments(self, func, *args, **kwargs):\n---&gt; 67         return self._execute_with_arguments(func, copy.deepcopy(self._workspace_arguments), *args, **kwargs)\n68\n69     def get_or_create_experiment(self, experiment_name, is_async=False):<\/p>\n<p>\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py in _execute_with_arguments(self, func, args_list, *args, **kwargs)\n536                 return self._call_paginated_api(func, *args_list, **kwargs)\n537             else:\n--&gt; 538                 return self._call_api(func, *args_list, **kwargs)\n539         except ErrorResponseException as e:\n540             raise ServiceException(e)<\/p>\n<p>\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py in _call_api(self, func, *args, **kwargs)\n234                 return AsyncTask(future, _ident=ident, _parent_logger=self._logger)\n235             else:\n--&gt; 236                 return self._execute_with_base_arguments(func, *args, **kwargs)\n237\n238     def _call_paginated_api(self, func, *args, **kwargs):<\/p>\n<p>\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py in _execute_with_base_arguments(self, func, *args, **kwargs)\n323         total_retry = 0 if self.retries &lt; 0 else self.retries\n324         return ClientBase._execute_func_internal(\n--&gt; 325             back_off, total_retry, self._logger, func, _noop_reset, *args, **kwargs)\n326\n327     @classmethod<\/p>\n<p>\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py in _execute_func_internal(cls, back_off, total_retry, logger, func, reset_func, *args, **kwargs)\n343                 return func(*args, **kwargs)\n344             except Exception as error:\n--&gt; 345                 left_retry = cls._handle_retry(back_off, left_retry, total_retry, error, logger, func)\n346\n347             reset_func(*args, **kwargs)  # reset_func is expected to undo any side effects from a failed func call.<\/p>\n<p>\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py in _handle_retry(cls, back_off, left_retry, total_retry, error, logger, func)\n384 3. You have multiple objects\/login session opened, please close all session and try again.\n385                 &quot;&quot;&quot;\n--&gt; 386                 raise_from(UserErrorException(error_msg), error)\n387\n388             elif error.response.status_code == 429:<\/p>\n<p>\/databricks\/python\/lib\/python3.7\/site-packages\/six.py in raise_from(value, from_value)<\/p>\n<p>Did anybody experience the same error and knows what is its cause and how to solve it?<\/p>\n<p>Best,\nJonas<\/p>\n<p>UPDATE:<\/p>\n<pre><code> model = sklearn.linear_model.LinearRegression ( )\n model_path = &quot;&lt;path to 'model.pkl' in my blob storage&gt;&quot;\n joblib.dump(model, model_path)\n aml_run = azureml.core.get_context ( )\n aml_run.upload_file (name = &quot;model.pkl&quot;, path_or_stream = model_path)\n # Until this point, everything works fine\n    \n aml_run.register_model (model_name = &quot;model.pkl&quot;)\n # This throws the posted &quot;Forbidden&quot;-Error\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1605265124633,
        "Question_score":2,
        "Question_tags":"azure-databricks|azure-machine-learning-studio",
        "Question_view_count":620,
        "Owner_creation_time":1594877973727,
        "Owner_last_access_time":1643018129407,
        "Owner_location":"Germany",
        "Owner_reputation":51,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1605513428203,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64819708",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":59144503,
        "Question_title":"Azure Machine Learning Service - dataset API question",
        "Question_body":"<p>I am trying to use autoML feature of AML. I saw that in the sample notebook it is using Dataset.Tabular.from_delimited_files(train_data) which only takes data from a https path. I am wondering how can I use pandas dataframe directly automl config instead of using dataset API. Alternatively, what is the way I can convert pandas dataframe to tabular dataset to pass into automl config?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1575310207200,
        "Question_score":0,
        "Question_tags":"automl|azure-machine-learning-service",
        "Question_view_count":982,
        "Owner_creation_time":1430707152817,
        "Owner_last_access_time":1581450353247,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59144503",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73028352,
        "Question_title":"How do i visualize azure ml predictions in power bi",
        "Question_body":"<p>I am working with azure ML notebook  for timeseries predictions\nI have done everything  and i have my predictions  however i want to have my data and my predictions  visualized  in power bi.\nHow do i save my data back to azure blob so i can utilize  it in power bi?\nOr anyotherway to visualize in power bi<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658176742183,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio|azureml-python-sdk",
        "Question_view_count":55,
        "Owner_creation_time":1607617423893,
        "Owner_last_access_time":1663931298390,
        "Owner_location":"Slovakia",
        "Owner_reputation":41,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73028352",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":52128396,
        "Question_title":"No Experimentation Account found in your Azure Subscriptions in Azure Machine Learning Workbench",
        "Question_body":"<p>I created one Experiment and hosted as web service in Azure ML Stdio<\/p>\n\n<p><a href=\"https:\/\/studio.azureml.net\/\" rel=\"nofollow noreferrer\">https:\/\/studio.azureml.net\/<\/a><\/p>\n\n<p>However, I have installed Azure Machine Learning Workbench and logging into same account. It says:<\/p>\n\n<p><strong>No Experimentation Account found in your Azure Subscriptions\nYou can create one in the Microsoft Azure Management Portal.<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1535807224517,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-workbench",
        "Question_view_count":113,
        "Owner_creation_time":1378277999787,
        "Owner_last_access_time":1664033338507,
        "Owner_location":"Hyderabad, India",
        "Owner_reputation":2273,
        "Owner_up_votes":37,
        "Owner_down_votes":7,
        "Owner_views":177,
        "Question_last_edit_time":1535822103232,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52128396",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":57488706,
        "Question_title":"Deploying custom model on Azure ML Studio",
        "Question_body":"<p>In Azure ML Studio, we have the option of choosing a number of inbuilt ML models like Classification, Regression, etc. , which we can drag and drop to our workflow.<\/p>\n\n<p>My question is, can I upload a custom ML model that I have built locally on my system in Python, and add it to the workflow?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1565761282107,
        "Question_score":1,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio",
        "Question_view_count":1187,
        "Owner_creation_time":1565761178210,
        "Owner_last_access_time":1613393617227,
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":1565767277470,
        "Answer_body":"<ol>\n<li>Take the model.pkl file, zip it, and upload it into Azure Machine Learning Studio. Click the \u201cNew\u201d icon in the bottom left:\n<a href=\"https:\/\/i.stack.imgur.com\/Iwvhi.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Iwvhi.jpg\" alt=\"\"><\/a><\/li>\n<li>In the pane that comes up, click on dataset, and then \u201cFrom Local File\u201d:\n<a href=\"https:\/\/i.stack.imgur.com\/DvyjO.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DvyjO.jpg\" alt=\"\"><\/a><\/li>\n<li>Select the zip file where you stored your serialized model and click the tick. You expirement should look like this:\n<a href=\"https:\/\/i.stack.imgur.com\/0efka.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0efka.jpg\" alt=\"\"><\/a><\/li>\n<li>Put the following code to run your classification experiment:<\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport sys\nimport pickle\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    sys.path.insert(0,\".\\Script Bundle\")\n    model = pickle.load(open(\".\\Script Bundle\\model.pkl\", 'rb'))\n    pred = model.predict(dataframe1)\n    return pd.DataFrame([pred[0]])\n<\/code><\/pre>\n\n<p><strong>Update<\/strong> \nIf you want to declare this experiment as an API you need to add web input and output to the Python script module.\n<a href=\"https:\/\/i.stack.imgur.com\/eqV8W.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eqV8W.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":7.0,
        "Answer_creation_time":1566202471556,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1566565696980,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57488706",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73714337,
        "Question_title":"Is it possible to use data cleansing in SQL Server by using Azure ML?",
        "Question_body":"<p>I have a SQL Server which I want to run data cleansing every night. To do this I want to use Azure Machine Learning.<\/p>\n<p>To my question; Is it possible to run data cleansing with Azure Machine Learning directly in my SQL Server?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663146771947,
        "Question_score":0,
        "Question_tags":"sql-server|data-cleaning|azure-machine-learning-service",
        "Question_view_count":22,
        "Owner_creation_time":1525701893967,
        "Owner_last_access_time":1663919853580,
        "Owner_location":"Uppsala, Sverige",
        "Owner_reputation":31,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73714337",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70852922,
        "Question_title":"ModuleNotFoundError: No module named 'azureml' in Azure ML Studio",
        "Question_body":"<p>I am learning Azure ML from Microsoft tutorials, <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-bring-data\" rel=\"nofollow noreferrer\">here<\/a>. The first two tutorials ran fine, but this one is giving me the following error.<\/p>\n<pre><code>[stderr]Traceback (most recent call last):\n[stderr]  File &quot;train.py&quot;, line 8, in &lt;module&gt;\n[stderr]    from azureml.core import Run\n[stderr]ModuleNotFoundError: No module named 'azureml'\n[stderr]\n<\/code><\/pre>\n<p>Working with Azure ML Studio and submitting the code to the environment, I am unable to find how to resolve this error.<\/p>\n<p>I have checked that the package is installed (running on Azure ML studio so this is basic assumption, but I have tested as well). Following is the code 'run-pytorch.py' which calls the script 'train.py'<\/p>\n<pre><code># run-pytorch.py\nfrom azureml.core import Workspace\nfrom azureml.core import Experiment\nfrom azureml.core import Environment\nfrom azureml.core import ScriptRunConfig\n\nif __name__ == &quot;__main__&quot;:\n    ws = Workspace.from_config()\n    experiment = Experiment(workspace=ws, name='day1-experiment-train')\n    config = ScriptRunConfig(source_directory='.\/src',\n                             script='train.py',\n                             compute_target='cpu-cluster')\n\n    # set up pytorch environment\n    env = Environment.from_conda_specification(\n        name='pytorch-env',\n        file_path='pytorch-env.yml'\n    )\n    config.run_config.environment = env\n\n    run = experiment.submit(config)\n\n    aml_url = run.get_portal_url()\n    print(aml_url)\n    print('Success...!!!')\n<\/code><\/pre>\n<p>Teh code snippet for train.py is as follows<\/p>\n<pre><code># train.py\nimport os\nimport argparse\nimport torch\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom model import Net\nfrom azureml.core import Run\n...\n...\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1643131009297,
        "Question_score":2,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":838,
        "Owner_creation_time":1387867856557,
        "Owner_last_access_time":1663872845690,
        "Owner_location":null,
        "Owner_reputation":453,
        "Owner_up_votes":91,
        "Owner_down_votes":2,
        "Owner_views":63,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70852922",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":69933296,
        "Question_title":"Azure ML Pipeline fails while running a grid search CV on a cluster",
        "Question_body":"<p>I implemented a gridsearchcv on Azure ML as a pipeline but I keep getting an error that says &quot;User program failed with TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.<\/p>\n<p>The exit codes of the workers are {SIGKILL(-9)}&quot;<\/p>\n<p>I tried changing the package versions but could not get it to work. The code runs well without error when I run it as a script but fails when I run it as a pipeline.<\/p>\n<p>Any idea on how to fix this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636655875797,
        "Question_score":0,
        "Question_tags":"python-3.x|azure|scikit-learn|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":175,
        "Owner_creation_time":1636655338437,
        "Owner_last_access_time":1643399394220,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69933296",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67867951,
        "Question_title":"AzureML SDK problems with pip-installed dependencies in environment file",
        "Question_body":"<p>i am try to run a Python Script in AzureML SDK. However in the runs log file, it prints the same error over and over again:<\/p>\n<blockquote>\n<p>Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.<\/p>\n<\/blockquote>\n<p>The defined conda environment looks like this:<\/p>\n<pre><code>    {\n    &quot;databricks&quot;: {\n        &quot;eggLibraries&quot;: [],\n        &quot;jarLibraries&quot;: [],\n        &quot;mavenLibraries&quot;: [],\n        &quot;pypiLibraries&quot;: [],\n        &quot;rcranLibraries&quot;: []\n    },\n    &quot;docker&quot;: {\n        &quot;arguments&quot;: [],\n        &quot;baseDockerfile&quot;: null,\n        &quot;baseImage&quot;: &quot;mcr.microsoft.com\/azureml\/openmpi3.1.2-ubuntu18.04:20210301.v1&quot;,\n        &quot;baseImageRegistry&quot;: {\n            &quot;address&quot;: null,\n            &quot;password&quot;: null,\n            &quot;registryIdentity&quot;: null,\n            &quot;username&quot;: null\n        },\n        &quot;enabled&quot;: false,\n        &quot;platform&quot;: {\n            &quot;architecture&quot;: &quot;amd64&quot;,\n            &quot;os&quot;: &quot;Linux&quot;\n        },\n        &quot;sharedVolumes&quot;: true,\n        &quot;shmSize&quot;: null\n    },\n    &quot;environmentVariables&quot;: {\n        &quot;EXAMPLE_ENV_VAR&quot;: &quot;EXAMPLE_VALUE&quot;\n    },\n    &quot;inferencingStackVersion&quot;: null,\n    &quot;name&quot;: &quot;MyEnvironment04&quot;,\n    &quot;python&quot;: {\n        &quot;baseCondaEnvironment&quot;: null,\n        &quot;condaDependencies&quot;: {\n            &quot;channels&quot;: [\n                &quot;anaconda&quot;,\n                &quot;conda-forge&quot;\n            ],\n            &quot;dependencies&quot;: [\n                &quot;python=3.6.2&quot;,\n                {\n                    &quot;pip&quot;: [\n                        &quot;pip=20.2.40&quot;\n                    ]\n                },\n                &quot;scikit-learn&quot;\n            ],\n            &quot;name&quot;: &quot;azureml_815589d460c271a1415198e7283fa9e9&quot;\n        },\n        &quot;condaDependenciesFile&quot;: null,\n        &quot;interpreterPath&quot;: &quot;python&quot;,\n        &quot;userManagedDependencies&quot;: false\n    },\n    &quot;r&quot;: null,\n    &quot;spark&quot;: {\n        &quot;packages&quot;: [],\n        &quot;precachePackages&quot;: true,\n        &quot;repositories&quot;: []\n    },\n    &quot;version&quot;: &quot;1&quot;\n}\n<\/code><\/pre>\n<p>I am assuming that the definition for my pip package in my environment is wrong.<\/p>\n<pre><code># Create the dependencies object\nmyenv_dep = CondaDependencies.create(conda_packages=['scikit-learn'], pip_packages=['pip=20.2.40'])\nmyenv.python.conda_dependencies = myenv_dep\n<\/code><\/pre>\n<p>Please let me know if i need to provide further information.<\/p>\n<p>Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1623052859247,
        "Question_score":0,
        "Question_tags":"python|pip|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":332,
        "Owner_creation_time":1596110326463,
        "Owner_last_access_time":1644575072690,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67867951",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":47735839,
        "Question_title":"how to reduce the Run time in Azure ML for decision tree and decision forest",
        "Question_body":"<p>I am trying to run a regression model for a data set containing over 2000000 rows. I tried using linear regression and boosted decision tree regression without tuning model hyperparameter, I didn't get the expected accuracy. so I tried to use Tune model hyperparameter for the boosted decision tree, the model runs over 20 min. the decision forest also takes to0 long (even without tuning model hyperparameter). Is there any way to reduce the runtime without compromising the result accuracy too much?<\/p>\n\n<p>will sampling affect the output (say I took  0.5 as sampling rate)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1512879883917,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":127,
        "Owner_creation_time":1512877126070,
        "Owner_last_access_time":1516544522587,
        "Owner_location":"Portugal",
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The execution time on AzureML Studio depends on the pricing tier. The free version does one node execution at time while the standard pricing tier do the execute multiple execution at one time. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1513656866752,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47735839",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73334181,
        "Question_title":"How to save and access pickle\/hdf5 files in azure machine learning studio",
        "Question_body":"<p>I have a pickle file parameters.pkl containing some parameters and their values of a model. The pickle file has been created through the following process:<\/p>\n<pre><code>dict={'scaler': scaler,\n'features': z_tags,\n'Z_reconstruction_loss': Z_reconstruction_loss} \npickle.dump(dict, open('parameters.pkl', 'wb'))\n\nmodel_V2.hdf5\n<\/code><\/pre>\n<p>I am new to azure machine learning studio.It will be helpful to know, how the pickle file and hdf5 files can be stored in Azure machine Learning Studio and an API endpoint be created, so that the the pickle file can be accessed through API. Objective is to access the pickle file and its contents through API.. I have tried the following:<\/p>\n<pre><code>pip install azureml , azureml-core\nfrom azureml.core import Workspace\nfrom azureml.core.webservice import AciWebservice\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.conda_dependencies import CondaDependencies\n\n\nws = Workspace.create(\n               name='myworkspace',            \n               subscription_id='&lt;azure-subscription-id&gt;',           \n               resource_group='myresourcegroup',                 \n               create_resource_group=True,                 \n               location='eastus2'                \n               )\n\nws.write_config()\n\nws = Workspace.from_config()\n\nmodel = Model.register(workspace = ws,\n              model_path =&quot;model\/parameters.pkl&quot;,\n              model_name = &quot;parameters&quot;,\n              tags = {&quot;version&quot;: &quot;1&quot;},\n              description = &quot;parameters&quot;,\n              )\n\n\n# to install required packages\nenv = Environment('env')\ncd = CondaDependencies.create(pip_packages=['pandas==1.1.5', 'azureml-defaults','joblib==0.17.0'], conda_packages = ['scikit-learn==0.23.2'])\nenv.python.conda_dependencies = cd\n# Register environment to re-use later\nenv.register(workspace = ws)\nprint(&quot;Registered Environment&quot;)\n\nmyenv = Environment.get(workspace=ws, name=&quot;env&quot;)\n\nmyenv.save_to_directory('.\/environ', overwrite=True)\n\naciconfig = AciWebservice.deploy_configuration(\n            cpu_cores=1,\n            memory_gb=1,\n            tags={&quot;data&quot;:&quot;parameters&quot;},\n            description='parameters MODEL',\n            )\n\ninference_config = InferenceConfig(entry_script=&quot;score.py&quot;, environment=myenv)\n<\/code><\/pre>\n<p>What to modify in following score script, as I don't want to predict anything but access the parameter values stored in the pickle file.<\/p>\n<pre><code>def init():\n    global modelmodel_path = Model.get_model_path(&quot;parameters&quot;)\n    print(&quot;Model Path is  &quot;, model_path)\n    model = joblib.load(model_path)\n\ndef run(data):\n   try:\n     data = json.loads(data)\n     result = model.predict(data['data'])\n     return {'data' : result.tolist() , 'message' : &quot;Successfully \n            accessed&quot;}\n   except Exception as e:\n      error = str(e)\n      return {'data' : error , 'message' : 'Failed to access'}\n\nDeploy the Model\nservice = Model.deploy(workspace=ws,\n                name='iris-model',\n                models=[model],\n                inference_config=inference_config,\n                deployment_config=aciconfig, \n                overwrite = True)\nservice.wait_for_deployment(show_output=True)\nurl = service.scoring_uri\nprint(url)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660307228527,
        "Question_score":1,
        "Question_tags":"python-3.x|azure|pickle|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":130,
        "Owner_creation_time":1431685926620,
        "Owner_last_access_time":1663756216813,
        "Owner_location":null,
        "Owner_reputation":543,
        "Owner_up_votes":17,
        "Owner_down_votes":2,
        "Owner_views":125,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73334181",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":33741912,
        "Question_title":"How connect Azure Machine Learning and Spark Streaming or Apache Storm",
        "Question_body":"<p>Is there possibility to get stream from Spark Streaming or Apache Storm into Azure Machine Learning? In <strong><em>reader<\/em><\/strong> option there is an input to read data from Hive database\n<a href=\"https:\/\/i.stack.imgur.com\/8Em26.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8Em26.png\" alt=\"hive\"><\/a><\/p>\n\n<p>but how to achive real time stream of data from Spark or Storm, for example <strong><em>Real-time fraud detection<\/em><\/strong><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1447697540763,
        "Question_score":0,
        "Question_tags":"azure|hadoop|cortana-intelligence|azure-machine-learning-studio",
        "Question_view_count":549,
        "Owner_creation_time":1327481639093,
        "Owner_last_access_time":1663925285370,
        "Owner_location":"Poznan, Poland",
        "Owner_reputation":2923,
        "Owner_up_votes":875,
        "Owner_down_votes":5,
        "Owner_views":838,
        "Question_last_edit_time":1456849966663,
        "Answer_body":"<p>To do real time Fraud detection typically you will create a Model on Azure ML, then publish that model to oWeb service, then on you Spark or Storm system you will call that Web service, in  sequence ( like payment happened on commercial sites for example), then you will get an immediate answer about the actual parameters you had sent in you web service call.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1448457555592,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/33741912",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":41451123,
        "Question_title":"An example to create a training model on real data in AzureML",
        "Question_body":"<p>Can you introduce a real sample for azure ML and show how can it be possible to see the result of the training?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1483472453410,
        "Question_score":0,
        "Question_tags":"machine-learning|azure-machine-learning-studio",
        "Question_view_count":76,
        "Owner_creation_time":1403553737940,
        "Owner_last_access_time":1664024595667,
        "Owner_location":"Belgium",
        "Owner_reputation":17835,
        "Owner_up_votes":636,
        "Owner_down_votes":1612,
        "Owner_views":2203,
        "Question_last_edit_time":null,
        "Answer_body":"<p><a href=\"http:\/\/blog.learningtree.com\/how-to-build-a-predictive-model-using-azure-machine-learning\/\" rel=\"nofollow noreferrer\">Here<\/a> is a good sample to create your first model.\nI should notice that I can't load data from url, as there is a forbidden error to load from url, and I don't know why! \nAnyhow, you can import data manually by copy the data from <a href=\"http:\/\/blog.learningtree.com\/wp-content\/uploads\/2015\/01\/breast-cancer-wisconsin.data_.arff_.txt\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Also, you can find the created model which is published here: \n<a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Cancer-Model-1\" rel=\"nofollow noreferrer\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Cancer-Model-1<\/a><\/p>\n\n<p>About see the result of the training model, you can right click on the tick (highlighted by a red circle in the following picture) of Evaluation Model. Then, in the opened menu, go to \"Evaluation Result -> Visualization\".\n<a href=\"https:\/\/i.stack.imgur.com\/vhJWE.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vhJWE.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>After that you can see a window like the following (which shows ROC curve and some related result such as accuracy of the training model):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/FAuzU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FAuzU.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/B39lI.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/B39lI.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Besides, you can see <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-customer-churn-scenario\" rel=\"nofollow noreferrer\">this example<\/a> as an another sample.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1483472453409,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1483537837232,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41451123",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":45121271,
        "Question_title":"Does Azure ML Studio not working with Chrome\/Firefox?",
        "Question_body":"<p>See attached, is it because I am on Chrome\/Firefox? I am currently on a Macbook, so I can't really test this out on Edge browser. Why does Microsoft build something that won't work for other browsers?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/v744g.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/v744g.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":7,
        "Question_creation_time":1500141920653,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":194,
        "Owner_creation_time":1297803164077,
        "Owner_last_access_time":1663909246143,
        "Owner_location":"San Francisco, CA",
        "Owner_reputation":2399,
        "Owner_up_votes":1178,
        "Owner_down_votes":2,
        "Owner_views":256,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45121271",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":49604773,
        "Question_title":"Azure Machine Learning Studio vs. Workbench",
        "Question_body":"<p>What is the difference between <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-studio\/\" rel=\"noreferrer\">Azure Machine Learning Studio<\/a> and <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-services\/\" rel=\"noreferrer\">Azure Machine Learning Workbench<\/a>?  What is the <em>intended<\/em> difference? And is it expected that Workbench is heading towards deprecation in favor of Studio?<\/p>\n\n<p>I have gathered an assorted collection of differences:<\/p>\n\n<ul>\n<li>Studio has a hard limit of 10 GB total input of training data per module, whereas Workbench has a variable limit by price.<\/li>\n<li>Studio appears to have a more fully-featured GUI and user-friendly deployment tools, whereas Workbench appears to have more powerful \/ customizable deployment tools.<\/li>\n<li>etc.<\/li>\n<\/ul>\n\n<p>However, I have also found several scattered references claiming that Studio is a renamed updated of Workbench, even though both services appear to still be offered.<\/p>\n\n<p>For a fresh Data Scientist looking to adopt the Microsoft stack (potentially on an enterprise scale within the medium-term and for the long-term), which offering should I prefer?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1522638099293,
        "Question_score":8,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-workbench",
        "Question_view_count":3387,
        "Owner_creation_time":1434736108840,
        "Owner_last_access_time":1663241421093,
        "Owner_location":"Dallas, TX, United States",
        "Owner_reputation":2045,
        "Owner_up_votes":1074,
        "Owner_down_votes":66,
        "Owner_views":166,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Azure Machine Learning Workbench is a preview downloadable application. It provides a UI for many of the Azure Machine Learning CLI commands, particularly around experimentation submission for Python based jobs to DSVM or HDI. The Azure Machine Learning CLI is made up of many key functions, such as job submisison, and creation of real time web services. The workbench installer provided a way to install everything required to participate in the preview. <\/p>\n\n<p>Azure Machine Learning Studio is an older product, and provides a drag and drop interface for creating simply machine learning processes. It has limitations about the size of the data that can be handled (about 10gigs of processing). Learning and customer requests have based on this service have contributed to the design of the new Azure Machine Learning CLI mentioned above.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1524806701632,
        "Answer_score":6.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49604773",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":42367141,
        "Question_title":"Annotated images classification",
        "Question_body":"<p>I've got a bunch of images (~3000) which have been manually classified (approved\/rejected) based on some business criteria. I've processed these images with Google Cloud Platform obtaining annotations and SafeSearch results, for example (csv format):<\/p>\n\n<p>file name; approved\/rejected; adult; spoof; medical; violence; annotations\nA.jpg;approved;VERY_UNLIKELY;VERY_UNLIKELY;VERY_UNLIKELY;UNLIKELY;boat|0.9,vehicle|0.8\nB.jpg;rejected;VERY_UNLIKELY;VERY_UNLIKELY;VERY_UNLIKELY;UNLIKELY;text|0.9,font|0.8<\/p>\n\n<p>I want to use machine learning to be able to predict if a new image should be approved or rejected (second column in the csv file).<\/p>\n\n<p>Which algorithm should I use? <\/p>\n\n<p>How should I format the data, especially the annotations column? Should I obtain first all the available annotation types and use them as a feature with the numerical value (0 if it doesn't apply)? Or would it be better to just process the annotation column as text?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1487679435450,
        "Question_score":1,
        "Question_tags":"machine-learning|azure-machine-learning-studio|amazon-machine-learning",
        "Question_view_count":97,
        "Owner_creation_time":1364894167927,
        "Owner_last_access_time":1584637381497,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/42367141",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58050526,
        "Question_title":"How to have my customized score file deployed on Azure with azure.mlflow sdk?",
        "Question_body":"<p>I have a customized score.py file which was generated within databricks but I didn't find a way to deploy it on a container.<\/p>\n\n<p>I am using the mlflow.azureml, on my image creation I couldn't find how to specify the score.py in particular.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow.azureml\n\nmodel_image, azure_model = mlflow.azureml.build_image(model_uri=model_uri, \n                                                      workspace=workspace,\n                                                      model_name=\"my_model\",\n                                                      image_name=\"image_name\",\n                                                      description=\"Predicts\",\n                                                      synchronous=False)\n<\/code><\/pre>\n\n<p>Is there a way to specify the score.py using the lib?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1569165044150,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service|mlflow",
        "Question_view_count":226,
        "Owner_creation_time":1396493951190,
        "Owner_last_access_time":1615670657030,
        "Owner_location":null,
        "Owner_reputation":85,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58050526",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62189492,
        "Question_title":"How to pass PipelineParameter into AutoMLStep in AzureML Python SDK",
        "Question_body":"<p>I am using AzureML SDK pipeline with AutoMLStep. How can I add PipelineParameter into AutoMLStep configuration? I would like to use it for a definition of max_horizon.\nIt should work with <\/p>\n\n<blockquote>\n  <p>passthru_automl_config=False<\/p>\n<\/blockquote>\n\n<p>but I am getting error <\/p>\n\n<blockquote>\n  <p>Message: Unsupported value of max_horizon. max_horizon must be integer or 'auto'<\/p>\n<\/blockquote>\n\n<pre><code>max_horizon = PipelineParameter(name='max_horizon', default_value=30)\n\nautoml_settings = {\n            \"iteration_timeout_minutes\" : 60\n            \"grain_column_names\": [\"COUNTRY_CODE\"],\n            \"time_column_name\": \"DATE\"\n        }        \n\nautoml_config = AutoMLConfig(task='forecasting',\n                             path = \".\/src\",\n                             primary_metric=primary_metric,\n                             iterations=iterations,\n                             max_concurrent_iterations=max_concurrent_iterations,\n                             training_data = train_data,\n                             label_column_name = label,\n                             n_cross_validations=5,\n                             compute_target = compute_target,\n                             max_horizon= max_horizon,\n                             **automl_settings)\n\ntrainWithAutomlStep = AutoMLStep(name=\"experiment_name\",\n                                 automl_config=automl_config,\n                                 passthru_automl_config=False,\n                                 outputs=[metrics_data, model_data],\n                                 allow_reuse=True)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1591257136050,
        "Question_score":3,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":316,
        "Owner_creation_time":1436771091480,
        "Owner_last_access_time":1652881662557,
        "Owner_location":"Brno, \u010cesko",
        "Owner_reputation":51,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1592201521009,
        "Answer_body":"<p>Here is a response from Microsoft:<\/p>\n\n<blockquote>\n  <p>PipelineParameter is currently not supported for use with AutoMLConfig parameters inside of AutoMLStep.<\/p>\n  \n  <p>Then, the only workaround in order to use PipelineParameter with\n  AutoMLConfig would be to use AutoML in a PythonScriptStep, which is a\n  similar usage\/approach when you use AutoMLConfig with\n  ParallelRunConfig in pipelines (without using AutoMLStep), like the\n  \u2018Many Models\u2019 solution accelerator does.<\/p>\n<\/blockquote>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1592202183327,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62189492",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72621994,
        "Question_title":"How to build Face\/ Image Classifier in Azure ML like Google Photos",
        "Question_body":"<p>I need to build an image classification model in Azure  ML- which initially takes an input from Phone (A check in app which takes information like ID and also we will capture the image of the person- Here ID is used to tag the image) which will be redirected to data storage. once it's done, we will upload the n number of images of person to the data storage, it should able to classify the image based on facial recognition and should categorize as separate image folder for different person( Just like Google Photos). In short, If there's a 100 unique people come for check in and during the event if we click random images of these 100 unique persons, when we load this data to blob - it should categorize the persons separately.<\/p>\n<p>Can I go with approach-<\/p>\n<p>1.Check in app-- Loads image with tag\n2.Blob- store the image\n3. custom vison- ML classifier\n4.Loding n number of images to blob\n5. comparing the image with check in app loaded image and categorizing as album just like google photos\n6. Loading albums to app to make attendees to see the images<\/p>\n<p>Please guide me with the solution and services need to be considered to make this possible in azure<\/p>\n<p>Thanks in adavance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655232509777,
        "Question_score":0,
        "Question_tags":"azure-cognitive-services|image-classification|azure-machine-learning-service|microsoft-custom-vision|facial-identification",
        "Question_view_count":34,
        "Owner_creation_time":1651208274493,
        "Owner_last_access_time":1660105136120,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72621994",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":69584991,
        "Question_title":"Azure AutoML seems to add extra input?",
        "Question_body":"<p>I'm using azure Automated ML to do some proof of concepts. I'm trying to identify a person based on some parameters.<\/p>\n<p>In my dataset I have 4 columns of floats and 1 column containing the name of the person. My ambition is to be able to detect the person, based on the input of these 4 floats.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/AEZg0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AEZg0.png\" alt=\"Dataset schema. Contains 5 columns\" \/><\/a><\/p>\n<p>I have successfully trained some models based on this information. The data transformation chart looks like this, which is as I would expect:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/r4X2r.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/r4X2r.png\" alt=\"Data Transformation Chart\" \/><\/a><\/p>\n<p>So it ignores one column (the &quot;person&quot; column I assume) and uses the remaining 4 as input to a RandomForrest classifier. All is well and good so far.<\/p>\n<p>When I then go and deploy the model, I now need to add a new variable simply called &quot;Column2&quot;. This variable seems to have significant influence on the output data<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ssBhq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ssBhq.png\" alt=\"Input example\" \/><\/a><\/p>\n<p>When I make a request to the endpoint with two inputs where the only difference is the value of the &quot;Column2&quot; I get two different probabilities back:<\/p>\n<pre><code>\n{'PCA_0': -574.0043295463845, 'PCA_1': 3455.9091610620617, 'PCA_2': 2352.2555893520835, 'PCA_3': -6941.596091271862, 'Column2': '0'} = [0.24, 0.4, 0.06, 0.3]\n{'PCA_0': -574.0043295463845, 'PCA_1': 3455.9091610620617, 'PCA_2': 2352.2555893520835, 'PCA_3': -6941.596091271862, 'Column2': '1'} = [0.26, 0.19, 0.54, 0.01]\n<\/code><\/pre>\n<p>Anyone has any idea about what I'm doing wrong here?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634302137757,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":100,
        "Owner_creation_time":1504519461013,
        "Owner_last_access_time":1663923545203,
        "Owner_location":"Copenhagen, Denmark",
        "Owner_reputation":159,
        "Owner_up_votes":23,
        "Owner_down_votes":3,
        "Owner_views":28,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69584991",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":31609319,
        "Question_title":"'Enter Data' as list instead of list of lists in Azure ML Web Service",
        "Question_body":"<p>In Azure ML, I want to enter data to a model through a published Web Service. \nThe way to tell this to the Web Service, as far as I can tell, it to have an 'Enter Data' box coming into the same input as the Web service. <\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/m1x5x.png\" alt=\"enter image description here\"><\/p>\n\n<p>You can then set you data format in the 'Enter Data' properties:<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/VQJ9V.png\" alt=\"enter image description here\"><\/p>\n\n<p>I want that list to be an arbitrary-length array of samples. This works if your input is:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"samples\"\n      ],\n      \"Values\": [\n        [\n          1\n        ],\n        [\n          2\n        ],\n        [\n          3\n        ],\n        [\n          4\n        ],\n        [\n          5\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>This is ok, but ideally it would be easier, and (more importantly) more network-efficient, if I could send them as:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"samples\"\n      ],\n      \"Values\": [\n        [\n          1,2,3,4,5\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>Is there a correct syntax to implement this? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1437737046243,
        "Question_score":0,
        "Question_tags":"python|json|azure|azure-machine-learning-studio",
        "Question_view_count":174,
        "Owner_creation_time":1266595927520,
        "Owner_last_access_time":1633931623067,
        "Owner_location":null,
        "Owner_reputation":7681,
        "Owner_up_votes":284,
        "Owner_down_votes":8,
        "Owner_views":361,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I have worked internally to request a confirmation of your concern - <\/p>\n\n<blockquote>\n  <p>'Enter Data' as list instead of list of lists in Azure ML Web Service<\/p>\n<\/blockquote>\n\n<p>but you expected feature is not available today in Azure ML Studio (The reason behind is Azure ML has to be able to read the input data as a tabular format, rows and columns). Such being the case, I would like to suggest you to submit a new feature request via below option:<\/p>\n\n<p>On Azure ML Studio -> the upper right corner, there is a smiley face, please click that and send the feedback.<\/p>\n\n<p>Should you have any further concerns, please feel free to let me know.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1438871325836,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/31609319",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60550897,
        "Question_title":"Pipeline error: \"AttributeError: 'ColumnTransformer' object has no attribute '_feature_names_in'\"",
        "Question_body":"<p>I am trying to use scikitlearn to predict over new data using a pipeline object I had trained back in February. Since Friday, February 28th, the predict function no longer works for my pipeline object, citing the error:<\/p>\n\n<pre><code>&gt;&gt;&gt; df = pd.read_csv('test_df_for_example.csv')\n&gt;&gt;&gt; mdl = joblib.load('split_0_model.pkl')\n&gt;&gt;&gt; mdl.predict(df)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"\/Library\/Frameworks\/Python.framework\/Versions\/3.6\/lib\/python3.6\/site-packages\/sklearn\/utils\/metaestimators.py\", line 116, in &lt;lambda&gt;\n    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n  File \"\/Library\/Frameworks\/Python.framework\/Versions\/3.6\/lib\/python3.6\/site-packages\/sklearn\/pipeline.py\", line 419, in predict\n    Xt = transform.transform(Xt)\n  File \"\/Library\/Frameworks\/Python.framework\/Versions\/3.6\/lib\/python3.6\/site-packages\/sklearn\/compose\/_column_transformer.py\", line 587, in transform\n    self._validate_features(X.shape[1], X_feature_names)\n  File \"\/Library\/Frameworks\/Python.framework\/Versions\/3.6\/lib\/python3.6\/site-packages\/sklearn\/compose\/_column_transformer.py\", line 411, in _validate_features\n    if ((self._feature_names_in is None or feature_names is None)\nAttributeError: 'ColumnTransformer' object has no attribute '_feature_names_in'\n<\/code><\/pre>\n\n<p>I am using Microsoft Azure's virtual machines to do this predicting (although the above code I ran on my local computer), so working with the versioning of the modules is difficult, and most of the time I am forced to use the latest versions of packages. I believe this error comes from scikitlearn's new version 0.22.2.post1, which I am using.<\/p>\n\n<p>I have an example CSV with testing data <a href=\"https:\/\/drive.google.com\/open?id=1iS2X1jM8eY246AJ5Eh1nnpwCJmjbilM0\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n\n<p>The model file pickled with joblib <a href=\"https:\/\/drive.google.com\/open?id=13Cz374Uvn-ssGZWGps3MQVzpWjilmNKO\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n\n<p>And code to reproduce the error <a href=\"https:\/\/drive.google.com\/open?id=1iS2X1jM8eY246AJ5Eh1nnpwCJmjbilM0\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n\n<p>And yaml environment file <a href=\"https:\/\/drive.google.com\/open?id=1hJTErtqCT3gNUKdcFeZ7EaUZhHm40Juv\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n\n<p>Is there any way I can upgrade my model so that this error does not occur?<\/p>\n\n<p>Thanks!\nKristine<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1583429013407,
        "Question_score":2,
        "Question_tags":"python|python-3.x|scikit-learn|azure-machine-learning-service",
        "Question_view_count":2071,
        "Owner_creation_time":1572270460550,
        "Owner_last_access_time":1593992185087,
        "Owner_location":null,
        "Owner_reputation":19,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60550897",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":49673502,
        "Question_title":"Azure Machine Learning - Predicting a win\/lose\/draw API",
        "Question_body":"<p>I'm experimenting with an existing experiment on the Azure Machine learning gallery.  Its called <a href=\"https:\/\/gallery.azure.ai\/Experiment\/Beat-the-Bookie\" rel=\"nofollow noreferrer\">beat the bookie<\/a>.  I want to take it to the next step and add a web service to it.  This particular experiment has a dataset for years of matches.  It has a small python script to calculate an ELO (like chess or most online games).  I'm struggling to create an input API with 2 inputs: homeTeam and awayTeam.  With an output of 1 value FTR (Full-Time Result) which is either W\/D\/L (Win, Draw or Lose).<\/p>\n\n<p>The issue I have is that when I create the API it has too many required inputs.  Do I have to give them averaged data or how do I reduce the inputs to 2?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1522934588567,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":64,
        "Owner_creation_time":1469533385120,
        "Owner_last_access_time":1656601407173,
        "Owner_location":"Ireland",
        "Owner_reputation":970,
        "Owner_up_votes":93,
        "Owner_down_votes":1,
        "Owner_views":68,
        "Question_last_edit_time":1526058346292,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49673502",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":37749862,
        "Question_title":"Azure Machine Learning, 1 web input with multiple outputs?",
        "Question_body":"<p>Im trying to deploy a web app that takes 1 web input, then \"Set Column In Dataset\" a few times for each model , and then sends out a web output for each model.  <\/p>\n\n<p>Right now the way I have it setup is I have a few web inputs, then a model that runs for each, and then a web output for each.  It works for now, but it's a hassle because every time I want to add a new model to be predicted I have to add a bunch of stuff in both azure and my web application.  Just wondering if there is an easier way I'm missing.  <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1465566251803,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":492,
        "Owner_creation_time":1365606882047,
        "Owner_last_access_time":1521227952720,
        "Owner_location":null,
        "Owner_reputation":810,
        "Owner_up_votes":10,
        "Owner_down_votes":1,
        "Owner_views":49,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I am not quite sure I understand the workflow you described. Can you provide more details on what are you trying to accomplish with your web app and your experiment? For example, what do you mean when you say \"I have to add a bunch of stuff\"?<\/p>\n\n<p>Azure ML does support multiple web service inputs and outputs. Adding a new model to the experiment requires you to re-deploy your web service.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1467051292427,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37749862",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":42204411,
        "Question_title":"Why require only R >= 3.3.0?",
        "Question_body":"<p>I'm building a R model in Azure machine learning with a zipped xgboost package attached to the 'execute R script'. <\/p>\n\n<p>Azure machine learning uses R 3.2.2.<\/p>\n\n<p>The model returns with an error saying \"This is R 3.2.2, package 'xgboost' needs >= 3.3.0\".<\/p>\n\n<p>Is there any reason it insists on >= 3.3.0. If not, can I get it down to run with R 3.2.2?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1486989987357,
        "Question_score":0,
        "Question_tags":"r|azure-machine-learning-studio",
        "Question_view_count":106,
        "Owner_creation_time":1351534968770,
        "Owner_last_access_time":1663768904350,
        "Owner_location":null,
        "Owner_reputation":823,
        "Owner_up_votes":16,
        "Owner_down_votes":4,
        "Owner_views":114,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/42204411",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":29906812,
        "Question_title":"forecast package versions different result",
        "Question_body":"<p>I am using R forecast package auto.arima() function, testing it against a predictable sine wave time series. When I run the R code on local machine in R studio, I get a significantly different output to running exactly the same code with the same source data as in azure ML. The only difference I can see is that azure has an older version of forecast package 5.4 whereas i have downloaded the latest version on local machine 5.9. (Interestingly the older version in azure ML correctly forecasts future values, the newer version predicts an attenuating amplitude, which is incorrect). <\/p>\n\n<p>My question then is for anyone who may know why a function's behaviour would change so significantly between package versions, which strikes me as very strange. Or am I missing something here? I am new to both R and azure ML.. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1430172286407,
        "Question_score":0,
        "Question_tags":"r|azure-virtual-machine|azure-machine-learning-studio",
        "Question_view_count":212,
        "Owner_creation_time":1430171674327,
        "Owner_last_access_time":1474914701380,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1483523471852,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/29906812",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":42105549,
        "Question_title":"Azure Machine Learning Studio and OpenCV",
        "Question_body":"<p>Has anyone successfully used the entire opencv library with Azure ML Studio and the Python Module? I know to use a python module that is not included in the ananconda version it must be uploaded in as a zip and from there you can use the entire library. Could someone explain to me exactly what to upload as a zip and then how to access specific functions of the opencv library once uploaded.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1486532576980,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-studio",
        "Question_view_count":2112,
        "Owner_creation_time":1444619451790,
        "Owner_last_access_time":1663806305207,
        "Owner_location":"Sydney NSW, Australia",
        "Owner_reputation":823,
        "Owner_up_votes":98,
        "Owner_down_votes":8,
        "Owner_views":48,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/42105549",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73119660,
        "Question_title":"SAME PARQUET FILE different float values read when using AzureML Dataset vs Directly Getting From Azure Storage using Bytestream",
        "Question_body":"<p>A little background, we are exploring a dataset given by a 3rd party vendor, which they've exposed to us using their SNOWFLAKE instance. What I did then was I exported this dataset into our Azure Blob Storage(s) as parquet files then created an AzureML dataset that encapsulates the parquet files that are stored in our BLOB CONTAINERS.<\/p>\n<p>As I was exploring the data, I raised to them that I saw erroneous data with their longitude and latitude data which they denied. I then sent screenshots of the erroneous data, and when we queried the same data using SNOWFLAKE that I am raising as erroneous, the values were indeed correct!<\/p>\n<p>I was baffled, and upon investigation, I narrowed it down to AzureML dataset somewhat gives INCORRECT data VERSUS if you directly read the parquet file \/ blob via stream into a pandas dataframe.<\/p>\n<p>Also, all text data were identical, but when it came to FLOAT values, the incorrect values are manifesting.<\/p>\n<p>I checked the datatypes when defining the schema of the AzureML dataset, and they are correct<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Gfr2n.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Gfr2n.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Honestly, Im stomped. Any ideas or anyone encountered this issue and can explain to me what is happening? Thank you<\/p>\n<h2><strong>SCREENSHOTS BELOW<\/strong><\/h2>\n<p>Here is when read using the AzureML dataset<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kqN0Y.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kqN0Y.png\" alt=\"Here is when read using the AzureML dataset\" \/><\/a><\/p>\n<hr \/>\n<p>Here is the same file, same record, read directly from blob storage using bytestream<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Bs5Pp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Bs5Pp.png\" alt=\"Here is the same file, same record, read directly from blob storage using bytestream\" \/><\/a><\/p>\n<p>Downloaded the parquet file to my local machine and viewed it<\/p>\n<hr \/>\n<p><a href=\"https:\/\/i.stack.imgur.com\/2UyOl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2UyOl.png\" alt=\"Downloaded the parquet file to my local machine and viewed it\" \/><\/a><\/p>\n<hr \/>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1658822270190,
        "Question_score":1,
        "Question_tags":"pandas|dataframe|azure|parquet|azure-machine-learning-service",
        "Question_view_count":129,
        "Owner_creation_time":1340063804277,
        "Owner_last_access_time":1664031102627,
        "Owner_location":null,
        "Owner_reputation":298,
        "Owner_up_votes":10,
        "Owner_down_votes":1,
        "Owner_views":38,
        "Question_last_edit_time":1658824845596,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73119660",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":68463080,
        "Question_title":"how create azure machine learning scoring image using local package",
        "Question_body":"<p>I have pkl package saved in my azure devops repository<\/p>\n<p>using below code it searches for package in workspace.\nHow to provide package saved in repository<\/p>\n<pre><code> ws = Workspace.get(\n         name=workspace_name,\n         subscription_id=subscription_id,\n        resource_group=resource_group,\n        auth=cli_auth)\n\nimage_config = ContainerImage.image_configuration(\n    execution_script=&quot;score.py&quot;,\n    runtime=&quot;python-slim&quot;,\n    conda_file=&quot;conda.yml&quot;,\n    description=&quot;Image with ridge regression model&quot;,\n    tags={&quot;area&quot;: &quot;ml&quot;, &quot;type&quot;: &quot;dev&quot;},\n)\n\nimage = Image.create(\n    name=image_name,  models=[model], image_config=image_config, workspace=ws\n)\n\nimage.wait_for_creation(show_output=True)\n\nif image.creation_state != &quot;Succeeded&quot;:\n    raise Exception(&quot;Image creation status: {image.creation_state}&quot;)\n\nprint(\n    &quot;{}(v.{} [{}]) stored at {} with build log {}&quot;.format(\n        image.name,\n        image.version,\n        image.creation_state,\n        image.image_location,\n        image.image_build_log_uri,\n    )\n)\n\n# Writing the image details to \/aml_config\/image.json\nimage_json = {}\nimage_json[&quot;image_name&quot;] = image.name\nimage_json[&quot;image_version&quot;] = image.version\nimage_json[&quot;image_location&quot;] = image.image_location\nwith open(&quot;aml_config\/image.json&quot;, &quot;w&quot;) as outfile:\n    json.dump(image_json, outfile)\n<\/code><\/pre>\n<p>I tried to provide path to models but its fails saying package not found<\/p>\n<p>models = $(System.DefaultWorkingDirectory)\/package_model.pkl<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626831896507,
        "Question_score":0,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":140,
        "Owner_creation_time":1567209656790,
        "Owner_last_access_time":1663349187993,
        "Owner_location":null,
        "Owner_reputation":417,
        "Owner_up_votes":53,
        "Owner_down_votes":0,
        "Owner_views":233,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Register model:\nRegister a file or folder as a model by calling <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none-\" rel=\"nofollow noreferrer\">Model.register()<\/a>.<\/p>\n<p>In addition to the content of the model file itself, your registered model will also store model metadata -- model description, tags, and framework information -- that will be useful when managing and deploying models in your workspace. Using tags, for instance, you can categorize your models and apply filters when listing models in your workspace.<\/p>\n<pre><code>model = Model.register(workspace=ws,\n                       model_name='',                # Name of the registered model in your workspace.\n                       model_path='',  # Local file to upload and register as a model.\n                       model_framework=Model.Framework.SCIKITLEARN,  # Framework used to create the model.\n                       model_framework_version=sklearn.__version__,  # Version of scikit-learn used to create the model.\n                       sample_input_dataset=input_dataset,\n                       sample_output_dataset=output_dataset,\n                       resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\n                       description='Ridge regression model to predict diabetes progression.',\n                       tags={'area': 'diabetes', 'type': 'regression'})\n\nprint('Name:', model.name)\nprint('Version:', model.version)\n<\/code><\/pre>\n<p>Deploy machine learning models to Azure: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python<\/a><\/p>\n<p>To Troubleshooting remote model deployment Please follow the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment?tabs=azcli#function-fails-get_model_path\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BL0Nm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BL0Nm.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1627276170736,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1627278873550,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68463080",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":36967126,
        "Question_title":"Why do I get good accuracy with IRIS dataset with a single hidden node?",
        "Question_body":"<p>I have a minimal example of a neural network with a back-propagation trainer, testing it on the IRIS data set. I started of with 7 hidden nodes and it worked well.<\/p>\n\n<p>I lowered the number of nodes in the hidden layer to 1 (expecting it to fail), but was surprised to see that the accuracy went up.<\/p>\n\n<p>I set up the experiment in azure ml, just to validate that it wasn't my code. Same thing there, 98.3333% accuracy with a single hidden node.<\/p>\n\n<p>Can anyone explain to me what is happening here?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1462108714660,
        "Question_score":4,
        "Question_tags":"machine-learning|neural-network|backpropagation|azure-machine-learning-studio",
        "Question_view_count":4488,
        "Owner_creation_time":1426502047333,
        "Owner_last_access_time":1663944347530,
        "Owner_location":null,
        "Owner_reputation":825,
        "Owner_up_votes":46,
        "Owner_down_votes":0,
        "Owner_views":82,
        "Question_last_edit_time":null,
        "Answer_body":"<p>First, it has been well established that a variety of classification models yield incredibly good results on Iris (Iris is very predictable); see <a href=\"http:\/\/lab.fs.uni-lj.si\/lasin\/wp\/IMIT_files\/neural\/doc\/seminar8.pdf\" rel=\"noreferrer\">here<\/a>, for example.<\/p>\n\n<p>Secondly, we can observe that there are relatively few features in the Iris dataset. Moreover, if you look at the <a href=\"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/iris\/iris.names\" rel=\"noreferrer\">dataset description<\/a> you can see that two of the features are very highly correlated with the class outcomes.<\/p>\n\n<p>These correlation values are linear, single-feature correlations, which indicates that one can most likely apply a linear model and observe good results. Neural nets are highly nonlinear; they become more and more complex and capture greater and greater nonlinear feature combinations as the number of hidden nodes and hidden layers is increased.<\/p>\n\n<p>Taking these facts into account, that (a) there are few features to begin with and (b) that there are high linear correlations with class, would all point to a less complex, linear function as being the appropriate predictive model-- by using a single hidden node, you are very nearly using a linear model.<\/p>\n\n<p>It can also be noted that, in the absence of any hidden layer (i.e., just input and output nodes), and when the logistic transfer function is used, this is equivalent to logistic regression.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1462119764790,
        "Answer_score":6.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1462144469996,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36967126",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":66557604,
        "Question_title":"Issues Formatting Azure Cognitive skill set input correctly for ML integration",
        "Question_body":"<p>I am attempting to integrate Azure machine learning with Azure Cognitive Search. I am using a Skill Set and currently the schema for the real-time ML endpoint is this:<\/p>\n<pre><code>&quot;Inputs&quot;: {\n        &quot;WebServiceInput0&quot;:\n        [\n            {\n\n                &quot;CompanyName&quot;: &quot;Apple Inc&quot;,\n            }\n        ]\n    }\n\n<\/code><\/pre>\n<p>When using a debugging session to determine if the skill set setup is correct, I get an error saying that the input data doesn't follow the schema.<\/p>\n<p>This is what the skill set outputs:<\/p>\n<pre><code>&quot;Inputs&quot;: {\n          &quot;WebServiceInput0&quot;: {\n            &quot;CompanyName&quot;: &quot;Apple Inc&quot;,\n          }\n        }\n<\/code><\/pre>\n<p>I have noticed in the schema that the <strong>WebServiceInput0<\/strong> property is an array and therefore this is potentially causing the issue.<\/p>\n<p>I am wondering how I could format the input and add &quot;[&quot; to make <strong>WebServiceInput0<\/strong> an array.<\/p>\n<p>I have tried the Shaper Skill although this wasn't helpful.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1615344173743,
        "Question_score":1,
        "Question_tags":"azure|machine-learning|azure-cognitive-search|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":30,
        "Owner_creation_time":1385945361683,
        "Owner_last_access_time":1628570902717,
        "Owner_location":"Sydney NSW, Australia",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66557604",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":65231534,
        "Question_title":"How to use Pipeline parameters on AzureML",
        "Question_body":"<p>I've built a pipeline on AzureML Designer and I'm trying to use pipeline parameters but I'm not able to get the values of those parameters on a python script module.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-your-first-pipeline\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-your-first-pipeline<\/a>\nThis documentation contains a section called &quot;<em>Use pipeline parameters for arguments that change at inference time&quot;<\/em> but, unfortunately, it is empty.<\/p>\n<p>I'm defining the parameters on the pipeline setting, see the screenshot on the bottom. Does anyone know how to use the parameters when using the Designer to build the pipeline?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/nJTlv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/nJTlv.png\" alt=\"pipeline parameters definition\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1607591444673,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":2215,
        "Owner_creation_time":1536515516827,
        "Owner_last_access_time":1663940912323,
        "Owner_location":null,
        "Owner_reputation":85,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1607679025792,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65231534",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67533091,
        "Question_title":"where are registered models in azure machine learning",
        "Question_body":"<p>I try to use azuremlsdk to deploy a locally trained model (a perfectly valid use case AFIK). I follow <a href=\"https:\/\/cran.r-project.org\/web\/packages\/azuremlsdk\/vignettes\/train-and-deploy-first-model.html\" rel=\"nofollow noreferrer\">this<\/a> and managed to create a ML workspace and register a &quot;model&quot; like so:<\/p>\n<pre><code>library(azuremlsdk)\n\ninteractive_auth &lt;- interactive_login_authentication(tenant_id=&quot;xxx&quot;)\nws &lt;- get_workspace(\n        name = &quot;xxx&quot;, \n        subscription_id = &quot;xxx&quot;, \n        resource_group =&quot;xxx&quot;, \n        auth = interactive_auth\n)\n\nadd &lt;- function(a, b) {\n    return(a + b)\n}\n\nadd(1,2)\n\nsaveRDS(add, file = &quot;D:\/add.rds&quot;)\n\nmodel &lt;- register_model(\n    ws, \n    model_path = &quot;D:\/add.rds&quot;, \n    model_name = &quot;add_model&quot;,\n    description = &quot;An amazing model&quot;\n)\n<\/code><\/pre>\n<p>This seemed to work fine, as I get some nice log messages telling me that the model was registered. For my sanity, I wonder where can I find this registered (&quot;materialised&quot;) model\/object\/function in the Azure UI please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620989367110,
        "Question_score":1,
        "Question_tags":"r|azure-machine-learning-service|azuremlsdk",
        "Question_view_count":41,
        "Owner_creation_time":1267440784443,
        "Owner_last_access_time":1664045779313,
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Question_last_edit_time":null,
        "Answer_body":"<p>On ml.azure.com, there is a &quot;Models&quot; option on the left-hand blade.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Y7cZe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Y7cZe.png\" alt=\"UI Sidebar\" \/><\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1621001633740,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67533091",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":28205136,
        "Question_title":"Does Microsoft Azure Machine Learning use Hadoop as its underlying layer?",
        "Question_body":"<p>I'm going to use Microsoft Azure ML for some text analysis purposes such as keyword extraction and as the size of my input is big I want to know whether ML package actually uses the Hadoop (HDP) as its underlying layer or not? If not, how can I use the ML in combination with Hadoop?<\/p>\n\n<p>Does Mahout have some text analysis tools?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1422491951960,
        "Question_score":1,
        "Question_tags":"java|azure|mahout|azure-machine-learning-studio",
        "Question_view_count":660,
        "Owner_creation_time":1355343131933,
        "Owner_last_access_time":1649125560750,
        "Owner_location":null,
        "Owner_reputation":5655,
        "Owner_up_votes":73,
        "Owner_down_votes":3,
        "Owner_views":629,
        "Question_last_edit_time":1642755247409,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/28205136",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":68912185,
        "Question_title":"Could not find member 'intellectualPropertyPublisher' on object of type 'JobProperties' in Azure Machine Learning Designer",
        "Question_body":"<p>From today (Aug 24th, 2021) I'm receiving the following error message when submit any operation in Azure Machine Learning Designer with a dataset:<\/p>\n<p><strong>Could not find member 'intellectualPropertyPublisher' on object of type 'JobProperties'<\/strong><\/p>\n<p>Complete error message:<\/p>\n<p><em>UserError: Job submission to AzureML Compute encountered an Exception with status code , Could not find member 'intellectualPropertyPublisher' on object of type 'JobProperties'. Path 'properties.intellectualPropertyPublisher', line 309, position 36.<\/em><\/p>\n<p>I'm seeing there's new items in user interface, maybe could be an updating error?\nSomeone is receiving something that?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1629828602630,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":30,
        "Owner_creation_time":1629828242253,
        "Owner_last_access_time":1653671859740,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1630364458667,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68912185",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73178409,
        "Question_title":"How to get absolute path to \"outputs\" folder in Azure ML",
        "Question_body":"<p>In the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-save-write-experiment-files\" rel=\"nofollow noreferrer\">documentation<\/a> of Azure Machine Learning, under &quot;Where to write files&quot;, it says<\/p>\n<blockquote>\n<p>Otherwise, write files to the <code>.\/outputs<\/code> and\/or <code>.\/logs<\/code> folder.<\/p>\n<\/blockquote>\n<p>These are relative paths, i.e. relative to the folder where my script is run by the Azure ML framework. I was not able to find a function in the Azure ML SDK that would return the absolute path -- have I missed it or is there none? (Meaning that I should read the <code>cwd<\/code> at the beginning of my script and store it myself.)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659208299903,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":111,
        "Owner_creation_time":1248371469417,
        "Owner_last_access_time":1664058388787,
        "Owner_location":null,
        "Owner_reputation":7506,
        "Owner_up_votes":752,
        "Owner_down_votes":11,
        "Owner_views":360,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73178409",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62571695,
        "Question_title":"AzureML Model.profile() timeout without ever running the model",
        "Question_body":"<p>When trying to profile our AzureML model we run into a timeout. According to the log statements, <strong>the model is initialized<\/strong> but the <strong>run function is never called<\/strong>.<\/p>\n<p>The dataset provided contains one column (&quot;profile_requests&quot;) and 100 samples of serialized json that the model usually consumes. Both model and score.py work fine when deploying via Model.deploy (see below).<\/p>\n<p>Why is the run function never called?<\/p>\n<pre><code>profile = Model.profile(ws, \n    profile_name='my-profile-name',\n    models=[latest_model], \n    inference_config=InferenceConfig(\n                                    entry_script='score.py', \n                                    source_directory=&quot;deployment&quot;,\n                                    environment=Environment.get(ws, &quot;my_environment&quot;)), \n    input_dataset=processed_dataset,\n    cpu=2,\n    memory_in_gb=3)\n<\/code><\/pre>\n<pre><code>{...,\n 'error': {'code': 'ModelTestTimeOut',\n  'statusCode': 500,\n  'message': 'The model did not finish the test within the allowed time: 30 min. Error logs URL: https:\/\/link-to-logfile',\n  'details': []},\n 'errorLogsUri': 'https:\/\/link-to-logfile'}\n<\/code><\/pre>\n<p>Profiling log file:<\/p>\n<pre><code>==========Logs from model deployed to container with 2 cpu and 3 GB memory==========\n[...]\nInvoking user's init function\nModel loaded.\nUsers's init has completed successfully\nScoring timeout setting is not found. Use default timeout: 3600000 ms\n<\/code><\/pre>\n<p>One sample from the DataFrame (<code>sample_event=processed_dataset.to_pandas_dataframe().loc[0,&quot;profile_requests&quot;]<\/code>)<\/p>\n<pre><code>'{&quot;allevents&quot;: [{&quot;temperature&quot;: 103.76252626686478, &quot;ambienttemperature&quot;: 20.763083531178108, &quot;timeCreated&quot;: &quot;2020-04-02T12:07:02&quot;, &quot;ConnectionDeviceId&quot;: &quot;milkbottleEdge&quot;, &quot;ConnectionDeviceGenerationId&quot;: &quot;637211838651873534&quot;}, {&quot;temperature&quot;: 104.00700291712167, &quot;ambienttemperature&quot;: 20.77088671236806, &quot;timeCreated&quot;: &quot;2020-04-02T12:07:07&quot;, &quot;ConnectionDeviceId&quot;: &quot;milkbottleEdge&quot;, &quot;ConnectionDeviceGenerationId&quot;: &quot;637211838651873534&quot;}, {&quot;temperature&quot;: 103.73538818128196, &quot;ambienttemperature&quot;: 20.927115571418366, &quot;timeCreated&quot;: &quot;2020-04-02T12:07:12&quot;, &quot;ConnectionDeviceId&quot;: &quot;milkbottleEdge&quot;, &quot;ConnectionDeviceGenerationId&quot;: &quot;637211838651873534&quot;}, {&quot;temperature&quot;: 103.26993925975171, &quot;ambienttemperature&quot;: 20.977784248757075, &quot;timeCreated&quot;: &quot;2020-04-02T12:07:17&quot;, &quot;ConnectionDeviceId&quot;: &quot;milkbottleEdge&quot;, &quot;ConnectionDeviceGenerationId&quot;: &quot;637211838651873534&quot;}, {&quot;temperature&quot;: 103.47584351197627, &quot;ambienttemperature&quot;: 20.528207412934027, &quot;timeCreated&quot;: &quot;2020-04-02T12:07:22&quot;, &quot;ConnectionDeviceId&quot;: &quot;milkbottleEdge&quot;, &quot;ConnectionDeviceGenerationId&quot;: &quot;637211838651873534&quot;}, {&quot;temperature&quot;: 103.53497833736942, &quot;ambienttemperature&quot;: 21.176729435416277, &quot;timeCreated&quot;: &quot;2020-04-02T12:07:27&quot;, &quot;ConnectionDeviceId&quot;: &quot;milkbottleEdge&quot;, &quot;ConnectionDeviceGenerationId&quot;: &quot;637211838651873534&quot;}, {&quot;temperature&quot;: 103.33621482217512, &quot;ambienttemperature&quot;: 21.083552645791112, &quot;timeCreated&quot;: &quot;2020-04-02T12:07:32&quot;, &quot;ConnectionDeviceId&quot;: &quot;milkbottleEdge&quot;, &quot;ConnectionDeviceGenerationId&quot;: &quot;637211838651873534&quot;}, {&quot;temperature&quot;: 103.13542993745558, &quot;ambienttemperature&quot;: 20.80351544511668, &quot;timeCreated&quot;: &quot;2020-04-02T12:07:37&quot;, &quot;ConnectionDeviceId&quot;: &quot;milkbottleEdge&quot;, &quot;ConnectionDeviceGenerationId&quot;: &quot;637211838651873534&quot;}, {&quot;temperature&quot;: 103.45331951321728, &quot;ambienttemperature&quot;: 21.404335822865523, &quot;timeCreated&quot;: &quot;2020-04-02T12:07:42&quot;, &quot;ConnectionDeviceId&quot;: &quot;milkbottleEdge&quot;, &quot;ConnectionDeviceGenerationId&quot;: &quot;637211838651873534&quot;}, {&quot;temperature&quot;: 103.52506972734126, &quot;ambienttemperature&quot;: 20.51882900857312, &quot;timeCreated&quot;: &quot;2020-04-02T12:07:47&quot;, &quot;ConnectionDeviceId&quot;: &quot;milkbottleEdge&quot;, &quot;ConnectionDeviceGenerationId&quot;: &quot;637211838651873534&quot;}, {&quot;temperature&quot;: 103.61395300883524, &quot;ambienttemperature&quot;: 21.110307039511532, &quot;timeCreated&quot;: &quot;2020-04-02T12:07:52&quot;, &quot;ConnectionDeviceId&quot;: &quot;milkbottleEdge&quot;, &quot;ConnectionDeviceGenerationId&quot;: &quot;637211838651873534&quot;}, {&quot;temperature&quot;: 103.49871551548077, &quot;ambienttemperature&quot;: 21.133206947070178, &quot;timeCreated&quot;: &quot;2020-04-02T12:07:57&quot;, &quot;ConnectionDeviceId&quot;: &quot;milkbottleEdge&quot;, &quot;ConnectionDeviceGenerationId&quot;: &quot;637211838651873534&quot;}]}'\n<\/code><\/pre>\n<p>Running the sample against the deployed webservice that uses the same environment and score.py <code>service.run(sample_event)<\/code> works as expected.<\/p>\n<pre><code>{'result': False,\n 'ConnectionDeviceId': 'milkbottleEdge',\n 'timeCreatedStart': '2020-04-02T12:07:02',\n 'timeCreatedEnd': '2020-04-02T12:07:57',\n 'hasError': False,\n 'errorMessage': None}\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1593075784063,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":176,
        "Owner_creation_time":1584715327427,
        "Owner_last_access_time":1629377918267,
        "Owner_location":"Hamburg, Germany",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62571695",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73761872,
        "Question_title":"Azure ML Studio Docker Endpoint Deployment Error 400 Client Error",
        "Question_body":"<p>I would like to deploy an ML endpoint (first locally to see the error messages) from custom environment. But I am getting the following error message. Can anybody clarify what does this stands for?<\/p>\n<p>The environment deployment is fine, I can pull and play with the environment image, the the ML model also works as it expected, but in the endpoint development I fail.<\/p>\n<pre><code>from azureml.core.model import InferenceConfig, Model\nfrom azureml.core.webservice import AciWebservice, LocalWebservice\n\nocr_model = Model(name='OCR_model', workspace=workspace)\nquality_model = Model(name='Quality_model', workspace=workspace)\ninference_config = InferenceConfig(entry_script=&quot;ml_scores.py&quot;, environment=python_env)\ndeployment_config = LocalWebservice.deploy_configuration(port=6789)\n\nservice = Model.deploy(workspace=workspace,\n                       name=&quot;ml-service-v2&quot;,\n                       overwrite=True,\n                       models=[ocr_model, quality_model],\n                       inference_config=inference_config,\n                       deployment_config=deployment_config)\n\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n<p>The error message:<\/p>\n<pre><code>Warning, custom base image or base dockerfile detected without a specified `inferencing_stack_version`. Please set environment.inferencing_stack_version='latest'\nWarning, custom base image or base dockerfile detected without a specified `inferencing_stack_version`. Please set environment.inferencing_stack_version='latest'\nDocker container start has failed:\n400 Client Error for http+docker:\/\/localhost\/v1.41\/containers\/f4ee0f109af60a21a11683dd3d69db87b6371b2c3b038ea0227a8cc8422c4200\/start: Bad Request (&quot;failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: &quot;runsvdir&quot;: executable file not found in $PATH: unknown&quot;)\n\nDownloading model OCR_model:42 to \/tmp\/azureml_25emn1le\/OCR_model\/42\nDownloading model Quality_model:40 to \/tmp\/azureml_25emn1le\/Quality_model\/40\nGenerating Docker build context.\nPackage creation Succeeded\nLogging into Docker registry 088e52be0d7942c4a6e11258fa37a140.azurecr.io\nLogging into Docker registry 088e52be0d7942c4a6e11258fa37a140.azurecr.io\nBuilding Docker image from Dockerfile...\nStep 1\/5 : FROM 088e52be0d7942c4a6e11258fa37a140.azurecr.io\/azureml\/azureml_152ffef9bfdf00715d63c8352ff2cc66\n ---&gt; 31b97aac1e18\nStep 2\/5 : COPY azureml-app \/var\/azureml-app\n ---&gt; a31c2d80deb2\nStep 3\/5 : RUN mkdir -p '\/var\/azureml-app' &amp;&amp; echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjIwMjQ0MjYwLWQyY2ItNDNkMy05NjZlLWQwZjcyMDE3ZWJhMCIsInJlc291cmNlR3JvdXBOYW1lIjoibWxfYmV0YV9yZyIsImFjY291bnROYW1lIjoibWFjaGluZS1sZWFybmluZy1iZXRhLXdzIiwid29ya3NwYWNlSWQiOiIwODhlNTJiZS0wZDc5LTQyYzQtYTZlMS0xMjU4ZmEzN2ExNDAifSwibW9kZWxzIjp7fSwibW9kZWxzSW5mbyI6e319 | base64 --decode &gt; \/var\/azureml-app\/model_config_map.json\n ---&gt; Running in 783f521ea037\n ---&gt; 8900434882b2\nStep 4\/5 : RUN mv '\/var\/azureml-app\/tmpdswbb3xk.py' \/var\/azureml-app\/main.py\n ---&gt; Running in 8b61d3f35a90\n ---&gt; 05890493cdd1\nStep 5\/5 : CMD [&quot;runsvdir&quot;,&quot;\/var\/runit&quot;]\n ---&gt; Running in 052ae96b5d75\n ---&gt; b15b8dfb9deb\nSuccessfully built b15b8dfb9deb\nSuccessfully tagged ml-service-v2:latest\nContainer (name:xenodochial_goldwasser, id:2757f295fa1836dd83d2dcb162ccbb566c04cba1f3fb0c35943f5d4d101d5479) cannot be killed.\nContainer has been successfully cleaned up.\nImage sha256:ed2b4e50c397597d857883a76e5d53a5c27ce700894d3b2f7167d477f7b82be2 successfully removed.\nStarting Docker container...\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/docker\/api\/client.py:268, in APIClient._raise_for_status(self, response)\n    267 try:\n--&gt; 268     response.raise_for_status()\n    269 except requests.exceptions.HTTPError as e:\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/requests\/models.py:1021, in Response.raise_for_status(self)\n   1020 if http_error_msg:\n-&gt; 1021     raise HTTPError(http_error_msg, response=self)\n\nHTTPError: 400 Client Error: Bad Request for url: http+docker:\/\/localhost\/v1.41\/containers\/f4ee0f109af60a21a11683dd3d69db87b6371b2c3b038ea0227a8cc8422c4200\/start\n\nDuring handling of the above exception, another exception occurred:\n\nAPIError                                  Traceback (most recent call last)\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/_model_management\/_util.py:495, in start_docker_container(container)\n    493 print(&quot;Starting Docker container...&quot;)\n--&gt; 495 container.start()\n    497 print('Docker container running.')\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/docker\/models\/containers.py:404, in Container.start(self, **kwargs)\n    396 &quot;&quot;&quot;\n    397 Start this container. Similar to the ``docker start`` command, but\n    398 doesn't support attach options.\n   (...)\n    402         If the server returns an error.\n    403 &quot;&quot;&quot;\n--&gt; 404 return self.client.api.start(self.id, **kwargs)\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/docker\/utils\/decorators.py:19, in check_resource.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapped(self, resource_id, *args, **kwargs)\n     16     raise errors.NullResource(\n     17         'Resource ID was not provided'\n     18     )\n---&gt; 19 return f(self, resource_id, *args, **kwargs)\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/docker\/api\/container.py:1109, in ContainerApiMixin.start(self, container, *args, **kwargs)\n   1108 res = self._post(url)\n-&gt; 1109 self._raise_for_status(res)\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/docker\/api\/client.py:270, in APIClient._raise_for_status(self, response)\n    269 except requests.exceptions.HTTPError as e:\n--&gt; 270     raise create_api_error_from_http_exception(e)\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/docker\/errors.py:31, in create_api_error_from_http_exception(e)\n     30         cls = NotFound\n---&gt; 31 raise cls(e, response=response, explanation=explanation)\n\nAPIError: 400 Client Error for http+docker:\/\/localhost\/v1.41\/containers\/f4ee0f109af60a21a11683dd3d69db87b6371b2c3b038ea0227a8cc8422c4200\/start: Bad Request (&quot;failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: &quot;runsvdir&quot;: executable file not found in $PATH: unknown&quot;)\n\nDuring handling of the above exception, another exception occurred:\n\nWebserviceException                       Traceback (most recent call last)\nInput In [20], in &lt;cell line: 12&gt;()\n      9 # deployment_config = AciWebservice.deploy_configuration(cpu_cores=2, memory_gb=4)\n     10 deployment_config = LocalWebservice.deploy_configuration(port=6789)\n---&gt; 12 service = Model.deploy(workspace=workspace,\n     13                        name=&quot;ml-service-v2&quot;,\n     14                        overwrite=True,\n     15                        models=[ocr_model, quality_model],\n     16                        inference_config=inference_config,\n     17                        deployment_config=deployment_config)\n     19 service.wait_for_deployment(show_output=True)\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/core\/model.py:1649, in Model.deploy(workspace, name, models, inference_config, deployment_config, deployment_target, overwrite, show_output)\n   1647 # Local webservice.\n   1648 if deployment_config and isinstance(deployment_config, LocalWebserviceDeploymentConfiguration):\n-&gt; 1649     return deployment_config._webservice_type._deploy(workspace, name, models,\n   1650                                                       inference_config=inference_config,\n   1651                                                       deployment_config=deployment_config)\n   1653 # IotWebservice does not support environment-style deployment,\n   1654 # so make sure we don't deploy IotWebservice with environment;\n   1655 # We only support ACI, AKS, AKS endpoint, and MIR for now.\n   1656 from azureml._model_management._constants import IOT_WEBSERVICE_TYPE\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/core\/webservice\/local.py:739, in LocalWebservice._deploy(workspace, name, models, image_config, deployment_config, wait, inference_config)\n    735     raise WebserviceException('Error, provided inference configuration must be of type InferenceConfig '\n    736                               'in order to deploy a local service.', logger=module_logger)\n    738 service = LocalWebservice(workspace, name, must_exist=False)\n--&gt; 739 service.update(models=models,\n    740                image_config=image_config,\n    741                inference_config=inference_config,\n    742                deployment_config=deployment_config,\n    743                wait=wait)\n    744 return service\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/core\/webservice\/local.py:72, in _in_state.&lt;locals&gt;.decorator.&lt;locals&gt;.decorated(self, *args, **kwargs)\n     69 if self.state not in states:\n     70     raise WebserviceException('Cannot call {}() when service is {}.'.format(func.__name__, self.state),\n     71                               logger=module_logger)\n---&gt; 72 return func(self, *args, **kwargs)\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/core\/webservice\/local.py:549, in LocalWebservice.update(self, models, image_config, deployment_config, wait, inference_config)\n    547 self._generate_docker_context()\n    548 self._build_image()\n--&gt; 549 self._run_container(wait=wait)\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/core\/webservice\/local.py:1021, in LocalWebservice._run_container(self, wait)\n   1018         LocalWebservice._copy_local_asset(container, self._inference_config.entry_script)\n   1020 # Engage!\n-&gt; 1021 start_docker_container(container)\n   1023 # Record the state of the deployment.\n   1024 self._container = container\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/_model_management\/_util.py:503, in start_docker_container(container)\n    500     raise WebserviceException('Docker container start has failed, the port you are attempting to use '\n    501                               'is already in use:\\n{}'.format(e), logger=module_logger)\n    502 else:\n--&gt; 503     raise WebserviceException('Docker container start has failed:\\n{}'.format(e), logger=module_logger)\n\nWebserviceException: WebserviceException:\n    Message: Docker container start has failed:\n400 Client Error for http+docker:\/\/localhost\/v1.41\/containers\/f4ee0f109af60a21a11683dd3d69db87b6371b2c3b038ea0227a8cc8422c4200\/start: Bad Request (&quot;failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: &quot;runsvdir&quot;: executable file not found in $PATH: unknown&quot;)\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;Docker container start has failed:\\n400 Client Error for http+docker:\/\/localhost\/v1.41\/containers\/f4ee0f109af60a21a11683dd3d69db87b6371b2c3b038ea0227a8cc8422c4200\/start: Bad Request (\\&quot;failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: \\&quot;runsvdir\\&quot;: executable file not found in $PATH: unknown\\&quot;)&quot;\n    }\n}\n<\/code><\/pre>\n<p>Thanks for any kind of guidance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663496243223,
        "Question_score":1,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":53,
        "Owner_creation_time":1663495768960,
        "Owner_last_access_time":1663888717353,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73761872",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":33708725,
        "Question_title":"Azure Machine learning: error with multiclass classification algo",
        "Question_body":"<p>I have <a href=\"https:\/\/yadi.sk\/i\/--Vkm7FTkTAxY\" rel=\"nofollow noreferrer\">training set<\/a> and <a href=\"https:\/\/yadi.sk\/i\/wYu0ZsmukTAw3\" rel=\"nofollow noreferrer\">test set<\/a> (csv files with header), in which I have to classify each value. There is 118.000 uniq values in X column, and only about 13000 in y1 column, so there will be 13000 categories.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Qc1i8.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Qc1i8.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>From Training set I need only <code>X<\/code> and <code>y1<\/code> column to train model. I need to classify X value to one of categories (find normal from of initial word). I tried all multi algo but failed trying to evaluate model.<\/p>\n\n<p>Visualizing Score model return this:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/EnDZq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EnDZq.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>What can be a problem, it just returns -2 code as error and this <a href=\"https:\/\/yadi.sk\/i\/I6WiEoCGkTCXc\" rel=\"nofollow noreferrer\">log<\/a><\/p>\n\n<p>UPD1: By Metadata Editor module under Project Column Module made column y1  as categorical,  nothing seems to be changed<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1447506491170,
        "Question_score":0,
        "Question_tags":"machine-learning|azure-machine-learning-studio",
        "Question_view_count":75,
        "Owner_creation_time":1349985908313,
        "Owner_last_access_time":1654028256077,
        "Owner_location":"Moscow, Russia",
        "Owner_reputation":4886,
        "Owner_up_votes":1215,
        "Owner_down_votes":15,
        "Owner_views":1228,
        "Question_last_edit_time":1454300637020,
        "Answer_body":"<p><strong>Moncef<\/strong> provided <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/aabb9080-adf8-4fc0-b332-2a3c8ee29a28\/azure-machine-learning-error-with-multiclass-classification-algo?forum=MachineLearning\" rel=\"nofollow\">here<\/a> the solution to my problem:<\/p>\n\n<p>The point is that Azure has limitations on maximum categories 8192, this is why the number should be decreased by R or python modules or own evaluation module may be created. Or there is another way, evaluation step may be skipped, because model`ve been trained successfully. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1448271863192,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/33708725",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67536581,
        "Question_title":"conda dependencies file r model in azure machine learning",
        "Question_body":"<p>I understand what the entry script\/scoring script is and does. See <a href=\"https:\/\/azure.github.io\/azureml-sdk-for-r\/articles\/deploying-models.html\" rel=\"nofollow noreferrer\">here<\/a> as an example. As I struggle to expose my deployed model via code as described <a href=\"https:\/\/azure.github.io\/azureml-sdk-for-r\/articles\/train-and-deploy-first-model.html\" rel=\"nofollow noreferrer\">here<\/a> (see also <a href=\"https:\/\/stackoverflow.com\/questions\/67535014\/deploy-model-to-azure-machine-learning-via-azuremlsdk\">here<\/a>), I am trying to use the UI ml.azure.com instead. I am a bit puzzled by the mandatory dependency: conda dependencies file:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BWtsE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BWtsE.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I have an R model but clearly this is a Python thing. What shall I use in this case?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1621004713270,
        "Question_score":1,
        "Question_tags":"r|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":609,
        "Owner_creation_time":1267440784443,
        "Owner_last_access_time":1664045779313,
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Question_last_edit_time":null,
        "Answer_body":"<p>conda is actually not just a Python thing, you might be thinking of pip?<\/p>\n<p>Conda is a package &amp; environment manager for nearly any kind of package, provided that it has been uploaded to anaconda. So you <em>can<\/em> use anaconda (and conda environment files) for R projects.<\/p>\n<p>The trouble is that the <code>azuremlsdk<\/code> CRAN package is not hosted as an anaconda package, but is probably needed for the scoring service. Worth using a file like below to see what it works.<\/p>\n<p>If it doesn't work, then I agree that this UI needs to generalized to better support R model deployment scenarios.<\/p>\n<p>It is also possible to add the <code>azuremlsdk<\/code> CRAN package to anaconda, but that requires <a href=\"https:\/\/stackoverflow.com\/a\/36653411\/3842610\">some extra work<\/a>, but ideally you shouldn't have to require this much manual effort.<\/p>\n<code>environment.yml<\/code>\n<p>Here's an example conda dependencies file for R.<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: scoring_environment\nchannels:\n  - defaults\ndependencies:\n  - r-base=3.6.1\n  - r-essentials=3.6.0\n  # whatever other dependencies you have\n  - r-tidyverse=1.2.1\n  - r-caret=6.0_83\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1621007147567,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67536581",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64139290,
        "Question_title":"How can I mark an Azure Dataset as a time series dataset reading from a parquet folder with date partitions?",
        "Question_body":"<p>I would like to create a Time series dataset from a folder that contains parquet files this way:<\/p>\n<ul>\n<li>timestamp=2018-01-06<\/li>\n<li>timestamp=2018-01-07<\/li>\n<\/ul>\n<p>How can I make Azure Dataset, through the GUI, recognises the timestamp partition as a date and mark my dataset as a time series dataset?<\/p>\n<p>It is supposed to be automatic, but it doesn't work.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1601474658457,
        "Question_score":2,
        "Question_tags":"parquet|azure-machine-learning-studio",
        "Question_view_count":109,
        "Owner_creation_time":1423640080283,
        "Owner_last_access_time":1663943557963,
        "Owner_location":"Lyon, France",
        "Owner_reputation":457,
        "Owner_up_votes":19,
        "Owner_down_votes":2,
        "Owner_views":125,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Thanks for reaching out to us.<\/p>\n<p>In Azure Machine Learning Studio, you would need to setup partition format similar to python SDK, as follows, assuming your data path is &quot;timeseries\/timestamp=2020-01-01\/data.parquet&quot;:\n<a href=\"https:\/\/i.stack.imgur.com\/HwYfF.png\" rel=\"nofollow noreferrer\">Set up partition format when creating time series dataset<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1601920685456,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64139290",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70268372,
        "Question_title":"How to adjust feature importance in Azure AutoML",
        "Question_body":"<p>I am hoping to have some <strong>low code model<\/strong> using Azure AutoML, which is really just going to the AutoML tab, running a classification experiment with my dataset, after it's done, I deploy the best selected model.<\/p>\n<p>The model kinda works (meaning, I publish the endpoint and then I do some manual validation, seems accurate), however, I am not confident enough, because when I am looking at the explanation, I can see something like this:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qM51x.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qM51x.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>4 top features are not really closely important. The most &quot;important&quot; one is really not the one I prefer it to use. I am hoping it will use the <code>Title<\/code> feature more.<\/p>\n<p>Is there such a thing I can adjust the importance of individual features, like ranking all features before it starts the experiment?<\/p>\n<p>I would love to do more reading, but I only found this:<\/p>\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/52484233\/increase-feature-importance\">Increase feature importance<\/a><\/p>\n<p>The only answer seems to be about how to measure if a feature is important.<\/p>\n<p>Hence, does it mean, if I want to customize the experiment, such as selecting which features to &quot;focus&quot;, I should learn how to use the &quot;designer&quot; part in Azure ML? Or is it something I can't do, even with the designer. I guess my confusion is, with ML being such a big topic, I am looking for a direction of learning, in this case of what I am having, so I can improve my current model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638921326880,
        "Question_score":2,
        "Question_tags":"machine-learning|azure-machine-learning-studio|azure-machine-learning-service|azure-auto-ml",
        "Question_view_count":119,
        "Owner_creation_time":1296571549840,
        "Owner_last_access_time":1663809103810,
        "Owner_location":"Seattle, WA",
        "Owner_reputation":1021,
        "Owner_up_votes":79,
        "Owner_down_votes":7,
        "Owner_views":138,
        "Question_last_edit_time":1638925038329,
        "Answer_body":"<p>Here is <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-automated-ml#customize-featurization\" rel=\"nofollow noreferrer\">link<\/a> to the document for feature customization.<\/p>\n<p>Using the SDK you can specify &quot;feauturization&quot;: 'auto' \/ 'off' \/ 'FeaturizationConfig' in your <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py\" rel=\"nofollow noreferrer\">AutoMLConfig<\/a> object. Learn more about <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-features\" rel=\"nofollow noreferrer\">enabling featurization<\/a>.<\/p>\n<p>Automated ML tries out different ML models that have different settings which control for overfitting.  Automated ML will pick which overfitting parameter configuration is best based on the best score (e.g. accuracy) it gets from hold-out data.  The kind of overfitting settings these models has includes:<\/p>\n<ul>\n<li>Explicitly penalizing overly-complex models in the loss function that the ML model is optimizing<\/li>\n<li>Limiting model complexity before training, for example by limiting the size of trees in an ensemble tree learning model (e.g. gradient boosting trees or random forest)<\/li>\n<\/ul>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1641210936729,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70268372",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":36106614,
        "Question_title":"Push parameters from powerBi to Azure ML",
        "Question_body":"<p>I am planning to build a model in Azure Ml and there are certain parameters that needs to passed to the model ,before the model could be run.These parameters should come from PowerBi,maybe based on some filters . Is it possible ?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1458418332563,
        "Question_score":0,
        "Question_tags":"azure|powerbi|azure-machine-learning-studio",
        "Question_view_count":102,
        "Owner_creation_time":1422821109973,
        "Owner_last_access_time":1664007162760,
        "Owner_location":"India",
        "Owner_reputation":1238,
        "Owner_up_votes":45,
        "Owner_down_votes":5,
        "Owner_views":172,
        "Question_last_edit_time":1458425260707,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36106614",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72878787,
        "Question_title":"How to authenticate within Azure ML pipeline and avoid browser interactive authentication?",
        "Question_body":"<p>I have created a very simple azure ml pipeline. Basically, it accesses data through an api and prints it.\nI have tried using a ClientSecretCredential and a ServicePrincipalAuthentication but the pipeline still asks me for a web browser authentication to continue (even though the script runs when I just run the python file in the terminal)<\/p>\n<pre><code>    from azureml.core.compute import ComputeTarget, AmlCompute\n    import azureml.core\n    from azureml.core import Workspace, Datastore\n    from azure.identity import ClientSecretCredential\n    from azureml.core.authentication import ServicePrincipalAuthentication\n    \n    \n    ws = Workspace.from_config()\n    \n    keyvault = ws.get_default_keyvault()\n    \n    client_id = keyvault.get_secret(&quot;ClientID&quot;)\n    tenant_id = keyvault.get_secret(&quot;TENANTID&quot;)\n    secret_id = keyvault.get_secret(&quot;CLIENTSECRET&quot;)\n    \n    sp = ClientSecretCredential(tenant_id = tenant_id, client_id = client_id, client_secret = secret_id)\n\n#  sp = ServicePrincipalAuthentication(tenant_id, client_id, secret_id)\n<\/code><\/pre>\n<p>This is the output from the pipeline:<\/p>\n<p>2022-07-06 06:08:21,496|azureml._vendor.azure_cli_core._session|INFO|Failed to load or parse file \/root\/.azureml\/auth\/azureProfile.json. It will be overridden by default settings.\n2022-07-06 06:08:21,496|azureml._vendor.azure_cli_core._session|INFO|Failed to load or parse file \/root\/.azureml\/auth\/az.json. It will be overridden by default settings.\n2022-07-06 06:08:21,496|azureml._vendor.azure_cli_core._session|INFO|Failed to load or parse file \/root\/.azureml\/auth\/az.sess. It will be overridden by default settings.\n2022-07-06 06:08:21,498|azureml._vendor.azure_cli_core|DEBUG|Current cloud config:\nAzureCloud\n2022-07-06 06:08:21,501|azureml._vendor.azure_cli_core|DEBUG|Current cloud config:\nAzureCloud\n2022-07-06 06:08:21,527|azureml._vendor.azure_cli_core._profile|INFO|No web browser is available. Fall back to device code.\n<strong>2022-07-06 06:08:21,628|azureml._vendor.azure_cli_core.auth.identity|WARNING|To sign in, use a web browser to open the page <a href=\"https:\/\/microsoft.com\/devicelogin\" rel=\"nofollow noreferrer\">https:\/\/microsoft.com\/devicelogin<\/a> and enter the code XXXXXXXXXX to authenticate.<\/strong>\n2022-07-06 06:08:49,950|azureml.core.authentication|DEBUG|Time to expire 1814345.049124 seconds<\/p>\n<p>What can I do for it to pick up the credential in the code and avoid asking for interactive authentication?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657088789413,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-service|azuremlsdk",
        "Question_view_count":67,
        "Owner_creation_time":1516041506383,
        "Owner_last_access_time":1663922766340,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":21,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72878787",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71407308,
        "Question_title":"Azure ML model to a container instance, call to the model fails when using the code provided in the \"Consume\" section of the endpoint (Python and C#)",
        "Question_body":"<p>After deploying an Azure ML model to a container instance, call to the model fails when using the code provided in the &quot;Consume&quot; section of the endpoint (Python and C#).<\/p>\n<p>I have trained a model in Azure Auto-ML and deployed the model to a container instance.<\/p>\n<p><strong>Now when I am try to use the Python code provided in the Endpoint's &quot;Consume&quot; section I get the following error:<\/strong><\/p>\n<pre><code>The request failed with status code: 502\nContent-Length: 55\nContent-Type: text\/html; charset=utf-8\nDate: Mon, 07 Mar 2022 12:32:07 GMT\nServer: nginx\/1.14.0 (Ubuntu)\nX-Ms-Request-Id: 768c2eb5-10f3-4e8a-9412-3fcfc0f6d648\nX-Ms-Run-Function-Failed: True\nConnection: close\n\n---------------------------------------------------------------------------\nJSONDecodeError Traceback (most recent call last)\n&lt;ipython-input-1-6eeff158e915&gt; in &lt;module&gt;\n48 # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n49 print(error.info())\n---&gt; 50 print(json.loads(error.read().decode(&quot;utf8&quot;, 'ignore')))\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/json\/init.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n352 parse_int is None and parse_float is None and\n353 parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 354 return _default_decoder.decode(s)\n355 if cls is None:\n356 cls = JSONDecoder\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/json\/decoder.py in decode(self, s, _w)\n337\n338 &quot;&quot;&quot;\n--&gt; 339 obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n340 end = _w(s, end).end()\n341 if end != len(s):\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/json\/decoder.py in raw_decode(self, s, idx)\n355 obj, end = self.scan_once(s, idx)\n356 except StopIteration as err:\n--&gt; 357 raise JSONDecodeError(&quot;Expecting value&quot;, s, err.value) from None\n358 return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n<\/code><\/pre>\n<p><strong>If I use C# code provided in the Endpoint's &quot;Consume&quot; section I get the following error:<\/strong><\/p>\n<pre><code>The request failed with status code: BadGateway\nConnection: keep-alive\nX-Ms-Request-Id: 5c3543cf-29ac-46a3-a9fb-dcb6a0041b08\nX-Ms-Run-Function-Failed: True\nDate: Mon, 07 Mar 2022 12:38:32 GMT\nServer: nginx\/1.14.0 (Ubuntu)\n\n'&lt;=' not supported between instances of 'str' and 'int'\n<\/code><\/pre>\n<p><strong>The Python code I am using:<\/strong><\/p>\n<pre><code> import urllib.request\n import json\n import os\n import ssl\n    \n def allowSelfSignedHttps(allowed):\n     # bypass the server certificate verification on client side\n     if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n         ssl._create_default_https_context = ssl._create_unverified_context\n    \n allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n    \n data = {\n     &quot;Inputs&quot;: {\n         &quot;data&quot;:\n         [\n             {\n                 &quot;SaleDate&quot;: &quot;2022-02-08T00:00:00.000Z&quot;,\n                 &quot;OfferingGroupId&quot;: &quot;0&quot;,\n                 &quot;week_of_year&quot;: &quot;7&quot;,\n                 &quot;month_of_year&quot;: &quot;2&quot;,\n                 &quot;day_of_week&quot;: &quot;1&quot;\n             },\n         ]\n     },\n     &quot;GlobalParameters&quot;: {\n         &quot;quantiles&quot;: &quot;0.025,0.975&quot;\n     }\n }\n    \n body = str.encode(json.dumps(data))\n    \n url = 'http:\/\/4a0427c2-30d4-477e-85f5-dfdfdfdfdsfdff623f.uksouth.azurecontainer.io\/score'\n api_key = '' # Replace this with the API key for the web service\n headers = {'Content-Type':'application\/json', 'Authorization':('Bearer '+ api_key)}\n    \n req = urllib.request.Request(url, body, headers)\n    \n try:\n     response = urllib.request.urlopen(req)\n    \n     result = response.read()\n     print(result)\n except urllib.error.HTTPError as error:\n     print(&quot;The request failed with status code: &quot; + str(error.code))\n    \n     # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n     print(error.info())\n     print(json.loads(error.read().decode(&quot;utf8&quot;, 'ignore')))\n<\/code><\/pre>\n<p><strong>The C# code I have tried<\/strong>:<\/p>\n<pre><code> using System;\n using System.Collections.Generic;\n using System.IO;\n using System.Net.Http;\n using System.Net.Http.Headers;\n using System.Text;\n using System.Threading.Tasks;\n using Newtonsoft.Json;\n    \n namespace MLModelAPICall\n {\n     class Program\n     {\n         static void Main(string[] args)\n         {\n             InvokeRequestResponseService().Wait();\n         }\n    \n         static async Task InvokeRequestResponseService()\n         {\n             var handler = new HttpClientHandler()\n             {\n                 ClientCertificateOptions = ClientCertificateOption.Manual,\n                 ServerCertificateCustomValidationCallback =\n                         (httpRequestMessage, cert, cetChain, policyErrors) =&gt; { return true; }\n             };\n             using (var client = new HttpClient(handler))\n             {\n                 \/\/ Request data goes here\n                 var scoreRequest = new\n                 {\n                     Inputs = new Dictionary&lt;string, List&lt;Dictionary&lt;string, string&gt;&gt;&gt;()\n                     {\n                         {\n                             &quot;data&quot;,\n                             new List&lt;Dictionary&lt;string, string&gt;&gt;()\n                             {\n                                 new Dictionary&lt;string, string&gt;()\n                                 {\n                                     {\n                                         &quot;SaleDate&quot;, &quot;2022-02-08T00:00:00.000Z&quot;\n                                     },\n                                     {\n                                         &quot;OfferingGroupId&quot;, &quot;0&quot;\n                                     },\n                                     {\n                                         &quot;week_of_year&quot;, &quot;7&quot;\n                                     },\n                                     {\n                                         &quot;month_of_year&quot;, &quot;2&quot;\n                                     },\n                                     {\n                                         &quot;day_of_week&quot;, &quot;1&quot;\n                                     }\n                                 }\n                             }\n                         }\n                     },\n                     GlobalParameters = new Dictionary&lt;string, string&gt;()\n                     {\n                         {\n                             &quot;quantiles&quot;, &quot;0.025,0.975&quot;\n                         }\n                     }\n                 };\n    \n    \n                 const string apiKey = &quot;&quot;; \/\/ Replace this with the API key for the web service\n                 client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(&quot;Bearer&quot;, apiKey);\n                 client.BaseAddress = new Uri(&quot;http:\/\/4a0427c2-30d4-477e-85f5-xxxxxxxxxxxxx.uksouth.azurecontainer.io\/score&quot;);\n    \n                 \/\/ WARNING: The 'await' statement below can result in a deadlock\n                 \/\/ if you are calling this code from the UI thread of an ASP.Net application.\n                 \/\/ One way to address this would be to call ConfigureAwait(false)\n                 \/\/ so that the execution does not attempt to resume on the original context.\n                 \/\/ For instance, replace code such as:\n                 \/\/      result = await DoSomeTask()\n                 \/\/ with the following:\n                 \/\/      result = await DoSomeTask().ConfigureAwait(false)\n    \n                 var requestString = JsonConvert.SerializeObject(scoreRequest);\n                 var content = new StringContent(requestString);\n    \n                 content.Headers.ContentType = new MediaTypeHeaderValue(&quot;application\/json&quot;);\n    \n                 HttpResponseMessage response = await client.PostAsync(&quot;&quot;, content);\n    \n                 if (response.IsSuccessStatusCode)\n                 {\n                     string result = await response.Content.ReadAsStringAsync();\n                     Console.WriteLine(&quot;Result: {0}&quot;, result);\n                 }\n                 else\n                 {\n                     Console.WriteLine(string.Format(&quot;The request failed with status code: {0}&quot;, response.StatusCode));\n    \n                     \/\/ Print the headers - they include the requert ID and the timestamp,\n                     \/\/ which are useful for debugging the failure\n                     Console.WriteLine(response.Headers.ToString());\n    \n                     string responseContent = await response.Content.ReadAsStringAsync();\n                     Console.WriteLine(responseContent);\n                     Console.ReadLine();\n                 }\n             }\n         }\n     }\n }\n<\/code><\/pre>\n<p>Could you please help me with this issue? I am not sure what do to if Microsoft's provided code is erroring out, don't know what else to do.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646819247667,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-service",
        "Question_view_count":89,
        "Owner_creation_time":1360013608220,
        "Owner_last_access_time":1663933881363,
        "Owner_location":"Bolton, United Kingdom",
        "Owner_reputation":65842,
        "Owner_up_votes":1661,
        "Owner_down_votes":333,
        "Owner_views":4569,
        "Question_last_edit_time":null,
        "Answer_body":"<p>After much more digging I found out that the &quot;Consume&quot; scripts provided with the endpoint are wrong (Python and C#) .<\/p>\n<p>When making a call to the endpoint the GlobalParameters expects an integer value, but the provided scripts have wrapped the values in double quotes hence making it a string:<\/p>\n<pre><code> },\n &quot;GlobalParameters&quot;: {\n     &quot;quantiles&quot;: &quot;0.025,0.975&quot;\n }\n<\/code><\/pre>\n<p>If you are using Python to consume the model, when making call to the endpoint your GlobalParameters should be define as this:<\/p>\n<pre><code> },\n &quot;GlobalParameters&quot;: {\n     &quot;quantiles&quot;: [0.025,0.975]\n }\n<\/code><\/pre>\n<p>wrapped in square brackets<\/p>\n<blockquote>\n<p>[0.025,0.975]<\/p>\n<\/blockquote>\n<p>and not in double quotes &quot;<\/p>\n<blockquote>\n<p><em>I have also opened a ticket with microsoft so hopefully they will fix the code provided in the &quot;consume&quot; section of every endpoint<\/em><\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1646835298180,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71407308",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58293193,
        "Question_title":"Using the Environment Class with Pipeline Runs",
        "Question_body":"<p>I am using an estimator step for a pipeline using the Environment class, in order to have a custom Docker image as I need some <code>apt-get<\/code> packages to be able to install a specific pip package. It appears from the logs that it's completely ignoring, unlike the non-pipeline version of the estimator, the docker portion of the environment variable. Very simply, this seems broken : <\/p>\n\n<p>I'm running on SDK v1.0.65, and my dockerfile is completely ignored, I'm using <\/p>\n\n<pre><code>FROM mcr.microsoft.com\/azureml\/base:latest\\nRUN apt-get update &amp;&amp; apt-get -y install freetds-dev freetds-bin vim gcc\n<\/code><\/pre>\n\n<p>in the base_dockerfile property of my code. \nHere's a snippet of my code : <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Environment\nfrom azureml.core.environment import CondaDependencies\nconda_dep = CondaDependencies()\nconda_dep.add_pip_package('pymssql==2.1.1')\nmyenv = Environment(name=\"mssqlenv\")\nmyenv.python.conda_dependencies=conda_dep\nmyenv.docker.enabled = True\nmyenv.docker.base_dockerfile = 'FROM mcr.microsoft.com\/azureml\/base:latest\\nRUN apt-get update &amp;&amp; apt-get -y install freetds-dev freetds-bin vim gcc'\nmyenv.docker.base_image = None\n<\/code><\/pre>\n\n<p>This works well when I use an Estimator by itself, but if I insert this estimator in a Pipeline, it fails. Here's my code to launch it from a Pipeline run: <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.pipeline.steps import EstimatorStep\n\nsql_est_step = EstimatorStep(name=\"sql_step\", \n                         estimator=est, \n                         estimator_entry_script_arguments=[],\n                         runconfig_pipeline_params=None, \n                         compute_target=cpu_cluster)\nfrom azureml.pipeline.core import Pipeline\nfrom azureml.core import Experiment\npipeline = Pipeline(workspace=ws, steps=[sql_est_step])\npipeline_run = exp.submit(pipeline)\n<\/code><\/pre>\n\n<p>When launching this, the logs for the container building service reveal:<\/p>\n\n<pre><code>FROM continuumio\/miniconda3:4.4.10... etc.\n<\/code><\/pre>\n\n<p>Which indicates it's ignoring my <code>FROM mcr....<\/code> statement in the Environment class I've associated with this Estimator, and my <code>pip install<\/code> fails.<\/p>\n\n<p>Am I missing something? Is there a workaround?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1570564388707,
        "Question_score":3,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":751,
        "Owner_creation_time":1538275960603,
        "Owner_last_access_time":1658458641830,
        "Owner_location":"Montreal, QC, Canada",
        "Owner_reputation":381,
        "Owner_up_votes":75,
        "Owner_down_votes":2,
        "Owner_views":50,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58293193",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58933565,
        "Question_title":"How to register model from the Azure ML Pipeline Script step",
        "Question_body":"<p>I am running the <code>pipeline.submit()<\/code> in AzureML, which has a <code>PythonScriptStep<\/code>.\nInside this step, I download a model from tensorflow-hub, retrain it and save it as a <code>.zip<\/code>, and finally, I would like to register it in the Azure ML.\nBut as inside the script I do not have a workspace, <code>Model.register()<\/code> is not the case.\nSo I am trying to use <code>Run.register_model()<\/code> method as below:<\/p>\n\n<pre><code>os.replace(os.path.join('.', archive_name + '.zip'), \n           os.path.join('.', 'outputs', archive_name + '.zip'))\n\nprint(os.listdir('.\/outputs'))\nprint('========================')\n\nrun_context = Run.get_context()\nfinetuning_model = run_context.register_model(model_name='finetuning_similarity_model',\n                                              model_path=os.path.join(archive_name+'.zip'),\n                                              tags={},\n                                              description=\"Finetuning Similarity model\")\n<\/code><\/pre>\n\n<p>But then I have got an error:<\/p>\n\n<blockquote>\n  <p>ErrorResponse \n  {\n      \"error\": {\n          \"message\": \"Could not locate the provided model_path retrained.zip in the set of files uploaded to the run:<\/p>\n<\/blockquote>\n\n<p>despite I have the retrained <code>.zip<\/code> in the <code>.\/outputs<\/code> dir as we can see from the log:<\/p>\n\n<pre><code>['retrained.zip']\n========================\n<\/code><\/pre>\n\n<p>I guess that I am doing something wrong?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1574164584153,
        "Question_score":7,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":3429,
        "Owner_creation_time":1574162655727,
        "Owner_last_access_time":1632472959343,
        "Owner_location":null,
        "Owner_reputation":75,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1578744224352,
        "Answer_body":"<p>I was able to fix the same issue (<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.exceptions.modelpathnotfoundexception?view=azure-ml-py\" rel=\"noreferrer\"><code>ModelPathNotFoundException<\/code><\/a>) by explicitly uploading the model into the run history record before trying to register the model:<\/p>\n\n<pre><code>run.upload_file(\"outputs\/my_model.pickle\", \"outputs\/my_model.pickle\")\n<\/code><\/pre>\n\n<p>Which I found surprising because this wasn't mentioned in many of the official examples and according to the <code>upload_file()<\/code> <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#upload-file-name--path-or-stream-\" rel=\"noreferrer\">documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>Runs automatically capture file in the specified output directory, which defaults to \".\/outputs\" for most run types. Use upload_file only when additional files need to be uploaded or an output directory is not specified.<\/p>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1578745983587,
        "Answer_score":14.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":1578746319987,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58933565",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72210450,
        "Question_title":"Is there any way to create or delete workspaces in AML studio using powershell?",
        "Question_body":"<p>I am working on a prediction model and am about to use the azure machine learning studio resources. The main operation is to create a workspace on azure ML studio through Powershell. I would like to operate my workspace through the command line. Is there any way to develop and operate the ML Studio workspace through Powershell?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1652332894457,
        "Question_score":0,
        "Question_tags":"azure|powershell|azure-machine-learning-studio",
        "Question_view_count":104,
        "Owner_creation_time":1652331444420,
        "Owner_last_access_time":1652336195337,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":"<p>According to the requirements, there is no procedure developed to create\/delete workspaces through PowerShell in machine learning studio. For reference of creation of workspaces, you can check the below link and the point to be noted is we can create\/delete workspaces using <em><strong>Az<\/strong><\/em><\/p>\n<p>Here is the table link to check PowerShell support table<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/previous-versions\/azure\/machine-learning\/classic\/powershell-module\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/previous-versions\/azure\/machine-learning\/classic\/powershell-module<\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PGfhb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PGfhb.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1652333522489,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72210450",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72589399,
        "Question_title":"What is Threshold in the Evaluate Model Module?",
        "Question_body":"<p>Notice in the image below, if I increase the value of &quot;Threshold,&quot; the accuracy of the model seems to increase (with diminishing returns after about .62).<\/p>\n<p>What does this mean and can I somehow update this value such that my model will retain this setting?<\/p>\n<p>For example, I am using a boosted decision tree, but I don't see any such value for &quot;threshold.&quot;<\/p>\n<p>Ref. <a href=\"https:\/\/docs.microsoft.com\/en-us\/previous-versions\/azure\/machine-learning\/studio-module-reference\/evaluate-model?redirectedfrom=MSDN\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/previous-versions\/azure\/machine-learning\/studio-module-reference\/evaluate-model?redirectedfrom=MSDN<\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/E0o5I.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E0o5I.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1655006504270,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":70,
        "Owner_creation_time":1340487313997,
        "Owner_last_access_time":1662779989497,
        "Owner_location":null,
        "Owner_reputation":20199,
        "Owner_up_votes":7887,
        "Owner_down_votes":10,
        "Owner_views":2396,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72589399",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62200162,
        "Question_title":"How to import custom functions on my experiment script for Azure ML?",
        "Question_body":"<p>I can successfully submit an experiment to processing on a remote compute target on Azure ML.<\/p>\n\n<p>In my notebook, for submitting the experiment, I have:<\/p>\n\n<pre><code># estimator\nestimator = Estimator(\n    source_directory='scripts',\n    entry_script='exp01.py',\n    compute_target='pc2',\n    conda_packages=['scikit-learn'],\n    inputs=[data.as_named_input('my_dataset')],\n    )\n\n# Submit\nexp = Experiment(workspace=ws, name='my_exp')\n\n# Run the experiment based on the estimator\nrun = exp.submit(config=estimator)\nRunDetails(run).show()\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n\n<p>However, in order to keep things clean, I want to define my general use functions on an auxiliary script, so the first will import it.<\/p>\n\n<p>On my script experiment file exp01.py, I wanted:<\/p>\n\n<pre><code>import custom_functions as custom\n\n# azure experiment start\nrun = Run.get_context()\n\n# the data from azure datasets\/datastorage\ndf = run.input_datasets['my_dataset'].to_pandas_dataframe()\n\n# prepare data\ndf_transformed = custom.prepare_data(df)\n\n# split data\nX_train, X_test, y_train, y_test = custom.split_data(df_transformed)\n\n# run my models.....\nmodel_name = 'RF'\nmodel = custom.model_x(model_name, a_lot_of_args)\n\n# log the results\nrun.log(model_name, results)\n\n# azure finish\nrun.complete()\n<\/code><\/pre>\n\n<p>The thing is: Azure wont let me import the custom_functions.py.<\/p>\n\n<p>How are you doing it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591289910473,
        "Question_score":2,
        "Question_tags":"python-3.x|azure|scikit-learn|azure-machine-learning-service",
        "Question_view_count":428,
        "Owner_creation_time":1469143327747,
        "Owner_last_access_time":1663952044637,
        "Owner_location":"Brazil",
        "Owner_reputation":914,
        "Owner_up_votes":791,
        "Owner_down_votes":5,
        "Owner_views":100,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62200162",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":57432959,
        "Question_title":"Azure ML Service dump logs",
        "Question_body":"<p>With the AzureML service, how can I dump the correct Loss curve or Accuracy curve for different epochs for keras deep learning on multiple nodes with Horovod?<\/p>\n\n<p>The Loss vs epochs plt from Keras deep learning using Horovod and AzureML appears to have issues.<\/p>\n\n<p>Training CNN with Keras\/Horovod (2 GPUs) and AMLS SDK generates weird graphs\n<a href=\"https:\/\/i.stack.imgur.com\/iKusU.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/iKusU.jpg\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/loJTI.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/loJTI.jpg\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1565363771400,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":168,
        "Owner_creation_time":1565362834960,
        "Owner_last_access_time":1663006027963,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1565375098712,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57432959",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58213125,
        "Question_title":"ModuleNotFoundError: No module named 'keras' in Azure ML Pipeline",
        "Question_body":"<p>I am trying to get a a simple Azure ML pipeline with the dogs vs cats data set following the steps  - <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline\" rel=\"nofollow noreferrer\">documented here<\/a><\/p>\n\n<p>My notebook contains the following -<\/p>\n\n<pre><code>import azureml.core\nfrom azureml.core import Workspace, Datastore\nfrom azureml.core import Environment\nfrom azureml.core.environment import CondaDependencies\nfrom azureml.pipeline.steps import PythonScriptStep\n\nws = Workspace.from_config()\n\nmyenv = Environment(name=\"myenv\")\nconda_dep = CondaDependencies()\nconda_dep.add_conda_package(\"keras\")\nconda_dep.add_conda_package(\"PIL\")\nmyenv.python.conda_dependencies=conda_dep\nmyenv.register(workspace=ws)\n<\/code><\/pre>\n\n<p>After setting up the data reference and the compute, here's how I am creating the pipeline -<\/p>\n\n<pre><code>trainStep = PythonScriptStep(\n    script_name=\"dogs_vs_cats.py\",\n    arguments=[\"--input\", blob_input_data, \"--output\", output_data1],\n    inputs=[blob_input_data],\n    outputs=[output_data1],\n    compute_target=compute_target,\n    source_directory=\"..\/dogs-vs-cats\"\n)\n\nSteps = [trainStep]\n\nfrom azureml.pipeline.core import Pipeline\npipeline1 = Pipeline(workspace=ws, steps=[Steps])\n\nfrom azureml.core import Experiment\n\npipeline_run1 = Experiment(ws, 'dogs_vs_cats_exp').submit(pipeline1)\npipeline_run1.wait_for_completion()\n<\/code><\/pre>\n\n<p>Once this steps is executed, the experiment fails and I get the following error after a bunch of information -<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"dogs_vs_cats.py\", line 30, in &lt;module&gt;\n    import keras\nModuleNotFoundError: No module named 'keras'\n<\/code><\/pre>\n\n<p>The terminal shows my conda environment set to azureml_py36 and Keras seems be listed in the output of <code>conda list<\/code>.<\/p>\n\n<p>Am I setting up the environment correctly? What is mising <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1570082438523,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":697,
        "Owner_creation_time":1570078371343,
        "Owner_last_access_time":1643112550783,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":1570094136667,
        "Answer_body":"<p>From the way you have specified your environment, it's hard to see if it's a proper RunConfiguration object. If it is, it should be a matter of adding it to you PythonScriptStep.<\/p>\n\n<pre><code>trainStep = PythonScriptStep(\n    script_name=\"dogs_vs_cats.py\",\n    arguments=[\"--input\", blob_input_data, \"--output\", output_data1],\n    inputs=[blob_input_data],\n    outputs=[output_data1],\n    compute_target=compute_target,\n    source_directory=\"..\/dogs-vs-cats\",\n    runconfig=myenv\n)\n<\/code><\/pre>\n\n<p>Right now you're defining the environment, but no using it anywhere it seems. If your trouble persists maybe try defining your RunConfiguration like they do under the \"Specify the environment to run the script\" step in this notebook:<\/p>\n\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1570089071900,
        "Answer_score":0.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58213125",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64772514,
        "Question_title":"How to download folder from AzureML notebook folder to local or blob storage?",
        "Question_body":"<p>I saved file to the same directory using (.\/folder_name) when I use AzureML jupyter. Now how can I download to my local machine or blob storage?<\/p>\n<p>The folder have a lot of files and sub-directory in it, which I scraped online. So it is not realistic to save one by one.<\/p>\n<pre><code>file_path = &quot;.\/&quot;\n\nfor i in target_key_word:\n    tem_str = i.replace(' ', '+')\n    dir_name = file_path + i\n    if not os.path.exists(dir_name):\n        os.mkdir(dir_name)\n    else:    \n        print(&quot;Directory &quot; , dir_name ,  &quot; already exists&quot;)\n<\/code><\/pre>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1605024635707,
        "Question_score":1,
        "Question_tags":"python|azure|azure-storage|azure-blob-storage|azure-machine-learning-service",
        "Question_view_count":3310,
        "Owner_creation_time":1589508579330,
        "Owner_last_access_time":1663966063383,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1605030541670,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64772514",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":59517355,
        "Question_title":"Permission denied: '.\\NTUSER.DAT' when trying to run an Azure ML Pipeline",
        "Question_body":"<p>The short story is, when I try to submit an azure ML pipeline run (an <em>azure ML pipeline<\/em>, not an <em>Azure pipeline<\/em>) from a jupyter notebook, I get PermissionError: [Errno 13] Permission denied: '.\\NTUSER.DAT'.  More details:<\/p>\n\n<p>Relevant code:<\/p>\n\n<pre><code>from azureml.train.automl import AutoMLConfig\nfrom azureml.train.automl.runtime import AutoMLStep\nautoml_settings = {\n    \"iteration_timeout_minutes\": 20,\n    \"experiment_timeout_minutes\": 30,\n    \"n_cross_validations\": 3,\n    \"primary_metric\": 'r2_score',\n    \"preprocess\": True,\n    \"max_concurrent_iterations\": 3,\n    \"max_cores_per_iteration\": -1,\n    \"verbosity\": logging.INFO,\n    \"enable_early_stopping\": True,\n    'time_column_name': \"DateTime\"\n}\n\nautoml_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             path = \".\",\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config,                               \n                             training_data = financeforecast_dataset,\n                             label_column_name = 'TotalUSD',\n                             **automl_settings\n                            )\n\nautoml_step = AutoMLStep(\n    name='automl_module',\n    automl_config=automl_config,\n    allow_reuse=False)\n\ntraining_pipeline = Pipeline(\n    description=\"training_pipeline\",\n    workspace=ws,    \n    steps=[automl_step])\n\ntraining_pipeline_run = Experiment(ws, 'test').submit(training_pipeline)\n<\/code><\/pre>\n\n<p>The training_pipeline step runs for apx 20 seconds, and then I get a long trace, ending in:<\/p>\n\n<pre><code>~\\AppData\\Local\\Continuum\\anaconda2\\envs\\forecasting\\lib\\site- \npackages\\azureml\\pipeline\\core\\_module_builder.py in _hash_from_file_paths(hash_src)\n    100             hasher = hashlib.md5()\n    101             for f in hash_src:\n--&gt; 102                 with open(str(f), 'rb') as afile:\n    103                     buf = afile.read()\n    104                     hasher.update(buf)\n\nPermissionError: [Errno 13] Permission denied: '.\\\\NTUSER.DAT'\n<\/code><\/pre>\n\n<p>According to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-your-first-pipeline\" rel=\"nofollow noreferrer\">Azure's docs on this topic<\/a>, submitting a pipeline uploads a \"snapshot\" of the \"source directory\" you specified.  Initially, I hadn't specified a source directory, so, to test that out, I added: <\/p>\n\n<pre><code>default_source_directory=\"testing\",\n<\/code><\/pre>\n\n<p>as a parameter for the training_pipeline object, but saw the same behavior when I then tried to run it.  Not sure if that is the same source directory the documentation is referring to.  The docs also say that if no source directory is specified, the \"current local directory\" is uploaded.  I used print (os.getcwd()) to get the working directory and gave \"Everyone\" full control permissions on the directory (working in a windows env).<\/p>\n\n<p>All the preceding code works fine, and I can submit an experiment if I use a ScriptRunConfig and run it on attached compute rather than using a pipeline\/training cluster.  <\/p>\n\n<p>Any ideas?  Thanks in advance to anyone who tries to help.  P.S. There is no \"azure-machine-learning-pipelines\" tag, and I can't add one because I don't have enough reputation points.  Someone else could though!  <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-ml-pipelines\" rel=\"nofollow noreferrer\">General<\/a> info on what they are.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1577601580777,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-studio|automl",
        "Question_view_count":409,
        "Owner_creation_time":1541802293200,
        "Owner_last_access_time":1663773706450,
        "Owner_location":null,
        "Owner_reputation":163,
        "Owner_up_votes":53,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I resolved this answer by setting the path and the data_script variables in the AutoMLConfig task object, like this (relevant code indicated by -->):<\/p>\n\n<pre><code>automl_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config,\n                             --&gt;path = \"c:\\\\users\\\\me\",\n                             data_script =\"script.py\",&lt;--\n                             **automl_settings\n                            )\n<\/code><\/pre>\n\n<p>Setting the data_script variable to include the full path, as shown below, did not work.<\/p>\n\n<pre><code>automl_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             path = \".\",\n                             --&gt;data_script = \"c:\\\\users\\\\me\\\\script.py\"&lt;--\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config, \n                             **automl_settings\n                            )\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1577994637207,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59517355",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":45051055,
        "Question_title":"Read multiple CSV files in Azure ML Python Script",
        "Question_body":"<p>I have 4 csv files that are inputs to the python script in azure ML, but the widget has only 2 inputs for dataframes and the third for a zip file. I tried to put the csv files in a zipped folder and connect it to the third input for the script but that also did not work :\n<a href=\"https:\/\/i.stack.imgur.com\/FkxRE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FkxRE.png\" alt=\"Image of workspace\"><\/a><\/p>\n\n<p>I would like to know how to read multiple csv files in the python script.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1499844354733,
        "Question_score":1,
        "Question_tags":"python|csv|azure|azure-machine-learning-studio",
        "Question_view_count":944,
        "Owner_creation_time":1470376815797,
        "Owner_last_access_time":1660941481503,
        "Owner_location":null,
        "Owner_reputation":596,
        "Owner_up_votes":72,
        "Owner_down_votes":3,
        "Owner_views":57,
        "Question_last_edit_time":1499850837700,
        "Answer_body":"<p>Here's some more detail on the approach others have outlined above. Try replacing the code currently in the \"Execute Python Script\" module with the following:<\/p>\n\n<pre><code>import pandas as pd\nimport os\ndef azureml_main(dataframe1=None, dataframe2=None):\n    print(os.listdir('.'))\n    return(pd.DataFrame([]))\n<\/code><\/pre>\n\n<p>After running the experiment, click on the module. There should be a \"View output log\" link now in the right-hand bar. I get something like the following:<\/p>\n\n<pre><code>[Information]         Started in [C:\\temp]\n[Information]         Running in [C:\\temp]\n[Information]         Executing 4af67c05ba02417a980f6a16e84e61dc with inputs [] and generating outputs ['.maml.oport1']\n[Information]         Extracting Script Bundle.zip to .\\Script Bundle\n[Information]         File Name                                             Modified             Size\n[Information]         temp.csv                                       2016-05-06 13:16:56           52\n[Information]         [ READING ] 0:00:00\n[Information]         ['4af67c05ba02417a980f6a16e84e61dc.py', 'Script Bundle', 'Script Bundle.zip']\n<\/code><\/pre>\n\n<p>This tells me that the contents of my zip file have been extracted to the <code>C:\\temp\\Script Bundle<\/code> folder. In my case the zip file contained just one CSV file, <code>temp.csv<\/code>: your output would probably have four files. You may also have zipped a folder containing your four files, in which case the filepath would be one layer deeper. You can use the <code>os.listdir()<\/code> to explore your directory structure further if necessary.<\/p>\n\n<p>Once you think you know the full filepaths for your CSV files, edit your Execute Python Script module's code to load them, e.g.:<\/p>\n\n<pre><code>import pandas as pd\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    df = pd.read_csv('C:\/temp\/Script Bundle\/temp.csv')\n    # ...load other files and merge into a single dataframe...\n    return(df)\n<\/code><\/pre>\n\n<p>Hope that helps!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1500412322169,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45051055",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58772261,
        "Question_title":"Public windows docker image for azure machine learning",
        "Question_body":"<p>Our machine learning workflow requires use of a custom windows .pyc file. Where can I find a windows docker image file.<\/p>\n\n<p>I am puzzled by this statement from <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-custom-docker-image#create-a-custom-base-image\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-custom-docker-image#create-a-custom-base-image<\/a>. Is it really true that azure cannot use windows images? <\/p>\n\n<pre><code>Image requirements: Azure Machine Learning only supports Docker images that provide the following software:\n\nUbuntu 16.04 or greater.\nConda 4.5.# or greater.\nPython 3.5.# or 3.6.#.\n\n<\/code><\/pre>\n\n<p>Searching on docker hub also did not turn up anything promising<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1573239057720,
        "Question_score":0,
        "Question_tags":"docker|azure-machine-learning-service",
        "Question_view_count":69,
        "Owner_creation_time":1565794118450,
        "Owner_last_access_time":1664072200673,
        "Owner_location":null,
        "Owner_reputation":113,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58772261",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":63158394,
        "Question_title":"Custom Docker file for Azure ML Environment that contains COPY statements errors with COPY failed: \/path no such file or directory",
        "Question_body":"<p>I'm trying to submit an experiment to Azure ML using a Python script.<\/p>\n<p>The Environment being initialised uses a custom Dockerfile.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>env = Environment(name=&quot;test&quot;)\nenv.docker.base_image = None\nenv.docker.base_dockerfile = '.\/Docker\/Dockerfile'\nenv.docker.enabled = True\n<\/code><\/pre>\n<p>However the DockerFile needs a few <code>COPY<\/code> statements but those fail as follow:<\/p>\n<pre><code>Step 9\/23 : COPY requirements-azure.txt \/tmp\/requirements-azure.txt\nCOPY failed: stat \/var\/lib\/docker\/tmp\/docker-builder701026190\/requirements-azure.txt: no such file or directory\n<\/code><\/pre>\n<p>The Azure host environment responsible to build the image does not contain the files the Dockerfile requires, those exist in my local development machine from where I initiate the python script.<\/p>\n<p>I've been searching for the whole day of a way to add to the environment these files but without success.<\/p>\n<p>Below an excerpt from the Dockerfile and the python script that submits the experiment.<\/p>\n<pre><code>FROM mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04 as base\nCOPY .\/Docker\/requirements-azure.txt \/tmp\/requirements-azure.txt # &lt;- breaks here\n\n[...]\n\n<\/code><\/pre>\n<p>Here is how I'm submitting the experiment:<\/p>\n<pre><code>from azureml.core.environment import Environment\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core import Workspace, Experiment\nfrom azureml.core.compute import ComputeTarget\n\nfrom azureml.core import Experiment, Workspace\nfrom azureml.train.estimator import Estimator\nimport os\n\nws = Workspace.from_config(path='\/mnt\/azure\/config\/workspace-config.json')\nenv = Environment(name=&quot;test&quot;)\nenv.docker.base_image = None\nenv.docker.base_dockerfile = '.\/Docker\/Dockerfile'\nenv.docker.enabled = True\ncompute_target = ComputeTarget(workspace=ws, name='GRComputeInstance')\nestimator = Estimator(\n    source_directory='\/workspace\/',\n    compute_target=compute_target,\n    entry_script=&quot;.\/src\/ml\/train\/main.py&quot;,\n    environment_definition=env\n)\nexperiment = Experiment(workspace=ws, name=&quot;estimator-test&quot;)\nrun = experiment.submit(estimator)\nrun.wait_for_completion(show_output=True, wait_post_processing=True)\n<\/code><\/pre>\n<p>Any idea?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1596040632287,
        "Question_score":7,
        "Question_tags":"python|azure|docker|dockerfile|azure-machine-learning-service",
        "Question_view_count":927,
        "Owner_creation_time":1248452771430,
        "Owner_last_access_time":1664006575380,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":3317,
        "Owner_up_votes":466,
        "Owner_down_votes":8,
        "Owner_views":296,
        "Question_last_edit_time":1596701189560,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63158394",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58285038,
        "Question_title":"Trying to query Azure SQL Database with Azure ML \/ Docker Image",
        "Question_body":"<p>I wanted to do a realtime deployment of my model on azure, so I plan to create an image which firsts queries an ID in azure SQL db to get the required features, then predicts using my model and returns the predictions. The error I get from PyODBC library is that drivers are not installed<\/p>\n\n<p>I tried it on the azure ML jupyter notebook to establish the connection and found that no drivers are being installed in the environment itself. After some research i found that i should create a docker image and deploy it there,  but i still met with the same results<\/p>\n\n<pre><code>    driver= '{ODBC Driver 13 for SQL Server}'\n    cnxn = pyodbc.connect('DRIVER='+driver+';SERVER='+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password+';Encrypt=yes'+';TrustServerCertificate=no'+';Connection Timeout=30;')\n<\/code><\/pre>\n\n<blockquote>\n  <p>('01000', \"[01000] [unixODBC][Driver Manager]Can't open lib 'ODBC\n  Driver 13 for SQL Server' : file not found (0) (SQLDriverConnect)\")<\/p>\n<\/blockquote>\n\n<p>i want a result to the query instead i get this message<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_time":1570532396097,
        "Question_score":1,
        "Question_tags":"sql-server|azure|azure-machine-learning-service",
        "Question_view_count":1319,
        "Owner_creation_time":1525963887537,
        "Owner_last_access_time":1575968994133,
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":55,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1570535747687,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58285038",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":44371692,
        "Question_title":"Install Python Packages in Azure ML?",
        "Question_body":"<p>Similar question as <a href=\"https:\/\/stackoverflow.com\/questions\/43176442\/install-r-packages-in-azure-ml\">here<\/a> but now on Python packages. Currently, the CVXPY is missing in Azure ML. I am also trying to get other solvers such as GLPK, CLP and COINMP working in Azure ML.<\/p>\n<p><strong>How can I install Python packages in Azure ML?<\/strong><\/p>\n<hr \/>\n<p><em>Update about trying to install the Python packages not found in Azure ML.<\/em><\/p>\n<blockquote>\n<p>I did as instructed by Peter Pan but I think the 32bits CVXPY files are wrong for the Anaconda 4 and Python 3.5 in Azure ML, logs and errors are <a href=\"https:\/\/pastebin.com\/zN5QrPtL\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<pre><code>[Information]         Running with Python 3.5.1 |Anaconda 4.0.0 (64-bit)| (default, Feb 16 2016, 09:49:46) [MSC v.1900 64 bit (AMD64)]\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rS0Us.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rS0Us.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6qz3p.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6qz3p.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9glSm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9glSm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/blockquote>\n<p><em>Update 2 with win_amd64 files (paste <a href=\"https:\/\/pastebin.com\/tisWuP5C\" rel=\"nofollow noreferrer\">here<\/a>)<\/em><\/p>\n<blockquote>\n<pre><code>[Information]         Extracting Script Bundle.zip to .\\Script Bundle\n[Information]         File Name                                             Modified             Size\n[Information]         cvxopt-1.1.9-cp35-cp35m-win_amd64.whl          2017-06-07 01:03:34      1972074\n[Information]         __MACOSX\/                                      2017-06-07 01:26:28            0\n[Information]         __MACOSX\/._cvxopt-1.1.9-cp35-cp35m-win_amd64.whl 2017-06-07 01:03:34          452\n[Information]         cvxpy-0.4.10-py3-none-any.whl                  2017-06-07 00:25:36       300880\n[Information]         __MACOSX\/._cvxpy-0.4.10-py3-none-any.whl       2017-06-07 00:25:36          444\n[Information]         ecos-2.0.4-cp35-cp35m-win_amd64.whl            2017-06-07 01:03:40        56522\n[Information]         __MACOSX\/._ecos-2.0.4-cp35-cp35m-win_amd64.whl 2017-06-07 01:03:40          450\n[Information]         numpy-1.13.0rc2+mkl-cp35-cp35m-win_amd64.whl   2017-06-07 01:25:02    127909457\n[Information]         __MACOSX\/._numpy-1.13.0rc2+mkl-cp35-cp35m-win_amd64.whl 2017-06-07 01:25:02          459\n[Information]         scipy-0.19.0-cp35-cp35m-win_amd64.whl          2017-06-07 01:05:12     12178932\n[Information]         __MACOSX\/._scipy-0.19.0-cp35-cp35m-win_amd64.whl 2017-06-07 01:05:12          452\n[Information]         scs-1.2.6-cp35-cp35m-win_amd64.whl             2017-06-07 01:03:34        78653\n[Information]         __MACOSX\/._scs-1.2.6-cp35-cp35m-win_amd64.whl  2017-06-07 01:03:34          449\n[Information]         [ READING ] 0:00:00\n[Information]         Input pandas.DataFrame #1:\n[Information]         Empty DataFrame\n[Information]         Columns: [1]\n[Information]         Index: []\n[Information]         [ EXECUTING ] 0:00:00\n[Information]         [ WRITING ] 0:00:00\n<\/code><\/pre>\n<p>where <code>import cvxpy<\/code>, <code>import cvxpy-0.4.10-py3-none-any.whl<\/code> or <code>cvxpy-0.4.10-py3-none-any<\/code> do not work so<\/p>\n<p><strong>How can I use the following wheel files downloaded from <a href=\"http:\/\/www.lfd.uci.edu\/%7Egohlke\/pythonlibs\/#cvxpy\" rel=\"nofollow noreferrer\">here<\/a> to use the external Python packages not found in Azure ML?<\/strong><\/p>\n<\/blockquote>\n<p><em>Update about permission problem about importing cvxpy (paste <a href=\"https:\/\/pastebin.com\/3kTKgLfc\" rel=\"nofollow noreferrer\">here<\/a>)<\/em><\/p>\n<blockquote>\n<pre><code> [Error]         ImportError: No module named 'canonInterface'\n<\/code><\/pre>\n<p>where the ZIP Bundle is organised a bit differently, the content of each wheel downloaded to a folder and the content having all zipped as a ZIP Bundle.<\/p>\n<\/blockquote>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1496674339213,
        "Question_score":8,
        "Question_tags":"python|azure|azure-machine-learning-studio|cvxpy",
        "Question_view_count":12625,
        "Owner_creation_time":1251372839053,
        "Owner_last_access_time":1653648989307,
        "Owner_location":null,
        "Owner_reputation":48616,
        "Owner_up_votes":1240,
        "Owner_down_votes":41,
        "Owner_views":3348,
        "Question_last_edit_time":1592644375060,
        "Answer_body":"<p>According to the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-execute-python-scripts#limitations\" rel=\"nofollow noreferrer\"><code>Limitations<\/code><\/a> and <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn955437.aspx#Anchor_3\" rel=\"nofollow noreferrer\"><code>Technical Notes<\/code><\/a> of <code>Execute Python Script<\/code> tutorial, the only way to add custom Python modules is via the zip file mechanism to package the modules and all dependencies.<\/p>\n\n<p>For example to install <code>CVXPY<\/code>, as below.<\/p>\n\n<ol>\n<li>Download the wheel file of <a href=\"http:\/\/www.lfd.uci.edu\/~gohlke\/pythonlibs\/#cvxpy\" rel=\"nofollow noreferrer\"><code>CVXPY<\/code><\/a> and its dependencies like <a href=\"http:\/\/www.lfd.uci.edu\/~gohlke\/pythonlibs\/#cvxopt\" rel=\"nofollow noreferrer\"><code>CVXOPT<\/code><\/a>.<\/li>\n<li>Decompress these wheel files, and package these files in the path <code>cvxpy<\/code> and <code>cvxopt<\/code>, etc as a zipped file with your script.<\/li>\n<li>Upload the zip file as a dataset and use it as the script bundle.<\/li>\n<\/ol>\n\n<p>If you were using IPython, you also can try to install the Python Package via the code <code>!pip install cvxpy<\/code>.<\/p>\n\n<p>And there are some similar SO threads which may be helpful for you, as below.<\/p>\n\n<ol>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/44285641\/azure-ml-python-with-script-bundle-cannot-import-module\">Azure ML Python with Script Bundle cannot import module<\/a><\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/8663046\/how-to-install-a-python-package-from-within-ipython\">How to install a Python package from within IPython?<\/a><\/li>\n<\/ol>\n\n<p>Hope it helps.<\/p>\n\n<hr>\n\n<p>Update:<\/p>\n\n<p>For IPython interface of Azure ML, you move to the <code>NOTEBOOKS<\/code> tab to create a notebook via <code>ADD TO PROJECT<\/code> button at the bottom of the page, as the figure below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/X2Asv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/X2Asv.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Or you can directly login to the website <code>https:\/\/notebooks.azure.com<\/code> to use it.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_creation_time":1496732346280,
        "Answer_score":2.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":1496760284030,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44371692",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":39073386,
        "Question_title":"Azure Machine Learning Experiment Creation",
        "Question_body":"<p>I am new to create Experiments in Azure ML. I want to done a sample and small POC on Azure ML.<\/p>\n\n<p>I have a data for the students consisting of StudentID, Student Name and Marks for Monthly Tests 1,2 and 3. I just to want to Predict data for the Final Monthly Test (i.e., Monthly Test 4).<\/p>\n\n<p>I don't know how to create and what kind of Transformations to be used in Predicting the Data.<\/p>\n\n<p>Anyone Please...<\/p>\n\n<p>Thanks in Advance\nPradeep<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1471850213390,
        "Question_score":0,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio",
        "Question_view_count":79,
        "Owner_creation_time":1406630302783,
        "Owner_last_access_time":1520847578177,
        "Owner_location":"Chennai, Tamil Nadu, India",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/39073386",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67255397,
        "Question_title":"What is real-time inference pipeline?",
        "Question_body":"<p>From Azure Machine Learning designer, to deploy a real-time inference pipeline as a service for others to consume, you must deploy the model to an Azure Kubernetes Service (AKS).\nWhat is real-time inference pipeline ?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1619366165967,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":211,
        "Owner_creation_time":1610715294930,
        "Owner_last_access_time":1648997946363,
        "Owner_location":"United States",
        "Owner_reputation":306,
        "Owner_up_votes":200,
        "Owner_down_votes":2,
        "Owner_views":53,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67255397",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":45051184,
        "Question_title":"Can i Pass Dataset as a parameter to AzureML experiment in Streaming Analytics Job?",
        "Question_body":"<p>Can i Pass Dataset as a parameter to AzureML experiment in Streaming Analytics Job? Right now im passing parameters like this ,  <\/p>\n\n<pre><code>   SELECT test (var1,var2,var3,var4,var5) as Result\n   FROM [Input-eventhub]\n<\/code><\/pre>\n\n<p>So instead of that can i pass dataset instead of this like,\n      SELECT test (datset) as Result\n       FROM [Input-eventhub]Azurestre<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1499844731840,
        "Question_score":0,
        "Question_tags":"tsql|azure-stream-analytics|azure-machine-learning-studio",
        "Question_view_count":47,
        "Owner_creation_time":1498115097193,
        "Owner_last_access_time":1567158670357,
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":1499928470636,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45051184",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64576059,
        "Question_title":"Azure ML Service Principal Password",
        "Question_body":"<p>We want to use a service principal in Azure ML to access the Workspace as described <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication#use-a-service-principal-from-the-sdk\" rel=\"nofollow noreferrer\">here<\/a>, and are wondering how to save the SP password. It says correctly to not store it in the juypter sources, but to use <code>os.environ['AML_PRINCIPAL_PASS']<\/code> instead. But how do I get it into the environment in a way that survives reboot of compute instance etc?<\/p>\n<p>There is a Keyvault in the workspace, but I need to authenticate the SP first before I can access the workspace and thus the keyvault.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1603898857127,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-service|azure-service-principal",
        "Question_view_count":142,
        "Owner_creation_time":1383178639610,
        "Owner_last_access_time":1659632827730,
        "Owner_location":"Germany",
        "Owner_reputation":389,
        "Owner_up_votes":12,
        "Owner_down_votes":1,
        "Owner_views":73,
        "Question_last_edit_time":1603952587467,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64576059",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":28929813,
        "Question_title":"Python support for Azure ML -- speed issue",
        "Question_body":"<p>We are trying to create an Azure ML web-service that will receive a (.csv) data file, do some processing, and return two similar files. The Python support recently added to the azure ML platform was very helpful and we were able to successfully port our code, run it in experiment mode and publish the web-service.<\/p>\n\n<p>Using the \"batch processing\" API, we are now able to direct a file from blob-storage to the service and get the desired output. However, run-time for small files (a few KB) is significantly slower than on a local machine, and more importantly, the process seems to never return for slightly larger input data files (40MB). Processing time on my local machine for the same file is under 1 minute. <\/p>\n\n<p>My question is if you can see anything we are doing wrong, or if there is a way to get this to speed up. Here is the DAG representation of the experiment:<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/MalhQ.png\" alt=\"The DAG representation of the experiment\"><\/p>\n\n<p>Is this the way the experiment should be set up? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1425837068693,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-studio",
        "Question_view_count":491,
        "Owner_creation_time":1419868247680,
        "Owner_last_access_time":1535114076180,
        "Owner_location":null,
        "Owner_reputation":877,
        "Owner_up_votes":46,
        "Owner_down_votes":0,
        "Owner_views":55,
        "Question_last_edit_time":1425890468080,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/28929813",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58642295,
        "Question_title":"Trying to connect Azure SQL database from Azure ML Service using MSI authentication (Without username and passowrd connect the Azure database)",
        "Question_body":"<p>I am trying to connect the <strong>Azure SQL Database<\/strong> from <strong>Azure Machine Learning Service<\/strong> with <strong>MSI Authentication (Without a username and password).<\/strong><\/p>\n\n<p>I am trying to Machine learning model on azure Machine learning service that purpose I need data that' why I want to connect Azure SQL Database from Azure Machine Learning Service using MSI Authentication.<\/p>\n\n<p>But I got below error:-<\/p>\n\n<pre><code> \"error\": {\"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"User program failed with KeyError: 'MSI_ENDPOINT'\\\",\\n\n<\/code><\/pre>\n\n<p>Please check the below code that I have used for the database connection.<\/p>\n\n<pre><code>import logging\nimport struct\nimport pyodbc\nimport os\nimport requests\n\n\nclass AzureDbConnect:\n    def __init__(self):\n        print(\"Inside msi database\")\n        msi_endpoint = os.environ[\"MSI_ENDPOINT\"]\n        msi_secret = os.environ[\"MSI_SECRET\"]\n\n        resource_uri = 'https:\/\/database.windows.net\/'\n\n        logging.info(msi_endpoint)\n        print(msi_endpoint)\n        logging.info(msi_secret)\n        print(msi_secret)\n        print(\"Inside token\")\n\n        token_auth_uri = f\"{msi_endpoint}?resource={resource_uri}&amp;api-version=2017-09-01\"\n        head_msi = {'Secret': msi_secret}\n        resp = requests.get(token_auth_uri, headers=head_msi)\n        access_token = resp.json()['access_token']\n        logging.info(access_token)\n        print(\"Token is :- \")\n        print(access_token)\n\n        accesstoken = bytes(access_token, 'utf-8')\n        exptoken = b\"\"\n        for i in accesstoken:\n            exptoken += bytes({i})\n            exptoken += bytes(1)\n        tokenstruct = struct.pack(\"=i\", len(exptoken)) + exptoken\n\n        conn = pyodbc.connect(\"Driver={ODBC Driver 17 for SQL Server};\"\n                              \"Server=tcp:&lt;Server Name&gt;\"\n                              \"1433;Database=&lt;Database Name&gt;\",\n                              attrs_before={1256: bytearray(tokenstruct)})\n\n        print(conn)\n\n        self.sql_db = conn.cursor()\n<\/code><\/pre>\n\n<p><strong>Is there any way to connect Azure, SQL Database from Azure Machine Learning Service With MSI Authentication?<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1572520944167,
        "Question_score":2,
        "Question_tags":"python|python-3.x|azure-sql-database|azure-machine-learning-service|azure-managed-identity",
        "Question_view_count":708,
        "Owner_creation_time":1554466050937,
        "Owner_last_access_time":1578409023530,
        "Owner_location":null,
        "Owner_reputation":219,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":1572588280712,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58642295",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":52908711,
        "Question_title":"Azure ML Web Service Parmeters Input",
        "Question_body":"<p>Looking to use Azure ML to allow the input value to be inputted through the web service call.  As of now, I have the datasource and and SQL Transformation.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/I5d6u.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/I5d6u.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/8XUDE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8XUDE.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>In the SQL transformation, here is my code that does not accept the parmeter:<\/p>\n\n<pre><code>select pagename, cp \nfrom t1\nwhere %webserviceparm\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1540059334607,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":30,
        "Owner_creation_time":1406068130647,
        "Owner_last_access_time":1663872279057,
        "Owner_location":null,
        "Owner_reputation":2113,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":183,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52908711",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":54747199,
        "Question_title":"Authentication problems",
        "Question_body":"<p>I am trying to automate the process to create a model with azure machine learning services and I get some problems with the authentication. When I run my code on my remote machine everything is fine but when I run the code on remote I get this authentication sentence:<\/p>\n\n<pre><code>Make sure your code doesn't require 'az login' to have happened before using azureml-SDK, except the case when you are specifying AzureCliAuthentication in azureml-SDK.\nPerforming interactive authentication. Please follow the instructions on the terminal.\nTo sign in, use a web browser to open the page https:\/\/microsoft.com\/devicelogin and enter the code CZMKCYS8B to authenticate\"\n<\/code><\/pre>\n\n<p>Azure ask me for authentication and I have to make it manually.\nI would like to know if there is some way to do it automatically.<\/p>\n\n<p>I was looking for it and I was investigated how to do it using tokens but I couldn't find any solution<\/p>\n\n<p>Someone can give me an advice?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1550492451810,
        "Question_score":2,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":653,
        "Owner_creation_time":1501608884543,
        "Owner_last_access_time":1641814166423,
        "Owner_location":null,
        "Owner_reputation":71,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1550494245456,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54747199",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71488101,
        "Question_title":"How to connect labeled data in Azure Machine Learning Studio\/Data Labeling to Power BI?",
        "Question_body":"<p>I am trying to connect already labeled dataset in <strong>Azure ML Studio<\/strong> in <strong>Data Labeling<\/strong> to <strong>PowerBI<\/strong>. I would like to see the progress of labeling or the result of labeled data without exporting it manually and connecting the exported files one by one.\nThank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647373302570,
        "Question_score":0,
        "Question_tags":"azure|powerbi|azure-machine-learning-studio",
        "Question_view_count":224,
        "Owner_creation_time":1615200289193,
        "Owner_last_access_time":1663836997667,
        "Owner_location":"Mil\u00e1no, Mil\u00e1n, It\u00e1lie",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71488101",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62770218,
        "Question_title":"Azure - prediction over time",
        "Question_body":"<p>I am currently studying machine learning and predictive analysis so thought a good way to start would be using Azure ML Studio.\nI did the Car Price Tutorial quite successfully however I now want to do something different.\nI thought about using Currency Data; Price and Volume to try and predict the price the next day.\nEverything has gone smoothly as I copied the Car Price tutorial. However when I come to test it the test wants all the variables to predict the new price, but I don't have any of &quot;tomorrows&quot; data. All I want to do is type tomorrows date and it will predict the price using yesterdays and before price and volume data.\nCould you help me please? I am sure it is a small amendment but not sure what!\nThank you\nSam<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1594107238817,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-service|predictive",
        "Question_view_count":42,
        "Owner_creation_time":1594107133893,
        "Owner_last_access_time":1594280258643,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1594110672470,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62770218",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71454995,
        "Question_title":"Getting the error \"str' object has no attribute 'decode\" when trying to use custom weights for image classification",
        "Question_body":"<p>I am trying to develop a simple image classification model in Azure ML notebooks. ResNet50 model was trained and the custom weights from the model is being used in the following code for image classification. The custom weights are saved in a folder called Model.<\/p>\n<pre><code>import os\nworking_directory = os.getcwd()\nmodel_directory = working_directory + &quot;\/Model\/model.h5&quot; \n<\/code><\/pre>\n<p>The above code is being used for accessing the saved model.<\/p>\n<pre><code>def classifyingImages(image_list):\n    value = 0\n\n    for image in image_list:\n        image_resized = cv2.resize(image, (img_height, img_width))\n        image = np.expand_dims(image_resized, axis=0)\n\n        \n        model = load_model(model_directory)\n        #classifying the image\n        prediction = model.predict(image)\n        output_class = class_names[np.argmax(prediction)]\n\n        #getting the image name\n        image_name = img_name_list[value]\n        print(image_name)\n\n        if(np.argmax(prediction)==0):\n            print(&quot;check negative&quot;)\n            # cv2.imwrite((negative_path+&quot;\/&quot;+image_name), image)\n        else:\n            print(&quot;check positive&quot;)\n            # cv2.imwrite((path_positive+&quot;\/&quot;+image_name), image)\n\n        value = value +1\n\n\n    return value\n\nclassifyingImages(image_list)\n<\/code><\/pre>\n<p>The code added above is the image classification code<\/p>\n<p>The <code>image_list<\/code> contains the test images which saved in the blob storage.<\/p>\n<p>After running the classification function i get the error <code>str' object has no attribute 'decode<\/code> and as a solution i tried to change the h5py lib version using below code. But still it gives me the same error. It would be great if i could a solution for this issue. Thank you in advance.\n<code>!pip install h5py==2.10.0<\/code><\/p>\n<p>The stack trace<\/p>\n<pre><code>AttributeError                            Traceback (most recent call last)\n&lt;ipython-input-96-6e38a2f74291&gt; in &lt;module&gt;\n     42     return value\n     43 \n---&gt; 44 classifyingImages(image_list)\n\n&lt;ipython-input-96-6e38a2f74291&gt; in classifyingImages(image_list)\n     20         print(image.dtype)\n     21 \n---&gt; 22         model = load_model(model_directory)\n     23 \n     24         #classifying the image\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tensorflow_core\/python\/keras\/saving\/save.py in load_model(filepath, custom_objects, compile)\n    144   if (h5py is not None and (\n    145       isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n--&gt; 146     return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\n    147 \n    148   if isinstance(filepath, six.string_types):\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tensorflow_core\/python\/keras\/saving\/hdf5_format.py in load_model_from_hdf5(filepath, custom_objects, compile)\n    164     if model_config is None:\n    165       raise ValueError('No model found in config file.')\n--&gt; 166     model_config = json.loads(model_config.decode('utf-8'))\n    167     model = model_config_lib.model_from_config(model_config,\n    168                                                custom_objects=custom_objects)\n\nAttributeError: 'str' object has no attribute 'decode'\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":9,
        "Question_creation_time":1647159554620,
        "Question_score":0,
        "Question_tags":"python|tensorflow|azure-machine-learning-studio",
        "Question_view_count":68,
        "Owner_creation_time":1556718046720,
        "Owner_last_access_time":1662885742377,
        "Owner_location":"1040\/3 Athurugiriya Road, Malabe, Sri Lanka",
        "Owner_reputation":25,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1647162639363,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71454995",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60832279,
        "Question_title":"Influx DB as a source for MLS as a direct connection",
        "Question_body":"<p>Can we use InfluxDB as a data source for Azure ML Serv, in the form of a direct connection.  If not, what are the proposed alternatives to setup this connection?\n(Put differently, Is it possible for M LServ to connect to an InfluxDB next to some API to fetch data from. Or do we have to put all data in a SQL database?)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1585057386130,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":105,
        "Owner_creation_time":1585057281607,
        "Owner_last_access_time":1614093458423,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60832279",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70376362,
        "Question_title":"Unable to use \"join data\" to combine multiple datasets into one using Azure Machine Learning Studio Designer",
        "Question_body":"<p>How can I combine mutiple datasets into one using Azure Machine Learning Studio?<\/p>\n<p>(The following graph doesn't work)\n<a href=\"https:\/\/i.stack.imgur.com\/ROmcV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ROmcV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Same question: <a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/666021\/unable-to-use-34join-data34-to-combine-multiple-da.html\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/666021\/unable-to-use-34join-data34-to-combine-multiple-da.html<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639645564350,
        "Question_score":0,
        "Question_tags":"azure|azure-pipelines|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":141,
        "Owner_creation_time":1626943035463,
        "Owner_last_access_time":1663968468203,
        "Owner_location":"Hong Kong",
        "Owner_reputation":43,
        "Owner_up_votes":21,
        "Owner_down_votes":0,
        "Owner_views":21,
        "Question_last_edit_time":1639709288792,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70376362",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":53273551,
        "Question_title":"Visualizing decision jungle in Azure Machine Learning Studio",
        "Question_body":"<p>I have trained a decision jungle model on Azure Machine Learning, and  now I want to visualize the trees, to see if I can identify the root nodes that are the most determinant in the decision.<\/p>\n\n<p>When I right-click and click Visualize on the Train Model, what is shown is the parameter set used for the training. How can I either visualize the jungle, or identify the features with highest information gain from this?<\/p>\n\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1542081348593,
        "Question_score":1,
        "Question_tags":"decision-tree|azure-machine-learning-studio|information-gain",
        "Question_view_count":88,
        "Owner_creation_time":1384225745143,
        "Owner_last_access_time":1654030338327,
        "Owner_location":"Milton, ON, Canada",
        "Owner_reputation":179,
        "Owner_up_votes":174,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53273551",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":51587956,
        "Question_title":"Activation function of Regression Neural Net in Azure ML Studio?",
        "Question_body":"<p>I am not able to find activation function for Regression Neural Network in Azure Machine Learning Studio. I am not able to identify what is the activation function taken for my NN. Followed this document also-<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/neural-network-regression\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/neural-network-regression<\/a><\/p>\n\n<p>Can someone suggest where to mention it\/ what is the default activation function used?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1532931331770,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-workbench",
        "Question_view_count":352,
        "Owner_creation_time":1439280053903,
        "Owner_last_access_time":1660234274593,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":541,
        "Owner_up_votes":457,
        "Owner_down_votes":6,
        "Owner_views":269,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51587956",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":52032535,
        "Question_title":"Does the primary key of Web Service API in ML Studio expire?",
        "Question_body":"<p>I deployed a web-service from an experiment in ML studio. I tested the API, and everything was working fine. I tested it in Postman. After 2 hours, I got an authentication error when I sent a request using the same API. So to resolve this, I republished my Web Service and got new authentication code, so the API is working fine for now. I have two questions:<\/p>\n\n<p>1) Does the primary key automatically expire after a while or by signing out from ML studio? \n2) What is the application of the second key in ML Studio APIs? Where do we need the second key? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1535343803617,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio|ml-studio",
        "Question_view_count":228,
        "Owner_creation_time":1501114346137,
        "Owner_last_access_time":1605792960497,
        "Owner_location":"Australia",
        "Owner_reputation":37,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":"<blockquote>\n  <p>1) Does the primary key automatically expire after a while or by signing out from ML studio?<\/p>\n<\/blockquote>\n\n<p>I could not find any limit of the primary key in the office docs. Per my test, my primary key does not expire more than two hours or sign out from ML studio.<\/p>\n\n<blockquote>\n  <p>2) What is the application of the second key in ML Studio APIs? Where do we need the second key?<\/p>\n<\/blockquote>\n\n<p>The second key is the same usage of the primary key, like a backup of the primary key. Also, the primary key equals the API key in the ML studio.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1535450343787,
        "Answer_score":0.0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52032535",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":35970126,
        "Question_title":"Increase the size of \/dev\/shm in Azure ML Studio",
        "Question_body":"<p>I'm trying to execute the following code in Azure ML Studio notebook:<\/p>\n\n<pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.cross_validation import KFold, cross_val_score\n\nfor C in np.linspace(0.01, 0.2, 30):\n    cv = KFold(n=X_train.shape[0], n_folds=7, shuffle=True, random_state=12345)\n    clf = LogisticRegression(C=C, random_state=12345)\n    print C, sum(cross_val_score(clf, X_train_scaled, y_train, scoring='roc_auc', cv=cv, n_jobs=2)) \/ 7.0\n<\/code><\/pre>\n\n<p>and I'm getting this error:<\/p>\n\n<pre><code>Failed to save &lt;type 'numpy.ndarray'&gt; to .npy file:\nTraceback (most recent call last):\n  File \"\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/sklearn\/externals\/joblib\/numpy_pickle.py\", line 271, in save\n    obj, filename = self._write_array(obj, filename)\n  File \"\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/sklearn\/externals\/joblib\/numpy_pickle.py\", line 231, in _write_array\n    self.np.save(filename, array)\n  File \"\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/numpy\/lib\/npyio.py\", line 491, in save\n    pickle_kwargs=pickle_kwargs)\n  File \"\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/numpy\/lib\/format.py\", line 585, in write_array\n    array.tofile(fp)\nIOError: 19834920 requested and 8384502 written\n\n---------------------------------------------------------------------------\nIOError                                   Traceback (most recent call last)\n&lt;ipython-input-29-9740e9942629&gt; in &lt;module&gt;()\n      6     cv = KFold(n=X_train.shape[0], n_folds=7, shuffle=True, random_state=12345)\n      7     clf = LogisticRegression(C=C, random_state=12345)\n----&gt; 8     print C, sum(cross_val_score(clf, X_train_scaled, y_train, scoring='roc_auc', cv=cv, n_jobs=2)) \/ 7.0\n\n\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/sklearn\/cross_validation.pyc in cross_val_score(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\n   1431                                               train, test, verbose, None,\n   1432                                               fit_params)\n-&gt; 1433                       for train, test in cv)\n   1434     return np.array(scores)[:, 0]\n   1435 \n\n\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/sklearn\/externals\/joblib\/parallel.pyc in __call__(self, iterable)\n    808                 # consumption.\n    809                 self._iterating = False\n--&gt; 810             self.retrieve()\n    811             # Make sure that we get a last message telling us we are done\n    812             elapsed_time = time.time() - self._start_time\n\n\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/sklearn\/externals\/joblib\/parallel.pyc in retrieve(self)\n    725                 job = self._jobs.pop(0)\n    726             try:\n--&gt; 727                 self._output.extend(job.get())\n    728             except tuple(self.exceptions) as exception:\n    729                 # Stop dispatching any new job in the async callback thread\n\n\/home\/nbcommon\/env\/lib\/python2.7\/multiprocessing\/pool.pyc in get(self, timeout)\n    565             return self._value\n    566         else:\n--&gt; 567             raise self._value\n    568 \n    569     def _set(self, i, obj):\n\nIOError: [Errno 28] No space left on device\n<\/code><\/pre>\n\n<p>With <code>n_jobs=1<\/code> it works fine.<\/p>\n\n<p>I think this is because <code>joblib<\/code> library tries to save my data to <code>\/dev\/shm<\/code>. The problem is that it has only 64M capacity:<\/p>\n\n<pre><code>Filesystem         Size  Used Avail Use% Mounted on\nnone               786G  111G  636G  15% \/\ntmpfs               56G     0   56G   0% \/dev\nshm                 64M     0   64M   0% \/dev\/shm\ntmpfs               56G     0   56G   0% \/sys\/fs\/cgroup\n\/dev\/mapper\/crypt  786G  111G  636G  15% \/etc\/hosts\n<\/code><\/pre>\n\n<p>I can't change this folder by setting <code>JOBLIB_TEMP_FOLDER<\/code> environment variable (<code>export<\/code> doesn't work).<\/p>\n\n<pre><code>In [35]: X_train_scaled.nbytes\n\nOut[35]: 158679360\n<\/code><\/pre>\n\n<p>Thanks for any advice!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1457871506333,
        "Question_score":2,
        "Question_tags":"python|azure|scikit-learn|joblib|azure-machine-learning-studio",
        "Question_view_count":630,
        "Owner_creation_time":1452022246890,
        "Owner_last_access_time":1657271456120,
        "Owner_location":"Moscow, Russia",
        "Owner_reputation":708,
        "Owner_up_votes":34,
        "Owner_down_votes":69,
        "Owner_views":167,
        "Question_last_edit_time":1457939841649,
        "Answer_body":"<p>The <code>\/dev\/shm<\/code> is a virtual filesystem for passing data between programs that implementation of traditional shared memory on Linux.<\/p>\n\n<p>So you could not increase it via set up some options on Application Layout.<\/p>\n\n<p>But for example, you can remount <code>\/dev\/shm<\/code> with 8G size in Linux Shell with administrator permission like <code>root<\/code> as follows.<\/p>\n\n<p><code>mount -o remount,size=8G \/dev\/shm<\/code><\/p>\n\n<p>However, it seems that Azure ML studio not support remote access via SSH protocol, so the feasible plan is upgrade the standard tier if using free tier at present.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1457947589352,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35970126",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":57998468,
        "Question_title":"Unpickling error when running fairseq on AML using multiple GPUs",
        "Question_body":"<p>I am trying to run fairseq translation task on AML using 4 GPUs (P100)and it fails with the following error:<\/p>\n\n<blockquote>\n  <p>-- Process 2 terminated with the following error: Traceback (most recent call last):   File\n  \"\/azureml-envs\/azureml_8ef3d311fd9072540e3352d9621cca49\/lib\/python3.6\/site-packages\/fairseq\/distributed_utils.py\",\n  line 174, in all_gather_list\n      result.append(pickle.loads(bytes(out_buffer[2 : size + 2].tolist())))\n  _pickle.UnpicklingError: invalid load key, '\\xad'.<\/p>\n  \n  <p>During handling of the above exception, another exception occurred:<\/p>\n  \n  <p>Traceback (most recent call last):   File\n  \"\/azureml-envs\/azureml_8ef3d311fd9072540e3352d9621cca49\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\",\n  line 19, in _wrap\n      fn(i, *args)   File \"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/nlx-ml-neuralrewrite\/azureml\/pytorch-fairseq_1568826205_6846ecb6\/mounts\/workspacefilestore\/azureml\/pytorch-fairseq_1568826205_6846ecb6\/train.py\",\n  line 272, in distributed_main\n      main(args, init_distributed=True)   File \"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/nlx-ml-neuralrewrite\/azureml\/pytorch-fairseq_1568826205_6846ecb6\/mounts\/workspacefilestore\/azureml\/pytorch-fairseq_1568826205_6846ecb6\/train.py\",\n  line 82, in main\n      train(args, trainer, task, epoch_itr)   File \"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/nlx-ml-neuralrewrite\/azureml\/pytorch-fairseq_1568826205_6846ecb6\/mounts\/workspacefilestore\/azureml\/pytorch-fairseq_1568826205_6846ecb6\/train.py\",\n  line 123, in train\n      log_output = trainer.train_step(samples)   File \"\/azureml-envs\/azureml_8ef3d311fd9072540e3352d9621cca49\/lib\/python3.6\/site-packages\/fairseq\/trainer.py\",\n  line 305, in train_step\n      [logging_outputs, sample_sizes, ooms, self._prev_grad_norm],   File\n  \"\/azureml-envs\/azureml_8ef3d311fd9072540e3352d9621cca49\/lib\/python3.6\/site-packages\/fairseq\/distributed_utils.py\",\n  line 178, in all_gather_list\n      'Unable to unpickle data from other workers. all_gather_list requires all ' Exception: Unable to unpickle data from other workers.\n  all_gather_list requires all workers to enter the function together,\n  so this error usually indicates that the workers have fallen out of\n  sync somehow. Workers can fall out of sync if one of them runs out of\n  memory, or if there are other conditions in your training script that\n  can cause one worker to finish an epoch while other workers are still\n  iterating over their portions of the data.<\/p>\n  \n  <p> 2019-09-18\n  17:28:44,727|azureml.WorkerPool|DEBUG|[STOP]<\/p>\n  \n  <p>Error occurred: User program failed with Exception: <\/p>\n  \n  <p>-- Process 2 terminated with the following error: Traceback (most recent call last):   File\n  \"\/azureml-envs\/azureml_8ef3d311fd9072540e3352d9621cca49\/lib\/python3.6\/site-packages\/fairseq\/distributed_utils.py\",\n  line 174, in all_gather_list\n      result.append(pickle.loads(bytes(out_buffer[2 : size + 2].tolist())))\n  _pickle.UnpicklingError: invalid load key, '\\xad'.<\/p>\n  \n  <p>During handling of the above exception, another exception occurred:<\/p>\n  \n  <p>Traceback (most recent call last):   File\n  \"\/azureml-envs\/azureml_8ef3d311fd9072540e3352d9621cca49\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\",\n  line 19, in _wrap\n      fn(i, *args)   File \"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/nlx-ml-neuralrewrite\/azureml\/pytorch-fairseq_1568826205_6846ecb6\/mounts\/workspacefilestore\/azureml\/pytorch-fairseq_1568826205_6846ecb6\/train.py\",\n  line 272, in distributed_main\n      main(args, init_distributed=True)   File \"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/nlx-ml-neuralrewrite\/azureml\/pytorch-fairseq_1568826205_6846ecb6\/mounts\/workspacefilestore\/azureml\/pytorch-fairseq_1568826205_6846ecb6\/train.py\",\n  line 82, in main\n      train(args, trainer, task, epoch_itr)   File \"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/nlx-ml-neuralrewrite\/azureml\/pytorch-fairseq_1568826205_6846ecb6\/mounts\/workspacefilestore\/azureml\/pytorch-fairseq_1568826205_6846ecb6\/train.py\",\n  line 123, in train\n      log_output = trainer.train_step(samples)   File \"\/azureml-envs\/azureml_8ef3d311fd9072540e3352d9621cca49\/lib\/python3.6\/site-packages\/fairseq\/trainer.py\",\n  line 305, in train_step\n      [logging_outputs, sample_sizes, ooms, self._prev_grad_norm],   File\n  \"\/azureml-envs\/azureml_8ef3d311fd9072540e3352d9621cca49\/lib\/python3.6\/site-packages\/fairseq\/distributed_utils.py\",\n  line 178, in all_gather_list\n      'Unable to unpickle data from other workers. all_gather_list requires all ' Exception: Unable to unpickle data from other workers.\n  all_gather_list requires all workers to enter the function together,\n  so this error usually indicates that the workers have fallen out of\n  sync somehow. Workers can fall out of sync if one of them runs out of\n  memory, or if there are other conditions in your training script that\n  can cause one worker to finish an epoch while other workers are still\n  iterating over their portions of the data.<\/p>\n<\/blockquote>\n\n<p>The same code with same param runs fine on a single local GPU. How do I resolve this issue?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1568829707947,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":216,
        "Owner_creation_time":1568829254257,
        "Owner_last_access_time":1573858168580,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57998468",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":55443270,
        "Question_title":"Separators in data file for azure ml studio",
        "Question_body":"<p>I have csv file news.csv with such data:<\/p>\n\n<pre><code>ID \\t TITLE \\t URL \\t PUBLISHER \\t CATEGORY \\t STORY \\t HOSTNAME \\t TIMESTAMP\n<\/code><\/pre>\n\n<p>But Azure ML studio experiments dont see Separators \\t and when I try to select column I cant do it. How to fix it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1554051628587,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":109,
        "Owner_creation_time":1553239567403,
        "Owner_last_access_time":1557156750967,
        "Owner_location":null,
        "Owner_reputation":67,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55443270",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70380861,
        "Question_title":"Azure ML Dataset Versioning: What is Different if it Points to the Same Data?",
        "Question_body":"<p><strong>Context<\/strong><\/p>\n<p>In AzureML, we are facing an error when running a pipeline. It fails on <code>to_pandas_dataframe<\/code> because a particular dataset &quot;could not be read beyond end of stream&quot;. On its own, this seems to be an issue with the parquet file that is being registered, maybe special characters being misinterpreted.<\/p>\n<p>However, when we explicitly load a previous &quot;version&quot; of this Dataset--which points to the exact same location of data--it works as expected. In the documentation (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-version-track-datasets#versioning-best-practice\" rel=\"nofollow noreferrer\">here<\/a>), Azure says that &quot;when you load data from a dataset, the current data content referenced by the dataset is always loaded.&quot; This makes me think that a new version of the dataset with the same schema will be, well, the same.<\/p>\n<p><strong>Questions<\/strong><\/p>\n<ol>\n<li><p>What makes a Dataset version <em>different<\/em> from another version when both point to the same location? Is it only the schema definition?<\/p>\n<\/li>\n<li><p>Based on these differences, is there a way to figure out why one version would be succeeding and another failing?<\/p>\n<\/li>\n<\/ol>\n<p><strong>Attempts<\/strong><\/p>\n<ul>\n<li>The schemas of the two versions are identical. We can profile both in AzureML, and all the fields have the same profile information.<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1639665519797,
        "Question_score":1,
        "Question_tags":"azure|version|azure-synapse|azure-machine-learning-service",
        "Question_view_count":245,
        "Owner_creation_time":1591385782727,
        "Owner_last_access_time":1663012868427,
        "Owner_location":"Chicago, IL, USA",
        "Owner_reputation":594,
        "Owner_up_votes":71,
        "Owner_down_votes":0,
        "Owner_views":53,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As rightly suggested by @Anand Sowmithiran in comment section, This looks more like a bug with the SDK.<\/p>\n<p>You can raise <a href=\"https:\/\/azure.microsoft.com\/en-us\/support\/create-ticket\/\" rel=\"nofollow noreferrer\">Azure support ticket<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1641212437096,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70380861",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70772040,
        "Question_title":"How to fix Azure ml model deployment Error",
        "Question_body":"<p>I'm trying to deploy a RandomForest model using azure ML with ACI , but after i deploy my service i keep getting this error :<\/p>\n<pre><code>Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception (cryptography 2.9.2 (\/opt\/miniconda\/lib\/python3.6\/site-packages), Requirement.parse('cryptography&gt;=3.3.1; extra == &quot;crypto&quot;'), {'PyJWT'}).*\n<\/code><\/pre>\n<p>Here's a snapshot of the code and the error :\n<a href=\"https:\/\/i.stack.imgur.com\/BbhDT.png\" rel=\"nofollow noreferrer\">enter image description here<\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/llrGg.png\" rel=\"nofollow noreferrer\">enter image description here<\/a>\nCan you please tell me what should i do to fix this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1642601193557,
        "Question_score":0,
        "Question_tags":"python|dependencies|random-forest|azure-machine-learning-service",
        "Question_view_count":250,
        "Owner_creation_time":1642600792490,
        "Owner_last_access_time":1660756607240,
        "Owner_location":null,
        "Owner_reputation":9,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1643392590832,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70772040",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73413792,
        "Question_title":"Error: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\"",
        "Question_body":"<p>I'm running this code on Azure machine learning notebook.<\/p>\n<pre><code>import os\nimport torch\nimport gradio as gr\nfrom vilmedic import AutoModel\nfrom vilmedic.blocks.scorers import RadGraph\nimport glob\n\nmodel, processor = AutoModel.from_pretrained(&quot;rrg\/baseline-mimic&quot;)\ndevice = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)\nmodel = model.to(device)\nradgraph = RadGraph(cuda=-1)\n<\/code><\/pre>\n<p>.....<\/p>\n<pre><code>def run(image, beam_size, num_return_sequences, include_words, exclude_words, do_radgraph):\n    if image is None:\n        return {}, '&lt;b&gt;Please select an image&lt;\/b&gt;'  # , &quot;&quot;\n    if num_return_sequences &gt; beam_size:\n        return {}, '&lt;b&gt;&quot;Beam size&quot;&lt;\/b&gt; must be greater or equal than &lt;b&gt;&quot;Number of generated reports&quot;&lt;\/b&gt;'  # , &quot;&quot;\n\n    try:\n        include_words_ids, include_words = get_token_from_strings(include_words)\n        exclude_words_ids, exclude_words = get_token_from_strings(exclude_words)\n\n        if include_words_ids is not None and [3] in include_words_ids:\n            return {}, '&lt;b&gt;&quot;' + include_words[\n                include_words_ids.index([3])] + '&quot;&lt;\/b&gt; is not in the vocabulary&quot;&lt;\/b&gt;'  # , &quot;&quot;\n\n        with torch.no_grad():\n            batch = processor.inference(image=[\n                [image]\n            ])\n            batch_size = 1\n            encoder_output, encoder_attention_mask = model.encode(**batch)\n            expanded_idx = torch.arange(batch_size).view(-1, 1).repeat(1, beam_size).view(-1)\n            input_ids = torch.ones((len(batch[&quot;images&quot;]), 1), dtype=torch.long)\n            if torch.cuda.is_available():\n                expanded_idx = expanded_idx.cuda()\n                print(&quot;89 line&quot;)\n                input_ids = input_ids.cuda()\n\n            # Using huggingface generate method\n            hyps = model.dec.generate(\n                input_ids=input_ids * model.dec.config.bos_token_id,\n                encoder_hidden_states=encoder_output.index_select(0, expanded_idx),\n                encoder_attention_mask=encoder_attention_mask.index_select(0, expanded_idx),\n                num_return_sequences=num_return_sequences,\n                max_length=processor.tokenizer_max_len,\n                num_beams=beam_size,\n                bad_words_ids=exclude_words_ids,\n                force_words_ids=include_words_ids,\n            )\n\n            # Decode\n            hyps = [processor.tokenizer.decode(h, skip_special_tokens=True, clean_up_tokenization_spaces=False) for h in\n                    hyps]\n\n            # RadGraph\n            if do_radgraph:\n                radgraph_annots = [radgraph(hyps=[h], refs=[h])[-1][0][&quot;entities&quot;] for h in hyps]\n                # Find entites : Radgraph\n                new_hyp_strs = []\n                for hyp_str, radgraph_annot in zip(hyps, radgraph_annots):\n                    values = radgraph_annot.values()\n                    new_hyp_str = hyp_str.split()\n                    for v in values:\n                        new_hyp_str[v[&quot;start_ix&quot;]] = highlight_radgraph_entities(v[&quot;tokens&quot;], v[&quot;label&quot;])\n                    new_hyp_strs.append(' '.join(new_hyp_str))\n            else:\n                new_hyp_strs = hyps\n\n            # Find user entites\n            if include_words is not None:\n                for w in include_words:\n                    new_hyp_strs = [h.replace(w, highlight_word(w, &quot;user&quot;)) for h in new_hyp_strs]\n\n            # Formating\n            new_hyp_strs = [&quot;&lt;p&gt;&lt;b&gt;Hypothesis {}:&lt;\/b&gt; &lt;br\/&gt; {} &lt;\/p&gt;&quot; \\\n                            &quot;&quot;.format(i + 1, h) for i, h in enumerate(new_hyp_strs)] + (\n                               [&quot;&lt;br\/&gt;&lt;br\/&gt;&lt;i&gt;Anat: anatomy&lt;br\/&gt;&quot;\n                                &quot;OBS: observation&lt;br\/&gt;&quot;\n                                &quot;DA: definitely absent&lt;br\/&gt;&quot;\n                                &quot;DP: definitely present&lt;\/i&gt;&quot;] if do_radgraph else [&quot;&quot;])\n\n            # Params\n            out_json = {\n                &quot;beam size&quot;: beam_size, &quot;number of generated reports&quot;: num_return_sequences,\n                &quot;included words&quot;: include_words, &quot;excluded words&quot;: exclude_words, &quot;show radgraph&quot;: do_radgraph\n            }\n\n            return out_json, str(''.join(new_hyp_strs))  # , str(refs[os.path.basename(image)])\n\n    except Exception as e:\n        print(e)\n        return {}, &quot;&lt;b&gt;An error occured, try again...&quot;\n\n<\/code><\/pre>\n<p>The full code is here:<\/p>\n<p><a href=\"https:\/\/huggingface.co\/spaces\/StanfordAIMI\/radiology_report_generation\/blob\/main\/app.py\" rel=\"nofollow noreferrer\">https:\/\/huggingface.co\/spaces\/StanfordAIMI\/radiology_report_generation\/blob\/main\/app.py<\/a><\/p>\n<p>It keeps giving me this error when I upload an image and click the submit button:\n<strong>Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!<\/strong>.\nI'm using an azure computation with following specifications:\nVirtual machine size: <em>Standard_NV6 (6 cores, 56 GB RAM, 380 GB disk)<\/em>\nProcessing unit: <em>GPU - 1 x NVIDIA Tesla M60<\/em><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1660897192447,
        "Question_score":0,
        "Question_tags":"pytorch|azure-machine-learning-studio|huggingface",
        "Question_view_count":147,
        "Owner_creation_time":1659682623830,
        "Owner_last_access_time":1663873466410,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1660905850609,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73413792",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":50414639,
        "Question_title":"How to Publish an Azure Bot",
        "Question_body":"<p>Just learning how to use <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/bot-service\/?view=azure-bot-service-3.0\" rel=\"nofollow noreferrer\">Azure Bot Service<\/a> and <code>Azure Bot Framework<\/code>. I created a Bot in Azure portal following <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/bot-service\/bot-service-quickstart?view=azure-bot-service-3.0\" rel=\"nofollow noreferrer\">this<\/a> Official Azure tutorial. Does this bot need to be published somewhere? I read somewhere that you <code>Build--&gt;Test--&gt;Publish--&gt;Evaluate<\/code>. I've tested it in Azure portal itself as explained <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/bot-service\/bot-service-quickstart?view=azure-bot-service-3.0\" rel=\"nofollow noreferrer\">here<\/a>. Not sure about the Publish part of it.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1526657122620,
        "Question_score":3,
        "Question_tags":"azure|botframework|azure-machine-learning-studio|azure-bot-service",
        "Question_view_count":844,
        "Owner_creation_time":1330144099340,
        "Owner_last_access_time":1664039192277,
        "Owner_location":null,
        "Owner_reputation":19815,
        "Owner_up_votes":2703,
        "Owner_down_votes":22,
        "Owner_views":2272,
        "Question_last_edit_time":null,
        "Answer_body":"<p>How do you intend to use your bot? Azure Bots work by connecting them to existing channels like Skype, Facebook Messenger, SMS, etc or making REST calls from a custom application.<\/p>\n\n<p>However you can also reach your bot directly from: <code>https:\/\/webchat.botframework.com\/embed\/YOUR_BOT_ID?t=YOUR_TOKEN_HERE<\/code><\/p>\n\n<p>You can embed it on any web page with this HTML tag:<\/p>\n\n<pre><code>&lt;iframe src=\"https:\/\/webchat.botframework.com\/embed\/YOUR_BOT_ID?t=YOUR_TOKEN_HERE\"&gt;&lt;\/iframe&gt;\n<\/code><\/pre>\n\n<p>Please note that both of these methods expose your token and would allow other developers to add your bot to their pages as well.<\/p>\n\n<p>Bot ID is the name of your bot and you can get the token from the portal by going to your bot and choosing \"Channel\" blade and then clicking the \"Get bot embed codes\" link.<\/p>\n\n<p>Edit: I went ahead and wrote a blog post on this topic <a href=\"https:\/\/medium.com\/@joelatwar\/how-to-embed-your-azure-web-app-bot-in-any-web-page-120dfda91fdc\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@joelatwar\/how-to-embed-your-azure-web-app-bot-in-any-web-page-120dfda91fdc<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1526659876240,
        "Answer_score":5.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1527282754360,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50414639",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71063820,
        "Question_title":"AzureML: Dataset Profile fails when parquet file is empty",
        "Question_body":"<p>I have created a Tabular Dataset using Azure ML python API. Data under question is a bunch of parquet files (~10K parquet files each of size of 330 KB) residing in Azure Data Lake Gen 2 spread across multiple partitions. When I trigger &quot;Generate Profile&quot; operation for the dataset, it throws following error while handling empty parquet file and then the profile generation stops.<\/p>\n<pre><code>User program failed with ExecutionError: \nError Code: ScriptExecution.StreamAccess.Validation\nValidation Error Code: NotSupported\nValidation Target: ParquetFile\nFailed Step: 77866d0a-8243-4d3d-8bc6-599d466488dd\nError Message: ScriptExecutionException was caused by StreamAccessException.\n  Failed to read Parquet file at: &lt;my_blob_path&gt;\/20211217.parquet\n    Current parquet file is not supported.\n      Exception of type 'Thrift.Protocol.TProtocolException' was thrown.\n| session_id=6be4db0b-bdc1-4dd6-b8a6-6e9466f7bc54\n\n<\/code><\/pre>\n<p>By empty parquet file, I mean that the if I read the individual parquet file using pandas (<code>pd.read_parquet<\/code>), it results in an empty DF (df.empty == True).<\/p>\n<p>Any suggestion to avoid this error will be appreciated.<\/p>\n<p><strong>Update<\/strong>\nThe issue has been fixed in the following version:<\/p>\n<ul>\n<li>azureml-dataprep : 3.0.1<\/li>\n<li>azureml-core :  1.40.0<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1644490387177,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":255,
        "Owner_creation_time":1280505139753,
        "Owner_last_access_time":1663935737867,
        "Owner_location":"Bangalore, India",
        "Owner_reputation":4265,
        "Owner_up_votes":315,
        "Owner_down_votes":11,
        "Owner_views":403,
        "Question_last_edit_time":1648643496672,
        "Answer_body":"<p>Thanks for reporting it.\nThis is a bug in handling of the parquet files with columns but empty row set. This has been fixed already and will be included in next release.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1646432534340,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71063820",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":32884296,
        "Question_title":"Web service input into SQL query into R in Azure ML",
        "Question_body":"<p>I have the following simple setup in Azure ML. <a href=\"https:\/\/i.stack.imgur.com\/kWi0S.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kWi0S.jpg\" alt=\"ML setup\"><\/a> \nBasically the Reader is a SQL query to a DB which returns a vector called Pdelta, which is then passed to the R script for further processing  and the results are then returned back to the web service. The DB query is simple (<code>SELECT Pdelta FROM ...<\/code>) and it works fine. I have set the DB query as a web service paramater as well. <\/p>\n\n<p>Everything seems to work fine, but at the end when i publish it as a web service and test it, it somehow asks for an additional input parameter. The additional parameter gets called <code>PDELTA<\/code>.\n<a href=\"https:\/\/i.stack.imgur.com\/mnzPZ.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mnzPZ.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I am wondering why is this happening, what is it that I am overlooking? I would like to make this web service ask for only one parameter - the SQL query (Delta Query) which would then deliver the Pdeltas. <\/p>\n\n<p>Any ideas or suggestions would be grealty appreciated! <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1443692439630,
        "Question_score":3,
        "Question_tags":"web-services|azure|cortana-intelligence|azure-machine-learning-studio",
        "Question_view_count":347,
        "Owner_creation_time":1432829415467,
        "Owner_last_access_time":1542573864587,
        "Owner_location":null,
        "Owner_reputation":501,
        "Owner_up_votes":184,
        "Owner_down_votes":0,
        "Owner_views":76,
        "Question_last_edit_time":1456850075240,
        "Answer_body":"<p>You can remove the web service input block and publish the web service without it. That way the Pdelta input will be passed in only from the Reader module.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1444147981107,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/32884296",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":37418265,
        "Question_title":"Azure Machine Learning using Javascript Ajax call",
        "Question_body":"<p>I wanted to know if there is a way to call the Azure Machine Learning webservice using JavaScript Ajax.<\/p>\n\n<p>The Azure ML gives sample code for C#, Python and R.<\/p>\n\n<p>I did try out to call the webservice using JQuery Ajax but it returns a failure.<\/p>\n\n<p>I am able to call the same service using a python script.<\/p>\n\n<p>Here is my Ajax code : <\/p>\n\n<pre><code>  $.ajax({\n        url: webserviceurl,\n        type: \"POST\",           \n        data: sampleData,            \n        dataType:'jsonp',                        \n        headers: {\n        \"Content-Type\":\"application\/json\",            \n        \"Authorization\":\"Bearer \" + apiKey                       \n        },\n        success: function (data) {\n          console.log('Success');\n        },\n        error: function (data) {\n           console.log('Failure ' +  data.statusText + \" \" + data.status);\n        },\n  });\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_time":1464104729493,
        "Question_score":2,
        "Question_tags":"javascript|ajax|azure|azure-machine-learning-studio",
        "Question_view_count":1607,
        "Owner_creation_time":1460664823627,
        "Owner_last_access_time":1504814384707,
        "Owner_location":null,
        "Owner_reputation":81,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1526047792276,
        "Answer_body":"<p>Well after a lot of RnD, I was able to finally call Azure ML using some workarounds.<\/p>\n\n<p>Wrapping Azure ML webservice on Azure API is one option.<\/p>\n\n<p>But, what I did was that I created a python webservice which calls the Azure webservice.<\/p>\n\n<p>So now my HTML App calls the python webservice which calls Azure ML and returns data to the HTML App.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1464718210400,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37418265",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70802006,
        "Question_title":"Clean Up Azure Machine Learning Blob Storage",
        "Question_body":"<p>I manage a frequently used Azure Machine Learning workspace. With several Experiments and active pipelines. Everything is working good so far. My problem is to get rid of old data from runs, experiments and pipelines. Over the last year the blob storage grew to enourmus size, because every pipeline data is stored.<\/p>\n<p>I have deleted older runs from experimnents by using the gui, but the actual pipeline data on the blob store is not deleted. Is there a smart way to clean up data on the blob store from runs which have been deleted ?<\/p>\n<p>On one of the countless Microsoft support pages, I found the following not very helpfull post:<\/p>\n<p>*Azure does not automatically delete intermediate data written with OutputFileDatasetConfig. To avoid storage charges for large amounts of unneeded data, you should either:<\/p>\n<ol>\n<li>Programmatically delete intermediate data at the end of a pipeline\nrun, when it is no longer needed<\/li>\n<li>Use blob storage with a short-term storage policy for intermediate data (see Optimize costs by automating Azure Blob Storage access tiers)<\/li>\n<li>Regularly review and delete no-longer-needed data*<\/li>\n<\/ol>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-data-in-out-of-pipelines#delete-outputfiledatasetconfig-contents-when-no-longer-needed\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-data-in-out-of-pipelines#delete-outputfiledatasetconfig-contents-when-no-longer-needed<\/a><\/p>\n<p>Any idea is welcome.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1642771426213,
        "Question_score":4,
        "Question_tags":"azure-blob-storage|azure-machine-learning-service",
        "Question_view_count":368,
        "Owner_creation_time":1635428968927,
        "Owner_last_access_time":1659693572257,
        "Owner_location":"Germany",
        "Owner_reputation":61,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70802006",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":66247129,
        "Question_title":"How can I use modules in Azure ML studio designer pipeline?",
        "Question_body":"<p>I am currently using a python script in my Azure pipeline<\/p>\n<pre><code>Import data as Dataframe  --&gt;  Run Python Script  --&gt;  Export Dataframe\n<\/code><\/pre>\n<p>My script is developed locally and <strong>I get import errors when trying to import tensorflow<\/strong>... No problem, guess I just have to add it to environment dependencies somewhere -- and it is here the documentation fails me. They seem to rely on the SDK without touching the GUI, but I am using the designer.<\/p>\n<p>I have at this point already build some enviroments with the dependencies, but utilizing these environments on the run or script level is not obvious to me.<\/p>\n<p>It seems trivial, so any help as to use modules is greatly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1613583198267,
        "Question_score":0,
        "Question_tags":"azure|azure-devops|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":283,
        "Owner_creation_time":1485179654243,
        "Owner_last_access_time":1663760467837,
        "Owner_location":null,
        "Owner_reputation":167,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66247129",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":56812071,
        "Question_title":"Is it possible to access datastores from a Azure ML Service webservice?",
        "Question_body":"<p>According to the Azure ML Service <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data#access-datastores-during-training\" rel=\"nofollow noreferrer\">documentation<\/a> it is possible to access datastores during training, but I couldn't find anything about using data from datastores inside the Webservice.<\/p>\n<p>Even though is not necessary use external data to make an Webservice work, to use my model as I intend I need to use some datasets with features created based on historical data. For example: imagine that I'm trying to forecast if a client is going to pay a bill in the right date a good strategy is to create a feature based on previous payments of this same client.<\/p>\n<p>The only external file that I could use in a Webservice is the 'model.pkl' which stores the ML model that I created previously.<\/p>\n<p>How can I get an Azure ML webservice access a datastore?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1561748683060,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-service",
        "Question_view_count":935,
        "Owner_creation_time":1561571259943,
        "Owner_last_access_time":1629948549313,
        "Owner_location":null,
        "Owner_reputation":19,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1658937485407,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56812071",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64137409,
        "Question_title":"How can I create an Azure dataset in Azure ML studio (through the GUI) from a parquet file created with Azure Spark",
        "Question_body":"<p>I'm trying to load files as a dataset in the GUI of Azure ML Studio. These parquet files have been created through Spark.<\/p>\n<p>In my folder, Spark creates files such as &quot;_SUCCESS&quot; or &quot;_committed_8998000&quot;.<\/p>\n<p>Azure ML Studio is not able to read them or ignore them and tells me:<\/p>\n<pre><code>The provided file(s) have invalid byte(s) for the specified file encoding.\n{\n  &quot;message&quot;: &quot; &quot;\n}\n<\/code><\/pre>\n<p>I selected &quot;Ignore unmatched files path&quot; and yet, it still does not work.<\/p>\n<p>If I remove the &quot;_SUCCESS&quot; and other Spark files, it works.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1601468292143,
        "Question_score":2,
        "Question_tags":"azure|apache-spark|parquet|azure-machine-learning-studio",
        "Question_view_count":178,
        "Owner_creation_time":1423640080283,
        "Owner_last_access_time":1663943557963,
        "Owner_location":"Lyon, France",
        "Owner_reputation":457,
        "Owner_up_votes":19,
        "Owner_down_votes":2,
        "Owner_views":125,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Thanks for the feedback. You can use globing in path. e.g. path = '**\/*.parquet' to select only the parquet files<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1601483944070,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64137409",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":49586005,
        "Question_title":"Azure ML workbench failed installing on Windows 10 Enterprise",
        "Question_body":"<p>Error installing component: azure_cli_ml_cliextension.windows \"The action failed catastrophically with <\/p>\n\n<p>Microsoft.MachineLearning.Installer.Engine.Actions.RegisteredActions.AzureCliException: Unable to get list of currently installed Azure CLI extensions<\/p>\n\n<p>at <\/p>\n\n<p>Microsoft.MachineLearning.Installer.Engine.Actions.RegisteredActions.InstallAzureCliExtensionAction.d__23.MoveNext() in C:\\swarm\\workspace\\Installer-1.2\\Installer.Engine\\Actions\\RegisteredActions\\InstallAzureCliExtensionAction.cs:line 97<\/p>\n\n<p>from there everything is stops....<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1522486296150,
        "Question_score":2,
        "Question_tags":"windows|azure-machine-learning-studio|azure-machine-learning-workbench",
        "Question_view_count":296,
        "Owner_creation_time":1506435894640,
        "Owner_last_access_time":1664081964040,
        "Owner_location":"Southeast Asia",
        "Owner_reputation":1922,
        "Owner_up_votes":1232,
        "Owner_down_votes":72,
        "Owner_views":404,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Workbench is a preview product and issues may occur. Please try and get a newer exe and try again. It also seems like you have azure powershell issues here which I would have expected to be taken care of by the installer, but perhaps you can try and install azure powershell first. <\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1525629892870,
        "Answer_score":2.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49586005",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67161293,
        "Question_title":"Issues accessing a FileDataset created from HTTP URIs in a PythonScriptStep",
        "Question_body":"<p>I\u2019m having some issues trying to access a FileDataset created from two http URIs in an Azure ML Pipeline PythonScriptStep.<\/p>\n<p>In the step, I\u2019m only getting a single file named <code>['https%3A\u2019]<\/code> when doing an <code>os.listdir()<\/code> on my mount point. I would have expected two files, with their actual names instead. This happens both when sending the dataset <code>as_upload<\/code> and <code>as_mount<\/code>. Even happens when I send the dataset reference to the pipeline step and mount it directly from the step.<\/p>\n<p>The dataset is registered in a notebook, the same notebook that creates and invokes the pipeline, as seen below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>tempFileData = Dataset.File.from_files(\n        ['https:\/\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg',\n        'https:\/\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg'])\ntempFileData.register(ws, name='FileData', create_new_version=True)\n\n#...\n\nread_datasets_step = PythonScriptStep(\n    name='The Dataset Reader',\n    script_name='read-datasets.py',\n    inputs=[fileData.as_named_input('Files'), fileData.as_named_input('Files_mount').as_mount(), fileData.as_named_input('Files_download').as_download()],\n    compute_target=compute_target,\n    source_directory='.\/dataset-reader',\n    allow_reuse=False,\n)\n\n<\/code><\/pre>\n<p>The <code>FileDataset<\/code> seems to be registered properly, if I examine it within the notebook I get the following result:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;source&quot;: [\n    &quot;https:\/\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg&quot;,\n    &quot;https:\/\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg&quot;\n  ],\n  &quot;definition&quot;: [\n    &quot;GetFiles&quot;\n  ],\n  &quot;registration&quot;: {\n    &quot;id&quot;: &quot;...&quot;,\n    &quot;name&quot;: &quot;FileData&quot;,\n    &quot;version&quot;: 4,\n    &quot;workspace&quot;: &quot;Workspace.create(...)&quot;\n  }\n}\n<\/code><\/pre>\n<p>For reference, the machine running the notebook is using AML SDK v1.24, whereas the node running the pipeline steps is running v1.25.<\/p>\n<p>Has anybody encountered anything like this? Is there a way to make it work?<\/p>\n<p>Note that I'm specifically looking at file datasets created from web uris, and not necessarily interested in getting a <code>FileDataset<\/code> to work with blob storage or similar.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618832324140,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":91,
        "Owner_creation_time":1250158552417,
        "Owner_last_access_time":1663847198323,
        "Owner_location":"Romania",
        "Owner_reputation":7916,
        "Owner_up_votes":1735,
        "Owner_down_votes":33,
        "Owner_views":801,
        "Question_last_edit_time":1618849094430,
        "Answer_body":"<p>The files should've been mounted at path &quot;https%3A\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg&quot; and &quot;https%3A\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg&quot;.<\/p>\n<p>We retain the directory structure following the url structure to avoid potential conflicts.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1618855169223,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67161293",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":59392060,
        "Question_title":"Azure ML platform wrongly interprets uploaded dataset",
        "Question_body":"<p>I have a data set with about 7999 attributes and 39 labels, with 3339 total observations (resulting in 3339x8038 data set), and I'm trying to upload id to Azure ML platform.\nI've selected the 'type' as 'tabular', encoding as 'utf-8', no row skipping, and use header from first file.\nThe problem is, that the headers are still not included and the data is interpreted as string with 0s, 1s, and commas (see pic <a href=\"https:\/\/imgur.com\/a\/QdQNt1y\" rel=\"nofollow noreferrer\">https:\/\/imgur.com\/a\/QdQNt1y<\/a>)<\/p>\n\n<p>Am I missing something? For smaller data sets it seemed to work. My headers are A1, ... A7999 for the attributes, and L1, ... L39 for the labels.<\/p>\n\n<p>Thanks for help in advance.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1576672108560,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-service",
        "Question_view_count":49,
        "Owner_creation_time":1527091507810,
        "Owner_last_access_time":1663872222817,
        "Owner_location":"Stockholm, Sweden",
        "Owner_reputation":235,
        "Owner_up_votes":9,
        "Owner_down_votes":1,
        "Owner_views":43,
        "Question_last_edit_time":null,
        "Answer_body":"<p>our system does our best guess over file settings when you try to create a dataset, but cannot guarantee perfect guesses in all cases. <\/p>\n\n<p>In such scenarios, you should be able to adjust the settings. We had a bug with the ability to change those settings, but rolled out a fix. Can you try to change those now?  <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1576813812910,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59392060",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":38927230,
        "Question_title":"Panda AssertionError columns passed, passed data had 2 columns",
        "Question_body":"<p>I am working on Azure ML implementation on text analytics with NLTK, the following execution is throwing <\/p>\n\n<pre><code>AssertionError: 1 columns passed, passed data had 2 columns\\r\\nProcess returned with non-zero exit code 1\n<\/code><\/pre>\n\n<p>Below is the code <\/p>\n\n<pre><code># The script MUST include the following function,\n# which is the entry point for this module:\n# Param&lt;dataframe1&gt;: a pandas.DataFrame\n# Param&lt;dataframe2&gt;: a pandas.DataFrame\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    # import required packages\n    import pandas as pd\n    import nltk\n    import numpy as np\n    # tokenize the review text and store the word corpus\n    word_dict = {}\n    token_list = []\n    nltk.download(info_or_id='punkt', download_dir='C:\/users\/client\/nltk_data')\n    nltk.download(info_or_id='maxent_treebank_pos_tagger', download_dir='C:\/users\/client\/nltk_data')\n    for text in dataframe1[\"tweet_text\"]:\n        tokens = nltk.word_tokenize(text.decode('utf8'))\n        tagged = nltk.pos_tag(tokens)\n\n\n      # convert feature vector to dataframe object\n    dataframe_output = pd.DataFrame(tagged, columns=['Output'])\n    return [dataframe_output]\n<\/code><\/pre>\n\n<p>Error is throwing here <\/p>\n\n<pre><code> dataframe_output = pd.DataFrame(tagged, columns=['Output'])\n<\/code><\/pre>\n\n<p>I suspect this to be the tagged data type passed to dataframe, can some one let me know the right approach to add this to dataframe.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1471040597197,
        "Question_score":7,
        "Question_tags":"python|pandas|dataframe|nltk|azure-machine-learning-studio",
        "Question_view_count":48200,
        "Owner_creation_time":1370924418390,
        "Owner_last_access_time":1663478900357,
        "Owner_location":"Toronto, ON, Canada",
        "Owner_reputation":1748,
        "Owner_up_votes":136,
        "Owner_down_votes":55,
        "Owner_views":339,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Try this:<\/p>\n\n<pre><code>dataframe_output = pd.DataFrame(tagged, columns=['Output', 'temp'])\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1471040769603,
        "Answer_score":13.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/38927230",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":57403281,
        "Question_title":"Installing textshape package for Microsoft R Open 3.4.4 on Azure ML Studio",
        "Question_body":"<p>I'm trying to use the R <code>sentimentr<\/code> package on Azure ML Studio. As this package is not supported, I'm trying to install it and its dependencies as described <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/execute-r-script#bkmk_AddingANewPackage\" rel=\"nofollow noreferrer\">in the documentation<\/a>.<\/p>\n\n<p>The steps that I have performed are:<\/p>\n\n<ul>\n<li><p>downloaded Windows binaries from the R Open 3.4.4 snapshot at <a href=\"https:\/\/mran.microsoft.com\/timemachine\" rel=\"nofollow noreferrer\">CRAN time machine<\/a><\/p>\n\n<ul>\n<li><code>sentimentr_2.2.3.zip<\/code><\/li>\n<li><code>syuzhet_1.0.4.zip<\/code><\/li>\n<li><code>textclean_0.6.3.zip<\/code><\/li>\n<li><code>lexicon_0.7.4.zip<\/code><\/li>\n<li><code>textshape_1.5.0.zip<\/code> <\/li>\n<\/ul><\/li>\n<li><p>zipped those zip files into a zipped folder <code>packages.zip<\/code><\/p><\/li>\n<li>uploaded <code>packages.zip<\/code> as a dataset to Microsoft Azure ML Studio<\/li>\n<\/ul>\n\n<p>In my ML experiment I connect the <code>packages.zip<\/code> dataset to the \"Script Bundle (Zip)\" input port on \"Execute R Script\" and include this code:<\/p>\n\n<pre><code># install R package contained in src  \ninstall.packages(\"src\/lexicon_0.7.4.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\ninstall.packages(\"src\/textclean_0.6.3.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\ninstall.packages(\"src\/textshape_1.5.0.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\ninstall.packages(\"src\/syuzhet_1.0.4.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\ninstall.packages(\"src\/sentimentr_2.2.3.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\n# load libraries\nlibrary(sentimentr, lib.loc = \".\", verbose = TRUE)\n<\/code><\/pre>\n\n<p>The experiment runs successfully, until I include a function from <code>sentimentr<\/code>:<\/p>\n\n<pre><code>mydata &lt;- mydata %&gt;%\n  get_sentences() %&gt;%\n  sentiment()\n<\/code><\/pre>\n\n<p>This gives the error:<\/p>\n\n<blockquote>\n  <p>there is no package called 'textshape'<\/p>\n<\/blockquote>\n\n<p>Which is difficult to understand given that the output log does not indicate an issue with the packages:<\/p>\n\n<pre><code>[Information]         The following files have been unzipped for sourcing in path=[\"src\"]:\n[Information]                           Name  Length                Date\n[Information]         1 sentimentr_2.2.3.zip 3366245 2019-08-07 14:57:00\n[Information]         2    syuzhet_1.0.4.zip 2918474 2019-08-07 15:05:00\n[Information]         3  textclean_0.6.3.zip 1154814 2019-08-07 15:13:00\n[Information]         4    lexicon_0.7.4.zip 4551995 2019-08-07 15:17:00\n[Information]         5  textshape_1.5.0.zip  463095 2019-08-07 15:42:00\n[Information]         Loading objects:\n[Information]           port1\n[Information]         [1] \"Loading variable port1...\"\n[Information]         package 'lexicon' successfully unpacked and MD5 sums checked   \n[Information]         package 'textclean' successfully unpacked and MD5 sums checked\n[Information]         package 'textshape' successfully unpacked and MD5 sums checked\n[Information]         package 'syuzhet' successfully unpacked and MD5 sums checked\n[Information]         package 'sentimentr' successfully unpacked and MD5 sums checked\n<\/code><\/pre>\n\n<p>Has anyone seen this, or similar issues? Is it possible that \"successfully unpacked\" is not the same as successfully installed and usable?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1565219530070,
        "Question_score":2,
        "Question_tags":"r|package|azure-machine-learning-studio",
        "Question_view_count":157,
        "Owner_creation_time":1239374952693,
        "Owner_last_access_time":1663925914630,
        "Owner_location":"Sydney, Australia",
        "Owner_reputation":30129,
        "Owner_up_votes":685,
        "Owner_down_votes":51,
        "Owner_views":2937,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I can now answer my own question thanks to <a href=\"https:\/\/twitter.com\/bryan_hepworth\/status\/1159432174225055749\" rel=\"nofollow noreferrer\">a hint on Twitter<\/a> from @bryan_hepworth.<\/p>\n\n<p>The R packages were installed correctly, but not in the standard library location. So when a function from <code>sentimentr<\/code> runs, R tries to load the dependency package <code>textshape<\/code>:<\/p>\n\n<pre><code>library(textshape)\n<\/code><\/pre>\n\n<p>Which of course does not exist <em>in the standard location<\/em> as Azure ML does not support it.<\/p>\n\n<p>The solution is to load <code>textshape<\/code> explicitly from its installed location:<\/p>\n\n<pre><code>library(textshape, lib.loc = \".\")\n<\/code><\/pre>\n\n<p>So the solution is: explicitly load packages that you installed at the start of your R code, rather than letting R try to load them as dependencies, which will fail.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1565302474167,
        "Answer_score":0.0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57403281",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60149987,
        "Question_title":"ERROR: Setup iteration failed: Unidentified error, check logs in portal \/ compute",
        "Question_body":"<p>Getting error when trying to run the autoML through training cluster. But it is running successfully via the local run. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1581335967710,
        "Question_score":1,
        "Question_tags":"python|automl|azure-machine-learning-service",
        "Question_view_count":91,
        "Owner_creation_time":1522597272890,
        "Owner_last_access_time":1662966923173,
        "Owner_location":"Kolkata, West Bengal, India",
        "Owner_reputation":671,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":76,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60149987",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62341639,
        "Question_title":"Run failed: Unable to establish SSH connection in Azure ML Service Pipeline",
        "Question_body":"<p>I am getting <code>Run failed: Unable to establish SSH connection<\/code> error when I trigger my Published Azure ML Pipeline using Azure Function App while the VM is closed. Normally The Azure ML Pipeline should be able to automatically turn the virtual machine on when I trigger it and close the VM when the process done. Otherwise, it doesn't make any sense. <\/p>\n\n<p>Sometimes I don't get such an error and the pipeline just works perfectly. <\/p>\n\n<p>Also, the Pipeline works without a problem when I manually start the VM from AzurePortal before trigger the pipeline.<\/p>\n\n<p>The Published Pipeline uses Azure Data Science Virtual Machine - Ubuntu. I am using username and password to access the VM.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1591955060000,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-service",
        "Question_view_count":253,
        "Owner_creation_time":1449521746610,
        "Owner_last_access_time":1658823186867,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1592055179887,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62341639",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58035744,
        "Question_title":"AML run.log() and run.log_list() fail without error",
        "Question_body":"<p>I have a Pipeline with DatabricksSteps each containing:<\/p>\n\n<pre><code>from azureml.core.run import Run\nrun = Run.get_context()\n#do stuff\nrun.log(name, val, desc)\nrun.log_list(name, vals, desc)\nrun.log_image(title, fig, desc)\n<\/code><\/pre>\n\n<p>Only <code>log_image()<\/code> seems to work.  The image appears in the \"images\" section of the AML experiment workspace as expected, but the \"tracked metrics\" and \"charts\" areas are blank.  In an interactive job, <code>run.log()<\/code> and <code>run.log_list()<\/code> work as expected.  I tested that there is no problem with the arguments by using <code>print()<\/code> instead of <code>run.log()<\/code>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1569018204223,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":121,
        "Owner_creation_time":1465320834943,
        "Owner_last_access_time":1617290067470,
        "Owner_location":"Redmond, WA, USA",
        "Owner_reputation":677,
        "Owner_up_votes":13,
        "Owner_down_votes":2,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Add run.flush() at the end of the script.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1569427861030,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58035744",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67552090,
        "Question_title":"Delete and recreate the registry for an azure machine learning workspace",
        "Question_body":"<p>Our azure machine learning workspace container registry has grown extremely large (4Tb) and has many obsolete entries. I would like to delete the registry and simply create a new one. We do not need any entries from the old one.<\/p>\n<p>If I delete the current registry, create a new one, how do I attach it to the workspace?  I dont want to create a new workspace.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1621122191180,
        "Question_score":2,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":172,
        "Owner_creation_time":1565794118450,
        "Owner_last_access_time":1664072200673,
        "Owner_location":null,
        "Owner_reputation":113,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67552090",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60964381,
        "Question_title":"Getting error while connecting ADLS to Notebook in AML",
        "Question_body":"<p>I am getting below error while connecting dataset created and registered in AML notebook and which is based on ADLS. When I connect this dataset in designer I am able to visualize the same. Below is the code that I am using. Please let me know the solution if anyone have faced the same error.<\/p>\n<h3>Examle 1 Import dataset to notebbok<\/h3>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace, Dataset\n\nsubscription_id = 'abcd'\nresource_group = 'RGB'\nworkspace_name = 'DSG'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndataset = Dataset.get_by_name(workspace, name='abc')\ndataset.to_pandas_dataframe()\n<\/code><\/pre>\n<h3>Error 1<\/h3>\n<pre><code>ExecutionError: Could not execute the specified transform.\n(Error in getting metadata for path \/local\/top.txt.\nOperation: GETFILESTATUS failed with Unknown Error: The operation has timed out..\nLast encountered exception thrown after 5 tries.\n[The operation has timed out.,The operation has timed out.,The operation has timed out.,The operation has timed out.,The operation has timed out.]\n[ServerRequestId:])|session_id=2d67\n<\/code><\/pre>\n<h3>Example 2 Import data from datastore to notebook<\/h3>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace, Datastore, Dataset\n\ndatastore_name = 'abc'\nworkspace = Workspace.from_config()\n\ndatastore = Datastore.get(workspace, datastore_name)\ndatastore_paths = [(datastore, '\/local\/top.txt')]\ndf_ds = Dataset.Tabular.from_delimited_files(\n    path=datastore_paths, validate=True,\n    include_path=False, infer_column_types=True,\n    set_column_types=None, separator='\\t',\n    header=True, partition_format=None\n    )\n\ndf = df_ds.to_pandas_dataframe()\n<\/code><\/pre>\n<h3>Error 2<\/h3>\n<pre><code>Cannot load any data from the specified path. Make sure the path is accessible.\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1585719922157,
        "Question_score":1,
        "Question_tags":"azure-data-lake|azure-machine-learning-service",
        "Question_view_count":277,
        "Owner_creation_time":1585719823390,
        "Owner_last_access_time":1637431208373,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1592644375060,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60964381",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":50024278,
        "Question_title":"How can I use the predictive experiment of Azure ML on my website",
        "Question_body":"<p>I used Azure ML studio and I obtained a predictive experiment with an API key.<\/p>\n\n<p><strong>My question is:<\/strong><br>\nHow can I embed this API key or the text obtained or the predictive experiment on a website or on my portfolio?<br>\nSo that others can use the predictive experiment from the website...<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1524664856743,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":28,
        "Owner_creation_time":1524663632880,
        "Owner_last_access_time":1525247437023,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1550566774003,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50024278",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":47281734,
        "Question_title":"Azure ML Web Service + Python for Querying Pandas Data Frame",
        "Question_body":"<p>I want to use Azure ML Web Service for a non machine learning task with Python. The goal is the following:<\/p>\n\n<p>I have a Pandas DF like this:<\/p>\n\n<pre><code>   Id   Value\n0  111  0.1\n1  222  7.3\n2  333  3.1\n3  444  5.0\n<\/code><\/pre>\n\n<p>I can query this DF successfully (what is the value of a certain row by Id?):<\/p>\n\n<pre><code>float(df.loc[pot['Id'] == 222, 'Value'])\n<\/code><\/pre>\n\n<p>Now, I want to deploy a function in Azure ML Web Service with this functionality where a function uses an uploaded data set as fix lookup table. I constructed the function which gets an Id number as argument, looks for the value in the pre-uploade dataset and gives it back as a float:<\/p>\n\n<pre><code>from azureml import services\nimport pandas as pd\n\n@services.publish(workspace_id, workspace_token)\n@services.types(id=int)\n@services.returns(float)\ndef my_func(id):\n    my_df = ws.datasets[\"uploaded_df.csv\"].to_dataframe()\n    return float(my_df.loc[cent['Id'] == id, 'Value'])\n<\/code><\/pre>\n\n<p>I can deploy it on Azure Web Services but when I try to run a test query It gets stuck (no way even to peep into the details). What is the problem here?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1510650824677,
        "Question_score":0,
        "Question_tags":"python|azure|azure-web-app-service|azure-machine-learning-studio",
        "Question_view_count":428,
        "Owner_creation_time":1466792392047,
        "Owner_last_access_time":1658205048127,
        "Owner_location":null,
        "Owner_reputation":1078,
        "Owner_up_votes":12,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47281734",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":40907847,
        "Question_title":"Azure machine learning endpoints with different column count",
        "Question_body":"<p>I would like to automatically set up Azure machine learning endpoints that don't necessarily have the same amount of variables. I am able to programmatically add new endpoints that are trained on different data as long as they have the same column and variable names (headers).<\/p>\n\n<p>When I try to create a new endpoint using a different column count it works. But when I try to call it it gives me errors.<\/p>\n\n<p>I set up an experiment where the default endpoint is accepting two parameters 'x' and 'y'. Then I trained it on a dataset using three columns 'x1', 'x2' and 'y'. The 'Train Model' module in the training experiment is picking out column 1.<\/p>\n\n<p>Calling the endpoint that was trained using three variables with three input columns:<\/p>\n\n<pre><code>{\n\"error\": {\n    \"code\": \"LibraryExecutionError\",\n    \"message\": \"Module execution encountered an internal library error.\",\n    \"details\": [\n        {\n            \"code\": \"TableSchemaColumnCountMismatch\",\n            \"target\": \" (AFx Library)\",\n            \"message\": \"data: The table column count (3) must match the schema column count (2).\"\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>}<\/p>\n\n<p>Calling the endpoint that was trained using three variables with ony two input columns:<\/p>\n\n<pre><code>{\n\"error\": {\n    \"code\": \"LibraryExecutionError\",\n    \"message\": \"Module execution encountered an internal library error.\",\n    \"details\": [\n        {\n            \"code\": \"ScoredFeaturesMustMatchTrainingFeatures\",\n            \"target\": \"Score Model (AFx Library)\",\n            \"message\": \"table: The data set being scored must contain all features used during training, missing feature(s): 'x2'.\"\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>}<\/p>\n\n<p>It seems to be remembering the setup of the default endpoint and expects all other endpoints to conform to it's metadata. Is there any way around this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1480587217760,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":975,
        "Owner_creation_time":1424173539343,
        "Owner_last_access_time":1663924722033,
        "Owner_location":"Malm\u00f6, Sweden",
        "Owner_reputation":794,
        "Owner_up_votes":1478,
        "Owner_down_votes":2,
        "Owner_views":136,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40907847",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73812159,
        "Question_title":"Azure ML Endpoint error 'GradientBoostingRegressor' object has no attribute 'n_features_'",
        "Question_body":"<p>While running the endpoint testing in Azure ML, I am experiencing one error related to the reading of input data.<\/p>\n<p>Steps followed :<\/p>\n<ol>\n<li>Running Gradient boost model\n2.Train and test the data and save it in the model. pkl file<\/li>\n<li>Registering the model on azure ML and deploying the configuration with the code<\/li>\n<li>Reading score.py for the init() and run()<\/li>\n<\/ol>\n<p>Train.py code<\/p>\n<pre><code>%%writefile $script_folder\/train.py\n\nimport argparse\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import SVC\nimport joblib\nimport pickle\nfrom azureml.core import Workspace, Dataset, Experiment\nfrom azureml.core import Run\nimport re\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import chi2\nimport math\nimport pickle\n#ws = Workspace.from_config()\n#az_dataset = Dataset.get_by_name(ws, 'pricing')\n\n# let user feed in 2 parameters, the location of the data files (from datastore), and the regularization rate of the logistic regression model\n#parser = argparse.ArgumentParser()\n#parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n#parser.add_argument('--regularization', type=float, dest='reg', default=0.01, help='regularization rate')\n#args = parser.parse_args()\n\ntrain_data = pd.read_csv(&quot;C:\\\\Users\\\\abhay\\\\Downloads\\\\Projects_DataScience\\\\Ensemble_Machine_Learning\\\\dataset\\\\train_update.csv&quot;)\n\ncolumn_datatypes = train_data.dtypes\ncategorical_columns = list(column_datatypes[column_datatypes==&quot;object&quot;].index.values)\ncontinuous_columns = list(column_datatypes[column_datatypes==&quot;float64&quot;].index.values)\ncontinuous_columns.remove('loss')\n\n\n\ntotal_rows = train_data.shape[0]\ncolumns_with_blanks_cat = np.random.randint(1,116,2)\ncolumns_with_blanks_cont = np.random.randint(117,130,3)\ncolumns_with_blank = np.append(columns_with_blanks_cat,columns_with_blanks_cont)\n\n#for every column insert 5 blanks at random locations\nfor col in columns_with_blank:\n    rows_with_blanks = np.random.randint(1,total_rows,5)\n    train_data.iloc[rows_with_blanks,col] = np.nan\n    \nclass Data_preprocessing:\n    def __init__(self,train_data):\n        self.train_data = train_data\n    \n    def missing_value_continuous(self,column_names_with_specific_type,imputation_type=&quot;mean&quot;): # null value imputation with mean value\n        if imputation_type==&quot;mean&quot;: # mean imputation \n            mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n            mean_imputer.fit(self.train_data[column_names_with_specific_type])\n            self.train_data[column_names_with_specific_type]=mean_imputer.transform(self.train_data[column_names_with_specific_type])\n        if imputation_type==&quot;median&quot;: # median imputation\n            median_imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n            median_imputer.fit(self.train_data[column_names_with_specific_type])\n            self.train_data[column_names_with_specific_type]=median_imputer.transform(self.train_data[column_names_with_specific_type])\n        return self.train_data\n    \n    def missing_value_categorical(self,column_names_with_specific_type,imputation_type=&quot;most_frequent&quot;): # check for missing categorical column values\n        most_frequent = SimpleImputer(strategy=&quot;most_frequent&quot;)\n        most_frequent.fit(self.train_data[column_names_with_specific_type])\n        self.train_data[column_names_with_specific_type] = most_frequent.transform(train_data[column_names_with_specific_type])\n        return self.train_data\n    \n    def outlier_treatment(self,Q1,Q3,IQR,columns_with_outlier,action): # outlier treatmenr\n        if action==&quot;median&quot;:\n            for i in range(len(columns_with_outlier)):\n                column_name = columns_with_outlier[i]\n                meadian_outlier = np.median(self.train_data[column_name])\n                self.train_data.loc[self.train_data[((self.train_data[column_name]&lt;(Q1[column_name]-(1.5*IQR[column_name])))|(self.train_data[column_name]&gt;(Q3[column_name]+(1.5*IQR[column_name]))))].index,column_name]=meadian_outlier\n        if action==&quot;mean&quot;:\n            for i in range(len(columns_with_outlier)):\n                column_name = columns_with_outlier[i]\n                mean_outlier = np.mean(self.train_data[column_name])\n                self.train_data.loc[self.train_data[((self.train_data[column_name]&lt;(Q1[column_name]-(1.5*IQR[column_name])))|(self.train_data[column_name]&gt;(Q3[column_name]+(1.5*IQR[column_name]))))].index,column_name]=mean_outlier\n        if action==&quot;remove&quot;:\n            for i in range(len(columns_with_outlier)):\n                column_name = columns_with_outlier[i]\n                self.train_data = self.train_data[~((self.train_data[column_name]&lt;(Q1[column_name]-(1.5*IQR[column_name])))|(self.train_data[column_name]&gt;(Q3[column_name]+(1.5*IQR[column_name]))))]\n        return self.train_data    \n    \n    \ncolumn_names = np.array(train_data.columns)\nData_preprocessing_obj = Data_preprocessing(train_data)\ntrain_data = Data_preprocessing_obj.missing_value_continuous(continuous_columns,&quot;median&quot;)\ntrain_data = Data_preprocessing_obj.missing_value_categorical(categorical_columns)\ncolumns_with_outlier = ['cont7','cont9','cont10']\nQ1 = train_data[continuous_columns].quantile(0.25)\nQ3 = train_data[continuous_columns].quantile(0.75)\nIQR = (Q3-Q1)\ntrain_data = Data_preprocessing_obj.outlier_treatment(Q1,Q3,IQR,columns_with_outlier,&quot;median&quot;)\ndef feature_selection_numerical_variables(train_data,qthreshold,corr_threshold,exclude_numerical_cols_list):\n    num_colums = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    numerical_columns = list(train_data.select_dtypes(include=num_colums).columns)\n    numerical_columns = [column for column in numerical_columns if column not in exclude_numerical_cols_list]\n    \n    #remove variables with constant variance\n    constant_filter = VarianceThreshold(threshold=0)\n    constant_filter.fit(train_data[numerical_columns])\n    constant_columns = [column for column in train_data[numerical_columns].columns \n                    if column not in train_data[numerical_columns].columns[constant_filter.get_support()]]\n    if len(constant_columns)&gt;0:\n        train_data.drop(labels=constant_columns, axis=1, inplace=True)\n\n    #remove deleted columns from dataframe\n    numerical_columns = [column for column in numerical_columns if column not in constant_columns]\n        \n    #remove variables with qconstant variance\n    #Remove quasi-constant variables\n    qconstant_filter = VarianceThreshold(threshold=qthreshold)\n    qconstant_filter.fit(train_data[numerical_columns])\n    qconstant_columns = [column for column in train_data[numerical_columns].columns \n                         if column not in train_data[numerical_columns].columns[constant_filter.get_support()]]\n    if len(qconstant_columns)&gt;0:\n        train_data.drop(labels=qconstant_columns, axis=1, inplace=True)\n    \n    #remove deleted columns from dataframe\n    numerical_columns = [column for column in numerical_columns if column not in qconstant_columns]\n    \n    #remove correlated variables\n    correlated_features = set()\n    correlation_matrix = train_data[numerical_columns].corr()\n    ax = sns.heatmap(\n    correlation_matrix, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True)\n    ax.set_xticklabels(\n        ax.get_xticklabels(),\n        rotation=45,\n        horizontalalignment='right');\n    #print(correlation_matrix)\n    \n    for i in range(len(correlation_matrix.columns)):\n        for j in range(i):\n            if abs(correlation_matrix.iloc[i, j]) &gt; corr_threshold:\n                colname = correlation_matrix.columns[i]\n                colcompared = correlation_matrix.columns[j]\n                #check if the column compared against is not in the columns excluded list\n                if colcompared not in correlated_features:\n                    correlated_features.add(colname)\n    train_data.drop(labels=correlated_features, axis=1, inplace=True)\n    \n    return train_data,constant_columns,qconstant_columns,correlated_features\ntrain_data,constant_columns,qconstant_columns,correlated_features =feature_selection_numerical_variables(train_data,0.01,0.75,['loss','id'],)\n\nfor cf1 in categorical_columns:\n    le = LabelEncoder()\n    le.fit(train_data[cf1].unique())\n    filename = cf1+&quot;.sav&quot;\n    pickle.dump(le, open(filename, 'wb'))\n    train_data[cf1] = le.transform(train_data[cf1])\n\n#snippet to calculate the unique values with a categorical columns\ndf = pd.DataFrame(columns=[&quot;Column_Name&quot;,&quot;Count&quot;])\nfor cat in categorical_columns:\n    unique_value_count = len(train_data[cat].unique())\n    df = df.append({'Column_Name': cat, &quot;Count&quot;:int(unique_value_count)}, ignore_index=True)\ncolumns_unique_value = np.array(df.Count.value_counts().index)\n\n#snippet to identify the dependent\/correlated categorical variables and drop them\ncolumns_to_drop_cat = set()\ncorrelated_columns = dict()\nfor unique_value_count in columns_unique_value:\n    if unique_value_count&gt;1:\n        categorical_columns = df.loc[df.Count==unique_value_count,'Column_Name']\n        categorical_columns = categorical_columns.reset_index(drop=True)\n        columns_length=len(categorical_columns)\n        for col in range(columns_length-1):\n            column_to_compare = categorical_columns[col]\n            columns_compare_against = categorical_columns[(col+1):columns_length]\n            chi_scores = chi2(train_data[columns_compare_against],train_data[column_to_compare])\n            if column_to_compare not in columns_to_drop_cat:\n                columns_to_be_dropped = [i for i in range(len(columns_compare_against)) if chi_scores[1][i]&lt;=0.05]\n                columns_to_drop_array = np.array(columns_compare_against)[columns_to_be_dropped]\n                correlated_columns[column_to_compare]=columns_to_drop_array\n                columns_to_drop_cat.update(columns_to_drop_array)\n                \ntrain_data = train_data.drop(columns_to_drop_cat,axis=1)\ncorrelated_features = list(correlated_features)\ncolumns_to_drop_cat = list(columns_to_drop_cat)\ncolumns_to_drop_cat.extend(correlated_features)\ncolumns_to_drop = columns_to_drop_cat.copy()\n\n#output the columns_to_drop file to a csv\ncolumns_to_drop_df=pd.DataFrame(columns_to_drop,columns=['colnames'])\n#columns_to_drop_df.to_csv(&quot;\/model\/columns_to_drop.csv&quot;,index=False)\n\ntrain_data['loss'] = np.log(train_data['loss'])\n\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV\n#convert the int64 columns categorical\nColumn_datatypes= train_data.dtypes\nInteger_columns = list(Column_datatypes.where(lambda x: x ==&quot;int64&quot;).dropna().index.values)\ntrain_data[Integer_columns] = train_data[Integer_columns].astype('category',copy=False)\nX,y = train_data.drop(['id','loss'],axis=1),train_data['loss']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # perform train test split\n\nref_cols=X_train.columns\n\n\nfrom sklearn.ensemble import GradientBoostingRegressor  #GBM algorithm\ngbm_base = GradientBoostingRegressor(\n    max_depth=2,\n    n_estimators=3,\n    learning_rate=1.0)\n\ntrained_model=gbm_base.fit(X_train,y_train)\n\n\n\n# Predict the outcome using Test data - Score Model \nY_test_predict_tuned = gbm_base.predict(X_test)\n\n# Get the probability score - Scored Probabilities\n#Y_prob = gbm_base.predict_proba(X_test)[:, 1]\n\n# Get Confusion matrix and the accuracy\/score - Evaluate\n\nscore =np.sqrt(mean_squared_error(y_test, Y_test_predict_tuned))\n\n#print('Export the model to model.pkl')\n#f = open('fwrk2.pkl', 'wb')\n#pickle.dump(trained_model, f)\n#f.close()\n\n#print('Import the model from model.pkl')\n#f2 = open('fwrk2.pkl', 'rb')\n#clf2 = pickle.load(f2)\n\n#X_new = [[154, 54, 35]]\n#print('New Sample:', X_new)\n#print('Predicted class:', clf2.predict(X_new))\n\n#os.makedirs('outputs', exist_ok=True)\n# note file saved in the outputs folder is automatically uploaded into experiment record\n#joblib.dump(value=trained_model, filename='outputs\/fwrk2.pkl')\n<\/code><\/pre>\n<p>Reading the score.py<\/p>\n<pre><code>%%writefile score.py\nimport json\nimport numpy as np\nimport os\nimport pickle\nimport pandas as pd\nimport joblib\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\nfrom inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n\nfrom azureml.core.model import Model\n\ndef init():\n    global model\n    #model = joblib.load('recommender.pkl')\n    model_path = Model.get_model_path('fwrk2')\n    model = joblib.load(model_path)\n\ninput_sample = pd.DataFrame(data=[{&quot;cat1&quot;:0, &quot;cat4&quot;: 0, &quot;cat14&quot;: 0, &quot;cat15&quot;: 0, &quot;cat18&quot;: 0, &quot;cat19&quot;: 0, &quot;cat20&quot;: 0, &quot;cat21&quot;: 0\n                                   , &quot;cat22&quot;: 0, &quot;cat35&quot;: 0, &quot;cat42&quot;:0, &quot;cat47&quot;: 0, &quot;cat48&quot;: 0, &quot;cat55&quot;: 0\n                                   , &quot;cat56&quot;: 0, &quot;cat58&quot;: 0, &quot;cat59&quot;: 0, &quot;cat60&quot;: 0, &quot;cat61&quot;: 0, &quot;cat62&quot;: 0\n                                   , &quot;cat63&quot;: 0, &quot;cat64&quot;: 0, &quot;cat68&quot;: 0, &quot;cat70&quot;: 0, &quot;cat76&quot;: 0, &quot;cat77&quot;:0\n                                   , &quot;cat78&quot;: 0, &quot;cat82&quot;: 0, &quot;cat85&quot;: 0, &quot;cat86&quot;: 0, &quot;cat89&quot;: 0, &quot;cat91&quot;: 0\n                                   , &quot;cat92&quot;: 0, &quot;cat93&quot;: 0, &quot;cat94&quot;:0, &quot;cat96&quot;: 0, &quot;cat97&quot;: 0, &quot;cat99&quot;: 0\n                                   , &quot;cat100&quot;: 0, &quot;cat101&quot;: 0, &quot;cat103&quot;: 0, &quot;cat105&quot;: 0, &quot;cat107&quot;: 0, &quot;cat109&quot;:0\n                                   , &quot;cat110&quot;: 0, &quot;cat111&quot;: 0, &quot;cat112&quot;: 0, &quot;cat113&quot;: 0, &quot;cat116&quot;: 0, &quot;cont1&quot;: 0\n                                   , &quot;cont2&quot;: 0, &quot;cont3&quot;: 0, &quot;cont4&quot;: 0, &quot;cont5&quot;: 0\n                                   , &quot;cont6&quot;: 0, &quot;cont7&quot;: 0, &quot;cont8&quot;: 0, &quot;cont14&quot;: 0}])\n\noutput_sample = np.array([0])              # This is a integer type sample. Use the data type that reflects the expected result\n\n@input_schema('data', PandasParameterType(input_sample))\n@output_schema(NumpyParameterType(output_sample))\n\ndef run(data):\n    try:\n        result = model.predict(data)\n        # you can return any datatype as long as it is JSON-serializable\n        return result.tolist()\n    except Exception as e:\n        error = str(e)\n        return error\n<\/code><\/pre>\n<p>The endpoint publish is succeeded, and I can see the test feature on the azure portal to enter values, post entering the values.<\/p>\n<pre><code>[{&quot;cat1&quot;:0, &quot;cat4&quot;: 0, &quot;cat14&quot;: 0, &quot;cat15&quot;: 0, &quot;cat18&quot;: 0, &quot;cat19&quot;: 0, &quot;cat20&quot;: 0, &quot;cat21&quot;: 0\n                                   , &quot;cat22&quot;: 0, &quot;cat35&quot;: 0, &quot;cat42&quot;:0, &quot;cat47&quot;: 0, &quot;cat48&quot;: 0, &quot;cat55&quot;: 0\n                                   , &quot;cat56&quot;: 0, &quot;cat58&quot;: 0, &quot;cat59&quot;: 0, &quot;cat60&quot;: 0, &quot;cat61&quot;: 0, &quot;cat62&quot;: 0\n                                   , &quot;cat63&quot;: 0, &quot;cat64&quot;: 0, &quot;cat68&quot;: 0, &quot;cat70&quot;: 0, &quot;cat76&quot;: 0, &quot;cat77&quot;:0\n                                   , &quot;cat78&quot;: 0, &quot;cat82&quot;: 0, &quot;cat85&quot;: 0, &quot;cat86&quot;: 0, &quot;cat89&quot;: 0, &quot;cat91&quot;: 0\n                                   , &quot;cat92&quot;: 0, &quot;cat93&quot;: 0, &quot;cat94&quot;:0, &quot;cat96&quot;: 0, &quot;cat97&quot;: 0, &quot;cat99&quot;: 0\n                                   , &quot;cat100&quot;: 0, &quot;cat101&quot;: 0, &quot;cat103&quot;: 0, &quot;cat105&quot;: 0, &quot;cat107&quot;: 0, &quot;cat109&quot;:0\n                                   , &quot;cat110&quot;: 0, &quot;cat111&quot;: 0, &quot;cat112&quot;: 0, &quot;cat113&quot;: 0, &quot;cat116&quot;: 0, &quot;cont1&quot;: 0\n                                   , &quot;cont2&quot;: 0, &quot;cont3&quot;: 0, &quot;cont4&quot;: 0, &quot;cont5&quot;: 0\n                                   , &quot;cont6&quot;: 0, &quot;cont7&quot;: 0, &quot;cont8&quot;: 0, &quot;cont14&quot;: 0}])\n<\/code><\/pre>\n<p>Error: &quot;'GradientBoostingRegressor' object has no attribute 'n_features&quot;<\/p>\n<p>Please can someone guide what could be the problem in executing the above input sample? Is it related to the version of the package, and if yes, then how to update it and solve it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663838335573,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":21,
        "Owner_creation_time":1517472996617,
        "Owner_last_access_time":1663916153343,
        "Owner_location":"Mumbai",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73812159",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":51546796,
        "Question_title":"How to Add Column (script) transform that queries another column for content",
        "Question_body":"<p>I\u2019m looking for a simple expression that puts a \u20181\u2019 in column E if \u2018SomeContent\u2019 is contained in column D.  I\u2019m doing this in Azure ML Workbench through their Add Column (script) function.  Here\u2019s some examples they give.<\/p>\n\n<pre><code>row.ColumnA + row.ColumnB is the same as row[\"ColumnA\"] + row[\"ColumnB\"] \n1 if row.ColumnA &lt; 4 else 2 \ndatetime.datetime.now() \nfloat(row.ColumnA) \/ float(row.ColumnB - 1) \n'Bad' if pd.isnull(row.ColumnA) else 'Good'\n<\/code><\/pre>\n\n<p>Any ideas on a 1 line script I could use for this?  Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1532635853827,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-workbench",
        "Question_view_count":32,
        "Owner_creation_time":1464172964160,
        "Owner_last_access_time":1633099588737,
        "Owner_location":"Brevard, NC, USA",
        "Owner_reputation":111,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51546796",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":52055933,
        "Question_title":"Azure Machine Learning Studio append rows to dataset",
        "Question_body":"<p>My \"experiment\" is like this,<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/AgnGE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AgnGE.png\" alt=\"Experiment\"><\/a><\/p>\n\n<p>I have 10 rows (excluding header) in \"Dataset.csv\" and 3 rows (excluding header) in the CSV being imported by <em>Import Data<\/em>. The schema of both CSVs is same. I want <em>Add Rows<\/em> to <strong>append<\/strong> the 3 rows to Dataset.csv.<\/p>\n\n<p>The real \"Dataset.csv\" has more than 25,000 rows and is expected to grow. Hence, using <em>Export Data<\/em> to generate a merged dataset (as a new CSV) is not a feasible solution. Any way to implement <strong>append<\/strong> for this scenario?<\/p>\n\n<p>Thanks<\/p>\n\n<p>Update 1:\nDataset.csv is present in ML Studios <em>Dataset<\/em>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/LBimY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LBimY.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_time":1535452910717,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio|ml-studio",
        "Question_view_count":590,
        "Owner_creation_time":1409841727700,
        "Owner_last_access_time":1663859408297,
        "Owner_location":null,
        "Owner_reputation":805,
        "Owner_up_votes":599,
        "Owner_down_votes":0,
        "Owner_views":137,
        "Question_last_edit_time":1535461331150,
        "Answer_body":"<p>So it turns out the <a href=\"https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python\" rel=\"nofollow noreferrer\">Python SDK<\/a> has an <code>update_from_dataframe<\/code> method on it that can be used to update a dataset that has been uploaded to Azure ML Studio. If you're unable to use a new CSV and need to update an existing data set, then this should do the trick.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1535538465923,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52055933",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":30068341,
        "Question_title":"How to write the results of Azure ML web service to the azure sql database (The output of Azure ML web service is in Json structure)",
        "Question_body":"<p>The results can be written to SQL Azure using the writer module in the experiment but after publishing the web service the output comes in the Json Structure and it doesn't go to the writer module <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1430890566837,
        "Question_score":0,
        "Question_tags":"azure-sql-database|azure-scheduler|azure-machine-learning-studio",
        "Question_view_count":990,
        "Owner_creation_time":1403541426413,
        "Owner_last_access_time":1592469887883,
        "Owner_location":"Bengaluru, India",
        "Owner_reputation":191,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Question_last_edit_time":1431065815883,
        "Answer_body":"<p>Don't set output port and use Batch execution service - details are provided here - <a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-publish-a-machine-learning-web-service\/\" rel=\"nofollow\">Publish web service<\/a> and <a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-consume-web-services\/\" rel=\"nofollow\">consume web service<\/a><\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1430897690129,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/30068341",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":57471007,
        "Question_title":"Is there way to fix CSV reading in Russian Language in Azure ML Studio?",
        "Question_body":"<p>I have a large csv file containing some text in Russian language. When I upload it to Azure ML Studio as dataset, it appears like \"\ufffd\ufffd\ufffd\ufffd\". What I can do to fix that problem?<\/p>\n\n<p>I tried changing encoding of my text to UTF8, KOI8-R.<\/p>\n\n<p>There is no code, but I can share part of the dataset for you to try.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1565669343940,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":141,
        "Owner_creation_time":1565668918750,
        "Owner_last_access_time":1566796327907,
        "Owner_location":"Nur-Sultan, Kazakhstan",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57471007",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70517166,
        "Question_title":"Azure ML Studio- Container has crashed. Did your init method fail",
        "Question_body":"<p>I am trying to deploy an ML model through the Azure ML Studio using the notebook itself. The commands we are using can be found here <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#define-an-inference-configuration\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#define-an-inference-configuration<\/a><\/p>\n<p>We have registered the model as below-<\/p>\n<pre><code>from azureml.core.model import Model\nmodel = Model.register(ws, model_name=&quot;pdmrfull&quot;, model_path=&quot;pdmrfull.model&quot;)\n<\/code><\/pre>\n<p>But while running this command-<\/p>\n<pre><code>service = Model.deploy(\n    ws,\n    &quot;myservice&quot;,\n    [&quot;pdmrfull.model&quot;],\n    dummy_inference_config,\n    deployment_config,\n    overwrite=True,\n)\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n<p>We are getting the error that <code>container has crashed. Did your init method fail?<\/code><\/p>\n<pre><code>  File &quot;\/var\/azureml-app\/pdmscore.py&quot;, line 3, in &lt;module&gt;\n    from pyspark.ml import Pipeline\nModuleNotFoundError: No module named 'pyspark'\n<\/code><\/pre>\n<p>The <code>init<\/code> method is-<\/p>\n<pre><code>def init():\n    pipeline = PipelineModel.load('pdmrfull.model')\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1640768195997,
        "Question_score":1,
        "Question_tags":"python|azure|machine-learning|azure-machine-learning-studio",
        "Question_view_count":499,
        "Owner_creation_time":1531888247987,
        "Owner_last_access_time":1663731443473,
        "Owner_location":"India",
        "Owner_reputation":981,
        "Owner_up_votes":34,
        "Owner_down_votes":11,
        "Owner_views":268,
        "Question_last_edit_time":1660803227492,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70517166",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60980937,
        "Question_title":"Recent change in create Dataset from Datastore UI?",
        "Question_body":"<p>I'm working on the new feature \"Data Drift in Azure ML\", but I had problems when I was testing this tool, the interface to create Datasets from Datastore is not like the documentation, is it different?.<\/p>\n\n<p>Also, when I try using format partition like the documentation, the configuration shows me some errors, it only works with this format <code>\/{datetime}\/filename.cvs<\/code> instead of <code>\/{timestamp:yyyy\/MM\/dd}\/filename.csv<\/code>, and the schema section doesn't display the schema like in the documentation.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1585777982300,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":80,
        "Owner_creation_time":1583169309800,
        "Owner_last_access_time":1652380247513,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1587600666860,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60980937",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":61225608,
        "Question_title":"Azure Machine Learning : unable to create compute instance with 12 month trial license",
        "Question_body":"<p>I am preparing for DP-100 certification.\nAs a part of self-learning, I created one trial account using my outlook email for 12 months.\nDuring the part where I needed to set up a <strong>Compute Instance<\/strong>, I encounter an error as shown below.\nWhat is the reason, Can't I create a <strong>Compute Instance<\/strong> with a trial account.<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/MWsnt.png\" alt=\"enter image description here\"><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1586943598133,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-service|azure-machine-learning-workbench",
        "Question_view_count":19,
        "Owner_creation_time":1586942899727,
        "Owner_last_access_time":1600269683563,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1586944099443,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61225608",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":40036956,
        "Question_title":"Increasing processing power of Azure Machine Learning workspace",
        "Question_body":"<p>Is there a way to increase the processing power of the Azure ML? I've deployed a neural network on a huge dataset (8000+ retina images, and Azure is taking an impossible amount of time to run the programme. Is it possible to deploy the ML workspace from a Virtual Machine, so that I can leverage increased processing speeds? Help!!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1476428657303,
        "Question_score":2,
        "Question_tags":"image-processing|virtual-machine|azure-machine-learning-studio|azure-dsvm",
        "Question_view_count":706,
        "Owner_creation_time":1476428113303,
        "Owner_last_access_time":1479196975893,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1510175957023,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40036956",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":69755442,
        "Question_title":"Getting all existing piplines from an Azure Machine Learning Workspace",
        "Question_body":"<p>I use azure machine learning services (aml) to run a ml-model. When I go to the GUI of AML I can see all the exisiting piplines, but I can't see how they are scheduled. I need to get all puplished piplines and the belonging meta data.<\/p>\n<p>How I can get information about an existing pipline with the python sdk?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1635429379477,
        "Question_score":2,
        "Question_tags":"python|azure-machine-learning-service|azure-machine-learning-workbench",
        "Question_view_count":82,
        "Owner_creation_time":1635428968927,
        "Owner_last_access_time":1659693572257,
        "Owner_location":"Germany",
        "Owner_reputation":61,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1636156255950,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69755442",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70538937,
        "Question_title":"Pipeline of pipelines in AzureML",
        "Question_body":"<p>I'm trying to create a flow which as different experiments doing train test split, train, validate, get the best model(among 8 diff algos) and predict. The issue is I need to create a dependency of experiments and I need help in that.\nI'm aware of azure ml pipelines, but I'm looking for something where we can create a pipeline of pipelines, or something which will help me create a pipeline of multiple experiments(with dependency).<\/p>\n<p>eg for sample pipeline:\n(train-test-split)-&gt;(train[custom,Many-model])-&gt;(validate)-&gt;(getbest alogos)-&gt;(predict)\nthere will be other experiments in between for tasks like registering modls.downloading pickles etc.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1640924168193,
        "Question_score":2,
        "Question_tags":"python|azure|azure-pipelines|azure-machine-learning-service|ml-studio",
        "Question_view_count":98,
        "Owner_creation_time":1489047127877,
        "Owner_last_access_time":1662790654517,
        "Owner_location":"India",
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70538937",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70255045,
        "Question_title":"Another Azure ML bug caused by new Compute Common Runtime",
        "Question_body":"<p>Many of my Azure ML Studio Designer pipelines began failing today.  I was able to make a minimum repro:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/2mXzI.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2mXzI.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Simply excluding columns with the <code>Select Columns In Dataset<\/code> node will fail with a <code>JobConfigurationMaxSizeExceeded<\/code> error.<\/p>\n<p>This appears to be a bug introduced by Microsoft's rollout of their new Compute Common Runtime.<\/p>\n<p>If I go into any nodes failing with the <code>JobConfigurationMaxSizeExceeded<\/code> exception and manually set <code>AZUREML_COMPUTE_USE_COMMON_RUNTIME:false<\/code> in their  <code>Environment JSON<\/code> field, then they will subsequently work correctly.  This is not documented anywhere that I could find, I stumbled over this fix through trial-and-error, and I wasted many hours trying to fix our failing pipelines today.<\/p>\n<p>Does anyone know where I can find a list of possible effects of the Compute Common Runtime migration in Azure ML? I could not find any documentation on this and\/or how it might affect existing Azure ML pipelines.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1638852213613,
        "Question_score":3,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":256,
        "Owner_creation_time":1340833876130,
        "Owner_last_access_time":1663795160110,
        "Owner_location":null,
        "Owner_reputation":751,
        "Owner_up_votes":68,
        "Owner_down_votes":5,
        "Owner_views":73,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70255045",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70092793,
        "Question_title":"Azure ML Studio Local Environment \u2014 Numpy package import failure using the Azure ML Python SDK",
        "Question_body":"<p>I am trying to create a local environment for the ML Studio using the Python SDK, following\n<a href=\"https:\/\/azure.github.io\/azureml-cheatsheets\/docs\/cheatsheets\/python\/v1\/environment\/\" rel=\"nofollow noreferrer\">this official cheatsheet<\/a>. The result should be a conda-like environment that can be used for local testing. However, I am running into an error when importing the Numpy package with the <code>add_conda_package()<\/code> method of the <code>CondaDependencies()<\/code> class. Where I've tried not specifying, as well as specifying package versions, like:\n<code>add_conda_package('numpy')<\/code> or <code>add_conda_package('numpy=1.21.2')<\/code>, but it does not seem to make a difference.<\/p>\n<p>Numpy's error message is extensive, and I've tried many of the suggestions, without success nonetheless. I'm grateful for any tips on what might resolve my issues!<\/p>\n<hr \/>\n<h2>Full code<\/h2>\n<pre><code>from azureml.core import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\n\ndef get_env() -&gt; Environment:\n    conda = CondaDependencies()\n\n    # add channels\n    conda.add_channel('defaults')\n    conda.add_channel('conda-forge')\n    conda.add_channel('pytorch')\n\n    # Python\n    conda.add_conda_package('python=3.8')\n\n    # Other conda packages\n    conda.add_conda_package('cudatoolkit=11.3')\n    conda.add_conda_package('pip')\n    conda.add_conda_package('python-dateutil')\n    conda.add_conda_package('python-dotenv')\n    conda.add_conda_package('pytorch=1.10')\n    conda.add_conda_package('torchaudio')\n    conda.add_conda_package('torchvision')\n    conda.add_conda_package('wheel')\n    conda.add_conda_package('numpy=1.21.2') # &lt;--- Error with this import \n\n    # create environment\n    env = Environment('test_env')\n    env.python.conda_dependencies = conda\n\n    return env\n<\/code><\/pre>\n<hr \/>\n<h2>Detailed error message:<\/h2>\n<p>User program failed with ImportError:<\/p>\n<p>IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!<\/p>\n<p>Importing the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.<\/p>\n<p>We have compiled some common reasons and troubleshooting tips at:<\/p>\n<pre><code>https:\/\/numpy.org\/devdocs\/user\/troubleshooting-importerror.html\n<\/code><\/pre>\n<p>Please note and check the following:<\/p>\n<ul>\n<li>The Python version is: Python3.8 from &quot;&lt;LOCAL_DIR&gt;.azureml\\envs\\azureml_&gt;\\python.exe&quot;<\/li>\n<li>The NumPy version is: &quot;1.19.1&quot;<\/li>\n<\/ul>\n<p>and make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.<\/p>\n<p>Original error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.<\/p>\n<hr \/>\n<h2>System specifications:<\/h2>\n<ul>\n<li>Local OS: Windows 10<\/li>\n<li>ML studio OS: Linux Ubuntu 18<\/li>\n<li>Python version: 3.8<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637742407110,
        "Question_score":0,
        "Question_tags":"python|azure|numpy|azure-machine-learning-studio|azureml-python-sdk",
        "Question_view_count":135,
        "Owner_creation_time":1615216681047,
        "Owner_last_access_time":1663944623320,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I was finally able to resolve the issue by using the pip method instead of the conda method:\n<code>add_pip_package('numpy')<\/code> instead of <code>add_conda_package('numpy')<\/code>\nI can imagine this being the reason for other packages as well.<\/p>\n<hr \/>\n<h2>Full solution<\/h2>\n<pre><code>from azureml.core import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\n\ndef get_env() -&gt; Environment:\n    conda = CondaDependencies()\n\n    # add channels\n    conda.add_channel('defaults')\n    conda.add_channel('conda-forge')\n    conda.add_channel('pytorch')\n\n    # Python\n    conda.add_conda_package('python=3.8')\n\n    # Other conda packages\n    conda.add_conda_package('cudatoolkit=11.3')\n    conda.add_conda_package('pip')\n    conda.add_conda_package('python-dateutil')\n    conda.add_conda_package('python-dotenv')\n    conda.add_conda_package('pytorch=1.10')\n    conda.add_conda_package('torchaudio')\n    conda.add_conda_package('torchvision')\n    conda.add_conda_package('wheel')\n    #conda.add_conda_package('numpy=1.21.2') # &lt;--- Error with this import \n\n    # Add pip packages\n    conda.add_pip_package('numpy') # &lt;--- Fixes import error\n\n    # create environment\n    env = Environment('test_env')\n    env.python.conda_dependencies = conda\n\n    return env\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1637743033889,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70092793",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60316009,
        "Question_title":"Authenticate With Workspace",
        "Question_body":"<p>I have a Pipeline registered in my AML workspace. Now I would like to trigger a pipeline run from an Azure Notebook in the same Workspace.\nIn order to get a reference object to the workspace in the notebook I need to authenticate, e.g. <\/p>\n\n<p><code>ws = Workspace.from_config()<\/code><\/p>\n\n<p>However, InteractiveLoginAthentication is blocked by my company's domain and MsiAuthentication throws an error as well. ServicePrincipalAuthentication works, but how do I keep the secret safe? What is the prefered way of dealing with secrets in the Azure Machine Learning Service Notebooks?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1582188630347,
        "Question_score":2,
        "Question_tags":"azure-machine-learning-service|azure-notebooks",
        "Question_view_count":116,
        "Owner_creation_time":1582187824717,
        "Owner_last_access_time":1618135237967,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60316009",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":68939332,
        "Question_title":"No module named 'azureml' >> !pip install azureml-core >> from azureml.core import Experiment",
        "Question_body":"<p>I am facing issues with <strong>Azure ML<\/strong> when i try to install the SDK with <code>pip install azureml-core<\/code> and then <code>import azureml.core<\/code> in my script. I do not understand how can it be possible to have this error assuming that the package installation is complete and confirmed by the terminal output with:<\/p>\n<blockquote>\n<p>&quot;Requirement already satisfied: azureml-core in\nc:\\python\\python38\\lib\\site-packages&quot;<\/p>\n<\/blockquote>\n<p><em>I have installed <code>azureml-core<\/code> package with the terminal and in the script with <code>!pip install azureml-core<\/code> but still get this error...<\/em><\/p>\n<p><strong>SCRIPT:<\/strong><\/p>\n<pre><code>!pip install azureml-core\nfrom azureml.core import Experiment \nprint(azureml.core.VERSION)\n<\/code><\/pre>\n<p><strong>OUTPUT:<\/strong><\/p>\n<pre><code>      1 #CONNNECTING TO AZURE INSTANCE\n      2 get_ipython().system('pip install azureml-core')\n----&gt; 3 from azureml.core import Experiment\n      4 print(azureml.core.VERSION)\n      5 \n\nModuleNotFoundError: No module named 'azureml'\n<\/code><\/pre>\n<p>I am running the script locally with Python 3.8.10 on a Windows 10 last update and VSCode Insider.<\/p>\n<blockquote>\n<p><strong>My goal is to compute on an Azure instance without going through a remote because I would like to use my local fodlers.<\/strong><\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1R4AD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1R4AD.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629984091987,
        "Question_score":1,
        "Question_tags":"visual-studio-code|python-3.8|azure-machine-learning-studio|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":2454,
        "Owner_creation_time":1587724337817,
        "Owner_last_access_time":1663840945433,
        "Owner_location":null,
        "Owner_reputation":513,
        "Owner_up_votes":37,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68939332",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":34320449,
        "Question_title":"Use Azure ML methods like an API",
        "Question_body":"<p>Is that possible to use machine learning methods from Microsoft Azure Machine Learning  as an API from my own code (without ML Studio) with possibility to calculate everything on their side?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1450294016510,
        "Question_score":1,
        "Question_tags":"frameworks|rapidminer|azure-machine-learning-studio",
        "Question_view_count":180,
        "Owner_creation_time":1336227824220,
        "Owner_last_access_time":1663836582560,
        "Owner_location":null,
        "Owner_reputation":834,
        "Owner_up_votes":159,
        "Owner_down_votes":2,
        "Owner_views":122,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-publish-a-machine-learning-web-service\/\" rel=\"nofollow\">publish<\/a> an experiment (machine learning functions you hooked together in Azure ML Studio) as an API. When you call that API in your custom code you give it your data and all the computation runs in the cloud in Azure ML. <\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1450317613532,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34320449",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":63947132,
        "Question_title":"Trouble connecting to AMLS web service on AKS using Python requests",
        "Question_body":"<p>I am having trouble contacting an AMLS web service hosted on AKS in a vnet. I am able to successfully provision AKS and deploy the models, but I am not able to access the web service using the Python requests module:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>headers = {'Content-Type':'application\/json',\n           'Authorization': 'Bearer ' + &lt;AKS_KEY&gt;}\nresp = requests.post(&lt;AKS_URI&gt;, json={&quot;data&quot;:{&quot;x&quot;: &quot;1&quot;}}, headers=headers)\nprint(resp.text)\n<\/code><\/pre>\n<p>I get the following error:<\/p>\n<blockquote>\n<p>Error: HTTPConnectionPool(host='', port=80): Max retries exceeded with url: &lt;AKS_URL&gt; (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f33f6035a10&gt;: Failed to establish a new connection: [Errno 110] Connection timed out'))<\/p>\n<\/blockquote>\n<p>However, I am able to successfully connect to the web service using Postman:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>curl --location --request POST &lt;AKS_URI&gt; \\\n--header 'Authorization: Bearer &lt;AKS_KEY&gt;' \\\n--header 'Content-Type: application\/json' \\\n--data-raw '{&quot;data&quot;: {&quot;x&quot;: &quot;1&quot;}}'\n<\/code><\/pre>\n<p>If I load the AKS service in my AMLS workspace <code>aks_service.run()<\/code> also gives me the same error message. I don't have these problems when I deploy without vnet integration.<\/p>\n<p>What could be causing this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1600383486567,
        "Question_score":0,
        "Question_tags":"curl|python-requests|postman|azure-aks|azure-machine-learning-service",
        "Question_view_count":76,
        "Owner_creation_time":1589738451347,
        "Owner_last_access_time":1656358607687,
        "Owner_location":null,
        "Owner_reputation":179,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":53,
        "Question_last_edit_time":1600397592800,
        "Answer_body":"<p>I fixed this by adding an inbound security rule enabled for the scoring endpoint in the NSG group that controls the virtual network.<\/p>\n<p>This should be done so that the scoring endpoint can be called from outside the virtual network (see <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-inferencing-vnet\" rel=\"nofollow noreferrer\">documentation<\/a>), but apparently Postman can figure out how to access the endpoint without this security rule!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1600397953580,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63947132",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72388777,
        "Question_title":"API Key for Azure Machine Learning Endpoint",
        "Question_body":"<p>I am using Azure ML, I made my models and now I want to connect them to Data Factory to run some process.<\/p>\n<p>I implement an endpoint, but I can't find the API key for the endpoints. Right now, I have the REST endpoint, but not in key-based authentication enabled, it's false. Do you know how to generate the API key?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653552924533,
        "Question_score":0,
        "Question_tags":"azure|machine-learning|endpoint|azure-machine-learning-service|automl",
        "Question_view_count":206,
        "Owner_creation_time":1653552346057,
        "Owner_last_access_time":1663916267507,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72388777",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":66062015,
        "Question_title":"Executing R script from Azure function",
        "Question_body":"<p>I want to execute a R script every time an azure function is triggered. The R script executes perfectly on Azure machine learning Studio. But I am failing to execute through azure function.\nIs there any way to execute it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1612522682103,
        "Question_score":3,
        "Question_tags":"r|azure-functions|azure-machine-learning-studio",
        "Question_view_count":640,
        "Owner_creation_time":1475153705707,
        "Owner_last_access_time":1663258260290,
        "Owner_location":null,
        "Owner_reputation":79,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":"<p>AFAIK you'll have to create your own Runtime as <code>R<\/code> isn't supported natively.<\/p>\n<p>Have you already tried <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/azure-functions\/functions-create-function-linux-custom-image?tabs=bash%2Cportal&amp;pivots=programming-language-other\" rel=\"nofollow noreferrer\">&quot;Create a function on Linux using a custom container&quot;<\/a>? Interestingly they have given <code>R<\/code> as the example of custom runtime, so hopefully that answers your question.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1612564671216,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66062015",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":68959934,
        "Question_title":"Contextual version conflict error, Microsoft Azure Machine Learning Studio",
        "Question_body":"<p>I'm trying to run this 10 line .ipynb file from Google Colab in Microsoft Azure Machine Learning Studio<\/p>\n<p><a href=\"https:\/\/colab.research.google.com\/drive\/1o_-QIR8yVphfnbNZGYemyEr111CHHxSv?usp=sharing\" rel=\"nofollow noreferrer\">https:\/\/colab.research.google.com\/drive\/1o_-QIR8yVphfnbNZGYemyEr111CHHxSv?usp=sharing<\/a><\/p>\n<p>When I get to this step:<\/p>\n<pre><code>import gradio as gr\nimport tensorflow as tf\nfrom transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n<\/code><\/pre>\n<p>I get this error:<\/p>\n<pre><code>ContextualVersionConflict: (Flask 1.0.3 (\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages), Requirement.parse('Flask&gt;=1.1.1'), {'gradio'})\n<\/code><\/pre>\n<p>I tried to install the Flask 1.1.1 version but I get more errors. Any idea what I should do to get past this step in Azure ML Studio?<\/p>\n<pre><code>!pip install \u2013force-reinstall Flask==1.1.1\n\/\/ More errors\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630103458310,
        "Question_score":2,
        "Question_tags":"python|jupyter-notebook|google-colaboratory|azure-machine-learning-studio",
        "Question_view_count":237,
        "Owner_creation_time":1630103231523,
        "Owner_last_access_time":1664075236640,
        "Owner_location":null,
        "Owner_reputation":17,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1630103807136,
        "Answer_body":"<p>The issue is because <code>gradio<\/code> package using existing Flask package (version 1.0.3). But as your application required Flask&gt;=1.1.1, therefore it is showing error. You need to uninstall the existing Flask package and then install the latest required version.<\/p>\n<p>To uninstall the existing package:\n<code>!pip uninstall Flask -y<\/code><\/p>\n<p>To install latest package:\n<code>!pip install Flask&gt;=1.1.1<\/code><\/p>\n<p><strong>Then, make sure to restart your runtime to pick up the new Flask using the Runtime -&gt; Restart runtime menu.<\/strong><\/p>\n<p>Finally, import gradio.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1630303052769,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68959934",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":57589238,
        "Question_title":"Tensorboard launched by Azure ML package doesn't work correctly",
        "Question_body":"<p>I want to access tfevent file created during training and stored in logs in Azure ML service. This tfevent file can be accessed and shown correctly on normal tensorboard so the file is not broken but when I use Azure ML's tensorboard library to access it, either nothing shows up on local tensorboard or get connection refused.<\/p>\n\n<p>I first logged it into .\/logs\/tensorboard like Azure ML has .\/logs\/azureml but tensorboard launched by Azure ML's module says there is no file to show like this below on the browser.<\/p>\n\n<pre><code>No dashboards are active for the current data set.\nProbable causes:\n\nYou haven\u2019t written any data to your event files.\nTensorBoard can\u2019t find your event files.\nIf you\u2019re new to using TensorBoard, and want to find out how to add data and set up your event files, check out the README and perhaps the TensorBoard tutorial.\nIf you think TensorBoard is configured properly, please see the section of the README devoted to missing data problems and consider filing an issue on GitHub.\n\nLast reload: Wed Aug 21 2019 *****\nData location: \/tmp\/tmpkfj7gswu\n<\/code><\/pre>\n\n<p>So I thought that saved location would not be recognized by AML and I changed the save location to .\/logs then browser shows that \"This site can\u2019t be reached. ****** refused to connect.\"<\/p>\n\n<p>My Azure ML Python SDK version is 1.0.57<\/p>\n\n<p>1) How can I fix this?<\/p>\n\n<p>2) Where should I save tfevent file for AML to recognize it? I couldn't find any information about it in the documentation here. \n<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-tensorboard\/azureml.tensorboard.tensorboard?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-tensorboard\/azureml.tensorboard.tensorboard?view=azure-ml-py<\/a><\/p>\n\n<p>This is how I'm launching tensorboard through Azure ML.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>if __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=f'This script is to lanuch TensorBoard with '\n        f'accessing run history from machine learning '\n        f'experiments that output Tensorboard logs')\n    parser.add_argument('--experiment-name',\n                        dest='experiment_name',\n                        type=str,\n                        help='experiment name in Azure ML')\n    parser.add_argument('--run-id',\n                        dest='run_id',\n                        type=str,\n                        help='The filename of merged json file.')\n\n    args = parser.parse_args()\n\n    logger = get_logger(__name__)\n    logger.info(f'SDK Version: {VERSION}')\n\n    workspace = get_workspace()\n    experiment_name = args.experiment_name\n    run_id = args.run_id\n    experiment = get_experiment(experiment_name, workspace, logger)\n    run = get_run(experiment, run_id)\n\n    # The Tensorboard constructor takes an array of runs, so pass it in as a single-element array here\n    tb = Tensorboard([run])\n\n    # If successful, start() returns a string with the URI of the instance.\n    url = tb.start()\n    print(url)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1566382098297,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-service",
        "Question_view_count":687,
        "Owner_creation_time":1566350007733,
        "Owner_last_access_time":1580220215467,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57589238",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":66888622,
        "Question_title":"How to effectively use azureml.exceptions.WebserviceException for efficient REST API Error Management?",
        "Question_body":"<p>In Azure Machine Learning Service, when we deploy a Model as an AKS Webservice Endpoint, how can we raise exceptions to let the end-user get proper feedback if their API call is unsuccessful? Azure mentions using <code>azureml.exceptions.WebserviceException<\/code> in their <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.exceptions.webserviceexception?view=azure-ml-py\" rel=\"nofollow noreferrer\">documentation<\/a>. However, how do we use this class to raise exceptions in case the API call cannot be processed properly and the end-user is responsible for it?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1617196440647,
        "Question_score":0,
        "Question_tags":"rest|azure-web-app-service|azure-machine-learning-service",
        "Question_view_count":250,
        "Owner_creation_time":1601729162437,
        "Owner_last_access_time":1663774065773,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":887,
        "Owner_up_votes":187,
        "Owner_down_votes":32,
        "Owner_views":130,
        "Question_last_edit_time":null,
        "Answer_body":"<p>To raise exceptions to let the end-user get proper feedback if their API call is unsuccessful, we use the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-contrib-services\/azureml.contrib.services.aml_response.amlresponse?view=azure-ml-py\" rel=\"nofollow noreferrer\"><code>azureml.contrib.services.aml_response.AMLResponse<\/code> Class<\/a>.<\/p>\n<p>Example of use in <code>score.py<\/code>:<\/p>\n<pre><code>if [some-condition]:    \n    return AMLResponse(&quot;bad request&quot;, 500)\n<\/code><\/pre>\n<p>Documentation Link can be found <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#binary-data\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1617344350896,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66888622",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":51622144,
        "Question_title":"scikit-learn regression prediction results are too good. What did I mess up?",
        "Question_body":"<p>We have some ML models running in Azure on top of the Azure ML Studio platform (the initial drag &amp; drop system). All has been good for over a year but we need to move on so we can scale better. So I'm working on rewriting these in Python using scikit-learn and testing them in an Jupyter notebook.<\/p>\n<p>The good news\/bad news is that our data to train on is fairly small (several hundred records in a database). It's very imperfect data making very imperfect regression predictions, so error is to be expected. And that's fine. And for this question, it's good. Because the problem is that, when I test these models, the predictions are way too perfect. I don't understand what I'm doing wrong, but I'm clearly doing <strong>something<\/strong> wrong.<\/p>\n<p>The obvious things to suspect (in my mind) are that either I'm training on the test data or there's an obvious\/perfect causation found via the correlations. My use of <code>train_test_split<\/code> tells me that I'm not training on my test data and I guarantee the second is false because of how messy this space is (we started doing manual linear regression on this data about 15 years ago, and still maintain Excel spreadsheets to be able to manually do it in a pinch, even if it's significantly less accurate than our Azure ML Studio models).<\/p>\n<p>Let's look at the code. Here's the relevant portion of my Jupyter notebook (sorry if there's a better way to format this):<\/p>\n<pre><code>X = myData\ny = myData.ValueToPredict\nX_train, X_test, y_train, y_test = train_test_split(\n    X, \n    y, \n    train_size = 0.75,\n    test_size = 0.25)\nprint(&quot;X_train: &quot;, X_train.shape)\nprint(&quot;y_train: &quot;, y_train.shape)\nprint(&quot;X_test:  &quot;, X_test.shape)\nprint(&quot;y_test:  &quot;, y_test.shape)\n<\/code><\/pre>\n<blockquote>\n<p>X_train:  (300, 17)<\/p>\n<p>y_train:  (300,)<\/p>\n<p>X_test:   (101, 17)<\/p>\n<p>y_test:   (101,)<\/p>\n<\/blockquote>\n<pre><code>ESTIMATORS = {\n    &quot;Extra Trees&quot;: ExtraTreesRegressor(criterion = &quot;mse&quot;,\n                                       n_estimators=10,\n                                       max_features=16,\n                                       random_state=42),\n    &quot;Decision Tree&quot;: DecisionTreeRegressor(criterion = &quot;mse&quot;,\n                                  splitter = &quot;best&quot;,\n                                       random_state=42),\n    &quot;Random Forest&quot;: RandomForestRegressor(criterion = &quot;mse&quot;,\n                                       random_state=42),\n    &quot;Linear regression&quot;: LinearRegression(),\n    &quot;Ridge&quot;: RidgeCV(),\n}\n\ny_test_predict = dict()\ny_test_rmse = dict()\nfor name, estimator in ESTIMATORS.items():\n    estimator.fit(X_train, y_train)\n    y_test_predict[name] = estimator.predict(X_test)\n    y_test_rmse[name] = np.sqrt(np.mean((y_test - y_test_predict[name]) ** 2)) # I think this might be wrong but isn't the source of my problem\nfor name, error in y_test_rmse.items():\n    print(name + &quot; RMSE: &quot; + str(error))\n<\/code><\/pre>\n<blockquote>\n<p>Extra Trees RMSE: 0.3843540838630157<\/p>\n<p>Decision Tree RMSE: 0.32838969545222946<\/p>\n<p>Random Forest RMSE: 0.4304701784728594<\/p>\n<p>Linear regression RMSE: 7.971345895791494e-15<\/p>\n<p>Ridge RMSE: 0.0001390197344951183<\/p>\n<\/blockquote>\n<pre><code>y_test_score = dict()\nfor name, estimator in ESTIMATORS.items():\n    estimator.fit(X_train, y_train)\n    y_test_predict[name] = estimator.predict(X_test)\n    y_test_score[name] = estimator.score(X_test, y_test)\nfor name, error in y_test_score.items():\n    print(name + &quot; Score: &quot; + str(error))\n<\/code><\/pre>\n<blockquote>\n<p>Extra Trees Score: 0.9990166492769291<\/p>\n<p>Decision Tree Score: 0.999282165241745<\/p>\n<p>Random Forest Score: 0.998766521504593<\/p>\n<p>Linear regression Score: 1.0<\/p>\n<p>Ridge Score: 0.9999999998713534<\/p>\n<\/blockquote>\n<p>I thought maybe I was doing the error metrics wrong, so I just looked at simple scores (which is why I included both). However, both show that these predictions are too good to be true. Keep in mind that the volume of inputs are small (~400 items in total?). And the data this is running over is essentially making predictions of a commodity consumption based on weather patterns, which is kind of a messy space to begin with, so lots of error should be present.<\/p>\n<p>What am I doing wrong here?<\/p>\n<p>(Also, if I can ask this question in a better way or provide more useful information, I'd greatly appreciate it!)<\/p>\n<hr \/>\n<p>Here is a heatmap of the data. I indicated the value we're predicting.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6wdsw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6wdsw.png\" alt=\"Seaborn heatmap of the data\" \/><\/a><\/p>\n<p>I also plotted a couple of those more important inputs vs the value we're predicting (color-coded by yet another dimension):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kOsz3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kOsz3.png\" alt=\"A plot of values we're predicting\" \/><\/a><\/p>\n<p>Here's column #2, as asked about in comments\n<a href=\"https:\/\/i.stack.imgur.com\/R00jM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/R00jM.png\" alt=\"Another plot\" \/><\/a><\/p>\n<hr \/>\n<h1>Solution!<\/h1>\n<p>As pointed out by @jwil, I wasn't pulling my <code>ValueToPredict<\/code> column out of my <code>X<\/code> variable. The solution was a one-liner added to remove that column:<\/p>\n<pre><code>X = myData\ny = myData.ValueToPredict\nX = X.drop(&quot;ValueToPredict&quot;, 1) # &lt;--- ONE-LINE FIX!\nX_train, X_test, y_train, y_test = train_test_split(\n    X, \n    y, \n    train_size = 0.75,\n    test_size = 0.25)\n<\/code><\/pre>\n<p>With this in place, my error &amp; scores are much more where I expect:<\/p>\n<blockquote>\n<p>Extra Trees RMSE: 1.6170428819849574<\/p>\n<p>Decision Tree RMSE: 1.990459810552763<\/p>\n<p>Random Forest RMSE: 1.699801032532343<\/p>\n<p>Linear regression RMSE: 2.5265108241534397<\/p>\n<p>Ridge RMSE: 2.528721533965162<\/p>\n<p>Extra Trees Score: 0.9825944193611161<\/p>\n<p>Decision Tree Score: 0.9736274412836977<\/p>\n<p>Random Forest Score: 0.9807672396970707<\/p>\n<p>Linear regression Score: 0.9575098985510281<\/p>\n<p>Ridge Score: 0.9574355079097321<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_time":1533071537980,
        "Question_score":2,
        "Question_tags":"python|machine-learning|scikit-learn|azure-machine-learning-studio",
        "Question_view_count":1278,
        "Owner_creation_time":1268752315037,
        "Owner_last_access_time":1661196671610,
        "Owner_location":"Indianapolis, IN",
        "Owner_reputation":12751,
        "Owner_up_votes":980,
        "Owner_down_votes":52,
        "Owner_views":1280,
        "Question_last_edit_time":1592644375060,
        "Answer_body":"<p>You're right; I strongly suspect that you have one or more features in your X data that is nearly perfectly correlated with the Y data. Usually this is bad, because those variables don't explain Y but are either explained by Y or jointly determined with Y. To troubleshoot this, consider performing a linear regression of Y on X and then using simple p values or AIC\/BIC to determine which X variables are the least relevant. Drop these and repeat the process until your R^2 begins to drop seriously (though it will drop a little each time). The remaining variables will be the most relevant in prediction, and hopefully you'll be able to identify from that subset which variables are so tightly correlated with Y.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1533073081376,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51622144",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":69495213,
        "Question_title":"Not able to read model.pkl from output folder in Azure ML",
        "Question_body":"<p>I'm try to read the model.pkl file from the artifacts output folder like this<\/p>\n<pre><code>def init():\n    global model\n    # infile = open('model.pkl','rb') \n    # model = pickle.load(infile)\n    #model = joblib.load('model.pkl')\n    model_path = Model.get_model_path(model_name = '&lt;&lt;modelname&gt;&gt;')\n    model_path=&quot;outputs\/model.pkl&quot;\n    # deserialize the model file back into a sklearn model\n    model = joblib.load(model_path)\n<\/code><\/pre>\n<p>But still its not working please guide me how to read model.pkl file from artifacts output folder, because of this it is failing to deploy into Azure container instance<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1633692984917,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service|azureml-python-sdk|azuremlsdk",
        "Question_view_count":172,
        "Owner_creation_time":1557408362973,
        "Owner_last_access_time":1663966969360,
        "Owner_location":null,
        "Owner_reputation":97,
        "Owner_up_votes":24,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69495213",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":29180287,
        "Question_title":"Azure ML & Pandas: How to convert String to DateTime",
        "Question_body":"<p>I've got a dataset at hand with a column of DateTime in String format, eg.<\/p>\n\n<pre><code>a = 'Tue Sep 22 1998 00:00:00 GMT+0000 (Coordinated Universal Time)'\n<\/code><\/pre>\n\n<p>and a is just a value from the column.<\/p>\n\n<p>If I use Metadata Editor in Azure Machine Learning Studio, it won't work and will complain that it can't do the conversion (from String to DateTime). I guess it's something to do with the format. So I'm trying the following:<\/p>\n\n<pre><code>a = str(a)[:10]+','+str(a)[10:15]\n#'Tue Sep 22, 1998'\n<\/code><\/pre>\n\n<p>Now .NET surely can do the conversion, I mean by method like Convert.ToDateTime(). However, when I visualized the output of the Python script, I found the String has been changed into 'Tue Sep 22, 1998 None,', which is quite weird. Anyone knows what's wrong with it? I'm attaching the excerpt of python code down below:<\/p>\n\n<pre><code>def azureml_main(dataframe1 = None, dataframe2 = None):\n\n  dataframe1['timestamp'] = dataframe1['timestamp'].apply(lambda a: str(a)[:10]+','+str(a)[10:15])\n\n  return dataframe1,\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1426922780947,
        "Question_score":3,
        "Question_tags":"python|pandas|machine-learning|azure-machine-learning-studio",
        "Question_view_count":2509,
        "Owner_creation_time":1340282566043,
        "Owner_last_access_time":1663814648533,
        "Owner_location":null,
        "Owner_reputation":1618,
        "Owner_up_votes":1035,
        "Owner_down_votes":5,
        "Owner_views":185,
        "Question_last_edit_time":1453514644060,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/29180287",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":46539159,
        "Question_title":"Call Azure ML Batch Job via Azure Data Factory",
        "Question_body":"<p>I have scheduled the Azure ML Batch Job via Azure Data Factory to run daily at 12:00 AM UTC.<\/p>\n\n<p>Don't know what is the issue, but it is failing for every month's 3rd day, otherwise it runs perfectly.<\/p>\n\n<p>Anybody facing same issue?<\/p>\n\n<p><strong>For September<\/strong> <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/z2sBN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/z2sBN.png\" alt=\"Error Log that display in Azure Data Factory for September\"><\/a><\/p>\n\n<p><strong>For October<\/strong><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/skXoU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/skXoU.png\" alt=\"Error Log that display in Azure Data Factory for October\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/mQ1Oy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mQ1Oy.png\" alt=\"Azure ML Batch Service Job Log\"><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1507015266990,
        "Question_score":1,
        "Question_tags":"azure|azure-data-factory|azure-machine-learning-studio",
        "Question_view_count":150,
        "Owner_creation_time":1347281202153,
        "Owner_last_access_time":1643314917583,
        "Owner_location":"Vadodara, Gujarat, India",
        "Owner_reputation":8112,
        "Owner_up_votes":47,
        "Owner_down_votes":28,
        "Owner_views":597,
        "Question_last_edit_time":1507036866960,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46539159",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":57143923,
        "Question_title":"How to combine two algorithms for predictive analysis in MS Azure",
        "Question_body":"<p>I am developing a combined-algorithm model in MS Azure Machine Learning Studio that should be able to predict whether a Telco-customer churn or not. Given that I have 19 variable features, e.g. monthly fee, usage length etc., how do I combine two useful algorithms? And how do I know these provide the highest Accuracy (Highest possible Accuracy needed), ie. which elements do I yet need to add and how should I use for predicting churn behaviour onto another dataset of a \"fresh\" set of customers? <\/p>\n\n<p>I have:\n1) Edit Metadata\nHaving excluded the User_ID variable I have used the Edit Metadata element to label the Attrition variable (Attrition is whether a customer has churned, i.e. Yes or No). Simultaneously I have transformed Attrition into a categorical variable, specifying that the selected values should be treated in two categories, i.e. Yes or No. <\/p>\n\n<p>Normalize data\nSince the three identified numerical variables (Usage_Length, Monthly_Fee and Total_Fee) a quite different in scale, e.g. Max(Monthly_Fee) is at 78,80 while Max(Total_Fee) is at 5.789,87, I have normalized Monthly_Fee and Total_Fee using the LogNormal Transformation method. <\/p>\n\n<p>Edit Metadata (2nd use)\nHaving normalized two of the numerical values, I have made all non-numerical features, e.g. User_Gender, Is_Senior etc., into categorical values to make them useful for the coming analysis. <\/p>\n\n<p>Split data\nOnce the above steps have been carried out, I have made a testing\/training split of 0.2 and 0.8, respectively on which I run the models.<\/p>\n\n<p>Choice of algorithm\nI have selected Two-class Boosed Decision Tree and Two-Class Decision Forest as they provide the highest possible individual Accuracy; 0.963 and 0.967, respectively. <\/p>\n\n<p>No coding used - only elements added. <\/p>\n\n<p>I expect the highest possible Accuracy, currently at 0.967 when combining the models into an Evaluation element<a href=\"https:\/\/i.stack.imgur.com\/HGPuI.png\" rel=\"nofollow noreferrer\">Current Model Screenshot<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":10,
        "Question_creation_time":1563790976267,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":39,
        "Owner_creation_time":1563790128347,
        "Owner_last_access_time":1565774206103,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1563792544860,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57143923",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":65044700,
        "Question_title":"Proper way to make a request to a model, deployed via Azure ML Designer",
        "Question_body":"<p>I am trying to make the POST request to the Azure ML Designer endpoint (model, I have deployed).\nHere is my code:<\/p>\n<pre><code>import requests\n\nscoring_uri = 'http:some-url\/score'\nkey = 'someKey'\n\nheaders = {'Content-Type': 'application\/json'}\nheaders['Authorization'] = f'Bearer {key}'\n\nresponse = requests.get('https:\/\/www.okino.ua\/media\/var\/news\/2019\/12\/04\/Quentin_Tarantino.jpg')\n\ninput_data = &quot;{\\&quot;data\\&quot;: [&quot; + str(response.content) + &quot;]}&quot;\nresp = requests.post(scoring_uri, data=response.content, headers=headers)\nprint(resp.text)\n<\/code><\/pre>\n<p>And I receive and error:<\/p>\n<pre><code>{&quot;error&quot;: {&quot;code&quot;: 400, &quot;message&quot;: &quot;Input Data Error. Input data are inconsistent with schema.\\nSchema: {'WebServiceInput0': {'columnAttributes': [{'name': 'image', 'type': 'Bytes', 'isFeature': True, 'elementType': {'typeName': 'bytes', 'isNullable': False}, 'properties': {'mime_type': 'image\/png', 'image_ref': 'image_info'}}, {'name': 'id', 'type': 'Numeri\\nData: b'\\\\xff\\\\xd8\\\\xff\\\\xe0\\\\x00\\\\x10JFIF\\\\x00\\\\x01\\\\x01\\\\x01\\\\x01,\\\\x01,\\\\x00\\\\x00\\\\xff\\\\xfe\\\\x00[Copyright Shutterstock 2019;82139424;3600;2400;1563865756;Tue, 23 Jul 2019 07:09:16 GMT;0\\\\xff\\\\xed\\\\x04\\\\x16Photoshop 3.0\\\\x008BIM\\\\x04\\\\x04\\\\x00\\\\x00\\\\x00\\\\x00\\\\x03\\\\xf9\\\\x1c\\\\x02\\\\x05\\\\x00\\\\n103\\nTraceback (most recent call last):\\n  File \\&quot;\/azureml-envs\/azureml_c1330288c44b762b0282b6f129c5292f\/lib\/python3.6\/site-packages\/azureml\/designer\/serving\/dagengine\/processor.py\\&quot;, line 18, in run\\n    webservice_input, global_parameters = self.pre_process(raw_data)\\n  File \\&quot;\/azureml-envs\/azureml_c1330288c44b762b0282b6f129c5292f\/lib\/python3.6\/site-packages\/azureml\/designer\/serving\/dagengine\/processor.py\\&quot;, line 45, in pre_process\\n    json_data = json.loads(raw_data)\\n  File \\&quot;\/azureml-envs\/azureml_c1330288c44b762b0282b6f129c5292f\/lib\/python3.6\/json\/__init__.py\\&quot;, line 349, in loads\\n    s = s.decode(detect_encoding(s), 'surrogatepass')\\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\\n&quot;, &quot;details&quot;: &quot;&quot;}}\n\n<\/code><\/pre>\n<p>Is anyone aware of how I should pass image data to the exposed by Azure ML endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1606517768417,
        "Question_score":1,
        "Question_tags":"python|azure|computer-vision|classification|azure-machine-learning-service",
        "Question_view_count":264,
        "Owner_creation_time":1565038435857,
        "Owner_last_access_time":1606639837553,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1606785467360,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65044700",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73316652,
        "Question_title":"How to read and write a dataset using parametrized Pipeline in Azure ML",
        "Question_body":"<p>I'm new to Azure ML and trying to create a <code>parameterized Pipeline<\/code> which will read data from a datastore and after transformation, will save to a specified datastore.\nI followed <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.graph.pipelineparameter?view=azure-ml-py\" rel=\"nofollow noreferrer\">PipelineParameter Class<\/a> documentation.<\/p>\n<p>Below is my code for <code>parameterized Pipeline<\/code><\/p>\n<pre><code>from azureml.core.datastore import Datastore\nfrom azureml.data import OutputFileDatasetConfig\nfrom azureml.pipeline.steps import PythonScriptStep\nfrom azureml.pipeline.core import PipelineParameter\nfrom azureml.data.datapath import DataPath, DataPathComputeBinding\n\ndefault_ds = ws.get_default_datastore()\ndefault_data_path = DataPath(datastore=default_ds, path_on_datastore=&quot;pipeline-data&quot;)\ninput_pipeline_data_param = PipelineParameter(name=&quot;raw_data&quot;, default_value=default_data_path)\ninput_pipeline_data = (input_pipeline_data_param, DataPathComputeBinding(mode='mount', overwrite=True))\n\n# output_data_path\noutput_pipeline_datapath = OutputFileDatasetConfig(name=&quot;pipeline_test&quot;,\n                                                   destination=(ws.get_default_datastore(), &quot;\/pipeline_output&quot;)).as_upload(overwrite=True)\n                              \nprep_step = PythonScriptStep(script_name=&quot;param_pipeline.py&quot;,\n                             source_directory=experiment_folder,\n                             inputs=[input_pipeline_data],\n                             outputs=[output_pipeline_datapath],\n                             compute_target = pipeline_cluster,\n                             runconfig = pipeline_run_config)\n\nprint(&quot;Pipeline steps defined&quot;)\n<\/code><\/pre>\n<p>And the python script <code>param_pipeline.py<\/code> is as followed:<\/p>\n<pre><code># Get the experiment run context\nrun = Run.get_context()\n\n# Get parameters\nparser = argparse.ArgumentParser()\nparser.add_argument(&quot;--input-dir&quot;, type=str, dest='input_dir', help='input dir')\nparser.add_argument(&quot;--output-dir&quot;, type=str, dest='output_dir', help='output dir')\nargs = parser.parse_args()\noutput_dir = args.output_dir\n\n# load the data file \nprint(&quot;Loading Data...&quot;)\ndf = run.input_datasets['raw_data'].to_pandas_dataframe()\n<\/code><\/pre>\n<p>I think the issue is bcz of how I'm passing the data in the above Pipeline &amp; accessing it using <code>run<\/code>. The reasoning behind this conclusion is I'm able to get the result with Standard pipeline code (shared below).<\/p>\n<pre><code>from azureml.data import OutputFileDatasetConfig\nfrom azureml.pipeline.steps import PythonScriptStep\n\n\n# Create an OutputFileDatasetConfig (temporary Data Reference)\npipelineOutput = OutputFileDatasetConfig(name=&quot;pipeline_test&quot;, \n                                         destination=(ws.get_default_datastore(), &quot;\/pipeline_output&quot;)).as_upload(overwrite=True)\n\n# Step 1, Run the data prep script\nprep_step = PythonScriptStep(name = &quot;Prepare Data&quot;,\n                             source_directory = experiment_folder,\n                             script_name = &quot;param_pipeline_test.py&quot;,\n                             arguments = ['--input-dir', pipeline_data.as_named_input('raw_data'),\n                                          '--output-dir', pipelineOutput],\n                             compute_target = pipeline_cluster,\n                             runconfig = pipeline_run_config,\n                             allow_reuse = True)\n\nprint(&quot;Pipeline steps defined&quot;)\n\n<\/code><\/pre>\n<p>In Standard Approach, I'm passing the input data as <code>pipeline_data.as_named_input('raw_data')<\/code> and then access it python script using <code>run.input_datasets['raw_data'].to_pandas_dataframe()<\/code> but I don't know how to achieve the same with parameterized pipeline.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1660202063943,
        "Question_score":0,
        "Question_tags":"python|azure|azure-pipelines|azure-machine-learning-service",
        "Question_view_count":86,
        "Owner_creation_time":1532570708217,
        "Owner_last_access_time":1663943705933,
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":159,
        "Owner_up_votes":45,
        "Owner_down_votes":0,
        "Owner_views":27,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73316652",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70585667,
        "Question_title":"Azure ML - Model not registering, encountering WebServiceException",
        "Question_body":"<p>I've successfully registered a model with the following exact same code snippet before:<\/p>\n<pre><code>#register model\nfrom azureml.core.model import Model\n\nregister_model = Model.register(model_path = &quot;.\/models&quot;,\n                       model_name = &quot;cr_tools&quot;,\n                       description = &quot;Tools relating to the Customer Relations classifier.&quot;,\n                       workspace = ws)\n\nregister_model\n<\/code><\/pre>\n<p>But now it's not working for a different model (different <code>.\/models<\/code> directory), and I'm encountering the following error:<\/p>\n<pre><code>ServiceException: ServiceException:\n    Code: 504\n    Message: Operation returned an invalid status code 'Gateway Time-out'\n    Details:\n\n    Headers: {\n        &quot;Date&quot;: &quot;Tue, 04 Jan 2022 22:12:54 GMT&quot;,\n        &quot;Content-Type&quot;: &quot;text\/html&quot;,\n        &quot;Content-Length&quot;: &quot;160&quot;,\n        &quot;Connection&quot;: &quot;keep-alive&quot;,\n        &quot;Strict-Transport-Security&quot;: &quot;max-age=15724800; includeSubDomains; preload&quot;,\n        &quot;X-Content-Type-Options&quot;: &quot;nosniff&quot;,\n        &quot;x-request-time&quot;: &quot;60.019&quot;\n    }\n    InnerException: 504 Server Error: Gateway Time-out for url: https:\/\/eastus.experiments.azureml.net\/artifact\/v2.0\/subscriptions\/c450f3d1-583c-495f-b5d3-0b38b99e70c0\/resourceGroups\/ba-p-zeaus-group020-rg\/providers\/Microsoft.MachineLearningServices\/workspaces\/p-group020-aml-ws-001\/artifacts\/batch\/metadata\/LocalUpload\/220104T215629-7c0d42b6\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641334481073,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-service",
        "Question_view_count":151,
        "Owner_creation_time":1513233137037,
        "Owner_last_access_time":1660060757103,
        "Owner_location":null,
        "Owner_reputation":399,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":47,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70585667",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":48560183,
        "Question_title":"how to predict more multiple values in azure ml?",
        "Question_body":"<p>I am creating Azure ML experienment to predict multiple values. but in azure ml we can not train a model to predict multiple values. my question is how to bring multiple trained models in single experienment and create webout put that gives me multiple prediction.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1517479996490,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":666,
        "Owner_creation_time":1517479193607,
        "Owner_last_access_time":1525680129147,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1517995658060,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48560183",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":45680457,
        "Question_title":"Is there any way to upload a file to Microsoft QnA Maker KB from visual studio?",
        "Question_body":"<p>I am trying to change the knowledge base of my QnA maker service in Microsoft QnA maker site. Is it possible to upload a file to this service from my code?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1502734142850,
        "Question_score":0,
        "Question_tags":"microsoft-cognitive|azure-machine-learning-studio|azure-language-understanding",
        "Question_view_count":1223,
        "Owner_creation_time":1502733867173,
        "Owner_last_access_time":1504074794777,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1502922984352,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45680457",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":45485184,
        "Question_title":"\"Entry Point Not Found\" Error LightGBM R package in Azure",
        "Question_body":"<p>When using a LightGBM R package in Azure ML I get the following error:<\/p>\n\n<pre><code>[Error]         +++ NT HARD ERROR (0xc0000139) +++\n[Error]             Parameter 0: 0x4ad4bc8 [log2f]\n[Error]             Parameter 1: 0x4b5e2e8 [C:\\src\\lightgbm\\libs\\x64\\lib_lightgbm.dll]\n[Error]             Parameter 2: 0xffffffffc0000139\n[Error]         [FATAL] Exception: 0xc0000139 (!! HARD ERROR !!) {Params: 0x4ad4bc8, 0x4b5e2e8, 0xffffffffc0000139, 0x0}\n[Error]         [ERROR] A fatal error occurred in the running application. The application will be terminated. Code: 0xc0000139.\n<\/code><\/pre>\n\n<p>on <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/cc704588.aspx\" rel=\"nofollow noreferrer\">page<\/a> <code>0xc0000139<\/code> is described as \"Entry Point Not Found\". What does this error mean and how can I solve it?<\/p>\n\n<p>I used XGBoost in the same way in Azure ML and it worked. And it did not ask for external libraries (dlls). LightGBM on the contrary asks lots of libraries (dlls) and I think the problem is connected with the dlls, but this error does not indicate what is actually missing.<\/p>\n\n<p><strong>What I did:<\/strong> <br>Installed LightGBM R package  on a Virtual Machine with Windows Server 2016. For this I used:<\/p>\n\n<ul>\n<li>CMake<\/li>\n<li>C++ Development kit (installed almost all packages)<\/li>\n<li>RTools<\/li>\n<\/ul>\n\n<p>Included in lightgbm\\libs\\x64 are the following packages because I previously got error <code>0xc0000135<\/code> with the names of these libraries:<\/p>\n\n<ul>\n<li>msvcp140.dll<\/li>\n<li>vcomp140.dll<\/li>\n<li>vcruntime140.dll<\/li>\n<li>all api-ms-win-core-*.dll and all api-ms-win-crt-*.dll<\/li>\n<\/ul>\n\n<p>I tried to change the R version from Microsoft R Open 3.2.2 to CRAN R 3.1.0. It executes witout errors but does not execute code after library import.<\/p>\n\n<p>The full output of Azure ML R script:<\/p>\n\n<pre><code>Record Starts at UTC 08\/03\/2017 12:28:27:\n\nRun the job:\"\/dll \"LanguageWorker, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca;Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS;RunRSNR\" \/Output0 \"..\\..\\Result Dataset\\Result Dataset.dataset\" \/Output1 \"..\\..\\R Device\\R Device.dataset\"  \/dataset1 \"..\\..\\Dataset1\\Dataset1.csv\"    \/bundlePath \"..\\..\\Script Bundle\\Script Bundle.zip\"  \/rStreamReader \"script.R\"  \/rLibVersion \"Microsoft R Open 3.2.2\"  \/ContextFile \"..\\..\\_context\\ContextFile.txt\"\"\n[Start] Program::Main\n[Start]     DataLabModuleDescriptionParser::ParseModuleDescriptionString\n[Stop]     DataLabModuleDescriptionParser::ParseModuleDescriptionString. Duration = 00:00:00.0045866\n[Start]     DllModuleMethod::DllModuleMethod\n[Stop]     DllModuleMethod::DllModuleMethod. Duration = 00:00:00.0000221\n[Start]     DllModuleMethod::Execute\n[Start]         DataLabModuleBinder::BindModuleMethod\n[Verbose]             moduleMethodDescription LanguageWorker, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca;Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS;RunRSNR\n[Verbose]             assemblyFullName LanguageWorker, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca\n[Start]             DataLabModuleBinder::LoadModuleAssembly\n[Verbose]                 Loaded moduleAssembly LanguageWorker, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca\n[Stop]             DataLabModuleBinder::LoadModuleAssembly. Duration = 00:00:00.0093763\n[Verbose]             moduleTypeName Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS\n[Verbose]             moduleMethodName RunRSNR\n[Information]             Module FriendlyName : Execute R Script\n[Information]             Module Release Status : Release\n[Stop]         DataLabModuleBinder::BindModuleMethod. Duration = 00:00:00.0125213\n[Start]         ParameterArgumentBinder::InitializeParameterValues\n[Verbose]             parameterInfos count = 6\n[Verbose]             parameterInfos[0] name = dataset1 , type = Microsoft.Numerics.Data.Local.DataTable\n[Start]             DataTableCsvHandler::HandleArgumentString\n[Stop]             DataTableCsvHandler::HandleArgumentString. Duration = 00:00:00.2364734\n[Verbose]             parameterInfos[1] name = dataset2 , type = Microsoft.Numerics.Data.Local.DataTable\n[Verbose]             Set optional parameter dataset2 value to NULL\n[Verbose]             parameterInfos[2] name = bundlePath , type = System.String\n[Verbose]             parameterInfos[3] name = rStreamReader , type = System.IO.StreamReader\n[Verbose]             parameterInfos[4] name = seed , type = System.Nullable`1[System.Int32]\n[Verbose]             Set optional parameter seed value to NULL\n[Verbose]             parameterInfos[5] name = rLibVersion , type = Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS+ExecuteRScriptRVersion\n[Verbose]             Converted string 'Microsoft R Open 3.2.2' to enum of type Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS+ExecuteRScriptRVersion\n[Stop]         ParameterArgumentBinder::InitializeParameterValues. Duration = 00:00:00.3175225\n[Verbose]         Begin invoking method RunRSNR ... \n[ModuleOutput] Executing Against R 3.2.2.0\n[ModuleOutput] Executing Against R 3.2.2.0\n[Information]         Microsoft Drawbridge Console Host [Version 1.0.2108.0]\n[Error]         +++ NT HARD ERROR (0xc0000139) +++\n[Error]             Parameter 0: 0x4ad4bc8 [log2f]\n[Error]             Parameter 1: 0x4b5e2e8 [C:\\src\\lightgbm\\libs\\x64\\lib_lightgbm.dll]\n[Error]             Parameter 2: 0xffffffffc0000139\n[Error]         [FATAL] Exception: 0xc0000139 (!! HARD ERROR !!) {Params: 0x4ad4bc8, 0x4b5e2e8, 0xffffffffc0000139, 0x0}\n[Error]         [ERROR] A fatal error occurred in the running application. The application will be terminated. Code: 0xc0000139.\n[Information]         [1] 56000\n[Information]         The following files have been unzipped for sourcing in path=[\"src\"]:\n[Information]                            Name  Length                Date\n[Information]         1 data.table_1.10.4.zip 1487417 2017-07-07 16:48:00\n[Information]         2            lgb1.model   45142 2017-08-02 17:38:00\n[Information]         3            lgb2.model   83455 2017-08-02 17:38:00\n[Information]         4    lightgbm_2.0.4.zip 1350111 2017-08-03 14:26:00\n[Information]         5      magrittr_1.5.zip  152732 2017-07-07 15:34:00\n[Information]         6                R6.zip  317766 2017-08-03 10:33:00\n[Information]         Loading objects:\n[Information]           port1\n[Information]         [1] \"Loading variable port1...\"\n[Information]         package 'magrittr' successfully unpacked and MD5 sums checked\n[Information]         package 'R6' successfully unpacked and MD5 sums checked\n[Information]         package 'data.table' successfully unpacked and MD5 sums checked\n[Information]         package 'lightgbm' successfully unpacked and MD5 sums checked\n[Information]         data.table 1.10.4\n[Information]           The fastest way to learn (by data.table authors): https:\/\/www.datacamp.com\/courses\/data-analysis-the-data-table-way\n[Information]           Documentation: ?data.table, example(data.table) and browseVignettes(\"data.table\")\n[Information]           Release notes, videos and slides: http:\/\/r-datatable.com\n[Error]         Process returned with non-zero exit code -1073741511\n[Stop]     DllModuleMethod::Execute. Duration = 00:00:14.8676292\n[Critical]     Error: Error 1000: RPackage library exception: Attempting to obtain R output before invoking execution process\n[Critical]     {\"InputParameters\":{\"DataTable\":[{\"Rows\":50,\"Columns\":131,\"estimatedSize\":16928768,...........\"Errors\":\"Microsoft.Analytics.Exceptions.ErrorMapping+ModuleException: Error 1000: RPackage library exception: Attempting to obtain R output before invoking execution process ---&gt; Microsoft.Analytics.Modules.R.ErrorHandling.RInvalidOperationException: Attempting to obtain R output before invoking execution process\\r\\n   at Microsoft.MetaAnalytics.LanguageWorker.NewRWorker.GetProcessOutputs(Boolean scrub) in d:\\\\_Bld\\\\8831\\\\7669\\\\Sources\\\\Product\\\\Source\\\\Modules\\\\LanguageWorker\\\\LanguageWorker.Dll\\\\TempWorkers\\\\NewRWorker.cs:line 459\\r\\n   at Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS.ExecuteR(NewRWorker worker, DataTable dataset1, DataTable dataset2, IEnumerable`1 bundlePath, StreamReader rStreamReader, Nullable`1 seed) in d:\\\\_Bld\\\\8831\\\\7669\\\\Sources\\\\Product\\\\Source\\\\Modules\\\\LanguageWorker\\\\LanguageWorker.Dll\\\\EntryPoints\\\\RModule.cs:line 278\\r\\n   at Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS._RunImpl(NewRWorker worker, DataTable dataset1, DataTable dataset2, String bundlePath, StreamReader rStreamReader, Nullable`1 seed, ExecuteRScriptExternalResource source, String url, ExecuteRScriptGitHubRepositoryType githubRepoType, SecureString accountToken) in d:\\\\_Bld\\\\8831\\\\7669\\\\Sources\\\\Product\\\\Source\\\\Modules\\\\LanguageWorker\\\\LanguageWorker.Dll\\\\EntryPoints\\\\RModule.cs:line 207\\r\\n   at Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS.RunRSNR(DataTable dataset1, DataTable dataset2, String bundlePath, StreamReader rStreamReader, Nullable`1 seed, ExecuteRScriptRVersion rLibVersion) in d:\\\\_Bld\\\\8831\\\\7669\\\\Sources\\\\Product\\\\Source\\\\Modules\\\\LanguageWorker\\\\LanguageWorker.Dll\\\\EntryPoints\\\\REntryPoint.cs:line 105\\r\\n   --- End of inner exception stack trace ---\",\"Warnings\":[],\"Duration\":\"00:00:14.8605990\"}\nModule finished after a runtime of 00:00:15.3186157 with exit code -2\nModule failed due to negative exit code of -2\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1501765744790,
        "Question_score":0,
        "Question_tags":"c++|r|azure|dll|azure-machine-learning-studio",
        "Question_view_count":245,
        "Owner_creation_time":1399036511573,
        "Owner_last_access_time":1663941160840,
        "Owner_location":"Moscow, Russia",
        "Owner_reputation":2853,
        "Owner_up_votes":1981,
        "Owner_down_votes":6,
        "Owner_views":228,
        "Question_last_edit_time":1501769547343,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45485184",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73234673,
        "Question_title":"Trying to work on R using Azure ML Studio Notebook and facing challenges with ODBC package",
        "Question_body":"<p>I am trying to work on R notebook on ML Studio. Using regular python is easy and works as expected but with R i am facing challenges.<\/p>\n<p>While trying to connect to MS SQL database using odbc() :<\/p>\n<pre><code>library(odbc)\ncon &lt;- dbConnect(odbc(),\n                 Driver = &quot;SQL Server&quot;,\n                 Server = &quot;server&quot;,\n                 Database = &quot;db&quot;,\n                 UID = &quot;user&quot;,\n                 PWD = &quot;password&quot;,\n                 Port = 1433)\n\n\n\nError: nanodbc\/nanodbc.cpp:1021: 00000: [unixODBC][Driver Manager]Can't open lib 'SQL Server' : file not found\n<\/code><\/pre>\n<p>As suggested in some posts, i have also tried replacing  Driver = &quot;SQL Server&quot;, with Driver = &quot;ODBC Driver 11 for SQL Server&quot;. But i see similar error<\/p>\n<pre><code>Error: nanodbc\/nanodbc.cpp:1021: 00000: [unixODBC][Driver Manager]Can't open lib 'ODBC Driver 11 for SQL Server' : file not found \nTraceback:\n<\/code><\/pre>\n<p>Please suggest a work around.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1659610475930,
        "Question_score":0,
        "Question_tags":"r|azure|azure-machine-learning-service",
        "Question_view_count":52,
        "Owner_creation_time":1496406450497,
        "Owner_last_access_time":1663656501120,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73234673",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62343056,
        "Question_title":"How to log a confusion matrix to azureml platform using python",
        "Question_body":"<p>Hello Stackoverflowers,<\/p>\n\n<p>I'm using azureml and I'm wondering if it is possible to log a confusion matrix of the xgboost model I'm training, together with the other metrics I'm already logging. Here's a sample of the code I'm using:<\/p>\n\n<pre><code>from azureml.core.model import Model\nfrom azureml.core import Workspace\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nimport json\n\nwith open('.\/azureml.config', 'r') as f:\n    config = json.load(f)\n\nsvc_pr = ServicePrincipalAuthentication(\n   tenant_id=config['tenant_id'],\n   service_principal_id=config['svc_pr_id'],\n   service_principal_password=config['svc_pr_password'])\n\n\nws = Workspace(workspace_name=config['workspace_name'],\n                        subscription_id=config['subscription_id'],\n                        resource_group=config['resource_group'],\n                        auth=svc_pr)\n\ny_pred = model.predict(dtest)\n\nacc = metrics.accuracy_score(y_test, (y_pred&gt;.5).astype(int))\nrun.log(\"accuracy\",  acc)\nf1 = metrics.f1_score(y_test, (y_pred&gt;.5).astype(int), average='binary')\nrun.log(\"f1 score\",  f1)\n\n\ncmtx = metrics.confusion_matrix(y_test,(y_pred&gt;.5).astype(int))\nrun.log_confusion_matrix('Confusion matrix', cmtx)\n<\/code><\/pre>\n\n<p>The above code raises this kind of error:<\/p>\n\n<pre><code>TypeError: Object of type ndarray is not JSON serializable\n<\/code><\/pre>\n\n<p>I already tried to transform the matrix in a simpler one, but another error occurred as before I logged a \"manual\" version of it (<code>cmtx = [[30000, 50],[40, 2000]]<\/code>).<\/p>\n\n<pre><code>run.log_confusion_matrix('Confusion matrix', [list([int(y) for y in x]) for x in cmtx])\n\nAzureMLException: AzureMLException:\n    Message: UserError: Resource Conflict: ArtifactId ExperimentRun\/dcid.3196bf92-4952-4850-9a8a-    c5103b205379\/Confusion matrix already exists.\n    InnerException None\n    ErrorResponse \n{\n    \"error\": {\n        \"message\": \"UserError: Resource Conflict: ArtifactId ExperimentRun\/dcid.3196bf92-4952-4850-9a8a-c5103b205379\/Confusion matrix already exists.\"\n    }\n}\n<\/code><\/pre>\n\n<p>This makes me think that I'm not properly handling the command <code>run.log_confusion_matrix()<\/code>. So, again, which is the best way I can log a confusion matrix to my azureml experiments?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591960207257,
        "Question_score":3,
        "Question_tags":"python|azure|xgboost|confusion-matrix|azure-machine-learning-service",
        "Question_view_count":1418,
        "Owner_creation_time":1506516283190,
        "Owner_last_access_time":1663256383867,
        "Owner_location":"Torino, TO, Italia",
        "Owner_reputation":875,
        "Owner_up_votes":28,
        "Owner_down_votes":3,
        "Owner_views":52,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I eventually found a solution thanks to colleague of mine. I'm hence answering myself, in order to close the question and, maybe, help somebody else.<\/p>\n<p>You can find the proper function in this link: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----<\/a>.<\/p>\n<p>Anyway, you also have to consider that, apparently, Azure doesn't work with the standard confusion matrix format returned by sklearn. It accepts indeed ONLY list of list, instead of numpy array, populated with numpy.int64 elements. So you also have to transform the matrix in a simpler format (for the sake of simplicity I used the nested list comprehension in the command below:<\/p>\n<pre><code>cmtx = metrics.confusion_matrix(y_test,(y_pred&gt;.5).astype(int))\ncmtx = {\n\n&quot;schema_type&quot;: &quot;confusion_matrix&quot;,\n&quot;parameters&quot;: params,\n &quot;data&quot;: {&quot;class_labels&quot;: [&quot;0&quot;, &quot;1&quot;],\n          &quot;matrix&quot;: [[int(y) for y in x] for x in cmtx]}\n}\nrun.log_confusion_matrix('Confusion matrix - error rate', cmtx)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1593519812260,
        "Answer_score":6.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62343056",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60202243,
        "Question_title":"Read\/Mount a csv file inside train.py of Azure ML Pipeline",
        "Question_body":"<p>We are collecting data from Eventhub and AppInsight and storing it in azure blob. By using AzureML pipeline I want to pass my dataset into train.py going through two different logics(one for ml, another for fraud analysis).<\/p>\n\n<p>But I couldnt read the csv file for further processing from inside the <code>train.py<\/code> <\/p>\n\n<p>This is my <code>train.py<\/code> which is running through <code>PythonScriptStep<\/code> in Azure Machine Learning Pipeline<\/p>\n\n<pre><code>import argparse\nimport os\nimport pandas as pd\n\nprint(\"In train.py\")\n\nparser = argparse.ArgumentParser(\"train\")\n\nparser.add_argument(\"--input_data\", type=str, help=\"input data\")\nparser.add_argument(\"--output_train\", type=str, help=\"output_train directory\")\n\nargs = parser.parse_args()\n\nprint(\"Argument 1: %s\" % args.input_data)\ndf = pd.read_csv(args.input_data)\nprint(df.head())\n\nprint(\"Argument 2: %s\" % args.output_train)\n\nif not (args.output_train is None):\n    os.makedirs(args.output_train, exist_ok=True)\n    print(\"%s created\" % args.output_train)\n<\/code><\/pre>\n\n<p>And this is the code for running the Pipeline<\/p>\n\n<pre><code>ws = Workspace.from_config()\ndef_blob_store = Datastore(ws, \"basic_data_store\")\naml_compute_target = \"test-cluster\"\ntry:\n    aml_compute = AmlCompute(ws, aml_compute_target)\n    print(\"found existing compute target.\")\nexcept ComputeTargetException:\n    print(\"Error\")\n\nsource_directory = '.\/train'\n\nblob_input_data = DataReference(\n    datastore=def_blob_store,\n    data_reference_name=\"device_data\",\n    path_on_datastore=\"_fraud_data\/test.csv\")\ntrainStep = PythonScriptStep(\n    script_name=\"train.py\", \n    arguments=[\"--input_data\", blob_input_data, \"--output_train\", processed_data1],\n    inputs=[blob_input_data],\n    outputs=[processed_data1],\n    compute_target=aml_compute, \n    source_directory=source_directory,\n    runconfig=run_config\n)\npipeline1 = Pipeline(workspace=ws, steps=[compareStep])\npipeline_run1 = Experiment(ws, 'Data_dependency').submit(pipeline1)\n<\/code><\/pre>\n\n<p>Down below in the output trace, you can see the output <code>Argument 1<\/code> is printing the path of the file<\/p>\n\n<p><code>Argument 1: \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pipeline-shohoz\/azureml\/d92be2ab-e63f-4883-a14b-a64fa5bb431d\/mounts\/basic_data_store\/_fraud_data\/test.csv<\/code><\/p>\n\n<p>So I have successfully pass the dataset but cant read the file inside train.py on line <code>pd.read_csv(args.input_data)<\/code>.  It is showing <\/p>\n\n<p><code>FileNotFoundError: [Errno 2] File b'\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pipeline-shohoz\/azureml\/d92be2ab-e63f-4883-a14b-a64fa5bb431d\/mounts\/basic_data_store\/_fraud_data\/test.csv'<\/code> <\/p>\n\n<p>this is the full trace from <code>70_driver_log.txt<\/code> which I have downloaded from azureml log,<\/p>\n\n<pre><code>Preparing to call script [ train.py ] with arguments: ['--input_data', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pipeline-shohoz\/azureml\/d92be2ab-e63f-4883-a14b-a64fa5bb431d\/mounts\/basic_data_store\/_fraud_data\/test.csv', '--output_train', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pipeline-shohoz\/azureml\/d92be2ab-e63f-4883-a14b-a64fa5bb431d\/mounts\/basic_data_store\/azureml\/d92be2ab-e63f-4883-a14b-a64fa5bb431d\/processed_data1']\nAfter variable expansion, calling script [ train.py ] with arguments: ['--input_data', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pipeline-shohoz\/azureml\/d92be2ab-e63f-4883-a14b-a64fa5bb431d\/mounts\/basic_data_store\/_fraud_data\/test.csv', '--output_train', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pipeline-shohoz\/azureml\/d92be2ab-e63f-4883-a14b-a64fa5bb431d\/mounts\/basic_data_store\/azureml\/d92be2ab-e63f-4883-a14b-a64fa5bb431d\/processed_data1']\n\nIn train.py\nArgument 1: \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pipeline-shohoz\/azureml\/d92be2ab-e63f-4883-a14b-a64fa5bb431d\/mounts\/basic_data_store\/_fraud_data\/test.csv\n\n\nThe experiment failed. Finalizing run...\nCleaning up all outstanding Run operations, waiting 300.0 seconds\n1 items cleaning up...\nCleanup took 0.001172780990600586 seconds\nStarting the daemon thread to refresh tokens in background for process with pid = 136\nTraceback (most recent call last):\n  File \"train.py\", line 18, in &lt;module&gt;\n    df = pd.read_csv(args.input_data) #str()\n  File \"\/azureml-envs\/azureml_eb042e80b9a6abdb5821a78683153a38\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 685, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"\/azureml-envs\/azureml_eb042e80b9a6abdb5821a78683153a38\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 457, in _read\n    parser = TextFileReader(fp_or_buf, **kwds)\n  File \"\/azureml-envs\/azureml_eb042e80b9a6abdb5821a78683153a38\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 895, in __init__\n    self._make_engine(self.engine)\n  File \"\/azureml-envs\/azureml_eb042e80b9a6abdb5821a78683153a38\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 1135, in _make_engine\n    self._engine = CParserWrapper(self.f, **self.options)\n  File \"\/azureml-envs\/azureml_eb042e80b9a6abdb5821a78683153a38\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 1917, in __init__\n    self._reader = parsers.TextReader(src, **kwds)\n  File \"pandas\/_libs\/parsers.pyx\", line 382, in pandas._libs.parsers.TextReader.__cinit__\n  File \"pandas\/_libs\/parsers.pyx\", line 689, in pandas._libs.parsers.TextReader._setup_parser_source\nFileNotFoundError: [Errno 2] File b'\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pipeline-shohoz\/azureml\/d92be2ab-e63f-4883-a14b-a64fa5bb431d\/mounts\/basic_data_store\/_fraud_data\/test.csv' does not exist: b'\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pipeline-shohoz\/azureml\/d92be2ab-e63f-4883-a14b-a64fa5bb431d\/mounts\/basic_data_store\/_fraud_data\/test.csv'\n<\/code><\/pre>\n\n<p>I have tried the relative path <\/p>\n\n<p><code>azureml\/8d2b7bee-6cc5-4c8c-a685-1300a240de8f\/mounts\/basic_data_store\/_fraud_data\/test.csv<\/code> <\/p>\n\n<p>and also the Uri <\/p>\n\n<p><code>wasbs:\/\/shohoz-container@shohozds.blob.core.windows.net\/azureml\/azureml\/8d2b7bee-6cc5-4c8c-a685-1300a240de8f\/mounts\/basic_data_store\/_fraud_data\/test.csv<\/code><\/p>\n\n<p>but ending with the same <code>FileNotFoundError<\/code> result. I am banging my head on the wall for last 3-4 days. Any help will save my brain.  <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1581577721350,
        "Question_score":4,
        "Question_tags":"python|azure|azure-machine-learning-service",
        "Question_view_count":1072,
        "Owner_creation_time":1337959444690,
        "Owner_last_access_time":1663815843037,
        "Owner_location":"Dhaka, Dhaka Division, Bangladesh",
        "Owner_reputation":6262,
        "Owner_up_votes":68,
        "Owner_down_votes":6,
        "Owner_views":988,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60202243",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":65343319,
        "Question_title":"Azure Wide and Deep Recommender real inference pipeline error - Invalid graph: You have required input port(s) unconnected",
        "Question_body":"<p>It seems like there is bug in create real-time endpoints for &quot;Wide &amp; Deep Recommender&quot; module at least with the sample workflow.  I kept getting &quot;Invalid graph: You have requested input port(s) unconnected&quot;.  Does anyone know how to get around this issue?<\/p>\n<p>Repro Steps:<\/p>\n<ol>\n<li><p>Go to Azure ML -&gt; Designer -&gt; &quot;Wide &amp; Deep based Recommendation - Restaurant&quot;\n<a href=\"https:\/\/i.stack.imgur.com\/QMT42.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QMT42.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Train the model -&gt; Create &quot;Real-time inference pipeline&quot; -&gt; in Real-time inference pipeline, click &quot;Submit&quot; -&gt; Error occurs<\/p>\n<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/QmyJi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QmyJi.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1608217771983,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":134,
        "Owner_creation_time":1446232229117,
        "Owner_last_access_time":1641407543500,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65343319",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60638587,
        "Question_title":"How to get insights in exceptions and logging of AzureML endpoint deployment",
        "Question_body":"<p>Because of a faulty score.py file in my InferenceConfig, a Model.Deploy failed to Azure Machine Learning, using ACI.  I wanted to create the endpoint in the cloud, but the only state I can see in the portal is Unhealthy.  My local script to deploy the model (using ) keeps running, until it times out. (using the <code>service.wait_for_deployment(show_output=True)<\/code>statement).<\/p>\n\n<p>Is there an option to get more insights in the actual reason\/error message of the deployment turning \"Unhealthy\"?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1583937572180,
        "Question_score":3,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":364,
        "Owner_creation_time":1360655430743,
        "Owner_last_access_time":1663784892907,
        "Owner_location":"Belgium",
        "Owner_reputation":2947,
        "Owner_up_votes":297,
        "Owner_down_votes":16,
        "Owner_views":355,
        "Question_last_edit_time":1584215688009,
        "Answer_body":"<p>Usually the timeout is caused by an error in init() function in scoring script. You can get the detailed logs using <code>print(service.get_logs())<\/code> to find the Python error.<\/p>\n\n<p>For more comprehensive troubleshooting guide, see:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1584133364176,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60638587",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":44000208,
        "Question_title":"Azure ML - How to retrain a model (classic web service)",
        "Question_body":"<p>I followed this guide to retrain a model (Guide: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-retrain-a-classic-web-service\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-retrain-a-classic-web-service<\/a>)<\/p>\n\n<p>Still there are some questions left. So before retraining a model, do I have to upload the new dataset into my  blob container storage ? If yes how do I do that via http ?<\/p>\n\n<p>Maybe is it possible to send the new dataset via the PATCH-call in the http body?<\/p>\n\n<p>Thanks in Advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1494934026127,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":537,
        "Owner_creation_time":1467130084653,
        "Owner_last_access_time":1663925192090,
        "Owner_location":null,
        "Owner_reputation":915,
        "Owner_up_votes":19,
        "Owner_down_votes":1,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44000208",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62332672,
        "Question_title":"Tracking separate train\/test processes with Trains",
        "Question_body":"<p>In my setup, I run a script that <strong>trains<\/strong> a model and starts generating checkpoints. Another script watches for new checkpoints and <strong>evaluates<\/strong> them. The scripts run in parallel, so evaluation is just a step behind training.<\/p>\n\n<p>What's the right Tracks configuration to support this scenario?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1591905931703,
        "Question_score":2,
        "Question_tags":"trains|clearml",
        "Question_view_count":99,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":1609771005756,
        "Answer_body":"<p>disclaimer: I'm part of the <a href=\"https:\/\/github.com\/allegroai\/trains\/\" rel=\"nofollow noreferrer\">allegro.ai Trains<\/a> team<\/p>\n<p>Do you have two experiments? one for testing one for training ?<\/p>\n<p>If you do have two experiments, then I would make sure the models are logged in both of them (which if they are stored on the same shared-folder\/s3\/etc will be automatic)\nThen you can quickly see the performance of each-one.<\/p>\n<p>Another option is sharing the same experiment, then the second process adds reports to the original experiment, that means that somehow you have to pass to it the experiment id.\nThen you can do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>task = Task.get_task(task_id='training_task_id`)\ntask.get_logger().report_scalar('title', 'loss', value=0.4, iteration=1)\n<\/code><\/pre>\n<p>EDIT:\nAre the two processes always launched together, or is the checkpoint test a general purpose code ?<\/p>\n<p>EDIT2:<\/p>\n<p>Let's assume you have main script training a model. This experiment has a unique task ID:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>my_uid = Task.current_task().id\n<\/code><\/pre>\n<p>Let's also assume you have a way to pass it to your second process (If this is an actual sub-process, it inherits the os environment variables so you could do <code>os.environ['MY_TASK_ID']=my_uid<\/code>)<\/p>\n<p>Then in the evaluation script you could report directly into the main training Task like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>train_task = Task.get_task(task_id=os.environ['MY_TASK_ID'])\ntrain_task.get_logger().report_scalar('title', 'loss', value=0.4, iteration=1)\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1591906575912,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1592833417200,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62332672",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":70423979,
        "Question_title":"Export metrics of ClearML to Prometheus and show them in Grafana",
        "Question_body":"<p>Are there any metrics I can get from the API server? or any docker image I can point to the backend and get some metrics?\nMost important is the see how many tasks running in real-time (like we can see on the worker's page) and also check how much time each task is running (also can be found on the worker's page)<\/p>\n<p>If it does not exist, do they have an API for getting all this information?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640013390177,
        "Question_score":0,
        "Question_tags":"prometheus|grafana|metrics|clearml",
        "Question_view_count":70,
        "Owner_creation_time":1569092651057,
        "Owner_last_access_time":1662456713617,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70423979",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":63586251,
        "Question_title":"Will Trains automagically log Tensorboard HParams?",
        "Question_body":"<p>I know that it's possible to send hyper-params as a dictionary to Trains.<\/p>\n<p>But can it also automagically log hyper-params that are logged using the TF2 HParams module?<\/p>\n<p>Edit: This is done in the <a href=\"https:\/\/www.tensorflow.org\/tensorboard\/hyperparameter_tuning_with_hparams\" rel=\"nofollow noreferrer\">HParams tutorial<\/a> using <code>hp.hparams(hparams)<\/code>.<\/p>\n<p><img src=\"https:\/\/www.tensorflow.org\/tensorboard\/images\/hparams_parallel_coordinates.png?raw=1\" alt=\"Tensorboard HParams\" \/><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1598385550273,
        "Question_score":1,
        "Question_tags":"trains|clearml",
        "Question_view_count":142,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":1609427497023,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63586251",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":65655382,
        "Question_title":"ClearML server IP address not used with localhost and SSH port forwarding",
        "Question_body":"<p>Trying to use clearml-server on own Ubuntu 18.04.5.<\/p>\n<p>I use env variables to set the IP Address of my clearml-server.<\/p>\n<pre><code>export CLEARML_HOST_IP=127.0.0.1\nexport TRAINS_HOST_IP=127.0.0.1\n<\/code><\/pre>\n<p>But it still is available thorugh the external server IP.\nHow can I deactivate the listeners for external IP in clearml-server config?<\/p>\n<p>Edit:\nAccording to this:\nI use SSH Port forward to access local instance from my computer outside of the network. But I can't access custom uploaded images (task-&gt; debug samples) as they will not use my port forwarded URLs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610294996320,
        "Question_score":2,
        "Question_tags":"devops|local|portforwarding|trains|clearml",
        "Question_view_count":261,
        "Owner_creation_time":1604391794420,
        "Owner_last_access_time":1663670435373,
        "Owner_location":null,
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1610296597729,
        "Answer_body":"<p>Disclaimer: I'm a ClearML (Trains) team member<\/p>\n<p>Basically the docker-compose will expose only the API\/Web\/File server , you can further limit the exposure to your localhost only, by changing the following section in your ClearML server <a href=\"https:\/\/github.com\/allegroai\/clearml-server\/blob\/master\/docker\/docker-compose.yml\" rel=\"nofollow noreferrer\">docker-compose.yml<\/a><\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>networks:\n  backend:\n    driver:\n      bridge\n    driver_opts:\n      com.docker.network.bridge.host_binding_ipv4: &quot;127.0.0.1&quot;\n  frontend:\n    driver: \n      bridge\n    driver_opts:\n      com.docker.network.bridge.host_binding_ipv4: &quot;127.0.0.1&quot;\n\n<\/code><\/pre>\n<p>Based on docker's <a href=\"https:\/\/docs.docker.com\/engine\/reference\/commandline\/network_create\/\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1610303095156,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65655382",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":67496760,
        "Question_title":"Mounting an S3 bucket in docker in a clearml agent",
        "Question_body":"<p>What is the best practice for mounting an S3 container inside a docker image that will be using as a ClearML agent?  I can think of 3 solutions, but have been unable to get any to work currently:<\/p>\n<ol>\n<li>Use <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/use_cases\/clearml_agent_use_case_examples.html?highlight=docker\" rel=\"nofollow noreferrer\">prefabbed configuration in ClearML<\/a>, specifically CLEARML_AGENT_K8S_HOST_MOUNT.  For this to work, the S3 bucket would be mounted separately on the host using <a href=\"https:\/\/rclone.org\/\" rel=\"nofollow noreferrer\">rclone<\/a> and then remapped into docker. This appears to only apply to Kubernetes and not Docker - and therefore would not work.<\/li>\n<li>Mount using s3fuse as specified <a href=\"https:\/\/stackoverflow.com\/questions\/35189251\/docker-mount-s3-container\">here<\/a>.  The issue is will it work with the S3 bucket secret stored in ClearML browser sessions?  This would also appear to be complicated and require custom docker images, not to mention running the docker image as --privileged or similar.<\/li>\n<li>Pass arguments to docker using &quot;docker_args and docker_bash_setup_script arguments to Task.create()&quot; as specified in the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/release_notes\/ver_1_0.html\" rel=\"nofollow noreferrer\">1.0 release notes<\/a>.  This would be similar to (1), but the arguments would be for <a href=\"https:\/\/docs.docker.com\/storage\/bind-mounts\/\" rel=\"nofollow noreferrer\">bind-mounting the volume<\/a>.  I do not see much documentation or examples on how this new feature may be used for this end.<\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1620788331577,
        "Question_score":1,
        "Question_tags":"docker|amazon-s3|wsl-2|rclone|clearml",
        "Question_view_count":770,
        "Owner_creation_time":1362580980910,
        "Owner_last_access_time":1663351509150,
        "Owner_location":"Akron, OH, USA",
        "Owner_reputation":4013,
        "Owner_up_votes":94,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Question_last_edit_time":1621022250560,
        "Answer_body":"<p>I was able to get another option entirely to work, namely, mount a drive on in WSL and then pass it to Docker.  Let's get to it:<\/p>\n<p>Why not host in Windows itself, why rclone in WSL?<\/p>\n<ul>\n<li>Docker running on WSL <a href=\"https:\/\/github.com\/billziss-gh\/winfsp\/issues\/61\" rel=\"nofollow noreferrer\">cannot access drives mounted through winfsp<\/a> (what rclone uses)<\/li>\n<\/ul>\n<p>Steps to mount the drive in ClearML in Windows:<\/p>\n<ul>\n<li>You can install rclone in WSL and the mount will be accessible to docker\n<ul>\n<li>create the folder <code>\/data\/my-mount<\/code> (this needs to be in <code>\/data<\/code> - I don't know why and I can't find out with a Google search, but I found out about it <a href=\"https:\/\/forum.rclone.org\/t\/fusermount-permission-denied-in-docker-rclone\/13914\/5\" rel=\"nofollow noreferrer\">here<\/a>)<\/li>\n<li>You can put the configuration file in windows (use the <code>--config<\/code> option).<\/li>\n<li>Note: ClearML will not support spaces in mounted paths, even though docker will.  Therefore your path has to be <code>\/data\/my-mount<\/code> rather than <code>\/data\/my mount<\/code>.  There is a <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/358\" rel=\"nofollow noreferrer\">bug that I opened about this<\/a>.<\/li>\n<\/ul>\n<\/li>\n<li>You can test mounting by calling docker and mounting the file.\n<ul>\n<li>Example: <code>docker run -it -v \\\\wsl$\\Ubuntu\\data:\/data my-docker-image:latest ls \/data\/my-mount<\/code><\/li>\n<li>Note: You will have to mount \/data rather than \/data\/my-mount, otherwise you may get this error: <code>docker: Error response from daemon: error while creating mount source path<\/code><\/li>\n<\/ul>\n<\/li>\n<li>Now, you can setup the clearml.conf file in <code>C:\\Users\\Myself\\clearml.conf<\/code> such that:<\/li>\n<\/ul>\n<pre><code>default_docker: {\n   # default docker image to use when running in docker mode\n   image: &quot;my-docker-image:latest&quot;\n\n   # optional arguments to pass to docker image\n   arguments: [&quot;-v&quot;,&quot;\\\\wsl$\\Ubuntu\\data:\/data&quot;, ]\n}\n<\/code><\/pre>\n<ul>\n<li>Note that you can also run clearml-agent out of WSL and then would only need to specify <code>[&quot;-v&quot;,&quot;\/data:\/data&quot;, ]<\/code>.<\/li>\n<li>Run clearml agent in cmd: <code>clearml-agent daemon --docker<\/code><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1621009841940,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1621012052112,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67496760",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":62591953,
        "Question_title":"Does clone experiment work on sklearn functions?",
        "Question_body":"<p>I'm trying to run a script and I'm constantly getting this while cloning experiment in allegro.ai\nAttributeError: 'Namespace' object has no attribute 'get'\nCan anybody help?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1593164125703,
        "Question_score":0,
        "Question_tags":"trains|clearml",
        "Question_view_count":27,
        "Owner_creation_time":1505152928440,
        "Owner_last_access_time":1593163986227,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1609501001127,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62591953",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":66320238,
        "Question_title":"ClearML get max value from logged values",
        "Question_body":"<p>I use ClearML to track my tensorboard logs (from PyTorch Lightning) during training.\nAt a point later I start another script which connects to existing task and do some testing.<\/p>\n<p>But unfortenautly I do not have all information in the second script, so I want to query them from the logged values from ClearML server.<\/p>\n<p>How would I do this?<\/p>\n<p>I thought about something like this, but havn't found anything in documentation:<\/p>\n<pre><code>task = Task.init(project_name=&quot;Project&quot;, task_name=&quot;name&quot;, reuse_last_task_id=&quot;Task_id, continue_last_task=True)\nx_value, y_value = task.get_value(key=&quot;val\/acc&quot;, mode=&quot;max&quot;)\nx_value2, y_value2 = task.get_value(key=&quot;epoch&quot;, mode=&quot;x&quot;, x=x_value)\n<\/code><\/pre>\n<ul>\n<li><code>x_value<\/code> would be my epoch or global step<\/li>\n<li><code>y_value<\/code> the maximum value of plot &quot;val\/acc&quot;<\/li>\n<li><code>x_value2<\/code> would be my epoch or global step<\/li>\n<li><code>y_value2<\/code> the value of plot &quot;epoch&quot; at <code>x_value<\/code><\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1614013817123,
        "Question_score":3,
        "Question_tags":"devops|clearml|trains",
        "Question_view_count":101,
        "Owner_creation_time":1604391794420,
        "Owner_last_access_time":1663670435373,
        "Owner_location":null,
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1614165129956,
        "Answer_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p>To get an existing <code>Task<\/code> object for a running (or completed\/failed) experiment, assuming we know Task ID:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>another_task = Task.get_task(task_id='aabbcc')\n<\/code><\/pre>\n<p>If we only know the Task project\/name<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>another_task = Task.get_task(project_name='the project', task_name='the name')\n<\/code><\/pre>\n<p>Notice that if you have multiple task under the same name it will return the most updated one.\nOnce we have the <code>Task<\/code> object, we can do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>latest_scalar_values_dict = another_task.get_last_scalar_metrics()\n<\/code><\/pre>\n<p>Which would return all the scalars min\/maxm\/last values, for example:<\/p>\n<pre><code>latest_scalar_values_dict = {\n            'title': {\n                'series': {\n                    'last': 0.5,\n                    'min': 0.1,\n                    'max': 0.9\n                    }\n                }\n            }\n<\/code><\/pre>\n<p><a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/references\/clearml_python_ref\/task_module\/task_task.html?highlight=get%20metrics#clearml.task.Task.get_last_scalar_metrics\" rel=\"nofollow noreferrer\">documentation here<\/a><\/p>\n<p>If you need to get the entire graphs you can use <code>task.get_reported_scalars()<\/code> <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/references\/clearml_python_ref\/task_module\/task_task.html?highlight=get_reported_scalars#clearml.task.Task.get_reported_scalars\" rel=\"nofollow noreferrer\">see docs<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1614215061400,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66320238",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73456627,
        "Question_title":"ClearML Remote execution does not use requirements.txt",
        "Question_body":"<p>I want to execute a function on a worker with clearml. I use the code below to do this. But worker will always create a new env, although I set it to use my python binary via environmet settings. I also tried to pass a requirements.txt but this will be ignored or misinterpreted.\nThe main issue is that I need opencv_contrib, but clearml always installs opencv afterwards and the contrib package will get overwritten, so the contrib methods are not available and the remote execution fails.<\/p>\n<p>How can I force to use my existing python (conda) environment?\nHow can I force to install the packages I define in my requirements.txt?<\/p>\n<pre><code>    def my_func(path:str):\n        # do calculation\n        return \n    paths = ['\/media\/hdd\/my_folder_1\/']\n    for i, path in enumerate(paths):\n        task.create_function_task(my_func, func_name=my_func-{i}',\n                              task_name=f'my_func - {i}', path=path)\n<\/code><\/pre>\n<p>Starting script for my clearml worker<\/p>\n<pre><code>#!\/bin\/bash\n\nexport CLEARML_HOST_IP=127.0.0.1\nexport CLEARML_AGENT_SKIP_PIP_VENV_INSTALL=\/home\/user\/miniconda3\/envs\/my_env\/bin\/python\nclearml-agent daemon --detached --queue default\n<\/code><\/pre>\n<p>I'm using latest clearml on ubuntu 20.04.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1661248448260,
        "Question_score":0,
        "Question_tags":"clearml",
        "Question_view_count":14,
        "Owner_creation_time":1604391794420,
        "Owner_last_access_time":1663670435373,
        "Owner_location":null,
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73456627",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73570817,
        "Question_title":"ClearML - dynamically updating Plotly plots?",
        "Question_body":"<p>I have a question related to ClearML plot logging. We are currently using:<\/p>\n<pre><code>self.task_logger.report_table(&quot;TableSpaceName&quot;, &quot;Some Info&quot;, iteration=0, table_plot=df)\n<\/code><\/pre>\n<p>To report tables. They appear under &quot;PLOTS&quot; section. Similarly, we are reporting plotly graphs:<\/p>\n<pre><code>self.task_logger.report_plotly(\n        title=&quot;PlotTitle&quot;, iteration=0, series='SeriesName', figure=fig\n    )\n<\/code><\/pre>\n<p>Both work fine. The issue is, each new <code>report_plotly<\/code> call, instead of replacing the image in the section, creates a new one, and leaves the previous one present too. This cloggs the PLOTS section (tables and figures). The question is, how does one report a plot, so that it's reported in-place (Such as e.g., scalars, where sample plot gets updated in time)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662041496990,
        "Question_score":0,
        "Question_tags":"python|plot|clearml",
        "Question_view_count":23,
        "Owner_creation_time":1397215330877,
        "Owner_last_access_time":1664044551867,
        "Owner_location":null,
        "Owner_reputation":1968,
        "Owner_up_votes":43,
        "Owner_down_votes":6,
        "Owner_views":190,
        "Question_last_edit_time":1662041933803,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73570817",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":62552414,
        "Question_title":"Can Trains config file be specified dynamically or relative to the running script path?",
        "Question_body":"<p>Suppose I have a server where many users run different experiments, possibly with different Trains Servers.<\/p>\n<p>I know about the <code>TRAINS_CONFIG_FILE<\/code> environment variable, but I wonder if this can be made more flexible in one of the following ways:<\/p>\n<ol>\n<li>Specifying the Trains config file dynamically, i.e. during runtime of the training script?<\/li>\n<li>Storing a config file in each of the training repos and specifying its path relatively to the running script path (instead of relatively to <code>~\/<\/code>)?<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592992723417,
        "Question_score":1,
        "Question_tags":"trains|clearml",
        "Question_view_count":129,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":1609531417760,
        "Answer_body":"<p>Disclaimer: I'm a member of Allegro Trains team<\/p>\n<ol>\n<li>Loading of the configuration is done at import time. This means that if you set the os environment before importing the package, you should be fine:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>os.environ['TRAINS_CONFIG_FILE']='~\/repo\/trains.conf'\nfrom trains import Task\n<\/code><\/pre>\n<ol start=\"2\">\n<li>The configuration file is loaded based on the current working directory, this means that if you have <code>os.environ['TRAINS_CONFIG_FILE']='trains.conf'<\/code> the trains.conf file will be loaded from the running directory at the time the import happens (which usually is the folder where your script is executed from). This means you can have it as part of the repository, and always set the <code>TRAINS_CONFIG_FILE<\/code> to point to it.<\/li>\n<\/ol>\n<p>A few notes:<\/p>\n<ul>\n<li>What is the use case for different configuration files ?<\/li>\n<li>Notice that when running with <a href=\"https:\/\/github.com\/allegroai\/trains-agent\" rel=\"nofollow noreferrer\">trains-agent<\/a> , this method will override the configuration that the <em>trains-agent<\/em> passes to the code.<\/li>\n<\/ul>",
        "Answer_comment_count":7.0,
        "Answer_creation_time":1592997291032,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62552414",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":64636294,
        "Question_title":"Trains: reusing previous task id",
        "Question_body":"<p>I am using <code>reuse_last_task_id=True<\/code> to overwrite an existing task (with same project and task name). But the experiment contains the torch model and therefore does not overwrite the existing task but creates a new one. How can I detach the model from the task?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1604260179110,
        "Question_score":1,
        "Question_tags":"trains|clearml",
        "Question_view_count":133,
        "Owner_creation_time":1410412149420,
        "Owner_last_access_time":1664064601540,
        "Owner_location":null,
        "Owner_reputation":187,
        "Owner_up_votes":21,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Question_last_edit_time":1609418272432,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64636294",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73442798,
        "Question_title":"Does ClearML have accounting of information security events",
        "Question_body":"<p>ClearML is one of the most famous MLOps tools existing. It has logging of machine learning processes, however I couldn't find any information regarding its system of accounting of <strong>information security events<\/strong>.<\/p>\n<p>My question is: does ClearML have such system? Does it register\/log events of client-server interaction? If ClearML does, then what format is used?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661159945233,
        "Question_score":1,
        "Question_tags":"logging|clearml",
        "Question_view_count":37,
        "Owner_creation_time":1661159047617,
        "Owner_last_access_time":1663851754370,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73442798",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":65509754,
        "Question_title":"Can ClearML (formerly Trains) work a local server?",
        "Question_body":"<p>I am trying to start my way with <a href=\"https:\/\/github.com\/allegroai\/clearml\" rel=\"nofollow noreferrer\">ClearML<\/a> (formerly known as Trains).<\/p>\n<p>I see on the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/getting_started\/index.html\" rel=\"nofollow noreferrer\">documentation<\/a> that I need to have server running, either on the ClearML platform itself, or on a remote machine using AWS etc.<\/p>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<p>According to <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/deploying_clearml\/index.html\" rel=\"nofollow noreferrer\">this<\/a> I can install the <code>trains-server<\/code> on any remote machine, so in theory I should also be able to install it on my local machine, but it still requires me to have Kubernetes or Docker, but I am not using any of them.<\/p>\n<p>Anyone had any luck using ClearML (or Trains, I think it's still quite the same API and all) on a local server?<\/p>\n<ul>\n<li>My OS is Ubuntu 18.04.<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1609343679217,
        "Question_score":5,
        "Question_tags":"trains|clearml",
        "Question_view_count":740,
        "Owner_creation_time":1383083414230,
        "Owner_last_access_time":1663937141383,
        "Owner_location":null,
        "Owner_reputation":2801,
        "Owner_up_votes":371,
        "Owner_down_votes":0,
        "Owner_views":131,
        "Question_last_edit_time":1609427009849,
        "Answer_body":"<p>Disclaimer: I'm a member of the ClearML team (formerly Trains)<\/p>\n<blockquote>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<\/blockquote>\n<p>A few options:<\/p>\n<ol>\n<li>The Clearml Free trier offers free hosting for your experiments, these experiment are only accessible to you, unless you specifically want to share them among your colleagues. This is probably the easiest way to <a href=\"https:\/\/app.community.clear.ml\" rel=\"nofollow noreferrer\">get started<\/a>.<\/li>\n<li>Install the ClearML-Server basically all you need is docker installed and you should be fine. There are full instructions <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/deploying_clearml\/clearml_server_linux_mac\/\" rel=\"nofollow noreferrer\">here<\/a> , this is the summary:<\/li>\n<\/ol>\n<pre class=\"lang-bash prettyprint-override\"><code>echo &quot;vm.max_map_count=262144&quot; &gt; \/tmp\/99-trains.conf\nsudo mv \/tmp\/99-trains.conf \/etc\/sysctl.d\/99-trains.conf\nsudo sysctl -w vm.max_map_count=262144\nsudo service docker restart\n\nsudo curl -L &quot;https:\/\/github.com\/docker\/compose\/releases\/latest\/download\/docker-compose-$(uname -s)-$(uname -m)&quot; -o \/usr\/local\/bin\/docker-compose\nsudo chmod +x \/usr\/local\/bin\/docker-compose\n\nsudo mkdir -p \/opt\/trains\/data\/elastic_7\nsudo mkdir -p \/opt\/trains\/data\/mongo\/db\nsudo mkdir -p \/opt\/trains\/data\/mongo\/configdb\nsudo mkdir -p \/opt\/trains\/data\/redis\nsudo mkdir -p \/opt\/trains\/logs\nsudo mkdir -p \/opt\/trains\/config\nsudo mkdir -p \/opt\/trains\/data\/fileserver\n\nsudo curl https:\/\/raw.githubusercontent.com\/allegroai\/trains-server\/master\/docker-compose.yml -o \/opt\/trains\/docker-compose.yml\ndocker-compose -f \/opt\/trains\/docker-compose.yml up -d\n<\/code><\/pre>\n<ol start=\"3\">\n<li>ClearML also supports full offline mode (i.e. no outside connection is made). Once your experiment completes, you can manually import the run to your server (either self hosted or free tier server)<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.set_offline(True)\ntask = Task.init(project_name='examples', task_name='offline mode experiment')\n<\/code><\/pre>\n<p>When the process ends you will get a link to a zip file containing the output of the entire offline session:<\/p>\n<pre><code>ClearML Task: Offline session stored in \/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip\n<\/code><\/pre>\n<p>Later you can import the session with:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.import_offline_session('\/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1609345139523,
        "Answer_score":6.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1661326401412,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65509754",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73716806,
        "Question_title":"How to make ClearML not upload annotations twice when they have the same ID?",
        "Question_body":"<p>The following uploads two annotations, though I expected there to be only one<\/p>\n<pre><code>from typing import List\nfrom allegroai import Dataset, DatasetVersion, SingleFrame, DataView\nfrom allegroai.dataframe.annotation import BoundingBox2D\n\nallegro_frame = SingleFrame(\n    source=&quot;\/irrelevant\/source.png&quot;\n)\nann_id = &quot;the_id&quot;\nlabel = &quot;the_label&quot;\nannotation = BoundingBox2D(id=ann_id)\nallegro_frame.add_annotation(id=ann_id, box2d_xywh=(100, 100, 100, 100), labels=(label,))\nallegro_frame.add_annotation(id=ann_id, box2d_xywh=(100, 100, 100, 100), labels=(label,))\n\nallegro_frames: List[SingleFrame] = [\n    allegro_frame\n]\n\ndataset_name = r&quot;clml_test_dataset&quot;\nversion_name = r&quot;clml_test_version&quot;\ndataset = Dataset.create(dataset_name=dataset_name)\nversion = DatasetVersion.create_version(dataset_name=dataset_name, version_name=version_name)\nversion.add_frames(allegro_frames)\n<\/code><\/pre>\n<p>What's the correct way to make only one annotation be uploaded for the frame?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663158280063,
        "Question_score":0,
        "Question_tags":"python|clearml",
        "Question_view_count":5,
        "Owner_creation_time":1314313109233,
        "Owner_last_access_time":1664062223147,
        "Owner_location":"Technion, Israel",
        "Owner_reputation":18777,
        "Owner_up_votes":2376,
        "Owner_down_votes":137,
        "Owner_views":2000,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73716806",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":62654203,
        "Question_title":"Trains: Can I reset the status of a task? (from 'Aborted' back to 'Running')",
        "Question_body":"<p>I had to stop training in the middle, which set the Trains status to <code>Aborted<\/code>.\nLater I continued it from the last checkpoint, but the status remained <code>Aborted<\/code>.\nFurthermore, automatic training metrics stopped appearing in the dashboard (though custom metrics still do).<\/p>\n<p>Can I reset the status back to <code>Running<\/code> and make Trains log training stats again?<\/p>\n<p><strong>Edit:<\/strong> When continuing training, I retrieved the task using <code>Task.get_task()<\/code> and not <code>Task.init()<\/code>. Maybe that's why training stats are not updated anymore?<\/p>\n<p><strong>Edit2:<\/strong> I also tried <code>Task.init(reuse_last_task_id=original_task_id_string)<\/code>, but it just creates a new task, and doesn't reuse the given task ID.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593508903533,
        "Question_score":1,
        "Question_tags":"trains|clearml",
        "Question_view_count":228,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":1609467668432,
        "Answer_body":"<p>Disclaimer: I'm a member of Allegro Trains team<\/p>\n<blockquote>\n<p>When continuing training, I retrieved the task using Task.get_task() and not Task.init(). Maybe that's why training stats are not updated anymore?<\/p>\n<\/blockquote>\n<p>Yes that's the only way to continue the same exact Task.\nYou can also mark it as started with <code>task.mark_started()<\/code> , that said the automatic logging will not kick in, as <code>Task.get_task<\/code> is usually used for accessing previously executed tasks and not continuing it (if you think the continue use case is important please feel free to open a GitHub issue, I can definitely see the value there)<\/p>\n<p>You can also do something a bit different, and justcreate a new Task continuing from the last iteration the previous run ended. Notice that if you load the weights file (PyTorch\/TF\/Keras\/JobLib) it will automatically connect it with the model that was created in the previous run (assuming the model was stored is the same location, or if you have the model on https\/S3\/Gs\/Azure and you are using <code>trains.StorageManager.get_local_copy()<\/code>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>previous_run = Task.get_task()\ntask = Task.init('examples', 'continue training')\ntask.set_initial_iteration(previous_run.get_last_iteration())\ntorch.load('\/tmp\/my_previous_weights')\n<\/code><\/pre>\n<p>BTW:<\/p>\n<blockquote>\n<p>I also tried Task.init(reuse_last_task_id=original_task_id_string), but it just creates a new task, and doesn't reuse the given task ID.<\/p>\n<\/blockquote>\n<p>This is a great idea for an interface to continue a previous run, feel free to add it as GitHub issue.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1593515579540,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1593557456892,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62654203",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":66279581,
        "Question_title":"ClearML multiple tasks in single script changes logged value names",
        "Question_body":"<p>I trained multiple models with different configuration for a custom hyperparameter search. I use pytorch_lightning and its logging (TensorboardLogger).\nWhen running my training script after Task.init() ClearML auto-creates a Task and connects the logger output to the server.<\/p>\n<p>I log for each straining stage <code>train<\/code>, <code>val<\/code> and <code>test<\/code> the following scalars at each epoch: <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code><\/p>\n<p>When I have multiple configuration, e.g. <code>networkA<\/code> and <code>networkB<\/code> the first training log its values to <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code>, but the second to <code>networkB:loss<\/code>, <code>networkB:acc<\/code> and <code>networkB:iou<\/code>. This makes values umcomparable.<\/p>\n<p>My training loop with Task initalization looks like this:<\/p>\n<pre><code>names = ['networkA', networkB']\nfor name in names:\n     task = Task.init(project_name=&quot;NetworkProject&quot;, task_name=name)\n     pl_train(name)\n     task.close()\n<\/code><\/pre>\n<p>method pl_train is a wrapper for whole training with Pytorch Ligtning. No ClearML code is inside this method.<\/p>\n<p>Do you have any hint, how to properly use the usage of a loop in a script using completly separated tasks?<\/p>\n<hr \/>\n<p>Edit: ClearML version was 0.17.4. Issue is fixed in main branch.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1613745436903,
        "Question_score":1,
        "Question_tags":"devops|clearml|trains",
        "Question_view_count":279,
        "Owner_creation_time":1604391794420,
        "Owner_last_access_time":1663670435373,
        "Owner_location":null,
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1614004159640,
        "Answer_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p><code>pytorch_lightning<\/code> is creating a new Tensorboard for each experiment. When ClearML logs the TB scalars, and it captures the same scalar being re-sent again, it adds a prefix so if you are reporting the same metric it will not overwrite the previous one. A good example would be reporting <code>loss<\/code> scalar in the training phase vs validation phase (producing &quot;loss&quot; and &quot;validation:loss&quot;). It might be the <code>task.close()<\/code> call does not clear the previous logs, so it &quot;thinks&quot; this is the same experiment, hence adding the prefix <code>networkB<\/code> to the <code>loss<\/code>. As long as you are closing the Task after training is completed you should have all experiments log with the same metric\/variant (title\/series). I suggest opening a GitHub issue, this should probably be considered a bug.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1613773903383,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66279581",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":72593187,
        "Question_title":"ModuleNotFoundError: No module named 'allegroai'",
        "Question_body":"<p>I am trying to use <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/getting_started\/mlops\/mlops_first_steps\" rel=\"nofollow noreferrer\">ClearML<\/a>.<\/p>\n<p>The only line in my file is<\/p>\n<pre><code>from allegroai import Dataset, DatasetVersion\n<\/code><\/pre>\n<p>which yields<\/p>\n<pre><code>ModuleNotFoundError: No module named 'allegroai'\n<\/code><\/pre>\n<p>Looks like some pip package is missing, but I couldn't for the life of me find it in the docs.<\/p>\n<p>What should I pip install?<\/p>\n<p><strong>Not working:<\/strong><\/p>\n<ul>\n<li><code>pip install clearml-agent<\/code><\/li>\n<li><code>pip install clearml<\/code> and <code>clearml-init<\/code> as in <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/getting_started\/ds\/ds_first_steps\" rel=\"nofollow noreferrer\">here<\/a><\/li>\n<li><code>pip install allegroai<\/code><\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1655044800580,
        "Question_score":0,
        "Question_tags":"python|clearml",
        "Question_view_count":49,
        "Owner_creation_time":1314313109233,
        "Owner_last_access_time":1664062223147,
        "Owner_location":"Technion, Israel",
        "Owner_reputation":18777,
        "Owner_up_votes":2376,
        "Owner_down_votes":137,
        "Owner_views":2000,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Allegroai package should be taken from ClearML PyPi server.\nThis is only for paying customers (I think), and the way to retrieve it is by:<\/p>\n<ol>\n<li>Going to ClearML website (login with username\/company).<\/li>\n<li>Press the ? on the top right of the screen (next to your user icon) and choose the first one\n<a href=\"https:\/\/i.stack.imgur.com\/B1rDo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/B1rDo.png\" alt=\"1\" \/><\/a><\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1657799581543,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72593187",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":72670966,
        "Question_title":"clearml-serving-triton | 2022-06-18 16:21:19,790 - clearml.Task - INFO - No repository found, storing script code instead",
        "Question_body":"<p>i found this error when running this command &quot;docker-compose --env-file example.env -f docker-compose-triton.yml up&quot;.\nActually, when i run this command for the first time, it worked. And then when I try to change to my friend's workspace it suddenly give this error when I run the command again.\nI've tried changing example.env file configuration and reinstall the docker, still not worked.\nCan anyone help? or have the solution? Thank You :)\n<a href=\"https:\/\/i.stack.imgur.com\/Fp8XK.png\" rel=\"nofollow noreferrer\">https:\/\/i.stack.imgur.com\/Fp8XK.png<\/a>\n[Error details on the image]<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655570110730,
        "Question_score":0,
        "Question_tags":"docker|clearml",
        "Question_view_count":43,
        "Owner_creation_time":1655569987980,
        "Owner_last_access_time":1664004364857,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1655570179420,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72670966",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":71432331,
        "Question_title":"ALB host based routing without domain name",
        "Question_body":"<p>I'm trying to configure host based routing in AWS ALB for ClearML server using <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/deploying_clearml\/clearml_server_config.html#configuration-procedures\" rel=\"nofollow noreferrer\">this tutorial<\/a>.\nHowever, I don't have a domain name. So can I only use alb's dns for this routing?<\/p>\n<p>For example, I will have the address as app.<em><strong>.ap-north-east-1.elb.amazonaws.com, api.<\/strong><\/em>.ap-north-east-1.elb.amazonaws.com.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646958308150,
        "Question_score":0,
        "Question_tags":"aws-application-load-balancer|clearml",
        "Question_view_count":190,
        "Owner_creation_time":1538398333233,
        "Owner_last_access_time":1660382572563,
        "Owner_location":"\u014csaka-shi, \u5927\u962a\u5e9c \u65e5\u672c",
        "Owner_reputation":51,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71432331",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":69127220,
        "Question_title":"how to capture logger values using clearml",
        "Question_body":"<p>I am using clearml for testing algorithms and it works well with library <a href=\"https:\/\/github.com\/DLR-RM\/stable-baselines3\/tree\/f8a08690737000e4d23eb643a21d70486ece038b\" rel=\"nofollow noreferrer\">Stable Baselines 3<\/a>, in which clearml automatically captures all the output and plot them in the Scalars tab.<\/p>\n<p>However, when I switched to another library <a href=\"https:\/\/github.com\/pfnet\/pfrl\" rel=\"nofollow noreferrer\">PFRL<\/a> clearml no longer output anything to the Scalar tab. After looking into the code I found PFRL outputs statistics using <code>logger.info<\/code>, which seemed to be the reason of empty Scalars tab (but there was output in the Console tab).\nI am wondering is there any method that I can make clearml automatically collect them into the Scalars tab.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1631247565663,
        "Question_score":0,
        "Question_tags":"python|pytorch|clearml",
        "Question_view_count":97,
        "Owner_creation_time":1631247013317,
        "Owner_last_access_time":1658201267683,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69127220",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":56762990,
        "Question_title":"How to manually register a sci-kit model with TRAINS python auto-magical experiment manager?",
        "Question_body":"<p>I'm working mostly with scikit-learn, as far as I understand, the TRAINS auto-magic doesn't catch scikit-learn model store\/load automatically.<\/p>\n\n<p>How do I manually register the model after I have 'pickled' it.<\/p>\n\n<p>For Example:<\/p>\n\n<pre><code>import pickle\nwith open(\"model.pkl\", \"wb\") as file:  \n    pickle.dump(my_model, file)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1561503148140,
        "Question_score":1,
        "Question_tags":"python|machine-learning|scikit-learn|trains|clearml",
        "Question_view_count":155,
        "Owner_creation_time":1528727701343,
        "Owner_last_access_time":1648026581957,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1609778626447,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56762990",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":70592241,
        "Question_title":"How to Deploy a ClearML Agent in a k8s setting?",
        "Question_body":"<p>I want to deploy a ClearML agent in the kubernetes environment while using the ClearML's Free Tier Demo server.\nI was able to deploy the Agent pod in the k8s cluster with the <code>allegroai\/clearml-agent<\/code> docker image. But was not able to link this agent to the ClearML Demo server.\nCan anyone help me with solving this issue of configuring the API access and secret keys for the k8s pod of CLearML Agent.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1641382824047,
        "Question_score":0,
        "Question_tags":"kubernetes|clearml",
        "Question_view_count":112,
        "Owner_creation_time":1641382442503,
        "Owner_last_access_time":1647005909747,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70592241",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":63606182,
        "Question_title":"How should Trains be used with hyper-param optimization tools like RayTune?",
        "Question_body":"<p>What could be a reasonable setup for this? Can I call Task.init() multiple times in the same execution?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1598478428030,
        "Question_score":2,
        "Question_tags":"trains|clearml",
        "Question_view_count":107,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":1609427499190,
        "Answer_body":"<p>Disclaimer: I'm part of the allegro.ai Trains team<\/p>\n<p>One solution is to inherit from <a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/838c9cb0d2a5df5c193dfc85286abe59a80217c2\/trains\/automation\/optimization.py#L226\" rel=\"nofollow noreferrer\">trains.automation.optimization.SearchStrategy<\/a> and extend the functionality. This is similar to the <a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/master\/trains\/automation\/optuna\/optuna.py\" rel=\"nofollow noreferrer\">Optuna<\/a> integration, where Optuna is used for the Bayesian optimization and Trains does the hyper-parameter setting, launching experiments, and retrieving performance metrics.<\/p>\n<p>Another option (not scalable but probably easier to start with), is to use have the RayTuner run your code (obviously setting the environment \/ git repo \/ docker etc is on the user), and have your training code look something like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># create new experimnt\ntask = Task.init('hp optimization', 'ray-tuner experiment', reuse_last_task_id=False)\n# store the hyperparams (assuming hparam is a dict) \ntask.connect(hparam) \n# training loop here\n# ...\n# shutdown experimnt\ntask.close()\n<\/code><\/pre>\n<p>This means every time the RayTuner executes the script a new experiment will be created, with new set of hyper parameters (assuming <code>haparm<\/code> is a dictionary, it will be registered on the experiment as hyper-parameters)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1598536233436,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63606182",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":72381916,
        "Question_title":"Remotely execute ClearML task using local-only repo",
        "Question_body":"<p>I want to execute ClearML task remotely. According to docs there are 2 options: 1) execute single python file; 2) ClearML would identify that script is part of repo, that repo will be cloned and installed into docker and executed on the worker.<\/p>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<p>I think it is somewhat extending scenario 1, where not a single file is passed for execution but whole directory with file in it.<\/p>\n<p>PS: i understand reproducibility concerns that arise, but repo is really not accessible from worker :(<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653499480970,
        "Question_score":0,
        "Question_tags":"clearml",
        "Question_view_count":25,
        "Owner_creation_time":1653498830777,
        "Owner_last_access_time":1662113390107,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<blockquote>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<\/blockquote>\n<p>well, no :( if your code is a single script, then yes ClearML would store the entire script, then the worker will reproduce it on the remote machine. But if your code base is composed of more than a single file, then why not use git? it is free hosted by GitHub, Bitbucket, GitLab etc.<\/p>\n<p>In theory this is doable and if you feel the need, I urge you to PR this feature. Basically you would store the entire folder as an artifact (ClearML will auto zip it for you), then the agent needs to unzip the artifact and run it. The main issue would be that cloning the Task will not clone the artifact...<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1659992618003,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72381916",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":64305945,
        "Question_title":"pip install trains fails",
        "Question_body":"<p>upon running <code>pip install trains<\/code> in my virtual env<\/p>\n<p>I am getting<\/p>\n<pre><code>    ERROR: Command errored out with exit status 1:\n     command: \/home\/epdadmin\/noam\/code\/venv_linux\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-lxz5t8pu\/install-record.txt --single-version-externally-managed --compile --install-headers \/home\/epdadmin\/noam\/code\/venv_linux\/include\/site\/python3.8\/retrying\n         cwd: \/tmp\/pip-install-owzh8lnl\/retrying\/\n    Complete output (10 lines):\n    running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib\n    copying retrying.py -&gt; build\/lib\n    running install_lib\n    copying build\/lib\/retrying.py -&gt; \/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\n    byte-compiling \/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\/retrying.py to retrying.cpython-38.pyc\n    error: [Errno 13] Permission denied: '\/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\/__pycache__\/retrying.cpython-38.pyc.139678407381360'\n    ----------------------------------------\nERROR: Command errored out with exit status 1: \/home\/epdadmin\/noam\/code\/venv_linux\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-lxz5t8pu\/install-record.txt --single-version-externally-managed --compile --install-headers \/home\/epdadmin\/noam\/code\/venv_linux\/include\/site\/python3.8\/retrying Check the logs for full command output.\n<\/code><\/pre>\n<p>I know that <a href=\"https:\/\/stackoverflow.com\/questions\/15028648\/is-it-acceptable-and-safe-to-run-pip-install-under-sudo\">I am not supposed to run under sudo when using a venv<\/a>, so I don't really understand the problem<\/p>\n<p>running for example <code>pip install pandas<\/code> does work.<\/p>\n<p>Python 3.8<\/p>\n<p>How to install trains?<\/p>\n<hr \/>\n<p>EDIT:<\/p>\n<p>running <code>pip install trains --user<\/code> or <code>pip install --user trains<\/code> gives<\/p>\n<pre><code>ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_time":1602431071093,
        "Question_score":1,
        "Question_tags":"python|pip|trains|clearml",
        "Question_view_count":1031,
        "Owner_creation_time":1314313109233,
        "Owner_last_access_time":1664062223147,
        "Owner_location":"Technion, Israel",
        "Owner_reputation":18777,
        "Owner_up_votes":2376,
        "Owner_down_votes":137,
        "Owner_views":2000,
        "Question_last_edit_time":1609353956110,
        "Answer_body":"<p>The problem was a permissions problem for the venv.\nAnother problem was trains required some packages that were not yet available with wheels on Python3.8, so I had to downgrade Python to 3.7<\/p>\n<p>That venv was created using Pycharm, and for some reason it was created with low permissions.<\/p>\n<p>There was probably a way to elevate its permissions, but instead I just deleted it and created another one using command line by<\/p>\n<pre><code>python -m virtualenv --python=\/usr\/bin\/python3.7 venv\n<\/code><\/pre>\n<p>And now <code>pip install trains<\/code> worked.<\/p>\n<p>Very annoying.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1602767930969,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1604308183096,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64305945",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":65671395,
        "Question_title":"ClearML SSH port forwarding fileserver not available in WEB Ui",
        "Question_body":"<p>Trying to use clearml-server on own Ubuntu 18.04.5 with SSH Port Forwarding and not beeing able to see my debug samples.<\/p>\n<p>My setup:<\/p>\n<ul>\n<li>ClearML server on hostA<\/li>\n<li>SSH Tunnel connections to access Web App from working machine via localhost:18080<\/li>\n<li>Web App: <code>ssh -N -L 18081:127.0.0.1:8081 user@hostA<\/code><\/li>\n<li>Fileserver: <code>ssh -N -L 18081:127.0.0.1:8081 user@hostA<\/code><\/li>\n<\/ul>\n<p>In Web App under Task-&gt;Results-&gt;Debug Samples the Images are still refrenced by localhost:8081<\/p>\n<p>Where can I set the fileserver URL to be localhost:18081 in Web App?\nI tried ~\/clearml.conf, but this did not work ( I think it is for my python script ).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610384778623,
        "Question_score":1,
        "Question_tags":"devops|local|portforwarding|trains|clearml",
        "Question_view_count":121,
        "Owner_creation_time":1604391794420,
        "Owner_last_access_time":1663670435373,
        "Owner_location":null,
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Disclaimer: I'm a member of the ClearML team (formerly Trains)<\/p>\n<p>In ClearML, debug images' URL is registered once they are uploaded to the fileserver. The WebApp doesn't actually decide on the URL for each debug image, but rather obtains it for each debug image from the server. This allows you to potentially upload debug images to a variety of storage targets, ClearML File Server simply being the most convenient, built-in option.<\/p>\n<p>So, the WebApp will always look for <code>localhost:8008<\/code> for debug images that have already been uploaded to the fileserver and contain <code>localhost:8080<\/code> in their URL.\nA possible solution is to simply add another tunnel in the form of <code>ssh -N -L 8081:127.0.0.1:8081 user@hostA<\/code>.<\/p>\n<p>For future experiments, you can choose to keep using <code>8081<\/code> (and keep using this new tunnel), or to change the default fileserver URL in <code>clearml.conf<\/code> to point to port <code>localhost:18081<\/code>, assuming you're running your experiments from the same machine where the tunnel to <code>18081<\/code> exists.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1610390345336,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65671395",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73108016,
        "Question_title":"Where do augmentations in ClearML run?",
        "Question_body":"<p>In ClearML Dataviews, it is possible to add <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/hyperdatasets\/dataviews\/#data-augmentation\" rel=\"nofollow noreferrer\">augmentations<\/a>.<\/p>\n<p>Where do these augmentations run?<\/p>\n<p>Options<\/p>\n<ol>\n<li>Original data gets downloaded to local, then runs (on which device? How is multiprocessing handled?)<\/li>\n<li>Only augmented data gets downloaded to local cache, augmentations run remotely (who pays for compute? How fast? Should pipelines be changed accordingly?)<\/li>\n<\/ol>\n<p>I couldn't find this in the docs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658746321690,
        "Question_score":0,
        "Question_tags":"deep-learning|data-augmentation|mlops|clearml",
        "Question_view_count":22,
        "Owner_creation_time":1314313109233,
        "Owner_last_access_time":1664062223147,
        "Owner_location":"Technion, Israel",
        "Owner_reputation":18777,
        "Owner_up_votes":2376,
        "Owner_down_votes":137,
        "Owner_views":2000,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73108016",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":57557070,
        "Question_title":"trains with grid search",
        "Question_body":"<p>I would like to test <a href=\"https:\/\/github.com\/allegroai\/trains\" rel=\"nofollow noreferrer\">trains<\/a> usage during grid search and it not clear how to do so.<\/p>\n\n<pre><code>from trains import Task \nTask.init(project_name=\"project name\", task_name='name')\n<\/code><\/pre>\n\n<p>creates an experiment in the demo server and logs all but you can't call init twice no matter the 'task_name' and <\/p>\n\n<pre><code>from trains import Task \nTask.create(project_name=\"project name\", task_name='name')\n<\/code><\/pre>\n\n<p>can be called with different 'task_name' but thus not log any data into the server and creates only 'Draft'.<\/p>\n\n<p>here is a sample code:<\/p>\n\n<pre><code> epochs=[160,300]\n for epoch in epochs:\n    model = define_model_run(epoch)\n    model.fit(x_train,y_train)\n    score = model.score(...)\n<\/code><\/pre>\n\n<p>my final try was:<\/p>\n\n<pre><code> epochs=[160,300]\n task=Task.init(project_name=\"demo\", task_name='search')\n for epoch in epochs:\n    task.create(project_name=\"demo\", task_name=f'search_{epoch}')\n    model = define_model_run(epoch)\n    model.fit(x_train,y_train)\n    score = model.score(...)\n<\/code><\/pre>\n\n<p>which logs all information under the experiments tab and none under the 'Draft'.\nI tried the last two hour the read the few documentations provided and reading the source code, but no luck.<\/p>\n\n<p>any help? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1566218806593,
        "Question_score":1,
        "Question_tags":"python|trains|clearml",
        "Question_view_count":121,
        "Owner_creation_time":1416942229380,
        "Owner_last_access_time":1663871932353,
        "Owner_location":null,
        "Owner_reputation":161,
        "Owner_up_votes":89,
        "Owner_down_votes":0,
        "Owner_views":58,
        "Question_last_edit_time":1609778640296,
        "Answer_body":"<p>Declaimer: I'm a member of TRAINS team<\/p>\n\n<p>Yes, that's exactly the answer.\nThe idea is that you always have one main Task, in order to create a new one you need to close the running Task, and re-initialize with a new name.\nKudos on solving it so quickly :)<\/p>\n\n<p>BTW: You can see examples <a href=\"https:\/\/stackoverflow.com\/q\/56744397\/11682840\">here<\/a>\/<a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/master\/docs\/faq.md#can-i-create-a-graph-comparing-hyper-parameters-vs-model-accuracy-\" rel=\"nofollow noreferrer\">and here<\/a>, showing how to send accuracy logs so it is easier to compare the experiments, especially when running hyper-parameter search.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1566240704540,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1566242122212,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57557070",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":70397010,
        "Question_title":"What would stop credentials from validation on a ClearML server?",
        "Question_body":"<p>I've set up a ClearML server in GCP using the sub-domain approach. I can access all three domains (<code>https:\/\/app.clearml.mydomain.com<\/code>, <code>https:\/\/api.clearml.mydomain.com<\/code> and <code>https:\/\/files.clearml.mydomain.com<\/code>) in a browser and see what I think is the correct response, but when connecting with the python SDK via <code>clearml-init<\/code> I get the following error:<\/p>\n<pre><code>clearml.backend_api.session.session.LoginError: Failed getting token (error 400 from https:\/\/api.clearml.mydomain.com): Bad Request\n<\/code><\/pre>\n<p>Are there any likely causes of this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639763350487,
        "Question_score":1,
        "Question_tags":"clearml",
        "Question_view_count":194,
        "Owner_creation_time":1600124498003,
        "Owner_last_access_time":1661436804020,
        "Owner_location":null,
        "Owner_reputation":46,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Following the discussion <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/517\" rel=\"nofollow noreferrer\">here<\/a>, it seemed that the load balancer being used was blocking <code>GET<\/code> requests with a payload which are used by ClearML. A <a href=\"https:\/\/github.com\/allegroai\/clearml\/pull\/521\" rel=\"nofollow noreferrer\">fix<\/a> is being worked on to allow the method to be changed to a <code>POST<\/code> request via an environment variable.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1640162037740,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70397010",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":58656998,
        "Question_title":"Parallel Coordinates Plot in TRAINS",
        "Question_body":"<p>Is there a way to create a parallel coordinates plot in TRAINS (<a href=\"https:\/\/github.com\/allegroai\/trains\" rel=\"nofollow noreferrer\">https:\/\/github.com\/allegroai\/trains<\/a>) package to compare several hyper-parameters in respect to a specific metric?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1572601152533,
        "Question_score":2,
        "Question_tags":"python|deep-learning|trains|clearml",
        "Question_view_count":57,
        "Owner_creation_time":1374041402270,
        "Owner_last_access_time":1579814770290,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1609781210992,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58656998",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":66216294,
        "Question_title":"ClearML how to change clearml.conf file in AWS Sagemaker",
        "Question_body":"<p>I am working in AWS Sagemaker Jupyter notebook.\nI have installed clearml package in AWS Sagemaker in Jupyter.\nClearML server was installed on AWS EC2.\nI need to store artifacts and models in AWS S3 bucket, so I want to specify credentials to S3 in clearml.conf file.\nHow can I change clearml.conf file in AWS Sagemaker instance? looks like permission denied to all folders on it.\nOr maybe somebody can suggest a better approach.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1613428648627,
        "Question_score":2,
        "Question_tags":"clearml",
        "Question_view_count":276,
        "Owner_creation_time":1585870038407,
        "Owner_last_access_time":1626728054187,
        "Owner_location":"Christchurch, \u041d\u043e\u0432\u0430\u044f \u0417\u0435\u043b\u0430\u043d\u0434\u0438\u044f",
        "Owner_reputation":45,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p>To set credentials (and <code>clearml-server<\/code> hosts) you can use <code>Task.set_credentials<\/code>.\nTo specify the S3 bucket as output for all artifacts (and debug images for that matter) you can just set it as the <code>files_server<\/code>.<\/p>\n<p>For example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\n\nTask.set_credentials(api_host='http:\/\/clearml-server:8008', web_host='http:\/\/clearml-server:8080', files_host='s3:\/\/my_bucket\/folder\/',\nkey='add_clearml_key_here', secret='add_clearml_key_secret_here')\n<\/code><\/pre>\n<p>To pass your S3 credentials, just add a cell at the top of your jupyter notebook, and set the standard AWS S3 environment variables:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nos.environ['AWS_ACCESS_KEY_ID'] = 's3_bucket_key_here'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 's3_bucket_secret_here'\n# optional\nos.environ['AWS_DEFAULT_REGION'] = 's3_bucket_region'\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1613774515323,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66216294",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":56744397,
        "Question_title":"Is there a way to create a graph comparing hyper-parameters vs model accuracy with TRAINS python package?",
        "Question_body":"<p>I would like to run multiple experiments, then report model accuracy per experiment.<\/p>\n\n<p>I'm training a toy MNIST example with pytorch (v1.1.0), but the goal is, once I can compare performance for the toy problem, to have it integrated with the actual code base.<\/p>\n\n<p>As I understand the TRAINS python package, with the \"two lines of code\" all my hyper-parameters are already logged (Command line argparse in my case). <\/p>\n\n<p>What do I need to do in order to report a final scalar and then be able to sort through all the different training experiments (w\/ hyper-parameters) in order to find the best one.<\/p>\n\n<p>What I'd like to get, is a graph\/s where on the X-axis I have hyper-parameter values and on the Y-axis I have the validation accuracy.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1561413272967,
        "Question_score":1,
        "Question_tags":"python|deep-learning|pytorch|trains|clearml",
        "Question_view_count":261,
        "Owner_creation_time":1342883896543,
        "Owner_last_access_time":1577391054037,
        "Owner_location":null,
        "Owner_reputation":81,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":1609861256560,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56744397",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":68798737,
        "Question_title":"ClearML How to get configurable hyperparameters?",
        "Question_body":"<p>How do I get args like epochs to show up in the UI configuration panel under hyperparameters? I want to be able to change number of epochs and learning rate from within the UI.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1629097990217,
        "Question_score":0,
        "Question_tags":"devops|mlops|clearml|trains",
        "Question_view_count":110,
        "Owner_creation_time":1616008398583,
        "Owner_last_access_time":1652343370277,
        "Owner_location":"Islamabad, Pakistan",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use <code>argparse<\/code> - ClearML will auto-magically log all parameters in the task's configuration section (under hyper-parameters section) - see <a href=\"https:\/\/github.com\/allegroai\/clearml\/blob\/master\/examples\/frameworks\/keras\/keras_tensorboard.py#L56\" rel=\"nofollow noreferrer\">this<\/a> example. You can also just connect any dictionary (see <a href=\"https:\/\/github.com\/allegroai\/clearml\/blob\/master\/examples\/frameworks\/ignite\/cifar_ignite.py#L23\" rel=\"nofollow noreferrer\">this<\/a> example)<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1629122761020,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68798737",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":66640850,
        "Question_title":"How to manage datasets in ClearML Web UI?",
        "Question_body":"<p>Using a self-deployed ClearML server with the clearml-data CLI, I would like to manage (or view) my datasets in the WebUI as shown on the ClearML webpage (<a href=\"https:\/\/clear.ml\/mlops\/clearml-feature-store\/\" rel=\"nofollow noreferrer\">https:\/\/clear.ml\/mlops\/clearml-feature-store\/<\/a>):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/2T8IC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2T8IC.png\" alt=\"ClearML feature store\" \/><\/a><\/p>\n<p>However, this feature does not show up in my Web UI. According to the pricing page, the feature store is not a premium feature. Do I need to configure my server in a special way to use this feature?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615821875663,
        "Question_score":3,
        "Question_tags":"clearml|trains",
        "Question_view_count":310,
        "Owner_creation_time":1551960780550,
        "Owner_last_access_time":1663614230163,
        "Owner_location":null,
        "Owner_reputation":354,
        "Owner_up_votes":54,
        "Owner_down_votes":3,
        "Owner_views":58,
        "Question_last_edit_time":1615822213440,
        "Answer_body":"<p>Disclaimer: I'm part of the ClearML (formerly Trains) Team<\/p>\n<p>I think this screenshot is taken from the premium version...\nThe feature itself exists in the open-source version, but I &quot;think&quot; some of the dataset visualization capabilities are not available in the open-source self hosted version.<\/p>\n<p>Nonetheless, you have a fully featured feature-store, with the ability to add your own metrics \/ samples for every dataset\/feature version. The open-source version also includes the advanced versioning &amp; delta based storage for datasets\/features (i.e. only the change set from the parent version is stored)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1615831186403,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66640850",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":71099048,
        "Question_title":"clearml hyperparameter optimization task fails on creating python environment",
        "Question_body":"<p>Trying to use clearml HyperparametersOptimizers. Launching task, but each experiment fails while trying to create python environment. Specificaly the error I get is related to pytorch<\/p>\n<pre><code>clearml_agent: Warning: could not resolve python wheel replacement for torch==1.1.0\nException when trying to resolve python wheel: Could not find pytorch wheel URL for: torch==1.1.0 with cuda 113 support\nclearml_agent: ERROR: Could not install task requirements!\nException when trying to resolve python wheel: Could not find pytorch wheel URL for: torch==1.1.0 with cuda 113 support\n<\/code><\/pre>\n<p>I personally don't need pytorch for the code I am running and I removed it from the environment I am using, but the pytorch channel is present somewhere as it still tries to download it:<\/p>\n<pre><code>Executing Conda: &quot;C:\\Users\\nir.s\\Anaconda3\\Scripts\\conda.exe&quot; install -p &quot;C:\\Users\\nir.s\\.clearml\\venvs-builds\\3.7&quot; -c defaults -c conda-forge -c pytorch &quot;pip&lt;20.2&quot; --quiet --json\n<\/code><\/pre>\n<p>I am not sure if there is some configuration file that I missed or if it's because there are other parts of the repository that use pytorch, either way, as things are now I can't run tasks with clearml-agent since it fails to build an interpreter. Just to clarify, I do have environments working with pytorch for separate projects that use it. So the problem is not with anaconda or pip as it is installed on my computer since I can make it work when running locally.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1644740916897,
        "Question_score":1,
        "Question_tags":"clearml",
        "Question_view_count":95,
        "Owner_creation_time":1629265578777,
        "Owner_last_access_time":1651657361480,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71099048",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73279794,
        "Question_title":"[Catboost][ClearML] Error: if loss-function is Logloss, then class weights should be given for 0 and 1 classes",
        "Question_body":"<p>Having recently started using ClearML to manage the MLOps, I am facing the following problem:\nWhen running a script that trains a CatBoost in a binary classification problem using different class weights from my computer, it works perfectly, logs the results and no issues at all.\nOnce I try to run that remotely using the ClearML agent, it results in the following error:<\/p>\n<pre><code>&lt;!-- language: lang-none --&gt;\nTraceback (most recent call last):\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/catboost_bind.py&quot;, line 102, in _fit\n    return original_fn(obj, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 5007, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2262, in _fit\n    train_params = self._prepare_train_params(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2194, in _prepare_train_params\n    _check_train_params(params)\n  File &quot;_catboost.pyx&quot;, line 6032, in _catboost._check_train_params\n  File &quot;_catboost.pyx&quot;, line 6051, in _catboost._check_train_params\n**_catboost.CatBoostError: catboost\/private\/libs\/options\/catboost_options.cpp:607: if loss-function is Logloss, then class weights should be given for 0 and 1 classes\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):**\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/task_repository\/RecSys.git\/src\/cli\/model_training_remote.py&quot;, line 313, in &lt;module&gt;\n    rfs.run(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/task_repository\/RecSys.git\/src\/cli\/model_training_remote.py&quot;, line 232, in run\n    model.fit(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/__init__.py&quot;, line 36, in _inner_patch\n    raise ex\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/__init__.py&quot;, line 34, in _inner_patch\n    ret = patched_fn(original_fn, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/catboost_bind.py&quot;, line 110, in _fit\n    return original_fn(obj, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 5007, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2262, in _fit\n    train_params = self._prepare_train_params(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2194, in _prepare_train_params\n    _check_train_params(params)\n  File &quot;_catboost.pyx&quot;, line 6032, in _catboost._check_train_params\n  File &quot;_catboost.pyx&quot;, line 6051, in _catboost._check_train_params\n**_catboost.CatBoostError: catboost\/private\/libs\/options\/catboost_options.cpp:607: if loss-function is Logloss, then class weights should be given for 0 and 1 classes**\n\n<\/code><\/pre>\n<p>I do have the dictionary being connected:<\/p>\n<pre><code>    model_params = {\n        &quot;loss_function&quot;: &quot;Logloss&quot;,\n        &quot;eval_metric&quot;: &quot;AUC&quot;,\n        &quot;class_weights&quot;: {0: 1, 1: 60},\n        &quot;learning_rate&quot;: 0.1\n    }\n<\/code><\/pre>\n<p>registered in the ClearML task as<\/p>\n<pre><code>task.connect(model_params, 'model_params')\n<\/code><\/pre>\n<p>and used as parameters for the model in the following call:<\/p>\n<pre><code>model = CatBoostClassifier(**model_params)\n<\/code><\/pre>\n<p>When running it from the container in ClearML interactive mode, it also works fine.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659970052813,
        "Question_score":1,
        "Question_tags":"python|catboost|clearml",
        "Question_view_count":54,
        "Owner_creation_time":1659969003173,
        "Owner_last_access_time":1663967691430,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<p>I think I understand the problem, basically I think the issue is:<\/p>\n<pre><code>task.connect(model_params, 'model_params')\n<\/code><\/pre>\n<p>Since this is a nested dict:<\/p>\n<pre><code>    model_params = {\n        &quot;loss_function&quot;: &quot;Logloss&quot;,\n        &quot;eval_metric&quot;: &quot;AUC&quot;,\n        &quot;class_weights&quot;: {0: 1, 1: 60},\n        &quot;learning_rate&quot;: 0.1\n    }\n<\/code><\/pre>\n<p>The class_weights is stored as a <code>String<\/code> key, but <code>catboost<\/code> expects <code>int<\/code> key, hence failing.\nOne option would be to remove the <code>task.connect(model_params, 'model_params')<\/code><\/p>\n<p>Another solution (until we fix it) would be to do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>task.connect(model_params, 'model_params')\nmodel_params[&quot;class_weights&quot;] = {\n0: model_params[&quot;class_weights&quot;].get(&quot;0&quot;, model_params[&quot;class_weights&quot;].get(0))\n1: model_params[&quot;class_weights&quot;].get(&quot;1&quot;, model_params[&quot;class_weights&quot;].get(1))\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1659991938292,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73279794",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":56768436,
        "Question_title":"How to Backup\/Restore TRAINS-server when moving from AMI to local machine",
        "Question_body":"<p>I recently started using TRAINS, with the server in AWS AMI. We are currently using v0.9.0.<\/p>\n\n<p>I would like to move the TRAINS-server to run on our on-premises kubernetes cluster. However, I don't want to lose the data on the current server in AWS (experiments, models, logins, etc...).\nIs there a way to backup the current server and restore it to the local server?<\/p>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1561537653917,
        "Question_score":1,
        "Question_tags":"kubernetes|deep-learning|trains|clearml",
        "Question_view_count":2703,
        "Owner_creation_time":1478632397167,
        "Owner_last_access_time":1561548101617,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1609861259227,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56768436",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":61642549,
        "Question_title":"How to disable crash output in comet ml?",
        "Question_body":"<p>Every time my script is crashing I have such output:<\/p>\n\n<pre><code>COMET INFO:     sys.cpu.percent.43       : (0.0, 0.0)\nCOMET INFO:     sys.cpu.percent.44       : (1.8, 1.8)\nCOMET INFO:     sys.cpu.percent.avg      : (6.579545454545454, 6.579545454545454)\nCOMET INFO:     sys.gpu.0.free_memory    : (34089664512.0, 34089664512.0)\nCOMET INFO:     sys.gpu.0.gpu_utilization: (0.0, 0.0)\nCOMET INFO:     sys.gpu.0.total_memory   : (34089730048.0, 34089730048.0)\nCOMET INFO:     sys.gpu.0.used_memory    : (65536.0, 65536.0)\nCOMET INFO:     sys.load.avg             : (39.42, 39.42)\nCOMET INFO:     sys.ram.total            : (1621711745024.0, 1621711745024.0)\nCOMET INFO:     sys.ram.used             : (78552326144.0, 78552326144.0)\nCOMET INFO:   Other [count]:\nCOMET INFO:     offline_experiment: True\n<\/code><\/pre>\n\n<p>How can I disable it?<\/p>\n\n<p>In comet-ml docs I found but probably it's not what I look for:<\/p>\n\n<blockquote>\n  <p>by setting the environmental variable COMET_DISABLE_AUTO_LOGGING to 1<\/p>\n<\/blockquote>\n\n<pre><code>$ export COMET_DISABLE_AUTO_LOGGING=1                                                                                             \n$ echo $COMET_DISABLE_AUTO_LOGGING \n1\n<\/code><\/pre>\n\n<p>But it didn't help me.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1588789562853,
        "Question_score":1,
        "Question_tags":"python|pytorch|comet|comet-ml",
        "Question_view_count":94,
        "Owner_creation_time":1446931610417,
        "Owner_last_access_time":1662985332847,
        "Owner_location":"Moscow, \u0420\u043e\u0441\u0441\u0438\u044f",
        "Owner_reputation":7659,
        "Owner_up_votes":395,
        "Owner_down_votes":3,
        "Owner_views":776,
        "Question_last_edit_time":1588796193240,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61642549",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":56881430,
        "Question_title":"Is it possible to visualize gradients in comet-ml?",
        "Question_body":"<p>It is straightforward to set up TensorBoard in Keras (it's just a <a href=\"https:\/\/keras.io\/callbacks\/#tensorboard\" rel=\"noreferrer\">callback<\/a>!) and then it is possible to visualize the distribution and magnitude of the weights and gradients. Is it possible to do the same with <a href=\"https:\/\/www.comet.ml\" rel=\"noreferrer\">comet.ml<\/a>? Comet.ml is easy to set up, but visualizes only the loss and accuracy evolution... Is there a way to \"restore\" all the TensorBoard features in comet.ml?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1562219704473,
        "Question_score":6,
        "Question_tags":"python-3.x|tensorflow|keras|tf.keras|comet-ml",
        "Question_view_count":117,
        "Owner_creation_time":1557443756590,
        "Owner_last_access_time":1664031547383,
        "Owner_location":null,
        "Owner_reputation":606,
        "Owner_up_votes":589,
        "Owner_down_votes":211,
        "Owner_views":91,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56881430",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":69642600,
        "Question_title":"Conda UnsatisfiableError of glibc when trying to install Comet ML 3.18 in Dockerfile",
        "Question_body":"<p>I'm trying to install Comet ML version <code>3.18<\/code> in a Dockerfile using Anaconda but the build fails with the following error:<\/p>\n<pre><code>Found conflicts! Looking for incompatible packages.\nThis can take several minutes.  Press CTRL-C to abort.\nfailed\n\nUnsatisfiableError: The following specifications were found to be incompatible with each other:\n\nOutput in format: Requested package -&gt; Available versionsThe following specifications were found to be incompatible with your system:\n\n  - feature:\/linux-64::__glibc==2.31=0\n  - python=3.8 -&gt; libgcc-ng[version='&gt;=7.5.0'] -&gt; __glibc[version='&gt;=2.17']\n\nYour installed version is: 2.31\n<\/code><\/pre>\n<p>Isn't my installed version exactly what is needed according to these lines?! I've googled this issue over and over again but couldn't find a solution that's why I'm now asking it here. Sorry if this is a duplicate but it's driving me nuts! (Had a similar issue trying to install Comet ML on the cluster directly using Anaconda - without Docker - and there the error was with <code>glibc 2.17<\/code>.)<\/p>\n<p>Any help is appreciated!<\/p>\n<hr \/>\n<p>Here's the <strong>full Dockerfile<\/strong>:<\/p>\n<pre><code>FROM continuumio\/anaconda3:latest\n\nRUN conda install -c comet_ml comet_ml=3.18 -y\n\nENTRYPOINT [ &quot;\/bin\/bash&quot; ]\n<\/code><\/pre>\n<p>I also have a version where I install Anaconda manually in Ubuntu 20.04 but it throws the same error and I thought a more concise Dockerfile would be nicer.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1634718135840,
        "Question_score":1,
        "Question_tags":"python|docker|glibc|anaconda3|comet-ml",
        "Question_view_count":499,
        "Owner_creation_time":1389012600070,
        "Owner_last_access_time":1663689374297,
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":3054,
        "Owner_up_votes":299,
        "Owner_down_votes":4,
        "Owner_views":487,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69642600",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":47855236,
        "Question_title":"How to configure comet (comet.ml) to create pull requests on GitHub?",
        "Question_body":"<p>Ive followed this this to link my comet.ml project to GitHub - <a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\/blob\/master\/github-pullrequest\/README.md\" rel=\"nofollow noreferrer\">link<\/a><\/p>\n\n<p>and had some models already trained (using keras)in my project<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/L44Bi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/L44Bi.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Ive linked my GitHub account and when creating a pull request I get error <\/p>\n\n<p><strong>Cant create pull request<\/strong><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/zlkYy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zlkYy.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>please advise<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1513514834923,
        "Question_score":1,
        "Question_tags":"github|keras|comet-ml",
        "Question_view_count":73,
        "Owner_creation_time":1389089640833,
        "Owner_last_access_time":1513514332643,
        "Owner_location":null,
        "Owner_reputation":303,
        "Owner_up_votes":35,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47855236",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":60839122,
        "Question_title":"ModuleNotFoundError: No module named 'comet_ml'",
        "Question_body":"<p>I am working in a conda environment. If i do<\/p>\n\n<p>(pytorch_env)  sudo pip3 freeze | grep comet<\/p>\n\n<p>I get<\/p>\n\n<p>comet-git-pure==0.19.15\ncomet-ml==3.1.3<\/p>\n\n<p>But on running the python file I get<\/p>\n\n<p>pytorch_env) ubuntu@ Traceback (most recent call last):\n  File \"vgg_11.py\", line 5, in \n    from comet_ml import Experiment \nModuleNotFoundError: No module named 'comet_ml'<\/p>\n\n<p>The code is just this <\/p>\n\n<pre><code>from comet_ml import Experiment\nfrom comet_ml.utils import ConfusionMatrix\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1585083094440,
        "Question_score":1,
        "Question_tags":"python-3.x|comet-ml",
        "Question_view_count":507,
        "Owner_creation_time":1458546810273,
        "Owner_last_access_time":1655861032703,
        "Owner_location":null,
        "Owner_reputation":113,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1585083746283,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60839122",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":46368389,
        "Question_title":"How to configure comet (comet.ml) to log Tensorflow?",
        "Question_body":"<p>Im trying to set comet (<a href=\"https:\/\/www.comet.ml\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml<\/a>) to track my Tensorflow experiment, after I create an Experiment and log the data set i dont get the accuracy in my report.<\/p>\n\n<p>my code:<\/p>\n\n<pre><code>mnist = get_data()\ntrain_step, cross_entropy, accuracy, x, y, y_ = build_model_graph(hyper_params)\n\nexperiment = Experiment(api_key=\"XXXX\", log_code=True)\nexperiment.log_multiple_params(hyper_params)\nexperiment.log_dataset_hash(mnist)\n<\/code><\/pre>\n\n<p>in the example account : <a href=\"https:\/\/www.comet.ml\/view\/Jon-Snow\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml\/view\/Jon-Snow<\/a> I see that accuracy is reported<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1506094686057,
        "Question_score":3,
        "Question_tags":"tensorflow|machine-learning|comet-ml",
        "Question_view_count":338,
        "Owner_creation_time":1506066897167,
        "Owner_last_access_time":1506108624433,
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1514341154200,
        "Answer_body":"<p>you can report accuracy using this method:<\/p>\n\n<ul>\n<li><code>experiment.log_accuracy(train_accuracy)<\/code><\/li>\n<\/ul>\n\n<p>take a look at the full Tensorflow example in our guide:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\/tree\/master\/tensorflow\" rel=\"nofollow noreferrer\">https:\/\/github.com\/comet-ml\/comet-quickstart-guide\/tree\/master\/tensorflow<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1506100257932,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1513514205487,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46368389",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":61239274,
        "Question_title":"Google Colab freezes my browser and pc when trying to reconnect to a notebook",
        "Question_body":"<p>I am training a Machine learning model in google colab, to be more specific I am training a GAN with PyTorch-lightning. The problem occurs is when I get disconnected from my current runtime due to inactivity. When I try to reconnect my Browser(tried on firefox and chrome) becomes first laggy and than freezes, my pc starts to lag so that I am not able to close my browser and it doesn't go away. I am forced to press the power button of my PC in order to restart the PC.\nI have no clue why this happens.\nI tried various batch sizes(also the size 1) but it still happens. It can't be that my dataset is too big either(since i tried it on a dataset with 10images for testing puposes).\nI hope someone can help me.<\/p>\n\n<p>Here is my code (For using the code you will need comet.nl and enter the comet.ml api key):<\/p>\n\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision  \nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\n\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport pytorch_lightning as pl\nfrom pytorch_lightning import loggers\n\nimport numpy as np\nfrom numpy.random import choice\n\nfrom PIL import Image\n\nimport os\nfrom pathlib import Path\nimport shutil\n\nfrom collections import OrderedDict\n\n# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n# randomly flip some labels\ndef noisy_labels(y, p_flip=0.05):  # # flip labels with 5% probability\n    # determine the number of labels to flip\n    n_select = int(p_flip * y.shape[0])\n    # choose labels to flip\n    flip_ix = choice([i for i in range(y.shape[0])], size=n_select)\n    # invert the labels in place\n    y[flip_ix] = 1 - y[flip_ix]\n    return y\n\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0.0, std=0.1):\n        self.std = std\n        self.mean = mean\n\n    def __call__(self, tensor):\n        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\ndef get_valid_labels(img):\n  return (0.8 - 1.1) * torch.rand(img.shape[0], 1, 1, 1) + 1.1  # soft labels\n\ndef get_unvalid_labels(img):\n  return noisy_labels((0.0 - 0.3) * torch.rand(img.shape[0], 1, 1, 1) + 0.3)  # soft labels\n\nclass Generator(nn.Module):\n    def __init__(self, ngf, nc, latent_dim):\n        super(Generator, self).__init__()\n        self.ngf = ngf\n        self.latent_dim = latent_dim\n        self.nc = nc\n\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d(latent_dim, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n             nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass Discriminator(nn.Module):\n    def __init__(self, ndf, nc):\n        super(Discriminator, self).__init__()\n        self.nc = nc\n        self.ndf = ndf\n\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass DCGAN(pl.LightningModule):\n\n    def __init__(self, hparams, logger, checkpoint_folder, experiment_name):\n        super().__init__()\n        self.hparams = hparams\n        self.logger = logger  # only compatible with comet_logger at the moment\n        self.checkpoint_folder = checkpoint_folder\n        self.experiment_name = experiment_name\n\n        # networks\n        self.generator = Generator(ngf=hparams.ngf, nc=hparams.nc, latent_dim=hparams.latent_dim)\n        self.discriminator = Discriminator(ndf=hparams.ndf, nc=hparams.nc)\n        self.generator.apply(weights_init)\n        self.discriminator.apply(weights_init)\n\n        # cache for generated images\n        self.generated_imgs = None\n        self.last_imgs = None\n\n        # For experience replay\n        self.exp_replay_dis = torch.tensor([])\n\n        # creating checkpoint folder\n        dirpath = Path(self.checkpoint_folder)\n        if not dirpath.exists():\n          os.makedirs(dirpath, 0o755)\n\n    def forward(self, z):\n        return self.generator(z)\n\n    def adversarial_loss(self, y_hat, y):\n        return F.binary_cross_entropy(y_hat, y)\n\n    def training_step(self, batch, batch_nb, optimizer_idx):\n        # For adding Instance noise for more visit: https:\/\/www.inference.vc\/instance-noise-a-trick-for-stabilising-gan-training\/\n        std_gaussian = max(0, self.hparams.level_of_noise - ((self.hparams.level_of_noise * 1.5) * (self.current_epoch \/ self.hparams.epochs)))\n        AddGaussianNoiseInst = AddGaussianNoise(std=std_gaussian) # the noise decays over time\n\n        imgs, _ = batch\n        imgs = AddGaussianNoiseInst(imgs) # Adding instance noise to real images\n        self.last_imgs = imgs\n\n        # train generator\n        if optimizer_idx == 0:\n            # sample noise\n            z = torch.randn(imgs.shape[0], self.hparams.latent_dim, 1, 1)\n\n            # generate images\n            self.generated_imgs = self(z)\n            self.generated_imgs = AddGaussianNoiseInst(self.generated_imgs) # Adding instance noise to fake images\n\n            # Experience replay\n            # for discriminator\n            perm = torch.randperm(self.generated_imgs.size(0))  # Shuffeling\n            r_idx = perm[:max(1, self.hparams.experience_save_per_batch)]  # Getting the index\n            self.exp_replay_dis = torch.cat((self.exp_replay_dis, self.generated_imgs[r_idx]), 0).detach()  # Add our new example to the replay buffer\n\n            # ground truth result (ie: all fake)\n            g_loss = self.adversarial_loss(self.discriminator(self.generated_imgs), get_valid_labels(self.generated_imgs)) # adversarial loss is binary cross-entropy\n\n            tqdm_dict = {'g_loss': g_loss}\n            log = {'g_loss': g_loss, \"std_gaussian\": std_gaussian}\n            output = OrderedDict({\n                'loss': g_loss,\n                'progress_bar': tqdm_dict,\n                'log': log\n            })\n            return output\n\n        # train discriminator\n        if optimizer_idx == 1:\n            # Measure discriminator's ability to classify real from generated samples\n            # how well can it label as real?\n            real_loss = self.adversarial_loss(self.discriminator(imgs), get_valid_labels(imgs))\n\n            # Experience replay\n            if self.exp_replay_dis.size(0) &gt;= self.hparams.experience_batch_size:\n              fake_loss = self.adversarial_loss(self.discriminator(self.exp_replay_dis.detach()), get_unvalid_labels(self.exp_replay_dis))  # train on already seen images\n\n              self.exp_replay_dis = torch.tensor([]) # Reset experience replay\n\n              # discriminator loss is the average of these\n              d_loss = (real_loss + fake_loss) \/ 2\n\n              tqdm_dict = {'d_loss': d_loss}\n              log = {'d_loss': d_loss, \"d_exp_loss\": fake_loss, \"std_gaussian\": std_gaussian}\n              output = OrderedDict({\n                  'loss': d_loss,\n                  'progress_bar': tqdm_dict,\n                  'log': log\n              })\n              return output\n\n            else:\n              fake_loss = self.adversarial_loss(self.discriminator(self.generated_imgs.detach()), get_unvalid_labels(self.generated_imgs))  # how well can it label as fake?\n\n              # discriminator loss is the average of these\n              d_loss = (real_loss + fake_loss) \/ 2\n\n              tqdm_dict = {'d_loss': d_loss}\n              log = {'d_loss': d_loss, \"std_gaussian\": std_gaussian}\n              output = OrderedDict({\n                  'loss': d_loss,\n                  'progress_bar': tqdm_dict,\n                  'log': log\n              })\n              return output\n\n    def configure_optimizers(self):\n        lr = self.hparams.lr\n        b1 = self.hparams.b1\n        b2 = self.hparams.b2\n\n        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n        return [opt_g, opt_d], []\n\n    def train_dataloader(self):\n        transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize([0.5], [0.5])])\n        dataset = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n        return DataLoader(dataset, batch_size=self.hparams.batch_size)\n        # transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n        #                                 transforms.ToTensor(),\n        #                                 transforms.Normalize([0.5], [0.5])\n        #                                 ])\n\n        # train_dataset = torchvision.datasets.ImageFolder(\n        #     root=\".\/drive\/My Drive\/datasets\/ghibli_dataset_small_overfit\/\",\n        #     transform=transform\n        # )\n        # return DataLoader(train_dataset, num_workers=self.hparams.num_workers, shuffle=True, batch_size=self.hparams.batch_size)\n\n    def on_epoch_end(self):\n        z = torch.randn(4, self.hparams.latent_dim, 1, 1)\n        # match gpu device (or keep as cpu)\n        if self.on_gpu:\n            z = z.cuda(self.last_imgs.device.index)\n\n        # log sampled images\n        sample_imgs = self.generator(z)\n        sample_imgs = sample_imgs.view(-1, self.hparams.nc, self.hparams.image_size, self.hparams.image_size)\n        grid = torchvision.utils.make_grid(sample_imgs, nrow=2)\n        self.logger.experiment.log_image(grid.permute(1, 2, 0), f'generated_images_epoch{self.current_epoch}', step=self.current_epoch)\n\n        # save model\n        if self.current_epoch % self.hparams.save_model_every_epoch == 0:\n          trainer.save_checkpoint(self.checkpoint_folder + \"\/\" + self.experiment_name + \"_epoch_\" + str(self.current_epoch) + \".ckpt\")\n          comet_logger.experiment.log_asset_folder(self.checkpoint_folder, step=self.current_epoch)\n\n          # Deleting the folder where we saved the model so that we dont upload a thing twice\n          dirpath = Path(self.checkpoint_folder)\n          if dirpath.exists() and dirpath.is_dir():\n                shutil.rmtree(dirpath)\n\n          # creating checkpoint folder\n          access_rights = 0o755\n          os.makedirs(dirpath, access_rights)\n\nfrom argparse import Namespace\n\nargs = {\n    'batch_size': 48,\n    'lr': 0.0002,\n    'b1': 0.5,\n    'b2': 0.999,\n    'latent_dim': 128, # tested value which worked(in V4_1): 100\n    'nc': 1,\n    'ndf': 32,\n    'ngf': 32,\n    'epochs': 10,\n    'save_model_every_epoch': 5,\n    'image_size': 64,\n    'num_workers': 2,\n    'level_of_noise': 0.15,\n    'experience_save_per_batch': 1, # this value should be very low; tested value which works: 1\n    'experience_batch_size': 50 # this value shouldnt be too high; tested value which works: 50\n}\nhparams = Namespace(**args)\n\n# Parameters\nexperiment_name = \"DCGAN_V4_2_MNIST\"\ndataset_name = \"MNIST\"\ncheckpoint_folder = \"DCGAN\/\"\ntags = [\"DCGAN\", \"MNIST\", \"OVERFIT\", \"64x64\"]\ndirpath = Path(checkpoint_folder)\n\n# init logger\ncomet_logger = loggers.CometLogger(\n    api_key=\"\",\n    rest_api_key=\"\",\n    project_name=\"gan\",\n    experiment_name=experiment_name,\n    #experiment_key=\"f23d00c0fe3448ee884bfbe3fc3923fd\"  # used for resuming trained id can be found in comet.ml\n)\n\n#defining net\nnet = DCGAN(hparams, comet_logger, checkpoint_folder, experiment_name)\n\n#logging\ncomet_logger.experiment.set_model_graph(str(net))\ncomet_logger.experiment.add_tags(tags=tags)\ncomet_logger.experiment.log_dataset_info(dataset_name)\n\ntrainer = pl.Trainer(#resume_from_checkpoint=\"GHIBLI_DCGAN_OVERFIT_64px_epoch_6000.ckpt\",\n                     logger=comet_logger,\n                     max_epochs=args[\"epochs\"]\n                     )\ntrainer.fit(net)\ncomet_logger.experiment.end()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586987562523,
        "Question_score":0,
        "Question_tags":"crash|google-colaboratory|freeze|comet-ml",
        "Question_view_count":1013,
        "Owner_creation_time":1493314794173,
        "Owner_last_access_time":1658745180780,
        "Owner_location":"Germany",
        "Owner_reputation":844,
        "Owner_up_votes":241,
        "Owner_down_votes":6,
        "Owner_views":170,
        "Question_last_edit_time":1587130797009,
        "Answer_body":"<p>I fixed it with importing this:<\/p>\n\n<pre><code>from IPython.display import clear_output \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1587034953183,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61239274",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":46359436,
        "Question_title":"How to configure comet (comet.ml) to track Keras?",
        "Question_body":"<p>im trying to setup <a href=\"https:\/\/www.comet.ml\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml<\/a> to log my experiment details <\/p>\n\n<p>getting strange error:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"train.py\", line 7, in &lt;module&gt;\n    from comet_ml import Experiment\nImportError: No module named comet_ml\n<\/code><\/pre>\n\n<p>trying in python 2 and python3<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1506066265147,
        "Question_score":3,
        "Question_tags":"python|keras|comet|comet-ml",
        "Question_view_count":1208,
        "Owner_creation_time":1505841491573,
        "Owner_last_access_time":1506108675090,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1506066568087,
        "Answer_body":"<p>it seems like comet isn't installed on your machine.<\/p>\n\n<p>you can use :<\/p>\n\n<pre><code>pip3 install comet_ml\npip install comet_ml\n<\/code><\/pre>\n\n<p>take a look at the example projects at: <\/p>\n\n<p><a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\" rel=\"nofollow noreferrer\">https:\/\/github.com\/comet-ml\/comet-quickstart-guide<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1506066553020,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1513514919392,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46359436",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":46352435,
        "Question_title":"comet (comet-ml) fails to run with Keras",
        "Question_body":"<p>Im running the keras examples from <a href=\"https:\/\/github.com\/comet-ml\/comet-keras-example\/blob\/master\/comet_keras_example.py\" rel=\"nofollow noreferrer\">Comet github project<\/a> .<\/p>\n\n<p>I add the import and create a new experiment:<\/p>\n\n<pre><code>def train(x_train,y_train,x_test,y_test):\nmodel = build_model_graph()\n\nfrom comet_ml import Experiment\n\nexperiment = Experiment(api_key=\"XXXX\", log_code=True)\n\nmodel.fit(x_train, y_train, batch_size=128, epochs=50, validation_data=(x_test, y_test))\n\nscore = model.evaluate(x_test, y_test, verbose=0)\n<\/code><\/pre>\n\n<p>and when i run my training code it fails.<\/p>\n\n<p>error:<\/p>\n\n<pre><code>Using TensorFlow backend.\nTraceback (most recent call last):\n  File \"\/Users\/nimrodlahav\/Code\/semantica\/experiment-logger-client\/train-examples\/keras-example.py\", line 21, in &lt;module&gt;\n    from comet_ml import Experiment\n  File \"..\/.\/comet-client-lib\/comet_ml\/__init__.py\", line 3, in &lt;module&gt;\n    from .comet import Experiment\n  File \"..\/.\/comet-client-lib\/comet_ml\/comet.py\", line 29, in &lt;module&gt;\n    from comet_ml import keras_logger\n  File \"..\/.\/comet-client-lib\/comet_ml\/keras_logger.py\", line 31, in &lt;module&gt;\n    raise SyntaxError(\"Please import Comet before importing any keras modules\")\nSyntaxError: Please import Comet before importing any keras modules\n<\/code><\/pre>\n\n<p>what am I missing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1506024659377,
        "Question_score":2,
        "Question_tags":"python|tensorflow|keras|comet|comet-ml",
        "Question_view_count":601,
        "Owner_creation_time":1505841491573,
        "Owner_last_access_time":1506108675090,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1506491876827,
        "Answer_body":"<p>I don't see start of the code but it looks like you have imported Keras before you have imported Comet.<\/p>\n\n<p>From the error message it looks like just need to switch the import lines (Comet first Keras second), like in your example:<\/p>\n\n<pre><code>from comet_ml import Experiment\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import RMSprop \n<\/code><\/pre>\n\n<p>view the full source code <a href=\"https:\/\/github.com\/comet-ml\/comet-keras-example\" rel=\"nofollow noreferrer\">example<\/a> .<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1506026797449,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1506066589407,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46352435",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":64640949,
        "Question_title":"How to suppress \"Comet.ml Experiment Summary\" console output in the beginning",
        "Question_body":"<p>I'm using <code>comet.ml<\/code> to display learning results of my machine learning models on <code>jupyter<\/code>.<\/p>\n<p>Initializing an experiment as follows<\/p>\n<pre><code>experiment = Experiment(project_name='...', auto_metric_logging=False)\nexperiment.add_tag('...')\n<\/code><\/pre>\n<p>the following information is shown (with light red color background) in the beginning every time:<\/p>\n<pre><code>COMET INFO: ----------------------------\nCOMET INFO: Comet.ml Experiment Summary:\nCOMET INFO:   Data:\nCOMET INFO:     url: https:\/\/www.comet.ml\/...\nCOMET INFO:   Metrics [count] (min, max):\nCOMET INFO:     loss_D_fake [210]            : (1.4689682722091675, 1.999974250793457)\n... 50+ lines ...\nCOMET INFO:     sys.ram.total [8]            : (269603381248.0, 269603381248.0)\nCOMET INFO:     sys.ram.used [8]             : (60270231552.0, 64912928768.0)\nCOMET INFO:   Uploads:\nCOMET INFO:     git-patch: 1\nCOMET INFO: ----------------------------\nCOMET INFO: Experiment is live on comet.ml https:\/\/www.comet.ml\/...\n<\/code><\/pre>\n<p>All I want to see is the last line that shows the URL for this experiment and that seems not to be included in the 'Comet.ml Experiment Summary' block. How can we suppress other information?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1604299707030,
        "Question_score":0,
        "Question_tags":"python|jupyter|comet-ml",
        "Question_view_count":156,
        "Owner_creation_time":1469364547800,
        "Owner_last_access_time":1663732990757,
        "Owner_location":"Kyoto Prefecture, Japan",
        "Owner_reputation":343,
        "Owner_up_votes":219,
        "Owner_down_votes":15,
        "Owner_views":60,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64640949",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":73435172,
        "Question_title":"Is there any way to log 'git hash' in hydra?",
        "Question_body":"<p>I want to control version of experiment configuration files with hydra and dvc without uploading original config files to git.<br \/>\nHydra does control config, and dvc controls version. But Hydra does not specify which 'code version' is needed to reproduce experiment. And I don't want to add 'git hash logging code' in every experiments.<\/p>\n<p>Is there any way to log git hash to hydra log in default? thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661090494277,
        "Question_score":1,
        "Question_tags":"git|fb-hydra|dvc",
        "Question_view_count":43,
        "Owner_creation_time":1603378831587,
        "Owner_last_access_time":1663547314893,
        "Owner_location":null,
        "Owner_reputation":161,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Good timing! A DVC-Hydra integration is in development. You can see the proposal in <a href=\"https:\/\/github.com\/iterative\/dvc\/discussions\/7044#discussioncomment-3271855\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/discussions\/7044#discussioncomment-3271855<\/a> and the development progress in <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/8093\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/pull\/8093<\/a>. This should allow you to take a Hydra config, pass your Hydra overrides via <code>dvc exp run --set-param=&lt;hydra_overrides&gt;<\/code>, and capture the output with DVC.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1661173080163,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73435172",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73602927,
        "Question_title":"Best method to compress large dataset features",
        "Question_body":"<p>10TB featurs which stored on aws s3 bucket.\nin order to decrease costs , we are looking for a loseless compression method for those features (current features are images but it can changed between projects)<\/p>\n<p>I heard about hdf5, but is it the best method nowadays?\nCan the the extraction be paralleled?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1662326892390,
        "Question_score":1,
        "Question_tags":"python|compression|hdf5|dvc",
        "Question_view_count":37,
        "Owner_creation_time":1573845661880,
        "Owner_last_access_time":1664058289083,
        "Owner_location":"Israel",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73602927",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":61386810,
        "Question_title":"Data version control (DVC) edit files in place results in cyclic dependency",
        "Question_body":"<p>we have a larger dataset and have several preprocessing scripts.\nThese scripts alter data in place.\nIt seems when I try to register it with <code>dvc run<\/code> it complains about cyclic dependencies (input is the same as output).\nI would assume this is a very common use case.<\/p>\n\n<p>What is the best practice here ?<\/p>\n\n<p>Tried to google around but i did not see any solution to this (besides creating another folder for the output).<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1587643471830,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":45,
        "Owner_creation_time":1530036936153,
        "Owner_last_access_time":1656059958773,
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":121,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61386810",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67505910,
        "Question_title":"Problem running a Docker container in Gitlab CI\/CD",
        "Question_body":"<p>I am trying to build and run my Docker image using Gitlab CI\/CD, but there is one issue I can't fix even though locally everything works well.<\/p>\n<p>Here's my Dockerfile:<\/p>\n<pre><code>FROM &lt;internal_docker_repo_image&gt;\nRUN apt update &amp;&amp; \\\n    apt install --no-install-recommends -y build-essential gcc\nCOPY requirements.txt \/requirements.txt\n\nRUN pip install --no-cache-dir --user -r \/requirements.txt\n\nCOPY . \/src\nWORKDIR \/src\nENTRYPOINT [&quot;python&quot;, &quot;-m&quot;, &quot;dvc&quot;, &quot;repro&quot;]\n<\/code><\/pre>\n<p>This is how I run the container:<\/p>\n<p><code>docker run --volume ${PWD}:\/src --env=GOOGLE_APPLICATION_CREDENTIALS=&lt;path_to_json&gt; &lt;image_name&gt; .\/dvc_configs\/free\/dvc.yaml --force<\/code><\/p>\n<p>Everything works great when running this locally, but it fails when run on Gitlab CI\/CD.<\/p>\n<pre><code>stages:\n  - build_image\n\nbuild_image:\n  stage: build_image\n  image: &lt;internal_docker_repo_image&gt;\n  script:\n    - echo &quot;Building Docker image...&quot;\n    - mkdir ~\/.docker\n    - cat $GOOGLE_CREDENTIALS &gt; ${CI_PROJECT_DIR}\/key.json\n    - docker build . -t &lt;image_name&gt;\n    - docker run --volume ${PWD}:\/src --env=GOOGLE_APPLICATION_CREDENTIALS=&lt;path_to_json&gt; &lt;image_name&gt; .\/dvc_configs\/free\/dvc.yaml --force\n  artifacts:\n        paths:\n          - &quot;.\/data\/*csv&quot;\n        expire_in: 1 week\n\n<\/code><\/pre>\n<p>This results in the following error:\n<code>ERROR: you are not inside of a DVC repository (checked up to mount point '\/src')<\/code><\/p>\n<p>Just in case you don't know what DVC is, this is a tool used in machine learning for versioning your models, datasets, metrics, and, in addition, setting up your pipelines, which I use it for in my case.<\/p>\n<p>Essentially, it requires two folders <code>.dvc<\/code> and <code>.git<\/code> in the directory from which <code>dvc repro<\/code> is executed.<\/p>\n<p>In this particular case, I have no idea why it's not able to run this command given that the contents of the folders are exactly the same and both <code>.dvc<\/code> and <code>.git<\/code> exist.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620830695847,
        "Question_score":4,
        "Question_tags":"docker|continuous-integration|gitlab-ci|dvc",
        "Question_view_count":746,
        "Owner_creation_time":1452169170337,
        "Owner_last_access_time":1663996681010,
        "Owner_location":null,
        "Owner_reputation":576,
        "Owner_up_votes":431,
        "Owner_down_votes":4,
        "Owner_views":68,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67505910",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":62529867,
        "Question_title":"Data version control (DVC) commands not working ---> TypeError: public() got an unexpected keyword argument 'SEP'",
        "Question_body":"<p>All of a sudden, dvc has stopped functioning.\nAny command typed fails and throws an exception.\nexample. dvc remote list results in -<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/home\/dev2\/.local\/bin\/dvc&quot;, line 5, in &lt;module&gt;\n    from dvc.main import main\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/dvc\/main.py&quot;, line 6, in &lt;module&gt;\n    from dvc import analytics\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/dvc\/analytics.py&quot;, line 16, in &lt;module&gt;\n    from dvc.lock import Lock, LockError\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/dvc\/lock.py&quot;, line 8, in &lt;module&gt;\n    import flufl.lock\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/flufl\/lock\/__init__.py&quot;, line 3, in &lt;module&gt;\n    from flufl.lock._lockfile import (\n  File &quot;\/home\/dev2\/.local\/lib\/python3.6\/site-packages\/flufl\/lock\/_lockfile.py&quot;, line 54, in &lt;module&gt;\n    public(SEP=SEP)\nTypeError: public() got an unexpected keyword argument 'SEP'\n \n<\/code><\/pre>\n<p>Any suggestions will be of great help.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1592898526370,
        "Question_score":2,
        "Question_tags":"linux|git|dvc",
        "Question_view_count":307,
        "Owner_creation_time":1534165472313,
        "Owner_last_access_time":1663626263697,
        "Owner_location":null,
        "Owner_reputation":449,
        "Owner_up_votes":3,
        "Owner_down_votes":2,
        "Owner_views":80,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62529867",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70478173,
        "Question_title":"How to track the big data stored in Gdrive through DVC?",
        "Question_body":"<p>I am currently working on the ML project and the data size is around 10 GB. The data I stored in google drive. Its impossible for me to download it on my local machine. So, how to use the DVC (data version control) to track that data? Thank you in advance for your time.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1640410347580,
        "Question_score":2,
        "Question_tags":"machine-learning|version-control|mlops|dvc",
        "Question_view_count":47,
        "Owner_creation_time":1567192416523,
        "Owner_last_access_time":1663279611567,
        "Owner_location":null,
        "Owner_reputation":85,
        "Owner_up_votes":76,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1640412287832,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70478173",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67393339,
        "Question_title":"dvc push, change the names of files on the remote storage",
        "Question_body":"<p>I'm working on a project with DVC (Data Version Control), when I push files in my remote storage, the name of the files are changed. How I can conserve the names?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620170453717,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":447,
        "Owner_creation_time":1379685720450,
        "Owner_last_access_time":1663797728603,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1620171022369,
        "Answer_body":"<p>Short answer: there is no way to do that.<\/p>\n<p>Long answer:\nDvc remote is a content-based storage, so names are not preserved. Dvc creates metafiles (*.dvc files) in your workspace that contain names and those files are usually tracked by git, so you need to use git remote and dvc remote together to have both filenames and their contents. Here is a more detailed explanation about the format of local and remote storage <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory<\/a> . Also, checkout <a href=\"https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620171651067,
        "Answer_score":5.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67393339",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73084915,
        "Question_title":"How to fix DVC error 'FileNotFoundError: [Errno 2] No such file or directory' in Github actions",
        "Question_body":"<p>Trying to pull a folder with test data into a GitHub actions container, I get<\/p>\n<blockquote>\n<p>FileNotFoundError: [Errno 2] No such file or directory<\/p>\n<\/blockquote>\n<p>I tried running <code>dvc checkout --relink<\/code> locally, but that did not work. I am using Gdrive for the data-repository with a service account. It seems the login works. But strangely the file is not present. Maybe I can push the files again somehow and recreate the data in the repository?<\/p>\n<hr \/>\n<h2><code>dvc doctor<\/code> output:<\/h2>\n<pre><code>DVC version: 2.10.2 (pip)\n---------------------------------\nPlatform: Python 3.7.13 on Linux-5.15.0-1014-azure-x86_64-with-debian-bullseye-sid\nSupports:\n    gdrive (pydrive2 = 1.14.0),\n    webhdfs (fsspec = 2022.5.0),\n    http (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n    https (aiohttp = 3.8.1, aiohttp-retry = 2.5.2)\nCache types: &lt;https:\/\/error.dvc.org\/no-dvc-cache&gt;\nCaches: local\nRemotes: gdrive\nWorkspace directory: ext4 on \/dev\/sda1\nRepo: dvc, git\n<\/code><\/pre>\n<p>I was able to clone the GIT repo and pull the data from the DVC data-registry, however, it did not work from GH.<\/p>\n<hr \/>\n<h2>Full traceback<\/h2>\n<pre><code>Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiohttp-retry-2.5.1 aiosignal-1.2.0 appdirs-1.4.4 async-timeout-4.0.2 asynctest-0.13.0 atpublic-2.3 attrs-21.4.0 cached-property-1.5.2 cachetools-4.2.4 certifi-2022.6.15 cffi-1.15.1 charset-normalizer-2.0.12 colorama-0.4.5 commonmark-0.9.1 configobj-5.0.6 contextvars-2.4 cryptography-37.0.4 dataclasses-0.8 decorator-4.4.2 dictdiffer-0.9.0 diskcache-5.4.0 distro-1.7.0 dpath-2.0.6 dulwich-0.20.45 dvc-2.8.1 flatten-dict-0.4.2 flufl.lock-3.2 frozenlist-1.2.0 fsspec-2022.1.0 ftfy-6.0.3 funcy-1.17 future-0.18.2 gitdb-4.0.9 gitpython-3.1.18 google-api-core-2.8.2 google-api-python-client-2.52.0 google-auth-2.9.1 google-auth-httplib2-0.1.0 googleapis-common-protos-1.56.3 grandalf-0.6 httplib2-0.20.4 idna-3.3 idna-ssl-1.1.0 immutables-0.18 importlib-metadata-4.8.3 importlib-resources-5.4.0 mailchecker-4.1.18 multidict-5.2.0 nanotime-0.5.2 networkx-2.5.1 oauth2client-4.1.3 packaging-21.3 pathspec-0.8.1 phonenumbers-8.12.52 ply-3.11 protobuf-3.19.4 psutil-5.9.1 pyOpenSSL-22.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.21 pydot-1.4.2 pydrive2-1.10.1 pygit2-1.6.1 pygments-2.12.0 pygtrie-2.5.0 pyparsing-2.4.7 python-benedict-0.25.2 python-dateutil-2.8.2 python-fsutil-0.6.1 python-slugify-6.1.2 requests-2.27.1 rich-12.5.1 rsa-4.9 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 shortuuid-1.0.9 shtab-1.5.5 six-1.16.0 smmap-5.0.0 tabulate-0.8.10 text-unidecode-1.3 toml-0.10.2 tqdm-4.64.0 typing-extensions-4.1.1 uritemplate-4.1.1 urllib3-1.26.10 voluptuous-0.13.1 wcwidth-0.2.5 xmltodict-0.13.0 yarl-1.7.2 zc.lockfile-2.0 zipp-3.6.0\nDVC version: 2.8.1 (pip)\n---------------------------------\nPlatform: Python 3.6.15 on Linux-5.15.0-1014-azure-x86_64-with-debian-bullseye-sid\nSupports:\n    gdrive (pydrive2 = 1.10.1),\n    webhdfs (fsspec = 2022.1.0),\n    http (aiohttp = 3.8.1, aiohttp-retry = 2.5.1),\n    https (aiohttp = 3.8.1, aiohttp-retry = 2.5.1)\nCache types: &lt;https:\/\/error.dvc.org\/no-dvc-cache&gt;\nCaches: local\nRemotes: gdrive\nWorkspace directory: ext4 on \/dev\/sdb1\nRepo: dvc, git\n2022-07-22 19:01:08,361 DEBUG: failed to pull cache for 'tests\/data'\n2022-07-22 19:01:08,364 WARNING: No file hash info found for 'tests\/data'. It won't be created.\n1 file failed\n2022-07-22 19:01:08,365 ERROR: failed to pull data from the cloud - Checkout failed for following targets:\ntests\/data\nIs your cache up to date?\n&lt;https:\/\/error.dvc.org\/missing-files&gt;\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/command\/data_sync.py&quot;, line 41, in run\n    glob=self.args.glob,\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/repo\/__init__.py&quot;, line 50, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/repo\/pull.py&quot;, line 44, in pull\n    recursive=recursive,\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/repo\/__init__.py&quot;, line 50, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.6.15\/x64\/lib\/python3.6\/site-packages\/dvc\/repo\/checkout.py&quot;, line 110, in checkout\n    raise CheckoutError(stats[&quot;failed&quot;], stats)\ndvc.exceptions.CheckoutError: Checkout failed for following targets:\ntests\/data\nIs your cache up to date?\n&lt;https:\/\/error.dvc.org\/missing-files&gt;\n------------------------------------------------------------\n2022-07-22 19:01:08,369 DEBUG: Analytics is enabled.\n2022-07-22 19:01:08,412 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp6i9uchzh']'\n2022-07-22 19:01:08,413 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp6i9uchzh']'\nError: The operation was canceled.\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":6,
        "Question_creation_time":1658515971860,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":209,
        "Owner_creation_time":1444959339707,
        "Owner_last_access_time":1663889479043,
        "Owner_location":null,
        "Owner_reputation":6513,
        "Owner_up_votes":1296,
        "Owner_down_votes":60,
        "Owner_views":1057,
        "Question_last_edit_time":1658518512243,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73084915",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67421254,
        "Question_title":"How to add a file to a dvc-tracked folder without pulling the whole folder's content?",
        "Question_body":"<p>Let's say I am working inside a git\/dvc repo. There is a folder <code>data<\/code> containing 100k small files. I track it with DVC as a single element, as recommended by the doc:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc add data\n<\/code><\/pre>\n<p>and because in my experience, DVC is kinda slow when tracking that many files one by one.<\/p>\n<p>I clone the repo on another workspace, and now I have the <code>data.dvc<\/code> file locally but none of the actual files inside yet. I want to add a file named <code>newfile.txt<\/code> to the <code>data<\/code> folder and track it with DVC. Is there a way to do this <em>without pulling the whole content of <code>data<\/code> locally<\/em> ?<\/p>\n<p>What I have tried for now:<\/p>\n<ol>\n<li><p>Adding the <code>data<\/code> folder again:<\/p>\n<pre><code>mkdir data\nmv path\/to\/newfile.txt data\/newfile.txt\ndvc add data\n<\/code><\/pre>\n<p>The <code>data.dvc<\/code> file is built again from the local state of <code>data<\/code> which only contains <code>newfile.txt<\/code> so this doesn't work.<\/p>\n<\/li>\n<li><p>Adding the file as a single element in <code>data<\/code> folder:<\/p>\n<pre><code> dvc add data\/newfile.txt\n<\/code><\/pre>\n<p>I get :<\/p>\n<pre><code> Cannot add 'data\/newfile.txt', because it is overlapping with other DVC tracked output: 'data'. \n To include 'data\/newfile.txt' in 'data', run 'dvc commit data.dvc'\n<\/code><\/pre>\n<\/li>\n<li><p>Using dvc commit as suggested<\/p>\n<pre><code> mkdir data\n mv path\/to\/newfile.txt data\/newfile.txt\n dvc commit data.dvc\n<\/code><\/pre>\n<p>Similarly as 1., the <code>data.dvc<\/code> is rebuilt again from local state of <code>data<\/code>.<\/p>\n<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1620314719220,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":1781,
        "Owner_creation_time":1620311720937,
        "Owner_last_access_time":1626773929723,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1620314912032,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67421254",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":68559059,
        "Question_title":"DVC connect to Min.IO to access S3",
        "Question_body":"<p>What is the proper way to connect DVC to Min.IO that is connected to some buckets on S3.<\/p>\n<pre><code>AWS-S3(My_Bucket) &gt; Min.io(MY_Bucket aliased as S3)\n<\/code><\/pre>\n<p>Right now i am accessing my bucket by using mc for example <code>mc cp s3\/my_bucket\/datasets datasets<\/code> to copy stuff from there. But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC so i can use for example <code>&quot;DVC mc-S3 pull&quot;<\/code> and <code>&quot;DVC AWS-S3 pull&quot;<\/code>.<\/p>\n<p>How do i got for it because while googling i couldn't find anything that i could easily follow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1627469557323,
        "Question_score":1,
        "Question_tags":"amazon-s3|minio|dvc",
        "Question_view_count":375,
        "Owner_creation_time":1578574709920,
        "Owner_last_access_time":1648636957603,
        "Owner_location":"Poland",
        "Owner_reputation":85,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It looks like you are looking for a combination of things.<\/p>\n<p>First, Jorge mentioned you can set <code>endpointurl<\/code> to access Minio the same way as you would access regular S3:<\/p>\n<pre><code>dvc remote add -d minio-remote s3:\/\/mybucket\/path\ndvc remote modify minio-remote endpointurl https:\/\/minio.example.com                          \n<\/code><\/pre>\n<p>Second, it seems you can create <em>two<\/em> remotes - one for S3, one for Minio and use <code>-r<\/code> option that is available for many data management related commands:<\/p>\n<pre><code>dvc pull -r minio-remote\ndvc pull -r s3-remote\ndvc push -r minio-remote\n...\n<\/code><\/pre>\n<p>This way you could <code>push<\/code>\/<code>pull<\/code> data to\/from a specific storage.<\/p>\n<blockquote>\n<p>But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC<\/p>\n<\/blockquote>\n<p>There are other possible ways, I think to organize this. It indeed depends on what semantics you expect from <code>DVC mc-S3 pull<\/code>. Please let us know if <code>-r<\/code> is not enough and clarify the question- that would help us here.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_creation_time":1627513406643,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68559059",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72622280,
        "Question_title":"How does one add individual files with DVC?",
        "Question_body":"<p>Suppose I run the following commands:<\/p>\n<pre><code># set up DVC\n\nmkdir foo\ncd foo &amp;&amp; git init\ndvc init\ngit add * &amp;&amp; git commit -m &quot;dvc init&quot;\n\n\n# make a data file\n\nmkdir -p bar\/biz\ntouch bar\/biz\/boz\n\n\n# add the data file\n\ndvc add bar\/biz\/boz\n<\/code><\/pre>\n<p>And DVC outputs the following:<\/p>\n<pre><code>To track the changes with git, run:\n\n  git add bar\/biz\/.gitignore bar\/biz\/boz.dvc\n<\/code><\/pre>\n<hr \/>\n<p>This last part is what I would like to avoid.  Preferably, DVC would only change the top level <code>.gitignore<\/code> (located at the project root, where <code>git init<\/code> was executed), and will change only DVC files at the top level.<\/p>\n<p><strong>And here's why:<\/strong><\/p>\n<p>I have a rather large dataset developed in an original work more or less ad-hoc. This data is not systematically organized, nor do I want to organize it as-is.<\/p>\n<p>Instead, I want to incrementally add this old, bespoke data to the DVC directory tree.  And each time I add some of the data to the tree, I want to check it in with DVC as I would if I were modifying code or mixing one project's code into another.<\/p>\n<p>However, DVC wants to create a local file and gitignore at every location I add.  This creates a mess and I have no reasonable faith that it will be easy to maintain all of these atomic and distributed datastores.<\/p>\n<hr \/>\n<p><strong>The question:<\/strong><\/p>\n<p>What is the preferred way to incrementally add data in DVC so that DVC uses the root gitignore and root DVC files\/items?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655234204993,
        "Question_score":2,
        "Question_tags":"git|dvc",
        "Question_view_count":112,
        "Owner_creation_time":1405262190020,
        "Owner_last_access_time":1663968225423,
        "Owner_location":"Atlanta, GA",
        "Owner_reputation":26244,
        "Owner_up_votes":434,
        "Owner_down_votes":35,
        "Owner_views":1383,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Assuming bar\/ is the dataset directory you're incrementally adding to, you can instead<\/p>\n<pre><code>dvc add bar\n<\/code><\/pre>\n<p>This creates a bar.dvc file and writes to .gitignore at the top level.<\/p>\n<p>When you update content in bar\/, <code>dvc add<\/code> it again or use <code>dvc commit<\/code> to register the new dataset version. The new files get added to the project cache and the .dvc file gets an updated <code>md5<\/code> hash that identifies to the latest directory structure.<\/p>\n<p>Some docs:<br \/>\n<a href=\"https:\/\/dvc.org\/doc\/start\/data-management#making-changes\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/start\/data-management#making-changes<\/a><br \/>\n<a href=\"https:\/\/dvc.org\/doc\/command-reference\/add\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/add<\/a><br \/>\n<a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1655349731160,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1655350036863,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72622280",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72069555,
        "Question_title":"Second dvc push on AWS Batch using IAM role gets \"Unable to locate credentials\"",
        "Question_body":"<p>I'm running a job on AWS Batch, and this job prepares some data and versions it using <code>dvc<\/code>. Secondly, the job does some transformation generating new data, and it should save this new data using <code>dvc<\/code> again. Also, in this case, i'm setting a instance-profile role to enable the AWS Batch to persist on my S3 bucket.<\/p>\n<p>The first <code>dvc push<\/code> works perfectly. But the second one generates the error <code>Unable to locate credentials<\/code><\/p>\n<p>I have also changed the script to just touch a file, add to dvc and push, and then repeat the process in with other file, and could replicate the problem.<\/p>\n<p>I have already solved, changing the command <code>dvc push<\/code> to <code>dvc push especific-file-to-push<\/code>, but I'm now trying to understand what is the problem with <code>dvc push<\/code> command without the parameter specifying the file.<\/p>\n<p>Does anybody know?<\/p>\n<p>I'm using dvc <code>dvc==2.9.5<\/code> and <code>boto3==1.21.21<\/code><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1651330586727,
        "Question_score":2,
        "Question_tags":"amazon-s3|boto3|amazon-iam|aws-batch|dvc",
        "Question_view_count":152,
        "Owner_creation_time":1379631386420,
        "Owner_last_access_time":1663244472720,
        "Owner_location":"Rio de Janeiro, Brazil",
        "Owner_reputation":101,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":1651331434543,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72069555",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72333237,
        "Question_title":"Why I got an invalid bucket name error using dvc mlflow on macos",
        "Question_body":"<p>Could anyone tell what's the reason for error:<\/p>\n<p>botocore.exceptions.ParamValidationError: Parameter validation failed:\nInvalid bucket name &quot;&quot;: Bucket name must match the regex &quot;^[a-zA-Z0-9.-_]{1,255}$&quot; or be an ARN matching the regex &quot;^arn:(aws).<em>:(s3|s3-object-lambda):[a-z-0-9]<\/em>:[0-9]{12}:accesspoint[\/:][a-zA-Z0-9-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z-0-9]+:[0-9]{12}:outpost[\/:][a-zA-Z0-9-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9-]{1,63}$&quot;<\/p>\n<p>I try to use mlflow with docker.\n.env file contains:<\/p>\n<pre><code>AWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\nAWS_S3_BUCKET=vla...rts\nMLFLOW_S3_ENDPOINT_URL=http:\/\/localhost:9000\nMLFLOW_TRACKING_URI=http:\/\/127.0.0.1:5000\nPOSTGRES_USER=...\nPOSTGRES_PASSWORD=...\nPOSTGRES_DB=test_db\n<\/code><\/pre>\n<p>Also tried to use:<\/p>\n<pre><code>AWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\nAWS_S3_BUCKET=vla...rts\nMLFLOW_S3_ENDPOINT_URL=http:\/\/localhost:9000\nMLFLOW_TRACKING_URI=http:\/\/localhost:5000\nPOSTGRES_USER=...\nPOSTGRES_PASSWORD=...\nPOSTGRES_DB=test_db\n<\/code><\/pre>\n<p>docker-compose contains:<\/p>\n<pre><code>... \n   mlflow:\n        restart: always\n        image: mlflow_server\n        container_name: mlflow_server\n        ports:\n          - &quot;5000:5000&quot;\n        networks:\n          - postgres\n          - s3\n        environment:\n          - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\n          - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\n          - MLFLOW_S3_ENDPOINT_URL=http:\/\/nginx:9000\n        command: mlflow server --backend-store-uri postgresql:\/\/${POSTGRES_USER}:${POSTGRES_PASSWORD}@db\/${POSTGRES_DB} --default-artifact-root s3:\/\/${AWS_S3_BUCKET}\/ --host 0.0.0.0\n...\n<\/code><\/pre>\n<p>As I understood, I get an exception cause bucket name is empty (&quot;&quot;). But in .env file I set bucket name as <code>vla...rts<\/code><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1653167453863,
        "Question_score":0,
        "Question_tags":"docker|mlflow|mlops|dvc",
        "Question_view_count":117,
        "Owner_creation_time":1577631947330,
        "Owner_last_access_time":1663676491067,
        "Owner_location":"Saint Petersburg",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":1653372987987,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72333237",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":53213596,
        "Question_title":"How to execute python from conda environment by dvc run",
        "Question_body":"<p>I have an environment of conda configurated with python 3.6 and dvc is installed there, but when I try to execute dvc run with python, dvc call the python version of main installation of conda and not find the installed libraries.<\/p>\n\n<pre><code>$ conda activate py36\n$ python --version\nPython 3.6.6 :: Anaconda custom (64-bit)\n$ dvc run python --version\nRunning command:\n    python --version\nPython 3.7.0\nSaving information to 'Dvcfile'.\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1541699972677,
        "Question_score":6,
        "Question_tags":"python|anaconda|conda|dvc",
        "Question_view_count":351,
        "Owner_creation_time":1420765480790,
        "Owner_last_access_time":1663614206113,
        "Owner_location":null,
        "Owner_reputation":340,
        "Owner_up_votes":477,
        "Owner_down_votes":2,
        "Owner_views":61,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The version 0.24.3 of dvc correct this problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1549414531500,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53213596",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56818930,
        "Question_title":"\"dvc push\" after several local commits",
        "Question_body":"<p>I work on a project with DVC (Data version control). Let's say I make a lot of local commits. Something like this:<\/p>\n\n<pre><code># make changes for experiment 1\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 1\"\n\n# make changes for experiment 2\n# which change both code and data\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 2\"\n\n# make changes for experiment 3\n# which change both code and data\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 3\"\n\n# Finally I'm done\n# push changes:\ndvc push\ngit push\n<\/code><\/pre>\n\n<p>However there is one problem: <code>dvc push<\/code> will only push data from experiment 3. Is there any way to push data from all local commits (i.e. starting from the first commit diverged from remote branch)?<\/p>\n\n<p>Currently I see two options:<\/p>\n\n<ol>\n<li>Tag each commit and push it with <code>dvc push -T<\/code><\/li>\n<li>After \"expermient 3\" commit execute <code>git checkout commit-hash &amp;&amp; dvc push<\/code> for all local commits not yet pushed to remote.<\/li>\n<\/ol>\n\n<p>Both these options seem cumbersome and error-prone. Is there any better way to do it?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1561823734517,
        "Question_score":2,
        "Question_tags":"git|dvc",
        "Question_view_count":916,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":null,
        "Answer_body":"<p>@NShiny, there is a related ticket:<\/p>\n\n<p><a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1691\" rel=\"nofollow noreferrer\">support push\/pull\/metrics\/gc, etc across different commits<\/a>.<\/p>\n\n<p>Please, give it a vote so that we know how to prioritize it.<\/p>\n\n<p>As a workaround, I would recommend to run <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/install\" rel=\"nofollow noreferrer\"><code>dvc install<\/code><\/a>. It installs a <code>pre-push<\/code> GIt hook and runs <code>dvc push<\/code> automatically:<\/p>\n\n<pre><code>Git pre-push hook executes dvc push before git push to upload files and directories under DVC control to remote.\n<\/code><\/pre>\n\n<p>It means, though you need to run <code>git push<\/code> after every <code>git commit<\/code> :(<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1561842649416,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1561847539150,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56818930",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":66505949,
        "Question_title":"problem with dvc import-url from google spreadsheet export",
        "Question_body":"<p>I'm in the process of converting a Makefile-based data workflow to dvc. I have a Google spreadsheet that I'm using in a data workflow to make it easy to update a few things in a makeshift database. Currently this works with something like this:<\/p>\n<pre><code># Makefile\ndata.csv:\n    curl -L https:\/\/docs.google.com\/spreadsheets\/d\/MY-GOOGLE-DOC-ID\/export?exportFormat=csv &gt; data.csv\n<\/code><\/pre>\n<p>Of course, I can incorporate the same step into my dvc pipeline directly with <code>dvc run<\/code>, but my understanding is that something like <code>dvc import-url<\/code> would be more appropriate but I'm getting an error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ poetry run dvc import-url https:\/\/docs.google.com\/spreadsheets\/d\/MY-GOOGLE-DOC-ID\/export?exportFormat=csv data.csv\nImporting 'https:\/\/docs.google.com\/spreadsheets\/d\/MY-GOOGLE-DOC-ID\/export?exportFormat=csv' -&gt; 'data.csv'\nERROR: unexpected error - 'NoneType' object has no attribute 'endswith'\n<\/code><\/pre>\n<p>My guess is that this is because the response data from the Google Spreadsheet export url doesn't have a filename suffix associated with it. Is there a way to work around this problem? Is there a better way to pull data from a google spreadsheet into a dvc workflow?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1615035920157,
        "Question_score":2,
        "Question_tags":"google-sheets|google-sheets-api|dvc",
        "Question_view_count":59,
        "Owner_creation_time":1294268936687,
        "Owner_last_access_time":1661618392827,
        "Owner_location":"Chicago, IL",
        "Owner_reputation":2893,
        "Owner_up_votes":294,
        "Owner_down_votes":1,
        "Owner_views":168,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66505949",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":58163305,
        "Question_title":"dvc gc and files in remote cache",
        "Question_body":"<p>dvc documentation for <code>dvc gc<\/code> command states, the <code>-r<\/code> option indicates \"Remote storage to collect garbage in\" but I'm not sure if I understand it correctly. For example I execute this command:<\/p>\n\n<pre><code>dvc gc -r myremote\n<\/code><\/pre>\n\n<p>What exactly happens if I execute this command? I have 2 possible answers:<\/p>\n\n<ol>\n<li>dvc checks which files should be deleted, then moves these files to \"myremote\" and then deletes all these files in local cache but not in remote.<\/li>\n<li>dvc checks which files should be deleted and deletes these files both in local cache and \"myremote\"<\/li>\n<\/ol>\n\n<p>Which one of them is correct?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1569828438393,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":1617,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":null,
        "Answer_body":"<p>one of DVC maintainers here.<\/p>\n\n<p>Short answer: 2. is correct.<\/p>\n\n<p>A bit of additional information:\nPlease be careful when using <code>dvc gc<\/code>. It will clear your cache from all dependencies that are not mentioned in the current HEAD of your git repository. \nWe are working on making <code>dvc gc<\/code> preserving whole history by default. <\/p>\n\n<p>So if you don't want to delete files from your history commits, it would be better to wait for completion of <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2325\" rel=\"nofollow noreferrer\">this<\/a> task.<\/p>\n\n<p>[EDIT]\nPlease see comment below.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1569833846627,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1569837069043,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58163305",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":61816429,
        "Question_title":"DVC dependencies for derived data without imports",
        "Question_body":"<p>I am new to DVC, and so far I like what I see. But possibly my question is fairly easy to answer.<\/p>\n\n<p><strong>My question:<\/strong> how do we correctly track the dependencies to files in an original hugedatarepo  (lets assume that this can also change) in a derivedData project, but WITHOUT the huge files being imported generally when the derived data is checked out? I don't think I can use <code>dvc import<\/code> to achieve this.<\/p>\n\n<p><strong>Details:<\/strong> We have a repository with a large amount of quite big data files (scans) and use this data to design and train various algorithms. Often we want to use only specific files and even only small chunks from within the files for training, annotation and so on. That is, we derive data for specific tasks, that we want to put in new repositories.<\/p>\n\n<p>Currently my Idea is to <code>dvc get<\/code> the relevant data, put it in a untracked temporary folder and then again manage the derived data with dvc. But still to put in the dependency to the original data.<\/p>\n\n<pre><code>hugeFileRepo\n +metaData.csv\n +dataFolder\n +-- hugeFile_1\n ...\n +-- hugeFile_n\n<\/code><\/pre>\n\n<p>in the derivedData repository I do<\/p>\n\n<pre><code> dvc import hugeFileRepo.git metaData.csv\n dvc run -f derivedData.dvc \\\n    -d metaData.csv \\\n    -d deriveData.py \\\n    -o derivedDataFolder \\\n    python deriveData.py \n<\/code><\/pre>\n\n<p>My deriveData.py does something along the line (pseudocode)<\/p>\n\n<pre><code>metaData = read(metaData.csv)\n\n#Hack because I don't know how to it right:\ngitRevision = getGitRevision(metaData.csv.dvc)          \n...\nfor metaDataForFile, file in metaData:\n   if(iWantFile(metaDataForFile) ):\n      #download specific file\n      !dvc get --rev {gitRevision} -o tempFolder\/{file} hugeFileRepo.git {file}\n\n      #do processing of huge file and store result in derivedDataFolder\n      processAndWrite(tempFolder\/file)\n<\/code><\/pre>\n\n<p>So I use the metaData file as a proxy for the actual data. The hugeFileRepo data will not change frequently and the metaData file will be kept up to date. And I am absolutely fine with having a dependency to the data in general and not to the actual files I used. So I believe this solution would work for me, but I am sure there is a better way.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1589536322927,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":179,
        "Owner_creation_time":1453724533500,
        "Owner_last_access_time":1663059905923,
        "Owner_location":null,
        "Owner_reputation":171,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61816429",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":60527213,
        "Question_title":"How does DVC store differences on the directory level into DVC cache?",
        "Question_body":"<p>Can someone explain how DVC stores differences on the directory level into DVC cache. <\/p>\n\n<p>I understand that the DVC-files (.dvc) are metafiles to track data, models and reproduce pipeline stages. However, it is not clear for me how the process of creating branches, commiting them and switching back to a master file is exactly saved in differences. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1583328485690,
        "Question_score":4,
        "Question_tags":"version-control|dvc",
        "Question_view_count":623,
        "Owner_creation_time":1583308412843,
        "Owner_last_access_time":1587640257240,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1583332201396,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60527213",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70928144,
        "Question_title":"Multiple users in DVC",
        "Question_body":"<p>I would like to ask if it is possible to use DVC with several accounts on the same machine. At the moment, all commands (<code>dvc pull<\/code>, <code>dvc push<\/code>, ...) are executed under my name. But after several people joined this project too, I do not want them to execute commands under my name.<\/p>\n<p>When I was alone on this project I generated ssh key:<\/p>\n<pre><code>ssh-keygen\n<\/code><\/pre>\n<p>Connected to server where DVC remote data is stored:<\/p>\n<pre><code>ssh-copy-id username@server_IP\n<\/code><\/pre>\n<p>Created config file which lets me execute all <code>dvc<\/code> commands using ssh:<\/p>\n<pre><code>[core]\n    remote = storage_server\n['remote &quot;storage_server&quot;']\n    url = ssh:\/\/username@server_IP:\/home\/DVC_remote\/DVC_project\n<\/code><\/pre>\n<p>What I should do so that several people could execute commands on their own name?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643641422880,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":181,
        "Owner_creation_time":1457469301700,
        "Owner_last_access_time":1663223040440,
        "Owner_location":null,
        "Owner_reputation":585,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You need to make the &quot;username&quot; part of the config personalized based on who is running the command. There are a few options to do this (based on <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#available-parameters-per-storage-type\" rel=\"nofollow noreferrer\">this document<\/a>, see the SSH part):<\/p>\n<h2>Basic options are:<\/h2>\n<ul>\n<li>User defined in the SSH config file (e.g. <code>~\/.ssh\/config<\/code>) for this host (URL);<\/li>\n<li>Current system user;<\/li>\n<\/ul>\n<p>So, the simplest even options could be just remove it from the URL and rely on the current system user?<\/p>\n<h2>Local (git-ignored or per-project DVC config) config<\/h2>\n<p>You could do is to remove the <code>username<\/code> part from the <code>url<\/code> and run something like this:<\/p>\n<pre><code>dvc remote modify --local storage_server user username\n<\/code><\/pre>\n<p><code>--local<\/code> here means that DVC will create a separate additional config that will be ignored by Git. This way if every user runs this command in every project they use they will customize the username.<\/p>\n<hr \/>\n<p>Let me know if that helps or something doesn't work. I'll try to help.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1643651143316,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70928144",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67122683,
        "Question_title":"DVC Files Incomplete",
        "Question_body":"<p>I'm in a team using dvc with git to version-control data files. We are using dvc 1.3.1, with the an S3 bucket remote. I'm getting this error when executing <code>dvc fetch<\/code> or <code>dvc pull<\/code> on a colleague's branch:<\/p>\n<pre><code>ERROR: failed to fetch data from the cloud - DVC-file 'C:\\Users\\blah\\Documents\\repo\\data\\processed_data.dvc' format error: extra keys not allowed @ data['outs'][0]['size']\n<\/code><\/pre>\n<p>When I check the dvc file for a cached file with which I have no problem I see this:<\/p>\n<pre><code>md5: ded591aacbe363f0518ceb9c3bc1836b\nouts:\n- md5: efdab20e8b59903b9523cc188ff727e5\n  path: completion_header.p\n  cache: true\n  metric: false\n  persist: false\n<\/code><\/pre>\n<p>but a problematic file only has this:<\/p>\n<pre><code>outs:\n- md5: f4e15187d9a0bbb328e629eabd8d1784.dir\n  size: 112007\n  nfiles: 3\n  path: processed_data\n<\/code><\/pre>\n<p>In all cases, files are added to dvc with the command <code>dvc add %dirname%<\/code>. This is the second time I've seen this on a colleague's branch (2 different people).<\/p>\n<p>Since posting, I have realized that my colleague dvc'd a directory. I have attempted creating the directory first, then calling <code>dvc fetch<\/code>, but get the same error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618565400113,
        "Question_score":2,
        "Question_tags":"git|dvc",
        "Question_view_count":548,
        "Owner_creation_time":1348150034833,
        "Owner_last_access_time":1663852884933,
        "Owner_location":"Glasgow, UK",
        "Owner_reputation":2400,
        "Owner_up_votes":66,
        "Owner_down_votes":12,
        "Owner_views":263,
        "Question_last_edit_time":1618826341416,
        "Answer_body":"<blockquote>\n<p>In all cases, files are added to dvc with the command dvc add %filename%.<\/p>\n<\/blockquote>\n<p>It seems like there is a high chance that one of the dvc files created in newer versions of dvc and you are trying to operate with an older version. Are all of your colleagues use the same dvc version when adding new files?<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1618566517710,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67122683",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":65771384,
        "Question_title":"Data Version Control (dvc) cannot push to remote storage because querying cache",
        "Question_body":"<p>I am setting up a remote storage with dvc using webdavs<\/p>\n<p>I can connect to the remote storage from Finder.<\/p>\n<p>I added the new remote and I see it when I check (dvc remote list)<\/p>\n<p>But when I try to push data, I have the request for password with 0% Querying cache<\/p>\n<p>It stays 0% forever. And when I enter the password, it ends with the following error:<\/p>\n<p>ERROR: unexpected error - No connection with LINK_OF_REMOTE_STORAGE<\/p>\n<p>The only thing I am thinking about is how to check if I can connect to the server from dvc and why querying cache never ends (maybe never starts even)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1610960223987,
        "Question_score":0,
        "Question_tags":"dvc",
        "Question_view_count":436,
        "Owner_creation_time":1411637192833,
        "Owner_last_access_time":1648687432467,
        "Owner_location":null,
        "Owner_reputation":175,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Question_last_edit_time":1610961648252,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65771384",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67104752,
        "Question_title":"\"dvc add -external S3:\/\/mybucket\/data.csv\" is failing with access error even after giving correct remote cache configurations",
        "Question_body":"<p>I'm using dvc and connecting to remote S3 for data track and also setting remote dvc cache in same remote S3.\nFollowing is configure file,<\/p>\n<pre><code>[core]\n    remote = s3remote\n[cache]\n    s3 = s3cache\n[\u2018remote \u201cs3remote\u201d\u2019]\n    url = S3:\/\/dvc-example\n    endpointurl = http:\/\/localhost:9000\/\n    access_key_id = user\n    secret_access_key = password\n    use_ssl = false\n[\u2018remote \u201cs3cache\u201d\u2019]\n    url = s3:\/\/dvc-example\/cache\n    endpointurl = http:\/\/localhost:9000\/\n    access_key_id = user\n    secret_access_key = password\n    use_ssl = false\n<\/code><\/pre>\n<p>I'm able to push and pull from remote repository to local using s3remote.<\/p>\n<p>But when I try to add external data by configuring cache(s3cache), am getting error.<\/p>\n<p>Both s3cache, s3remote has same credentials, then why is it failing when I add external data in dvc?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1618474768363,
        "Question_score":4,
        "Question_tags":"git|amazon-web-services|machine-learning|amazon-s3|dvc",
        "Question_view_count":352,
        "Owner_creation_time":1496203946490,
        "Owner_last_access_time":1664009984060,
        "Owner_location":null,
        "Owner_reputation":838,
        "Owner_up_votes":89,
        "Owner_down_votes":2,
        "Owner_views":33,
        "Question_last_edit_time":1618497721960,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67104752",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":58511529,
        "Question_title":"Highlight.js not respecting parent regex of a sub mode",
        "Question_body":"<p>I need to write a lexer which highlights my command-line tool commands properly.<\/p>\n\n<pre><code>$ dvc add file.csv\n$ dvc pipeline list\n<\/code><\/pre>\n\n<p>So the command starts with <code>dvc<\/code> and it may have one or two subcommands - <code>add<\/code> or <code>pipeline list<\/code> respectively.<\/p>\n\n<p>Therefore, it should highlight <code>dvc add<\/code> and <code>dvc pipeline list<\/code> in first and second case respectively.<\/p>\n\n<pre><code>contains: [\n          {\n            begin: \/^\\s*\\$\\s(dvc|git) [a-z-]+\/,\n            returnBegin: true,\n            contains: [\n              {\n                begin: \/dvc [a-z-]+ ?\/,\n                lexemes: '[a-z-]+',\n                keywords: {\n                  built_in:\n                    'dvc'\n                },\n                contains: [\n                  {\n                    begin: \/\\w+(?![\\S])\/,\n                    keywords: {\n                      built_in: 'list'\n                    }\n                  }\n                ],\n                className: 'strong'\n              }\n            ]\n          }\n        ]\n<\/code><\/pre>\n\n<p>It matches <code>dvc pipeline list<\/code> even though the parent regex i.e. <code>\/^\\s*\\$\\s(dvc|git) [a-z-]+\/<\/code> should only match till <code>dvc pipeline<\/code>. How is it exactly functioning?<\/p>\n\n<p>How does <code>\/dvc [a-z-]+ ?\/<\/code> override it and continues matching the expression?<\/p>\n\n<p>Please refer to this library docs here: <a href=\"https:\/\/highlightjs.readthedocs.io\/en\/latest\/reference.html\" rel=\"nofollow noreferrer\">https:\/\/highlightjs.readthedocs.io\/en\/latest\/reference.html<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1571774550387,
        "Question_score":3,
        "Question_tags":"javascript|syntax-highlighting|highlight|highlight.js|dvc",
        "Question_view_count":177,
        "Owner_creation_time":1562529879143,
        "Owner_last_access_time":1625679183400,
        "Owner_location":null,
        "Owner_reputation":209,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":27,
        "Question_last_edit_time":1572048186932,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58511529",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":71378280,
        "Question_title":"Error with DVC on Google Colab - dvc.scm.CloneError: Failed to clone repo",
        "Question_body":"<p>I'm having a problem trying to run &quot;dvc pull&quot; on Google Colab. I have two repositories (let's call them A and B) where repository A is for my machine learning codes and repository B is for my dataset.<\/p>\n<p>I've successfully pushed my dataset to repository B with DVC (using gdrive as my remote storage) and I also managed to successfully run &quot;dvc import&quot; (as well as &quot;dvc pull\/update&quot;) on my local project of repository A.<\/p>\n<p>The problem comes when I use colab to run my project. So what I did was the following:<\/p>\n<ol>\n<li>Created a new notebook on colab<\/li>\n<li>Successfully git-cloned my machine learning project (repository A)<\/li>\n<li>Ran &quot;!pip install dvc&quot;<\/li>\n<li>Ran &quot;!dvc pull -v&quot; (This is what causes the error)<\/li>\n<\/ol>\n<p>On step 4, I got the error (this is the full stack trace. Note that I changed the repo URL in the stack trace for confidentiality reasons)<\/p>\n<pre><code>2022-03-08 08:53:31,863 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/config.local' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/tmp' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/cache' to gitignore file.\n2022-03-08 08:53:31,916 DEBUG: Creating external repo https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git@3a3f4559efabff8ec74486da39b86688d1b98d75\n2022-03-08 08:53:31,916 DEBUG: erepo: git clone 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to a temporary dir\nEverything is up to date.\n2022-03-08 08:53:32,154 ERROR: failed to pull data from the cloud - Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 185, in clone\n    tmp_repo = clone_from()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1148, in clone_from\n    return cls._clone(git, url, to_path, GitCmdObjectDB, progress, multi_options, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1079, in _clone\n    finalize_process, decode_streams=False)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 176, in handle_process_output\n    return finalizer(process)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/util.py&quot;, line 386, in finalize_process\n    proc.wait(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 502, in wait\n    raise GitCommandError(remove_password_if_present(self.args), status, errstr)\ngit.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v --no-single-branch --progress https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git \/tmp\/tmp2x7y7xgedvc-clone\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 104, in clone\n    return Git.clone(url, to_path, progress=pbar.update_git, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/__init__.py&quot;, line 121, in clone\n    backend.clone(url, to_path, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 190, in clone\n    raise CloneError(url, to_path) from exc\nscmrepo.exceptions.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/command\/data_sync.py&quot;, line 41, in run\n    glob=self.args.glob,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/pull.py&quot;, line 38, in pull\n    run_cache=run_cache,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/fetch.py&quot;, line 50, in fetch\n    revs=revs,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 437, in used_objs\n    with_deps=with_deps,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/index.py&quot;, line 190, in used_objs\n    filter_info=filter_info,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/stage\/__init__.py&quot;, line 660, in get_used_objs\n    for odb, objs in out.get_used_objs(*args, **kwargs).items():\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 918, in get_used_objs\n    return self.get_used_external(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 973, in get_used_external\n    return dep.get_used_objs(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 94, in get_used_objs\n    used, _ = self._get_used_and_obj(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 108, in _get_used_and_obj\n    locked=locked, cache_dir=local_odb.cache_dir\n  File &quot;\/usr\/lib\/python3.7\/contextlib.py&quot;, line 112, in __enter__\n    return next(self.gen)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 35, in external_repo\n    path = _cached_clone(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 155, in _cached_clone\n    clone_path, shallow = _clone_default_branch(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 45, in wrapper\n    return deco(call, *dargs, **dkwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/flow.py&quot;, line 274, in wrap_with\n    return call()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 66, in __call__\n    return self._func(*self._args, **self._kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 220, in _clone_default_branch\n    git = clone(url, clone_path)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 106, in clone\n    raise CloneError(str(exc))\ndvc.scm.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\n2022-03-08 08:53:32,161 DEBUG: Analytics is enabled.\n2022-03-08 08:53:32,192 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n2022-03-08 08:53:32,193 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n<\/code><\/pre>\n<p>And btw this is how I cloned my git repository (repo A)<\/p>\n<pre><code>!git config - global user.name &quot;Zharfan&quot;\n!git config - global user.email &quot;zharfan@myemail.com&quot;\n!git clone https:\/\/&lt;MyTokenName&gt;:&lt;MyToken&gt;@link-to-my-repo-A.git\n<\/code><\/pre>\n<p>Does anyone know why? Any help would be greatly appreciated. Thank you in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":12,
        "Question_creation_time":1646641948613,
        "Question_score":1,
        "Question_tags":"git|dataset|google-colaboratory|dvc",
        "Question_view_count":707,
        "Owner_creation_time":1525227015313,
        "Owner_last_access_time":1663842020310,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1652856778060,
        "Answer_body":"<p>To summarize the discussion in the comments thread.<\/p>\n<p>Most likely it's happening since DVC can't get access to a private repo on GitLab. (The error message is obscure and should be fixed.)<\/p>\n<p>The same way you would not be able to run:<\/p>\n<pre><code>!git clone https:\/\/gitlab.com\/org\/&lt;private-repo&gt;\n<\/code><\/pre>\n<p>It also returns a pretty obscure error:<\/p>\n<pre><code>Cloning into '&lt;private-repo&gt;'...\nfatal: could not read Username for 'https:\/\/gitlab.com': No such device or address\n<\/code><\/pre>\n<p>(I think it's something related to how tty is setup in Colab?)<\/p>\n<p>The best approach to solve this is to use SSH like described <a href=\"https:\/\/medium.com\/@sadiaafrinpurba\/how-to-clone-private-github-repo-in-google-colab-using-ssh-77384cfef18f\" rel=\"nofollow noreferrer\">here<\/a> for example.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1647022114532,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71378280",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72551630,
        "Question_title":"How to setup a DVC shared cache without git repository between different services in minikube?",
        "Question_body":"<p>I need to setup a shared cache in minikube in such a way that different services can use that cache to pull and update DVC models and data needed for training Machine Learning models. The structure of the project is to use 1 pod to periodically update the cache with new models and outputs. Then, multiple pods can read the cache to recreate the updated models and data. So I need to be able to update the local cache directory and pull from it using DVC commands, so that all the services have consistent view on the latest models and data created by a service.<\/p>\n<p>More specifically, I have a docker image called <code>inference-service<\/code> that should only <code>dvc pull<\/code> or some how use the info in the shared dvc cache to get the latest model and data locally in <code>models<\/code> and <code>data<\/code> folders (see dockerfile) in minikube. I have another image called <code>test-service<\/code> that\nruns the ML pipeline using <code>dvc repro<\/code> which creates the models and data that DVC needs (dvc.yaml) to track and store in the shared cache. So <code>test-service<\/code> should push created outputs from the ML pipeline into the shared cache so that <code>inference-service<\/code> can pull it and use it instead of running dvc repro by itself. <code>test-service<\/code> should only re-train and write the updated models and data into the shared cache while <code>inference-service<\/code> should only read and recreate the updated\/latest models and data from the shared cache.<\/p>\n<p><em><strong>Problem: the cache does get mounted on the minikube VM, but the inference service does not pull (using <code>dvc pull -f<\/code>) the data and models after the test service is done with <code>dvc repro<\/code> and results the following warnings and failures:<\/strong><\/em><\/p>\n<p><em>relevant kubernetes pod log of inference-service<\/em><\/p>\n<pre><code>WARNING: Output 'data\/processed\/train_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/train_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/validation_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/validation_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/test_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/test_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/interim\/train_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/train_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/validation_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/validation_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/test_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/test_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'models\/mlb.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/tfidf_vectorizer.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/model.pkl'(stage: 'train') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'reports\/scores.json'(stage: 'evaluate') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: No file hash info found for '\/root\/models\/model.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/reports\/scores.json'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/train_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/validation_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/test_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/train_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/validation_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/test_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/mlb.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/tfidf_vectorizer.pkl'. It won't be created.\n10 files failed\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\n\/root\/models\/model.pkl\n\/root\/reports\/scores.json\n\/root\/data\/processed\/train_preprocessed.pkl\n\/root\/data\/processed\/validation_preprocessed.pkl\n\/root\/data\/processed\/test_preprocessed.pkl\n\/root\/data\/interim\/train_featurized.pkl\n\/root\/data\/interim\/validation_featurized.pkl\n\/root\/data\/interim\/test_featurized.pkl\n\/root\/models\/mlb.pkl\n\/root\/models\/tfidf_vectorizer.pkl\nIs your cache up to date?\n<\/code><\/pre>\n<p><em>relevant kubernetes pod log of test-service<\/em><\/p>\n<pre><code>Stage 'preprocess' is cached - skipping run, checking out outputs\nGenerating lock file 'dvc.lock'\nUpdating lock file 'dvc.lock'\nStage 'featurize' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'train' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'evaluate' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nUse `dvc push` to send your updates to remote storage.\n<\/code><\/pre>\n<p><strong>Project Tree<\/strong><\/p>\n<pre><code>\u251c\u2500 .dvc\n\u2502  \u251c\u2500 .gitignore\n\u2502  \u251c\u2500 config\n\u2502  \u2514\u2500 tmp\n\u251c\u2500 deployment\n\u2502  \u251c\u2500 docker-compose\n\u2502  \u2502  \u251c\u2500 docker-compose.yml\n\u2502  \u251c\u2500 minikube-dep\n\u2502  \u2502  \u251c\u2500 inference-test-services_dep.yaml\n\u2502  \u251c\u2500 startup_minikube_with_mount.sh.sh\n\u251c\u2500 Dockerfile # for inference service\n\u251c\u2500 dvc-cache # services should push and pull from this cache folder and see this as the DVC repo\n\u251c- dvc.yaml\n\u251c- params.yaml\n\u251c\u2500 src\n\u2502  \u251c\u2500 build_features.py\n|  \u251c\u2500 preprocess_data.py\n|  \u251c\u2500 serve_model.py\n|  \u251c\u2500 startup.sh  \n|  \u251c\u2500 requirements.txt\n\u251c\u2500 test_dep\n\u2502  \u251c\u2500 .dvc # same as .dvc in the root folder\n|  |  \u251c\u2500...\n\u2502  \u251c\u2500 Dockerfile # for test service\n\u2502  \u251c\u2500 dvc.yaml\n|  \u251c\u2500 params.yaml\n\u2502  \u2514\u2500 src\n\u2502     \u251c\u2500 build_features.py # same as root src folder\n|     \u251c\u2500 preprocess_data.py # same as root src folder\n|     \u251c\u2500 serve_model.py # same as root src folder\n|     \u251c\u2500 startup_test.sh  \n|     \u251c\u2500 requirements.txt  # same as root src folder\n<\/code><\/pre>\n<p><strong>dvc.yaml<\/strong><\/p>\n<pre><code>stages:\n  preprocess:\n    cmd: python ${preprocess.script}\n    params:\n      - preprocess\n    deps:\n      - ${preprocess.script}\n      - ${preprocess.input_train}\n      - ${preprocess.input_val}\n      - ${preprocess.input_test}\n    outs:\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n  featurize:\n    cmd: python ${featurize.script}\n    params:\n      - preprocess\n      - featurize\n    deps:\n      - ${featurize.script}\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n    outs:\n      - ${featurize.output_train}\n      - ${featurize.output_val}\n      - ${featurize.output_test}\n      - ${featurize.mlb_out}\n      - ${featurize.tfidf_vectorizer_out}\n  train:\n    cmd: python ${train.script}\n    params:\n      - featurize\n      - train\n    deps:\n      - ${train.script}\n      - ${featurize.output_train}\n    outs:\n      - ${train.model_out}\n  evaluate:\n    cmd: python ${evaluate.script}\n    params:\n      - featurize\n      - train\n      - evaluate\n    deps:\n      - ${evaluate.script}\n      - ${train.model_out}\n      - ${featurize.output_val}\n    metrics:\n      - ${evaluate.scores_path}\n<\/code><\/pre>\n<p><strong>params.yaml<\/strong><\/p>\n<pre><code>preprocess:\n  script: src\/preprocess\/preprocess_data.py\n  input_train: data\/raw\/train.tsv\n  input_val: data\/raw\/validation.tsv\n  input_test: data\/raw\/test.tsv\n  output_train: data\/processed\/train_preprocessed.pkl\n  output_val: data\/processed\/validation_preprocessed.pkl\n  output_test: data\/processed\/test_preprocessed.pkl\n\nfeaturize:\n  script: src\/features\/build_features.py\n  output_train: data\/interim\/train_featurized.pkl\n  output_val: data\/interim\/validation_featurized.pkl\n  output_test: data\/interim\/test_featurized.pkl\n  mlb_out: models\/mlb.pkl\n  tfidf_vectorizer_out: models\/tfidf_vectorizer.pkl\n\ntrain:\n  script: src\/models\/train_model.py\n  model_out: models\/model.pkl\n\nevaluate:\n  script: src\/models\/evaluate_model.py\n  scores_path: reports\/scores.json\n  roc_json: reports\/roc_plot.json\n  prc_json: reports\/prc_plot.json\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654718868090,
        "Question_score":0,
        "Question_tags":"docker|kubernetes|minikube|dvc",
        "Question_view_count":196,
        "Owner_creation_time":1562403039337,
        "Owner_last_access_time":1659463717997,
        "Owner_location":null,
        "Owner_reputation":298,
        "Owner_up_votes":14,
        "Owner_down_votes":1,
        "Owner_views":47,
        "Question_last_edit_time":1655157056467,
        "Answer_body":"<p>After running <code>dvc repro<\/code> in <code>test-service<\/code>, a new <code>dvc.lock<\/code> will be created, containing the file hashes relative to your pipeline (i.e. the hash for <code>models\/model.pkl<\/code> etc).<\/p>\n<p>If you're running a shared cache, <code>inference-service<\/code> should have access to the updated <code>dvc.lock<\/code>. If that is present, it will be sufficient to run <code>dvc checkout<\/code> to populate the workspace with the files corresponding to the hashes in the shared cache.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1654875420463,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72551630",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":61245284,
        "Question_title":"Is it necessary to commit DVC files from our CI pipelines?",
        "Question_body":"<p>DVC uses git commits to save the experiments and navigate between experiments.<\/p>\n<p>Is it possible to avoid making auto-commits in CI\/CD (to save data artifacts after <code>dvc repro<\/code> in CI\/CD side).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1587023782283,
        "Question_score":6,
        "Question_tags":"git|machine-learning|continuous-integration|dvc|mlops",
        "Question_view_count":1047,
        "Owner_creation_time":1558529684193,
        "Owner_last_access_time":1642243398997,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":79,
        "Owner_up_votes":107,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":1594640021920,
        "Answer_body":"<blockquote>\n  <p>will you make it part of CI pipeline<\/p>\n<\/blockquote>\n\n<p>DVC often serves as a part of MLOps infrastructure. There is a popular <a href=\"https:\/\/martinfowler.com\/articles\/cd4ml.html\" rel=\"noreferrer\">blog post about CI\/CD for ML<\/a> where DVC is used under the hood. <a href=\"https:\/\/blog.codecentric.de\/en\/2020\/01\/remote-training-gitlab-ci-dvc\/\" rel=\"noreferrer\">Another example<\/a> but with GitLab CI\/CD.<\/p>\n\n<blockquote>\n  <p>scenario where you will integrate dvc commit command with CI\n  pipelines?<\/p>\n<\/blockquote>\n\n<p>If you mean <code>git commit<\/code> of DVC files (not <code>dvc commit<\/code>) then yes, you need to commit dvc-files into Git during CI\/CD process. Auto-commit is not the best practice.<\/p>\n\n<p>How to avoid Git commit in CI\/CD:<\/p>\n\n<ol>\n<li>After ML model training in CI\/CD, save changed dvc-files in external storage (for example GitLab artifact\/releases), then get the files to a developer machine and commit there. Users usually write scripts to automate it.<\/li>\n<li>Wait for DVC 1.0 release when <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1234\" rel=\"noreferrer\">run-cache (like build-cache)<\/a> will be implemented. Run-cache makes dvc-files ephemeral and no additional Git commits will be required. Technically, run-cache is an associative storage <code>repo state --&gt; run results<\/code> outside of Git repo (in data remote).<\/li>\n<\/ol>\n\n<p>Disclaimer: I'm one of the creators of DVC.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1587029761967,
        "Answer_score":6.0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61245284",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72645665,
        "Question_title":"How to track a folder again when used \"git rm -rf --cached folder_name\" : Error: The following paths are ignored by one of your .gitignore files",
        "Question_body":"<p>I wanted to un-track my git files so I put <code>.dvc<\/code> inside my <code>.gitignore<\/code> file, and run<\/p>\n<pre><code>git rm -rf --cached .dvc\n<\/code><\/pre>\n<p>and then committed.<\/p>\n<p>I realised my mistake soon and then wanted to add the files again . I tried deleting the <code>gitignore<\/code> file, commit, make a new <code>.gitignore<\/code> and then try adding but all is futile. <code>git add .dvc<\/code> does not track my files and using <code>git add .dvc\/*<\/code> gives me error:<\/p>\n<pre><code>The following paths are ignored by one of your .gitignore files:\n.dvc\/cache\n.dvc\/tmp\nUse -f if you really want to add them.\n<\/code><\/pre>\n<p>Running the command <code>git check-ignore -v .dvc\/*<\/code> gives me:<\/p>\n<pre><code>.dvc\/.gitignore:3:\/cache    .dvc\/cache\n.dvc\/.gitignore:2:\/tmp  .dvc\/tmp\n\n<\/code><\/pre>\n<p>What can be done now?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1655381585170,
        "Question_score":1,
        "Question_tags":"git|gitignore|dvc",
        "Question_view_count":41,
        "Owner_creation_time":1561995857583,
        "Owner_last_access_time":1664035105223,
        "Owner_location":"Noida, Uttar Pradesh, India",
        "Owner_reputation":2716,
        "Owner_up_votes":403,
        "Owner_down_votes":26,
        "Owner_views":668,
        "Question_last_edit_time":1655382233167,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72645665",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70939675,
        "Question_title":"DVC Push KeyError fileSize",
        "Question_body":"<p>I've added a large list of CSV files to my dvc repository but when I try to do DVC push it complains with<\/p>\n<pre><code>ERROR: unexpected error - KeyError('fileSize')\n<\/code><\/pre>\n<p><strong>Edit<\/strong>\nSo searching around it seem that it might help to include the verbose log with regards to the error.<\/p>\n<pre><code>T11:27:08~\/documents\/*****\/data$ dvc push -v\n2022-02-01 11:32:13,186 DEBUG: Adding '\/home\/jhylands\/Documents\/*****\/.dvc\/config.local' to gitignore file.\n2022-02-01 11:32:13,199 DEBUG: Adding '\/home\/jhylands\/Documents\/*****\/.dvc\/tmp' to gitignore file.\n2022-02-01 11:32:13,200 DEBUG: Adding '\/home\/jhylands\/Documents\/*****\/.dvc\/cache' to gitignore file.\n2022-02-01 11:32:14,102 DEBUG: Preparing to transfer data from '\/home\/jhylands\/Documents\/*****\/.dvc\/cache' to '*********'\n2022-02-01 11:32:14,102 DEBUG: Preparing to collect status from '********'\n2022-02-01 11:32:14,103 DEBUG: Collecting status from '*******'\n2022-02-01 11:32:14,439 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/jhylands\/Documents\/*****\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '*****.apps.googleusercontent.com', 'client_secret': '****************', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-02-01 11:32:14,994 DEBUG: Estimated remote size: 256 files\n2022-02-01 11:32:14,995 DEBUG: Querying '316' hashes via traverse\n2022-02-01 11:32:15,325 ERROR: unexpected error - KeyError('fileSize')\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/pydrive2\/files.py&quot;, line 226, in __getitem__\n    return dict.__getitem__(self, key)\nKeyError: 'fileSize'\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/main.py&quot;, line 55, in main\n    ret = cmd.do_run()\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/command\/base.py&quot;, line 45, in do_run\n    return self.run()\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/command\/data_sync.py&quot;, line 57, in run\n    processed_files_count = self.repo.push(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/repo\/push.py&quot;, line 56, in push\n    pushed += self.cloud.push(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py&quot;, line 85, in push\n    return transfer(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/transfer.py&quot;, line 153, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py&quot;, line 158, in compare_status\n    dest_exists, dest_missing = status(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py&quot;, line 131, in status\n    exists.update(odb.hashes_exist(hashes, name=odb.fs_path, **kwargs))\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 499, in hashes_exist\n    remote_hashes = set(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 334, in _list_hashes_traverse\n    yield from itertools.chain.from_iterable(in_remote)\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 611, in result_iterator\n    yield fs.pop().result()\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 439, in result\n    return self.__get_result()\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 388, in __get_result\n    raise self._exception\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 324, in list_with_update\n    return list(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 215, in _list_hashes\n    for path in self._list_paths(prefix, progress_callback):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 195, in _list_paths\n    for file_info in self.fs.find(fs_path, prefix=prefix):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py&quot;, line 107, in find\n    yield from self.fs.find(path)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/pydrive2\/fs\/spec.py&quot;, line 323, in find\n    &quot;size&quot;: int(item[&quot;fileSize&quot;]),\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/pydrive2\/files.py&quot;, line 229, in __getitem__\n    raise KeyError(e)\nKeyError: KeyError('fileSize')\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1643714210580,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":54,
        "Owner_creation_time":1333908536530,
        "Owner_last_access_time":1664025870700,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":884,
        "Owner_up_votes":185,
        "Owner_down_votes":3,
        "Owner_views":59,
        "Question_last_edit_time":1643715642689,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70939675",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73700203,
        "Question_title":"ERROR: Cannot add 'folder-path', because it is overlapping with other DVC tracked output:",
        "Question_body":"<p>Goal: <code>add<\/code> <code>commit<\/code> <code>push<\/code> all contents of <code>project_model\/data\/<\/code> to <strong>dvcstore<\/strong>.<\/p>\n<p>I don't have any <code>.dvc<\/code> files in my project.<\/p>\n<pre><code>$ dvc add .\/project_model\/data\/\nERROR: Cannot add '\/home\/me\/PycharmProjects\/project\/project_model\/data\/images', because it is overlapping with other DVC tracked output: '\/home\/me\/PycharmProjects\/project\/project_model\/data'.\nTo include '\/home\/me\/PycharmProjects\/project\/project_model\/data\/images' in '\/home\/me\/PycharmProjects\/project\/project_model\/data', run 'dvc commit project_model\/data.dvc'\n\n$ dvc commit project_model\/data.dvc\nERROR: failed to commit project_model\/data.dvc - 'project_model\/data.dvc' does not exist\n<\/code><\/pre>\n<p>I've deleted contents from <code>.dvc\/cache\/<\/code> and <strong>S3<\/strong> <code>s3:\/\/foo\/bar\/dvcstore\/<\/code>, with no luck.<\/p>\n<hr \/>\n<pre><code>$ dvc -V\n2.10.2\n<\/code><\/pre>\n<pre><code>$ dvc doctor\nDVC version: 2.10.2 (pip)\n---------------------------------\nPlatform: Python 3.9.12 on Linux-5.15.0-47-generic-x86_64-with-glibc2.35\nSupports:\n        webhdfs (fsspec = 2022.5.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        s3 (s3fs = 2022.5.0, boto3 = 1.21.21)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/nvme0n1p5\nCaches: local\nRemotes: s3\nWorkspace directory: ext4 on \/dev\/nvme0n1p5\nRepo: dvc, git\n<\/code><\/pre>\n<p>Please let me know if there's anything else I can add to post.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1663059618623,
        "Question_score":1,
        "Question_tags":"git|amazon-s3|caching|pycharm|dvc",
        "Question_view_count":31,
        "Owner_creation_time":1631019482980,
        "Owner_last_access_time":1663946675073,
        "Owner_location":null,
        "Owner_reputation":234,
        "Owner_up_votes":708,
        "Owner_down_votes":14,
        "Owner_views":155,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In my case, the problem was in <code>dvc.yaml<\/code>.<\/p>\n<p>For a few <code>stages<\/code>, I had cyclical dependencies, where a file-path was mentioned in both the <code>deps<\/code> and <code>outs<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1663149911323,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73700203",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":69254525,
        "Question_title":"ERROR: bad DVC file name 'my_server\\models\\*.tar.gz.dvc' is git-ignored",
        "Question_body":"<p>I just started with DVC. I have a git repo in which there are heavy models that i want to push to dvc. So I initialized the dvc by<\/p>\n<pre><code>dvc init\n<\/code><\/pre>\n<p>and then configured the bucket<\/p>\n<pre><code>dvc remote add -d storage s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>\n<p>Now there is <code>\/models<\/code> folders, in which there was <code>.gitkeep<\/code> file and trained models. Following entry was in my <code>.gitignore<\/code><\/p>\n<pre><code>*.tar.gz\n<\/code><\/pre>\n<p>I ran the following command<\/p>\n<pre><code>git rm -r --cached my_server\\models\n<\/code><\/pre>\n<p>and added the following in the <code>.gitignore<\/code><\/p>\n<pre><code>models\n<\/code><\/pre>\n<p>I want to add all the <code>tar.gz<\/code> files to push on dvc<\/p>\n<p>so i tried<\/p>\n<pre><code>dvc add .\/my_server\/models\/*.tar.gz\n<\/code><\/pre>\n<p>but this is showing<\/p>\n<pre><code>ERROR: bad DVC file name 'my_server\\models\\*.tar.gz.dvc' is git-ignored.\n<\/code><\/pre>\n<p>If I do\ndvc add .\/my_server\/models\/<\/p>\n<p>then this folder is added and a <code>models.dvc<\/code> file gets created. then git code shows for the changes.<\/p>\n<p>what is the correct way, do i need to mention <code>*.dvc<\/code> to <code>.gitignore<\/code> as well?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1632141296913,
        "Question_score":2,
        "Question_tags":"git|dvc",
        "Question_view_count":760,
        "Owner_creation_time":1363322632587,
        "Owner_last_access_time":1664084323070,
        "Owner_location":"Chandigarh, India",
        "Owner_reputation":13237,
        "Owner_up_votes":2454,
        "Owner_down_votes":19,
        "Owner_views":2675,
        "Question_last_edit_time":1632236886743,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69254525",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":68284742,
        "Question_title":"DVC experiment is restoring deleted files",
        "Question_body":"<p>I am using DVC to run experiments in my project using<\/p>\n<pre><code>dvc exp run\n<\/code><\/pre>\n<p>Now when i make changes to a file(example train.py) and run &quot;dvc exp run&quot; everything goes well,\nbut my problem is that when making changes by <strong>deleting<\/strong> a file(example train.py or an image in the data folder) as soon as i run the &quot;dvc exp run&quot; the file is restored.\nhow to stop that from happening?<\/p>\n<p>This is my dvc.yaml:<\/p>\n<pre><code>stages:\n  train:\n    cmd: python train.py\n    deps:\n    - train.py\n    metrics:\n    - metrics.txt:\n        cache: false\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_time":1625655452383,
        "Question_score":0,
        "Question_tags":"dvc",
        "Question_view_count":272,
        "Owner_creation_time":1557314563687,
        "Owner_last_access_time":1645485519443,
        "Owner_location":"Tunis, Tunisia",
        "Owner_reputation":113,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68284742",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":60861552,
        "Question_title":"Getting this weird error when trying to run DVC pull",
        "Question_body":"<p>I am new to using DVC and just exploring it. I am trying to pull data from s3 that was pushed by another person on my team. But I am getting this error:<\/p>\n\n<pre><code>WARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:\nname: head_test_file.csv, md5: 45db668193ba44228d61115b1d0304fe\nWARNING: Cache '45db668193ba44228d61115b1d0304fe' not found. File 'head_test_file.csv' won't be created.\nNo changes.\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\nhead_test_file.csv\nDid you forget to fetch?\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1585201186813,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":7063,
        "Owner_creation_time":1491917519307,
        "Owner_last_access_time":1648587168753,
        "Owner_location":null,
        "Owner_reputation":1756,
        "Owner_up_votes":82,
        "Owner_down_votes":5,
        "Owner_views":199,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60861552",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56456463,
        "Question_title":"Unable to ignore .DS_Store files in DVC",
        "Question_body":"<p>I use DVC to track my media files. I use MacOS and I want\".DS_Store\" files to be ignored by DVC. According to DVC documentation I can achieve it with  <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvcignore\" rel=\"nofollow noreferrer\">.dvcignore<\/a>. I created <code>.dvcignore<\/code> file with \".DS_Store\" rule. However every time \".DS_Store\" is created <code>dvc status<\/code> still says that content has changed<\/p>\n\n<p>Here is the little test to reproduce my issue:<\/p>\n\n<pre><code>$ git init\n$ dvc init\n\n# create directory to store data\n# and track it's content with DVC\n$ mkdir data\n$ dvc add data\n\n# Ignore .DS_Store files created by MacOS\n$ echo \".DS_Store\" &gt; .dvcignore\n\n# create .DS_Store in data dir\n$ touch \"data\/.DS_Store\"\n<\/code><\/pre>\n\n<p>If I understand DVC documentation correctly then <code>dvc status<\/code> should print something like \"Pipeline is up to date. Nothing to reproduce\". However <code>dvc status<\/code> gives me:<\/p>\n\n<pre><code>data.dvc:\n        changed outs:\n                modified:           data\n<\/code><\/pre>\n\n<p>How I can really ignore \".DS_Store\" files?<\/p>\n\n<p><strong>UPDATE:<\/strong> The .dvcignore support noticeably improved in latest versions and the problem is no more relevant.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1559722020690,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":326,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":1568948355943,
        "Answer_body":"<p>The current implementation of <code>.dvcignore<\/code> is very limited. Read more on it <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvcignore\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Please, mention that you are interested in this feature here - <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1876\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/issues\/1876<\/a>. That would help our team to prioritize issues properly.<\/p>\n\n<p>The possible workaround for now would be to use one of these approaches - <a href=\"https:\/\/stackoverflow.com\/questions\/18015978\/how-to-stop-creating-ds-store-on-mac\">How to stop creating .DS_Store on Mac?<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1559758177416,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56456463",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":68517516,
        "Question_title":"Forbidden: An error occurred (403) when calling the HeadObject operation:",
        "Question_body":"<p>my ~\/.aws\/credentials looks like<\/p>\n<pre><code>[default]\naws_access_key_id = XYZ\naws_secret_access_key = ABC\n\n[testing]\nsource_profile = default\nrole_arn = arn:aws:iam::54:role\/ad\n<\/code><\/pre>\n<p>I add my remote like<\/p>\n<pre><code>dvc remote add --local -v myremote s3:\/\/bib-ds-models-testing\/data\/dvc-test\n<\/code><\/pre>\n<p>I have made my .dvc\/config.local to look like<\/p>\n<pre><code>[\u2018remote \u201cmyremote\u201d\u2019]\nurl = s3:\/\/bib-ds-models-testing\/data\/dvc-test\naccess_key_id = XYZ\nsecret_access_key = ABC\/h2hOsRcCIFqwYWV7eZaUq3gNmS\nprofile=\u2018testing\u2019\ncredentialpath = \/Users\/nyt21\/.aws\/credentials\n<\/code><\/pre>\n<p>but still after running <code>dvc push -r myremote<\/code> I get<\/p>\n<blockquote>\n<p>ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden<\/p>\n<\/blockquote>\n<p>** Update\nhere is the output of <code>dvc push -v<\/code><\/p>\n<pre><code>2021-07-25 22:40:38,887 DEBUG: Check for update is enabled.\n2021-07-25 22:40:39,022 DEBUG: Preparing to upload data to 's3:\/\/bib-ds-models-testing\/data\/dvc-test'\n2021-07-25 22:40:39,022 DEBUG: Preparing to collect status from s3:\/\/bib-ds-models-testing\/data\/dvc-test\n2021-07-25 22:40:39,022 DEBUG: Collecting information from local cache...\n2021-07-25 22:40:39,022 DEBUG: Collecting information from remote cache...                                                                                                                     \n2021-07-25 22:40:39,022 DEBUG: Matched '0' indexed hashes\n2021-07-25 22:40:39,022 DEBUG: Querying 1 hashes via object_exists\n2021-07-25 22:40:39,644 ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden                                                          \n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 246, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/aiobotocore\/client.py&quot;, line 154, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 1057, in _info\n    out = await self._simple_info(path)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 970, in _simple_info\n    out = await self._call_s3(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 265, in _call_s3\n    raise translate_boto_error(err)\nPermissionError: Access Denied\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 246, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/aiobotocore\/client.py&quot;, line 154, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/main.py&quot;, line 55, in main\n    ret = cmd.do_run()\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/command\/base.py&quot;, line 50, in do_run\n    return self.run()\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/command\/data_sync.py&quot;, line 57, in run\n    processed_files_count = self.repo.push(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py&quot;, line 51, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/repo\/push.py&quot;, line 44, in push\n    pushed += self.cloud.push(objs, jobs, remote=remote)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py&quot;, line 79, in push\n    return remote_obj.push(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 57, in wrapper\n    return f(obj, *args, **kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 494, in push\n    ret = self._process(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 351, in _process\n    dir_status, file_status, dir_contents = self._status(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 195, in _status\n    self.hashes_exist(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/remote\/base.py&quot;, line 145, in hashes_exist\n    return indexed_hashes + self.odb.hashes_exist(list(hashes), **kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 438, in hashes_exist\n    remote_hashes = self.list_hashes_exists(hashes, jobs, name)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 389, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 619, in result_iterator\n    yield fs.pop().result()\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 444, in result\n    return self.__get_result()\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 389, in __get_result\n    raise self._exception\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/concurrent\/futures\/thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 380, in exists_with_progress\n    ret = self.fs.exists(path_info)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py&quot;, line 92, in exists\n    return self.fs.exists(self._with_bucket(path_info))\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/fsspec\/asyn.py&quot;, line 87, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/fsspec\/asyn.py&quot;, line 68, in sync\n    raise result[0]\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/fsspec\/asyn.py&quot;, line 24, in _runner\n    result[0] = await coro\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 802, in _exists\n    await self._info(path, bucket, key, version_id=version_id)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 1061, in _info\n    out = await self._version_aware_info(path, version_id)\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 1004, in _version_aware_info\n    out = await self._call_s3(\n  File &quot;\/Users\/nyt21\/opt\/miniconda3\/envs\/dvc\/lib\/python3.8\/site-packages\/s3fs\/core.py&quot;, line 265, in _call_s3\n    raise translate_boto_error(err)\nPermissionError: Forbidden\n------------------------------------------------------------\n2021-07-25 22:40:39,712 DEBUG: Version info for developers:\nDVC version: 2.5.4 (pip)\n---------------------------------\nPlatform: Python 3.8.10 on macOS-10.16-x86_64-i386-64bit\nSupports:\n        http (requests = 2.26.0),\n        https (requests = 2.26.0),\n        s3 (s3fs = 2021.6.1, boto3 = 1.18.6)\nCache types: reflink, hardlink, symlink\nCache directory: apfs on \/dev\/disk3s1s1\nCaches: local\nRemotes: s3\nWorkspace directory: apfs on \/dev\/disk3s1s1\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2021-07-25 22:40:39,713 DEBUG: Analytics is enabled.\n2021-07-25 22:40:39,765 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/var\/folders\/4x\/xhm22wt16gl6m9nvkl9gllkc0000gn\/T\/tmpo86jdns5']'\n2021-07-25 22:40:39,769 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/var\/folders\/4x\/xhm22wt16gl6m9nvkl9gllkc0000gn\/T\/tmpo86jdns5']'\n<\/code><\/pre>\n<p>I can upload through python<\/p>\n<pre><code>import boto3\nimport os\nimport pickle\n\nbucket_name = 'bib-ds-models-testing'\nos.environ[&quot;AWS_PROFILE&quot;] = &quot;testing&quot;\nsession = boto3.Session()\ns3_client = boto3.client('s3')\n\ns3_client.upload_file('\/Users\/nyt21\/Devel\/DVC\/test\/data\/iris.csv',\n    'bib-ds-models-testing',\n    'data\/dvc-test\/my_iris.csv')\n<\/code><\/pre>\n<p>I don't use aws CLI but the following also gives an access deny !<\/p>\n<pre><code>aws s3 ls s3:\/\/bib-ds-models-testing\/data\/dvc-test\n<\/code><\/pre>\n<blockquote>\n<p>An error occurred (AccessDenied) when calling the ListObjectsV2\noperation: Access Denied<\/p>\n<\/blockquote>\n<p>but it works if I add --profile=testing<\/p>\n<pre><code>aws s3 ls s3:\/\/bib-ds-models-testing\/data\/dvc-test --profile=testing\n                       \n<\/code><\/pre>\n<blockquote>\n<p>PRE dvc-test\/<\/p>\n<\/blockquote>\n<p>just you know environment variable <code>AWS_PROFILE<\/code> is already set to 'testing'<\/p>\n<p><strong>UPDATE<\/strong><\/p>\n<p>I have tried both <code>AWS_PROFILE='testing'<\/code> and <code>AWS_PROFILE=testing<\/code>, neither of them worked.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DZQlz.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DZQlz.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":7,
        "Question_creation_time":1627207477173,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":1526,
        "Owner_creation_time":1297356892887,
        "Owner_last_access_time":1663786584013,
        "Owner_location":"Copenhagen, Denmark",
        "Owner_reputation":4988,
        "Owner_up_votes":350,
        "Owner_down_votes":22,
        "Owner_views":416,
        "Question_last_edit_time":1627383297972,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68517516",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73567700,
        "Question_title":"What DVC does when git merge is executed?",
        "Question_body":"<p>I have two git branches (master and develop). DVC maps a data folder in both of them. When I go into master and merging with develop is correct that DVC does not add any new file inside the data folder created in the develop branch but leaves the folder as it is unchanged?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662027331777,
        "Question_score":1,
        "Question_tags":"git|dvc",
        "Question_view_count":40,
        "Owner_creation_time":1580668804397,
        "Owner_last_access_time":1664031434117,
        "Owner_location":null,
        "Owner_reputation":498,
        "Owner_up_votes":453,
        "Owner_down_votes":9,
        "Owner_views":66,
        "Question_last_edit_time":1662097250203,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73567700",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70560288,
        "Question_title":"DVC Shared Windows Directory Setup",
        "Question_body":"<p>I have one Linux machine and one Windows machine for developments. For data sharing, we have set up a shared Windows directory in another Windows machine, which both my Linux and Windows can access.<\/p>\n<p>I am now using <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> for version control of the shared data. To make it easy, I mount the shared Windows folder both in Windows and in Linux development machine. In Windows, it looks like<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>In Linux, it looks like:<\/p>\n<pre><code>[core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>As you can see, Windows and Linux have different mounting points. So my question is: is there a way to make that both Windows and Linux have the same <code>\u00f9rl<\/code> in the DVC configuration file?<\/p>\n<p>If this is impossible, is there another alternative solution for DVC keeps data in remote shared Windows folder? Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641163263100,
        "Question_score":1,
        "Question_tags":"linux|dvc",
        "Question_view_count":128,
        "Owner_creation_time":1331553057367,
        "Owner_last_access_time":1663939436043,
        "Owner_location":null,
        "Owner_reputation":10643,
        "Owner_up_votes":1174,
        "Owner_down_votes":7,
        "Owner_views":504,
        "Question_last_edit_time":1641199464667,
        "Answer_body":"<p>If you are using a local remote this way, you won't be able to have to the same <code>url<\/code> on both platforms since the mount points are different (as you already realized).<\/p>\n<p>The simplest way to configure this would be to pick one (Linux or Windows) <code>url<\/code> to use as your default case that gets git-committed into <code>.dvc\/config<\/code>. On the other platform you (or your users) can override that <code>url<\/code> in the local configuration file: <code>.dvc\/config.local<\/code>.<\/p>\n<p>(Note that <code>.dvc\/config.local<\/code> is a git-ignored file and will not be included in any commits)<\/p>\n<p>So if you wanted Windows to be the default case, in <code>.dvc\/config<\/code> you would have:<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>and on your Linux machine you would add the file <code>.dvc\/config.local<\/code> containing:<\/p>\n<pre><code>['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>See the DVC docs for <code>dvc config --local<\/code> and <code>dvc remote modify --local<\/code> for more details:<\/p>\n<ul>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#description\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/config#description<\/a><\/li>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1641179335767,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70560288",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72641284,
        "Question_title":"Undo changes in pandas Dataframe(column drops, row drops, edits performed on a single cell)",
        "Question_body":"<p>I am currently working on developing a 'undo' operation for my interface that deals with changes performed on csv files. I want to provide an option for the user to revert the changes that he had done to the csv file, these changes include edit a cell, deleting column, deleting row, adding row, adding column etc. For this I want to know, does version control works in this scenario? If yes, which data version control should I prefer? If not, please suggest me an another alternative.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1655360134273,
        "Question_score":1,
        "Question_tags":"python|pandas|git|version-control|dvc",
        "Question_view_count":39,
        "Owner_creation_time":1655359768507,
        "Owner_last_access_time":1658381602853,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1655716541576,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72641284",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":52871630,
        "Question_title":"Resolving paths in mingw fails with Data Version Control",
        "Question_body":"<p>I am following the <a href=\"https:\/\/blog.dataversioncontrol.com\/data-version-control-tutorial-9146715eda46\" rel=\"nofollow noreferrer\">tutorial<\/a> about <a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"nofollow noreferrer\">Data Version Control<\/a> using <code>mingw32<\/code> on Windows 7.<\/p>\n\n<p>I am getting very strange error when I try to use <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/run\" rel=\"nofollow noreferrer\">run<\/a>:<\/p>\n\n<pre><code>$ dvc run -v echo \"hello\"\nDebug: updater is not old enough to check for updates\nDebug: PRAGMA user_version;\nDebug: fetched: [(2,)]\nDebug: CREATE TABLE IF NOT EXISTS state (inode INTEGER PRIMARY KEY, mtime TEXT NOT NULL, md5 TEXT NOT NULL, timestamp TEXT NOT NULL)\nDebug: CREATE TABLE IF NOT EXISTS state_info (count INTEGER)\nDebug: CREATE TABLE IF NOT EXISTS link_state (path TEXT PRIMARY KEY, inode INTEGER NOT NULL, mtime TEXT NOT NULL)\nDebug: INSERT OR IGNORE INTO state_info (count) SELECT 0 WHERE NOT EXISTS (SELECT * FROM state_info)\nDebug: PRAGMA user_version = 2;\nRunning command:\n        echo hello\n\/c: \/c: Is a directory\nDebug: SELECT count from state_info WHERE rowid=1\nDebug: fetched: [(1,)]\nDebug: UPDATE state_info SET count = 1 WHERE rowid = 1\nError: Traceback (most recent call last):\n  File \"dvc\\command\\run.py\", line 18, in run\n  File \"dvc\\project.py\", line 265, in run\n  File \"dvc\\stage.py\", line 435, in run\nStageCmdFailedError: Stage 'Dvcfile' cmd echo hello failed\n\nError: Failed to run command: Stage 'Dvcfile' cmd echo hello failed\n<\/code><\/pre>\n\n<h3>Question:<\/h3>\n\n<p>Where does the <code>\/c: \/c: Is a directory<\/code> come from?  How can I fix it? <\/p>\n\n<h3>My findings<\/h3>\n\n<ol>\n<li><p>I supposed that it was resolving path to echo, but ech is a builtin.<\/p>\n\n<pre><code>$ type echo\necho is a shell builtin\n<\/code><\/pre>\n\n<p>I tried also with <code>exit<\/code> and <code>cd<\/code> but I am getting the same error.<\/p><\/li>\n<li><p>Calling commands without dvc works fine.<\/p><\/li>\n<li><p><code>dvc<\/code> with <code>--no-exec<\/code> flag works fine, but when later executed with <code>repro<\/code> gives the same error. <\/p><\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1539857086453,
        "Question_score":4,
        "Question_tags":"windows|mingw|dvc",
        "Question_view_count":109,
        "Owner_creation_time":1508231047660,
        "Owner_last_access_time":1663539768267,
        "Owner_location":"Krak\u00f3w, Poland",
        "Owner_reputation":860,
        "Owner_up_votes":658,
        "Owner_down_votes":18,
        "Owner_views":118,
        "Question_last_edit_time":1539881078543,
        "Answer_body":"<p>I'm one of the dvc developers. Similar error has affected dvc running on cygwin. We've released a fix for it in <code>0.20.0<\/code>. Please upgrade.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1540629268432,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52871630",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":60861593,
        "Question_title":"How do I specify encryption type when using s3remote for DVC",
        "Question_body":"<p>I have just started to explore DVC. I am trying with s3 as my DVC remote. I am getting <\/p>\n\n<p>But when I run the <code>dvc push<\/code> command, I get the generic error saying <\/p>\n\n<pre><code>An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n<\/code><\/pre>\n\n<p>which I know for a fact that I get that error when I don't specify the encryption.<\/p>\n\n<p>It is similar to running <code>aws s3 cp<\/code> with <code>--sse<\/code> flag or specifying <code>ServerSideEncryption<\/code> when using boto3 library. How can I specify the encryption type when using DVC. Coz underneath DVC uses boto3 so there must be an easy way to do this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1585201508167,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":293,
        "Owner_creation_time":1491917519307,
        "Owner_last_access_time":1648587168753,
        "Owner_location":null,
        "Owner_reputation":1756,
        "Owner_up_votes":82,
        "Owner_down_votes":5,
        "Owner_views":199,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60861593",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72665109,
        "Question_title":"dvc.exceptions.CyclicGraphError: Pipeline has a cycle involving: load_extract_save",
        "Question_body":"<pre><code>stages:\n  load_extract_save: \n    cmd: python src\/stage_01_load_extract_save.py --config=config\/config.yaml\n    deps:\n      - config\/config.yaml\n      - src\/utils\/all_utils.py\n      - src\/stage_01_load_extract_save.py\n      - artifacts\/data\n    outs:\n      - artifacts\/data\n      - artifacts\/clean_data\/X.npy\n      - artifacts\/clean_data\/Y.npy\n\n  train_test_split_save:\n    cmd: python src\/stage_02_train_test_split_save.py --config=config\/config.yaml --params=params.yaml\n    deps:\n      - artifacts\/clean_data\/X.npy\n      - artifacts\/clean_data\/Y.npy\n      - src\/utils\/all_utils.py\n      - params.yaml\n      - config\/config.yaml\n      - src\/stage_02_train_test_split_save.py\n    outs:\n      - artifacts\/train_data\/X_train.npy\n      - artifacts\/train_data\/Y_train.npy\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n  \n  train_model:\n    cmd:  python src\/stage_03_train.py --config=config\/config.yaml --params=params.yaml\n    deps:\n      - artifacts\/train_data\/X_train.npy\n      - artifacts\/train_data\/Y_train.npy\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n      - src\/stage_03_train.py\n      - src\/utils\/all_utils.py\n      - config\/config.yaml\n      - params.yaml\n    outs:\n      - artifacts\/checkpoints\n      - artifacts\/model\n  \n  metrics:\n    cmd: python src\/stage_04_metrics.py --config=config\/config.yaml\n    deps:\n      - src\/stage_04_metrics.py\n      - config\/config.yaml\n      - src\/utils\/all_utils.py\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n      - artifacts\/checkpoints\n      - artifacts\/model\n    outs:\n      - confusion_matrix.png\n<\/code><\/pre>\n<p>This is my DVC.yaml.<\/p>\n<p>I have created Github workflow to reproduce it, but whenever I run it it gives me the following error - <code>... ERROR: Pipeline has a cycle involving: load_extract_save.<\/code><\/p>\n<p>The error looks <a href=\"https:\/\/i.stack.imgur.com\/1mt1Y.png\" rel=\"nofollow noreferrer\">like this<\/a>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655502527920,
        "Question_score":2,
        "Question_tags":"python|dvc",
        "Question_view_count":38,
        "Owner_creation_time":1587308895270,
        "Owner_last_access_time":1663873987327,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1655524559503,
        "Answer_body":"<p>Stage <code>load_extract_save<\/code> both outputs and depends on the same path (<code>artifacts\/data<\/code>). That's a cycle.<\/p>\n<p>Pipeline structures should be <a href=\"https:\/\/dvc.org\/doc\/command-reference\/dag#directed-acyclic-graph\" rel=\"nofollow noreferrer\">directed <strong>acyclical<\/strong> graphs<\/a>, otherwise <code>dvc repro<\/code> could execute that stage over and over forever.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655524277716,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72665109",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67744934,
        "Question_title":"Is it possible to check that the version of a file tracked by a DVC metadata file exists in remote storage without pulling the file?",
        "Question_body":"<p>My team has a set up wherein we track datasets and models in DVC, and have a GitLab repository for tracking our code and DVC metadata files. We have a job in our dev GitLab pipeline (run on each push to a merge request) that has the goal of checking to be sure that the developer remembered to run <code>dvc push<\/code> to keep DVC remote storage up-to-date. Right now, the way we do this is by running <code>dvc pull<\/code> on the GitLab runner, which will fail with errors telling you which files (new files or latest versions of existing files) were not found.<\/p>\n<p>The downside to this approach is that we are loading the entirety of our data stored in DVC onto a GitLab runner, and we've run into out-of-memory issues, not to mention lengthy run time to download all that data. Since the path and md5 hash of the objects are stored in the DVC metadata files, I would think that's all the information that DVC would need to be able to answer the question &quot;is the remote storage system up-to-date&quot;.<\/p>\n<p>It seems like <code>dvc status<\/code> is similar to what I'm asking for, but compares the cache or workspace and remote storage. In other words, it requires the files to actually be present on whatever filesystem is making the call.<\/p>\n<p>Is there some way to achieve the goal I laid out above (&quot;inform the developer that they need to run <code>dvc push<\/code>&quot;) without pulling everything from DVC?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622232629793,
        "Question_score":5,
        "Question_tags":"git|gitlab|continuous-integration|dvc",
        "Question_view_count":488,
        "Owner_creation_time":1618255062697,
        "Owner_last_access_time":1645555346683,
        "Owner_location":null,
        "Owner_reputation":75,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1622257491983,
        "Answer_body":"<blockquote>\n<p>It seems like dvc status is similar to what I'm asking for<\/p>\n<\/blockquote>\n<p><code>dvc status --cloud<\/code> will give you a list of &quot;new&quot; files if they that haven't been pushed to the (default) remote. It won't error out though, so your CI script should fail depending on the stdout message.<\/p>\n<p>More info: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/status#options\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/status#options<\/a><\/p>\n<p>I'd also ask everyone to run <code>dvc install<\/code>, which will setup some Git hooks, including automatic <code>dvc push<\/code> with <code>git push<\/code>.<\/p>\n<p>See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/install\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/install<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1622257759209,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1622503453296,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67744934",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":54324146,
        "Question_title":"Azure DataLake with DVC",
        "Question_body":"<p>We are thinking to use DVC for versioning input data for DataScience project.\nmy data resides in Azure DataLake Gen1.<\/p>\n\n<p>how do i configure DVC to push data to Azure DataLake using Service Principal?\ni want DVC to store cache and data into Azure DataLake instead on local disk.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1548236331560,
        "Question_score":5,
        "Question_tags":"azure-data-lake|service-principal|dvc",
        "Question_view_count":473,
        "Owner_creation_time":1257862078920,
        "Owner_last_access_time":1662705544550,
        "Owner_location":"Pune, India",
        "Owner_reputation":6219,
        "Owner_up_votes":227,
        "Owner_down_votes":7,
        "Owner_views":587,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54324146",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67215839,
        "Question_title":"Use parameters from additional configs in dvc 2.0",
        "Question_body":"<p>Using dvc version 2.0.18 and python 3.9.2 I want to use parameters defined in a config file different from params.yaml when configuring the parameters of the stages in <code>dvc.yaml<\/code>. However, it does not work as I expected.<\/p>\n<p>MWE:\nGit repo + dvc init:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 dvc.yaml\n\u251c\u2500\u2500 preproc.yaml\n\u2514\u2500\u2500 test.py\n<\/code><\/pre>\n<p>dvc.yaml:<\/p>\n<pre><code>vars:\n  - preproc.yaml\nstages:\n  test:\n    cmd: python test.py\n    deps:\n      - test.py\n    params:\n      - important_parameter\n<\/code><\/pre>\n<p>preproc.yaml:<\/p>\n<pre><code>important_parameter: 123\n<\/code><\/pre>\n<p>Running <code>dvc repro<\/code> lead to the following error:<\/p>\n<pre><code>ERROR: failed to reproduce 'dvc.yaml': dependency 'params.yaml' does not exist\n<\/code><\/pre>\n<p>Creating a dummy params.yaml without content gives:<\/p>\n<pre><code>WARNING: 'params.yaml' is empty.\nERROR: failed to reproduce 'dvc.yaml': Parameters 'important_parameter' are missing from 'params.yaml'.\n<\/code><\/pre>\n<p>What am I missing? Is this possible at all with the templating feature?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1619103654467,
        "Question_score":2,
        "Question_tags":"python|dvc",
        "Question_view_count":588,
        "Owner_creation_time":1542537900087,
        "Owner_last_access_time":1663940500713,
        "Owner_location":null,
        "Owner_reputation":147,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I think you don't need the templating feature in this case. As shown in this <a href=\"https:\/\/dvc.org\/doc\/command-reference\/params#examples-python-parameters-file\" rel=\"nofollow noreferrer\">example<\/a>:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>stages:\n  train:\n    cmd: python train.py\n    deps:\n      - users.csv\n    params:\n      - params.py:\n          - BOOL\n          - INT\n          - TrainConfig.EPOCHS\n          - TrainConfig.layers\n    outs:\n      - model.pkl\n<\/code><\/pre>\n<p>The way to redefine the default <code>params.yaml<\/code> is to specify the file name explicitly in the <code>params:<\/code> section:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>params:\n  - preproc.yaml:\n    - important_parameter\n<\/code><\/pre>\n<p>Also, when you create a stage either with <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run\" rel=\"nofollow noreferrer\"><code>dvc run<\/code><\/a> (not recommended) or <a href=\"https:\/\/dvc.org\/doc\/command-reference\/stage\/add\" rel=\"nofollow noreferrer\"><code>dvc stage add<\/code><\/a>, you can provide the params file name explicitly as a prefix:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc run -n train -d train.py -d logs\/ -o users.csv -f \\\n          -p parse_params.yaml:threshold,classes_num \\\n          python train.py\n<\/code><\/pre>\n<p>Here ^^ <code>parse_params.yaml<\/code> is a custom params file.<\/p>\n<p>Please, let me know if it solves the problem and if you have any other questions :)<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1619127242127,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67215839",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67891465,
        "Question_title":"ERROR: bad DVC file name 'Training_Batch_Files\\Wafer12_20012.csv.dvc' is git-ignored",
        "Question_body":"<p>Getting the error &quot;<em>ERROR: bad DVC file name 'Training_Batch_Files\\Wafer12_20012.csv.dvc' is git-ignored.<\/em>&quot; while trying to add local files for tracking<\/p>\n<p>Python Version : 3.7<\/p>\n<p>Library used:<\/p>\n<p><code>pip install dvc  pip install dvc[gdrive]   dvc init   <\/code><\/p>\n<p><strong>dvc add -R Training_Batch_Files<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/AVw9i.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AVw9i.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1623171603727,
        "Question_score":3,
        "Question_tags":"python|git|dvc",
        "Question_view_count":1844,
        "Owner_creation_time":1593952964623,
        "Owner_last_access_time":1664007410690,
        "Owner_location":null,
        "Owner_reputation":139,
        "Owner_up_votes":23,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67891465",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73565648,
        "Question_title":"DVC shows files not tracked in source control in visual studio code",
        "Question_body":"<p>I'm using DVC extension in VScode inside a python project. The problem is that dvc shows files not tracked by dvc in the source control panel! As in the following picture.\nDVC track only data folder and not the src folder. How can I fix it? Have you also encountered these problems?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/sn8YY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sn8YY.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1662017466370,
        "Question_score":0,
        "Question_tags":"visual-studio-code|dvc",
        "Question_view_count":39,
        "Owner_creation_time":1580668804397,
        "Owner_last_access_time":1664031434117,
        "Owner_location":null,
        "Owner_reputation":498,
        "Owner_up_votes":453,
        "Owner_down_votes":9,
        "Owner_views":66,
        "Question_last_edit_time":1662021092696,
        "Answer_body":"<p>The files shown are completely untracked. They are shown in both SCM trees so you can add them to either Git or DVC using inline actions.\nOnce the files are tracked by one of the tools they should only show up under the appropriate tree.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1662022756172,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1662022916689,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73565648",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67635688,
        "Question_title":"Installation DVC on MinIO storage",
        "Question_body":"<p>Does anybody install DVC on MinIO storage?<\/p>\n<p>I have read <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#available-parameters-per-storage-type\" rel=\"nofollow noreferrer\">docs<\/a> but not all clear for me.<\/p>\n<p>Which command should I use for setup MinIO storage with this entrance parameters:<\/p>\n<p>storage url: <a href=\"https:\/\/minio.mysite.com\/minio\/bucket-name\/\" rel=\"nofollow noreferrer\">https:\/\/minio.mysite.com\/minio\/bucket-name\/<\/a>\nlogin: my_login\npassword: my_password<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1621595380413,
        "Question_score":5,
        "Question_tags":"python|minio|dvc",
        "Question_view_count":1547,
        "Owner_creation_time":1526481416047,
        "Owner_last_access_time":1663922031783,
        "Owner_location":null,
        "Owner_reputation":85,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1621622008696,
        "Answer_body":"<p><strong>Install<\/strong><\/p>\n<p>I usually use it as a Python package, int this case you need to install:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install &quot;dvc[s3]&quot;\n<\/code><\/pre>\n<p><strong>Setup remote<\/strong><\/p>\n<p>By default DVC supports AWS S3 storages and they work fine.<br \/>\nAlso they support &quot;S3-compatible storage&quot;, but setup for this type of remotes is nod described properly. In particular case of MinIO you have <strong>bucket<\/strong> - directory on MinIO server where actual data stores (it is similar to AWS bucket), but DVC uses AWS CLI to authenticate. In case of MinIO you need to pass them explicitly.<\/p>\n<p>Then follow commands to setup your DVC remote:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code># setup default remote (change &quot;bucket-name&quot; to your minio backet name)\ndvc remote add -d minio s3:\/\/bucket-name -f\n\n# add information about storage url (where &quot;https:\/\/minio.mysite.com&quot; your url)\ndvc remote modify minio endpointurl https:\/\/minio.mysite.com\n\n#  add info about login and password\ndvc remote modify minio access_key_id my_login\ndvc remote modify minio secret_access_key my_password\n<\/code><\/pre>\n<p><strong>If you move from old remote<\/strong>, use follow command to move your data:<\/p>\n<p>Before setup (download all old remote cache to local machine):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc pull -r &lt;old_remote_name&gt; --all-commits --all-tags --all-branches\n<\/code><\/pre>\n<p>After setup (upload all cache to a new remote):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc push -r &lt;new_remote_name&gt; --all-commits --all-tags --all-branches\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1621599285543,
        "Answer_score":6.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67635688",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":65847574,
        "Question_title":"Failed to pull existing files from SSH DVC Remote",
        "Question_body":"<p>After running <code>dvc push data.csv<\/code> (to ssh-remote), when i try to dvc-pull the same file on another machine from the same remote, it won't get pulled. Below are the logs and the error:<\/p>\n<pre><code>2021-01-21 22:17:26,643 DEBUG: checking if 'data.csv'('HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)') has changed.\n2021-01-21 22:17:26,643 DEBUG: 'data.csv' doesn't exist.\n2021-01-21 22:17:26,644 WARNING: Cache 'HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)' not found. File 'data.csv' won't be created.\n2021-01-21 22:17:26,644 DEBUG: cache '\/usr\/src\/bohr\/.dvc\/cache\/27\/9936268f488e1e613f81a537f29055' expected 'HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)' actual 'None'\n...\n2021-01-21 22:17:26,660 ERROR: failed to pull data from the cloud - Checkout failed for following targets:\ndata.csv\n<\/code><\/pre>\n<p>However, the file is present on the remote:<\/p>\n<pre><code>$ ls -la ~\/.dvcstorage\/bohr\/27\/9936268f488e1e613f81a537f29055\n-rw-rw-r-- 1 hbabii hbabii 1458311 Jan 22 00:19 \/home\/hbabii\/.dvcstorage\/bohr\/27\/9936268f488e1e613f81a537f29055\n<\/code><\/pre>\n<p>I double-checked that I am pulling from and pushing to the same remote. I am using DVC v1.11.11.<\/p>\n<p>Could you please give me any hints on what could be wrong?<\/p>\n<p>Cheers, Hlib<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1611327752360,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":1715,
        "Owner_creation_time":1355002392777,
        "Owner_last_access_time":1663450068853,
        "Owner_location":"Bolzano, Italia",
        "Owner_reputation":530,
        "Owner_up_votes":282,
        "Owner_down_votes":3,
        "Owner_views":91,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In the end, the problem was that I indeed was pulling from the wrong remote (I had multiple remotes, their configuration was tricky, and local configurations differed on different machines).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1616082756536,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65847574",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72651603,
        "Question_title":"Adding files that rely on pipeline outputs",
        "Question_body":"<p>In my workflow, I do the following:<\/p>\n<ol>\n<li>Acquire raw data (e.g. a video containing people)<\/li>\n<li>Transform it (e.g. automatically extract all crops with faces)<\/li>\n<li>Manually label them (e.g. identify the person in each crop). The labels are stored in json files along with the crops.<\/li>\n<li>Train a model on these data.<\/li>\n<\/ol>\n<p><strong>How should I track this pipeline with DVC?<\/strong><\/p>\n<p>My concerns:<\/p>\n<ol>\n<li>If stage 2 is changed (e.g. crops are extracted with a different size), the manual data should be invalidated (and so should the final model).<\/li>\n<li>The 3rd step is manual and therefore not precisely reproducible. But I do need its input to be reproducible.<\/li>\n<li>Stage 4 has an element of randomness, so it's not precisely reproducible either.<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1655411665790,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":39,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":1655415427550,
        "Answer_body":"<p>Stage 3 is manual so you can't really codify it or automate it, nor guarantee its reproducibility (due to possible human error). But there's a way to get you as close as possible:<\/p>\n<p>You could replace it with a helper script that just checks whether all the labels are annotated. If so, output a text file with content &quot;green&quot;, otherwise &quot;red&quot; (for example) and error out.<\/p>\n<p>Stage 4 should depend on both the inputs from stages 2 and 3, so it will only run if BOTH the face crops changed AND if they are thoroughly annotated.\nInternally, it first checks the semaphore file (from 3) and dies on red. On green, it trains the model :)<\/p>\n<p>The <a href=\"https:\/\/dvc.org\/doc\/command-reference\/dag#directed-acyclic-graph\" rel=\"nofollow noreferrer\">DAG<\/a> looks like this:<\/p>\n<pre><code>          +-----------+       \n          | 1-acquire |       \n          +-----------+       \n                *          \n                *          \n                *          \n          +---------+       \n          | 2-xform |       \n          +---------+       \n you      **        **     \n   --&gt;  **            **   \n       *                ** \n+---------+               *\n| 3-check |             ** \n+---------+           **   \n          **        **     \n            **    **       \n              *  *         \n          +---------+      \n          | 4-train |      \n          +---------+      \n<\/code><\/pre>\n<blockquote>\n<p>re randomness: while not ideal, non-determinism technically only <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#avoiding-unexpected-behavior\" rel=\"nofollow noreferrer\">affects intermediate stages<\/a> of the pipeline, because it causes everything after that to always run. In this case, since it's in the last stage, it won't affect DVC's job.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655523571800,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1655706111940,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72651603",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":69725612,
        "Question_title":"Is the default DVC behavior to store connection data in git?",
        "Question_body":"<p>I've recently started to play with <a href=\"https:\/\/dvc.org\" rel=\"nofollow noreferrer\">DVC<\/a>, and I was a bit surprised to see the <a href=\"https:\/\/dvc.org\/doc\/start\/data-and-model-versioning#storing-and-sharing\" rel=\"nofollow noreferrer\">getting started docs<\/a> are suggesting to store <code>.dvc\/config<\/code> in git.<\/p>\n<p>This seemed like a fine idea at first, but then I noticed that my Azure Blob Storage account (i.e. my Azure username) is also stored in .dvc\/config, which means it would end up in git. Making it not ideal for team collaboration scenarios.<\/p>\n<p>What's even less ideal (read: really scary) is that connection strings entered using <code>dvc remote modify blah connection_string ...<\/code> also end up in <code>.dvc\/config<\/code>, making them end up in git and, in the case of open source projects, making them end up in <strong>very<\/strong> interesting places.<\/p>\n<p>Am I doing something obviously wrong? I wouldn't expect the getting started docs to go very deep into security issues, but I wouldn't expect them to store connection strings in source control either.<\/p>\n<p>My base assumption is that I'm misunderstanding\/misconfiguring something, I'd be curious to know what.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635260868803,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":77,
        "Owner_creation_time":1250158552417,
        "Owner_last_access_time":1663847198323,
        "Owner_location":"Romania",
        "Owner_reputation":7916,
        "Owner_up_votes":1735,
        "Owner_down_votes":33,
        "Owner_views":801,
        "Question_last_edit_time":null,
        "Answer_body":"<p>DVC has few &quot;levels&quot; of config, that can be controlled with proper flag:<\/p>\n<ul>\n<li><code>--local<\/code> - repository level, ignored by git by default - designated for project-scope, sensitive data<\/li>\n<li>project - same as above, not ignored - designated to specify non-sensitive data (it is the default)<\/li>\n<li><code>--global<\/code> \/ <code>--system<\/code> - for common config for more repositories.<\/li>\n<\/ul>\n<p>More information can be found in the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#description\" rel=\"nofollow noreferrer\">docs<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1635332461183,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1635332764020,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69725612",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70711015,
        "Question_title":"Is there an alternative to DVC pipelines to create a DAG which is also aware of inputs\/outputs to nodes to cache results?",
        "Question_body":"<p>I recently started to use DVC pipelines to create DAG in my application. I work on Machine Learning projects, and I need to experiment a lot with different nodes of my system. For example:<\/p>\n<p><code>Data preprocessing -&gt; feature extraction -&gt; model training -&gt; model evaluation<\/code><\/p>\n<p>Each node produces an output, and the output of each node is used in another node. What DVC allows me to do is to create a pipeline in which I can specify dependencies between nodes. I also use <code>.yaml<\/code> files to configure parameters of my application, and you can also specify these parameters as dependencies for different nodes. So, whenever a dependency changes between nodes (it can be either configuration parameters or inputs\/outputs specified), DVC is able to detect this, and run the necessary parts of the pipeline. If a dependency hasn't changed for a particular node, DVC can use its cache to skip that step. This is really useful for me, since some nodes take really long time to execute, and they don't always need to be ran (if their dependencies hasn't changed).<\/p>\n<p>I also started to use hydra to manage my config files, and to be honest, DVC doesn't work well with hydra. It expects a static config to specify parameter dependencies, and with hydra it is a bit tricky to do, and complicate things.<\/p>\n<p>My question is: is there any alternative to DVC Pipelines which also goes well with hydra?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1642165507820,
        "Question_score":1,
        "Question_tags":"pipeline|directed-acyclic-graphs|dvc|hydra-core",
        "Question_view_count":181,
        "Owner_creation_time":1548055925357,
        "Owner_last_access_time":1664039002623,
        "Owner_location":"Warsaw, Poland",
        "Owner_reputation":551,
        "Owner_up_votes":9,
        "Owner_down_votes":2,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70711015",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":64456396,
        "Question_title":"How do I unit test a function in the CI pipeline that uses model files that are not part of the git remote?",
        "Question_body":"<p>I am developing machine learning repositories that require fairly large trained model files to run. These files are not part of the git remote but is tracked by DVC and is saved in a separate remote storage. I am running into issues when I am trying to run unit tests in the CI pipeline for functions that require these model files to make their prediction. Since I don't have access them in the git remote, I can't test them.<\/p>\n<p>What is the best practice that people usually do in this situation? I can think of couple of options -<\/p>\n<ul>\n<li>Pull the models from the DVC remote inside the CI pipeline. I don't want to do this becasue downloading models every time you want to run push some code will quickly eat up my usage minutes for CI and is an expensive option.<\/li>\n<li>Use <code>unittest.mock<\/code> to simulate the output of from the model prediction and test other parts of my code. This is what I am doing now but it's sort of a pain with unittest's mock functionalities. That module wasn't really developed with ML in mind from what I can tell. It's missing (or is hard to find) some functionalities that I would have really liked. Are there any good tools for doing this geared specifically towards ML?<\/li>\n<li>Do weird reformatting of the function definition that allows me to essentially do option 2 but without a mock module. That is, just test the surrounding logic and don't worry about the model output.<\/li>\n<li>Just put the model files in the git remote and be done with it. Only use DVC to track data.<\/li>\n<\/ul>\n<p>What do people usually do in this situation?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603252093370,
        "Question_score":4,
        "Question_tags":"unit-testing|machine-learning|mocking|continuous-integration|dvc",
        "Question_view_count":289,
        "Owner_creation_time":1446746840593,
        "Owner_last_access_time":1664083975707,
        "Owner_location":null,
        "Owner_reputation":2545,
        "Owner_up_votes":845,
        "Owner_down_votes":386,
        "Owner_views":382,
        "Question_last_edit_time":1603305923907,
        "Answer_body":"<p>If we talk about unit tests, I think it's indeed better to do a mock. It's best to have unit tests small, testing actual logic of the unit, etc. It's good to have other tests though that would pull the model and run some logic on top of that - I would call them integration tests.<\/p>\n<p>It's not black and white though. If you for some reason see that it's easier to use an actual model (e.g. it changes a lot and it is easier to use it instead of maintaining and updating stubs\/fixtures), you could potentially cache it.<\/p>\n<p>I think, to help you with the mock, you would need to share some technical details- how does the function look like, what have you tried, what breaks, etc.<\/p>\n<blockquote>\n<p>to do this because downloading models every time you want to run push some code will quickly eat up my usage minutes for CI and is an expensive option.<\/p>\n<\/blockquote>\n<p>I think you can potentially utilize CI systems cache to avoid downloading it over and over again. This is the GitHub Actions related <a href=\"https:\/\/github.com\/actions\/cache#cache-limits\" rel=\"nofollow noreferrer\">repository<\/a>, this is <a href=\"https:\/\/circleci.com\/docs\/2.0\/caching\" rel=\"nofollow noreferrer\">CircleCI<\/a>. The idea is the same across all common CI providers. Which one are considering to use, btw?<\/p>\n<blockquote>\n<p>Just put the model files in the git remote and be done with it. Only use DVC to track data.<\/p>\n<\/blockquote>\n<p>This can be the way, but if models are large enough you will pollute Git history significantly. On some CI systems it can become even slower since they will be fetching this with regular <code>git clone<\/code>. Effectively, downloading models anyway.<\/p>\n<p>Btw, if you use DVC or not take a look at another open-source project that is made specifically to do CI\/CD for ML - <a href=\"https:\/\/cml.dev\" rel=\"nofollow noreferrer\">CML<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1603314290769,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1603325349590,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64456396",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56269391,
        "Question_title":"Readding missing files to DVC",
        "Question_body":"<p>A ran into problem with DVC when some files are missing in remote. For example when I execute <code>dvc pull<\/code> I get the output<\/p>\n\n<pre><code>[##############################] 100% Analysing status.\nWARNING: Cache 'c31bcdd6910977a0e3a86446f2f3bdaa' not found. File 'data\/2.mp4' won't be created.\nWARNING: Cache '77186c4596da7dbc85fefec6d0779049' not found. File 'data\/3.mp4' won't be created.\n<\/code><\/pre>\n\n<p>The <code>dvc status<\/code> command gives me:<\/p>\n\n<pre><code>data\/2.mp4.dvc:\n    changed outs:\n        not in cache:       data\/2.mp4\ndata\/3.mp4.dvc:\n    changed outs:\n        not in cache:       data\/3.mp4\n<\/code><\/pre>\n\n<p>It seems that <code>2.mp4<\/code> and <code>3.mp4<\/code> where added under dvc control but <code>dvc push<\/code> command has not been executed.<\/p>\n\n<p>I have access to the original mp4 files and I have tried to readd them. I copied mp4 files to data folder and executed the command:<\/p>\n\n<pre><code>dvc remove data\/2.mp4.dvc\ndvc remove data\/3.mp4.dvc\n\ndvc add data\/2.mp4 \ndvc add data\/3.mp4 \n<\/code><\/pre>\n\n<p>But there is no effect. How can I remove files from under dvc control and add them again?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1558593719503,
        "Question_score":4,
        "Question_tags":"dvc",
        "Question_view_count":1065,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":1558637203487,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56269391",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":69265000,
        "Question_title":"DVC - Forbidden: An error occurred (403) when calling the HeadObject operation",
        "Question_body":"<p>I just started with DVC. following are the steps I am doing to push my models on S3<\/p>\n<p>Initialize<\/p>\n<pre><code>dvc init\n<\/code><\/pre>\n<p>Add bucket url<\/p>\n<pre><code>dvc remote add -d storage s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>\n<p>add some files<\/p>\n<pre><code>dvc add somefiles\n<\/code><\/pre>\n<p>Add aws keys<\/p>\n<pre><code>dvc remote modify storage access_key_id AWS_ACCESS_KEY_ID\ndvc remote modify storage secret_access_key AWS_SECRET_ACCESS_KEY\n<\/code><\/pre>\n<p>now when I push<\/p>\n<pre><code>dvc push\n<\/code><\/pre>\n<p>it shows<\/p>\n<pre><code>ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n<p>Am i missing something?<\/p>\n<p><strong>update1<\/strong><\/p>\n<p>result of <code>dvc doctor<\/code><\/p>\n<pre><code>C:\\my-server&gt;dvc doctor\nDVC version: 2.7.4 (pip)\n---------------------------------\nPlatform: Python 3.8.0 on Windows-10-10.0.19041-SP0\nSupports:\n        http (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        https (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        s3 (s3fs = 2021.8.1, boto3 = 1.17.106)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: s3\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n<\/code><\/pre>\n<p>and the <code>dvc push-vv<\/code><\/p>\n<pre><code>C:\\my-server&gt;dvc push -vv  \n2021-09-21 13:21:38,382 TRACE: Namespace(all_branches=False, all_commits=False, all_tags=False, cd='.', cmd='push', cprofile=False, cprofile_dump=None, func=&lt;class 'dvc.command.data_sync.CmdDataPush'&gt;, glob=False, instrument=False, instrument_open=False, jobs=None, pdb=False, quiet=0, recursive=False, remote=None, run_cache=False, targets=[], verbose=2, version=None, with_deps=False)\n2021-09-21 13:21:39,293 TRACE: Assuming 'C:\\my-server\\.dvc\\cache\\02\\5b196462b86d2f10a9f659e2224da8.dir' is unchanged since \nit is read-only\n2021-09-21 13:21:39,296 TRACE: Assuming 'C:\\my-server\\.dvc\\cache\\02\\5b196462b86d2f10a9f659e2224da8.dir' is unchanged since \nit is read-only\n2021-09-21 13:21:40,114 DEBUG: Preparing to transfer data from '.dvc\\cache' to 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,117 DEBUG: Preparing to collect status from 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,119 DEBUG: Collecting status from 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,121 DEBUG: Querying 1 hashes via object_exists\n2021-09-21 13:21:44,840 ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 248, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\aiobotocore\\client.py&quot;, line 155, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (InvalidAccessKeyId) when calling the ListObjectsV2 operation: The AWS Access Key Id you provided does not exist in our records.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1080, in _info\n    out = await self._simple_info(path)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 993, in _simple_info\n    out = await self._call_s3(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 268, in _call_s3\n    raise err\nPermissionError: The AWS Access Key Id you provided does not exist in our records.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 248, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\aiobotocore\\client.py&quot;, line 155, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\main.py&quot;, line 55, in main\n    ret = cmd.do_run()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\command\\base.py&quot;, line 45, in do_run\n    return self.run()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\command\\data_sync.py&quot;, line 57, in run\n    processed_files_count = self.repo.push(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\repo\\__init__.py&quot;, line 50, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\repo\\push.py&quot;, line 48, in push\n    pushed += self.cloud.push(obj_ids, jobs, remote=remote, odb=odb)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\data_cloud.py&quot;, line 85, in push\n    return transfer(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\transfer.py&quot;, line 153, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 160, in compare_status\n    dest_exists, dest_missing = status(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 122, in status\n    exists = hashes.intersection(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 48, in _indexed_dir_hashes\n    dir_exists.update(odb.list_hashes_exists(dir_hashes - dir_exists))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\db\\base.py&quot;, line 415, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 611, in result_iterator\n    yield fs.pop().result()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 439, in result\n    return self.__get_result()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 388, in __get_result\n    raise self._exception\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\db\\base.py&quot;, line 406, in exists_with_progress\n    ret = self.fs.exists(path_info)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\fs\\fsspec_wrapper.py&quot;, line 97, in exists\n    return self.fs.exists(self._with_bucket(path_info))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 88, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 69, in sync\n    raise result[0]\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 25, in _runner\n    result[0] = await coro\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 820, in _exists\n    await self._info(path, bucket, key, version_id=version_id)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1084, in _info\n    out = await self._version_aware_info(path, version_id)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1027, in _version_aware_info\n    out = await self._call_s3(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 268, in _call_s3\n    raise err\nPermissionError: Forbidden\n------------------------------------------------------------\n2021-09-21 13:21:45,178 DEBUG: Version info for developers:\nDVC version: 2.7.4 (pip)\n---------------------------------\nPlatform: Python 3.8.0 on Windows-10-10.0.19041-SP0\nSupports:\n        http (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        https (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        s3 (s3fs = 2021.8.1, boto3 = 1.17.106)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: s3\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2021-09-21 13:21:45,185 DEBUG: Analytics is enabled.\n2021-09-21 13:21:45,446 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\sgarg\\\\AppData\\\\Local\\\\Temp\\\\tmpm_p9f3eq']'\n2021-09-21 13:21:45,456 DEBUG: Spawned '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\sgarg\\\\AppData\\\\Local\\\\Temp\\\\tmpm_p9f3eq']'\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1632209467140,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":816,
        "Owner_creation_time":1363322632587,
        "Owner_last_access_time":1664084323070,
        "Owner_location":"Chandigarh, India",
        "Owner_reputation":13237,
        "Owner_up_votes":2454,
        "Owner_down_votes":19,
        "Owner_views":2675,
        "Question_last_edit_time":1632236873616,
        "Answer_body":"<p>Could you please run <code>dvc doctor<\/code> and rerun <code>dvc push<\/code> and add <code>-vv<\/code> flag. And give the two results?<\/p>\n<pre><code>PermissionError: The AWS Access Key Id you provided does not exist in our records.\n<\/code><\/pre>\n<p>Does the <code>aws cli<\/code> works correctly for you? First setup <code>AWS_ACCESS_KEY_ID<\/code> and <code>AWS_SECRET_ACCESS_KEY<\/code> in envs then<\/p>\n<pre><code>aws s3 ls s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1632210565512,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1632211831430,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69265000",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":57966851,
        "Question_title":"Undo 'dvc add' operation",
        "Question_body":"<p>I <code>dvc add<\/code>-ed a file I did not mean to add. I have not yet committed.<\/p>\n\n<p>How do I undo this operation? In Git, you would do <code>git rm --cached &lt;filename&gt;<\/code>.<\/p>\n\n<p>To be clear: I want to make DVC forget about the file, and I want the file to remain untouched in my working tree. This is the opposite of what <code>dvc remove<\/code> does.<\/p>\n\n<p>One <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1524\" rel=\"nofollow noreferrer\">issue<\/a> on the DVC issue tracker suggests that <code>dvc unprotect<\/code> is the right command. But reading the <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/unprotect\" rel=\"nofollow noreferrer\">manual page<\/a> suggests otherwise.<\/p>\n\n<p>Is this possible with DVC?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1568689927047,
        "Question_score":4,
        "Question_tags":"dvc",
        "Question_view_count":1304,
        "Owner_creation_time":1383611307000,
        "Owner_last_access_time":1664061570950,
        "Owner_location":"New York",
        "Owner_reputation":10846,
        "Owner_up_votes":1581,
        "Owner_down_votes":95,
        "Owner_views":984,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As per mroutis on the DVC Discord server:<\/p>\n\n<ol>\n<li><code>dvc unprotect<\/code> the file; this won't be necessary if you don't use <code>symlink<\/code> or <code>hardlink<\/code> caching, but it can't hurt.<\/li>\n<li>Remove the .dvc file<\/li>\n<li>If you need to delete the cache entry itself, run <code>dvc gc<\/code>, or look up the MD5 in <code>data.dvc<\/code> and manually remove it from <code>.dvc\/cache<\/code>.<\/li>\n<\/ol>\n\n<p><em>Edit<\/em> -- there is now an issue on their Github page to add this to the manual: <a href=\"https:\/\/github.com\/iterative\/dvc.org\/issues\/625\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc.org\/issues\/625<\/a><\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_time":1568693889196,
        "Answer_score":7.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1568725966083,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57966851",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73126208,
        "Question_title":"Python: Ssl Certificate verify failed",
        "Question_body":"<p>I have installed <code>dvc<\/code> on my <code>ubuntu-18.04-LTS<\/code> system and while trying to download the <code>data<\/code> files from github using dvc, it fails with below error.<\/p>\n<pre><code>$ dvc get https:\/\/github.com\/iterative\/dataset-registry get-started\/data.xml -o data\/data.xml -v\n\n2022-07-22 12:55:22,260 DEBUG: Creating external repo https:\/\/github.com\/iterative\/dataset-registry@None\n2022-07-22 12:55:22,260 DEBUG: erepo: git clone 'https:\/\/github.com\/iterative\/dataset-registry' to a temporary dir\n2022-07-22 12:55:23,683 DEBUG: Removing '\/dvc\/dvc_test\/data\/.UEeAzwmJCY3q85YQuCeahx'\n2022-07-22 12:55:23,684 ERROR: failed to get 'get-started\/data.xml' from 'https:\/\/github.com\/iterative\/dataset-registry' - Failed to clone repo 'https:\/\/github.com\/iterative\/dataset-registry' to '\/tmp\/tmpvmrmu9qsdvc-clone'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;urllib3\/connectionpool.py&quot;, line 703, in urlopen\n  File &quot;urllib3\/connectionpool.py&quot;, line 386, in _make_request\n  File &quot;urllib3\/connectionpool.py&quot;, line 1042, in _validate_conn\n  File &quot;urllib3\/connection.py&quot;, line 414, in connect\n  File &quot;urllib3\/util\/ssl_.py&quot;, line 449, in ssl_wrap_socket\n  File &quot;urllib3\/util\/ssl_.py&quot;, line 493, in _ssl_wrap_socket_impl\n  File &quot;ssl.py&quot;, line 500, in wrap_socket\n  File &quot;ssl.py&quot;, line 1040, in _create\n  File &quot;ssl.py&quot;, line 1309, in do_handshake\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;dvc\/scm.py&quot;, line 145, in clone\n  File &quot;scmrepo\/git\/__init__.py&quot;, line 143, in clone\n  File &quot;scmrepo\/git\/backend\/dulwich\/__init__.py&quot;, line 199, in clone\nscmrepo.exceptions.CloneError: Failed to clone repo 'https:\/\/github.com\/iterative\/dataset-registry' to '\/tmp\/tmpvmrmu9qsdvc-clone'\n<\/code><\/pre>\n<p>Already our corporate proxy certificate has been installed and traffic to <code>github.com<\/code> allowed I'm able to clone above repository separately on CLI. But with <code>dvc<\/code>the above errors are occurring, Even the below couldn't solve the issue.<\/p>\n<pre><code>$ python -c &quot;import ssl; print(ssl.get_default_verify_paths())&quot;\n\nDefaultVerifyPaths(cafile=None, capath='\/usr\/lib\/ssl\/certs', openssl_cafile_env='SSL_CERT_FILE', openssl_cafile='\/usr\/lib\/ssl\/cert.pem', openssl_capath_env='SSL_CERT_DIR', openssl_capath='\/usr\/lib\/ssl\/certs')\n<\/code><\/pre>\n<pre><code>export SSL_CERT_DIR=\/etc\/ssl\/certs\/\nexport REQUESTS_CA_BUNDLE=\/usr\/local\/lib\/python2.7\/dist-packages\/certifi\/cacert.pem\npip install --upgrade certifi\nexport PYTHONHTTPSVERIFY=0\n\nsudo apt install ca-certificates\nsudo update-ca-certificates --fresh\n<\/code><\/pre>\n<pre><code>$ python --version\nPython 2.7.17\n\n$ dvc doctor\nDVC version: 2.13.0 (deb)\n---------------------------------\nPlatform: Python 3.8.3 on Linux-5.4.0-92-generic-x86_64-with-glibc2.14\nSupports:\n        azure (adlfs = 2022.7.0, knack = 0.9.0, azure-identity = 1.10.0),\n        gdrive (pydrive2 = 1.10.1),\n        gs (gcsfs = 2022.5.0),\n        hdfs (fsspec = 2022.5.0, pyarrow = 8.0.0),\n        webhdfs (fsspec = 2022.5.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.5.1),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.5.1),\n        s3 (s3fs = 2022.5.0, boto3 = 1.21.21),\n        ssh (sshfs = 2022.6.0),\n        oss (ossfs = 2021.8.0),\n        webdav (webdav4 = 0.9.7),\n        webdavs (webdav4 = 0.9.7)\n<\/code><\/pre>\n<p>Tp bypass the ssl validation in git we have <code>git config http.sslVerify &quot;false&quot;<\/code> Similarly do we have option in dvc?<\/p>\n<p>Further what should i update to resolve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1658849780953,
        "Question_score":1,
        "Question_tags":"python|ssl|pip|ssl-certificate|dvc",
        "Question_view_count":157,
        "Owner_creation_time":1432810927473,
        "Owner_last_access_time":1664076190223,
        "Owner_location":null,
        "Owner_reputation":1609,
        "Owner_up_votes":68,
        "Owner_down_votes":0,
        "Owner_views":447,
        "Question_last_edit_time":1658898077489,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73126208",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67407702,
        "Question_title":"Corrupted dvc.lock",
        "Question_body":"<p>I'm using DAGsHub storage as a remote and running into the following error message (when trying to DVC pull):<\/p>\n<blockquote>\n<p>ERROR: Lockfile 'bias_tagging_model\/dvc.lock' is corrupted.<\/p>\n<\/blockquote>\n<p>I thought I might have messed something up, but when cloning the git repo again and DVC pulling I am still running into this.\nThe data looks ok when viewed in the browser.\nIf you have any ideas, I would appreciate your help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1620243318287,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":362,
        "Owner_creation_time":1620132740443,
        "Owner_last_access_time":1641927840700,
        "Owner_location":"New York, NY, USA",
        "Owner_reputation":75,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Usually, the reason for this error is the DVC version.<\/p>\n<p>If the dvc.lock file has a DVC 2.* schema and you are using a lower version, it will throw this error.<\/p>\n<p>Upgrade your DVC version, and it should work.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620292401492,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67407702",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":66409283,
        "Question_title":"updating data in dvc registry from other projects",
        "Question_body":"<p>I have a couple of projects that are using and updating the same data sources. I recently learned about <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries\" rel=\"nofollow noreferrer\">dvc's data registries<\/a>, which sound like a great way of versioning data across these different projects (e.g. scrapers, computational pipelines).<\/p>\n<p>I have put all of the relevant data into <code>data-registry<\/code> and then I imported the relevant files into the scraper project with:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ poetry run dvc import https:\/\/github.com\/username\/data-registry raw\n<\/code><\/pre>\n<p>where <code>raw<\/code> is a directory that stores the scraped data. This seems to have worked properly, but then when I went to build <a href=\"https:\/\/dvc.org\/doc\/start\/data-pipelines\" rel=\"nofollow noreferrer\">a dvc pipeline<\/a> that <em>outputted<\/em> data into a file that was already tracked by dvc, I got an error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ dvc run -n menu_items -d src\/ -o raw\/menu_items\/restaurant.jsonl scrapy crawl restaurant\nERROR: Paths for outs:                                                \n'raw'('raw.dvc')\n'raw\/menu_items\/restaurant.jsonl'('menu_items')\noverlap. To avoid unpredictable behaviour, rerun command with non overlapping outs paths.\n<\/code><\/pre>\n<p>Can someone help me understand what is going on here? <strong>What is the best way to use data registries to share and update data across projects?<\/strong><\/p>\n<p>I would ideally like to update the data-registry with new data from the scraper project and then allow other dependent projects to update their data when they are ready to do so.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1614516713937,
        "Question_score":3,
        "Question_tags":"data-management|dvc",
        "Question_view_count":388,
        "Owner_creation_time":1294268936687,
        "Owner_last_access_time":1661618392827,
        "Owner_location":"Chicago, IL",
        "Owner_reputation":2893,
        "Owner_up_votes":294,
        "Owner_down_votes":1,
        "Owner_views":168,
        "Question_last_edit_time":1614699218992,
        "Answer_body":"<p>When you <code>import<\/code> (or <code>add<\/code>) something into your project, a .dvc file is created with that lists that something (in this case the <code>raw\/<\/code> dir) as an &quot;output&quot;.<\/p>\n<p>DVC doesn't allow overlapping outputs among .dvc files or dvc.yaml stages, meaning that your &quot;menu_items&quot; stage shouldn't write to <code>raw\/<\/code> since it's already under the control of <code>raw.dvc<\/code>.<\/p>\n<p>Can you make a separate directory for the pipeline outputs? E.g. use <code>processed\/menu_items\/restaurant.jsonl<\/code><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1614537291720,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1614698988012,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66409283",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56285351,
        "Question_title":"Updating tracked dir in DVC",
        "Question_body":"<p>According to <a href=\"https:\/\/dvc.org\/doc\/user-guide\/update-tracked-file\" rel=\"nofollow noreferrer\">this tutorial<\/a> when I update file I should remove file from under DVC control first (i.e. execute <code>dvc unprotect &lt;myfile&gt;.dvc<\/code> or <code>dvc remove &lt;myfile&gt;.dvc<\/code>) and then add it again via <code>dvc add &lt;mifile&gt;<\/code>. However It's not clear if I should apply the same workflow for the directories.<\/p>\n\n<p>I have the directory under DVC control with the following structure:<\/p>\n\n<pre><code>data\/\n    1.jpg\n    2.jpg\n<\/code><\/pre>\n\n<p>Should I run <code>dvc unprotect data<\/code> every time the directory content is updated?<\/p>\n\n<p>More specifically I'm interested if I should run <code>dvc unprotect data<\/code> in the following use cases:<\/p>\n\n<ul>\n<li><strong>New file is added.<\/strong> For example if I put <code>3.jpg<\/code> image in the data dir<\/li>\n<li><strong>File is deleted.<\/strong> For example if I delete <code>2.jpg<\/code> image in the <code>data<\/code> dir<\/li>\n<li><strong>File is updated.<\/strong> For example if I edit <code>1.jpg<\/code> image via graphic editor.<\/li>\n<li>A combination of the previous use cases (i.e. some files are updated, other deleted and new files are added)<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1558667422490,
        "Question_score":4,
        "Question_tags":"dvc",
        "Question_view_count":995,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":1558708772616,
        "Answer_body":"<p>Only when file is updated - i.e. edit <code>1.jpg<\/code> with your editor <strong>AND<\/strong> only if hadrlink or symlink cache type is enabled.<\/p>\n\n<p>Please, check this <a href=\"https:\/\/dvc.org\/doc\/user-guide\/update-tracked-file\" rel=\"nofollow noreferrer\">link<\/a>:<\/p>\n\n<blockquote>\n  <p>updating tracked files has to be carried out with caution to avoid data corruption when the DVC config option cache.type is set to hardlink or\/and symlink<\/p>\n<\/blockquote>\n\n<p>I would strongly recommend reading this document: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/cache-file-linking\" rel=\"nofollow noreferrer\">Performance Optimization for Large Files<\/a> it explains benefits of using hardlinks\/symlinks.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1558674266680,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56285351",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":71663330,
        "Question_title":"What is the advantage of DVC, git-annex, git-lfs for large or binary files over git?",
        "Question_body":"<p>If I have different versions of a file, e.g., in different branches, and I try to reconcile those, git will has great mechanisms for that. However, in order to do the reconciliations, e.g., in a merge, git requires access to the &quot;inside&quot; of the file. Thus files should be text files.<\/p>\n<p>If I change a version controlled file, git does not save the delta between those files, but safes and entire snapshot of the file. If one makes a change, even a small change, to a large file, the entire files will be stored twice by git. Thus files should be small.<\/p>\n<p>Files that are either large or binary (or both), they should not be tracked by Git. If I still need them in my project, I should use something like DVC, git-annex, git-lfs.<\/p>\n<p>As far as I understand, all three of those keep the those other files outside of git, and keep a reference, which is tracked by git. I will use DVC as a stand-in, as I know even less about the other two.<\/p>\n<ol>\n<li><p>In DVC, the reference is a text file and thus, git will not get confused. However, since it is only a reference, there is not much merging to be done by git anyways. So, git's reconciliation-capabilities are not really required. What is the advantage of using DVC then regarding this aspect? Can't I just use git and just not use those mechanisms?<\/p>\n<\/li>\n<li><p>In DVC, it seems that if I change a large file, just like in git, a snapshot of that file is created (not a delta saved). So, how does this improve the situation compared to git? I still get lots of (near) copies of this big file.<\/p>\n<\/li>\n<\/ol>\n<p>I understand from <a href=\"https:\/\/stackoverflow.com\/a\/35578715\/4533188\">here<\/a> that git-lfs keeps most of the (near) copies of my file in the remote storage. Only if I checkout the respective version of the large file, the files is downloaded. In that case, while I would be correct about my point 2, at least it is only a &quot;problem&quot; of the server (in terms of space), but not on my local disk space and also not for the internet bandwidth usage. This might be the same for DVC.<\/p>\n<p>Are my &quot;objections&quot; or &quot;caveats&quot; of the points 1 and 2 valid?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_time":1648561944137,
        "Question_score":0,
        "Question_tags":"git|git-lfs|dvc|git-annex",
        "Question_view_count":370,
        "Owner_creation_time":1423144219917,
        "Owner_last_access_time":1663937106050,
        "Owner_location":null,
        "Owner_reputation":11374,
        "Owner_up_votes":415,
        "Owner_down_votes":2,
        "Owner_views":845,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71663330",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72451922,
        "Question_title":"How can dvc pipeline recognize when to use encoding pipeline while new data added for the modeling?",
        "Question_body":"<p>I have created separate pipelines for feature encoding and feature scaling in DVC.\nNow, when I will input new data from my flask API, how these DVC pipelines will automatically run and encode and scale data for modelling?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1654017845533,
        "Question_score":0,
        "Question_tags":"machine-learning|pipeline|mlops|dvc",
        "Question_view_count":25,
        "Owner_creation_time":1654016718443,
        "Owner_last_access_time":1664013919797,
        "Owner_location":null,
        "Owner_reputation":9,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72451922",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":66925614,
        "Question_title":"How to access DVC-controlled files from Oracle?",
        "Question_body":"<p>I have been storing my large files in CLOBs within Oracle, but I am thinking of storing my large files in a shared drive, then having a column in Oracle contain pointers to the files. This would use DVC.<\/p>\n<p>When I do this,<\/p>\n<p>(a) are the paths in Oracle paths that point to the files in my shared drive, as in, the actual files themselves?<\/p>\n<p>(b) or do the paths in Oracle point somehow to the DVC metafile?<\/p>\n<p>Any insight would help me out!<\/p>\n<p>Thanks :)\nJustin<\/p>\n<hr \/>\n<p>EDIT to provide more clarity:<\/p>\n<p>I checked here (<a href=\"https:\/\/dvc.org\/doc\/api-reference\/open\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/api-reference\/open<\/a>), and it helped, but I'm not fully there yet ...<\/p>\n<p>I want to pull a file from a remote dvc repository using python (which I have connected to the Oracle database). So, if we can make that work, I think I will be good. But, I am confused. If I specify 'remote' below, then how do I name the file (e.g., 'activity.log') when the remote files are all encoded?<\/p>\n<pre><code>with dvc.api.open(\n        'activity.log',\n        repo='location\/of\/dvc\/project',\n        remote='my-s3-bucket'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p>(NOTE: For testing purposes, my &quot;remote&quot; DVC directory is just another folder on my MacBook.)<\/p>\n<p>I feel like I'm missing a key concept about getting remote files ...<\/p>\n<p>I hope that adds more clarity. Any help figuring out remote file access is appreciated! :)<\/p>\n<p>Justin<\/p>\n<hr \/>\n<p>EDIT to get insights on 'rev' parameter:<\/p>\n<p>Before my question, some background\/my setup:\n(a) I have a repo on my MacBook called 'basics'.\n(b) I copied into 'basics' a directory of 501 files (called 'surface_files') that I subsequently pushed to a remote storage folder called 'gss'. After the push, 'gss' contains 220 hash directories.<\/p>\n<p>The steps I used to get here are as follows:<\/p>\n<pre><code>&gt; cd ~\/Desktop\/Work\/basics\n&gt; git init\n&gt; dvc init\n&gt; dvc add ~\/Desktop\/Work\/basics\/surface_files\n&gt; git add .gitignore surface_files.dvc\n&gt; git commit -m &quot;Add raw data&quot;\n&gt; dvc remote add -d remote_storage ~\/Desktop\/Work\/gss\n&gt; git commit .dvc\/config -m &quot;Configure remote storage&quot;\n&gt; dvc push\n&gt; rm -rf .\/.dvc\/cache\n&gt; rm -rf .\/surface_files\n<\/code><\/pre>\n<p>Next, I ran the following Python code to take one of my surface files, named <code>surface_100141.dat<\/code>, and used <code>dvc.api.get_url()<\/code> to get the corresponding remote storage file name. I then copied this remote storage file into my desktop under the file's original name, i.e., <code>surface_100141.dat<\/code>.<\/p>\n<p>The code that does all this is as follows, but FIRST, MY QUESTION --- when I run the code as it is shown below, no problems; but when I uncomment the 'rev=' line, it fails. I am not sure why this is happening. I used <code>git log<\/code> and <code>cat .git\/refs\/heads\/master<\/code> to make sure that I was getting the right hash. WHY IS THIS FAILING? That is my question.<\/p>\n<p>(In full disclosure, my git knowledge is not too strong yet. I'm getting there, but it's still a work in progress! :))<\/p>\n<pre><code>import dvc.api\nimport os.path\nfrom os import path\nimport shutil\n\nfilename = 'surface_100141.dat' # This file name would be stored in my Oracle database\nhome_dir = os.path.expanduser('~')+'\/' # This simply expanding '~' into '\/Users\/ricej\/'\n\nresource_url = dvc.api.get_url(\n    path=f'surface_files\/{filename}', # Works when 'surface_files.dvc' exists, even when 'surface_files' directory and .dvc\/cache do not\n    repo=f'{home_dir}Desktop\/Work\/basics',\n    # rev='5c92710e68c045d75865fa24f1b56a0a486a8a45', # Commit hash, found using 'git log' or 'cat .git\/refs\/heads\/master'\n    remote='remote_storage')\nresource_url = home_dir+resource_url\nprint(f'Remote file: {resource_url}')\n\nnew_dir = f'{home_dir}Desktop\/' # Will copy fetched file to desktop, for demonstration\nnew_file = new_dir+filename\nprint(f'Remote file copy: {new_file}')\n\nif path.exists(new_file):\n    os.remove(new_file)\n    \ndest = shutil.copy(resource_url, new_file) # Check your desktop after this to see remote file copy\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1617399923477,
        "Question_score":1,
        "Question_tags":"python|oracle|dvc",
        "Question_view_count":389,
        "Owner_creation_time":1407091594730,
        "Owner_last_access_time":1654654035420,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1617643343263,
        "Answer_body":"<p>I'm not 100% sure that I understand the question (it would be great to expand it a bit on the actual use case you are trying to solve with this database), but I can share a few thoughts.<\/p>\n<p>When we talk about DVC, I think you need to specify a few things to identify the file\/directory:<\/p>\n<ol>\n<li>Git commit + path (actual path like <code>data\/data\/xml<\/code>). Commit (or to be precise any Git revision) is needed to identify the version of the data file.<\/li>\n<li>Or path in the DVC storage (<code>\/mnt\/shared\/storage\/00\/198493ef2343ao<\/code> ...<code>) + actual name of this file. This way you would be saving info that <\/code>.dvc` files have.<\/li>\n<\/ol>\n<p>I would say that second way is <em>not<\/em> recommended since to some extent it's an implementation detail - how does DVC store files internally. The public interface to DVC organized data storage is its repository URL + commit + file name.<\/p>\n<p>Edit (example):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with dvc.api.open(\n        'activity.log',\n        repo='location\/of\/dvc\/project',\n        remote='my-s3-bucket'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p><code>location\/of\/dvc\/project<\/code> this path must point to an actual Git repo. This repo should have a <code>.dvc<\/code> or <code>dvc.lock<\/code> file that has <code>activity.log<\/code> name in it + its hash in the remote storage:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n  - md5: a304afb96060aad90176268345e10355\n    path: activity.log\n<\/code><\/pre>\n<p>By reading this Git repo and analyzing let's say <code>activity.log.dvc<\/code> DVC will be able to create the right path <code>s3:\/\/my-bucket\/storage\/a3\/04afb96060aad90176268345e10355<\/code><\/p>\n<p><code>remote='my-s3-bucket'<\/code> argument is optional. By default it will use the one that is defined in the repo itself.<\/p>\n<p>Let's take another real example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with dvc.api.open(\n        'get-started\/data.xml',\n        repo='https:\/\/github.com\/iterative\/dataset-registry'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p>In the <code>https:\/\/github.com\/iterative\/dataset-registry<\/code> you could find the <a href=\"https:\/\/github.com\/iterative\/dataset-registry\/blob\/master\/get-started\/data.xml.dvc\" rel=\"nofollow noreferrer\"><code>.dvc<\/code> file<\/a> that is enough for DVC to create a path to the file by also analyzing its <a href=\"https:\/\/github.com\/iterative\/dataset-registry\/blob\/master\/.dvc\/config\" rel=\"nofollow noreferrer\">config<\/a><\/p>\n<pre><code>https:\/\/remote.dvc.org\/dataset-registry\/a3\/04afb96060aad90176268345e10355\n<\/code><\/pre>\n<p>you could run <code>wget<\/code> on this file to download it<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1617404822540,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1617478114567,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66925614",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":58952962,
        "Question_title":"How to use different remotes for different folders?",
        "Question_body":"<p>I want my data and models stored in separate Google Cloud buckets. The idea is that I want to be able to share the data with others without sharing the models.<\/p>\n\n<p>One idea I can think of is using separate git submodules for data and models. But that feels cumbersome and imposes some additional requirements from the end user (e.g. having to do <code>git submodule update<\/code>).<\/p>\n\n<p>So can I do this without using git submodules?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1574248229420,
        "Question_score":12,
        "Question_tags":"dvc",
        "Question_view_count":1984,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can first add the different <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\" rel=\"nofollow noreferrer\">DVC remotes<\/a> you want to establish (let's say you call them <code>data<\/code> and <code>models<\/code>, each one pointing to a different <a href=\"https:\/\/cloud.google.com\/storage\/docs\/json_api\/v1\/buckets\" rel=\"nofollow noreferrer\">GC bucket<\/a>). <strong>But don't set any remote as the project's default<\/strong>; This way, <a href=\"https:\/\/dvc.org\/doc\/command-reference\/push\" rel=\"nofollow noreferrer\"><code>dvc push<\/code><\/a> won't work without the <code>-r<\/code> (or <code>--remote<\/code>) option.<\/p>\n<p>You would then need to push each directory or file individually to the appropriate remote, like <code>dvc push data\/ -r data<\/code> and <code>dvc push model.dat -r models<\/code>.<\/p>\n<p>Note that a feature request to configure this exists on the DVC repo too. See <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2095\" rel=\"nofollow noreferrer\">Specify file types that can be pushed to remote<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1574267475363,
        "Answer_score":13.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1642527991692,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58952962",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":68771979,
        "Question_title":"problems installing a DVC lower version [0.9.4]",
        "Question_body":"<p>I need to install an older version of DVC, namely 0.9.4, in a Python virtual environment.<\/p>\n<p>I used the command:<\/p>\n<pre><code>pip install dvc==0.9.4\n<\/code><\/pre>\n<p>Everything seemed to work fine. However, when I try to run a <code>dvc pull<\/code> command, I get the following error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\runpy.py&quot;, line 193, in _run_module_as_main\n    &quot;__main__&quot;, mod_spec)\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\runpy.py&quot;, line 85, in _run_code\n    exec(code, run_globals)\n  File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\Scripts\\dvc.exe\\__main__.py&quot;, line 4, in &lt;module&gt;\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\main.py&quot;, line 2, in &lt;module&gt;\n    from dvc.cli import parse_args\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\cli.py&quot;, line 8, in &lt;module&gt;\n    from dvc.command.init import CmdInit\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\command\\init.py&quot;, line 1, in &lt;module&gt;\n    from dvc.project import Project\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\project.py&quot;, line 15, in &lt;module&gt;\n    from dvc.cloud.data_cloud import DataCloud\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\cloud\\data_cloud.py&quot;, line 11, in &lt;module&gt;\n    from dvc.cloud.gcp import DataCloudGCP\n  File &quot;c:\\users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\cloud\\gcp.py&quot;, line 4, in &lt;module&gt;\n    from google.cloud import storage as gc\nModuleNotFoundError: No module named 'google.cloud'\n<\/code><\/pre>\n<p>When I print the dvc version, I see:<\/p>\n<pre><code>0.9.4+6bb66e.mod\n<\/code><\/pre>\n<p>Can anyone please help? Thanks.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1628856076700,
        "Question_score":2,
        "Question_tags":"python|google-cloud-storage|dvc",
        "Question_view_count":234,
        "Owner_creation_time":1569838509623,
        "Owner_last_access_time":1663596099763,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":1628882501660,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68771979",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67454531,
        "Question_title":"Git bash command prompt hanging when running dvc push to DAGsHub",
        "Question_body":"<p>I'm having problems pushing files with DVC to DAGsHub.<\/p>\n<p>Workflow:<\/p>\n<ul>\n<li>I used my email to signup to DAGsHub.<\/li>\n<li>I created a repo and clone it to my computer.<\/li>\n<li>I added files to the repo and track them using DVC and Git to track the pointer files.<\/li>\n<li>Running DVC push -r origin, it asks me for my password. When I enter the password and hit enter - nothing happens.<\/li>\n<\/ul>\n<p>It sits and waits, barring me from even canceling the operation with Ctrl+C.\nI'm forced to manually close the terminal, open a new one, ending the &quot;Python&quot; process in task manager and delete the lock file in .dvc\/tmp\/lock.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1620537862843,
        "Question_score":5,
        "Question_tags":"dvc",
        "Question_view_count":254,
        "Owner_creation_time":1620537484800,
        "Owner_last_access_time":1642059984007,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1620625906412,
        "Answer_body":"<p><strong>Short answer<\/strong><\/p>\n<p>Do not use <code>ask_password<\/code>.\nInstead, save your token in the local config by running once:<\/p>\n<pre><code>dvc remote modify origin --local --unset ask_password\ndvc remote modify origin --local password &lt;--access token--&gt;\n<\/code><\/pre>\n<p><code>dvc push -r origin<\/code> should work then.<\/p>\n<p><strong>Long answer<\/strong><\/p>\n<p><a href=\"https:\/\/www.atlassian.com\/git\/tutorials\/git-bash#:%7E:text=What%20is%20Git%20Bash%3F,operating%20system%20through%20written%20commands.\" rel=\"nofollow noreferrer\">Git Bash<\/a> is not running the regular Windows command prompt but an emulated Unix-style bash prompt. From the information in your question, I cannot know for sure, but this is probably causing the <code>msvcrt<\/code> package used by DVC to prompt the password on windows machines to fail\/hang.<\/p>\n<p>There are potentially 3 ways to deal with the issue:<\/p>\n<ol>\n<li>Run <code>dvc pull<\/code> from the regular Windows cmd prompt.<\/li>\n<li>Find a way to make Git Bash wrap Python calls with <code>winpty<\/code> - I am not 100% positive about how to do this, but not using <code>winpty<\/code> seems to be the reason <code>msvcrt<\/code> fails at prompting for your password.<\/li>\n<li>The simplest solution - Do not use <code>ask_password<\/code>.\nInstead, save your token in the local config by running once:\n<pre><code>dvc remote modify origin --local --unset ask_password\ndvc remote modify origin --local password &lt;--access token--&gt;\n<\/code><\/pre>\nYou can get your access token by clicking on the question mark beside the DVC\nremote of your DAGsHub repository, then click on &quot;Reveal my token&quot;.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620591578072,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67454531",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":62441146,
        "Question_title":"Revert a dvc remove -p command",
        "Question_body":"<p>I have just removed a DVC tracking file by mistake using the command <code>dvc remove training_data.dvc -p<\/code>, which led to all my training dataset gone completely. I know in Git, we can easily revert a deleted branch based on its hash. Does anyone know how to revert all my lost data in DVC?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592445622650,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":687,
        "Owner_creation_time":1467943515393,
        "Owner_last_access_time":1664083114540,
        "Owner_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Owner_reputation":173,
        "Owner_up_votes":97,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You should be safe (at least data is not gone) most likely. From the <code>dvc remove<\/code> <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remove\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>Note that it does not remove files from the DVC cache or remote storage (see dvc gc). However, remember to run <code>dvc push<\/code> to save the files you actually want to use or share in the future.<\/p>\n<\/blockquote>\n\n<p>So, if you created <code>training_data.dvc<\/code> as with <code>dvc add<\/code> and\/or <code>dvc run<\/code> and <code>dvc remove -p<\/code> didn't ask\/warn you about anything, means that data is cached similar to Git in the <code>.dvc\/cache<\/code>. <\/p>\n\n<p>There are ways to retrieve it, but I would need to know a little bit more details - how exactly did you add your dataset? Did you commit <code>training_data.dvc<\/code> or it's completely gone? Was it the only data you have added so far? (happy to help you in comments).<\/p>\n\n<h2>Recovering a directory<\/h2>\n\n<p>First of all, <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files-and-directories#structure-of-cache-directory\" rel=\"nofollow noreferrer\">here<\/a> is the document that describes briefly how DVC stores directories in the cache.<\/p>\n\n<p>What we can do is to find all <code>.dir<\/code> files in the <code>.dvc\/cache<\/code>:<\/p>\n\n<p><code>find .dvc\/cache -type f -name \"*.dir\"<\/code><\/p>\n\n<p>outputs something like:<\/p>\n\n<pre><code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir\n.dvc\/cache\/00\/db872eebe1c914dd13617616bb8586.dir\n.dvc\/cache\/2d\/1764cb0fc973f68f31f5ff90ee0883.dir\n<\/code><\/pre>\n\n<p>(if the local cache is lost and we are restoring data from the remote storage, the same logic applies, commands (e.g. to find files on S3 with .dir extension) look different)<\/p>\n\n<p>Each <code>.dir<\/code> file is a JSON with a content of one version of a directory (file names, hashes, etc). It has all the information needed to restore it. The next thing we need to do is to understand which one do we need. There is no one single rule for that, what I would recommend to check (and pick depending on your use case):<\/p>\n\n<ul>\n<li>Check the date modified (if you remember when this data was added).<\/li>\n<li>Check the content of those files - if you remember a specific file name that was present only in the directory you are looking for - just grep it.<\/li>\n<li>Try to restore them one by one and check the directory content.<\/li>\n<\/ul>\n\n<p>Okay, now let's imagine we decided that we want to restore <code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir<\/code>, (e.g. because content of it looks like:<\/p>\n\n<pre><code>[\n{\"md5\": \"6f597d341ceb7d8fbbe88859a892ef81\", \"relpath\": \"test.tsv\"}, {\"md5\": \"32b715ef0d71ff4c9e61f55b09c15e75\", \"relpath\": \"train.tsv\"}\n]\n<\/code><\/pre>\n\n<p>and we want to get a directory with <code>train.tsv<\/code>).<\/p>\n\n<p>The only thing we need to do is to create a <code>.dvc<\/code> file that references this directory:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n- md5: 20b786b6e6f80e2b3fcf17827ad18597.dir\n  path: my-directory\n<\/code><\/pre>\n\n<p>(note, that path \/20\/b786b6e6f80e2b3fcf17827ad18597.dir became a hash value: 20b786b6e6f80e2b3fcf17827ad18597.dir)<\/p>\n\n<p>And run <code>dvc pull<\/code> on this file.<\/p>\n\n<p>That should be it.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1592457436920,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1592496929889,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62441146",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67209146,
        "Question_title":"DVC - make scheduled csv dumps",
        "Question_body":"<p>Suppose we got some database (any database, that supports csv dumping), collecting raw data in real time for further usage in ML.\nOn the other side, we got DVC, that can work with csv files.<\/p>\n<p>I want to organize a scheduled run of stored SELECT to that DB with datetime parameters (and also support a manual run), to make a new csv files, and send them to DVC.<\/p>\n<p>In DVC documentation and examples I found, csv file already exists.<\/p>\n<p>Can I make this interaction with database with DVC itself, or I got something wrong, and there is a separate tool for csv dump?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619078487020,
        "Question_score":2,
        "Question_tags":"export-to-csv|dvc",
        "Question_view_count":65,
        "Owner_creation_time":1580932981007,
        "Owner_last_access_time":1663744754923,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There are 3 steps in this process:<\/p>\n<ol>\n<li>Create a CSV dump. Many DBs have these tools but DVC does not support this natively.<\/li>\n<li>Version the CSV dump and move it to some storage. DVC does this job.<\/li>\n<li>Schedule periodical dump. You can use Cron (easy), AirFlow (not easy) or <a href=\"https:\/\/docs.github.com\/en\/actions\/reference\/events-that-trigger-workflows\" rel=\"nofollow noreferrer\">periodical jobs in GitHub Actions<\/a>\/<a href=\"https:\/\/docs.gitlab.com\/ee\/ci\/pipelines\/schedules.html\" rel=\"nofollow noreferrer\">GitLab CI\/CD<\/a>. Another project from the DVC team can help with CI\/CD option: <a href=\"https:\/\/cml.dev\" rel=\"nofollow noreferrer\">https:\/\/cml.dev<\/a>.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1619081803750,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1619084940030,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67209146",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73651050,
        "Question_title":"DVC imports authentication to blob storage",
        "Question_body":"<p>I'm using <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> to track and version data that is stored locally on the file system and in Azure Blob storage.<\/p>\n<p>My setup is as follows:<\/p>\n<ul>\n<li><p><code>DataProject1<\/code>, it uses a local file location as a remote therefore it does not require any authentication.<\/p>\n<\/li>\n<li><p><code>DataProject2<\/code>, it uses Azure Blob Storage as a remote, it is using sas_token for authentication, I can push pull data to\/from the remote when I'm within this project.<\/p>\n<\/li>\n<li><p><code>MLProject<\/code>, it uses dvc import to import data from <code>DataProjec1<\/code> and <code>DataProject2<\/code>.<\/p>\n<\/li>\n<\/ul>\n<p>When I run the import with the command against <code>DataProject1<\/code> everything works fine:<\/p>\n<p><code>dvc import -o 'data\/project1' 'https:\/\/company.visualstudio.com\/DefaultCollection\/proj\/_git\/DataProject1' 'data\/project1'<\/code> - Successful<\/p>\n<p>Howevever when I run a similar command against <code>DataProject2<\/code> the command fails:<\/p>\n<p><code>dvc import -o 'data\/project2' 'https:\/\/company.visualstudio.com\/DefaultCollection\/proj\/_git\/DataProject2' 'data\/project2'<\/code> - it fails with:<\/p>\n<blockquote>\n<p>ERROR: unexpected error - Operation returned an invalid status 'This\nrequest is not authorized to perform this operation using this\npermission.'  ErrorCode:AuthorizationPermissionMismatch.<\/p>\n<\/blockquote>\n<p>I would like to configure the <code>dvc import<\/code> so that I can set the required <code>sas_token<\/code> but I cannot find a way to do that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1662648529560,
        "Question_score":1,
        "Question_tags":"python|dvc|dvc-import",
        "Question_view_count":34,
        "Owner_creation_time":1248452771430,
        "Owner_last_access_time":1664006575380,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":3317,
        "Owner_up_votes":466,
        "Owner_down_votes":8,
        "Owner_views":296,
        "Question_last_edit_time":1662650337132,
        "Answer_body":"<p>This happens since DVC is not using <code>MLProject<\/code>'s config when it clones and does <code>dvc fetch<\/code> in the <code>DataProject2<\/code> during the <code>import<\/code>. And it doesn't know where it can find the token (clearly, it's not in the Git repo, right?).<\/p>\n<p>There are a few ways to specify it: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#--system\" rel=\"nofollow noreferrer\"><code>global\/system<\/code> configs<\/a> and\/or <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#authenticate-with-environment-variables\" rel=\"nofollow noreferrer\">environment variables<\/a>.<\/p>\n<p>To implement the first option:<\/p>\n<p>On a machine where you do <code>dvc import<\/code>, you could create a remote in the <code>--global<\/code>, or <code>--system<\/code> configs with the same name and specify the token there. Global config fields will be merged with the config in the <code>DataProject2<\/code> repo when DVC is pulling data to import.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>dvc remote add --global &lt;DataProject2-remote-name&gt; azure:\/\/DataProject2\/storage\ndvc remote modify --global &lt;DataProject2-remote-name&gt; account_name &lt;name&gt;\ndvc remote modify --global &lt;DataProject2-remote-name&gt; sas_token &lt;token&gt;\n<\/code><\/pre>\n<p>The second option:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>export AZURE_STORAGE_SAS_TOKEN='mysecret'\nexport AZURE_STORAGE_ACCOUNT='myaccount'\n<\/code><\/pre>\n<p>Please give it a try, let me know if that works or not.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1662656090387,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73651050",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":60365473,
        "Question_title":"By how much can i approx. reduce disk volume by using dvc?",
        "Question_body":"<p>I want to classify ~1m+ documents and have a Version Control System for in- and Output of the corresponding model. <\/p>\n\n<p>The data changes over time:<\/p>\n\n<ul>\n<li>sample size increases over time<\/li>\n<li>new Features might appear<\/li>\n<li>anonymization procedure might Change over time<\/li>\n<\/ul>\n\n<p>So basically \"everything\" might change: amount of observations, Features and the values.\nWe are interested in making the ml model Building reproducible without using 10\/100+ GB \nof disk volume, because we save all updated versions of Input data. Currently the volume size of the data is ~700mb.<\/p>\n\n<p>The most promising tool i found is: <a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"noreferrer\">https:\/\/github.com\/iterative\/dvc<\/a>. Currently the data\nis stored in a database in loaded in R\/Python from there.<\/p>\n\n<p><strong>Question:<\/strong><\/p>\n\n<p>How much disk volume can be (very approx.) saved by using dvc? <\/p>\n\n<p>If one can roughly estimate that. I tried to find out if only the \"diffs\" of the data are saved. I didnt find much info by reading through: <a href=\"https:\/\/github.com\/iterative\/dvc#how-dvc-works\" rel=\"noreferrer\">https:\/\/github.com\/iterative\/dvc#how-dvc-works<\/a> or other documentation. <\/p>\n\n<p><strong>I am aware that this is a very vague question. And it will highly depend on the dataset. However, i would still be interested in getting a very approximate idea.<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1582482701247,
        "Question_score":7,
        "Question_tags":"python|sql|r|git|dvc",
        "Question_view_count":689,
        "Owner_creation_time":1504097190907,
        "Owner_last_access_time":1647022258673,
        "Owner_location":null,
        "Owner_reputation":1365,
        "Owner_up_votes":145,
        "Owner_down_votes":3,
        "Owner_views":193,
        "Question_last_edit_time":1582486128287,
        "Answer_body":"<p>Let me try to summarize how does DVC store data and I hope you'll be able to figure our from this how much space will be saved\/consumed in your specific scenario.<\/p>\n\n<p><strong>DVC is storing and deduplicating data on the individual <em>file level<\/em>.<\/strong> So, what does it usually mean from a practical perspective.<\/p>\n\n<p>I will use <code>dvc add<\/code> as an example, but the same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add<\/code>, <code>dvc run<\/code>, etc.<\/p>\n\n<h2>Scenario 1: Modifying file<\/h2>\n\n<p>Let's imagine I have a single 1GB XML file. I start tracking it with DVC:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ dvc add data.xml\n<\/code><\/pre>\n\n<p>On the modern file system (or if <code>hardlinks<\/code>, <code>symlinks<\/code> are enabled, see <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"noreferrer\">this<\/a> for more details) after this command we still consume 1GB (even though file is moved into DVC cache and is still present in the workspace).<\/p>\n\n<p>Now, let's change it a bit and save it again:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ echo \"&lt;test\/&gt;\" &gt;&gt; data.xml\n$ dvc add data.xml\n<\/code><\/pre>\n\n<p>In this case we will have 2GB consumed. <strong>DVC does not do diff between two versions of the same file<\/strong>, neither it splits files into chunks or blocks to understand that only small portion of data has changed.<\/p>\n\n<blockquote>\n  <p>To be precise, it calculates <code>md5<\/code> of each file and save it in the content addressable key-value storage. <code>md5<\/code> of the files serves as a key (path of the file in cache) and value is the file itself:<\/p>\n  \n  <pre class=\"lang-sh prettyprint-override\"><code>(.env) [ivan@ivan ~\/Projects\/test]$ md5 data.xml\n0c12dce03223117e423606e92650192c\n\n(.env) [ivan@ivan ~\/Projects\/test]$ tree .dvc\/cache\n.dvc\/cache\n\u2514\u2500\u2500 0c\n   \u2514\u2500\u2500 12dce03223117e423606e92650192c\n\n1 directory, 1 file\n\n(.env) [ivan@ivan ~\/Projects\/test]$ ls -lh data.xml\ndata.xml ----&gt; .dvc\/cache\/0c\/12dce03223117e423606e92650192c (some type of link)\n<\/code><\/pre>\n<\/blockquote>\n\n<h2>Scenario 2: Modifying directory<\/h2>\n\n<p>Let's now imagine we have a single large 1GB directory <code>images<\/code> with a lot of files:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ du -hs images\n1GB\n\n$ ls -l images | wc -l\n1001\n\n$ dvc add images\n<\/code><\/pre>\n\n<p>At this point we still consume 1GB. Nothing has changed. But if we modify the directory by adding more files (or removing some of them):<\/p>\n\n<pre><code>$ cp \/tmp\/new-image.png images\n\n$ ls -l images | wc -l\n1002\n\n$ dvc add images\n<\/code><\/pre>\n\n<p>In this case, after saving the new version we <strong>still close to 1GB<\/strong> consumption. <strong>DVC calculates diff on the directory level.<\/strong> It won't be saving all the files that were existing before in the directory.<\/p>\n\n<p>The same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add<\/code>, <code>dvc run<\/code>, etc.<\/p>\n\n<p>Please, let me know if it's clear or we need to add more details, clarifications.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1582487867856,
        "Answer_score":12.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60365473",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70752204,
        "Question_title":"How to resolve DVC Pull Error on Pycharm?",
        "Question_body":"<p>When I execute 'DVC Pull', I get the following error<\/p>\n<pre><code>&gt; dvc pull ERROR: unexpected error - invalid syntax (tz.py, line 78)    \n&gt; Traceback (most recent call last):   File\n&gt; &quot;\/home\/jasma\/miniconda3\/envs\/earth\/lib\/python3.10\/site-packages\/dvc\/main.py&quot;,\n&gt; line 55, in main\n&gt;     ret = cmd.do_run()   File &quot;\/home\/jasma\/miniconda3\/envs\/earth\/lib\/python3.10\/site-packages\/dvc\/command\/base.py&quot;,\n &quot;\/home\/jasma\/miniconda3\/envs\/earth\/lib\/python3.10\/site-packages\/dateutil\/tz.py&quot;,\n&gt; line 78\n&gt;     `self._name`,\n&gt;     ^ SyntaxError: invalid syntax\n<\/code><\/pre>\n<p><a href=\"https:\/\/github.com\/jasma-balasangameshwara\/ml-heroku-fastapi\" rel=\"nofollow noreferrer\">How to resolve it? The link to the Github repository is <\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1642493442793,
        "Question_score":1,
        "Question_tags":"python|dvc",
        "Question_view_count":105,
        "Owner_creation_time":1579349493753,
        "Owner_last_access_time":1652115144133,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70752204",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":71155959,
        "Question_title":"How to merge data (CSV) files from multiple branches (Git and DVC)?",
        "Question_body":"<p><strong>Background<\/strong>: In my projects I'm using GIT and <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> to keep track of versions:<\/p>\n<ul>\n<li>GIT - only for source codes<\/li>\n<li>DVC - for dataset, model objects and outputs<\/li>\n<\/ul>\n<p>I'm testing different approaches in separate branches, i.e:<\/p>\n<ul>\n<li>random_forest<\/li>\n<li>neural_network_1<\/li>\n<li>...<\/li>\n<\/ul>\n<p>Typically as an output I'm keeping predictions in csv file with standarised name (i.e.: pred_test.csv). As a consequence in different branches I've different pred_test.csv files. The structure of the file is very simple, it contains two columns:<\/p>\n<ul>\n<li>ID<\/li>\n<li>Prediction<\/li>\n<\/ul>\n<p><strong>Question<\/strong>: What is the best way to merge those prediction files into single big file?<\/p>\n<p>I would like to obtain a file with structure:<\/p>\n<ul>\n<li>ID<\/li>\n<li>Prediction_random_forest<\/li>\n<li>Prediction_neural_network_1<\/li>\n<li>Prediction_...<\/li>\n<\/ul>\n<p>My main issue is how to access files with predictions which are in different branches?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1645092064283,
        "Question_score":0,
        "Question_tags":"git|data-science|dvc",
        "Question_view_count":274,
        "Owner_creation_time":1265742671200,
        "Owner_last_access_time":1658834085253,
        "Owner_location":null,
        "Owner_reputation":2735,
        "Owner_up_votes":190,
        "Owner_down_votes":7,
        "Owner_views":552,
        "Question_last_edit_time":1645132430276,
        "Answer_body":"<p>I would try to use <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\" rel=\"nofollow noreferrer\"><code>dvc get<\/code><\/a> in this case:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc get -o random_forest_pred.csv --rev random_forest . pred_test.csv\n<\/code><\/pre>\n<p>It should bring the <code>pred_test.csv<\/code> from the <code>random_forest<\/code> branch.<\/p>\n<blockquote>\n<p>Mind the <code>.<\/code> before the <code>pred_test.csv<\/code> please, it's needed and it means that &quot;use the current repo&quot;, since <code>dvc get<\/code> could also be used on other repos (e.g. GitHub URL)<\/p>\n<\/blockquote>\n<p>Then I think you could use some CLI or write a script to join the files:<\/p>\n<p><a href=\"https:\/\/unix.stackexchange.com\/questions\/293775\/merging-contents-of-multiple-csv-files-into-single-csv-file\">https:\/\/unix.stackexchange.com\/questions\/293775\/merging-contents-of-multiple-csv-files-into-single-csv-file<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1645113091963,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71155959",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":65824766,
        "Question_title":"SSH automation in jenkins",
        "Question_body":"<p>So I've been working on the automation of processes and it includes fetching data from an external source through DVC(data version control) for which I am using SSH client to pull and push changes. For automation, I'm using <strong>Jenkins<\/strong> and the problem I'm facing is that for ssh we need to give a password on runtime, and in automation that's not an option. I've tried multiple ways to specify passwords for ssh like sshpass and ssh config but it turns out Jenkins when building creates some file name <strong>script.sh<\/strong> in a directory <em>repoName@tmp<\/em> in var\/lib\/jenkins\/.... and therefore it is giving permission denied error. no matter what I try. If anyone could give any suggestions to this problem it would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611222255303,
        "Question_score":2,
        "Question_tags":"jenkins|ssh|dvc",
        "Question_view_count":121,
        "Owner_creation_time":1493101921290,
        "Owner_last_access_time":1663968152030,
        "Owner_location":"Pakistan",
        "Owner_reputation":133,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You could use key-based auth for SSH instead instead of password auth so that your Jenkins user can access your SSH DVC remote without needing to specify a password.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1611228029327,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65824766",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56881619,
        "Question_title":"What are the pros and cons of using DVC and Pachyderm?",
        "Question_body":"<p>What are the pros and cons of using either of these?<\/p>\n\n<p><a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc<\/a><\/p>\n\n<p><a href=\"https:\/\/github.com\/pachyderm\/pachyderm\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pachyderm\/pachyderm<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1562220779043,
        "Question_score":1,
        "Question_tags":"machine-learning|version-control|data-science|dvc|pachyderm",
        "Question_view_count":1635,
        "Owner_creation_time":1562220403123,
        "Owner_last_access_time":1563409319090,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56881619",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73727709,
        "Question_title":"Can we connect oracle database with DVC ? and if yes then how?",
        "Question_body":"<p>I was trying to connect dvc with oracle database but unable to do it. So, Please can anyone help me with that.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1663229583107,
        "Question_score":1,
        "Question_tags":"oracle|dvc",
        "Question_view_count":20,
        "Owner_creation_time":1663229291407,
        "Owner_last_access_time":1663926949987,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73727709",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73276837,
        "Question_title":"Is it possible to use authentication and authorization in MLFlow Server?",
        "Question_body":"<p>MLFlow does not have integrated authentication (openID, LDAP, kerberos, AAD...) or authorization (RBAC, ABAC, ACL...)<\/p>\n<p>Is it only possible with a web proxy in MLFlow? p.e: nginx<\/p>\n<p>Does anyone know of an option similar to Apache Sentry or Apache Ranger for MLFlow?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1659956478063,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":57,
        "Owner_creation_time":1463943156183,
        "Owner_last_access_time":1663834484890,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73276837",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67953241,
        "Question_title":"mlflow run git-uri clone to specific directory",
        "Question_body":"<p>I am using mlflow run with a GitHub uri.<\/p>\n<p>When I run using the below command<\/p>\n<pre><code>mlflow run &lt;git-uri&gt;\n<\/code><\/pre>\n<p>The command sets up a conda environment and then <em>clones the Git repo into a <strong>temp<\/strong> directory, But I need it setup in a <strong>specific<\/strong> directory<\/em><\/p>\n<p>I checked the entire document, but I can't find it. Is there no such option to do so in one shot?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1623534343453,
        "Question_score":1,
        "Question_tags":"python-3.x|mlflow",
        "Question_view_count":239,
        "Owner_creation_time":1575348765723,
        "Owner_last_access_time":1663079223380,
        "Owner_location":"Chennai, Tamil Nadu, India",
        "Owner_reputation":1049,
        "Owner_up_votes":55,
        "Owner_down_votes":68,
        "Owner_views":192,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For non-local URIs, MLflow uses the Python's <code>tempfile.mkdtemp<\/code> function (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/1c43176cefb5531fbb243975b9c8c5bfb9775e66\/mlflow\/projects\/utils.py#L140\" rel=\"nofollow noreferrer\">source code<\/a>), that creates the temporary directory.  You may have some control over it by setting the <code>TMPDIR<\/code> environment variable as described in <a href=\"https:\/\/docs.python.org\/3\/library\/tempfile.html#tempfile.mkstemp\" rel=\"nofollow noreferrer\">Python docs<\/a> (it lists <code>TMP<\/code> &amp; <code>TEMP<\/code> as well, but they didn't work for me on MacOS) - but it will set only &quot;base path&quot; for temporary directories and files, the directory\/file names are still will be random.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1623570743820,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1623614468896,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67953241",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70620074,
        "Question_title":"Serving multiple ML models using mlflow in a single VM",
        "Question_body":"<p>I have setup an mlflow service in a VM and I am able to serve the model using mlflow serve command.\nWanted to know if we can host multiple models in a single VM ?<\/p>\n<p>I am using the below command to serve a model using mlflow in a vm.<\/p>\n<p>command:<\/p>\n<pre><code>\/mlflow models serve -m models:\/$Model-Name\/$Version --no-conda -p 443 -h 0.0.0.0\n<\/code><\/pre>\n<p>Above command creates a model serving and runs it on 443 port.\nIs it possible to have an endpoint like below being created with model name in it ?<\/p>\n<p>Current URL:\nhttps:\/\/localhost:443\/invocations<\/p>\n<p>Expected URL:\nhttps:\/\/localhost:443\/model-name\/invocations ?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1641552500160,
        "Question_score":0,
        "Question_tags":"apache-spark|machine-learning|databricks|mlflow",
        "Question_view_count":544,
        "Owner_creation_time":1568969745383,
        "Owner_last_access_time":1663915778593,
        "Owner_location":null,
        "Owner_reputation":344,
        "Owner_up_votes":39,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Question_last_edit_time":1641558820040,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70620074",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69466354,
        "Question_title":"MLflow S3UploadFailedError: Failed to upload",
        "Question_body":"<p>I've created with docker a MinioS3 artifact storage and a mysql bakend storage using the next docker-compose:<\/p>\n<pre><code>    version: '3.8'\n    services:\n        db:\n           environment:\n              - MYSQL_DATABASE=${MYSQL_DATABASE}\n              - MYSQL_USER=${MYSQL_USER}\n              - MYSQL_PASSWORD=${MYSQL_PASSWORD}\n              - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}\n           expose:\n              - '3306'        \n           volumes:\n              - '(path)\/server_backend:\/var\/lib\/mysql '\n           image: 'mysql'\n           container_name: db\n\n        storage:\n            environment:\n                - MINIO_ACCESS_KEY=${MINIO_USR}\n                - MINIO_SECRET_KEY=${MINIO_PASS}\n            expose:\n                - '9000'\n            ports:\n                - '9000:9000'        \n            depends_on:\n                - db\n            command: server \/data\n            volumes:\n                - '(path)\/server_artifact:\/data'\n            image: minio\/minio:RELEASE.2021-02-14T04-01-33Z\n            container_name: MinIO\n\n        mlflow:\n            build: .\/mlflow\n            environment:\n                - AWS_ACCESS_KEY_ID=${MINIO_USR}\n                - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}       \n            expose:\n                - '5000'\n            ports:\n                - '5000:5000'\n            depends_on:\n                - storage                       \n            image: 'mlflow:Dockerfile'\n            container_name: server\n<\/code><\/pre>\n<p>The Mlflow server docker was created using the next Dockerfile:<\/p>\n<pre><code>    FROM python:3.8-slim-buster\n    WORKDIR \/usr\/src\/app\n    RUN pip install cryptography mlflow psycopg2-binary boto3 pymysql\n    ENV MLFLOW_S3_ENDPOINT_URL=http:\/\/storage:9000\n    CMD mlflow server \\\n        --backend-store-uri mysql+pymysql:\/\/MLFLOW:temporal@db:3306\/DBMLFLOW \\\n        --default-artifact-root s3:\/\/artifacts \\\n        --host 0.0.0.0\n<\/code><\/pre>\n<p>The credantials are defined in a <code>.env<\/code> file.<\/p>\n<p>The results of the <code>docker-compose<\/code> up command :<\/p>\n<pre><code>\n    [+] Running 21\/22\n     - mlflow Error                                                                                                                              5.6s\n     - storage Pulled                                                                                                                           36.9s\n       - a6b97b4963f5 Pull complete                                                                                                             24.6s\n       - 13948a011eec Pull complete                                                                                                             24.7s\n       - 40cdef9976a6 Pull complete                                                                                                             24.7s\n       - f47162848743 Pull complete                                                                                                             24.8s\n       - 5f2758d8e94c Pull complete                                                                                                             24.9s\n       - c2950439edb8 Pull complete                                                                                                             25.0s\n       - 1b08f8a15998 Pull complete                                                                                                             30.7s\n     - db Pulled                                                                                                                                45.8s\n       - 07aded7c29c6 Already exists                                                                                                             0.0s\n       - f68b8cbd22de Pull complete                                                                                                              0.7s\n       - 30c1754a28c4 Pull complete                                                                                                              2.1s\n       - 1b7cb4d6fe05 Pull complete                                                                                                              2.2s\n       - 79a41dc56b9a Pull complete                                                                                                              2.3s\n       - 00a75e3842fb Pull complete                                                                                                              6.7s\n       - b36a6919c217 Pull complete                                                                                                              6.8s\n       - 635b0b84d686 Pull complete                                                                                                              6.8s\n       - 6d24c7242d02 Pull complete                                                                                                             39.4s\n       - 5be6c5edf16f Pull complete                                                                                                             39.5s\n       - cb35eac1242c Pull complete                                                                                                             39.5s\n       - a573d4e1c407 Pull complete                                                                                                             39.6s\n    [+] Building 1.4s (7\/7) FINISHED\n     =&gt; [internal] load build definition from Dockerfile                                                                                         0.0s\n     =&gt; =&gt; transferring dockerfile: 32B                                                                                                          0.0s\n     =&gt; [internal] load .dockerignore                                                                                                            0.0s\n     =&gt; =&gt; transferring context: 2B                                                                                                              0.0s\n     =&gt; [internal] load metadata for docker.io\/library\/python:3.8-slim-buster                                                                    1.3s\n     =&gt; [1\/3] FROM docker.io\/library\/python:3.8-slim-buster@sha256:13a3f2bffb4b18ff7eda2763a3b0ba316dd82e548f52ea8b4fd11c94b97afa7d              0.0s\n     =&gt; CACHED [2\/3] WORKDIR \/usr\/src\/app                                                                                                        0.0s\n     =&gt; CACHED [3\/3] RUN pip install cryptography mlflow psycopg2-binary boto3 pymysql                                                           0.0s\n     =&gt; exporting to image                                                                                                                       0.0s\n     =&gt; =&gt; exporting layers                                                                                                                      0.0s\n     =&gt; =&gt; writing image sha256:76d4e4462b5c7c1826734e59a54488b56660de0dd5ecc188c308202608a8f20b                                                 0.0s\n     =&gt; =&gt; naming to docker.io\/library\/mlflow:Dockerfile                                                                                         0.0s\n    \n    Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\n    [+] Running 3\/3\n     - Container db  Created                                                                                                       0.5s\n     - Container MinIO      Created                                                                                                       0.1s\n     - Container server     Created                                                                                                       0.1s\n    Attaching to server, MinIO, db\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.26-1debian10 started.\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.26-1debian10 started.\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Initializing database files\n    db  | 2021-10-06T12:12:57.679527Z 0 [System] [MY-013169] [Server] \/usr\/sbin\/mysqld (mysqld 8.0.26) initializing of server in progress as process 44\n    db  | 2021-10-06T12:12:57.687748Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.\n    db  | 2021-10-06T12:12:58.230036Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.\n    db  | 2021-10-06T12:12:59.888820Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:12:59.889102Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:12:59.997461Z 6 [Warning] [MY-010453] [Server] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option.\n    MinIO      | Attempting encryption of all config, IAM users and policies on MinIO backend\n    MinIO      | Endpoint: http:\/\/172.18.0.3:9000  http:\/\/127.0.0.1:9000\n    MinIO      |\n    MinIO      | Browser Access:\n    MinIO      |    http:\/\/172.18.0.3:9000  http:\/\/127.0.0.1:9000\n    MinIO      |\n    MinIO      | Object API (Amazon S3 compatible):\n    MinIO      |    Go:         https:\/\/docs.min.io\/docs\/golang-client-quickstart-guide\n    MinIO      |    Java:       https:\/\/docs.min.io\/docs\/java-client-quickstart-guide\n    MinIO      |    Python:     https:\/\/docs.min.io\/docs\/python-client-quickstart-guide\n    MinIO      |    JavaScript: https:\/\/docs.min.io\/docs\/javascript-client-quickstart-guide\n    MinIO      |    .NET:       https:\/\/docs.min.io\/docs\/dotnet-client-quickstart-guide\n    server     | 2021\/10\/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 0.1 seconds\n    server     | 2021\/10\/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 0.3 seconds\n    server     | 2021\/10\/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 0.7 seconds\n    server     | 2021\/10\/06 12:13:03 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 1.5 seconds\n    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Database files initialized\n    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Starting temporary server\n    db  | 2021-10-06T12:13:04.422603Z 0 [System] [MY-010116] [Server] \/usr\/sbin\/mysqld (mysqld 8.0.26) starting as process 93\n    db  | 2021-10-06T12:13:04.439806Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.\n    db  | 2021-10-06T12:13:04.575773Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.\n    db  | 2021-10-06T12:13:04.827307Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:04.827865Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:04.832827Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.\n    db  | 2021-10-06T12:13:04.834132Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.\n    db  | 2021-10-06T12:13:04.841629Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '\/var\/run\/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.\n    db  | 2021-10-06T12:13:04.855748Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Socket: \/var\/run\/mysqld\/mysqlx.sock\n    db  | 2021-10-06T12:13:04.855801Z 0 [System] [MY-010931] [Server] \/usr\/sbin\/mysqld: ready for connections. Version: '8.0.26'  socket: '\/var\/run\/mysqld\/mysqld.sock'  port: 0  MySQL Community Server - GPL.\n    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Temporary server started.\n    server     | 2021\/10\/06 12:13:05 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 3.1 seconds\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/iso3166.tab' as time zone. Skipping it.\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/leap-seconds.list' as time zone. Skipping it.\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/zone.tab' as time zone. Skipping it.\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/zone1970.tab' as time zone. Skipping it.\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Creating database DBMLFLOW\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Creating user MLFLOW\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Giving user MLFLOW access to schema DBMLFLOW\n    db  |\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Stopping temporary server\n    db  | 2021-10-06T12:13:06.948482Z 13 [System] [MY-013172] [Server] Received SHUTDOWN from user root. Shutting down mysqld (Version: 8.0.26).\n    server     | 2021\/10\/06 12:13:08 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 6.3 seconds\n    db  | 2021-10-06T12:13:08.716131Z 0 [System] [MY-010910] [Server] \/usr\/sbin\/mysqld: Shutdown complete (mysqld 8.0.26)  MySQL Community Server - GPL.\n    db  | 2021-10-06 12:13:08+00:00 [Note] [Entrypoint]: Temporary server stopped\n    db  |\n    db  | 2021-10-06 12:13:08+00:00 [Note] [Entrypoint]: MySQL init process done. Ready for start up.\n    db  |\n    db  | 2021-10-06T12:13:09.159115Z 0 [System] [MY-010116] [Server] \/usr\/sbin\/mysqld (mysqld 8.0.26) starting as process 1\n    db  | 2021-10-06T12:13:09.167405Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.\n    db  | 2021-10-06T12:13:09.298925Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.\n    db  | 2021-10-06T12:13:09.488958Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:09.489087Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:09.489934Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.\n    db  | 2021-10-06T12:13:09.490169Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.\n    db  | 2021-10-06T12:13:09.494728Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '\/var\/run\/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.\n    db  | 2021-10-06T12:13:09.509856Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Bind-address: '::' port: 33060, socket: \/var\/run\/mysqld\/mysqlx.sock\n    db  | 2021-10-06T12:13:09.509982Z 0 [System] [MY-010931] [Server] \/usr\/sbin\/mysqld: ready for connections. Version: '8.0.26'  socket: '\/var\/run\/mysqld\/mysqld.sock'  port: 3306  MySQL Community Server - GPL.\n    db  | mbind: Operation not permitted\n    server     | 2021\/10\/06 12:13:14 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n    server     | 2021\/10\/06 12:13:14 INFO mlflow.store.db.utils: Updating database tables\n    server     | INFO  [alembic.runtime.migration] Context impl MySQLImpl.\n    server     | INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n    server     | INFO  [alembic.runtime.migration] Running upgrade  -&gt; 451aebb31d03, add metric step\n    server     | INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -&gt; 90e64c465722, migrate user column to tags\n    server     | INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -&gt; 181f10493468, allow nulls for metric values\n    server     | INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -&gt; df50e92ffc5e, Add Experiment Tags Table\n    server     | INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -&gt; 7ac759974ad8, Update run tags with larger limit\n    server     | INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -&gt; 89d4b8295536, create latest metrics table\n    server     | INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!\n    server     | INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -&gt; 2b4d017a5e9b, add model registry tables to db\n    server     | INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.\n    server     | INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!\n    server     | INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -&gt; cfd24bdc0731, Update run status constraint with killed\n    server     | INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -&gt; 0a8213491aaa, drop_duplicate_killed_constraint\n    server     | INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -&gt; 728d730b5ebd, add registered model tags table\n    server     | INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -&gt; 27a6a02d2cf1, add model version tags table\n    server     | INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -&gt; 84291f40a231, add run_link to model_version\n    server     | INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -&gt; a8c4a736bde6, allow nulls for run_id\n    server     | INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -&gt; 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n    server     | INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -&gt; c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n    server     | INFO  [alembic.runtime.migration] Context impl MySQLImpl.\n    server     | INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n    db  | mbind: Operation not permitted\n    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Starting gunicorn 20.1.0\n    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Listening at: http:\/\/0.0.0.0:5000 (17)\n    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Using worker: sync\n    server     | [2021-10-06 12:13:16 +0000] [19] [INFO] Booting worker with pid: 19\n    server     | [2021-10-06 12:13:16 +0000] [20] [INFO] Booting worker with pid: 20\n    server     | [2021-10-06 12:13:16 +0000] [21] [INFO] Booting worker with pid: 21\n    server     | [2021-10-06 12:13:16 +0000] [22] [INFO] Booting worker with pid: 22\n\n<\/code><\/pre>\n<p>It makes me suspect because on the second line appears <code>- mlflow Error<\/code> but i think that this is why the other builds haven't finished.<\/p>\n<p>Then I've set my environment variables on the client to create the information flow between my script and the storages:<\/p>\n<pre><code>\n    os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http:\/\/localhost:9000\/'\n    os.environ['AWS_ACCESS_KEY_ID'] = 'key'\n    os.environ['AWS_SECRET_ACCESS_KEY'] = 'pw'\n    \n    remote_server_uri = &quot;http:\/\/localhost:5000\/&quot; # server URI\n    mlflow.set_tracking_uri(remote_server_uri)\n    \n    mlflow.set_experiment(&quot;mnist_mLflow_demo&quot;)\n\n<\/code><\/pre>\n<p>finally i trained a tensorflow network and i didn't have problems storing parameters and metrics but gave me some warnings (refering to next error). But the model haven't been auto log, so i tryed to do it manually:<\/p>\n<pre><code>    with mlflow.start_run(run_name = &quot;test0&quot;) as run:\n    \n        mlflow.keras.log_model(model2, 'model2')\n\n    mlflow.end_run()\n<\/code><\/pre>\n<p>It dosen't work and it gives me the next INFO (but essencialy an error):<\/p>\n<pre><code>    INFO:tensorflow:Assets written to: (path)\\Temp\\tmpgr5eaha2\\model\\data\\model\\assets\n    INFO:tensorflow:Assets written to: (path)\\Temp\\tmpgr5eaha2\\model\\data\\model\\assets\n    2021\/10\/06 14:16:00 ERROR mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: (path)\\AppData\\Local\\Temp\\tmpgr5eaha2\\model, flavor: keras)\n    Traceback (most recent call last):\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\environment.py&quot;, line 212, in infer_pip_requirements\n        return _infer_requirements(model_uri, flavor)\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\requirements_utils.py&quot;, line 263, in _infer_requirements\n        modules = _capture_imported_modules(model_uri, flavor)\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\requirements_utils.py&quot;, line 221, in _capture_imported_modules\n        _run_command(\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\requirements_utils.py&quot;, line 163, in _run_command\n        stderr = stderr.decode(&quot;utf-8&quot;)\n    UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in position 349: invalid continuation byte\n\n<\/code><\/pre>\n<p>And the next error:<\/p>\n<pre><code>\n    ClientError                               Traceback (most recent call last)\n    ~\\Python\\Python39\\lib\\site-packages\\boto3\\s3\\transfer.py in upload_file(self, filename, bucket, key, callback, extra_args)\n        278         try:\n    --&gt; 279             future.result()\n        280         # If a client error was raised, add the backwards compatibility layer\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\futures.py in result(self)\n        105             # out of this and propogate the exception.\n    --&gt; 106             return self._coordinator.result()\n        107         except KeyboardInterrupt as e:\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\futures.py in result(self)\n        264         if self._exception:\n    --&gt; 265             raise self._exception\n        266         return self._result\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\tasks.py in __call__(self)\n        125             if not self._transfer_coordinator.done():\n    --&gt; 126                 return self._execute_main(kwargs)\n        127         except Exception as e:\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\tasks.py in _execute_main(self, kwargs)\n        149 \n    --&gt; 150         return_value = self._main(**kwargs)\n        151         # If the task is the final task, then set the TransferFuture's\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\upload.py in _main(self, client, fileobj, bucket, key, extra_args)\n        693         with fileobj as body:\n    --&gt; 694             client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)\n        695 \n    \n    ~\\Python\\Python39\\lib\\site-packages\\botocore\\client.py in _api_call(self, *args, **kwargs)\n        385             # The &quot;self&quot; in this scope is referring to the BaseClient.\n    --&gt; 386             return self._make_api_call(operation_name, kwargs)\n        387 \n    \n    ~\\Python\\Python39\\lib\\site-packages\\botocore\\client.py in _make_api_call(self, operation_name, api_params)\n        704             error_class = self.exceptions.from_code(error_code)\n    --&gt; 705             raise error_class(parsed_response, operation_name)\n        706         else:\n    \n    ClientError: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.\n    \n    During handling of the above exception, another exception occurred:\n    \n    S3UploadFailedError                       Traceback (most recent call last)\n    C:\\Users\\FCAIZA~1\\AppData\\Local\\Temp\/ipykernel_7164\/2476247499.py in &lt;module&gt;\n          1 with mlflow.start_run(run_name = &quot;test0&quot;) as run:\n          2 \n    ----&gt; 3     mlflow.keras.log_model(model2, 'model2')\n          4 \n          5 mlflow.end_run()\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\keras.py in log_model(keras_model, artifact_path, conda_env, custom_objects, keras_module, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, **kwargs)\n        402             mlflow.keras.log_model(keras_model, &quot;models&quot;)\n        403     &quot;&quot;&quot;\n    --&gt; 404     Model.log(\n        405         artifact_path=artifact_path,\n        406         flavor=mlflow.keras,\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\models\\model.py in log(cls, artifact_path, flavor, registered_model_name, await_registration_for, **kwargs)\n        186             mlflow_model = cls(artifact_path=artifact_path, run_id=run_id)\n        187             flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n    --&gt; 188             mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)\n        189             try:\n        190                 mlflow.tracking.fluent._record_logged_model(mlflow_model)\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\tracking\\fluent.py in log_artifacts(local_dir, artifact_path)\n        582     &quot;&quot;&quot;\n        583     run_id = _get_or_start_run().info.run_id\n    --&gt; 584     MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\n        585 \n        586 \n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\tracking\\client.py in log_artifacts(self, run_id, local_dir, artifact_path)\n        975             is_dir: True\n        976         &quot;&quot;&quot;\n    --&gt; 977         self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\n        978 \n        979     @contextlib.contextmanager\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py in log_artifacts(self, run_id, local_dir, artifact_path)\n        332         :param artifact_path: If provided, the directory in ``artifact_uri`` to write to.\n        333         &quot;&quot;&quot;\n    --&gt; 334         self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)\n        335 \n        336     def list_artifacts(self, run_id, path=None):\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\store\\artifact\\s3_artifact_repo.py in log_artifacts(self, local_dir, artifact_path)\n        102                 upload_path = posixpath.join(dest_path, rel_path)\n        103             for f in filenames:\n    --&gt; 104                 self._upload_file(\n        105                     s3_client=s3_client,\n        106                     local_file=os.path.join(root, f),\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\store\\artifact\\s3_artifact_repo.py in _upload_file(self, s3_client, local_file, bucket, key)\n         78         if environ_extra_args is not None:\n         79             extra_args.update(environ_extra_args)\n    ---&gt; 80         s3_client.upload_file(Filename=local_file, Bucket=bucket, Key=key, ExtraArgs=extra_args)\n         81 \n         82     def log_artifact(self, local_file, artifact_path=None):\n    \n    ~\\Python\\Python39\\lib\\site-packages\\boto3\\s3\\inject.py in upload_file(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\n        128     &quot;&quot;&quot;\n        129     with S3Transfer(self, Config) as transfer:\n    --&gt; 130         return transfer.upload_file(\n        131             filename=Filename, bucket=Bucket, key=Key,\n        132             extra_args=ExtraArgs, callback=Callback)\n    \n    ~\\Python\\Python39\\lib\\site-packages\\boto3\\s3\\transfer.py in upload_file(self, filename, bucket, key, callback, extra_args)\n        283         # client error.\n        284         except ClientError as e:\n    --&gt; 285             raise S3UploadFailedError(\n        286                 &quot;Failed to upload %s to %s: %s&quot; % (\n        287                     filename, '\/'.join([bucket, key]), e))\n    \n    S3UploadFailedError: Failed to upload (path)\\AppData\\Local\\Temp\\tmpgr5eaha2\\model\\conda.yaml to artifacts\/1\/5ae5fcef2d07432d811c3d7eb534382c\/artifacts\/model2\/conda.yaml: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.\n\n<\/code><\/pre>\n<p>Do you know how to help me with it? I have been looking all this morning but i did not find a solution. Thank you!!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633525784987,
        "Question_score":1,
        "Question_tags":"python|mysql|docker|minio|mlflow",
        "Question_view_count":969,
        "Owner_creation_time":1580841805373,
        "Owner_last_access_time":1663792613347,
        "Owner_location":"Seville, Spain",
        "Owner_reputation":33,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I found the solution of this issue. It is a tricky problem due to spanish characters, my system's user profile in &quot;C:\/&quot; is &quot;fca\u00f1izares&quot; (Ca\u00f1izares is my first last name). I have created another user named &quot;fcanizares&quot; and all is working fine. Hope you find this solution helpfull.<\/p>\n<p>PS: Moral of the issue, get rid of the extrange characters!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1633680248312,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69466354",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70781419,
        "Question_title":"mlflow static_prefix url in set_tracking_uri is not working",
        "Question_body":"<p>I am starting mlflow with below command<\/p>\n<pre><code>mlflow server --static_prefix=\/myprefix --backend-store-uri postgresql:\/\/psql_user_name:psql_password@localhost\/mlflow_db --default-artifact-root s3:\/\/my-mlflow-bucket\/ --host 0.0.0.0 -p 8000\n<\/code><\/pre>\n<p>everything worked fine and I can see mlflow UI when I open url http:\/\/localhost:8000\/myprefix\nbut when I use mlflow.set_tracking_uri() i have to give url path as &quot;http:\/\/localhost:8000\/&quot;<\/p>\n<p>why cant we use full url , which has static prefix &quot;http:\/\/localhost:8000\/myprefix&quot; ?<\/p>\n<p>if i use full url ,I am getting request to api endpoint fail and api is experiments\/list error 404 !=200\nis there any way to add url with static prefix in set_tracking_uri<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1642658164867,
        "Question_score":0,
        "Question_tags":"machine-learning|artificial-intelligence|mlflow",
        "Question_view_count":246,
        "Owner_creation_time":1625731242200,
        "Owner_last_access_time":1659594320980,
        "Owner_location":null,
        "Owner_reputation":145,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70781419",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":64209196,
        "Question_title":"How to update a previous run into MLFlow?",
        "Question_body":"<p>I would like to update previous runs done with MLFlow, ie. changing\/updating a parameter value to accommodate a change in the implementation. Typical uses cases:<\/p>\n<ul>\n<li>Log runs using a parameter A, and much later, log parameters A and B. It would be useful to update the value of parameter B of previous runs using its default value.<\/li>\n<li>&quot;Specialize&quot; a parameter. Implement a model using a boolean flag as a parameter. Update the implementation to take a string instead. Now we need to update the values of the parameter for the previous runs so that it stays consistent with the new behavior.<\/li>\n<li>Correct a wrong parameter value loggued in the previous runs.<\/li>\n<\/ul>\n<p>It is not always easy to trash the whole experiment as I need to keep the previous runs for statistical purpose. I would like also not to generate new experiments just for a single new parameter, to keep a single database of runs.<\/p>\n<p>What is the best way to do this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1601903047533,
        "Question_score":6,
        "Question_tags":"logging|data-science|mlflow",
        "Question_view_count":2834,
        "Owner_creation_time":1347312347147,
        "Owner_last_access_time":1664043273217,
        "Owner_location":null,
        "Owner_reputation":1022,
        "Owner_up_votes":1127,
        "Owner_down_votes":19,
        "Owner_views":66,
        "Question_last_edit_time":1607788863849,
        "Answer_body":"<p>To add or correct a parameter, metric or artifact of an existing run, pass run_id instead of experiment_id to mlflow.start_run function<\/p>\n<pre><code>with mlflow.start_run(run_id=&quot;your_run_id&quot;) as run:\n    mlflow.log_param(&quot;p1&quot;,&quot;your_corrected_value&quot;)\n    mlflow.log_metric(&quot;m1&quot;,42.0) # your corrected metrics\n    mlflow.log_artifact(&quot;data_sample.html&quot;) # your corrected artifact file\n<\/code><\/pre>\n<p>You can correct, add to, or delete any MLflow run any time after it is complete. Get the run_id either from the UI or by using <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.search_runs\" rel=\"noreferrer\">mlflow.search_runs<\/a>.<\/p>\n<p>Source: <a href=\"https:\/\/towardsdatascience.com\/5-tips-for-mlflow-experiment-tracking-c70ae117b03f\" rel=\"noreferrer\">https:\/\/towardsdatascience.com\/5-tips-for-mlflow-experiment-tracking-c70ae117b03f<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1606920349240,
        "Answer_score":10.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64209196",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71315446,
        "Question_title":"How do I take an already existing MLflow model on my local filesystem and log it to a remote tracking server?",
        "Question_body":"<p>Let's say I already have an existing MLflow model on my local system of the <code>mlflow.pyfunc<\/code> flavor.<\/p>\n<p>The directory looks like this<\/p>\n<pre><code>model\/\n  data\/\n  code\/\n  conda.yml\n  MLmodel\n<\/code><\/pre>\n<p>Where <code>MLmodel<\/code> is something like<\/p>\n<pre><code>flavors:\n  python_function:\n    code: code\n    data: data\n    env: conda.yml\n    loader_module: loader # model\/code\/loader.py has the entrypoint\n<\/code><\/pre>\n<p>I now try and log this model to a remote tracking server using (I'm in the directory above <code>model\/<\/code>, so <code>.\/model\/data<\/code> works, etc)<\/p>\n<pre><code>import mlflow\nmlflow.set_tracking_uri(&quot;http:\/\/localhost:5000&quot;)\nmlflow.pyfunc.log_model(\n  &quot;my-model-artifact&quot;,\n  registered_model_name=&quot;my-model&quot;, # same for all model versions,\n  data_path=&quot;model\/data&quot;,\n  code_path=&quot;model\/code&quot;,\n  loader_module=&quot;model\/code\/loader&quot;\n)\n<\/code><\/pre>\n<p>The tracking server ends up logging a nested MLflow model.. this is inside of the <code>.\/artifacts\/my-model-artifact<\/code> directory on the tracking server<\/p>\n<pre><code>.\/artifacts\/my-model-artifact\n  conda.yaml\n  MLmodel # *not* my MLmodel, one newly generated by MLflow\n  data\/\n  code\/\n<\/code><\/pre>\n<p>Where <code>data<\/code> now points nested to my entire <code>model\/data<\/code> directory and <code>code<\/code> points to a nested <code>model\/code<\/code> directory.<\/p>\n<p>It's like it doesn't understand that I already have this full artifact..<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1646174584637,
        "Question_score":0,
        "Question_tags":"model|mlflow",
        "Question_view_count":336,
        "Owner_creation_time":1425965839877,
        "Owner_last_access_time":1663961190290,
        "Owner_location":"Santa Cruz, CA",
        "Owner_reputation":3256,
        "Owner_up_votes":81,
        "Owner_down_votes":0,
        "Owner_views":164,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71315446",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60088889,
        "Question_title":"How Do You \"Permanently\" Delete An Experiment In Mlflow?",
        "Question_body":"<p>Permanent deletion of an experiment isn't documented anywhere. I'm using Mlflow w\/ backend postgres db<\/p>\n\n<p>Here's what I've run: <\/p>\n\n<pre><code>client = MlflowClient(tracking_uri=server)\nclient.delete_experiment(1)\n<\/code><\/pre>\n\n<p>This deletes the the experiment, but when I run a new experiment with the same name as the one I just deleted, it will return this error:<\/p>\n\n<pre><code>mlflow.exceptions.MlflowException: Cannot set a deleted experiment 'cross-sell' as the active experiment. You can restore the experiment, or permanently delete the  experiment to create a new one.\n<\/code><\/pre>\n\n<p>I cannot find anywhere in the documentation that shows how to permanently delete everything.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1580970401043,
        "Question_score":20,
        "Question_tags":"python|mlflow",
        "Question_view_count":13984,
        "Owner_creation_time":1443225809767,
        "Owner_last_access_time":1663898334270,
        "Owner_location":"Vancouver, BC, Canada",
        "Owner_reputation":2332,
        "Owner_up_votes":133,
        "Owner_down_votes":3,
        "Owner_views":560,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Unfortunately it seems there is no way to do this via the UI or CLI at the moment :-\/<\/p>\n\n<p>The way to do it depends on the type of backend file store that you are using.<\/p>\n\n<p><strong>Filestore<\/strong>:<\/p>\n\n<p>If you are using the filesystem as a storage mechanism (the default) then it is easy. The 'deleted' experiments are moved to a <code>.trash<\/code> folder. You just need to clear that out:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>rm -rf mlruns\/.trash\/*\n<\/code><\/pre>\n\n<p>As of the current version of the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/cli.html#mlflow-experiments-delete\" rel=\"noreferrer\">documentation<\/a> (1.7.2), they remark:<\/p>\n\n<blockquote>\n  <p>It is recommended to use a cron job or an alternate workflow mechanism to clear <code>.trash<\/code> folder.<\/p>\n<\/blockquote>\n\n<p><strong>SQL Database:<\/strong><\/p>\n\n<p>This is more tricky, as there are dependencies that need to be deleted. I am using MySQL, and these commands work for me:<\/p>\n\n<pre class=\"lang-sql prettyprint-override\"><code>USE mlflow_db;  # the name of your database\nDELETE FROM experiment_tags WHERE experiment_id=ANY(\n    SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n);\nDELETE FROM latest_metrics WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM metrics WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM tags WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM runs WHERE experiment_id=ANY(\n    SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n);\nDELETE FROM experiments where lifecycle_stage=\"deleted\";\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1585231513452,
        "Answer_score":22.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60088889",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70967052,
        "Question_title":"Does Hugging face defaults allow to log mlflow artifacts and name every run of mlflow log?",
        "Question_body":"<p>I am training a simple binary classification model using Hugging face models using pytorch.<\/p>\n<p>Bert PyTorch HuggingFace.<\/p>\n<p>Here is the code:<\/p>\n<pre><code>import transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\nfrom transformers import AutoTokenizer\n\n \nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup,BertConfig\n<\/code><\/pre>\n<pre><code>def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n   \n\n    predictions = np.argmax(logits, axis=-1)\n    \n    acc = np.sum(predictions == labels) \/ predictions.shape[0]\n    return {&quot;accuracy&quot;: acc,\n            'precision': metrics.precision_score(labels, predictions),\n            'recall': metrics.recall_score(labels, predictions),\n            'f1': metrics.f1_score(labels, predictions)}\n\ntraining_args = tr.TrainingArguments(\n    #report_to = 'wandb',\n    output_dir='\/home\/pc\/proj\/Exp2_conv_stampy_data\/results_exp0',          # output directory\n    overwrite_output_dir = True,\n    num_train_epochs=2,              # total number of training epochs\n    per_device_train_batch_size=32,  # batch size per device during training\n    per_device_eval_batch_size=32,   # batch size for evaluation\n    learning_rate=2e-5,\n    warmup_steps=200,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='.\/logs_exp0',            # directory for storing logs\n    logging_steps=137,\n    evaluation_strategy=&quot;epoch&quot;\n    ,save_strategy=&quot;epoch&quot;\n    ,load_best_model_at_end=True\n    ,fp16=True\n    ,run_name=&quot;final_model0&quot;\n    \n)\n\n\n# counter = 0\n# results_lst = []\n\nfrom transformers import TrainerCallback\nfrom copy import deepcopy\n\nmodel = tr.XLMRobertaForSequenceClassification.from_pretrained(&quot;\/home\/pc\/multilingual_toxic_xlm_roberta&quot;,problem_type=&quot;single_label_classification&quot;, num_labels=2,ignore_mismatched_sizes=True, id2label={0: 'negative', 1: 'positive'})\n\n\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512, return_tensors=&quot;pt&quot;)\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512, return_tensors=&quot;pt&quot;)\n\n\ntrain_data = SEDataset(train_encodings, train_labels)\nval_data = SEDataset(val_encodings, val_labels)\n\nmodel.to(device)\n\nclass CustomCallback(TrainerCallback):\n    \n    def __init__(self, trainer) -&gt; None:\n        super().__init__()\n        self._trainer = trainer\n    \n    def on_epoch_end(self, args, state, control, **kwargs):\n        if control.should_evaluate:\n            control_copy = deepcopy(control)\n            self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=&quot;train&quot;)\n            return control_copy\n\ntrainer = tr.Trainer(\n    model=model,                         # the instantiated Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_data,         # training dataset\n    eval_dataset=val_data,          # evaluation dataset\n    compute_metrics=compute_metrics    # the callback that computes metrics of interest\n)\ntrainer.add_callback(CustomCallback(trainer)) \ntrain = trainer.train()\n\n\n\ntrainer.save_model(&quot;\/home\/pc\/proj\/Exp2_conv_stampy_data\/result_toxic_model_exp0&quot;)\n\n<\/code><\/pre>\n<p>I see by default <code>mlruns<\/code> directory is created.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rD25L.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rD25L.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>What is <code>0' and what are these 2 folders inside <\/code>0`?<\/strong><\/p>\n<p><strong>How can rename to something useful and understandable.?<\/strong><\/p>\n<p><strong>If I run multiple runs, how can I log every run of model with something like <code>run1<\/code>, <code>run2<\/code> under same experiment?<\/strong><\/p>\n<p><strong>Also I see artifact folder is empty, how to log final model?<\/strong><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1643871620400,
        "Question_score":0,
        "Question_tags":"pytorch|huggingface-transformers|mlflow",
        "Question_view_count":818,
        "Owner_creation_time":1528361086053,
        "Owner_last_access_time":1663924548837,
        "Owner_location":null,
        "Owner_reputation":1127,
        "Owner_up_votes":526,
        "Owner_down_votes":93,
        "Owner_views":283,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70967052",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60531166,
        "Question_title":"How to safely shutdown mlflow ui?",
        "Question_body":"<p>After running <code>mlflow ui<\/code> on a remote server, I'm unable to reopen the <code>mlflow ui<\/code> again.<br>\nA workaround is to kill all my processes in the server using <code>pkill -u MyUserName<\/code>.<br>\nOtherwise I get the following error:  <\/p>\n\n<pre><code>[INFO] Starting gunicorn 20.0.4  \n[ERROR] Connection in use: ('127.0.0.1', 5000)\n[ERROR] Retrying in 1 second.  \n...\nRunning the mlflow server failed. Please see ther logs above for details.\n<\/code><\/pre>\n\n<p>I understand the error but I don't understand:<br>\n1. What is the correct way to shutdown <code>mlflow ui<\/code><br>\n2. How can I identify the <code>mlflow ui<\/code> process in order to only kill that process and not use the <code>pkill<\/code>  <\/p>\n\n<p>Currently I close the browser or use ctrl+C <\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1583341247020,
        "Question_score":6,
        "Question_tags":"python|r|machine-learning|mlflow",
        "Question_view_count":9850,
        "Owner_creation_time":1453321149903,
        "Owner_last_access_time":1662925257297,
        "Owner_location":"Israel",
        "Owner_reputation":1153,
        "Owner_up_votes":113,
        "Owner_down_votes":14,
        "Owner_views":168,
        "Question_last_edit_time":1630433880327,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60531166",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":66858683,
        "Question_title":"How to integrate mlflow and airflow? Is there any way to connect to mlflow server from airflow",
        "Question_body":"<p>Lets say I have a ML model in mlflow server artifacts. I want to run this model from airflow Dag. Also after running in airflow, metric logs should be visible in mlflow.\nHow can I achieve this?\nThere are connections in airflow, I couldn't find any connection type for mlflow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617038196660,
        "Question_score":2,
        "Question_tags":"airflow|mlflow",
        "Question_view_count":389,
        "Owner_creation_time":1496203946490,
        "Owner_last_access_time":1664009984060,
        "Owner_location":null,
        "Owner_reputation":838,
        "Owner_up_votes":89,
        "Owner_down_votes":2,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66858683",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60286506,
        "Question_title":"mlflow Exception: Run with UUID is already active",
        "Question_body":"<p>Used mlflow.set_tracking_uri to set up tracking_uri and set_experiment, got an error and check back to run following code again. got an error that \"Exception: Run with UUID  is already active.\"\nTry to use <code>mlflow.end_run<\/code> to end current run, but got RestException: RESOURCE_DOES_NOT_EXIST: Run UUID not found.\nCurrently stuck in this infinite loop. Any suggestion?    <\/p>\n\n<pre><code>    mlflow.set_experiment(\"my_experiment\")\n    mlflow.start_run(run_name='my_project')\n    mlflow.set_tag('input_len',len(input))\n    mlflow.log_param('metrics', r2)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1582047247790,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":2813,
        "Owner_creation_time":1409958744170,
        "Owner_last_access_time":1663993998083,
        "Owner_location":"Los Angeles, CA, USA",
        "Owner_reputation":1997,
        "Owner_up_votes":33,
        "Owner_down_votes":0,
        "Owner_views":175,
        "Question_last_edit_time":1582049042567,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60286506",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73806968,
        "Question_title":"How to use a @pandas_udf function inside a class with pyspark?",
        "Question_body":"<p>I am trying to use one of the Hugging Face models with ML flow. My input is a pyspark DataFrame.\nThe issue is Mlflow doesn't support directly HuggingFace models, so need to use the flavor pyfunc to save it. So I need create a Python class that inherits from PythonModel and then place everything needed there.\nHow can I use a pandas_udf function inside this PythonModel? It keeps failing because I haven't specified the hint type for all the parameters inside my pandas_udf.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class RobertaClassifier(PythonModel):\n\n    def load_context(self, context: PythonModelContext):\n        import os\n        from transformers.models.auto import AutoConfig,   AutoModelForSequenceClassification\n        from transformers.models.auto.tokenization_auto import AutoTokenizer\n        \n        config_file = os.path.dirname(context.artifacts[&quot;config&quot;])\n        self.config = AutoConfig.from_pretrained(config_file)\n        self.tokenizer = AutoTokenizer.from_pretrained(config_file)\n        self.model = AutoModelForSequenceClassification.from_pretrained(config_file, config=self.config)\n        \n        if torch.cuda.is_available():\n            print('[INFO] Model is being sent to CUDA device as GPU is available')\n            self.model = self.model.cuda()\n        else:\n            print('[INFO] Model will use CPU runtime')\n        \n        _ = self.model.eval()\n    \n    \n    @pandas_udf(&quot;label string, score float&quot;)\n    def predict_batch_udf(self, data: pd.Series) -&gt; pd.Series:\n        import torch\n        import pandas as pd\n        \n        with torch.no_grad():\n            inputs = preprocessing(data['content'])\n            inputs = self.tokenizer(inputs, padding=True, return_tensors='pt', max_length=512, truncation=True)\n        \n            if self.model.device.index != None:\n                torch.cuda.empty_cache()\n                for key in inputs.keys():\n                    inputs[key] = inputs[key].to(self.model.device.index)\n\n            predictions = self.model(**inputs)\n            probs = torch.nn.Softmax(dim=1)(predictions.logits)\n            probs = probs.detach().cpu().numpy()\n\n            labels = probs.argmax(axis=1)\n            scores = probs.max(axis=1)\n\n            return labels, scores\n        \n    def predict(self, context: PythonModelContext, data: pd.Series) -&gt; pd.Series:\n        import math\n        import numpy as np\n        \n        batch_size = 64\n        sample_size = len(data)\n        \n        labels = np.zeros(sample_size)\n        scores = np.zeros(sample_size)\n\n        for batch_idx in range(0, math.ceil(sample_size \/ batch_size)):\n            bfrom = batch_idx * batch_size\n            bto = bfrom + batch_size\n            \n            l, s = self._predict_batch(data.iloc[bfrom:bto])\n            labels[bfrom:bto] = l\n            scores[bfrom:bto] = s\n            \n        return pd.DataFrame({'label': [self.config.id2label[l] for l in labels], \n                             'score': scores })\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663794595057,
        "Question_score":0,
        "Question_tags":"dataframe|class|pyspark|mlflow|pandas-udf",
        "Question_view_count":18,
        "Owner_creation_time":1663793210607,
        "Owner_last_access_time":1663883366877,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1663827527230,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73806968",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56866214,
        "Question_title":"No usable temporary directory found with AWS Lambda function",
        "Question_body":"<p>I am trying to download a model with <code>mlflow<\/code> in an <code>aws lambda function<\/code> as described here: <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#referencing-artifacts\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#referencing-artifacts<\/a><\/p>\n\n<p>However the following error is thrown:<\/p>\n\n<pre><code>  File \"\/tmp\/mlflow-api-server\/mlflow\/tracking\/artifact_utils.py\", line 66, in _download_artifact_from_uri\n  artifact_path=artifact_path, dst_path=output_path)\n  File \"\/tmp\/mlflow-api-server\/mlflow\/store\/artifact_repo.py\", line 94, in download_artifacts\n  dst_path = tempfile.mkdtemp()\n  File \"\/var\/lang\/lib\/python3.6\/tempfile.py\", line 360, in mkdtemp\n  prefix, suffix, dir, output_type = _sanitize_params(prefix, suffix, dir)\n  File \"\/var\/lang\/lib\/python3.6\/tempfile.py\", line 130, in _sanitize_params\n  dir = gettempdir()\n  File \"\/var\/lang\/lib\/python3.6\/tempfile.py\", line 298, in gettempdir\n  tempdir = _get_default_tempdir()\n  File \"\/var\/lang\/lib\/python3.6\/tempfile.py\", line 233, in _get_default_tempdir\n  dirlist)\nFileNotFoundError: [Errno 2] No usable temporary directory found in ['\/tmp', '\/var\/tmp', '\/usr\/tmp']\n<\/code><\/pre>\n\n<p>The sklearn <code>model.pkl<\/code> file that <code>mlflow<\/code> should download has 627 Byte and the <code>aws lambda<\/code> limit should be 512 MB which should be enough space.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1562143532867,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|aws-lambda|mlflow",
        "Question_view_count":2058,
        "Owner_creation_time":1510064331503,
        "Owner_last_access_time":1664041948290,
        "Owner_location":"M\u00fcnchen, Deutschland",
        "Owner_reputation":5537,
        "Owner_up_votes":1253,
        "Owner_down_votes":7,
        "Owner_views":215,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56866214",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70010405,
        "Question_title":"Run experiments on Azure ML with Kedro and Mlflow",
        "Question_body":"<p>I'm trying to run the whole Kedro pipeline as an Azure ML experiment. I had two options here. The first one was to use the built-in logging feature of Azure ML and the second one was to use the azumeml-mlflow package that integrates Azure ML with Mlflow.<\/p>\n<p>I only tried the second approach as I did not know how to implement the Run() method of Azure ML inside the Kedro hooks.<\/p>\n<p>So, for the second approach, I presumed everything should be the same as when using Mlflow only. However, I couldn't get it to work even though it worked well outside of the Kedro structure ==&gt; I could launch experiments from other scripts.<\/p>\n<p>What I get with Kedro is that the pipeline runs well but nothing happens on Azure ML.<\/p>\n<p>Here's the code (hooks are inside a ModelTrackingHooks class):<\/p>\n<pre><code>@hook_impl\ndef before_pipeline_run(self, run_params: Dict[str, Any]) -&gt; None:\n    &quot;&quot;&quot;Hook implementation to start an MLflow run\n    with the same run_id as the Kedro pipeline run.\n    &quot;&quot;&quot;\n\n\n    # Get Azure workspace\n    ws = Workspace.get(name=&quot;...&quot;,\n                       subscription_id=&quot;...&quot;,\n                       resource_group=&quot;...&quot;)\n    # Set tracking uri\n    mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n\n    # Create an Azure ML experiment in the workspace\n    experiment = Experiment(workspace=ws, name='kedro-mlflow-experiment')\n    mlflow.set_experiment(experiment.name)\n\n    #Start logging\n    mlflow.start_run(run_name=run_params[&quot;run_id&quot;])\n    mlflow.log_params(run_params)        \n\n@hook_impl\ndef after_node_run(\n    self, node: Node, outputs: Dict[str, Any], inputs: Dict[str, Any]\n) -&gt; None:\n    &quot;&quot;&quot;Hook implementation to add model tracking after some node runs.\n    In this example, we will:\n    * Log the parameters after the data splitting node runs.\n    * Log the model after the model training node runs.\n    * Log the model's metrics after the model evaluating node runs.\n    &quot;&quot;&quot;\n    \n    if node._func_name == &quot;cross_val&quot;:\n        mlflow.log_params(\n            {&quot;best_estimator&quot;: outputs[&quot;best_estimator&quot;],\n             &quot;best_params&quot;: outputs[&quot;best_params&quot;]}\n        )\n        model = outputs[&quot;validated_model&quot;]\n        mlflow.sklearn.log_model(model, &quot;model&quot;)\n\n    elif node._func_name == &quot;fit_and_save_transformer&quot;:\n        transformer = outputs[&quot;custom_transformer&quot;]\n        mlflow.sklearn.log_model(transformer, &quot;customer_transformer&quot;)\n\n    elif node._func_name == &quot;classification_reporting&quot;:\n        mlflow.log_metrics(outputs[&quot;metrics&quot;])\n    \n\n@hook_impl\ndef after_pipeline_run(self) -&gt; None:\n    &quot;&quot;&quot;Hook implementation to end the MLflow run\n    after the Kedro pipeline finishes.\n    &quot;&quot;&quot;\n\n    mlflow.end_run()\n<\/code><\/pre>\n<p>Am I doing it the wrong way ?<\/p>\n<p>Do you have any idea or examples on how to use Kedro and Azure ML by leveraging only the built-in capabilities of Azure ML (i.e. without going through Mlflow) ?<\/p>\n<p>Thank you in advance.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1637177187910,
        "Question_score":0,
        "Question_tags":"python|mlflow|azure-machine-learning-service|kedro|mlops",
        "Question_view_count":266,
        "Owner_creation_time":1586517832390,
        "Owner_last_access_time":1660328393330,
        "Owner_location":null,
        "Owner_reputation":127,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":1637184025436,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70010405",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73304176,
        "Question_title":"mlflow warning: Experiment ID mismatch for exp Iris. ID recorded as \u20181\u2019 in metadata\u2026",
        "Question_body":"<p>I\u2019m completely new to mlflow.<\/p>\n<p>I\u2019ve started with a couple of the standard tutorials and created examples using the well-known Iris and wine-quality datasets and named the experiments accordingly.<\/p>\n<p>When I run my code I get warnings:<\/p>\n<pre><code>WARNING:root:Experiment ID mismatch for exp iris.  ID recorded as \u20181\u2019 in meta data.  Experiment will be ignored.\n\nNoneType: None\n\nWARNING:root:Experiment ID mismatch for exp mlruns. ID recorded as \u20188\u2019 in meta data.  Experiment will be ignored.\n\nNoneType: None\n\nWARNING:root:Experiment ID mismatch for exp wine-quality. ID recorded as \u20185\u2019 in meta data.  Experiment will be ignored.\n\nNoneType: None\n<\/code><\/pre>\n<p>However, when I run the following in a jupyter notebook cell:<\/p>\n<pre><code>From mlflow.tracking import MlflowClient\n\nclient=Mlflow.Client()\n\nexperiments=client.list_experiments()\n\nexperiments\n<\/code><\/pre>\n<p>I see the list of experiments, including the 3 above, whose ID matches their names.<\/p>\n<p>When I look in the <code>mlruns\/wine-quality\/1<\/code> folder says, the <code>meta.yaml<\/code> states correctly that <code>Experiment id<\/code> is 5.<\/p>\n<p>Can someone help explain why I am getting these warnings?<\/p>\n<p>(When I run <code>mlflow ui \u2014backend-store-uri file:C:\/Users\/jb8\/mlruns<\/code> I see the experimental runs being logged as I\u2019d hope\u2026)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1660125120537,
        "Question_score":0,
        "Question_tags":"python|warnings|mlflow",
        "Question_view_count":36,
        "Owner_creation_time":1513941499320,
        "Owner_last_access_time":1663929826060,
        "Owner_location":"Portsmouth, United Kingdom",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1660143634743,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73304176",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65824711,
        "Question_title":"Mlflow download_artifacts giving Not Found error",
        "Question_body":"<p>I have mlflow and minio running under docker compose. Mlflow successfully logs artifacts to minio and retrieves them. Minio and mlflow have their relevant ports 900 and 5000 exposed by docker.\nIf I run MlflowClient().download_artifacts for predictions from within the docker environment, everything works smoothly.\nIf I run MlflowClient().download_artifacts from outside docker (local machine or remotely), I get the below error. There is no issue in fetching the logged metrics.<\/p>\n<p>botocore.exceptions.ClientError: An error occurred (404) when calling the ListObjectsV2 operation: Not Found<\/p>\n<p>My code:<\/p>\n<pre><code>os.environ['AWS_ACCESS_KEY_ID'] = &quot;x&quot;\nmlflow.set_tracking_uri('http:\/\/10.0.0.1:5000')\nos.environ['AWS_SECRET_ACCESS_KEY'] = &quot;x&quot;\nos.environ['MINIO_ACCESS_KEY_ID'] = &quot;x&quot;\nos.environ['MINIO_SECRET_ACCESS_KEY'] = &quot;x\/me &quot;\n\nfrom mlflow.tracking import MlflowClient\nMlflowClient().download_artifacts(&quot;323e1527d49d4e77bd14c387bbdf6372&quot;, &quot;model&quot;, local_dir)\n<\/code><\/pre>\n<p>Any help would be most appreciated.<\/p>\n<p>Thanks<\/p>\n<p>Best Regards,<\/p>\n<p>Adeel<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1611222048143,
        "Question_score":2,
        "Question_tags":"docker|docker-compose|mlflow",
        "Question_view_count":254,
        "Owner_creation_time":1445990517173,
        "Owner_last_access_time":1663982428387,
        "Owner_location":"Sydney, New South Wales, Australia",
        "Owner_reputation":689,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":87,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65824711",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72994988,
        "Question_title":"How to mlflow-autolog a sklearn ConfusionMatrixDisplay?",
        "Question_body":"<p>I'm trying to log the plot of a <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator\" rel=\"nofollow noreferrer\">confusion matrix generated with scikit-learn<\/a> for a <em>test<\/em> set using <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.sklearn.html\" rel=\"nofollow noreferrer\">mlflow's support for scikit-learn<\/a>.<\/p>\n<p>For this, I tried something that resemble the code below (I'm using mlflow hosted on Databricks, and <code>sklearn==1.0.1<\/code>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sklearn.datasets\nimport pandas as pd\nimport numpy as np\nimport mlflow\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nmlflow.set_tracking_uri(&quot;databricks&quot;)\nmlflow.set_experiment(&quot;\/Users\/name.surname\/plotcm&quot;)\n\ndata = sklearn.datasets.fetch_20newsgroups(categories=['alt.atheism', 'sci.space'])\n\ndf = pd.DataFrame(data = np.c_[data['data'], data['target']])\\\n       .rename({0:'text', 1:'class'}, axis = 'columns')\n\ntrain, test = train_test_split(df)\n\nmy_pipeline = Pipeline([\n    ('vectorizer', TfidfVectorizer()),\n    ('classifier', SGDClassifier(loss='modified_huber')),\n])\n\nmlflow.sklearn.autolog()\n\nfrom sklearn.metrics import ConfusionMatrixDisplay # should I import this after the call to `.autolog()`?\n\nmy_pipeline.fit(train['text'].values, train['class'].values)\n\ncm = ConfusionMatrixDisplay.from_predictions(\n      y_true=test[&quot;class&quot;], y_pred=my_pipeline.predict(test[&quot;text&quot;])\n  )\n<\/code><\/pre>\n<p>while the confusion matrix for the training set is saved in my mlflow run, no png file is created in the mlflow frontend for the <code>test<\/code> set.<\/p>\n<p>If I try to add<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>cm.figure_.savefig('test_confusion_matrix.png')\nmlflow.log_artifact('test_confusion_matrix.png')\n<\/code><\/pre>\n<p>that does the job, but requires explicitly logging the artifact.<\/p>\n<p>Is there an idiomatic\/proper way to autolog the confusion matrix computed using a test set after <code>my_pipeline.fit()<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657892681357,
        "Question_score":0,
        "Question_tags":"python|scikit-learn|confusion-matrix|mlflow",
        "Question_view_count":157,
        "Owner_creation_time":1415722650717,
        "Owner_last_access_time":1664051478173,
        "Owner_location":"Verona, VR, Italy",
        "Owner_reputation":4811,
        "Owner_up_votes":376,
        "Owner_down_votes":73,
        "Owner_views":713,
        "Question_last_edit_time":1658083880967,
        "Answer_body":"<p>The proper way to do this is to use <code>mlflow.log_figure<\/code> as a fluent API announced in <code>MLflow 1.13.0<\/code>. You can read the documentation <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_figure\" rel=\"nofollow noreferrer\">here<\/a>. This code will do the job.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.log_figure(cm.figure_, 'test_confusion_matrix.png')\n<\/code><\/pre>\n<p>This function implicitly store the image, and then calls <code>log_artifact<\/code> against that path, something like you did.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1658304934100,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72994988",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58956459,
        "Question_title":"How to run authentication on a mlFlow server?",
        "Question_body":"<p>As I am logging my entire models and params into mlflow I thought it will be a good idea to have  it protected under a user name and password.<\/p>\n\n<p>I use the following code to run the mlflow server<\/p>\n\n<p><code>mlflow server --host 0.0.0.0 --port 11111<\/code>\nworks perfect,in mybrowser i type <code>myip:11111<\/code> and i see everything (which eventually is the problem)<\/p>\n\n<p>If I understood the documentation and the following <a href=\"https:\/\/groups.google.com\/forum\/#!topic\/mlflow-users\/E9QW4HdS8a8\" rel=\"noreferrer\">https:\/\/groups.google.com\/forum\/#!topic\/mlflow-users\/E9QW4HdS8a8<\/a> link here correct, I should use nginx to create the authentication.<\/p>\n\n<p>I installed <code>nginx open sourcre<\/code>  and <code>apache2-utils<\/code><\/p>\n\n<p>created <code>sudo htpasswd -c \/etc\/apache2\/.htpasswd user1<\/code> user and passwords.<\/p>\n\n<p>I edited my <code>\/etc\/nginx\/nginx.conf<\/code> to the following:<\/p>\n\n<pre><code>server {\n        listen 80;\n        listen 443 ssl;\n\n        server_name my_ip;\n        root NOT_SURE_WHICH_PATH_TO_PUT_HERE, THE VENV?;\n        location \/ {\n            proxy_pass                      my_ip:11111\/;\n            auth_basic                      \"Restricted Content\";\n            auth_basic_user_file \/home\/path to the password file\/.htpasswd;\n        }\n    }\n<\/code><\/pre>\n\n<p><strong>but no authentication appears.<\/strong><\/p>\n\n<p>if I change the conf to listen to  <code>listen 11111<\/code>\nI get an error that the port is already in use ( of course, by the mlflow server....)<\/p>\n\n<p>my wish is to have a authentication window before anyone can enter by the mlflow with a browser.<\/p>\n\n<p>would be happy to hear any suggestions.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":1,
        "Question_creation_time":1574259400087,
        "Question_score":11,
        "Question_tags":"nginx|basic-authentication|mlflow",
        "Question_view_count":13870,
        "Owner_creation_time":1554298968017,
        "Owner_last_access_time":1662838530057,
        "Owner_location":"wondeland",
        "Owner_reputation":1540,
        "Owner_up_votes":21,
        "Owner_down_votes":3,
        "Owner_views":118,
        "Question_last_edit_time":null,
        "Answer_body":"<p>the problem here is that both <code>mlflow<\/code> and <code>nginx<\/code> are trying to run on the <strong>same port<\/strong>... <\/p>\n\n<ol>\n<li><p>first lets deal with nginx:<\/p>\n\n<p>1.1 in \/etc\/nginx\/sites-enable make a new file <code>sudo nano mlflow<\/code> and delete the exist default.<\/p>\n\n<p>1.2 in mlflow file:<\/p><\/li>\n<\/ol>\n\n<pre><code>server {\n    listen YOUR_PORT;\n    server_name YOUR_IP_OR_DOMAIN;\n    auth_basic           \u201cAdministrator\u2019s Area\u201d;\n    auth_basic_user_file \/etc\/apache2\/.htpasswd; #read the link below how to set username and pwd in nginx\n\n    location \/ {\n        proxy_pass http:\/\/localhost:8000;\n        include \/etc\/nginx\/proxy_params;\n        proxy_redirect off;\n    }\n}\n<\/code><\/pre>\n\n<p>1.3.  restart nginx <code>sudo systemctl restart nginx<\/code><\/p>\n\n<ol start=\"2\">\n<li>on your server run mlflow  <code>mlflow server --host localhost --port 8000<\/code><\/li>\n<\/ol>\n\n<p>Now if you try access the YOUR_IP_OR_DOMAIN:YOUR_PORT within your browser an auth popup should appear, enter your host and pass and now you in mlflow<\/p>\n\n<ol start=\"3\">\n<li><p>now there are 2 options to tell the mlflow server about it:<\/p>\n\n<p>3.1 set username and pwd as environment variable \n<code>export MLFLOW_TRACKING_USERNAME=user export MLFLOW_TRACKING_PASSWORD=pwd<\/code><\/p>\n\n<p>3.2 edit in your <code>\/venv\/lib\/python3.6\/site-packages\/mlflowpackages\/mlflow\/tracking\/_tracking_service\/utils.py<\/code> the function <\/p><\/li>\n<\/ol>\n\n<pre><code>def _get_rest_store(store_uri, **_):\n    def get_default_host_creds():\n        return rest_utils.MlflowHostCreds(\n            host=store_uri,\n            username=replace with nginx user\n            password=replace with nginx pwd\n            token=os.environ.get(_TRACKING_TOKEN_ENV_VAR),\n            ignore_tls_verification=os.environ.get(_TRACKING_INSECURE_TLS_ENV_VAR) == 'true',\n        )\n<\/code><\/pre>\n\n<p>in your .py file where you work with mlflow:<\/p>\n\n<pre><code>import mlflow\nremote_server_uri = \"YOUR_IP_OR_DOMAIN:YOUR_PORT\" # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\nmlflow.set_experiment(\"\/my-experiment\")\nwith mlflow.start_run():\n    mlflow.log_param(\"a\", 1)\n    mlflow.log_metric(\"b\", 2)\n<\/code><\/pre>\n\n<p>A link to nginx authentication doc <a href=\"https:\/\/docs.nginx.com\/nginx\/admin-guide\/security-controls\/configuring-http-basic-authentication\/\" rel=\"noreferrer\">https:\/\/docs.nginx.com\/nginx\/admin-guide\/security-controls\/configuring-http-basic-authentication\/<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1576255052616,
        "Answer_score":8.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58956459",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58519367,
        "Question_title":"MLFlow model not logging to Azure Blob Storage",
        "Question_body":"<p>I am trying to use MLFlow to log artifacts to Azure Blob Storage. Though the logging to dbfs works fine, when I try to log it to Azure Blob Storage, I only see a folder with the corresponding runid but inside it there are no contents.<\/p>\n\n<p>Here is what I do-<\/p>\n\n<ol>\n<li><p>Create a experiment from Azure Databricks, give it a name and the artifacts location as wasbs:\/\/mlartifacts@myazurestorageaccount.blob.core.windows.net\/ .<\/p><\/li>\n<li><p>In the spark cluster, in the environemtn Variables section pass on the AZURE_STORAGE_ACCESS_KEY=\"ValueoftheKey\" <\/p><\/li>\n<li>In the notebook, use mlflow to log metrics, param and finally the model using a snippet like below<\/li>\n<\/ol>\n\n<pre><code>\nwith mlflow.start_run():\n      lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n      lr.fit(train_x, train_y)\n\n      predicted_qualities = lr.predict(test_x)\n\n      (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n      print(\"Elasticnet model (alpha=%f, l1_ratio=%f):\" % (alpha, l1_ratio))\n      print(\"  RMSE: %s\" % rmse)\n      print(\"  MAE: %s\" % mae)\n      print(\"  R2: %s\" % r2)\n\n      mlflow.log_param(\"alpha\", alpha)\n      mlflow.log_param(\"l1_ratio\", l1_ratio)\n      mlflow.log_metric(\"rmse\", rmse)\n      mlflow.log_metric(\"r2\", r2)\n      mlflow.log_metric(\"mae\", mae)\n\n      mlflow.sklearn.log_model(lr, \"model\")\n<\/code><\/pre>\n\n<p>Of course before using it , I set the experiment to the one where I have defined the artifacts store to be azure blob storage<\/p>\n\n<pre><code>experiment_name = \"\/Users\/user@domain.com\/mltestazureblob\"\nmlflow.set_experiment(experiment_name)\n<\/code><\/pre>\n\n<p>The metrices and params I can from the MLFlow  UI within Databricks but as since my artifacts location is Azure Blob Storage , I expect the model, the .pkl and conda.yaml file to be in the container in the Azure Blob Storage but when I go to check it, I only see a folder corresponding to the run id of the experiment but with nothing inside.<\/p>\n\n<p>I do not know what I am missing. In case, someone needs additional details I will be happy to provide.<\/p>\n\n<p>Point to note everything works fine when I use the default location i.e. dbfs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1571821965343,
        "Question_score":0,
        "Question_tags":"databricks|azure-databricks|mlflow",
        "Question_view_count":812,
        "Owner_creation_time":1428654714763,
        "Owner_last_access_time":1664012257383,
        "Owner_location":null,
        "Owner_reputation":596,
        "Owner_up_votes":53,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58519367",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67062145,
        "Question_title":"Continue stopped run in MLflow",
        "Question_body":"<p>We run our experiment on AWS spot instances. Sometimes the experiments are stopped, and we would prefer to continue logging to the same run. How can you set the run-id of the active run?<\/p>\n<p>Something like this pseudocode (not working):<\/p>\n<pre><code>if new:\n    mlflow.start_run(experiment_id=1, run_name=x)\nelse:\n    mlflow.set_run(run_id)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618244956103,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":131,
        "Owner_creation_time":1484838464573,
        "Owner_last_access_time":1663858511743,
        "Owner_location":"Amsterdam, Nederland",
        "Owner_reputation":3937,
        "Owner_up_votes":672,
        "Owner_down_votes":27,
        "Owner_views":387,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can pass the run_id directly to <code>start_run<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.start_run(experiment_id=1,\n                 run_name=x,\n                 run_id=&lt;run_id_of_interrupted_run&gt; # pass None to start a new run\n                 ) \n<\/code><\/pre>\n<p>Of course, you have to store the run_id for this. You can get it with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.entities.html#mlflow.entities.RunInfo.run_id\" rel=\"nofollow noreferrer\"><code>run.info.run_id<\/code><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1631884865500,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67062145",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70259594,
        "Question_title":"Setting array of tags to MLFlow registered model",
        "Question_body":"<p>I have a model registered in ML Flow and would like associate a list of tags to that model.\nBut when i looked at the reference APIs, it looks like we can add only one tag at a time with a single http request.<\/p>\n<pre><code>https:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#set-registered-model-tag\n<\/code><\/pre>\n<p>Is it possible to create an array of tags and associate that with model in a single http call ?\nLike how we do during model creation API ?<\/p>\n<pre><code>https:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#create-registeredmodel\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1638877684190,
        "Question_score":0,
        "Question_tags":"databricks|azure-databricks|mlflow",
        "Question_view_count":173,
        "Owner_creation_time":1568969745383,
        "Owner_last_access_time":1663915778593,
        "Owner_location":null,
        "Owner_reputation":344,
        "Owner_up_votes":39,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Question_last_edit_time":1638878114696,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70259594",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71151054,
        "Question_title":"How to log a table of metrics into mlflow",
        "Question_body":"<p>I am trying to see if mlflow is the right place to store my metrics in the model tracking.  According to the doc log_metric takes either a key value or a dict of key-values.  I am wondering how to log something like below into mlflow so it can be visualized meaningfully.<\/p>\n<pre><code>          precision    recall  f1-score   support\n\n  class1       0.89      0.98      0.93       174\n  class2       0.96      0.90      0.93        30\n  class3       0.96      0.90      0.93        30\n  class4       1.00      1.00      1.00         7\n  class5       0.93      1.00      0.96        13\n  class6       1.00      0.73      0.85        15\n  class7       0.95      0.97      0.96        39\n  class8       0.80      0.67      0.73         6\n  class9       0.97      0.86      0.91        37\n class10       0.95      0.81      0.88        26\n class11       0.50      1.00      0.67         5\n class12       0.93      0.89      0.91        28\n class13       0.73      0.84      0.78        19\n class14       1.00      1.00      1.00         6\n class15       0.45      0.83      0.59         6\n class16       0.97      0.98      0.97       245\n class17       0.93      0.86      0.89       206\n\naccuracy                           0.92       892\n<\/code><\/pre>\n<p>macro avg       0.88      0.90      0.88       892\nweighted avg       0.93      0.92      0.92       892<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1645058143563,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":546,
        "Owner_creation_time":1426639280947,
        "Owner_last_access_time":1650573965300,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71151054",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60530176,
        "Question_title":"mlflow How to save a sklearn pipeline with custom transformer?",
        "Question_body":"<p>I am trying to save with mlflow a sklearn machine-learning model, which is a pipeline containing a custom transformer I have defined, and load it in another project.\nMy custom transformer inherits from BaseEstimator and TransformerMixin.<\/p>\n\n<p>Let's say I have 2 projects:<\/p>\n\n<ul>\n<li>train_project: it has the custom transformers in src.ml.transformers.py<\/li>\n<li>use_project: it has other things in src, or has no src catalog at all<\/li>\n<\/ul>\n\n<p>So in my train_project I do :<\/p>\n\n<pre><code>mlflow.sklearn.log_model(preprocess_pipe, 'model\/preprocess_pipe')\n<\/code><\/pre>\n\n<p>and then when I try to load it into use_project :<\/p>\n\n<pre><code>preprocess_pipe = mlflow.sklearn.load_model(f'{ref_model_path}\/preprocess_pipe')\n<\/code><\/pre>\n\n<p>An error occurs :<\/p>\n\n<pre><code>[...]\nFile \"\/home\/quentin\/anaconda3\/envs\/api_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py\", line 210, in _load_model_from_local_file\n    return pickle.load(f)\nModuleNotFoundError: No module named 'train_project'\n<\/code><\/pre>\n\n<p>I tried to use format mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE :<\/p>\n\n<pre><code>mlflow.sklearn.log_model(preprocess_pipe, 'model\/preprocess_pipe', serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE)\n<\/code><\/pre>\n\n<p>but I get the same error during load.<\/p>\n\n<p>I saw option <strong>code_path<\/strong> into <strong>mlflow.pyfunc.log_model<\/strong> but its use and purpose is not clear to me. <\/p>\n\n<p>I thought mlflow provide a easy way to save model and serialize them so they can be used anywhere, Is that true only if you have native sklearn models (or keras, ...)?<\/p>\n\n<p>It's seem that this issue is more related to pickle functioning (mlflow use it and pickle needs to have all dependencies installed). <\/p>\n\n<p>The only solution I found so far is to make my transformer a package, import it in both project. Save version of my transformer library with <em>conda_env<\/em> argument of <em>log_model<\/em>, and check if it's same version when I load the model into my use_project.\nBut it's painfull if I have to change my transformer or debug in it...<\/p>\n\n<p>Is anybody have a better solution? \nMore elegent? Maybe there is some mlflow functionality I would have missed?<\/p>\n\n<p>other informations :<br>\nworking on linux (ubuntu)<br>\nmlflow=1.5.0<br>\npython=3.7.3   <\/p>\n\n<p>I saw in test of mlflow.sklearn api that they do a test with custom transformer, but they load it into the same file so it seems not resolve my issue but maybe it can helps other poeple :<\/p>\n\n<p><a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/tests\/sklearn\/test_sklearn_model_export.py\" rel=\"noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/tests\/sklearn\/test_sklearn_model_export.py<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1583338083850,
        "Question_score":8,
        "Question_tags":"python|machine-learning|scikit-learn|pickle|mlflow",
        "Question_view_count":3317,
        "Owner_creation_time":1583251456143,
        "Owner_last_access_time":1583925117020,
        "Owner_location":null,
        "Owner_reputation":81,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60530176",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73729205,
        "Question_title":"MLFlow: how to get additional methods from a loaded model?",
        "Question_body":"<p><strong>Use case:<\/strong><\/p>\n<p>A <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.pyfunc.html#mlflow.pyfunc.PyFuncModel\" rel=\"nofollow noreferrer\"><code>mlflow.pyfunc.PyFuncModel<\/code><\/a> is defined with some more utilities methods in order to provide a way of parsing its prediction result to different formats.<\/p>\n<p><strong>After the model is loaded from registry, is there a way to access those methods?<\/strong><\/p>\n<p><strong>A contrived example:<\/strong><\/p>\n<p>A <code>mlflow.pyfunc.PyFuncModel<\/code> model defining additional methods:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class MyModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, model_input):\n        prediction = # do some prediction\n        return prediction\n\n    @staticmethod\n    def parse_prediction_to_format_x(prediction):\n        prediction_formatted = # do some parsing\n        return prediction_formatted\n\n    def parse_prediction_to_format_y(self, prediction):\n        prediction_formatted = # do some parsing\n        return prediction_formatted\n<\/code><\/pre>\n<p>Note: I added one static and one non static, because both use cases are relevant.<\/p>\n<p>Now, some other system goes to MLFlow Registry and loads the model from there:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>        loaded_model = mlflow.pyfunc.load_model(\n            model_uri=saved_model_path.absolute().as_uri()\n        )\n<\/code><\/pre>\n<p>This system, which naturally does not hold the model source code, but the registry path to load it from there, wants to use the additional methods above.\nIt can use predict, since it is part of all pyfunc models:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>predicted = loaded_model.predict(input_data)\n<\/code><\/pre>\n<p><strong>But how can this system access helper methods in the model class (static or instance methods)?<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>predicted = loaded_model.predict(input_data)\n\n# pseudo code:\npredicted_and_formated = loaded_model.parse_prediction_to_format_y(predicted)\n<\/code><\/pre>\n<p>Thank you.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663236278537,
        "Question_score":0,
        "Question_tags":"python|databricks|mlflow",
        "Question_view_count":15,
        "Owner_creation_time":1581629032807,
        "Owner_last_access_time":1663889245757,
        "Owner_location":null,
        "Owner_reputation":477,
        "Owner_up_votes":59,
        "Owner_down_votes":0,
        "Owner_views":56,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73729205",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71292619,
        "Question_title":"Error on Spark MLFlow Model Registery using DataBricks after upgrade: cannot load trained XGBoost model",
        "Question_body":"<p>I had some code for training and then using XGBoost models on a Databricks environment. As my runtime version got deprecated, I upgraded it, but I quickly noticed I could not load my trained models anymore. The reason seems to be a change in the naming of functions in Sparkdl:<\/p>\n<pre><code>Error loading metadata: Expected class name sparkdl.xgboost.xgboost_core.XgboostClassifierModel but found class name sparkdl.xgboost.xgboost.XgboostClassifierModel\n<\/code><\/pre>\n<p>Would anyone have advise on how to fix this issue? Maybe modify the metadata?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1646039018097,
        "Question_score":1,
        "Question_tags":"pyspark|xgboost|azure-databricks|mlflow",
        "Question_view_count":111,
        "Owner_creation_time":1498319618333,
        "Owner_last_access_time":1662636545553,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71292619",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72238610,
        "Question_title":"how to save mlflow metrics and paramters to an s3 bucket without a server?",
        "Question_body":"<p>I want to save the parameters and metrics gotten from mlflow into an s3 bucket. Usually I get these from setting the <code>tracking_uri<\/code> in mlflow and that saves it on a server but I can't have a server in this case(was told no) and just want to store my parameters and metrics on the s3 bucket in the same manner as it would using the <code>tracking_uri<\/code>.<\/p>\n<p>I can store the artifacts on the s3 bucket without issue but not the params\/metrics.<\/p>\n<p>Here is some code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def mlflow_testing():\n    \n    tracking_uri =  &quot;s3:\/\/bucket_name\/mlflow\/&quot;,\n    experiment_name = &quot;test&quot;,\n    artifact_uri= &quot;s3:\/\/bucket_name\/mlflow\/&quot;\n    \n    mlflow.set_tracking_uri(tracking_uri)\n    mlflow.create_experiment(experiment_name, artifact_uri)\n    mlflow.set_experiment(experiment_name)\n    \n    with mlflow.start_run() as run:\n        mlflow.log_param(&quot;test1&quot;, 0)\n        mlflow.log_metric(&quot;test2&quot;, 1)\n    \n        with open(&quot;test.txt&quot;, &quot;w&quot;) as f:\n            f.write(&quot;this is an artifact&quot;)\n    \n        mlflow.log_artifact(&quot;test.txt&quot;)\n        mlflow.end_run()\n<\/code><\/pre>\n<p>This is capable of storing the artifact text file on the s3 bucket(so long as I make the uri a local path like <code>local_data\/mlflow<\/code> instead of the s3 bucket).<\/p>\n<p>Setting the s3 bucket for the <code>tracking_uri<\/code> results in this error:<\/p>\n<pre><code>mlflow.tracking.registry.UnsupportedModelRegistryStoreURIException:\nModel registry functionality is unavailable; got unsupported URI\n's3:\/\/bucket_location\/mlflow\/' for model registry data storage.\nSupported URI schemes are: ['', 'file', 'databricks', 'http', 'https',\n'postgresql', 'mysql', 'sqlite', 'mssql']. See\nhttps:\/\/www.mlflow.org\/docs\/latest\/tracking.html#storage for how to\nrun an MLflow server against one of the supported backend storage\nlocations.\n<\/code><\/pre>\n<p>Does anyone have advice on getting around this without setting up a server? I just want those metrics and params.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1652517689357,
        "Question_score":2,
        "Question_tags":"python|amazon-s3|amazon-sagemaker|mlflow",
        "Question_view_count":818,
        "Owner_creation_time":1615994492347,
        "Owner_last_access_time":1657796021117,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":1652537683396,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72238610",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56370096,
        "Question_title":"Deploying ml model using azureml and mlflow",
        "Question_body":"<p>I'm new to databricks and deploying models using mlflow and azureml, I'm trying to deploy my model but haven't found a lot of documentation or examples.<\/p>\n\n<p>I have my model which I save using:<\/p>\n\n<pre><code>mlflow.sklearn.save_model(model, model_path, \n                          conda_env=conda_env_file_name)\n<\/code><\/pre>\n\n<p>I created the workspace and the aci webservice, the next step is to create the image and the webservice:<\/p>\n\n<pre><code># image creation\nfrom azureml.core.image import ContainerImage\nmyimage_config = ContainerImage.image_configuration(execution_script = driver_file, \n                                                    runtime = \"python\", \n                                                    conda_file = conda_env_file_name)\n\n# Webservice creation\nmyservice = AciWebservice.deploy_from_model(\n  workspace=ws, \n  name=\"service\",\n  deployment_config = aciconfig,\n  models = [model_path],\n  image_config = myimage_config)\n\nmyservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n\n<p>However when I try to create the webservice I receive an error and looking at the log:<\/p>\n\n<pre><code>mlflow.exceptions.MlflowException: Could not find an \"MLmodel\" configuration file at \"mode_path\"\n<\/code><\/pre>\n\n<p>My score file init function is like this:<\/p>\n\n<pre><code>def init():\n    global model\n    # retreive the path to the model file using the model name\n    model_path = Model.get_model_path('model_path')\n    model = joblib.load(model_path)\n<\/code><\/pre>\n\n<p>It seems like it cannot find the path to the model. I'm not sure in the moment the image is saved, the model is not saved in it and thus it cannot be found by sklearn.load_model. I'm quite confused cause I've seen that a model can be deployed using mlflow or azureml. I think the problems is that mlflow.save_model does not register the model and then there's no path. Have someone been able to solve this? What is the best way to deploy a model?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1559175361693,
        "Question_score":3,
        "Question_tags":"web-services|deployment|mlflow",
        "Question_view_count":933,
        "Owner_creation_time":1461539594160,
        "Owner_last_access_time":1663956732217,
        "Owner_location":null,
        "Owner_reputation":737,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":116,
        "Question_last_edit_time":1559831718176,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56370096",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68219396,
        "Question_title":"MLFlow SparkTrials maxNumConcurrentTasks([]) does not exist",
        "Question_body":"<p>I'm very new at using mlflow and I'm currently having some issues on its SparkTrials. I'm running the following code in my Jupyter notebook using Anaconda:<\/p>\n<pre><code>import mlflow\nfrom hyperopt import hp, fmin, tpe, rand, SparkTrials, STATUS_OK, STATUS_FAIL, space_eval\n\n# replicate input_pd dataframe to workers in Spark cluster\ninputs = sc.broadcast(input_pd)\n\n# configure hyperopt settings to distribute to all executors on workers\nspark_trials = SparkTrials()\n\n# select optimization algorithm\nalgo = tpe.suggest\n\n# perform hyperparameter tuning (logging iterations to mlflow)\nargmin = fmin(\n  fn=evaluate_model,\n  space=search_space,\n  algo=algo,\n  max_evals=100,\n  trials=spark_trials\n  )\n\n# release the broadcast dataset\ninputs.unpersist()\n<\/code><\/pre>\n<p>But, I get the following error:<\/p>\n<pre><code>  Py4JError: An error occurred while calling o233.maxNumConcurrentTasks. Trace:\n    py4j.Py4JException: Method maxNumConcurrentTasks([]) does not exist\n        at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n        at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n        at py4j.Gateway.invoke(Gateway.java:274)\n        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n        at py4j.commands.CallCommand.execute(CallCommand.java:79)\n        at py4j.GatewayConnection.run(GatewayConnection.java:238)\n        at java.lang.Thread.run(Unknown Source)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1625197883137,
        "Question_score":2,
        "Question_tags":"python|jupyter|mlflow",
        "Question_view_count":187,
        "Owner_creation_time":1617289926807,
        "Owner_last_access_time":1647594928473,
        "Owner_location":null,
        "Owner_reputation":165,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":68,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68219396",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63230793,
        "Question_title":"How to handle errors in MLflow when a model has been served using \"mlflow models serve\"?",
        "Question_body":"<p>During training, it is possible to use tags as a way to handle exceptions according to <a href=\"https:\/\/stackoverflow.com\/questions\/59856641\/how-can-i-throw-an-exception-from-within-an-mlflow-project\">this question<\/a>.<\/p>\n<p>If a model has been created using <code>mlflow.pyfunc.PythonModel<\/code>, is it possible to throw exceptions? Is there a way to allow error handling for a model that has been served?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596462943310,
        "Question_score":0,
        "Question_tags":"python|rest|mlflow",
        "Question_view_count":266,
        "Owner_creation_time":1472932425400,
        "Owner_last_access_time":1623748857057,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63230793",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60979310,
        "Question_title":"Not able to log model or artifact to Mlflow",
        "Question_body":"<p>I have a MLFlow remote server and I am able to log things from rstudio like:<\/p>\n\n<pre><code>  mlflow_log_param(\"param1\", 5)\n  mlflow_log_param(\"param2\", 5)\n  mlflow_log_metric(\"foo\", 1)\n  mlflow_log_metric(\"foo\", 2)\n  mlflow_log_metric(\"foo\", 3)\n<\/code><\/pre>\n\n<p>But when I try to log things like:<\/p>\n\n<pre><code>  writeLines(\"Hello world!\", \"output.txt\") \n  mlflow_log_artifact(\"output.txt\")\n<\/code><\/pre>\n\n<p>I have this error:<\/p>\n\n<pre><code>2020\/04\/01 21:45:27 INFO mlflow.store.artifact.cli: Logged artifact from local file output.txt to artifact_path=None\nRoot URI: .\/mlruns\/12\/3256cfd3cd1b44b99334040bd5c7c9ee\/artifacts\n<\/code><\/pre>\n\n<p>And when I try to log a model:<\/p>\n\n<pre><code> mlflow_log_model(predictor, \"model1\")\n<\/code><\/pre>\n\n<p>I have next error:<\/p>\n\n<pre><code> 2020\/04\/01 21:56:44 INFO mlflow.store.artifact.cli: Logged artifact from local dir D:\/user\/AppData\/Local\/Temp\/RtmpcBwDOP\/model1 to artifact_path=model1\n    Root URI: .\/mlruns\/12\/07d84e0f252a4248bb2473229297d318\/artifacts\n    # A tibble: 0 x 0\n    Warning message:\n    In value[[3L]](cond) :\n      Logging model metadata to the tracking server has failed, possibly due to older server version. The model artifacts have been logged successfully. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the  tracking store. If logging to a mlflow server via REST, consider  upgrading the server version to MLflow 1.7.0 or above.\n<\/code><\/pre>\n\n<p>The server is updated and also the client.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1585771185263,
        "Question_score":2,
        "Question_tags":"r|mlflow",
        "Question_view_count":605,
        "Owner_creation_time":1521885997267,
        "Owner_last_access_time":1663857830210,
        "Owner_location":null,
        "Owner_reputation":99,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":1627866203927,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60979310",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":57878280,
        "Question_title":"How do you start using MLflow SQL storage instead of the file system storage?",
        "Question_body":"<p>If I were getting started with MLflow, then how would I set up a database store? Is it sufficient to create a new MySQL database or a SQLite database and point MLflow to that?<\/p>\n\n<p>I tried to set the tracking URI, but that didn't create a database if it didn't exist.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1568150011390,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":882,
        "Owner_creation_time":1364931488083,
        "Owner_last_access_time":1663957662590,
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":31,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57878280",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73746436,
        "Question_title":"Mlflow-got error no host supplied,provided uri tracking,help me to resolve it",
        "Question_body":"<p>In below image can see i mention tracking uri and trying to load model but facing error in host supplied. <a href=\"https:\/\/i.stack.imgur.com\/GmLeq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/GmLeq.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663338869023,
        "Question_score":0,
        "Question_tags":"mlflow|mlops",
        "Question_view_count":12,
        "Owner_creation_time":1576127245140,
        "Owner_last_access_time":1664053852373,
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73746436",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65939058,
        "Question_title":"MLflow stores artifacts on GCP buckets but is not able to read them",
        "Question_body":"<p>I've found an almost identical question <a href=\"https:\/\/stackoverflow.com\/questions\/63727235\/mlflow-artifacts-storing-artifactsgoogle-cloud-storage-but-not-displaying-them?newreg=923da08a362547daab64c7d7e2275423\">here<\/a> but don't have enough reputation to add comments so will ask again hoping that someone has found a solution in the mean time.<\/p>\n<p>I am using MLflow (1.13.1) to track model performance and GCP Storage to store model artifacts.\nMLflow is running on a GCP VM instance and my python application uses a service account with Storage Object Creator and Storage Object Viewer roles (and then I've also added storage.buckets.get permissions) to store artifacts in GCP buckets and read from them.\nEverything is working as expected with parameters and metrics correctly displaying in MLflow UI and model artifacts correctly stored in buckets. The problem is that the model artifacts do not show up in MLflow UI because of this error:<\/p>\n<pre><code>Unable to list artifacts stored under gs:\/******\/artifacts for the current run. \nPlease contact your tracking server administrator to notify them of this error, \nwhich can happen when the tracking server lacks permission to list artifacts under the current run's root artifact directory.\n<\/code><\/pre>\n<p>The quoted artifacts location exists and contains the correct model artifacts, and MLflow should be able to read the artifacts because of the Storage Object Viewer role and the storage.buckets.get permissions.<\/p>\n<p>Any suggestion on what could be wrong? Thank you.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611843895217,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|mlflow",
        "Question_view_count":428,
        "Owner_creation_time":1611841996690,
        "Owner_last_access_time":1637883583270,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I've found the problem just after posting the question.\nI had forgotten to install the <code>google-cloud-storage<\/code> library on the GCP VM. Everything works as expected now.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1611845294603,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65939058",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59051212,
        "Question_title":"MLFlow deployment example",
        "Question_body":"<p>I have  models  create  I want learn to deploy   ML Flow  model on production. can I get setp by sep  tutorial  whee  I can deply model.on my PC [assuming it to production env]<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1574772113833,
        "Question_score":0,
        "Question_tags":"deep-learning|artificial-intelligence|mlflow",
        "Question_view_count":70,
        "Owner_creation_time":1555011534987,
        "Owner_last_access_time":1647953103740,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59051212",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71687131,
        "Question_title":"How to import MLflow tracking server WSGI application via Flask or FastAPI?",
        "Question_body":"<p>MLflow provides a very cool tracking server, however, this server does not provide authentication or RBAC which is required for my needs.<\/p>\n<p>I would like to add my own authentication and RBAC functionality. I think one way to accomplish this is to import the MLflow WSGI application object and add some middleware layers to perform authentication \/ authorization before passing requests through to the tracking server, essentially proxying requests through my custom middleware stack.<\/p>\n<p>How do I go about doing this? I can see from <a href=\"https:\/\/fastapi.tiangolo.com\/advanced\/wsgi\/\" rel=\"nofollow noreferrer\">these docs<\/a> that I can use FastAPI to import another WSGI application and add custom middleware, but I'm not sure of a few things<\/p>\n<ol>\n<li>Where do I find the MLflow tracking server WSGI app (where can it be imported from)?<\/li>\n<li>How do I pass through the relevant arguments to the MLflow tracking server? I.e. the tracking server expects params to configure the backend storage layer, host, and port. If I just import the application object, how do I pass those parameters to it?<\/li>\n<\/ol>\n<p>edit - it looks like the Flask application can be found here <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/server\/__init__.py#L28\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/server\/__init__.py#L28<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648703157780,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":394,
        "Owner_creation_time":1425965839877,
        "Owner_last_access_time":1663961190290,
        "Owner_location":"Santa Cruz, CA",
        "Owner_reputation":3256,
        "Owner_up_votes":81,
        "Owner_down_votes":0,
        "Owner_views":164,
        "Question_last_edit_time":1648705670280,
        "Answer_body":"<p>This was actually very simple, below is an example using FastAPI to import and mount the MLflow WSGI application.<\/p>\n<pre><code>import os\nimport subprocess\nfrom fastapi import FastAPI\nfrom fastapi.middleware.wsgi import WSGIMiddleware\n\nfrom mlflow.server import app as mlflow_app\n\napp = FastAPI()\napp.mount(&quot;\/&quot;, WSGIMiddleware(mlflow_app))\n\nBACKEND_STORE_URI_ENV_VAR = &quot;_MLFLOW_SERVER_FILE_STORE&quot;\nARTIFACT_ROOT_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACT_ROOT&quot;\nARTIFACTS_DESTINATION_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACT_DESTINATION&quot;\nPROMETHEUS_EXPORTER_ENV_VAR = &quot;prometheus_multiproc_dir&quot;\nSERVE_ARTIFACTS_ENV_VAR = &quot;_MLFLOW_SERVER_SERVE_ARTIFACTS&quot;\nARTIFACTS_ONLY_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACTS_ONLY&quot;\n\ndef parse_args():\n    a = argparse.ArgumentParser()\n    a.add_argument(&quot;--host&quot;, type=str, default=&quot;0.0.0.0&quot;)\n    a.add_argument(&quot;--port&quot;, type=str, default=&quot;5000&quot;)\n    a.add_argument(&quot;--backend-store-uri&quot;, type=str, default=&quot;sqlite:\/\/\/mlflow.db&quot;)\n    a.add_argument(&quot;--serve-artifacts&quot;, action=&quot;store_true&quot;, default=False)\n    a.add_argument(&quot;--artifacts-destination&quot;, type=str)\n    a.add_argument(&quot;--default-artifact-root&quot;, type=str)\n    a.add_argument(&quot;--gunicorn-opts&quot;, type=str, default=&quot;&quot;)\n    a.add_argument(&quot;--n-workers&quot;, type=str, default=1)\n    return a.parse_args()\n\ndef run_command(cmd, env, cwd=None):\n    cmd_env = os.environ.copy()\n    if cmd_env:\n        cmd_env.update(env)\n    child = subprocess.Popen(\n        cmd, env=cmd_env, cwd=cwd, text=True, stdin=subprocess.PIPE\n    )\n    child.communicate()\n    exit_code = child.wait()\n    if exit_code != 0:\n        raise Exception(&quot;Non-zero exitcode: %s&quot; % (exit_code))\n    return exit_code\n\ndef run_server(args):\n    env_map = dict()\n    if args.backend_store_uri:\n        env_map[BACKEND_STORE_URI_ENV_VAR] = args.backend_store_uri\n    if args.serve_artifacts:\n        env_map[SERVE_ARTIFACTS_ENV_VAR] = &quot;true&quot;\n    if args.artifacts_destination:\n        env_map[ARTIFACTS_DESTINATION_ENV_VAR] = args.artifacts_destination\n    if args.default_artifact_root:\n        env_map[ARTIFACT_ROOT_ENV_VAR] = args.default_artifact_root\n\n    print(f&quot;Envmap: {env_map}&quot;)\n\n    #opts = args.gunicorn_opts.split(&quot; &quot;) if args.gunicorn_opts else []\n    opts = args.gunicorn_opts if args.gunicorn_opts else &quot;&quot;\n\n    cmd = [\n        &quot;gunicorn&quot;, &quot;-b&quot;, f&quot;{args.host}:{args.port}&quot;, &quot;-w&quot;, f&quot;{args.n_workers}&quot;, &quot;-k&quot;, &quot;uvicorn.workers.UvicornWorker&quot;, &quot;server:app&quot;\n    ]\n    run_command(cmd, env_map)\n\ndef main():\n    args = parse_args()\n    run_server(args)\n\nif __name__ == &quot;__main__&quot;:\n    main()\n<\/code><\/pre>\n<p>Run like<\/p>\n<pre><code>python server.py --artifacts-destination s3:\/\/mlflow-mr --default-artifact-root s3:\/\/mlflow-mr --serve-artifacts\n<\/code><\/pre>\n<p>Then navigate to your browser and see the tracking server running! This allows you to insert custom FastAPI middleware in front of the tracking server<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1648708838089,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71687131",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68356746,
        "Question_title":"Changing subdirectory of MLflow artifact store",
        "Question_body":"<p>Is there anything in the Python API that lets you alter the artifact subdirectories? For example, I have a .json file stored here:<\/p>\n<p><code>s3:\/\/mlflow\/3\/1353808bf7324824b7343658882b1e45\/artifacts\/feature_importance_split.json<\/code><\/p>\n<p>MlFlow creates a <code>3\/<\/code> key in s3. Is there a way to change to modify this key to something else (a date or the name of the experiment)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1626152546053,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":1493,
        "Owner_creation_time":1394078070850,
        "Owner_last_access_time":1663707363467,
        "Owner_location":null,
        "Owner_reputation":913,
        "Owner_up_votes":156,
        "Owner_down_votes":5,
        "Owner_views":88,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As I commented above, yes, <code>mlflow.create_experiment()<\/code> does allow you set the artifact location using the <code>artifact_location<\/code> parameter.<\/p>\n<p>However, sort of related, the problem with setting the <code>artifact_location<\/code> using the <code>create_experiment()<\/code> function is that once you create a experiment, MLflow will throw an error if you run the <code>create_experiment()<\/code> function again.<\/p>\n<p>I didn't see this in the docs but it's confirmed that if an experiment already exists in the backend-store, MlFlow will not allow you to run the same <code>create_experiment()<\/code> function again. And as of this post, MLfLow does not have <code>check_if_exists<\/code> flag or a <code>create_experiments_if_not_exists()<\/code> function.<\/p>\n<p>To make things more frustrating, you cannot set the <code>artifcact_location<\/code> in the <code>set_experiment()<\/code> function either.<\/p>\n<p>So here is a pretty easy work around, it also avoids the &quot;ERROR mlflow.utils.rest_utils...&quot; stdout logging as well.\n:<\/p>\n<pre><code>import os\nfrom random import random, randint\n\nfrom mlflow import mlflow,log_metric, log_param, log_artifacts\nfrom mlflow.exceptions import MlflowException\n\ntry:\n    experiment = mlflow.get_experiment_by_name('oof')\n    experiment_id = experiment.experiment_id\nexcept AttributeError:\n    experiment_id = mlflow.create_experiment('oof', artifact_location='s3:\/\/mlflow-minio\/sample\/')\n\nwith mlflow.start_run(experiment_id=experiment_id) as run:\n    mlflow.set_tracking_uri('http:\/\/localhost:5000')\n    print(&quot;Running mlflow_tracking.py&quot;)\n\n    log_param(&quot;param1&quot;, randint(0, 100))\n    \n    log_metric(&quot;foo&quot;, random())\n    log_metric(&quot;foo&quot;, random() + 1)\n    log_metric(&quot;foo&quot;, random() + 2)\n\n    if not os.path.exists(&quot;outputs&quot;):\n        os.makedirs(&quot;outputs&quot;)\n    with open(&quot;outputs\/test.txt&quot;, &quot;w&quot;) as f:\n        f.write(&quot;hello world!&quot;)\n\n    log_artifacts(&quot;outputs&quot;)\n<\/code><\/pre>\n<p>If it is the user's first time creating the experiment, the code will run into an AttributeError since <code>experiment_id<\/code> does not exist and the <code>except<\/code> code block gets executed creating the experiment.<\/p>\n<p>If it is the second, third, etc the code is run, it will only execute the code under the <code>try<\/code> statement since the experiment now exists. Mlflow will now create a 'sample' key in your s3 bucket. Not fully tested but it works for me at least.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1626213927412,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1626236794420,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68356746",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62579563,
        "Question_title":"NameError: name 'dbutils' is not defined",
        "Question_body":"<p>I've .py file with following code line and it lives in git.<\/p>\n<pre><code>dbutils.widgets.text(name='CORPORATION_ID', defaultValue='1234') \n<\/code><\/pre>\n<p>I am using mlflow to run it in remote databricks job cluster. I've conda.yml and MLProject file to pick it up from git and run it in databricks job cluster but I am getting following error.<\/p>\n<pre><code>  File &quot;tea\/src\/cltv_xgb_tea.py&quot;, line 40, in &lt;module&gt;\n    dbutils.widgets.text(name='CORPORATION_ID', defaultValue='1234')\nNameError: name 'dbutils' is not defined\n<\/code><\/pre>\n<p>Any help\/solution is much appreciated.<\/p>\n<hr \/>\n<p>My current files in git<\/p>\n<p>Conda.yml has<\/p>\n<pre><code>name: cicd-environment\nchannels:\n  - defaults\ndependencies:\n  - python=3.7\n  - pip=19.0.3\n  - pip:\n    - mlflow==1.7.2\n    - DBUtils==1.3\n    - ipython==7.14.0\n    - databricks-connect==6.5.1\n    - invoke==1.4.1\n    - awscli==1.18.87\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1593101026770,
        "Question_score":0,
        "Question_tags":"pyspark|conda|databricks|mlflow",
        "Question_view_count":810,
        "Owner_creation_time":1555347036127,
        "Owner_last_access_time":1663694487237,
        "Owner_location":"Minnesota, USA",
        "Owner_reputation":352,
        "Owner_up_votes":27,
        "Owner_down_votes":1,
        "Owner_views":88,
        "Question_last_edit_time":1593403896032,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62579563",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63133902,
        "Question_title":"How to send data to server for Prediction - MLflow",
        "Question_body":"<p>I am able to create ml model server using following command<\/p>\n<pre><code>mlflow models serve -m file:\/\/\/C:\/Users\/SawarkarFamily\/Desktop\/mlflow-master\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/9aeb7ba16d7e4c20870b664e267524ea\/artifacts\/model -p 8000\n2020\/07\/28 17:10:59 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n2020\/07\/28 17:11:03 INFO mlflow.pyfunc.backend: === Running command 'conda activate mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d &amp; waitress-serve --host=127.0.0.1 --port=8000 --ident=mlflow mlflow.pyfunc.scoring_server.wsgi:app'\nc:\\users\\sawarkarfamily\\anaconda3\\envs\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\lib\\site-packages\\waitress\\adjustments.py:441: DeprecationWarning: In future versions of Waitress clear_untrusted_proxy_headers will be set to True by default. You may opt-out by setting this value to False, or opt-in explicitly by setting this to True.\n  warnings.warn(\nServing on http:\/\/DESKTOP-AO59MJC:8000\n<\/code><\/pre>\n<p>In documentation it is given that send that for prediction using curl command as follows:<\/p>\n<pre><code>curl -X POST -H &quot;Content-Type:application\/json; format=pandas-split&quot; --data '{&quot;columns&quot;:[&quot;alcohol&quot;, &quot;chlorides&quot;, &quot;citric acid&quot;, &quot;density&quot;, &quot;fixed acidity&quot;, &quot;free sulfur dioxide&quot;, &quot;pH&quot;, &quot;residual sugar&quot;, &quot;sulphates&quot;, &quot;total sulfur dioxide&quot;, &quot;volatile acidity&quot;],&quot;data&quot;:[[12.8, 0.029, 0.48, 0.98, 6.2, 29, 3.33, 1.2, 0.39, 75, 0.66]]}' http:\/\/127.0.0.1:1234\/invocations\n<\/code><\/pre>\n<p>I replaced port number with 8000, but getting error.<\/p>\n<pre><code>curl: (6) Could not resolve host: chlorides,\ncurl: (6) Could not resolve host: citric acid,\ncurl: (6) Could not resolve host: density,\ncurl: (6) Could not resolve host: fixed acidity,\ncurl: (6) Could not resolve host: free sulfur dioxide,\ncurl: (6) Could not resolve host: pH,\ncurl: (6) Could not resolve host: residual sugar,\ncurl: (6) Could not resolve host: sulphates,\ncurl: (6) Could not resolve host: total sulfur dioxide,\ncurl: (3) [globbing] unmatched close brace\/bracket in column 17\ncurl: (6) Could not resolve host: 0.029,\ncurl: (6) Could not resolve host: 0.48,\ncurl: (6) Could not resolve host: 0.98,\ncurl: (6) Could not resolve host: 6.2,\ncurl: (6) Could not resolve host: 29,\ncurl: (6) Could not resolve host: 3.33,\ncurl: (6) Could not resolve host: 1.2,\ncurl: (6) Could not resolve host: 0.39,\ncurl: (6) Could not resolve host: 75,\ncurl: (3) [globbing] unmatched close brace\/bracket in column 5\n{&quot;error_code&quot;: &quot;MALFORMED_REQUEST&quot;, &quot;message&quot;: &quot;Failed to parse input as a Pandas DataFrame. Ensure that the input is a valid JSON-formatted Pandas DataFrame with the `split` orient produced using the `pandas.DataFrame.to_json(..., orient='split')` method.&quot;, &quot;stack_trace&quot;: &quot;Traceback (most recent call last):\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\mlflow\\\\pyfunc\\\\scoring_server\\\\__init__.py\\&quot;, line 74, in parse_json_input\\n    return _dataframe_from_json(json_input, pandas_orient=orient, schema=schema)\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\mlflow\\\\utils\\\\proto_json_utils.py\\&quot;, line 106, in _dataframe_from_json\\n    return pd.read_json(path_or_str, orient=pandas_orient, dtype=False,\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\pandas\\\\util\\\\_decorators.py\\&quot;, line 214, in wrapper\\n    return func(*args, **kwargs)\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\json\\\\_json.py\\&quot;, line 608, in read_json\\n    result = json_reader.read()\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\json\\\\_json.py\\&quot;, line 731, in read\\n    obj = self._get_object_parser(self.data)\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\json\\\\_json.py\\&quot;, line 753, in _get_object_parser\\n    obj = FrameParser(json, **kwargs).parse()\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\json\\\\_json.py\\&quot;, line 857, in parse\\n    self._parse_no_numpy()\\n  File \\&quot;c:\\\\users\\\\sawarkarfamily\\\\anaconda3\\\\envs\\\\mlflow-76d7aedf36021b9bb7f176264305cf2b7868ca8d\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\json\\\\_json.py\\&quot;, line 1094, in _parse_no_numpy\\n    for k, v in loads(json, precise_float=self.precise_float).items()\\nValueError: Expected object or value\\n&quot;}\n<\/code><\/pre>\n<p>Kindly someone help me with this.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1595938501813,
        "Question_score":3,
        "Question_tags":"python|json|curl|mlflow|mlops",
        "Question_view_count":1362,
        "Owner_creation_time":1595938236260,
        "Owner_last_access_time":1633010020190,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1595942184983,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63133902",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69841126,
        "Question_title":"Adjusting docker environment for each MLflow model",
        "Question_body":"<p>Imagine a situation, where you have two teams working on some models, for example XGBoost. They\ntrain it and then log the model to common MLflow Tracking Server. One of the teams is using older version,\nfor example 1.1 and the other team is using newest 1.6 version.<\/p>\n<p>Is it possible to use both of these models to make predictions inside one container,\nwhich downloades the models from MLflow tracking server? Since these two mentioned models\nuse different versions of XGboost, and the runtime in which they are going to be run has\ncertain version of the pip packages installed, there is going to be compatibility problem -&gt;\nonly one model will be able to run in this enviroment without changes. However, both of these models have\nrequirements.txt file saved as the artifact, and this file specifies the package versions, which the\ngiven model needs. Is it possible to adjust package versions running in the container according to\nthe currently used model using the requirements.txt artifact file? Or is the only\nsolution to create two separate docker images, for each of the models and with the\npackages that the model needs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636037303067,
        "Question_score":0,
        "Question_tags":"docker|pip|mlflow",
        "Question_view_count":95,
        "Owner_creation_time":1636036906520,
        "Owner_last_access_time":1663933591417,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69841126",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71203995,
        "Question_title":"RuntimeError: Java gateway process exited before sending its port number when Deploying Pyspark model to Azure Container Instance",
        "Question_body":"<p>I am trying to deploy a PySpark model trained in Azure Databricks with MLflow to an ACI in Azure Machine Learning.<\/p>\n<p>I am following the steps in this link:<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-mlflow-models#example-notebooks\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-mlflow-models#example-notebooks<\/a><\/p>\n<p>but I get this error:<\/p>\n<pre><code>SPARK_HOME not set. Skipping PySpark Initialization.\nInitializing logger\n2022-02-21 09:29:30,269 | root | INFO | Starting up app insights client\nlogging socket was found. logging is available.\nlogging socket was found. logging is available.\n2022-02-21 09:29:30,270 | root | INFO | Starting up request id generator\n2022-02-21 09:29:30,270 | root | INFO | Starting up app insight hooks\n2022-02-21 09:29:30,270 | root | INFO | Invoking user's init function\nJAVA_HOME is not set\n2022-02-21 09:29:31,267 | root | ERROR | User's init function failed\n2022-02-21 09:29:31,268 | root | ERROR | Encountered Exception Traceback (most recent call last):\n  File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 191, in register\n    main.init()\n  File &quot;\/var\/azureml-app\/execution_script.py&quot;, line 15, in init\n    model = load_model(model_path)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 667, in load_model\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/mlflow\/spark.py&quot;, line 703, in _load_pyfunc\n    pyspark.sql.SparkSession.builder.config(&quot;spark.python.worker.reuse&quot;, True)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/sql\/session.py&quot;, line 228, in getOrCreate\n    sc = SparkContext.getOrCreate(sparkConf)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/context.py&quot;, line 392, in getOrCreate\n    SparkContext(conf=conf or SparkConf())\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/context.py&quot;, line 144, in __init__\n    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/context.py&quot;, line 339, in _ensure_initialized\n    SparkContext._gateway = gateway or launch_gateway(conf)\n  File &quot;\/azureml-envs\/azureml_5d25bdfadca034daea176336163db1e0\/lib\/python3.8\/site-packages\/pyspark\/java_gateway.py&quot;, line 108, in launch_gateway\n    raise RuntimeError(&quot;Java gateway process exited before sending its port number&quot;)\nRuntimeError: Java gateway process exited before sending its port number\n<\/code><\/pre>\n<p>My code looks like this:<\/p>\n<pre><code>from mlflow.deployments import get_deploy_client\n\n# set the tracking uri as the deployment client\nclient = get_deploy_client(mlflow.get_tracking_uri())\n\n# set the model path \nmodel_path = &quot;k_means_model&quot;\n\n    # define the model path and the name is the service name\n    # the model gets registered automatically and a name is autogenerated using the &quot;name&quot; parameter below \n    client.create_deployment(model_uri='runs:\/{}\/{}'.format(run_id, model_path), name = 'k-means-model-ml-flow')\n<\/code><\/pre>\n<p>While my model settings are:<\/p>\n<pre><code>artifact_path: k_means_model\ndatabricks_runtime: 10.3.x-cpu-ml-scala2.12\nflavors:\n  python_function:\n    data: sparkml\n    env: conda.yaml\n    loader_module: mlflow.spark\n    python_version: 3.8.10\n  spark:\n    model_data: sparkml\n    pyspark_version: 3.2.1\nmodel_uuid: 76ba9dfb01e1428ab8145a161ec3cf32\nrun_id: c0090fa9-b382-45b8-be08-d05e16f3cd62\nutc_time_created: '2022-02-21 08:47:34.967167'\n<\/code><\/pre>\n<p>Can someone help please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1645436044400,
        "Question_score":1,
        "Question_tags":"pyspark|azure-databricks|azure-machine-learning-service|mlflow",
        "Question_view_count":289,
        "Owner_creation_time":1635865186583,
        "Owner_last_access_time":1657599189513,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1645439952347,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71203995",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62160734,
        "Question_title":"Hyperopt failed to execute mlflow.end_run() with tracking URI: databricks",
        "Question_body":"<p>I'm using Azure Databricks + Hyperopt + MLflow for some hyperparameter tuning on a small dataset.  Seem like the job is running, and I get output in MLflow, but the job ends with the following error message:<\/p>\n\n<pre><code>Hyperopt failed to execute mlflow.end_run() with tracking URI: databricks\n<\/code><\/pre>\n\n<p>Here is my code code with some information redacted:<\/p>\n\n<pre><code>from pyspark.sql import SparkSession\n\n# spark session initialization\nspark = (SparkSession.builder.getOrCreate())\nsc = spark.sparkContext\n\n# Data Processing\nimport pandas as pd\nimport numpy as np\n# Hyperparameter Tuning\nfrom hyperopt import fmin, tpe, hp, anneal, Trials, space_eval, SparkTrials, STATUS_OK\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n# Modeling\nfrom sklearn.ensemble import RandomForestClassifier\n# cleaning\nimport gc\n# tracking\nimport mlflow\n# track runtime\nfrom datetime import date, datetime\n\nmlflow.set_experiment('\/user\/myname\/myexp')\n# notebook settings \\ variable settings\nn_splits = #\nn_repeats = #\nmax_evals = #\n\ndfL = pd.read_csv(\"\/my\/data\/loc\/mydata.csv\")\n\nx_train = dfL[['f1','f2','f3']]\ny_train = dfL['target']\n\ndef define_model(params):\n    model = RandomForestClassifier(n_estimators=int(params['n_estimators']),\n                                   criterion=params['criterion'], \n                                   max_depth=int(params['max_depth']), \n                                   min_samples_split=params['min_samples_split'], \n                                   min_samples_leaf=params['min_samples_leaf'], \n                                   min_weight_fraction_leaf=params['min_weight_fraction_leaf'], \n                                   max_features=params['max_features'], \n                                   max_leaf_nodes=None, \n                                   min_impurity_decrease=params['min_impurity_decrease'], \n                                   min_impurity_split=None, \n                                   bootstrap=params['bootstrap'], \n                                   oob_score=False, \n                                   n_jobs=-1, \n                                   random_state=int(params['random_state']), \n                                   verbose=0, \n                                   warm_start=False, \n                                   class_weight={0:params['class_0_weight'], 1:params['class_1_weight']})\n        return model\n\n\nspace = {'n_estimators': hp.quniform('n_estimators', #, #, #),\n         'criterion': hp.choice('#', ['#','#']),\n         'max_depth': hp.quniform('max_depth', #, #, #),\n         'min_samples_split': hp.quniform('min_samples_split', #, #, #),\n         'min_samples_leaf': hp.quniform('min_samples_leaf', #, #, #),\n         'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', #, #, #),\n         'max_features': hp.quniform('max_features', #, #, #),\n         'min_impurity_decrease': hp.quniform('min_impurity_decrease', #, #, #),\n         'bootstrap': hp.choice('bootstrap', [#,#]),\n         'random_state': hp.quniform('random_state', #, #, #),\n         'class_0_weight': hp.choice('class_0_weight', [#,#,#]),\n         'class_1_weight': hp.choice('class_1_weight', [#,#,#])}\n\n# define hyperopt objective\ndef objective(params, n_splits=n_splits, n_repeats=n_repeats):\n\n    # define model\n    model = define_model(params)\n    # get cv splits\n    kfold = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=1331)\n    # define and run sklearn cv scorer\n    scores = cross_val_score(model, x_train, y_train, cv=kfold, scoring='roc_auc')\n    score = scores.mean()\n\n    return {'loss': score*(-1), 'status': STATUS_OK}\n\nspark_trials = SparkTrials(parallelism=36, spark_session=spark)\nwith mlflow.start_run():\n  best = fmin(objective, space, algo=tpe.suggest, trials=spark_trials, max_evals=max_evals)\n<\/code><\/pre>\n\n<p>and then at the end I get..<\/p>\n\n<pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200\/200 [1:35:28&lt;00:00, 100.49s\/trial, best loss: -0.9584565527065526]\n\nHyperopt failed to execute mlflow.end_run() with tracking URI: databricks\n\nException: 'MLFLOW_RUN_ID'\n\nTotal Trials: 200: 200 succeeded, 0 failed, 0 cancelled.\n<\/code><\/pre>\n\n<p>My Azure Databricks cluster is..<\/p>\n\n<pre><code>6.6 ML (includes Apache Spark 2.4.5, Scala 2.11)\nStandard_DS3_v2\nmin 9 max 18 nodes\n<\/code><\/pre>\n\n<p>Am I doing something wrong or is this a bug?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1591129195507,
        "Question_score":2,
        "Question_tags":"pyspark|databricks|azure-databricks|mlflow|hyperopt",
        "Question_view_count":604,
        "Owner_creation_time":1528603052363,
        "Owner_last_access_time":1663971435213,
        "Owner_location":null,
        "Owner_reputation":247,
        "Owner_up_votes":637,
        "Owner_down_votes":0,
        "Owner_views":53,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62160734",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73448308,
        "Question_title":"How to configure `backend-store-uri` with huggingface Trainer",
        "Question_body":"<p>When configuring a Hugging Face TrainingArguments <a href=\"https:\/\/huggingface.co\/transformers\/v4.8.0\/main_classes\/trainer.html\" rel=\"nofollow noreferrer\">https:\/\/huggingface.co\/transformers\/v4.8.0\/main_classes\/trainer.html<\/a> you can set the <code>logging_dir<\/code> and <code>output_dir<\/code>.<\/p>\n<p>There is also the <code>mlruns<\/code> directory which according to <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#backend-stores\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tracking.html#backend-stores<\/a> you can configure using <code>--set-backend-uri<\/code>. Though that is an mlflow doc, not a Hugging Face doc.<\/p>\n<p>What is the best way to specify a different <code>mlruns<\/code> directory programmatically when setting up the Hugging Face Trainer?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1661185918393,
        "Question_score":0,
        "Question_tags":"mlflow|huggingface",
        "Question_view_count":19,
        "Owner_creation_time":1484305691263,
        "Owner_last_access_time":1663933845823,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":161,
        "Owner_up_votes":169,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73448308",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68668190,
        "Question_title":"Shape error while converting Image to JSON file",
        "Question_body":"<p>I am trying to convert image to JSON file and POST it with REST API by using MLFLow. Below you can see my code. I got an error like &quot;cannot reshape array of size 535500 into shape (1,4096)&quot;. Can you please help me. Thank you in advance.<\/p>\n<pre><code>import json\nimport cv2\nimport requests\nimport base64\nimport numpy as np\nfrom PIL import Image\n\n\ndata = np.asarray(Image.open('Dataset\/test2\/dog_PNG50348.png').convert('LA'))\ndata = data.reshape((1, 64*64))\ncolumns = [f&quot;col_{c}&quot; for c in range(0, data[0].shape[0])]\ndct = {&quot;columns&quot;: columns, &quot;data&quot;: [data[0].tolist()]}\nprint(json.dumps(dct, indent=2) + &quot;\\n&quot;)\n\n#print(data)\nheaders = {'Content-Type': 'application\/json'}\nrequest_uri = 'http:\/\/127.0.0.1:5000\/invocations'\n\nif __name__ == '__main__':\n    try:\n        response = requests.post(request_uri, data=json.dumps(dct,indent=2)+&quot;\\n&quot;, headers=headers)\n        print(response.content)\n        print('done!!!')\n    except Exception as ex:\n        raise (ex)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1628172503093,
        "Question_score":0,
        "Question_tags":"python|json|numpy|rest|mlflow",
        "Question_view_count":70,
        "Owner_creation_time":1460657116247,
        "Owner_last_access_time":1652910305520,
        "Owner_location":null,
        "Owner_reputation":98,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68668190",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73789674,
        "Question_title":"How can I know how many iterations are left when tuning accross multiple hyperparameters in SparkML?",
        "Question_body":"<p>I'm running a crossvalidation accross a grid of multiple hyperparameters with XgBoost model using Pyspark in Databricks and I would like to know the progress of this operation...So far it has been running for almost 24 hours and I have no idea if it's halfway done or only 10% of the way. I have a 128k combinations of hyperparameters of 5 folds each so a total of 640k runs...<\/p>\n<p>I've tried clicking on MLflow logged run but it's an empty page with an UNFINISHED status. Is there any way to know the progress ?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663690227917,
        "Question_score":0,
        "Question_tags":"databricks|cross-validation|apache-spark-mllib|hyperparameters|mlflow",
        "Question_view_count":13,
        "Owner_creation_time":1558118783690,
        "Owner_last_access_time":1663939366183,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1663694262409,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73789674",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":57693162,
        "Question_title":"Specify database backend store creation in specific schema",
        "Question_body":"<p>When creating an mlflow tracking server and specifying that a SQL Server database is to be used as a backend store, mlflow creates a bunch of table within the dbo schema. Does anyone know if it is possible to specify a different schema in which to create these tables?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1566997596983,
        "Question_score":1,
        "Question_tags":"python|sqlalchemy|mlflow",
        "Question_view_count":823,
        "Owner_creation_time":1566996760530,
        "Owner_last_access_time":1630269065473,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57693162",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72368541,
        "Question_title":"Does mlflow support spacy model serving\/ life-cycle management",
        "Question_body":"<p>How much does MLflow support for spaCy model lifecycle management?<\/p>\n<p>SpaCy model building <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/spacy\" rel=\"nofollow noreferrer\">example<\/a> is given here.<\/p>\n<p>But model serving is failing and showing below error:\n<a href=\"https:\/\/i.stack.imgur.com\/04InD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/04InD.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1653420858453,
        "Question_score":0,
        "Question_tags":"spacy|cicd|mlflow",
        "Question_view_count":75,
        "Owner_creation_time":1495572503257,
        "Owner_last_access_time":1664063965120,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1653464335820,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72368541",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63175684,
        "Question_title":"fitting and predicting model with mlflow",
        "Question_body":"<p>I'm very new to understanding the use of MLFlow but need assistance, I'm trying to understand on how to try and fit and predict my model once again. I'm able to call my model by:<\/p>\n<pre><code>PLS_model = mlflow.pyfunc.load_model(&quot;runs:\/FFFFF!@#!@#@!#!\/logged_model&quot;, suppress_warnings = True)\n<\/code><\/pre>\n<p>and get:<\/p>\n<pre><code>mlflow.pyfunc.loaded_model:\n  artifact_path: logged_model\n  flavor: mlflow.sklearn\n  run_id: FFFFF!@#!@#@!#!\n<\/code><\/pre>\n<p>But when I try to call any methods as:<\/p>\n<p>1).fit or .predict. I get the following error<\/p>\n<pre><code>AttributeError: 'PyFuncModel' object has no attribute 'fit'\n\nAttributeError: 'PyFuncModel' object has no attribute 'predict'\n<\/code><\/pre>\n<p>Here I encountered on how to actually call these functions but not sure if I'm doing this correctly. In summary, how can I predict, fit to my new data.<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596120433613,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":1702,
        "Owner_creation_time":1508861451607,
        "Owner_last_access_time":1661949233680,
        "Owner_location":"Netherlands",
        "Owner_reputation":458,
        "Owner_up_votes":104,
        "Owner_down_votes":1,
        "Owner_views":71,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63175684",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68059536,
        "Question_title":"Could not connect to MLFlow model hosted on Docker",
        "Question_body":"<p>I hosted a model inside a docker container.\nOn running the DockerFile, It runs the following command:<\/p>\n<p><code>mlflow models serve -m model --port 8080 --no-conda<\/code><\/p>\n<p>It serves the model succesfully , And I can now make calls to it.\nBut, I keep getting Max retries exceeded with url<\/p>\n<p>When I host the same model without using Docker(And follow the same steps), it works perfectly.<\/p>\n<p>I use the following command to run the docker container\n<code>docker run -it --rm --network host imagename:random<\/code><\/p>\n<p>I have also tried mapping port 8080, But still not able to get a response.<\/p>\n<p>Not able to understand what the possible issues could be.<\/p>\n<p>Dockerfile for reference<\/p>\n<pre><code>  \nFROM ubuntu:20.04\nENV DEBIAN_FRONTEND=noninteractive\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential software-properties-common\\\n    libboost-dev libboost-serialization-dev libssl-dev \\\n    cmake vim\\\n    wget \\\n    make libbz2-dev libexpat1-dev swig python-dev\nRUN add-apt-repository -y ppa:ubuntugis\/ppa &amp;&amp; apt-get -q update\nRUN apt-get -y install gdal-bin libgdal-dev\nRUN apt-get update\n\nRUN apt install -y python3-pip\nRUN pip3 install --upgrade pip\nRUN pip install mlflow\nRUN pip install pandas\n\nRUN mkdir -p \/tmp\nCOPY .\/main.py \/tmp\/\nCOPY .\/run.sh \/tmp\/\n\nENV LC_ALL=C.UTF-8\nENV LANG=C.UTF-8\nRUN chmod +x run.sh\nCMD .\/run.sh\n<\/code><\/pre>\n<p>Where, run.sh is<\/p>\n<pre><code>python3 main.py\nmlflow models serve -m \/tmp\/mlflow_model --port 8080 --no-conda\n<\/code><\/pre>\n<p>When I run the commands of run.sh file outside of docker container, It is able to serve the model correctly,And I get the correct response.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624218724787,
        "Question_score":1,
        "Question_tags":"docker|python-requests|port|mlflow",
        "Question_view_count":195,
        "Owner_creation_time":1539037621213,
        "Owner_last_access_time":1658940050953,
        "Owner_location":"New Delhi, Delhi, India",
        "Owner_reputation":75,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":21,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68059536",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69905990,
        "Question_title":"Registering model without weights with MLFLow",
        "Question_body":"<p>I would like to be able to register untrained models with MLFLow to use as prototypes for instantiating models for training. I need this because we need to train thousands of models of the same type. Is this possible?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636497891150,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":46,
        "Owner_creation_time":1636497545873,
        "Owner_last_access_time":1640893909690,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69905990",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61266578,
        "Question_title":"How can an mlflow model be scaled to serve more requests?",
        "Question_body":"<p>I would like to have multiple instances of my MLFlow model running in parallel but hidden behind a common the same endpoint\/port so it's not visible to the user. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1587109863510,
        "Question_score":0,
        "Question_tags":"multithreading|gunicorn|mlflow",
        "Question_view_count":228,
        "Owner_creation_time":1579017872663,
        "Owner_last_access_time":1637341304770,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61266578",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73499320,
        "Question_title":"Error loading model from mlflow: java.io.StreamCorruptedException: invalid type code: 00",
        "Question_body":"<p>I'm using Databricks Connect version 9.1.16 to connect to a databricks external cluster with spark version 3.1 and download a Pyspark ML model that's been trained and saved using mlflow.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.set_tracking_uri(&quot;databricks&quot;)\nmodel_h = mlflow.spark.load_model(model_uri=&quot;models:\/model_name\/model_version&quot;)\n<\/code><\/pre>\n<p>I get the following output and error:<\/p>\n<pre><code>2022\/08\/26 11:54:18 INFO mlflow.spark: 'models:\/model_name\/model_version' resolved as 'dbfs:\/\/databricks\/databricks\/mlflow-registry\/model_id\/models\/model'\n2022\/08\/26 11:54:25 INFO mlflow.spark: URI 'dbfs:\/\/databricks\/databricks\/mlflow-registry\/model_id\/models\/model\/sparkml' does not point to the current DFS.\n2022\/08\/26 11:54:25 INFO mlflow.spark: File 'dbfs:\/\/databricks\/databricks\/mlflow-registry\/model_id\/models\/model\/sparkml' not found on DFS. Will attempt to upload the file.\n2022\/08\/26 11:55:06 INFO mlflow.spark: Copied SparkML model to \/tmp\/mlflow\/model_id\n---------------------------------------------------------------------------\nPy4JJavaError                             Traceback (most recent call last)\nc:\\Users\\carlafernandez\\Documents\\my_notebook.ipynb Celda 5 in &lt;cell line: 2&gt;()\n      1 mlflow.set_tracking_uri(&quot;databricks&quot;)\n----&gt; 2 model_h = mlflow.spark.load_model(model_uri=&quot;models:\/model_name\/model_version&quot;)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\mlflow\\spark.py:711, in load_model(model_uri, dfs_tmpdir)\n    708 local_model_path = _download_artifact_from_uri(model_uri)\n    709 _add_code_from_conf_to_system_path(local_model_path, flavor_conf)\n--&gt; 711 return _load_model(model_uri=model_uri, dfs_tmpdir_base=dfs_tmpdir)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\mlflow\\spark.py:660, in _load_model(model_uri, dfs_tmpdir_base)\n    658     return _load_model_databricks(model_uri, dfs_tmpdir)\n    659 model_uri = _HadoopFileSystem.maybe_copy_from_uri(model_uri, dfs_tmpdir)\n--&gt; 660 return PipelineModel.load(model_uri)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\pyspark\\ml\\util.py:463, in MLReadable.load(cls, path)\n    460 @classmethod\n    461 def load(cls, path):\n    462     &quot;&quot;&quot;Reads an ML instance from the input path, a shortcut of `read().load(path)`.&quot;&quot;&quot;\n--&gt; 463     return cls.read().load(path)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\pyspark\\ml\\pipeline.py:258, in PipelineModelReader.load(self, path)\n    256 metadata = DefaultParamsReader.loadMetadata(path, self.sc)\n    257 if 'language' not in metadata['paramMap'] or metadata['paramMap']['language'] != 'Python':\n--&gt; 258     return JavaMLReader(self.cls).load(path)\n    259 else:\n    260     uid, stages = PipelineSharedReadWrite.load(metadata, self.sc, path)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\pyspark\\ml\\util.py:413, in JavaMLReader.load(self, path)\n    411 if not isinstance(path, str):\n    412     raise TypeError(&quot;path should be a string, got type %s&quot; % type(path))\n--&gt; 413 java_obj = self._jread.load(path)\n    414 if not hasattr(self._clazz, &quot;_from_java&quot;):\n    415     raise NotImplementedError(&quot;This Java ML type cannot be loaded into Python currently: %r&quot;\n    416                               % self._clazz)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\py4j\\java_gateway.py:1304, in JavaMember.__call__(self, *args)\n   1298 command = proto.CALL_COMMAND_NAME +\\\n   1299     self.command_header +\\\n   1300     args_command +\\\n   1301     proto.END_COMMAND_PART\n   1303 answer = self.gateway_client.send_command(command)\n-&gt; 1304 return_value = get_return_value(\n   1305     answer, self.gateway_client, self.target_id, self.name)\n   1307 for temp_arg in temp_args:\n   1308     temp_arg._detach()\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\pyspark\\sql\\utils.py:117, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)\n    115 def deco(*a, **kw):\n    116     try:\n--&gt; 117         return f(*a, **kw)\n    118     except py4j.protocol.Py4JJavaError as e:\n    119         converted = convert_exception(e.java_exception)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\py4j\\protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)\n    324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n    325 if answer[1] == REFERENCE_TYPE:\n--&gt; 326     raise Py4JJavaError(\n    327         &quot;An error occurred while calling {0}{1}{2}.\\n&quot;.\n    328         format(target_id, &quot;.&quot;, name), value)\n    329 else:\n    330     raise Py4JError(\n    331         &quot;An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n&quot;.\n    332         format(target_id, &quot;.&quot;, name, value))\n\nPy4JJavaError: An error occurred while calling o645.load.\n: java.io.StreamCorruptedException: invalid type code: 00\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1698)\n    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n    at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n    at sun.reflect.GeneratedMethodAccessor311.invoke(Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184)\n    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2296)\n    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n    at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)\n    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n    at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)\n    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n    at org.apache.spark.sql.util.ProtoSerializer.$anonfun$deserializeObject$1(ProtoSerializer.scala:6631)\n    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n    at org.apache.spark.sql.util.ProtoSerializer.deserializeObject(ProtoSerializer.scala:6616)\n    at com.databricks.service.SparkServiceRPCHandler.execute0(SparkServiceRPCHandler.scala:728)\n    at com.databricks.service.SparkServiceRPCHandler.$anonfun$executeRPC0$1(SparkServiceRPCHandler.scala:477)\n    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n    at com.databricks.service.SparkServiceRPCHandler.executeRPC0(SparkServiceRPCHandler.scala:372)\n    at com.databricks.service.SparkServiceRPCHandler$$anon$2.call(SparkServiceRPCHandler.scala:323)\n    at com.databricks.service.SparkServiceRPCHandler$$anon$2.call(SparkServiceRPCHandler.scala:309)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at com.databricks.service.SparkServiceRPCHandler.$anonfun$executeRPC$1(SparkServiceRPCHandler.scala:359)\n    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n    at com.databricks.service.SparkServiceRPCHandler.executeRPC(SparkServiceRPCHandler.scala:336)\n    at com.databricks.service.SparkServiceRPCServlet.doPost(SparkServiceRPCServer.scala:167)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n    at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)\n    at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n    at org.eclipse.jetty.server.Server.handle(Server.java:516)\n    at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n    at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n    at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n    at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n    at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n    at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n    at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n    at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n    at java.lang.Thread.run(Thread.java:748)\n<\/code><\/pre>\n<p>So it seems like it's able to find a copy the model, but then somehow it cannot read it. It's worth noting that the same <strong>works in a databricks notebook<\/strong>, the problem only occurs using databricks connect.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1661508308487,
        "Question_score":0,
        "Question_tags":"pyspark|databricks|mlflow|databricks-connect",
        "Question_view_count":23,
        "Owner_creation_time":1506240907953,
        "Owner_last_access_time":1663933103760,
        "Owner_location":"Madrid, Espa\u00f1a",
        "Owner_reputation":56,
        "Owner_up_votes":47,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73499320",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71679081,
        "Question_title":"How can I connect mlflow server via nginx ssl authentication?",
        "Question_body":"<p>System information\nOS Platform and Distribution: Windows 10\nMLflow installed: using pip\nMLflow version: version 1.24.0\n**Python version: Python 3.9.7 **<\/p>\n<p>Describe the problem\nI have created a docker-compose system with a backend\/artifact storages, mlflow server and nginx to add an authentication layer.<\/p>\n<pre><code>...\nmlflow:\n        restart: always\n        build: .\n        environment:\n            - AWS_ACCESS_KEY_ID=${MINIO_USR}\n            - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}       \n        expose:\n            - '5000'\n        networks:\n            - frontend\n            - backend\n        depends_on:\n            - storage                       \n        image: 'mlflow:Dockerfile'\n        container_name: mlflow_server_nginx\n\n    nginx:\n        restart: always\n        build: .\/nginx\n        container_name: mlflow_nginx\n        ports:\n            - 5043:443\n        links:\n            - mlflow:mlflow\n        volumes:\n            - 'path\/to\/nginx\/auth:\/etc\/nginx\/conf.d'\n            - 'path\/to\/nginx\/nginx.conf:\/etc\/nginx\/nginx.conf:ro'\n        networks:\n            - frontend\n        depends_on:\n            - mlflow\n<\/code><\/pre>\n<p>I have created an user\/password via htpasswd and a custom SSL CA (.pem\/.key) using openssl and my-mlflow.com server-name.<\/p>\n<p>When the docker-compose system is built i can access to mlflow UI via my browser. But when i try to create a new experiment using python trying diferent approaches, i get next errors:\nExecuted code 1:<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\n#os.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n#os.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate (_ssl.c:1108)')))\n<\/code><\/pre>\n<p>After read some notes in the documentation and realated issues I tryed next<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\n#os.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\nos.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLError(9, '[SSL] PEM lib (_ssl.c:4012)')))\n<\/code><\/pre>\n<p>Finally<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\nos.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n#os.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLCertVerificationError(&quot;hostname 'localhost' doesn't match '*.my-mlflow.com'&quot;)))\n<\/code><\/pre>\n<p>Can you give me some hints about how to solve it?<\/p>\n<p>Thank you very much!\nFernando....<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648650339347,
        "Question_score":1,
        "Question_tags":"python-3.x|docker|nginx|docker-compose|mlflow",
        "Question_view_count":625,
        "Owner_creation_time":1580841805373,
        "Owner_last_access_time":1663792613347,
        "Owner_location":"Seville, Spain",
        "Owner_reputation":33,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can set:<\/p>\n<pre><code>os.environ['MLFLOW_TRACKING_INSECURE_TLS'] = 'true'\n<\/code><\/pre>\n<p>And then try to get your cert-chain straight from there for production use.<\/p>\n<p>Also see Documentation: <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#id19\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tracking.html#id19<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1652276299263,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1652448943407,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71679081",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56328367,
        "Question_title":"Linear regression model with integration of MLFlow",
        "Question_body":"<p>Does anybody has a link to sample Linear Regression code integrated with MLFlow and explaining all three concepts of MLFlow i.e. Tracking, Project and Model? <\/p>\n\n<p>I'm particularly looking for a demo link to the same.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1558968873863,
        "Question_score":1,
        "Question_tags":"python|machine-learning|linear-regression|azure-databricks|mlflow",
        "Question_view_count":204,
        "Owner_creation_time":1453907226643,
        "Owner_last_access_time":1604947189130,
        "Owner_location":null,
        "Owner_reputation":329,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56328367",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":54233746,
        "Question_title":"Filter mlflow runs by commit ID",
        "Question_body":"<p>When using the UI of MlFlow, is it possible to filter\/search the runs using the (git) commit ID? I manage to search by parameters but it doesn't seem like there's a way to filter by the commit ID.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/npkFO.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/npkFO.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1547720560897,
        "Question_score":4,
        "Question_tags":"machine-learning|version-control|mlflow",
        "Question_view_count":982,
        "Owner_creation_time":1300789717227,
        "Owner_last_access_time":1663941101560,
        "Owner_location":null,
        "Owner_reputation":11410,
        "Owner_up_votes":2846,
        "Owner_down_votes":6,
        "Owner_views":1782,
        "Question_last_edit_time":1547796230323,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54233746",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60633328,
        "Question_title":"Mlflow it is possible to log confusion matrix every step?",
        "Question_body":"<p>It is possible to log with mlflow the confusion matrix every step like a simple metrics?\nIf it is possible it have a visualization like this?\n<a href=\"https:\/\/i.stack.imgur.com\/bYbgo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bYbgo.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1583920669630,
        "Question_score":0,
        "Question_tags":"data-visualization|confusion-matrix|mlflow",
        "Question_view_count":2334,
        "Owner_creation_time":1547467399037,
        "Owner_last_access_time":1663856296190,
        "Owner_location":"Busto Arsizio, VA, Italia",
        "Owner_reputation":171,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Question_last_edit_time":1584009263430,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60633328",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69482686,
        "Question_title":"logging models in mlflow with a pyspark process in Kerberized HDP 3.1.5",
        "Question_body":"<p>I'm currently testing mlflow to log pyspark models in a HDP3.1.x Cluster KERBERIZED.\nI've configured mlflow to use HDFS (of the same HDP cluster) for model storage.<\/p>\n<p>Whenever I launch a pyspark process to log a model on MLFlow with &quot;spark-submit --deploy-mode=cluster ...&quot;, I've got the exception<\/p>\n<blockquote>\n<p>AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]java.io.IOException: DestHost:destPort\nnamenode01.hdp.site:8020 , LocalHost:localPort\nworker05.hdp.site\/192.168.0.208:0. Failed on local exception:\njava.io.IOException:\n<strong>org.apache.hadoop.security.AccessControlException: Client cannot\nauthenticate via:[TOKEN, KERBEROS]<\/strong><\/p>\n<p>(...)<\/p>\n<p>Caused by: java.io.IOException:\norg.apache.hadoop.security.AccessControlException: Client cannot\nauthenticate via:[TOKEN, KERBEROS]    at\norg.apache.hadoop.ipc.Client$Connection$1.run(Client.java:758)    at\njava.security.AccessController.doPrivileged(Native Method)    at\njavax.security.auth.Subject.doAs(Subject.java:422)*<\/p>\n<\/blockquote>\n<p>It seems that libhdfs used by mlflow cannot properly authenticate with delegation tokens. Do you know any way to fix or circumvent this problem?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1633615788157,
        "Question_score":0,
        "Question_tags":"apache-spark|pyspark|kerberos|mlflow|hdp",
        "Question_view_count":102,
        "Owner_creation_time":1633614827870,
        "Owner_last_access_time":1663923745037,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1633622029736,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69482686",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70680222,
        "Question_title":"Does MLflow allow to log artifacts from remote locations like S3?",
        "Question_body":"<h2>My setting<\/h2>\n<p>I have developed an environment for ML experiments that looks like the following: training happens in the AWS cloud with SageMaker Training Jobs. The trained model is stored in the <code>\/opt\/ml\/model<\/code> directory, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-output.html\" rel=\"nofollow noreferrer\">which is reserved by SageMaker to pack models<\/a> as a <code>.tar.gz<\/code> in SageMaker's own S3 bucket. Several evaluation metrics are computed during training and testing, and recorded to an MLflow infrastructure consisting of an S3-based artifact store (see <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#scenario-4-mlflow-with-remote-tracking-server-backend-and-artifact-stores\" rel=\"nofollow noreferrer\">Scenario 4<\/a>). Note that this is a different S3 bucket than SageMaker's.<\/p>\n<p>A very useful feature from MLflow is that any model artifacts can be logged to a training run, so data scientists have access to both metrics and more complex outputs through the UI. These outputs include (but are not limited to) the trained model itself.<\/p>\n<p>A limitation is that, as I understand it, the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifact\" rel=\"nofollow noreferrer\">MLflow API for logging artifacts<\/a> only accepts as input a local path to the artifact itself, and will always upload it to its artifact store. This is suboptimal when the artifacts are stored somewhere outside MLflow, as you have to store them twice. A transformer model may weigh more than 1GB.<\/p>\n<h2>My questions<\/h2>\n<ul>\n<li>Is there a way to pass an S3 path to MLflow and make it count as an artifact, without having to download it locally first?<\/li>\n<li>Is there a way to avoid pushing a copy of an artifact to the artifact store? If my artifacts already reside in another remote location, it would be ideal to just have a link to such location in MLflow and not a copy in MLflow storage.<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641984552913,
        "Question_score":0,
        "Question_tags":"python|amazon-s3|amazon-sagemaker|mlflow|mlops",
        "Question_view_count":533,
        "Owner_creation_time":1452286548080,
        "Owner_last_access_time":1663928318030,
        "Owner_location":"Madrid, Spain",
        "Owner_reputation":118,
        "Owner_up_votes":28,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70680222",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59363969,
        "Question_title":"mlflow: problems with pip installation",
        "Question_body":"<p>I read through many threads regarding installation issues using pip. However, I could find a solution to help me fix my problem.\nI installed mlflow with :<\/p>\n\n<pre><code>    pip3 install mlflow\n<\/code><\/pre>\n\n<p>so mlflow is installed in \/usr\/local\/bin\/mlflow<\/p>\n\n<p>Since it is not in \/Users\/xxxx\/opt\/anaconda3\/lib\/python3.7\/site-packages, I get \"ModuleNotFoundError: No module named 'mlflow' error when I try to run code that imports mlflow module. How should I fix this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1576528631793,
        "Question_score":1,
        "Question_tags":"pip|python-import|python-3.7|importerror|mlflow",
        "Question_view_count":9843,
        "Owner_creation_time":1524783572553,
        "Owner_last_access_time":1639508068437,
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":87,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59363969",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72604450,
        "Question_title":"MLflow load model fails Python",
        "Question_body":"<p>I am trying to build an API using an MLflow model.<\/p>\n<p>the funny thing is it works from one location on my PC and not from another. So, the reason for doing I wanted to change my repo etc.<\/p>\n<p>So, the simple code of<\/p>\n<pre><code>from mlflow.pyfunc import load_model\nMODEL_ARTIFACT_PATH = &quot;.\/model\/model_name\/&quot;\nMODEL = load_model(MODEL_ARTIFACT_PATH)\n<\/code><\/pre>\n<p>now fails with<\/p>\n<pre><code>ERROR:    Traceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 540, in lifespan\n    async for item in self.lifespan_context(app):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 481, in default_lifespan\n    await self.startup()\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 516, in startup\n    await handler()\n  File &quot;\/code\/.\/app\/main.py&quot;, line 32, in startup_load_model\n    MODEL = load_model(MODEL_ARTIFACT_PATH)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 733, in load_model\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/spark.py&quot;, line 737, in _load_pyfunc\n    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/spark.py&quot;, line 656, in _load_model\n    return PipelineModel.load(model_uri)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/util.py&quot;, line 332, in load\n    return cls.read().load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/pipeline.py&quot;, line 258, in load\n    return JavaMLReader(self.cls).load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/util.py&quot;, line 282, in load\n    java_obj = self._jread.load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/py4j\/java_gateway.py&quot;, line 1321, in __call__\n    return_value = get_return_value(\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/sql\/utils.py&quot;, line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.\n<\/code><\/pre>\n<p>The model artifacts are already downloaded to the folder \/model folder which has the following structure.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oqxRW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oqxRW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>the load model call is in the main.py file\nAs I mentioned it works from another directory, but there is no reference to any absolute paths. Also, I have made sure that my package references are identical. e,g I have pinned them all down<\/p>\n<pre><code># Model\nmlflow==1.25.1\nprotobuf==3.20.1\npyspark==3.2.1\nscipy==1.6.2\nsix==1.15.0\n<\/code><\/pre>\n<p>also, the same docker file is used both places, which among other things, makes sure that the final resulting folder structure is the same<\/p>\n<pre><code>......other stuffs\n\nCOPY .\/app \/code\/app\nCOPY .\/model \/code\/model\n<\/code><\/pre>\n<p>what can explain it throwing this exception whereas in another location (on my PC), it works (same model artifacts) ?<\/p>\n<p>Since it uses load_model function, it should be able to read the parquet files ?<\/p>\n<p>Any question and I can explain.<\/p>\n<p>EDIT1: I have debugged this a little more in the docker container and it seems the parquet files in the itemFactors folder (listed in my screenshot above) are not getting copied over to my image , even though I have the copy command to copy all files under the model folder. It is copying the _started , _committed and _SUCCESS files, just not the parquet files. Anyone knows why would that be? I DO NOT have a .dockerignore file. Why are those files ignored while copying?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655130099670,
        "Question_score":2,
        "Question_tags":"python|docker|databricks|mlflow",
        "Question_view_count":109,
        "Owner_creation_time":1428654714763,
        "Owner_last_access_time":1664012257383,
        "Owner_location":null,
        "Owner_reputation":596,
        "Owner_up_votes":53,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Question_last_edit_time":1655134670387,
        "Answer_body":"<p>I found the problem. Like I wrote in the EDIT1 of my post, with further observations, the parquet files were missing in the docker container. That was strange because I was copying the entire folder in my Dockerfile.<\/p>\n<p>I then realized that I was hitting this problem <a href=\"https:\/\/github.com\/moby\/buildkit\/issues\/1366\" rel=\"nofollow noreferrer\">mentioned here<\/a>. File paths exceeding 260 characters, silently fail and do not get copied over to the docker container. This was really frustrating because nothing failed during build and then during run, it gave me that cryptic error of &quot;unable to infer schema for parquet&quot;, essentially because the parquet files were not copied over during docker build.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655191745992,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1655202841060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72604450",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67456172,
        "Question_title":"Unable to connect to MLFLOW_TRACKING_URI when running MLflow run in Docker container",
        "Question_body":"<p>I have setup a mlflow server locally at http:\/\/localhost:5000<\/p>\n<p>I followed the instructions at <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker<\/a> and tried to run the example docker with<\/p>\n<pre><code>\/mlflow\/examples\/docker$ mlflow run . -P alpha=0.5\n<\/code><\/pre>\n<p>but I encountered the following error.<\/p>\n<pre><code>2021\/05\/09 17:11:20 INFO mlflow.projects.docker: === Building docker image docker-example:7530274 ===\n2021\/05\/09 17:11:20 INFO mlflow.projects.utils: === Created directory \/tmp\/tmp9wpxyzd_ for downloading remote URIs passed to arguments of type 'path' ===\n2021\/05\/09 17:11:20 INFO mlflow.projects.backend.local: === Running command 'docker run --rm -v \/home\/mlf\/mlf\/0\/ae69145133bf49efac22b1d390c354f1\/artifacts:\/home\/mlf\/mlf\/0\/ae69145133bf49efac22b1d390c354f1\/artifacts -e MLFLOW_RUN_ID=ae69145133bf49efac22b1d390c354f1 -e MLFLOW_TRACKING_URI=http:\/\/localhost:5000 -e MLFLOW_EXPERIMENT_ID=0 docker-example:7530274 python train.py --alpha 0.5 --l1-ratio 0.1' in run with ID 'ae69145133bf49efac22b1d390c354f1' === \n\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/__init__.py:55: DeprecationWarning: MLflow support for Python 2 is deprecated and will be dropped in a future release. At that point, existing Python 2 workflows that use MLflow will continue to work without modification, but Python 2 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3 - see https:\/\/docs.python.org\/3\/howto\/pyporting.html for a migration guide.\n  &quot;for a migration guide.&quot;, DeprecationWarning)\nTraceback (most recent call last):\n  File &quot;train.py&quot;, line 56, in &lt;module&gt;\n    with mlflow.start_run():\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/client.py&quot;, line 96, in get_run\n    return self._tracking_client.get_run(run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 49, in get_run\n    return self.store.get_run(run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py&quot;, line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py&quot;, line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 133, in call_endpoint\n    host_creds=host_creds, endpoint=endpoint, method=method, params=json_body)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 70, in http_request\n    url=url, headers=headers, verify=verify, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 51, in request_with_ratelimit_retries\n    response = requests.request(**kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/api.py&quot;, line 58, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/sessions.py&quot;, line 508, in request\n    resp = self.send(prep, **send_kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/sessions.py&quot;, line 618, in send\n    r = adapter.send(request, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/adapters.py&quot;, line 508, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get?run_uuid=ae69145133bf49efac22b1d390c354f1&amp;run_id=ae69145133bf49efac22b1d390c354f1 (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f5cbd80d690&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))\n2021\/05\/09 17:11:22 ERROR mlflow.cli: === Run (ID 'ae69145133bf49efac22b1d390c354f1') failed ===\n<\/code><\/pre>\n<p>Any ideas how to fix this? I tried adding the following in MLproject file but it doesn't help<\/p>\n<pre><code>environment: [[&quot;network&quot;, &quot;host&quot;], [&quot;add-host&quot;, &quot;host.docker.internal:host-gateway&quot;]]\n<\/code><\/pre>\n<p>Thanks for your help! =)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620552530280,
        "Question_score":0,
        "Question_tags":"docker|mlflow",
        "Question_view_count":1151,
        "Owner_creation_time":1316620102630,
        "Owner_last_access_time":1643692547643,
        "Owner_location":null,
        "Owner_reputation":1308,
        "Owner_up_votes":177,
        "Owner_down_votes":1,
        "Owner_views":151,
        "Question_last_edit_time":1620554070856,
        "Answer_body":"<p>Run MLflow server such was that it will use your machine IP instead of <code>localhost<\/code>.  Then point the <code>mlflow run<\/code> to that IP instead of <code>http:\/\/localhost:5000<\/code>.   The main reason is that <code>localhost<\/code> of Docker process is its own, not your machine.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1620627546969,
        "Answer_score":1.0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67456172",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67835498,
        "Question_title":"MLflow - How to point interface path to show the expected result",
        "Question_body":"<p>I just started MLflow today and fail to display the log result on MLflow ui interface.\nWill appreciate a lot if someone can give me some hint..<\/p>\n<p>tried the sample code below<\/p>\n<pre><code>import os\nfrom random import random, randint\nfrom mlflow import log_metric, log_param, log_artifacts\n\nif __name__ == &quot;__main__&quot;:\n    # Log a parameter (key-value pair)\n    log_param(&quot;param1&quot;, randint(0, 100))\n\n    # Log a metric; metrics can be updated throughout the run\n    log_metric(&quot;foo&quot;, random())\n    log_metric(&quot;foo&quot;, random() + 1)\n    log_metric(&quot;foo&quot;, random() + 2)\n\n    # Log an artifact (output file)\n    if not os.path.exists(&quot;outputs&quot;):\n        os.makedirs(&quot;outputs&quot;)\n    with open(&quot;outputs\/test.txt&quot;, &quot;w&quot;) as f:\n        f.write(&quot;hello world!&quot;)\n    log_artifacts(&quot;outputs&quot;)\n<\/code><\/pre>\n<p>ran the script above for 3 times and it gave me the result in the following structure. 3 folders representing 3 runs separately:<\/p>\n<pre><code>file:\/\/\/home\/devuser\/project\/mlruns\/0\n0 - 0737fec7d4824384b6320070cd688b78\n    355d57e092a242b7aa263451d280b497 \n    ed2614ffe2fd4f2db991d5d7166635f8  \n    meta.yaml\n<\/code><\/pre>\n<p>with folders\/files <code>artifacts, meta.yaml, metrics, params, tags<\/code> in each folder separately.<\/p>\n<p>I ran <code>mlflow ui<\/code> under <code>file:\/\/\/home\/devuser\/project\/mlruns\/<\/code> but nothing was showed on the interface. tried to look this up but no one has come across this problem with this kind of simple code.<\/p>\n<p>Appreciate a lot if someone could kindly let me know how I can change my setting.. Thank you..<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622801740880,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":379,
        "Owner_creation_time":1445157877637,
        "Owner_last_access_time":1638455223627,
        "Owner_location":null,
        "Owner_reputation":267,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":111,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You need to run <code>mlflow ui<\/code> in the project directory itself, not inside the <code>mlruns<\/code> - if you look into the <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#mlflow-ui\" rel=\"nofollow noreferrer\">documentation for <code>mlflow ui<\/code> command<\/a>, it says:<\/p>\n<blockquote>\n<p><code>--default-artifact-root &lt;URI&gt;<\/code><\/p>\n<p>Path to local directory to store artifacts, for new experiments. Note that this flag does not impact already-created experiments. <strong>Default: .\/mlruns<\/strong><\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1622968783720,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67835498",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59887618,
        "Question_title":"Is it possible to use MLFlow and H2o.ai sparkling water in a Scala based project?",
        "Question_body":"<p>I'm solving a Scala data science problem in Intellij using maven. I noticed that MLFlow spark (<a href=\"https:\/\/mvnrepository.com\/artifact\/org.mlflow\/mlflow-spark\/1.5.0\" rel=\"nofollow noreferrer\">https:\/\/mvnrepository.com\/artifact\/org.mlflow\/mlflow-spark\/1.5.0<\/a>) is dependent on scala 2.12 while h2o.ai sparkling water is dependent on scala 2.11 (<a href=\"https:\/\/mvnrepository.com\/artifact\/ai.h2o\/sparkling-water-core\" rel=\"nofollow noreferrer\">https:\/\/mvnrepository.com\/artifact\/ai.h2o\/sparkling-water-core<\/a>). Is there any way to use both of these together using Scala? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1579816453420,
        "Question_score":0,
        "Question_tags":"scala|apache-spark|h2o|sparkling-water|mlflow",
        "Question_view_count":217,
        "Owner_creation_time":1579815947657,
        "Owner_last_access_time":1661917140247,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59887618",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68402406,
        "Question_title":"MLflow webserver returns 400 status, \"Incompatible input types for column X. Can not safely convert float64 to <U0.\"",
        "Question_body":"<p>I am implementing an anomaly detection web service using <code>MLflow<\/code> and <code>sklearn.pipeline.Pipeline()<\/code>. The aim of the model is to detect web crawlers using server log and <code>response_length<\/code> column is one of my features. After serving model, for testing the web service I send below request that contains the 20 first columns of the train data.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>$ curl  --location --request POST '127.0.0.1:8000\/invocations'\n        --header 'Content-Type: text\/csv' \\\n        --data-binary 'datasets\/test.csv'\n<\/code><\/pre>\n<p>But response of the web server has status code 400 (BAD REQUEST) and this JSON body:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;error_code&quot;: &quot;BAD_REQUEST&quot;,\n    &quot;message&quot;: &quot;Incompatible input types for column response_length. Can not safely convert float64 to &lt;U0.&quot;\n}\n<\/code><\/pre>\n<p>Here is the model compilation MLflow Tracking component log:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>[Pipeline] ......... (step 1 of 3) Processing transform, total=11.8min\n[Pipeline] ............... (step 2 of 3) Processing pca, total=   4.8s\n[Pipeline] ........ (step 3 of 3) Processing rule_based, total=   0.0s\n2021\/07\/16 04:55:12 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n2021\/07\/16 04:55:12 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: &quot;\/home\/matin\/workspace\/Rahnema College\/venv\/lib\/python3.8\/site-packages\/mlflow\/models\/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values &lt;https:\/\/www.mlflow.org\/docs\/latest\/models.html#handling-integers-with-missing-values&gt;`_ for more details.&quot;\nLogged data and model in run: 8843336f5c31482c9e246669944b1370\n\n---------- logged params ----------\n{'memory': 'None',\n 'pca': 'PCAEstimator()',\n 'rule_based': 'RuleBasedEstimator()',\n 'steps': &quot;[('transform', &lt;log_transformer.LogTransformer object at &quot;\n          &quot;0x7f05a8b95760&gt;), ('pca', PCAEstimator()), ('rule_based', &quot;\n          'RuleBasedEstimator())]',\n 'transform': '&lt;log_transformer.LogTransformer object at 0x7f05a8b95760&gt;',\n 'verbose': 'True'}\n\n---------- logged metrics ----------\n{}\n\n---------- logged tags ----------\n{'estimator_class': 'sklearn.pipeline.Pipeline', 'estimator_name': 'Pipeline'}\n\n---------- logged artifacts ----------\n['model\/MLmodel',\n 'model\/conda.yaml',\n 'model\/model.pkl',\n 'model\/requirements.txt']\n<\/code><\/pre>\n<p>Could anyone tell me exactly how I can fix this model serve problem?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626398499507,
        "Question_score":1,
        "Question_tags":"scikit-learn|webserver|mlflow",
        "Question_view_count":787,
        "Owner_creation_time":1553088438367,
        "Owner_last_access_time":1664054397820,
        "Owner_location":"Tehran, Tehran Province, Iran",
        "Owner_reputation":415,
        "Owner_up_votes":300,
        "Owner_down_votes":2,
        "Owner_views":37,
        "Question_last_edit_time":1657456057947,
        "Answer_body":"<p>The problem caused by <code>mlflow.utils.autologging_utils<\/code> WARNING.<\/p>\n<p>When the model is created, data input signature is saved on the <code>MLmodel<\/code> file with some.\nYou should change <code>response_length<\/code> signature input type from <code>string<\/code> to <code>double<\/code> by replacing<\/p>\n<pre><code>{&quot;name&quot;: &quot;response_length&quot;, &quot;type&quot;: &quot;double&quot;}\n<\/code><\/pre>\n<p>instead of<\/p>\n<pre><code>{&quot;name&quot;: &quot;response_length&quot;, &quot;type&quot;: &quot;string&quot;}\n<\/code><\/pre>\n<p>so it doesn't need to be converted. After serving the model with edited <code>MLmodel<\/code> file, the web server worked as expected.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1626398499507,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68402406",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73620580,
        "Question_title":"Hi everyone, I'm trying to authentication and authorize MLFlow my infra is in AWS and my mlflow will run in eks ? can we do it with mlflow plugin",
        "Question_body":"<p>Do we have any way to do this?<\/p>\n<ol>\n<li>I tried to authenticate using AWS ALB that did not work for me.<\/li>\n<\/ol>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1662460519567,
        "Question_score":0,
        "Question_tags":"kubernetes|mlflow",
        "Question_view_count":26,
        "Owner_creation_time":1562213601020,
        "Owner_last_access_time":1663912595940,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73620580",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67968777,
        "Question_title":"How to get run id from run name in MLflow",
        "Question_body":"<p>To download artifacts from a run, you need run id. I get the run id from the UI as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/FJtAe.png\" rel=\"nofollow noreferrer\">Run id from the UI<\/a><\/p>\n<p>But when I set the run name parameter, run id is not visible in the UI. How to find the run Id of a particular run in MLflow ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1623667148760,
        "Question_score":2,
        "Question_tags":"mlflow",
        "Question_view_count":1492,
        "Owner_creation_time":1585919790787,
        "Owner_last_access_time":1641475609990,
        "Owner_location":null,
        "Owner_reputation":29,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67968777",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":64645798,
        "Question_title":"How to log a sklearn pipeline with a Keras step using mlflow.pyfunc.log_model()? TypeError: can't pickle _thread.RLock objects",
        "Question_body":"<p>I would like to log into MlFlow a <code>sklearn<\/code> pipeline with a Keras step.<\/p>\n<p>The pipeline has 2 steps: a <code>sklearn<\/code> StandardScale and a Keras TensorFlow model.<\/p>\n<p>I am using mlflow.pyfunc.log_model() as possible solution, but I am having this error:<\/p>\n<pre><code>TypeError: can't pickle _thread.RLock objects\n---&gt;   mlflow.pyfunc.log_model(&quot;test1&quot;, python_model=wrappedModel, signature=signature)\n<\/code><\/pre>\n<p>Here is my code:<\/p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport keras\nfrom keras import layers, Input\nfrom keras.wrappers.scikit_learn import KerasRegressor\nimport mlflow.pyfunc\nfrom sklearn.pipeline import Pipeline\nfrom mlflow.models.signature import infer_signature\n\n#toy dataframe\ndf1 = pd.DataFrame([[1,2,3,4,5,6], [10,20,30,40,50,60],[100,200,300,400,500,600]] )\n\n#create train test datasets\nX_train, X_test = train_test_split(df1, random_state=42, shuffle=True)\n\n#scale X_train\nscaler = StandardScaler()\nX_train_s = scaler.fit_transform(X_train)\nX_train_s = pd.DataFrame(X_train_s)\n\n#wrap the keras model to use it inside of sklearn pipeline\ndef create_model(optimizer='adam', loss='mean_squared_error', s = X_train.shape[1]):\n  input_layer = keras.Input(shape=(s,))\n  # &quot;encoded&quot; is the encoded representation of the input\n  encoded = layers.Dense(25, activation='relu')(input_layer)\n  encoded = layers.Dense(2, activation='relu')(encoded)\n\n  # &quot;decoded&quot; is the lossy reconstruction of the input\n  decoded = layers.Dense(2, activation='relu')(encoded)\n  decoded = layers.Dense(25, activation='relu')(encoded)\n  decoded = layers.Dense(s, activation='linear')(decoded)\n  \n  model = keras.Model(input_layer, decoded)\n  model.compile(optimizer, loss)\n  return model\n\n# wrap the model\nmodel = KerasRegressor(build_fn=create_model, verbose=1)\n\n# create the pipeline\npipe = Pipeline(steps=[\n    ('scale', StandardScaler()),\n    ('model',model)\n])\n\n#function to wrap the pipeline to be logged by mlflow\nclass SklearnModelWrapper(mlflow.pyfunc.PythonModel):\n  def __init__(self, model):\n    self.model = model\n    \n  def predict(self, context, model_input):\n    return self.model.predict(model_input)[:,1]\n  \n  \nmlflow.end_run()\nwith mlflow.start_run(run_name='test1'):\n\n  #train the pipeline\n  pipe.fit(X_train, X_train_s, model__epochs=2)\n  \n  #wrap the model for mlflow log\n  wrappedModel = SklearnModelWrapper(pipe)\n\n  # Log the model with a signature that defines the schema of the model's inputs and outputs. \n  signature = infer_signature(X_train, wrappedModel.predict(None, X_train))\n  mlflow.pyfunc.log_model(&quot;test1&quot;, python_model=wrappedModel, signature=signature)\n  \n<\/code><\/pre>\n<p>From what I googled, it seems like this type of error is related to concurrency of threads. It could be then related to the TensorFlow, since it distributes the code during the model training phase.<\/p>\n<p>However, the offending code line is after the training phase. If I remove this line, the rest of the code works, which makes me think that it happens after the concurrency phase of the model training. I have no idea why I am getting this error in this context.\nI am a beginner? Can someone please help me?\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1604320799543,
        "Question_score":4,
        "Question_tags":"python|keras|scikit-learn|mlflow",
        "Question_view_count":787,
        "Owner_creation_time":1339148818537,
        "Owner_last_access_time":1616602620223,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":1604334305112,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64645798",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61615818,
        "Question_title":"Setting-up MLflow on Google Colab",
        "Question_body":"<p>I frequently use Google Colab to train TF\/PyTorch models as Colab provides me with GPU\/TPU runtime. Besides, I like working with MLflow to store and compare trained models, tracking progress, sharing, etc.  What are the available solutions to use MLflow with Google Colab?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1588689839033,
        "Question_score":7,
        "Question_tags":"google-colaboratory|mlflow|mlops",
        "Question_view_count":6053,
        "Owner_creation_time":1552661046210,
        "Owner_last_access_time":1663946284313,
        "Owner_location":"Paris, France\/ Ternopil, Ukraine",
        "Owner_reputation":1131,
        "Owner_up_votes":55,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Question_last_edit_time":1621937850167,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61615818",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63973530,
        "Question_title":"Convert an instance of xgboost.Booster into a model that implements the scikit-learn API",
        "Question_body":"<p>I am trying to use <code>mlflow<\/code> to save a model and then load it later to make predictions.<\/p>\n<p>I'm using a <code>xgboost.XGBRegressor<\/code> model and its sklearn functions <code>.predict()<\/code> and <code>.predict_proba()<\/code> to make predictions but it turns out that <code>mlflow<\/code> doesn't support models that implements the sklearn API, so when loading the model later from mlflow, mlflow returns an instance of <code>xgboost.Booster<\/code>, and it doesn't implements the <code>.predict()<\/code> or <code>.predict_proba()<\/code> functions.<\/p>\n<p>Is there a way to convert a <code>xgboost.Booster<\/code> back into a <code>xgboost.sklearn.XGBRegressor<\/code> object that implements the sklearn API functions?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1600550858547,
        "Question_score":3,
        "Question_tags":"scikit-learn|save|xgboost|mlflow|xgbclassifier",
        "Question_view_count":1317,
        "Owner_creation_time":1592264086427,
        "Owner_last_access_time":1663962090790,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Have you tried wrapping up your model in custom class, logging and loading it using <code>mlflow.pyfunc.PythonModel<\/code>?\nI put up a simple example and upon loading back the model it correctly shows <code>&lt;class 'xgboost.sklearn.XGBRegressor'&gt;<\/code> as a type.<\/p>\n<p>Example:<\/p>\n<pre><code>import xgboost as xgb\nxg_reg = xgb.XGBRegressor(...)\n\nclass CustomModel(mlflow.pyfunc.PythonModel):\n    def __init__(self, xgbRegressor):\n        self.xgbRegressor = xgbRegressor\n\n    def predict(self, context, input_data):\n        print(type(self.xgbRegressor))\n        \n        return self.xgbRegressor.predict(input_data)\n\n# Log model to local directory\nwith mlflow.start_run():\n     custom_model = CustomModel(xg_reg)\n     mlflow.pyfunc.log_model(&quot;custome_model&quot;, python_model=custom_model)\n\n\n# Load model back\nfrom mlflow.pyfunc import load_model\nmodel = load_model(&quot;\/mlruns\/0\/..\/artifacts\/custome_model&quot;)\nmodel.predict(X_test)\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>&lt;class 'xgboost.sklearn.XGBRegressor'&gt;\n[ 9.107417 ]\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1600607182583,
        "Answer_score":4.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63973530",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65316586,
        "Question_title":"get the run id for an mlflow experiment with the name?",
        "Question_body":"<p>I currently created an experiment in mlflow and created multiple runs in the experiment.<\/p>\n<pre><code>from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nimport mlflow\n\nexperiment_name=&quot;experiment-1&quot;\nmlflow.set_experiment(experiment_name)\n\nno_of_trees=[100,200,300]\ndepths=[2,3,4]\nfor trees in no_of_trees:\n    for depth in depths:\n        with mlflow.start_run() as run:\n            model=RandomForestRegressor(n_estimators=trees, criterion='mse',max_depth=depth)\n            model.fit(x_train, y_train)\n            predictions=model.predict(x_cv)\n            mlflow.log_metric('rmse',mean_squared_error(y_cv, predictions))\n<\/code><\/pre>\n<p>after creating the runs, I wanted to get the best run_id for this experiment. for now, I can get the best run by looking at the UI of mlflow but how can we do right the program?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1608086727487,
        "Question_score":6,
        "Question_tags":"python|mlflow",
        "Question_view_count":6374,
        "Owner_creation_time":1479190327737,
        "Owner_last_access_time":1660584310400,
        "Owner_location":"R G U K T , basar, Andhra Pradesh, India",
        "Owner_reputation":2470,
        "Owner_up_votes":265,
        "Owner_down_votes":22,
        "Owner_views":251,
        "Question_last_edit_time":null,
        "Answer_body":"<p>we can get the experiment id from the experiment name and we can use python API to get the best runs.<\/p>\n<pre><code>experiment_name = &quot;experiment-1&quot;\ncurrent_experiment=dict(mlflow.get_experiment_by_name(experiment_name))\nexperiment_id=current_experiment['experiment_id']\n<\/code><\/pre>\n<p>By using the experiment id, we can get all the runs and we can sort them based on metrics like below. In the below code, rmse is my metric name (so it may be different for you based on metric name)<\/p>\n<pre><code>df = mlflow.search_runs([experiment_id], order_by=[&quot;metrics.rmse DESC&quot;])\nbest_run_id = df.loc[0,'run_id']\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1608086727487,
        "Answer_score":15.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65316586",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60695933,
        "Question_title":"MLflow: Why can't backend-store-uri be an s3 location?",
        "Question_body":"<p>I'm new to mlflow and I can't figure out why the <code>artifact store<\/code> can't be the same as the <code>backend store<\/code>? <\/p>\n\n<p>The only reason I can think of is to be able to query the experiments with SQL syntax... but since we can interact with the runs using <code>mlflow ui<\/code> I just don't understand why all artifacts and parameters can't go to a same location (which is what happens when using local storage).<\/p>\n\n<p>Can anyone shed some light on this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1584294693613,
        "Question_score":4,
        "Question_tags":"amazon-s3|mlflow",
        "Question_view_count":768,
        "Owner_creation_time":1528574640850,
        "Owner_last_access_time":1663976741153,
        "Owner_location":null,
        "Owner_reputation":133,
        "Owner_up_votes":10,
        "Owner_down_votes":2,
        "Owner_views":76,
        "Question_last_edit_time":null,
        "Answer_body":"<p>MLflow's Artifacts are typically ML models, i.e. relatively large binary files. On the other hand, run data are typically a couple of floats.<\/p>\n<p>In the end it is not a question of what is possible or not (many things are possible if you put enough effort into it), but rather to follow good practices:<\/p>\n<ul>\n<li>storing large binary artifacts in an SQL database is possible but is bound the degrade the performance of the database sooner or later, and this in turn will degrade your user experience.<\/li>\n<li>storing a couple of floats from a SQL database for quick retrieval for display in a front-end or via command line is a robust industry-proven classic<\/li>\n<\/ul>\n<p>It remains true that the documentation of MLflow on the architecture design rationale could be improved (as of 2020)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1611045077983,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60695933",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":55943088,
        "Question_title":"Databricks notebook integrated mlflow artifact location and retention",
        "Question_body":"<ol>\n<li><p>Currently by default in notebook run, it will create an experiment ID, but the Artifact Location would point to something under dbfs:\/databricks\/mlflow\/{experiment id}. If there is a way we may change this in default experiment creation? We like to manage the storage outside databricks.<\/p><\/li>\n<li><p>How long is default TTL for experiment runs and metrics? Is it configurable and how?<\/p><\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1556747098277,
        "Question_score":1,
        "Question_tags":"azure-databricks|mlflow",
        "Question_view_count":587,
        "Owner_creation_time":1458276216763,
        "Owner_last_access_time":1575678794047,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55943088",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72855204,
        "Question_title":"MLflow is taking longer than expected time to finish logging metrics and parameters",
        "Question_body":"<p>I'm running a code where I have to perform multiple iterations for a set of products to select the best performing model. While running multiple iterations for a single product, I need to log details of every single run using mlflow(using mlflow with pandas-udf). While logging for individual iterations are taking around 2 seconds but the parent run under which I'm tracking every iteration details is taking 1.5 hours to finish. Here is the code -<\/p>\n<pre><code>@F.pandas_udf( model_results_schema, F.PandasUDFType.GROUPED_MAP )\ndef get_gam_pe_results( model_input ):\n    ...\n    ...\n    for j, gam_terms in enumerate(term_list[-1]):\n        results_iteration_output_1, results_iteration_output, results_iteration_all = run_gam_model(gam_terms)\n        \n        results_iteration_version = results_iteration_version.append(results_iteration_output)\n        unique_id = uuid.uuid1()\n        metric_list = [&quot;AIC&quot;, &quot;AICc&quot;, &quot;GCV&quot;, &quot;adjusted_R2&quot;, &quot;deviance&quot;, &quot;edof&quot;, &quot;elasticity_in_k&quot;, &quot;loglikelihood&quot;,\n                      &quot;scale&quot;]\n        param_list = [&quot;features&quot;]\n        start_time = str(datetime.now())\n        with mlflow.start_run(run_id=parent_run_id, experiment_id=experiment_id):\n            with mlflow.start_run(run_name=str(model_input['prod_id'].iloc[1]) + &quot;-&quot; + unique_id.hex,\n                                  experiment_id=experiment_id, nested=True):\n                for item in results_iteration_output.columns.values.tolist():\n                        if item in metric_list:\n                            mlflow.log_metric(item, results_iteration_output[item].iloc[0])\n                        if item in param_list:\n                            mlflow.log_param(item, results_iteration_output[item].iloc[0])\n                            \n                end_time = str(datetime.now())\n                mlflow.log_param(&quot;start_time&quot;, start_time)\n                mlflow.log_param(&quot;end_time&quot;, end_time)\n<\/code><\/pre>\n<p>Outside pandas-udf -<\/p>\n<pre><code>current_time = str(datetime.today().replace(microsecond=0))\nrun_id = None\nwith mlflow.start_run(run_name=&quot;MLflow_pandas_udf_testing-&quot;+current_time, experiment_id=experiment_id) as run:\n    run_id = run.info.run_uuid\n    gam_model_output = (Product_data\n                        .withColumn(&quot;run_id&quot;, F.lit(run_id))\n                        .groupby(['prod_id'])\n                        .apply(get_gam_pe_results)\n                       )\n<\/code><\/pre>\n<p>Note - Running this entire code in Databricks(cluster has 8 cores and 28gb ram).<\/p>\n<p>Any idea why this parent run is taking so long to finish while it's only 2 seconds to finish each iterations?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jzqkx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jzqkx.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1656930055540,
        "Question_score":0,
        "Question_tags":"python-3.x|machine-learning|azure-databricks|mlflow|pandas-udf",
        "Question_view_count":76,
        "Owner_creation_time":1452602206603,
        "Owner_last_access_time":1663948894330,
        "Owner_location":"Kolkata, West Bengal, India",
        "Owner_reputation":55,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1663949207680,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72855204",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73320708,
        "Question_title":"Set run description programmatically in mlflow",
        "Question_body":"<p>Similar to <a href=\"https:\/\/stackoverflow.com\/questions\/57199472\/is-it-possible-to-set-change-mlflow-run-name-after-run-initial-creation#:%7E:text=It%20is%20possible%20to%20edit,you%27d%20like%20to%20edit.&amp;text=There%27s%20currently%20no%20stable%20public,the%20tag%20with%20key%20mlflow.\">this question<\/a>, I'd like to edit\/set the description of a run via code, instead of editing it via UI.<\/p>\n<p>To clarify, I don't want to set the description of my entire experiment, only of a single run.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ogUgu.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ogUgu.png\" alt=\"Image showing what I want to edit\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660221143727,
        "Question_score":1,
        "Question_tags":"python|artificial-intelligence|mlflow",
        "Question_view_count":89,
        "Owner_creation_time":1527525798183,
        "Owner_last_access_time":1664059587093,
        "Owner_location":"Sarajevo, Bosnia and Herzegovina",
        "Owner_reputation":736,
        "Owner_up_votes":829,
        "Owner_down_votes":8,
        "Owner_views":57,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There are two ways to set the description.<\/p>\n<h3>1. <code>description<\/code> parameter<\/h3>\n<p>You can set a description using a markdown string for your run in <code>mlflow.start_run()<\/code> using <code>description<\/code> parameter. Here is an example.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if __name__ == &quot;__main__&quot;:\n    # load dataset and other stuff\n\n    run_description = &quot;&quot;&quot;\n### Header\nThis is a test **Bold**, *italic*, ~~strikethrough~~ text.\n[And this is an example hayperlink](http:\/\/example.com\/).\n    &quot;&quot;&quot;\n\n    with mlflow.start_run(description=run_description) as run:\n        # train model and other stuff\n<\/code><\/pre>\n<h3>2. <code>mlflow.note.content<\/code> tag<\/h3>\n<p>You can set\/edit run names by setting the tag with the key <code>mlflow.note.content<\/code>, which is what the UI (currently) does under the hood.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if __name__ == &quot;__main__&quot;:\n    # load dataset and other stuff\n\n    run_description = &quot;&quot;&quot;\n### Header\nThis is a test **Bold**, *italic*, ~~strikethrough~~ text.\n[And this is an example hayperlink](http:\/\/example.com\/).\n    &quot;&quot;&quot;\n\n    tags = {\n        'mlflow.note.content': run_description\n    }\n\n    with mlflow.start_run(tags=tags) as run:\n        # train model and other stuff\n<\/code><\/pre>\n<h3>Result<\/h3>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4zZa9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4zZa9.png\" alt=\"output of the given example\" \/><\/a><\/p>\n<hr \/>\n<p>If you set <code>description<\/code> parameter and <code>mlflow.note.content<\/code> tag in <code>mlflow.start_run()<\/code>, you'll get this error.<\/p>\n<pre><code>Description is already set via the tag mlflow.note.content in tags.\nRemove the key mlflow.note.content from the tags or omit the description.\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1660231264752,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1660307815687,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73320708",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":64771247,
        "Question_title":"Logging a PySpark dataframe into a MLFlow Artifact",
        "Question_body":"<p>I am currently writing an MLFlow artifact to the dbfs but I am using pandas using the code below...<\/p>\n<pre><code>temp = tempfile.NamedTemporaryFile(prefix=&quot;*****&quot;, suffix=&quot;.csv&quot;)\ntemp_name = temp.name\ntry:\n  df.to_csv(temp_name, index=False)\n  mlflow.log_artifact(temp_name, &quot;******&quot;)\nfinally:\n  temp.close() # Delete the temp file\n<\/code><\/pre>\n<p>How would I write this if 'df' was a spark dataframe?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1605020000127,
        "Question_score":1,
        "Question_tags":"python|pyspark|mlflow",
        "Question_view_count":810,
        "Owner_creation_time":1593519939237,
        "Owner_last_access_time":1660311748437,
        "Owner_location":null,
        "Owner_reputation":129,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64771247",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68233056,
        "Question_title":"Logging SKLearn Models in the same folder while running multiple models in Pandas UDF",
        "Question_body":"<p>I am trying to run multiple XGBoost models and save the resulting models in the form of experiments. However, when I call the UDF function on my pyspark dataframe the models are being saved in a multiple folders.<\/p>\n<p>It appears that they are being randomly split in smaller batches and stored. Is there a way to ensure that all models are saved in the same run\/ folder such that I can easily load them back later.<\/p>\n<pre><code>def classification_xgb(df):\n  #modeling code\n  mlflow.sklearn.log_model(xgb, model_name)\n\n\ndat_m.groupBy(&quot;Product&quot;).applyInPandas(classification_xgb, schema).show(10000,False)\n<\/code><\/pre>\n<p>I have over 100 products for which I need to create models and save in the same run instance.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1625283818387,
        "Question_score":1,
        "Question_tags":"pandas|pyspark|azure-databricks|sklearn-pandas|mlflow",
        "Question_view_count":79,
        "Owner_creation_time":1625282679850,
        "Owner_last_access_time":1641308459047,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1625285948292,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68233056",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58554979,
        "Question_title":"Cannot Start mlflow ui on google cloud platform virtual machine instance",
        "Question_body":"<p>after running mlflow ui on command line\nand  clicking <a href=\"http:\/\/127.0.0.1:5000\/\" rel=\"nofollow noreferrer\">http:\/\/127.0.0.1:5000\/<\/a>\ni get site cannot be reached\n127.0.0.1 refused to connect.<\/p>\n<p>I have already updated firewall rules on VPC network in GCP and on my local machine and activated the ports<\/p>\n<blockquote>\n<p>This site can\u2019t be reached127.0.0.1 refused to connect.<\/p>\n<p>Try:<\/p>\n<ul>\n<li>Checking the connection<\/li>\n<li>Checking the proxy and the firewall<\/li>\n<\/ul>\n<p>ERR_CONNECTION_REFUSED<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1571992526673,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|mlflow",
        "Question_view_count":360,
        "Owner_creation_time":1516552952317,
        "Owner_last_access_time":1616233012253,
        "Owner_location":"Haryana, India",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1592644375060,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58554979",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":66855807,
        "Question_title":"How to add more metrics to a finished MLflow run?",
        "Question_body":"<p>Once an MLflow run is finished, external scripts can access its parameters and metrics using python <code>mlflow<\/code> client and <code>mlflow.get_run(run_id)<\/code> method, but the <code>Run<\/code> object returned by <code>get_run<\/code> seems to be read-only.<\/p>\n<p>Specifically, <code>.log_param<\/code> <code>.log_metric<\/code>, or <code>.log_artifact<\/code> cannot be used on the object returned by <code>get_run<\/code>, raising errors like these:<\/p>\n<pre><code>AttributeError: 'Run' object has no attribute 'log_param'\n<\/code><\/pre>\n<p>If we attempt to run any of the <code>.log_*<\/code> methods on <code>mlflow<\/code>, it would log them into to a new run  with auto-generated run ID in the <code>Default<\/code> experiment.<\/p>\n<p>Example:<\/p>\n<pre><code>final_model_mlflow_run = mlflow.get_run(final_model_mlflow_run_id)\n\nwith mlflow.ActiveRun(run=final_model_mlflow_run) as myrun:    \n    \n    # this read operation uses correct run\n    run_id = myrun.info.run_id\n    print(run_id)\n    \n    # this write operation writes to a new run \n    # (with auto-generated random run ID) \n    # in the &quot;Default&quot; experiment (with exp. ID of 0)\n    mlflow.log_param(&quot;test3&quot;, &quot;This is a test&quot;)\n   \n<\/code><\/pre>\n<p>Note that the above problem exists regardless of the <code>Run<\/code> status (<code>.info.status<\/code> can be both &quot;FINISHED&quot; or &quot;RUNNING&quot;, without making any difference).<\/p>\n<p>I wonder if this read-only behavior is by design (given that immutable modeling runs improve experiments reproducibility)? I can appreciate that, but it also goes against code modularity if everything has to be done within a single monolith like the <code>with mlflow.start_run()<\/code> context...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1617027126043,
        "Question_score":3,
        "Question_tags":"python|databricks|mlflow",
        "Question_view_count":1269,
        "Owner_creation_time":1529411528930,
        "Owner_last_access_time":1664050484457,
        "Owner_location":"EU",
        "Owner_reputation":3260,
        "Owner_up_votes":1100,
        "Owner_down_votes":4,
        "Owner_views":466,
        "Question_last_edit_time":1617040232536,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66855807",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69796285,
        "Question_title":"Artifacts generations for pmml models in mlflow",
        "Question_body":"<p>I generated a sample pmml file which contains lightgbm model, when i am trying to log it and generate artifacts in mlflow i am getting error: 'Model' object has no attribute 'save_model'<\/p>\n<p>The code i used for logging:<\/p>\n<pre><code>import mlflow\n\nfrom pypmml import Model\n\nlr = Model.fromFile('iris.pmml') \n\nmlflow.lightgbm.log_model(lr,&quot;model&quot;)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1635766055407,
        "Question_score":0,
        "Question_tags":"mlflow|pmml",
        "Question_view_count":95,
        "Owner_creation_time":1635765403513,
        "Owner_last_access_time":1642406857823,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1635843903449,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69796285",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71121324,
        "Question_title":"Mlflow registed model version increment in python",
        "Question_body":"<p>I want to keep versions of the model in mlflow but not as version[1,2,3,...]\ninstead, i want to increment model's versions like 1.1 1.2 and when I feel that there is some major change I want increment to 2.0<\/p>\n<p>please let me know how this can be done.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1644901430677,
        "Question_score":0,
        "Question_tags":"python|model|version-control|mlflow",
        "Question_view_count":24,
        "Owner_creation_time":1516006161920,
        "Owner_last_access_time":1645106959933,
        "Owner_location":"Arunachal Pradesh, India",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71121324",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62711259,
        "Question_title":"Customize metric visualization in MLFlow UI when using mlflow.tensorflow.autolog()",
        "Question_body":"<p>I'm trying to integrate MLFlow to my project. Because I'm using <code>tf.keras.fit_generator()<\/code> for my training so I take advantage of <code>mlflow.tensorflow.autolog()<\/code>(<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a> here) to enable automatic logging of metrics and parameters:<\/p>\n<pre><code>    model = Unet()\n    optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n\n    metrics = [IOUScore(threshold=0.5), FScore(threshold=0.5)]\n    model.compile(optimizer, customized_loss, metrics)\n\n    callbacks = [\n        tf.keras.callbacks.ModelCheckpoint(&quot;model.h5&quot;, save_weights_only=True, save_best_only=True, mode='min'),\n        tf.keras.callbacks.TensorBoard(log_dir='.\/logs', profile_batch=0, update_freq='batch'),\n    ]\n\n\n    train_dataset = Dataset(src_dir=SOURCE_DIR)\n\n    train_data_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n\n   \n    with mlflow.start_run():\n        mlflow.tensorflow.autolog()\n        mlflow.log_param(&quot;batch_size&quot;, BATCH_SIZE)\n\n        model.fit_generator(\n            train_data_loader,\n            steps_per_epoch=len(train_data_loader),\n            epochs=EPOCHS,\n            callbacks=callbacks   \n            )\n<\/code><\/pre>\n<p>I expected something like this (just a demonstration taken from the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#visualizing-metrics\" rel=\"nofollow noreferrer\">docs<\/a>):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" alt=\"Visualization on the docs\" \/><\/a><\/p>\n<p>However, after the training finished, this is what I got:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" alt=\"f1_score visualization\" \/><\/a><\/p>\n<p>How can I configure so that the metric plot will update and display its value at each epoch instead of just showing the latest value?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593764146630,
        "Question_score":2,
        "Question_tags":"tf.keras|mlflow",
        "Question_view_count":1035,
        "Owner_creation_time":1467943515393,
        "Owner_last_access_time":1664083114540,
        "Owner_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Owner_reputation":173,
        "Owner_up_votes":97,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":1594008626392,
        "Answer_body":"<p>After searching around, I found <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/2390\" rel=\"nofollow noreferrer\">this issue<\/a> related to my problem above. Actually, all my metrics just logged once each training (instead of each epoch as my intuitive thought). The reason is I didn't specify the <code>every_n_iter<\/code> parameter in <code>mlflow.tensorflow.autolog()<\/code>, which indicates how many 'iterations' must pass before MLflow logs metric executed (see the <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a>). So, changing my code to:<\/p>\n<p><code>mlflow.tensorflow.autolog(every_n_iter=1)<\/code><\/p>\n<p>fixed the problem.<\/p>\n<p>P\/s: Remember that in TF 2.x, an 'iteration' is an epoch (in TF 1.x it's a batch).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1594008525280,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62711259",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":51064366,
        "Question_title":"Can't run MLflow web-based user interface",
        "Question_body":"<p>I've installed <a href=\"https:\/\/mlflow.org\/\" rel=\"nofollow noreferrer\">MLflow<\/a> on Ubuntu Server 18.04 LTS, in a virtual environment (Python 3), using its <a href=\"https:\/\/mlflow.org\/docs\/latest\/quickstart.html\" rel=\"nofollow noreferrer\">Quickstart documentation<\/a>:<\/p>\n\n<pre><code>$ python3 -m venv mlflow\n$ source \/home\/emre\/mlflow\/bin\/activate\n$ pip install mlflow\n<\/code><\/pre>\n\n<p>that gave the following output during install:<\/p>\n\n<pre><code>Collecting mlflow\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e8\/b3\/cf358e182be34a62fcd6843e5df793f278bd9d24f78f565509cb927c6a22\/mlflow-0.1.0.tar.gz (4.3MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3MB 323kB\/s\nCollecting Flask (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7f\/e7\/08578774ed4536d3242b14dacb4696386634607af824ea997202cd0edb4b\/Flask-1.0.2-py2.py3-none-any.whl (91kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 9.4MB\/s\nCollecting awscli (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ba\/32\/d6d254f6ccc2ed21f02d81f38709ff06feca9cbdb2e68ea90635fa483a73\/awscli-1.15.46-py2.py3-none-any.whl (1.3MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3MB 1.0MB\/s\nCollecting boto3 (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/24\/e0\/a98898b94d8093bbd8fd4576fb2e89620adac1e24a2bfc28d11c4ce29a5b\/boto3-1.7.46-py2.py3-none-any.whl (128kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 8.8MB\/s\nCollecting click&gt;=6.7 (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/34\/c1\/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77\/click-6.7-py2.py3-none-any.whl (71kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 9.3MB\/s\nCollecting databricks-cli (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/58\/78\/4bda6f29a091ab7b0ad29efdba2491e5d0b56bd09d608857e6f0b799be48\/databricks-cli-0.7.2.tar.gz\nCollecting gitpython (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ac\/c9\/96d7c86c623cb065976e58c0f4898170507724d6b4be872891d763d686f4\/GitPython-2.1.10-py2.py3-none-any.whl (449kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 450kB 2.9MB\/s\nCollecting numpy (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/68\/1e\/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2\/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.2MB 110kB\/s\nCollecting pandas (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/57\/eb\/6ab533ea8e35e7dd159af6922ac1123d4565d89f3926ad9a6aa46530978f\/pandas-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (11.8MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11.8MB 116kB\/s\nCollecting protobuf (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/fc\/f0\/db040681187496d10ac50ad167a8fd5f953d115b16a7085e19193a6abfd2\/protobuf-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (7.1MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.1MB 177kB\/s\nCollecting pygal (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/5f\/b7\/201c9254ac0d2b8ffa3bb2d528d23a4130876d9ba90bc28e99633f323f17\/pygal-2.4.0-py2.py3-none-any.whl (127kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 9.7MB\/s\nCollecting python-dateutil (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/cf\/f5\/af2b09c957ace60dcfac112b669c45c8c97e32f94aa8b56da4c6d1682825\/python_dateutil-2.7.3-py2.py3-none-any.whl (211kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 215kB 6.0MB\/s\nCollecting pyyaml (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/10\/7d\/6efe0bd69580fecd40adf47ebaf8d807238308ccb851f0549881fa7605aa\/PyYAML-4.1.tar.gz (153kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 7.8MB\/s\nCollecting querystring_parser (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/57\/64\/3086a9a991ff3aca7b769f5b0b51ff8445a06337ae2c58f215bcee48f527\/querystring_parser-1.2.3.tar.gz\nCollecting requests&gt;=2.17.3 (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/65\/47\/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda\/requests-2.19.1-py2.py3-none-any.whl (91kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 8.2MB\/s\nCollecting scikit-learn (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/3d\/2d\/9fbc7baa5f44bc9e88ffb7ed32721b879bfa416573e85031e16f52569bc9\/scikit_learn-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.4MB 108kB\/s\nCollecting scipy (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/a8\/0b\/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730\/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 31.2MB 42kB\/s\nCollecting six&gt;=1.10.0 (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/67\/4b\/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a\/six-1.11.0-py2.py3-none-any.whl\nCollecting uuid (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ce\/63\/f42f5aa951ebf2c8dac81f77a8edcc1c218640a2a35a03b9ff2d4aa64c3d\/uuid-1.30.tar.gz\nCollecting zipstream (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/1a\/a4\/58f0709cef999db1539960aa2ae77100dc800ebb8abb7afc97a1398dfb2f\/zipstream-1.1.4.tar.gz\nCollecting itsdangerous&gt;=0.24 (from Flask-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/dc\/b4\/a60bcdba945c00f6d608d8975131ab3f25b22f2bcfe1dab221165194b2d4\/itsdangerous-0.24.tar.gz (46kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 10.4MB\/s\nCollecting Werkzeug&gt;=0.14 (from Flask-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/20\/c4\/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243\/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 327kB 4.0MB\/s\nCollecting Jinja2&gt;=2.10 (from Flask-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7f\/ff\/ae64bacdfc95f27a016a7bed8e8686763ba4d277a78ca76f32659220a731\/Jinja2-2.10-py2.py3-none-any.whl (126kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 8.2MB\/s\nCollecting rsa&lt;=3.5.0,&gt;=3.1.2 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e1\/ae\/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e\/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 10.5MB\/s\nCollecting botocore==1.10.46 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b4\/04\/ddaad5574f70a539d106e8d53b4685e3de4387de7a16884a95459f8c7691\/botocore-1.10.46-py2.py3-none-any.whl (4.4MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.4MB 314kB\/s\nCollecting s3transfer&lt;0.2.0,&gt;=0.1.12 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/d7\/14\/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d\/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 10.6MB\/s\nCollecting colorama&lt;=0.3.9,&gt;=0.2.5 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/db\/c8\/7dcf9dbcb22429512708fe3a547f8b6101c0d02137acbd892505aee57adf\/colorama-0.3.9-py2.py3-none-any.whl\nCollecting docutils&gt;=0.10 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/36\/fa\/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d\/docutils-0.14-py3-none-any.whl (543kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 552kB 2.5MB\/s\nCollecting jmespath&lt;1.0.0,&gt;=0.7.1 (from boto3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b7\/31\/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365\/jmespath-0.9.3-py2.py3-none-any.whl\nCollecting configparser&gt;=0.3.5 (from databricks-cli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7c\/69\/c2ce7e91c89dc073eb1aa74c0621c3eefbffe8216b3f9af9d3885265c01c\/configparser-3.5.0.tar.gz\nCollecting tabulate&gt;=0.7.7 (from databricks-cli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/12\/c2\/11d6845db5edf1295bc08b2f488cf5937806586afe42936c3f34c097ebdc\/tabulate-0.8.2.tar.gz (45kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 7.9MB\/s\nCollecting gitdb2&gt;=2.0.0 (from gitpython-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e0\/95\/c772c13b7c5740ec1a0924250e6defbf5dfdaee76a50d1c47f9c51f1cabb\/gitdb2-2.0.3-py2.py3-none-any.whl (63kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 11.2MB\/s\nCollecting pytz&gt;=2011k (from pandas-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/dc\/83\/15f7833b70d3e067ca91467ca245bae0f6fe56ddc7451aa0dc5606b120f2\/pytz-2018.4-py2.py3-none-any.whl (510kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 512kB 421kB\/s\nRequirement already satisfied: setuptools in .\/mlflow\/lib\/python3.6\/site-packages (from protobuf-&gt;mlflow)\nCollecting chardet&lt;3.1.0,&gt;=3.0.2 (from requests&gt;=2.17.3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/bc\/a9\/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8\/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 143kB 8.7MB\/s\nCollecting idna&lt;2.8,&gt;=2.5 (from requests&gt;=2.17.3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/4b\/2a\/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165\/idna-2.7-py2.py3-none-any.whl (58kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 10.3MB\/s\nCollecting urllib3&lt;1.24,&gt;=1.21.1 (from requests&gt;=2.17.3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/bd\/c9\/6fdd990019071a4a32a5e7cb78a1d92c53851ef4f56f62a3486e6a7d8ffb\/urllib3-1.23-py2.py3-none-any.whl (133kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 143kB 8.3MB\/s\nCollecting certifi&gt;=2017.4.17 (from requests&gt;=2.17.3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7c\/e6\/92ad559b7192d846975fc916b65f667c7b8c3a32bea7372340bfe9a15fa5\/certifi-2018.4.16-py2.py3-none-any.whl (150kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 8.0MB\/s\nCollecting MarkupSafe&gt;=0.23 (from Jinja2&gt;=2.10-&gt;Flask-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/4d\/de\/32d741db316d8fdb7680822dd37001ef7a448255de9699ab4bfcbdf4172b\/MarkupSafe-1.0.tar.gz\nCollecting pyasn1&gt;=0.1.3 (from rsa&lt;=3.5.0,&gt;=3.1.2-&gt;awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/a0\/70\/2c27740f08e477499ce19eefe05dbcae6f19fdc49e9e82ce4768be0643b9\/pyasn1-0.4.3-py2.py3-none-any.whl (72kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 10.9MB\/s\nCollecting smmap2&gt;=2.0.0 (from gitdb2&gt;=2.0.0-&gt;gitpython-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e3\/59\/4e22f692e65f5f9271252a8e63f04ce4ad561d4e06192478ee48dfac9611\/smmap2-2.0.3-py2.py3-none-any.whl\nBuilding wheels for collected packages: mlflow, databricks-cli, pyyaml, querystring-parser, uuid, zipstream, itsdangerous, configparser, tabulate, MarkupSafe\n  Running setup.py bdist_wheel for mlflow ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/mlflow\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmp10fdrz2ypip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for mlflow\n  Running setup.py clean for mlflow\n  Running setup.py bdist_wheel for databricks-cli ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/databricks-cli\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpy_2acqi3pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for databricks-cli\n  Running setup.py clean for databricks-cli\n  Running setup.py bdist_wheel for pyyaml ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/pyyaml\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmp4bs2fwrtpip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for pyyaml\n  Running setup.py clean for pyyaml\n  Running setup.py bdist_wheel for querystring-parser ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/querystring-parser\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmp_cnm9w_tpip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for querystring-parser\n  Running setup.py clean for querystring-parser\n  Running setup.py bdist_wheel for uuid ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/uuid\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpenr2igaxpip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for uuid\n  Running setup.py clean for uuid\n  Running setup.py bdist_wheel for zipstream ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/zipstream\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpnzsjh5e2pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for zipstream\n  Running setup.py clean for zipstream\n  Running setup.py bdist_wheel for itsdangerous ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/itsdangerous\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmp7imi3zv2pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for itsdangerous\n  Running setup.py clean for itsdangerous\n  Running setup.py bdist_wheel for configparser ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/configparser\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpyk9qtmi1pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for configparser\n  Running setup.py clean for configparser\n  Running setup.py bdist_wheel for tabulate ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/tabulate\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpjim2qr00pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for tabulate\n  Running setup.py clean for tabulate\n  Running setup.py bdist_wheel for MarkupSafe ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/MarkupSafe\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpsdpdd8ulpip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for MarkupSafe\n  Running setup.py clean for MarkupSafe\nFailed to build mlflow databricks-cli pyyaml querystring-parser uuid zipstream itsdangerous configparser tabulate MarkupSafe\nInstalling collected packages: click, itsdangerous, Werkzeug, MarkupSafe, Jinja2, Flask, pyasn1, rsa, jmespath, six, python-dateutil, docutils, botocore, s3transfer, colorama, pyyaml, awscli, boto3, configparser, chardet, idna, urllib3, certifi, requests, tabulate, databricks-cli, smmap2, gitdb2, gitpython, numpy, pytz, pandas, protobuf, pygal, querystring-parser, scikit-learn, scipy, uuid, zipstream, mlflow\n  Running setup.py install for itsdangerous ... done\n  Running setup.py install for MarkupSafe ... done\n  Running setup.py install for pyyaml ... done\n  Running setup.py install for configparser ... done\n  Running setup.py install for tabulate ... done\n  Running setup.py install for databricks-cli ... done\n  Running setup.py install for querystring-parser ... done\n  Running setup.py install for uuid ... done\n  Running setup.py install for zipstream ... done\n  Running setup.py install for mlflow ... done\nSuccessfully installed Flask-1.0.2 Jinja2-2.10 MarkupSafe-1.0 Werkzeug-0.14.1 awscli-1.15.46 boto3-1.7.46 botocore-1.10.46 certifi-2018.4.16 chardet-3.0.4 click-6.7 colorama-0.3.9 configparser-3.5.0 databricks-cli-0.7.2 docutils-0.14 gitdb2-2.0.3 gitpython-2.1.10 idna-2.7 itsdangerous-0.24 jmespath-0.9.3 mlflow-0.1.0 numpy-1.14.5 pandas-0.23.1 protobuf-3.6.0 pyasn1-0.4.3 pygal-2.4.0 python-dateutil-2.7.3 pytz-2018.4 pyyaml-4.1 querystring-parser-1.2.3 requests-2.19.1 rsa-3.4.2 s3transfer-0.1.13 scikit-learn-0.19.1 scipy-1.1.0 six-1.11.0 smmap2-2.0.3 tabulate-0.8.2 urllib3-1.23 uuid-1.30 zipstream-1.1.4\n<\/code><\/pre>\n\n<p>After that I checked the following didn't give any errors:<\/p>\n\n<pre><code>import os\nfrom mlflow import log_metric, log_param, log_artifact\n<\/code><\/pre>\n\n<p>But when I try to run the web-based user interface, I get the following errors:<\/p>\n\n<pre><code>$ mlflow ui\nTraceback (most recent call last):\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 574, in _build_master\n    ws.require(__requires__)\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 892, in require\n    needed = self.resolve(parse_requirements(requirements))\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 783, in resolve\n    raise VersionConflict(dist, req).with_context(dependent_req)\npkg_resources.ContextualVersionConflict: (PyYAML 4.1 (\/home\/emre\/mlflow\/lib\/python3.6\/site-packages), Requirement.parse('PyYAML&lt;=3.12,&gt;=3.10'), {'awscli'})\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/home\/emre\/mlflow\/bin\/mlflow\", line 6, in &lt;module&gt;\n    from pkg_resources import load_entry_point\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 3088, in &lt;module&gt;\n    @_call_aside\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 3072, in _call_aside\n    f(*args, **kwargs)\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 3101, in _initialize_master_working_set\n    working_set = WorkingSet._build_master()\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 576, in _build_master\n    return cls._build_from_requirements(__requires__)\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 589, in _build_from_requirements\n    dists = ws.resolve(reqs, Environment())\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 783, in resolve\n    raise VersionConflict(dist, req).with_context(dependent_req)\npkg_resources.ContextualVersionConflict: (PyYAML 4.1 (\/home\/emre\/mlflow\/lib\/python3.6\/site-packages), Requirement.parse('PyYAML&lt;=3.12,&gt;=3.10'), {'awscli'})\n<\/code><\/pre>\n\n<p>Any ideas how I can fix this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1530106954863,
        "Question_score":0,
        "Question_tags":"python|python-3.x|pip|mlflow",
        "Question_view_count":791,
        "Owner_creation_time":1261400320737,
        "Owner_last_access_time":1663928633650,
        "Owner_location":"Antwerp, Belgium",
        "Owner_reputation":7876,
        "Owner_up_votes":4051,
        "Owner_down_votes":47,
        "Owner_views":924,
        "Question_last_edit_time":1530112131487,
        "Answer_body":"<p>Apparently I had to install the <code>wheel<\/code> module inside my virtual environment. I deleted the virtual environment, re-created it, and then installed the <code>wheel<\/code> module:<\/p>\n\n<pre><code>pip install wheel\n<\/code><\/pre>\n\n<p>after that <code>pip install mlflow<\/code>, as well as <code>mlflow ui<\/code> worked successfully.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1530109158067,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51064366",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65145994,
        "Question_title":"Saving an Matlabplot as an MLFlow artifact",
        "Question_body":"<p>I am using DataBricks and Spark 7.4ML,<\/p>\n<p>The following code successfully logs the params and metrics, and I can see the ROCcurve.png in the MLFLOW gui (just the item in the tree below the model). But the actually plot is blank. Why?<\/p>\n<pre><code>with mlflow.start_run(run_name=&quot;logistic-regression&quot;) as run:\n  pipeModel = pipe.fit(trainDF)\n  mlflow.spark.log_model(pipeModel, &quot;model&quot;)\n  predTest = pipeModel.transform(testDF)\n  predTrain = pipeModel.transform(trainDF)\n  evaluator=BinaryClassificationEvaluator(labelCol=&quot;arrivedLate&quot;)\n  trainROC = evaluator.evaluate(predTrain)\n  testROC = evaluator.evaluate(predTest)\n  print(f&quot;Train ROC: {trainROC}&quot;)\n  print(f&quot;Test ROC: {testROC}&quot;)\n  mlflow.log_param(&quot;Dataset Name&quot;, &quot;Flights &quot; + datasetName)\n  mlflow.log_metric(key=&quot;Train ROC&quot;, value=trainROC)\n  mlflow.log_metric(key=&quot;Test ROC&quot;, value=testROC)\n\n  lrModel = pipeModel.stages[3]\n  trainingSummary = lrModel.summary\n  roc = trainingSummary.roc.toPandas()\n  plt.plot(roc['FPR'],roc['TPR'])\n  plt.ylabel('False Positive Rate')\n  plt.xlabel('True Positive Rate')\n  plt.title('ROC Curve')\n  plt.show()\n  plt.savefig(&quot;ROCcurve.png&quot;)\n  mlflow.log_artifact(&quot;ROCcurve.png&quot;)\n  plt.close()\n  \n  display(predTest.select(stringCols + [&quot;arrivedLate&quot;, &quot;prediction&quot;]))\n<\/code><\/pre>\n<p>What the notebook shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What the MLFlow shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1607094596640,
        "Question_score":8,
        "Question_tags":"apache-spark|matplotlib|pyspark|databricks|mlflow",
        "Question_view_count":5219,
        "Owner_creation_time":1316705139197,
        "Owner_last_access_time":1663085821243,
        "Owner_location":"Boston, MA",
        "Owner_reputation":6711,
        "Owner_up_votes":353,
        "Owner_down_votes":3,
        "Owner_views":819,
        "Question_last_edit_time":1607191847983,
        "Answer_body":"<p>Put <code>plt.show()<\/code> after <code>plt.savefig()<\/code> - <code>plt.show()<\/code> will remove your plot because it is shown already.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1607094854147,
        "Answer_score":7.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65145994",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63515370,
        "Question_title":"How to add coefficients, p-values and relevant variable name in mlflow?",
        "Question_body":"<p>I am running a linear regression model and I would like to add the coefficients and P-values of each variable and the variable name in to the metrics of the mlflow output. I am new to using mlflow and not very familiar in doing this. Below is an example of part of my code<\/p>\n<pre><code>with mlflow.start_run(run_name=p_key + '_' + str(o_key)):\n    \n    lr = LinearRegression(\n      featuresCol = 'features',\n      labelCol = target_var,\n      maxIter = 10,\n      regParam = 0.0,\n      elasticNetParam = 0.0,\n      solver=&quot;normal&quot;\n        )\n    \n    lr_model_item = lr.fit(train_model_data)\n    lr_coefficients_item = lr_model_item.coefficients\n    lr_coefficients_intercept = lr_model_item.intercept\n    \n    lr_predictions_item = lr_model_item.transform(train_model_data)\n    lr_predictions_item_oos = lr_model_item.transform(test_model_data)\n    \n    rsquared = lr_model_item.summary.r2\n    \n    # Log mlflow attributes for mlflow UI\n    mlflow.log_metric(&quot;rsquared&quot;, rsquared)\n    mlflow.log_metric(&quot;intercept&quot;, lr_coefficients_intercept)\n    for i in lr_coefficients_item:\n      mlflow.log_metric('coefficients', lr_coefficients_item[i])\n<\/code><\/pre>\n<p>Would like to know whether this is possible? In the final output I should have the intercept, coefficients, p-values and the relevant variable name.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597974389493,
        "Question_score":1,
        "Question_tags":"databricks|mlflow",
        "Question_view_count":242,
        "Owner_creation_time":1557281764547,
        "Owner_last_access_time":1621398878877,
        "Owner_location":null,
        "Owner_reputation":129,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63515370",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73808627,
        "Question_title":"Can't get model inference using mlflow.pytorch.log_model, but could get it with mlflow.pyfunc.log_model",
        "Question_body":"<p>I've used <code>mlflow.pyfunc.log_model<\/code> and I was able to get model inference with this, but not with<code>mlflow.pytorch.log_model<\/code>. The error was Verify that the serialized input Dataframe is compatible with the model for inference.<\/p>\n<pre><code>    data = torch.randn(10, 3, 224, 224)  # shape: [bs, channel, size, size]\n    model_input = {\n                &quot;inputs&quot;: { \n                    &quot;x&quot;: data.tolist() }\n                }\n    request = json.dumps(model_input)\n    headers = {&quot;content-type&quot;: &quot;application\/json&quot;}\n    response = requests.post(URL, data=request, headers=headers) # to mlflow\n    response = response.json() \n    print(response)\n<\/code><\/pre>\n<p>The very same input to the model, but I could get inference on one but not the other? Am I missing something here? I would like to use <code>mlflow.pytorch.log_model<\/code> so I don't have to do a model wrapper for generalisation with <code>mlflow.pyfunc.log_model<\/code>.<\/p>\n<p>Can anyone help me with this please.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663812765467,
        "Question_score":0,
        "Question_tags":"rest|deployment|pytorch|mlflow|serving",
        "Question_view_count":14,
        "Owner_creation_time":1487831504143,
        "Owner_last_access_time":1664005735500,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1663812910112,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73808627",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68722437,
        "Question_title":"Azure : Ansible role for deploying ML model integrated over databricks",
        "Question_body":"<p>I have developed ML predictive model on historical data in Azure Databricks using python notebook.\nWhich means i have done data extraction, preparation, feature engineering and model training everything done in Databricks using python notebook.\nI have almost completed development part of it, now we want to deploy ML model into production using ansible roles.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1628579607800,
        "Question_score":1,
        "Question_tags":"deployment|databricks|mlflow|mlmodel",
        "Question_view_count":128,
        "Owner_creation_time":1605508510493,
        "Owner_last_access_time":1638163317803,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1628663676903,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68722437",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68887243,
        "Question_title":"Autolog metrics mlflow",
        "Question_body":"<p>I have a question about autologging. I use it and I want to record another metrics.\nCan I change the recorded metrics for autologging?<\/p>\n<p>I found a class in the documentation <code>_AutologgingMetricsManager<\/code> but I don't know how can I use it.<\/p>\n<p>Thanks,\nIrina<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1629692108827,
        "Question_score":1,
        "Question_tags":"metrics|mlflow",
        "Question_view_count":54,
        "Owner_creation_time":1629691503070,
        "Owner_last_access_time":1657106037790,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1629701490176,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68887243",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72660569,
        "Question_title":"mlflow UI server doesn't start",
        "Question_body":"<p>When I run mlflow UI inside my repository and environment (anaconda), I receive the following error<\/p>\n<pre><code>Cannot open C:\\Users\\XXX\\Anaconda3\\envs\\haea\\Scripts\\waitress-serve-script.py\nRunning the mlflow server failed. Please see the logs above for details\n<\/code><\/pre>\n<p>When I checked the anaconda environment folder, I don't see a waitress-serve-script there, that might be the reason, but I can't find other online resources for the issue. Any recommendations would help.<\/p>\n<p><em>What I have tried so far<\/em><\/p>\n<ol>\n<li>Reinstalling mlflow<\/li>\n<li>Creating a new environment<\/li>\n<li>Manually install waitress (pip install waitress)<\/li>\n<\/ol>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1655474363667,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":158,
        "Owner_creation_time":1466863372793,
        "Owner_last_access_time":1658241993080,
        "Owner_location":"Michigan",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72660569",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71612603,
        "Question_title":"How does one invert an encoded prediction in Keras for model serving?",
        "Question_body":"<p>I have a Keras model in which i have successfully added a <code>StringLookUp<\/code> pre-processing step as part of the model definition. This is generally a good practice because i can then feed it the raw data to get back a prediction.<\/p>\n<p>I am feeding the model string words that are mapped to an integer. The Y values are also string words that have been mapped to an integer.<\/p>\n<p>Here is the implementation of the encoder and decoders:<\/p>\n<pre><code>#generate the encoder and decoders\nencoder = tf.keras.layers.StringLookup(vocabulary=vocab, )\ndecoder = tf.keras.layers.StringLookup(vocabulary=vocab, output_mode=&quot;int&quot;, invert=True)\n<\/code><\/pre>\n<p>Here is the some of the code that makes the inference model<\/p>\n<pre><code># For inference, you can export a model that accepts strings as input\ninputs = Input(shape=(6,), dtype=&quot;string&quot;)\nx = encoder(inputs)\noutputs = keras_model(x)\ninference_model = Model(inputs, outputs)\n\ninference_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  \ninference_model.summary()\n<\/code><\/pre>\n<p>The <code>encoder<\/code> above is just a function that implements <code>tf.keras.layers.StringLookup<\/code><\/p>\n<p>Now, inside the notebook, I can easily convert the predictions back to the Original String representations by using a <code>decoder<\/code> which implements the reverse of <code>StringLookUp<\/code>.<\/p>\n<p><em><strong>Here's my problem<\/strong><\/em>\nWhile this works fine inside the notebook, this isn't very practical for deploying the model as a REST API because the calling program has no way of knowing how the encoded integer maps back to the original string representation.<\/p>\n<p><em><strong>So the question is what strategy should I use to implement the keras predict so that it returns the original string which I can then serialize using mlflow &amp; cloudpickle to deploy it as a servable model in databricks<\/strong><\/em><\/p>\n<p>Any guidance would be very much appreciated. I've seen a lot of example of Keras, but none that show how to do enact this kind of behavior for model deployment.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1648186526013,
        "Question_score":0,
        "Question_tags":"tensorflow|keras|deep-learning|mlflow",
        "Question_view_count":176,
        "Owner_creation_time":1427492676943,
        "Owner_last_access_time":1663998407850,
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71612603",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68319208,
        "Question_title":"Unable to import mlflow, getting ModuleNotFoundError: No module named 'mlflow'",
        "Question_body":"<p>Unable to import <code>mlflow<\/code> in a .py script.<\/p>\n<pre><code>ModuleNotFoundError: No module named 'mlflow'\n<\/code><\/pre>\n<p>The script runs in a <code>python:3.7-stretch Docker<\/code> container<\/p>\n<p>Use <code>requirements.txt<\/code> to pip install packages.<\/p>\n<pre><code>(...)\nsqlalchemy==1.4.1\npsycopg2==2.8.6\nmlflow==1.18.0\n<\/code><\/pre>\n<pre><code>RUN pip3 install --default-timeout=5000 --use-deprecated=legacy-resolver -r \/root\/requirements.txt\n<\/code><\/pre>\n<p>Can see that it is installed.<\/p>\n<pre><code>root@abc:~# pip uninstall mlflow\nFound existing installation: mlflow 1.18.0\nUninstalling mlflow-1.18.0:\n  Would remove:\n    \/usr\/local\/bin\/mlflow\n    \/usr\/local\/lib\/python3.7\/site-packages\/mlflow-1.18.0.dist-info\/*\n    \/usr\/local\/lib\/python3.7\/site-packages\/mlflow\/*\nProceed (y\/n)? n\n<\/code><\/pre>\n<p>Can do an import from python shell.<\/p>\n<pre><code>root@abc:~# python\nPython 3.7.10 (default, Feb 16 2021, 19:46:13)\n[GCC 6.3.0 20170516] on linux\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; import mlflow\n&gt;&gt;&gt;\n<\/code><\/pre>\n<p>But no joy when running from .py script.<\/p>\n<p>Other packages installed from <code>requirements.txt<\/code> can be imported.<\/p>\n<p>Any ideas what is wrong ?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1625844282463,
        "Question_score":0,
        "Question_tags":"python-3.x|docker|machine-learning|python-module|mlflow",
        "Question_view_count":814,
        "Owner_creation_time":1461829551257,
        "Owner_last_access_time":1663945644783,
        "Owner_location":null,
        "Owner_reputation":375,
        "Owner_up_votes":241,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":1650287875787,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68319208",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63232368,
        "Question_title":"Storing mlflow artifacts to s3, while having SQL databse as backend",
        "Question_body":"<p>When using a SQL database as backend for <code>mlflow<\/code> are the artifacts stored in the same database or in default <code>.\/mlruns<\/code> directory?<\/p>\n<p>Is it possible to store them in different location as in AWS S3?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1596468560843,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":610,
        "Owner_creation_time":1596468107063,
        "Owner_last_access_time":1624957000150,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1596534279300,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63232368",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56851463,
        "Question_title":"How do I specify mlflow MLproject with zero parameters?",
        "Question_body":"<p>I tried to create MLproject with zero parameters as:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    parameters:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>when I get an error:<\/p>\n\n<pre><code>  Traceback (most recent call last):\n File \"\/home\/ubuntu\/.local\/bin\/mlflow\", line 11, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/cli.py\", line 137, in run\n    run_id=run_id,\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 230, in run\n    use_conda=use_conda, storage_dir=storage_dir, synchronous=synchronous, run_id=run_id)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 85, in _run\n    project = _project_spec.load_project(work_dir)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 40, in load_project\n    entry_points[name] = EntryPoint(name, parameters, command)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 87, in __init__\n    self.parameters = {k: Parameter(k, v) for (k, v) in parameters.items()}\nAttributeError: 'NoneType' object has no attribute 'items'\n<\/code><\/pre>\n\n<p>Am I missing something or mlflow does not allow project with  zero parameters?<\/p>\n\n<p>I have also posted this at my public repo of: <a href=\"https:\/\/github.com\/sameermahajan\/mlflow-try\" rel=\"nofollow noreferrer\">https:\/\/github.com\/sameermahajan\/mlflow-try<\/a> if someone would like to try out:<\/p>\n\n<pre><code>mlflow run https:\/\/github.com\/sameermahajan\/mlflow-try.git\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1562066976100,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":353,
        "Owner_creation_time":1384343462317,
        "Owner_last_access_time":1663916912330,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":478,
        "Owner_up_votes":65,
        "Owner_down_votes":4,
        "Owner_views":118,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For this, you completely drop the 'parameters' section as below:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>(I thought I had tried it earlier but I was trying too many different ways to may be miss out on this one)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1562240543612,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56851463",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59401800,
        "Question_title":"MLFlow run passing Google Application credentials",
        "Question_body":"<p>I want to pass my <code>GOOGLE_APPLICATION_CREDENTIALS<\/code> environmental variable when I run <code>mlflow run<\/code> using a Docker container:<\/p>\n\n<p>This is my current <code>docker run<\/code> when using mlflow run:<\/p>\n\n<pre><code> Running command 'docker run --rm -e MLFLOW_RUN_ID=f18667e37ecb486cac4631cbaf279903 -e MLFLOW_TRACKING_URI=http:\/\/3.1.1.11:5000 -e MLFLOW_EXPERIMENT_ID=0 mlflow_gcp:33156ee python -m trainer.task --job-dir \/tmp\/ \\\n    --num-epochs 10 \\\n    --train-steps 1000 \\\n    --eval-steps 1 \\\n    --train-files gs:\/\/cloud-samples-data\/ml-engine\/census\/data\/adult.data.csv \\\n    --eval-files gs:\/\/cloud-samples-data\/ml-engine\/census\/data\/adult.test.csv \\\n    --batch-size 128\n<\/code><\/pre>\n\n<p>This is how I would normally pass it:<\/p>\n\n<pre><code>docker run \\\n   -p 9090:${PORT} \\\n   -e PORT=${PORT} \\\n   -e GOOGLE_APPLICATION_CREDENTIALS=\/tmp\/keys\/[FILE_NAME].json\n<\/code><\/pre>\n\n<p>What is the best way to option to pass this value to mlflow? I'm writing files in GCS and Docker requires access to GCP.<\/p>\n\n<p>MLproject contents<\/p>\n\n<pre><code>name: mlflow_gcp\ndocker_env:\n  image: mlflow-gcp-example\nentry_points:\n  main:\n    parameters:\n      job_dir:\n        type: string\n        default: '\/tmp\/'\n      num_epochs:\n        type: int\n        default: 10\n      train_steps:\n        type: int\n        default: 1000\n      eval_steps:\n        type: int\n        default: 1\n      batch_size:\n        type: int\n        default: 64\n      train_files:\n        type: string\n        default: 'gs:\/\/cloud-samples-data\/ml-engine\/census\/data\/adult.data.csv'\n      eval_files:\n        type: string\n        default: 'gs:\/\/cloud-samples-data\/ml-engine\/census\/data\/adult.test.csv'\n      mlflow_tracking_uri:\n        type: uri\n        default: ''\n\n    command: |\n        python -m trainer.task --job-dir {job_dir} \\\n            --num-epochs {num_epochs} \\\n            --train-steps {train_steps} \\\n            --eval-steps {eval_steps} \\\n            --train-files {train_files} \\\n            --eval-files {eval_files} \\\n            --batch-size {batch_size} \\\n            --mlflow-tracking-uri {mlflow_tracking_uri}\n\n<\/code><\/pre>\n\n<p>I already tried in Python file and fails since Docker has no access to local file system:<\/p>\n\n<pre><code>import os\n    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"\/Users\/user\/key.json\"\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1576715118127,
        "Question_score":2,
        "Question_tags":"docker|mlflow",
        "Question_view_count":243,
        "Owner_creation_time":1264671735677,
        "Owner_last_access_time":1664082395287,
        "Owner_location":"San Francisco, CA",
        "Owner_reputation":8619,
        "Owner_up_votes":1916,
        "Owner_down_votes":102,
        "Owner_views":1286,
        "Question_last_edit_time":1576715456443,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59401800",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58306468,
        "Question_title":"Gridsearch on an experiment in Sacred",
        "Question_body":"<p>I'm trying to see some ways to store my ML experiments and I came across some python libraries like Sacred, ModelChimp, MLFlow, ....<\/p>\n\n<p>The one I like the most is Sacred, but I would like to know how to save the <code>GridSearchCV<\/code> sklearn object the way ModelChimp does, for example. Is there any way to include each of the tests that the <code>GridSearchCV<\/code> object does in Sacred like ModelChimp does?<\/p>\n\n<p>Additionally I would like to be able to visualize an interactive map of the folium library (which I would simply export to HTML), but I haven't seen that any of these libraries accept objects to visualize beyond an image.<\/p>\n\n<p>Are Sacred or ModelChimp good options? The little I've seen of MLflow or other libraries hasn't convinced me either but I'm open to suggestions. <a href=\"https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/bx0apm\/d_how_do_you_manage_your_machine_learning\/\" rel=\"nofollow noreferrer\">Here<\/a> are a few more alternatives. Which one do you use?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1570631306603,
        "Question_score":1,
        "Question_tags":"python|machine-learning|folium|mlflow|python-sacred",
        "Question_view_count":205,
        "Owner_creation_time":1550233102177,
        "Owner_last_access_time":1663835213537,
        "Owner_location":null,
        "Owner_reputation":621,
        "Owner_up_votes":87,
        "Owner_down_votes":26,
        "Owner_views":103,
        "Question_last_edit_time":1571068502089,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58306468",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73769728,
        "Question_title":"What is a 'XGBoostLabelEncoder' object?",
        "Question_body":"<p>I'm trying to load a model from an mlflow run. When I do that I get an 'XGBoostLabelEncoder' object, an object with no attributes like predict or predict_proba. I don't really know what you can do with it.<\/p>\n<p>I've googled around but can't find any information about what an 'XGBoostLabelEncoder' object is.<\/p>\n<p>Anybody who knows?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663571938730,
        "Question_score":0,
        "Question_tags":"python|scikit-learn|xgboost|mlflow",
        "Question_view_count":25,
        "Owner_creation_time":1503559541763,
        "Owner_last_access_time":1664083799790,
        "Owner_location":"Malm\u00f6, Sverige",
        "Owner_reputation":796,
        "Owner_up_votes":605,
        "Owner_down_votes":4,
        "Owner_views":136,
        "Question_last_edit_time":1663573301816,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73769728",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68622242,
        "Question_title":"Copying Experiments from a MLFlow server to another MLFlow server",
        "Question_body":"<p>I have a user in a Linux machine and I run a mlflow server from this user. Artifacts are stored in local mlruns folder. Lets call this user as user A. Then I run another mlflow server from another Linux user and call this user as user B. I wanted to move older experiments that resides in mlruns directory of user A to mlflow that run in user B. I simply moved mlruns directory of user A to the home directory of user B and run mlflow from there again. When I accessed to mlflow UI by browser I saw that artifact location is configured correctly to mlruns folder of user B, but I couldn't see the experiments that moved from user A's mlruns directory. How can I see them in the UI too?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1627910044223,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":303,
        "Owner_creation_time":1533205766067,
        "Owner_last_access_time":1663159397120,
        "Owner_location":"\u0130stanbul, T\u00fcrkiye",
        "Owner_reputation":275,
        "Owner_up_votes":167,
        "Owner_down_votes":2,
        "Owner_views":85,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68622242",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58666136,
        "Question_title":"How can I set run_name in mlflow command line?",
        "Question_body":"<p>MLFlow version: 1.4.0\nPython version: 3.7.4<\/p>\n\n<p>I'm running the UI as <code>mlflow server...<\/code> with all the required command line options. <\/p>\n\n<p>I am logging to MLFlow as an MLFlow project, with the appropriate <code>MLproject.yaml<\/code> file. The project is being run on a Docker container, so the CMD looks like this: <\/p>\n\n<p><code>mlflow run . -P document_ids=${D2V_DOC_IDS} -P corpus_path=...  --no-conda --experiment-name=${EXPERIMENT_NAME}<\/code><\/p>\n\n<p>Running the experiment like this results in a blank run_name. I know there's a run_id but I'd also like to see the run_name and set it in my code -- either in the command line, or in my code as <code>mlflow.log....<\/code>.  <\/p>\n\n<p>I've looked at <a href=\"https:\/\/stackoverflow.com\/questions\/57199472\/is-it-possible-to-set-change-mlflow-run-name-after-run-initial-creation\">Is it possible to set\/change mlflow run name after run initial creation?<\/a> but I want to programmatically set the run name instead of changing it manually on the UI.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1572643824847,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":2230,
        "Owner_creation_time":1364599762503,
        "Owner_last_access_time":1649458568280,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":48,
        "Question_last_edit_time":null,
        "Answer_body":"<p>One of the parameters to <code>mlflow.start_run()<\/code> is <code>run_name<\/code>.  This would give you programmatic access to set the run name with each iteration. See the docs <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.start_run\" rel=\"nofollow noreferrer\">here<\/a>. <\/p>\n\n<p>Here's an example:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from datetime import datetime\n\n## Define the name of our run\nname = \"this run is gonna be bananas\" + datetime.now()\n\n## Start a new mlflow run and set the run name\nwith mlflow.start_run(run_name = name):\n\n    ## ...train model, log metrics\/params\/model...\n\n    ## End the run\n    mlflow.end_run()\n<\/code><\/pre>\n\n<p>If you want to include set the name as part of an MLflow Project, you'll have to specify it as a parameter in the entry points to the project.  This is located in in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/projects.html#mlproject-file\" rel=\"nofollow noreferrer\">MLproject file<\/a>.  Then you can pass those values into the <code>mlflow.start_run()<\/code> function from the command line.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1573412741883,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58666136",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58198968,
        "Question_title":"mlflow not work after installation (Ubuntu 16, Centos 7)",
        "Question_body":"<p><img src=\"https:\/\/i.stack.imgur.com\/5DC76.png\" alt=\"enter link description here\"><\/p>\n\n<p>I try to install and run the web-based interface mlflow on VM Azure Ubuntu 16 and Centos 7.\nAfter running the command:\nsudo mlflow ui<\/p>\n\n<p>I can not get access url, either through the dns (mydomain.com:5000), or by IP: <a href=\"http:\/\/123.456.789.123:5000\/\" rel=\"nofollow noreferrer\">http:\/\/123.456.789.123:5000\/<\/a><\/p>\n\n<p>Executing on the server:<\/p>\n\n<p>wget <a href=\"http:\/\/localhost:5000\" rel=\"nofollow noreferrer\">http:\/\/localhost:5000<\/a><\/p>\n\n<p>I get the html-page mlflow, ie the server is running, but then why can not I connect to it in a browser? - Error:The connection has timed out<\/p>\n\n<p>p.s. Firewall disabled on this VM.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1570009106153,
        "Question_score":0,
        "Question_tags":"ubuntu|centos|gunicorn|mlflow",
        "Question_view_count":218,
        "Owner_creation_time":1554820347590,
        "Owner_last_access_time":1652262856667,
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1570173908487,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58198968",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73716706,
        "Question_title":"Using MLFlow for commercial use",
        "Question_body":"<p>It seems that from April 2020, we cannot use anaconda for &quot;commercial use&quot; meaning for example (organizations with more than 200 employees for example)<\/p>\n<p>Since MLFlow seems to use yaml files that contain allusions to conda, how is the situation with MLFlow?<\/p>\n<p>Can MLFlow be used for commercial use?<\/p>\n<p>Note: This question <em>is<\/em> about programming since I intend to use MLFlow in our programs and I have to decide if we can or not<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663157783360,
        "Question_score":0,
        "Question_tags":"anaconda|mlflow",
        "Question_view_count":21,
        "Owner_creation_time":1421198269333,
        "Owner_last_access_time":1664010554427,
        "Owner_location":null,
        "Owner_reputation":5585,
        "Owner_up_votes":792,
        "Owner_down_votes":53,
        "Owner_views":1350,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73716706",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59415473,
        "Question_title":"Why is the MLFlow UI different on installation?",
        "Question_body":"<p>My MLFlow installation results in a significantly different UI experience  that does not neatly stack the Parameters and Metrics columns as in the QuickStart. <\/p>\n\n<p>Here's what my UI looks like after logging some basic information: <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/L7xEe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/L7xEe.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Whereas every other example of MLFlow I've come across online looks like this (image taken from MLFlow website quickstart): \n<a href=\"https:\/\/i.stack.imgur.com\/N4d6d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/N4d6d.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The other thing that's missing is the toggle between \"list\" and \"table\" views. Below is what MLFlow documentation says I should see: \n<a href=\"https:\/\/i.stack.imgur.com\/K5VI9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/K5VI9.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Whereas here's what I see in my installation: \n<a href=\"https:\/\/i.stack.imgur.com\/bgFKt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bgFKt.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>My environments are as follows: <\/p>\n\n<ol>\n<li>Ubuntu 16.04, Docker + pip installation of MLFlow<\/li>\n<li>Mac OS, \n\n<ol>\n<li>Conda + pip installation of MLFlow<\/li>\n<li>Brew installation of Python, then pip3 installation of MLFlow<\/li>\n<\/ol><\/li>\n<\/ol>\n\n<p>I've tried tweaking the following: <\/p>\n\n<ol>\n<li>Version of MLFlow from 1.3 to 1.4<\/li>\n<li>Version of Python from 3.7 to 3.8 <\/li>\n<li>Brand new installation vs. existing upgrade <\/li>\n<\/ol>\n\n<p>I'm out of ideas at this point as to why my UI looks so different. It doesn't necessarily affect my usage of MLFlow, but I'm trying to sell it to my colleagues as a good experiment tracking system and I want the UI to be the best possible representation. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1576781811567,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":403,
        "Owner_creation_time":1364599762503,
        "Owner_last_access_time":1649458568280,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":48,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59415473",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60398594,
        "Question_title":"mlflow artifact storage to AWS s3 artifacts",
        "Question_body":"<p>Is there anyway to store the logs stored by mlflow to AWS S3? <\/p>\n\n<pre><code>mlflow server \\\n    --backend-store-uri \/mnt\/persistent-disk \\\n    --default-artifact-root s3:\/\/my-mlflow-bucket\/ \\\n    --host 0.0.0.0\n<\/code><\/pre>\n\n<p>Is it possible to only provide the default-artifact-root instead of providing both backend-store-uri and default-artifact-root? <\/p>\n\n<p>Also is there anyway to set default-artifact-root programatically from MlFlowClient or MlFlowContext instead of running mlflow server command line? <\/p>\n\n<p>FYI, I have already defined all AWS_ACCESS_KEY and AWS_SECRET_KEY in my environment variables, and exported ENDPOINTS to S3.<\/p>\n\n<p>Is logArtifacts from ActiveRun class a correct method to set the artifact_uri which points to AWS s3 bucket?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1582646236957,
        "Question_score":1,
        "Question_tags":"amazon-s3|mlflow",
        "Question_view_count":5057,
        "Owner_creation_time":1579635994630,
        "Owner_last_access_time":1583333598867,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1582655395240,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60398594",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68745216,
        "Question_title":"Curl returning single output for multiple string inputs",
        "Question_body":"<p>I logged a sentiment-analysis model in <code>Mlflow<\/code> with the custom signature, everything is working fine but as soon as I serve the model and hit it with the curl command, then for my multiple inputs it's returning a single output, please help if someone can point to the issue<\/p>\n<p>Curl command i am using :<\/p>\n<pre><code>curl http:\/\/127.0.0.1:2000\/invocations -H 'Content-Type: application\/json' -d '{&quot;columns&quot;: [&quot;text&quot;],&quot;data&quot;: [[&quot;Its a Bad day&quot;],[&quot;what are you&quot;]]}'\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>[&quot;negative&quot;]\n<\/code><\/pre>\n<p>Expected Output:<\/p>\n<pre><code>[&quot;negative&quot;,&quot;neutral&quot;]\n<\/code><\/pre>\n<p>Here is the model signature :<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/iNZHX.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/iNZHX.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I have tried two different models, both of them are giving the same issue and if I am trying a model which takes integer values then it's working as expected.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1628696944257,
        "Question_score":0,
        "Question_tags":"python|curl|mlflow",
        "Question_view_count":65,
        "Owner_creation_time":1594195651540,
        "Owner_last_access_time":1663919696387,
        "Owner_location":"India",
        "Owner_reputation":857,
        "Owner_up_votes":83,
        "Owner_down_votes":5,
        "Owner_views":35,
        "Question_last_edit_time":1628697805627,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68745216",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71795643,
        "Question_title":"Error when loading ML model from the remote MLflow instance",
        "Question_body":"<p>I tried to load a model from the remote MLflow instance, using <code>load_model<\/code> function:<\/p>\n<pre><code>import mlflow\n\nmodel = mlflow.pyfunc.load_model(&quot;http:\/\/remote_IP_address:5000\/runs:\/&lt;run_id&gt;\/model&quot;)\n<\/code><\/pre>\n<p>I found the run_id by using the REST API:<\/p>\n<pre><code>import requests\n\nrequests.get(&quot;http:\/\/remote_IP_address:5000\/api\/2.0\/preview\/mlflow\/runs\/search&quot;,params={&quot;experiment_ids&quot;:[0,1]})\n<\/code><\/pre>\n<p>But I am receiving an error:<\/p>\n<pre><code>ValueError: not enough values to unpack (expected 2, got 1)\n<\/code><\/pre>\n<p>I suppose the error is in the URI that I am using. Can you tell me the correct way to access the remote Mlflow instance and load the model?<\/p>\n<p>p.s.\nI also tried:<\/p>\n<pre><code>mlflow.pyfunc.load_model(&quot;http:\/\/remote_Ip_address:5000\/models:\/&lt;model_name&gt;\/production&quot;)\n<\/code><\/pre>\n<p>but I received the same error.<\/p>\n<p>Thank you in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649414682147,
        "Question_score":0,
        "Question_tags":"python|jupyter-notebook|mlflow",
        "Question_view_count":286,
        "Owner_creation_time":1528365488027,
        "Owner_last_access_time":1663858441063,
        "Owner_location":null,
        "Owner_reputation":499,
        "Owner_up_votes":167,
        "Owner_down_votes":1,
        "Owner_views":59,
        "Question_last_edit_time":1649415164790,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71795643",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72723433,
        "Question_title":"Does MLflow support darknet framework?",
        "Question_body":"<p>I am learning yolov4 with darknet and using that model for service development.<\/p>\n<p>However, I want to track and manage the performance metric of the model.<\/p>\n<p>So, I've heard of MLflow Tracking and am looking into it.<\/p>\n<p>Does MLflow support darknet?<\/p>\n<p>If so, is there a tracking management tool for using darknet?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1655945856957,
        "Question_score":0,
        "Question_tags":"yolo|mlflow|darknet",
        "Question_view_count":85,
        "Owner_creation_time":1546387948563,
        "Owner_last_access_time":1662079308337,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72723433",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72280328,
        "Question_title":"MLflow artifacts on S3 but not in UI",
        "Question_body":"<p>I'm running mlflow on my local machine and logging everything through a remote tracking server with my artifacts going to an S3 bucket.  I've confirmed that they are present in S3 after a run but when I look at the UI the artifacts section is completely blank.  There's no error, just empty space.  <a href=\"https:\/\/i.stack.imgur.com\/ZeHJ8.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZeHJ8.png\" alt=\"enter image description here\" \/><\/a>\nAny idea why this is?  I've included a picture from the UI.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1652819462347,
        "Question_score":1,
        "Question_tags":"amazon-s3|mlflow",
        "Question_view_count":502,
        "Owner_creation_time":1639614248310,
        "Owner_last_access_time":1663891212383,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1659109833416,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72280328",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59856641,
        "Question_title":"How can I throw an exception from within an MLflow project?",
        "Question_body":"<p>I have an Mlflow project that raises an exception. I execute that function using <code>mlflow.run<\/code>, but I get <code>mlflow.exceptions.ExecutionException(\"Run (ID '&lt;run_id&gt;') failed\")<\/code>. <\/p>\n\n<p>Is there any way I could get the exception that is being raised where I am executing <code>mlflow.run<\/code>? <\/p>\n\n<p>Or is it possible to send an <code>mlflow.exceptions.ExecutionException<\/code> with custom message set from within the project?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579686060327,
        "Question_score":0,
        "Question_tags":"python|exception|mlflow",
        "Question_view_count":428,
        "Owner_creation_time":1472932425400,
        "Owner_last_access_time":1623748857057,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Unfortunately not at the moment. mlflow run starts a new process and there is no protocol for exception passing right now. In general the other project does not even have to be in the same language. <\/p>\n\n<p>One workaround I can think of is to pass the exception via mlflow by setting run tag. E.g.:<\/p>\n\n<pre><code>try:\n    ...\nexcept Exception as ex:\n    mlflow.set_tag(\"exception\", str(ex))\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1579719419647,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1579728648287,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59856641",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69554275,
        "Question_title":"MLFLow: Install github-package dependency via pip and 1 building-job",
        "Question_body":"<p>I want to use MLFlow and I have to specify a Github python package as a pip dependency in the yaml-file.\nThe problem is, that I need to force pip to only use 1 job to build it (otherwise it would run out of memory).\nHow can I do this?<\/p>\n<p>I tried already: mlflow run hello_ml -n 1\nBut n is no option. Nether j (job).<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1634122665053,
        "Question_score":0,
        "Question_tags":"python|pip|mlflow",
        "Question_view_count":26,
        "Owner_creation_time":1582009662117,
        "Owner_last_access_time":1652282252807,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69554275",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70169519,
        "Question_title":"How can I save more metadata on an MLFlow model",
        "Question_body":"<p>I am trying to save a model to MLFlow, but as I have a custom prediction pipeline to retrieve data, I need to save extra metadata into the model.<\/p>\n<p>I tried using my custom signature class, which It does the job correctly and saves the model with the extra metadata in the MLModel file (YAML format). But when want to load the model from the MLFlow registry, the signature is not easy accesible.<\/p>\n<pre><code>mlflow.sklearn.log_model(model, &quot;model&quot;, signature = signature)\n<\/code><\/pre>\n<p>I've also tried to save an extra dictionary at the log_model function, but it saves it in the conda.yaml file:<\/p>\n<pre><code>mlflow.sklearn.log_model(model, &quot;model&quot;, {&quot;metadata1&quot;:&quot;value1&quot;, &quot;metadata2&quot;:&quot;value2&quot;})\n<\/code><\/pre>\n<p>Should I make my own flavour? Or my own Model inheritance? I've seen <a href=\"https:\/\/github1s.com\/mlflow\/mlflow\/blob\/HEAD\/mlflow\/pyfunc\/__init__.py\" rel=\"nofollow noreferrer\">here<\/a> that the PyFuncModel recieves some metadata class and an implementation to solve this, but I don't know where should I pass my own implementations to PyFuncModel on an experiment script. Here's a minimal example:<\/p>\n<pre><code>import mlflow\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\nmetadata_dic = {&quot;metadata1&quot;: &quot;value1&quot;, \n                &quot;metadata2&quot;: &quot;value2&quot;}\n\nX = np.array([[-2, -1, 0, 1, 2, 1],[-2, -1, 0, 1, 2, 1]]).T\ny = np.array([0, 0, 1, 1, 1, 0])\n\nX = pd.DataFrame(X, columns=[&quot;X1&quot;, &quot;X2&quot;])\ny = pd.DataFrame(y, columns=[&quot;y&quot;])\n\n\nmodel = LogisticRegression()\nmodel.fit(X, y)\n\nmlflow.sklearn.log_model(model, &quot;model&quot;)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1638276045390,
        "Question_score":3,
        "Question_tags":"python|scikit-learn|mlflow|mlops",
        "Question_view_count":323,
        "Owner_creation_time":1550233102177,
        "Owner_last_access_time":1663835213537,
        "Owner_location":null,
        "Owner_reputation":621,
        "Owner_up_votes":87,
        "Owner_down_votes":26,
        "Owner_views":103,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Finally, I made a class that contains every metadata and saved it as an model argument:<\/p>\n<pre><code>model = LogisticRegression()\nmodel.fit(X, y)\nmodel.metadata = ModelMetadata(**metadata_dic)\nmlflow.sklearn.log_model(model, &quot;model&quot;)\n<\/code><\/pre>\n<p>Here I lost the customizable <code>predict<\/code> process, but after reading the <code>MLFlow<\/code> documentation is not very clear how to proceed.<\/p>\n<p>If anyone finds a good approach It would be very appreciated.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1638361888372,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70169519",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56967364,
        "Question_title":"Keep track of all the parameters of spark-submit",
        "Question_body":"<p>I have a team where many member has permission to submit Spark tasks to YARN (the resource management) by command line. It's hard to track who is using how much cores, who is using how much memory...e.g. Now I'm looking for a software, framework or something could help me monitor the parameters that each member used. It will be a bridge between client and YARN. Then I could used it to filter the submit commands.<\/p>\n\n<p>I did take a look at <a href=\"http:\/\/www.mlflow.org\" rel=\"nofollow noreferrer\">mlflow<\/a> and I really like the MLFlow Tracking but it was designed for ML training process. I wonder if there is an alternative for my purpose? Or there is any other solution for the problem.<\/p>\n\n<p>Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1562750084187,
        "Question_score":0,
        "Question_tags":"apache-spark|parameters|hadoop-yarn|spark-submit|mlflow",
        "Question_view_count":93,
        "Owner_creation_time":1413431014113,
        "Owner_last_access_time":1566529894493,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>My recommendation would be to build such a tool yourself as its not too complicated,\nhave a wrapper script to spark submit which logs the usage in a DB and after the spark job finishes the wrapper will know to release information. could be done really easily.\nIn addition you can even block new spark submits if your team already asked for too much information.<\/p>\n\n<p>And as you build it your self its really flexible as you can even create \"sub teams\" or anything you want.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1562766864887,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56967364",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72416831,
        "Question_title":"mlflow: saving signature gives me warning",
        "Question_body":"<p>I am using mlflow with sqlite backend. started the server with:<\/p>\n<pre><code>mlflow server --backend-store-uri sqlite:\/\/\/mlruns_db\/mlruns.db --default-artifact-root $PWD\/mlruns --host 0.0.0.0 -p 5000\n<\/code><\/pre>\n<p>in the code, I log the model with signature as such<\/p>\n<pre><code>...\nsignature = infer_signature(X, y)\nmlflow.sklearn.log_model(model, model_name, signature=signature)\n...\n<\/code><\/pre>\n<p>then I get warnings<\/p>\n<blockquote>\n<p>2022\/05\/26 19:52:17 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under .\/mlruns\/1\/d4c8f611d3f24986a32d19c7d8b03f06\/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the tracking store. If logging to a mlflow server via REST, consider upgrading the server version to MLflow 1.7.0 or above.<\/p>\n<\/blockquote>\n<p>I am using <code>mlflow, version 1.24.0<\/code>, though.<\/p>\n<p>I see that the signature is correctly logged inside <code>MLmodel<\/code> file, but the nice rendering of mlflow ui is lost.<\/p>\n<ol>\n<li><p>with logging signature\n<a href=\"https:\/\/i.stack.imgur.com\/r2FwI.png\" rel=\"nofollow noreferrer\">mlflow ui with logging signature<\/a><\/p>\n<\/li>\n<li><p>without logging signature\n<a href=\"https:\/\/i.stack.imgur.com\/9nQ8w.png\" rel=\"nofollow noreferrer\">mlflow ui without logging signature<\/a><\/p>\n<\/li>\n<\/ol>\n<p>Does this have any consequence later when serving models with signature enforcement?\nAlso, I see many blog examples with postgres instead of sqlite, and sftp\/minio instead of filestore. maybe changing to those setups will solve this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1653750895023,
        "Question_score":1,
        "Question_tags":"postgresql|sqlite|metadata|mlflow",
        "Question_view_count":194,
        "Owner_creation_time":1653748227087,
        "Owner_last_access_time":1661359833540,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72416831",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65330061,
        "Question_title":"working directory changes to \/tmp\/ when python script runs with mlflow",
        "Question_body":"<p>I have a strange issue with python working directory when running with mlflow run -e build .\nThe script running successfully locally\/using IDE, but when running it with mlflow the problem is that the working directory changes to \/tmp folders instead of the correct working directory where the script resides (I have some path dependencies that certain folders should be present in .\/* so thats why my process fails.<\/p>\n<p>I had a feeling that something with the working directory messed up so I did os.getcwd() prints and saw the issue with temp folders.<\/p>\n<p>I had a similar project that I configured in a similar manner before and didn't have these issues.<\/p>\n<p>any idea what might be the issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1608147574247,
        "Question_score":0,
        "Question_tags":"python|path|mlflow",
        "Question_view_count":286,
        "Owner_creation_time":1511185810877,
        "Owner_last_access_time":1644828875333,
        "Owner_location":null,
        "Owner_reputation":174,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":62,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65330061",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71610688,
        "Question_title":"Is it possible to change the legend of the plot chart in mlflow metrics?",
        "Question_body":"<p>Thanks for the development of mlflow. I love it very much.<\/p>\n<p>I want to compare several runs with different hyper parameters, but I found that it is very difficult to differenciate these runs from the legend (some random numbers as the run ID) as shown in the screenshot.<\/p>\n<p>I hope the legend could be set to the hyper parameters in which these runs have different values. For instance, the legend could be set to different <code>patch size<\/code>, or different <code>learning rate<\/code>, etc.<\/p>\n<p>So is it possible for the current mlflow? If not, do you have the plan to develop this feature?<\/p>\n<p>This question is similar with this <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/5523\" rel=\"nofollow noreferrer\">issue<\/a>. But the issue proposed to use the customed <code>name<\/code> as the legend, while I think it is better to set it as the different hyperparemeters. Or it is best to let users to choose how to set the legend.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/0Wagl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0Wagl.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1648165795930,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":68,
        "Owner_creation_time":1516197597947,
        "Owner_last_access_time":1663970048630,
        "Owner_location":"Leiden, \u8377\u5170",
        "Owner_reputation":908,
        "Owner_up_votes":568,
        "Owner_down_votes":12,
        "Owner_views":151,
        "Question_last_edit_time":1648167115347,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71610688",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73015639,
        "Question_title":"Issues with deploying spark and mlflow to sagemaker",
        "Question_body":"<p>My goal is to deploy a spark\/mlflow to sagemaker with the following command:<\/p>\n<pre><code>    mlflow sagemaker deploy .. \n<\/code><\/pre>\n<p>I've successfully pushed a image to EC2 with<\/p>\n<pre><code>mlflow sagemaker build-and-push-container\n<\/code><\/pre>\n<p>I encounter errors when attempting to run mlflow sagemaker deploy:<\/p>\n<pre><code>[error] 446#446: *69 upstream timed out (110: Connection timed out) while reading response header from upstream, client: 10.32.0.2, server: , request: &quot;GET \/ping HTTP\/1.1&quot;, upstream: &quot;http:\/\/127.0.0.1:8000\/ping&quot;, host: &quot;model.aws.local:8080&quot;\njava.io.IOException: Failed to connect to model.aws.local\/172.17.0.2:34473\n<\/code><\/pre>\n<p>Therefore, I added the following as I thought I was mishandling pyspark in sagemaker:<\/p>\n<pre><code>classpath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars()) \nspark = SparkSession.builder.config( &quot;spark.driver.extraClassPath&quot;, classpath ).appName('audit-risk-predictor-training').getOrCreate()          \n<\/code><\/pre>\n<p>However this outputted the following error:<\/p>\n<pre><code>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org\/apache\/commons\/configuration\/Configuration\n    at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;init&gt;(DefaultMetricsSystem.java:38)\n    at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;clinit&gt;(DefaultMetricsSystem.java:36)\n    at org.apache.hadoop.security.UserGroupInformation$UgiMetrics.create(UserGroupInformation.java:134)\n    at org.apache.hadoop.security.UserGroupInformation.&lt;clinit&gt;(UserGroupInformation.java:254)\n    at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)\n    at scala.Option.getOrElse(Option.scala:189)\n    at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)\n    at org.apache.spark.SecurityManager.&lt;init&gt;(SecurityManager.scala:79)\n    at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)\n    at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)\n    at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)\n    at scala.Option.map(Option.scala:230)\n    at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)\n    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)\n    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)\n    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)\n    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\nCaused by: java.lang.ClassNotFoundException: org.apache.commons.configuration.Configuration\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n    ... 20 more\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\nInput In [7], in &lt;cell line: 3&gt;()\n      1 # Create Spark Session\n      2 classpath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars()) \n----&gt; 3 spark = SparkSession.builder.config( &quot;spark.driver.extraClassPath&quot;, classpath ).appName('audit-risk-predictor-training').getOrCreate()\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/sql\/session.py:228, in SparkSession.Builder.getOrCreate(self)\n    226         sparkConf.set(key, value)\n    227     # This SparkContext may be an existing one.\n--&gt; 228     sc = SparkContext.getOrCreate(sparkConf)\n    229 # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n    230 # by all sessions.\n    231 session = SparkSession(sc)\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/context.py:384, in SparkContext.getOrCreate(cls, conf)\n    382 with SparkContext._lock:\n    383     if SparkContext._active_spark_context is None:\n--&gt; 384         SparkContext(conf=conf or SparkConf())\n    385     return SparkContext._active_spark_context\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/context.py:144, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\n    139 if gateway is not None and gateway.gateway_parameters.auth_token is None:\n    140     raise ValueError(\n    141         &quot;You are trying to pass an insecure Py4j gateway to Spark. This&quot;\n    142         &quot; is not allowed as it is a security risk.&quot;)\n--&gt; 144 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    145 try:\n    146     self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n    147                   conf, jsc, profiler_cls)\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/context.py:331, in SparkContext._ensure_initialized(cls, instance, gateway, conf)\n    329 with SparkContext._lock:\n    330     if not SparkContext._gateway:\n--&gt; 331         SparkContext._gateway = gateway or launch_gateway(conf)\n    332         SparkContext._jvm = SparkContext._gateway.jvm\n    334     if instance:\n\nFile ~\/.local\/lib\/python3.8\/site-packages\/pyspark\/java_gateway.py:108, in launch_gateway(conf, popen_kwargs)\n    105     time.sleep(0.1)\n    107 if not os.path.isfile(conn_info_file):\n--&gt; 108     raise Exception(&quot;Java gateway process exited before sending its port number&quot;)\n    110 with open(conn_info_file, &quot;rb&quot;) as info:\n    111     gateway_port = read_int(info)\n\nException: Java gateway process exited before sending its port number\n<\/code><\/pre>\n<p>Any insight on where I'm going wrong? Is spark capable of running in sagemaker?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1658095201223,
        "Question_score":0,
        "Question_tags":"python|apache-spark|deployment|amazon-sagemaker|mlflow",
        "Question_view_count":59,
        "Owner_creation_time":1642068779037,
        "Owner_last_access_time":1663949787333,
        "Owner_location":null,
        "Owner_reputation":45,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73015639",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73153142,
        "Question_title":"Mlflow UI can't show artifacts",
        "Question_body":"<p>I have mlflow running on an azure VM and connected to Azure Blob as the artifact storage.<\/p>\n<p>After uploading artifacts to the storage from the Client.<\/p>\n<p>I tried the MLflow UI and successfully was able to show the uploaded file.<\/p>\n<p>The problem happens when I try to run MLFLOW with Docker, I get the error:\n<strong>Unable to list artifacts stored under <code>{artifactUri}<\/code> for the current run. Please contact your tracking server administrator to notify them of this error, which can happen when the tracking server lacks permission to list artifacts under the current run's root artifact directory<\/strong><\/p>\n<p>Dockerfile:<\/p>\n<pre><code>FROM python:3.7-slim-buster\n# Install python packages\nRUN pip install mlflow pymysql\n\nRUN pip install azure-storage-blob\n\nENV AZURE_STORAGE_ACCESS_KEY=&quot;#########&quot;\nENV AZURE_STORAGE_CONNECTION_STRING=&quot;#######&quot;\n<\/code><\/pre>\n<p>docker-compose.yml<\/p>\n<pre><code>web:\n        restart: always\n        build: .\/mlflow_server\n        image: mlflow_server\n        container_name: mlflow_server\n        expose:\n            - &quot;5000&quot;\n        networks:\n            - frontend\n            - backend\n        environment:\n            - AZURE_STORAGE_ACCESS_KEY=&quot;#####&quot;\n            - AZURE_STORAGE_CONNECTION_STRING=&quot;#####&quot;\n        command: mlflow server --backend-store-uri mysql+pymysql:\/\/mlflow_user:123456@db:3306\/mlflow --default-artifact-root wasbs:\/\/etc..\n<\/code><\/pre>\n<p>I tried multiple solutions:<\/p>\n<ol>\n<li>Making sure that boto3 is installed (Didn't do anything)<\/li>\n<li>Adding Environment Variables in the Dockerfile so the command runs after they're set<\/li>\n<li>I double checked the url of the storage blob<\/li>\n<\/ol>\n<p>And MLFLOW doesn't show any logs it just kills the process and restarts again.<\/p>\n<p>Anyone got any idea what might be the solution or how can i access the logs<\/p>\n<p>here're the docker logs of the container:<\/p>\n<pre><code>[2022-07-28 12:23:33 +0000] [10] [INFO] Starting gunicorn 20.1.0\n[2022-07-28 12:23:33 +0000] [10] [INFO] Listening at: http:\/\/0.0.0.0:5000 (10)\n[2022-07-28 12:23:33 +0000] [10] [INFO] Using worker: sync\n[2022-07-28 12:23:33 +0000] [13] [INFO] Booting worker with pid: 13\n[2022-07-28 12:23:33 +0000] [14] [INFO] Booting worker with pid: 14\n[2022-07-28 12:23:33 +0000] [15] [INFO] Booting worker with pid: 15\n[2022-07-28 12:23:33 +0000] [16] [INFO] Booting worker with pid: 16\n[2022-07-28 12:24:24 +0000] [10] [CRITICAL] WORKER TIMEOUT (pid:14)\n[2022-07-28 12:24:24 +0000] [14] [INFO] Worker exiting (pid: 14)\n[2022-07-28 12:24:24 +0000] [21] [INFO] Booting worker with pid: 21\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1659012211500,
        "Question_score":0,
        "Question_tags":"docker|docker-compose|azure-blob-storage|mlflow",
        "Question_view_count":97,
        "Owner_creation_time":1659010615837,
        "Owner_last_access_time":1663625482013,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73153142",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71950167,
        "Question_title":"What is the use of git commits in mlflow?",
        "Question_body":"<p>Why mlflow tracks git commits, we already have run_id for tracking experiment. Can we use those commits to go back to previous commit like we do in git.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1650524426890,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":161,
        "Owner_creation_time":1608194065657,
        "Owner_last_access_time":1662033766403,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71950167",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56984854,
        "Question_title":"fcntl error with \u201cmlflow ui\u201d on windows - mlflow 1.0",
        "Question_body":"<p>I am getting the following error message when trying mlflow examples and running 'mlflow ui'.<\/p>\n\n<p><strong>Error:<\/strong><\/p>\n\n<blockquote>\n  <p>ModuleNotFoundError: No module named 'fcntl' Running the mlflow server\n  failed. Please see the logs above for details<\/p>\n<\/blockquote>\n\n<p>Is anyone aware of a solution to this issue?<\/p>\n\n<p>I have tried the solutions suggested at <a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/1080\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/pull\/1080<\/a><\/p>\n\n<p>without success. Replacing the modified files in mlflow source code, it raises other issues for not finding what it is looking for with the following:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\thesis_mlflow\\Scripts\\mlflow.exe\\__main__.py\", line 9, in &lt;module&gt;\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\mlflow\\cli.py\", line 198, in ui\n    _run_server(backend_store_uri, default_artifact_root, \"127.0.0.1\", port, None, 1)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\mlflow\\server\\__init__.py\", line 90, in _run_server\n    exec_cmd(full_command, env=env_map, stream_output=True)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\mlflow\\utils\\process.py\", line 34, in exec_cmd\n    stdin=subprocess.PIPE, **kwargs)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\subprocess.py\", line 729, in __init__\n    restore_signals, start_new_session)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\subprocess.py\", line 1017, in _execute_child\n    startupinfo)\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1562833873410,
        "Question_score":1,
        "Question_tags":"python|windows|mlflow",
        "Question_view_count":725,
        "Owner_creation_time":1529408888483,
        "Owner_last_access_time":1649930832187,
        "Owner_location":"London, UK",
        "Owner_reputation":79,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1562909693510,
        "Answer_body":"<p>Just solved the issue: for some reason, waitress was not installed in the running environment. After installing it, everything seems working fine with the solution #1080 linked above in the question.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1562919127687,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56984854",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61975133,
        "Question_title":"Run mlflow project on multiple remote servers?",
        "Question_body":"<p>Can <code>MLflow<\/code> be used to dispatch <strong>projects<\/strong> to multiple remote servers?(not aws,azure etc.) from a local tracking server?<br>\nI have the following scenario-<Br>\nMultiple servers, where I would like to dispatch the <code>mlflow<\/code> project to all with different parameters, and let them \"report\" back to the current <strong>tracking server:<\/strong><\/p>\n\n<pre><code>for ip in servers_ips:\n    start_remote_mlflow(entry_point=GITHUBPATH,tracking_server=this_server_ip,hparams)\n<\/code><\/pre>\n\n<p>I see one can dispatch <code>mlflow<\/code> projects to aws or azure by specifying the ip or the remote machine. Can it be done with desktops as well?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1590251060187,
        "Question_score":2,
        "Question_tags":"python|remote-server|mlflow",
        "Question_view_count":221,
        "Owner_creation_time":1476768953033,
        "Owner_last_access_time":1664082143130,
        "Owner_location":"Israel",
        "Owner_reputation":2057,
        "Owner_up_votes":201,
        "Owner_down_votes":2,
        "Owner_views":269,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61975133",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70099987,
        "Question_title":"Setting Mlflow region in Python script",
        "Question_body":"<p>Hi all I deployed my minio on my localhost using Kuberenetes @ url: mlflow-minio.local<\/p>\n<p>I also deployed my mlflow server <a href=\"http:\/\/mlflow-server.local\" rel=\"nofollow noreferrer\">http:\/\/mlflow-server.local<\/a> and set the parameters as below in my python script<\/p>\n<pre><code>def savemlflow(lm, output, test_size, random_state, coeff_df):\n    mlflow.set_tracking_uri('http:\/\/mlflow-server.local')\n    os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http:\/\/mlflow-minio.local\/'\n    os.environ['AWS_ACCESS_KEY_ID'] = 'minio'\n    os.environ['AWS_SECRET_ACCESS_KEY'] = 'minio'\n    with mlflow.start_run():\n        mlflow.sklearn.log_model(lm, &quot;model&quot;)\n        mlflow.log_metric(&quot;MAE&quot;, metrics.mean_absolute_error(output[&quot;ACTUAL_SPEND&quot;], output[&quot;PREDICTED_SPEND&quot;]))\n        mlflow.log_metric(&quot;MSE&quot;, metrics.mean_squared_error(output[&quot;ACTUAL_SPEND&quot;], output[&quot;PREDICTED_SPEND&quot;]))\n        mlflow.log_metric(&quot;RMSE&quot;, np.sqrt(metrics.mean_squared_error(output[&quot;ACTUAL_SPEND&quot;], output[&quot;PREDICTED_SPEND&quot;])))\n<\/code><\/pre>\n<p>However, I keep getting a <code>boto3.exceptions.S3UploadFailedError: Failed to upload \/var\/folders\/xw\/6jppt2490z30qk9pz1fxm2c80000gp\/T\/tmpjen_w4gh\/model\/requirements.txt to mlflow\/0\/adf90c9806e64f64aaf14c4513a00dbf\/artifacts\/model\/requirements.txt: An error occurred (InvalidRegion) when calling the PutObject operation: Region does not match.<\/code><\/p>\n<p>Im guessing I need to set the region somehow for my minio in my python. But how do i do know what is the current region so I can be able to set it?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1637772817887,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":62,
        "Owner_creation_time":1381186868793,
        "Owner_last_access_time":1664071153520,
        "Owner_location":null,
        "Owner_reputation":1069,
        "Owner_up_votes":43,
        "Owner_down_votes":0,
        "Owner_views":178,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70099987",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62841162,
        "Question_title":"How can I password protect my mlflow portal",
        "Question_body":"<p>I have installed my mlflow on centos7 and hosting it at a port 5000.\nI followed this article for reference: <a href=\"https:\/\/medium.com\/analytics-vidhya\/setup-mlflow-in-production-d72aecde7fef\" rel=\"nofollow noreferrer\">Install MLFlow with postgres<\/a><\/p>\n<p>I am looking to secure my mlflow UI with username and password. Any authentication method should be fine, however, Single Sign On is preferred.<\/p>\n<p>I looked at this article: <a href=\"https:\/\/karimlahrichi.com\/2020\/03\/13\/add-authentication-to-mlflow\/\" rel=\"nofollow noreferrer\">Add Authentication to MLFlow<\/a> It allows me to secure all the traffic going from port 80. After successful authentication I will be redirected to port 5000 where my MLFlow application is running. However, if I directly go to host:5000 my mlflow doesn't ask me for any authentication.\nPlease help me understand how I can enable mandatory authentication before you can reach the mlflow dashboard.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1594409600090,
        "Question_score":1,
        "Question_tags":"python|authentication|nginx|centos7|mlflow",
        "Question_view_count":1682,
        "Owner_creation_time":1490818955347,
        "Owner_last_access_time":1619802281030,
        "Owner_location":null,
        "Owner_reputation":175,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":48,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62841162",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70968664,
        "Question_title":"How to run data bricck notebook with mlflow in azure data factory pipeline?",
        "Question_body":"<p>My colleagues and I are facing an issue when trying to run my databricks notebook in Azure Data Factory and the error is coming from MLFlow.<\/p>\n<p>The command that is failing is the following:<\/p>\n<pre><code># Take the parent notebook path to use as path for the experiment\ncontext = json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())\nnb_base_path = context['extraContext']['notebook_path'][:-len(&quot;00_training_and_validation&quot;)]\n\nexperiment_path = nb_base_path + 'trainings'\nmlflow.set_experiment(experiment_path)\nexperiment = mlflow.get_experiment_by_name(experiment_path)\nexperiment_id = experiment.experiment_id\n\nrun = mlflow.start_run(experiment_id=experiment_id, run_name=f&quot;run_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}&quot;)\n<\/code><\/pre>\n<p>And the error that is throwing is:<\/p>\n<p>An exception was thrown from a UDF: 'mlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: No experiment ID was specified. An experiment ID must be specified in Databricks Jobs and when logging to the MLflow server from outside the Databricks workspace. If using the Python fluent API, you can set an active experiment under which to create runs by calling mlflow.set_experiment(&quot;\/path\/to\/experiment\/in\/workspace&quot;) at the start of your program.', from , line 32.<\/p>\n<p>The pipeline just runs the notebook from ADF, it does not have any other step and the cluster we are using is type 7.3 ML.<\/p>\n<p>Could you please help us?<\/p>\n<p>Thank you in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643880182260,
        "Question_score":0,
        "Question_tags":"azure-data-factory|databricks|mlflow",
        "Question_view_count":392,
        "Owner_creation_time":1586330708360,
        "Owner_last_access_time":1643881412287,
        "Owner_location":"Madrid, Spain",
        "Owner_reputation":35,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70968664",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69175969,
        "Question_title":"Data bricks:- Cannot display the predicted output by using ml flow registered model",
        "Question_body":"<p>I have created a model using diabetes dataset for prediction. I have trained, evaluated, logged and registered it as a new model in ML flow. Now I am trying to load the registered model and trying to predict on new data. All though I was able to predict the results. I am not able to display it. When I try to display using command <code>.show()<\/code> or <code>display()<\/code> it is throwing an error. What is the cause of the error? and How do I display the results?<\/p>\n<p>Note: I have programmed using pure pyspark and all the ML flow operation was done on Data bricks<\/p>\n<p>Code:-<\/p>\n<pre><code>model_details = mlflow.tracking.MlflowClient().get_latest_versions('model1',stages=['staging'])[0]\nmodel = mlflow.pyfunc.spark_udf(spark,model_details.source)\ninput_df = sdf.drop('progression')\ncolumns = list(map(lambda c: f&quot;{c}&quot;, input_df.columns))\ndf = input_df.withColumn(&quot;progression&quot;, model(*columns))\ndf.show(truncate=False)\n<\/code><\/pre>\n<p>Error :-<\/p>\n<pre><code>PythonException: An exception was thrown from a UDF: 'Exception: Java gateway process exited before sending its port number'. Full traceback below:\nPythonException                           Traceback (most recent call last)\n&lt;command-1343735193245452&gt; in &lt;module&gt;\n     34 df = input_df.withColumn(&quot;progression&quot;, model(*columns))\n     35 \n---&gt; 36 df.show(truncate=False)\n\n\/databricks\/spark\/python\/pyspark\/sql\/dataframe.py in show(self, n, truncate, vertical)\n    441             print(self._jdf.showString(n, 20, vertical))\n    442         else:\n--&gt; 443             print(self._jdf.showString(n, int(truncate), vertical))\n    444 \n    445     def __repr__(self):\n\n\/databricks\/spark\/python\/lib\/py4j-0.10.9-src.zip\/py4j\/java_gateway.py in __call__(self, *args)\n   1303         answer = self.gateway_client.send_command(command)\n   1304         return_value = get_return_value(\n-&gt; 1305             answer, self.gateway_client, self.target_id, self.name)\n   1306 \n   1307         for temp_arg in temp_args:\n\n\/databricks\/spark\/python\/pyspark\/sql\/utils.py in deco(*a, **kw)\n    131                 # Hide where the exception came from that shows a non-Pythonic\n    132                 # JVM exception message.\n--&gt; 133                 raise_from(converted)\n    134             else:\n    135                 raise\n\n\/databricks\/spark\/python\/pyspark\/sql\/utils.py in raise_from(e)\n\nPythonException: An exception was thrown from a UDF: 'Exception: Java gateway process exited before sending its port number'. Full traceback below:\nTraceback (most recent call last):\n  File &quot;\/databricks\/spark\/python\/pyspark\/worker.py&quot;, line 654, in main\n    process()\n  File &quot;\/databricks\/spark\/python\/pyspark\/worker.py&quot;, line 646, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &quot;\/databricks\/spark\/python\/pyspark\/sql\/pandas\/serializers.py&quot;, line 281, in dump_stream\n    timely_flush_timeout_ms=self.timely_flush_timeout_ms)\n  File &quot;\/databricks\/spark\/python\/pyspark\/sql\/pandas\/serializers.py&quot;, line 97, in dump_stream\n    for batch in iterator:\n  File &quot;\/databricks\/spark\/python\/pyspark\/sql\/pandas\/serializers.py&quot;, line 271, in init_stream_yield_batches\n    for series in iterator:\n  File &quot;\/databricks\/spark\/python\/pyspark\/worker.py&quot;, line 467, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File &quot;\/databricks\/spark\/python\/pyspark\/worker.py&quot;, line 467, in &lt;genexpr&gt;\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File &quot;\/databricks\/spark\/python\/pyspark\/worker.py&quot;, line 111, in &lt;lambda&gt;\n    verify_result_type(f(*a)), len(a[0])), arrow_return_type)\n  File &quot;\/databricks\/spark\/python\/pyspark\/util.py&quot;, line 109, in wrapper\n    return f(*args, **kwargs)\n  File &quot;\/databricks\/python\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 827, in predict\n    model = SparkModelCache.get_or_load(archive_path)\n  File &quot;\/databricks\/python\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/spark_model_cache.py&quot;, line 64, in get_or_load\n    SparkModelCache._models[archive_path] = load_pyfunc(temp_dir)\n  File &quot;\/databricks\/python\/lib\/python3.7\/site-packages\/mlflow\/utils\/annotations.py&quot;, line 43, in deprecated_func\n    return func(*args, **kwargs)\n  File &quot;\/databricks\/python\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 693, in load_pyfunc\n    return load_model(model_uri, suppress_warnings)\n  File &quot;\/databricks\/python\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 667, in load_model\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/databricks\/python\/lib\/python3.7\/site-packages\/mlflow\/spark.py&quot;, line 707, in _load_pyfunc\n    .master(&quot;local[1]&quot;)\n  File &quot;\/databricks\/spark\/python\/pyspark\/sql\/session.py&quot;, line 189, in getOrCreate\n    sc = SparkContext.getOrCreate(sparkConf)\n  File &quot;\/databricks\/spark\/python\/pyspark\/context.py&quot;, line 384, in getOrCreate\n    SparkContext(conf=conf or SparkConf())\n  File &quot;\/databricks\/spark\/python\/pyspark\/context.py&quot;, line 134, in __init__\n    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n  File &quot;\/databricks\/spark\/python\/pyspark\/context.py&quot;, line 333, in _ensure_initialized\n    SparkContext._gateway = gateway or launch_gateway(conf)\n  File &quot;\/databricks\/spark\/python\/pyspark\/java_gateway.py&quot;, line 105, in launch_gateway\n    raise Exception(&quot;Java gateway process exited before sending its port number&quot;)\nException: Java gateway process exited before sending its port number\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1631614691113,
        "Question_score":1,
        "Question_tags":"pyspark|apache-spark-sql|user-defined-functions|databricks|mlflow",
        "Question_view_count":168,
        "Owner_creation_time":1628599797537,
        "Owner_last_access_time":1637600430000,
        "Owner_location":null,
        "Owner_reputation":111,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69175969",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70539698,
        "Question_title":"MlFlow - Unable to run with S3 as default-artifact-root",
        "Question_body":"<p>I am trying to store my model artifacts using mlflow to s3. In the API services, we use <code>MLFLOW_S3_ENDPOINT_URL<\/code> as the s3 bucket. In the mlflow service, we pass it as an environment variable. But, the mlflow container servicer fails with the below exception:<\/p>\n<pre><code>mflow_server  | botocore.exceptions.HTTPClientError: An HTTP Client raised an unhandled exception: Not supported URL scheme s3\n<\/code><\/pre>\n<p>docker-compose file as below:<\/p>\n<pre><code>version: &quot;3.3&quot;\nservices:\n  prisim-api:\n    image: prisim-api:latest\n    container_name: prisim-api\n    expose:\n      - &quot;8000&quot;\n    environment: \n    - S3_URL=s3:\/\/mlflow-automation-artifacts\/\n    - MLFLOW_SERVER=http:\/\/mlflow:5000\n    - AWS_ID=xyz+\n    - AWS_KEY=xyz\n\n    networks:\n      - prisim \n    depends_on:\n      - mlflow\n    links:\n            - mlflow\n    volumes:\n      - app_data:\/usr\/data\n  mlflow:\n    image: mlflow_server:latest\n    container_name: mflow_server\n    ports:\n      - &quot;5000:5000&quot;    \n    environment:\n      - AWS_ACCESS_KEY_ID=xyz+\n      - AWS_SECRET_ACCESS_KEY=xyz\n      - MLFLOW_S3_ENDPOINT_URL=s3:\/\/mlflow-automation-artifacts\/\n    healthcheck:\n      test: [&quot;CMD&quot;, &quot;echo&quot;, &quot;mlflow server is running&quot;]\n      interval: 1m30s\n      timeout: 10s\n      retries: 3\n    networks:\n       - prisim \nnetworks:\n prisim:\nvolumes:\n  app_data:\n<\/code><\/pre>\n<p>Why the scheme s3 is not supported?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1640933189943,
        "Question_score":1,
        "Question_tags":"amazon-s3|docker-compose|mlflow",
        "Question_view_count":932,
        "Owner_creation_time":1310893185210,
        "Owner_last_access_time":1663988189020,
        "Owner_location":"Thiruvananthapuram, Kerala, India",
        "Owner_reputation":2763,
        "Owner_up_votes":373,
        "Owner_down_votes":7,
        "Owner_views":851,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I found the solution.<\/p>\n<p>I have added <code>[&quot;AWS_DEFAULT_REGION&quot;]<\/code> to the environment variables and it worked.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1641275216080,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70539698",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73040570,
        "Question_title":"How to share models in a multitenant enviroment with Mlflow?",
        "Question_body":"<p>The company I work for are using Databricks with Azure as a storage service. My group is trying to create a centralized model registry that allows us to push and pull models into different instances of Databricks. We are aware that we can share models within the same subscription (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/machine-learning\/manage-model-lifecycle\/multiple-workspaces\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/machine-learning\/manage-model-lifecycle\/multiple-workspaces<\/a>) however we have multiple subscriptions so this wont work for us. From what I've read there are two solutions for this. Use Azure blob storage or an SQL solution. Unfortunately I cant find much info online. Anyone have any idea how I can implement this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658248426997,
        "Question_score":0,
        "Question_tags":"azure|azure-blob-storage|databricks|mlflow",
        "Question_view_count":49,
        "Owner_creation_time":1632199544943,
        "Owner_last_access_time":1664060085030,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73040570",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":66566031,
        "Question_title":"MLFLow artifact logging and retrieve on remote server",
        "Question_body":"<p>I am trying to setup a MLFlow tracking server on a remote machine as a systemd service.\nI have a sftp server running and created a SSH key pair.<\/p>\n<p>Everything seems to work fine except the artifact logging. MLFlow seems to not have permissions to list the artifacts saved in the <code>mlruns<\/code> directory.<\/p>\n<p>I create an experiment and log artifacts in this way:<\/p>\n<pre><code>uri = 'http:\/\/192.XXX:8000' \nmlflow.set_tracking_uri(uri)\n\nmlflow.create_experiment('test', artifact_location='sftp:\/\/192.XXX:_path_to_mlruns_folder_')\n\nexperiment=mlflow.get_experiment_by_name('test')\nwith mlflow.start_run(experiment_id=experiment.experiment_id, run_name=run_name) as run:\n       mlflow.log_param(_parameter_name_, _parameter_value_)     \n       mlflow.log_artifact(_an_artifact_, _artifact_folder_name_)\n<\/code><\/pre>\n<p>I can see the metrics in the UI and the artifacts in the correct destination folder on the remote machine. However, in the UI I receive this message when trying to see the artifacts:<\/p>\n<blockquote>\n<p>Unable to list artifacts stored\nunder sftp:\/\/192.XXX:<em>path_to_mlruns_folder<\/em>\/<em>run_id<\/em>\/artifacts\nfor the current run. Please contact your tracking server administrator\nto notify them of this error, which can happen when the tracking\nserver lacks permission to list artifacts under the current run's root\nartifact directory.<\/p>\n<\/blockquote>\n<p>I cannot figure out why as the <code>mlruns<\/code> folder has <code>drwxrwxrwx<\/code> permissions and all the subfolders have <code>drwxrwxr-x<\/code>. What am I missing?<\/p>\n<hr \/>\n<p>UPDATE\nLooking at it with fresh eyes, it seems weird that it tries to list files through <code>sftp:\/\/192.XXX:<\/code>, it should just look in the folder <code>_path_to_mlruns_folder_\/_run_id_\/artifacts<\/code>. However, I still do not know how to circumvent that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615383956893,
        "Question_score":3,
        "Question_tags":"python|mlflow",
        "Question_view_count":2283,
        "Owner_creation_time":1403084852693,
        "Owner_last_access_time":1664082389037,
        "Owner_location":null,
        "Owner_reputation":2210,
        "Owner_up_votes":1124,
        "Owner_down_votes":117,
        "Owner_views":262,
        "Question_last_edit_time":1615451686849,
        "Answer_body":"<p>The problem seems to be that by default the systemd service is run by root.\nSpecifying a user and creating a ssh key pair for that user to access the same remote machine worked.<\/p>\n<pre><code>[Unit]\n\nDescription=MLflow server\n\nAfter=network.target \n\n[Service]\n\nRestart=on-failure\n\nRestartSec=20\n\nUser=_user_\n\nGroup=_group_\n\nExecStart=\/bin\/bash -c 'PATH=_yourpath_\/anaconda3\/envs\/mlflow_server\/bin\/:$PATH exec mlflow server --backend-store-uri postgresql:\/\/mlflow:mlflow@localhost\/mlflow --default-artifact-root sftp:\/\/_user_@192.168.1.245:_yourotherpath_\/MLFLOW_SERVER\/mlruns -h 0.0.0.0 -p 8000' \n\n[Install]\n\nWantedBy=multi-user.target\n<\/code><\/pre>\n<p><code>_user_<\/code> and <code>_group_<\/code> should be the same listed by <code>ls -la<\/code> in the <code>mlruns<\/code> directory.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1615544206663,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66566031",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68918223,
        "Question_title":"Databricks: Migrate a registered model from one workspace to another?",
        "Question_body":"<p>We have multiple Databricks Workspaces on Azure. On one of them we trained multiple models and registered them in the MLflow registry. Our goal is to move those model from one databricks workspace to another and so far, i could not find a straight forwared way to do this except running the training script again on the new databricks workspace.<\/p>\n<p>Downloading the model an registering them in the new workspace didn't work so far. Should I create a &quot;dummy&quot; training script, that just loads the model, does nothing with it and then logs it away in the new workspace?<\/p>\n<p>Seems to me like databricks never anticipated, that someone might want to migrate ML models?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1629874787863,
        "Question_score":1,
        "Question_tags":"azure|machine-learning|migration|databricks|mlflow",
        "Question_view_count":588,
        "Owner_creation_time":1485121974840,
        "Owner_last_access_time":1659449289970,
        "Owner_location":null,
        "Owner_reputation":188,
        "Owner_up_votes":0,
        "Owner_down_votes":1,
        "Owner_views":22,
        "Question_last_edit_time":1629877329852,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68918223",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56701139,
        "Question_title":"Model-logging for \"hybrid models\" (e.g. SKlearn Pipeline including KerasWrapper) possible?",
        "Question_body":"<p>I have wrapped my keras-tf-model into a Sklearn Pipeline, which also does some pre- and postprocessing. I want to serialize this model and capture its dependencies via MLflow.<\/p>\n\n<p>I have tried <code>mlflow.keras.save_model()<\/code>, which seems not appropriate. (it's not a \"pure\" keras model and as no <code>save()<\/code> attribute)<\/p>\n\n<p>I also tried <code>mlflow.sklearn.save_model()<\/code> and <code>mlflow.pyfunc.save_model()<\/code>, which both lead my to the same error: <\/p>\n\n<p><code>NotImplementedError: numpy() is only available when eager execution is enabled.<\/code><\/p>\n\n<p>(This error seems to stem from a clash between python and tensorflow, maybe?)<\/p>\n\n<p>I wonder, should it already\/ generally be possible to serialize these kind of \"hybrid\" models with mlflow?<\/p>\n\n<h3>Please finde a minimal example below<\/h3>\n\n<pre><code># In[1]:\n\n\nfrom mlflow.sklearn import save_model\nimport mlflow.sklearn\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\n\nfrom tensorflow.keras.models import Sequential\n\nimport numpy as np\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\n\n\n# ### Save Keras Model\n\n# In[2]:\n\n\niris_data = load_iris() \n\nx = iris_data.data\ny_ = iris_data.target.reshape(-1, 1)\n\n# One Hot encode the class labels\nencoder = OneHotEncoder(sparse=False)\ny = encoder.fit_transform(y_)\n\n# Split the data for training and testing\ntrain_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20)\n\n# Build the model\nmodel = Sequential()\n\nmodel.add(Dense(10, input_shape=(4,), activation='relu', name='fc1'))\nmodel.add(Dense(10, activation='relu', name='fc2'))\nmodel.add(Dense(3, activation='softmax', name='output'))\n\noptimizer = Adam(lr=0.001)\nmodel.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(train_x, train_y, verbose=2, batch_size=5, epochs=20)\n\n\n# In[3]:\n\n\nimport mlflow.keras\n\nmlflow.keras.save_model(model, \"modelstorage\/model40\")\n\n\n# ### Save Minimal SKlearn-Pipeline (with Keras)\n\n# In[4]:\n\n\nfrom category_encoders.target_encoder import TargetEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n\n# In[5]:\n\n\ndef define_model():\n    \"\"\"\n    Create fully connected network with given parameters.\n    \"\"\"\n    keras_model = Sequential()\n\n    keras_model.add(Dense(10, input_shape=(4,), activation='relu', name='fc1'))\n    keras_model.add(Dense(10, activation='relu', name='fc2'))\n    keras_model.add(Dense(3, activation='softmax', name='output'))\n\n    optimizer = Adam(lr=0.001)\n    keras_model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\n# In[6]:\n\n\n# target_encoder = TargetEncoder() \nscaler = StandardScaler()\nkeras_model = KerasClassifier(define_model, batch_size=5, epochs=20)\n\n\n# In[7]:\n\n\npipeline = Pipeline([\n#     ('encoding', target_encoder),\n    ('scaling', scaler),\n    ('modeling', keras_model)\n])\n\n\n# In[8]:\n\n\npipeline.fit(train_x, train_y)\n\n\n# In[9]:\n\n\nmlflow.keras.save_model(pipeline, \"modelstorage\/model42\")   #not working\n\n\n# In[10]:\n\n\nimport mlflow.sklearn\n\nmlflow.sklearn.save_model(pipeline, \"modelstorage\/model43\")\n\nOutput from modelstorage\/model43\/conda.yaml:\n\n======================\nchannels:\n- defaults\ndependencies:\n- python=3.6.7\n- scikit-learn=0.21.2\n- pip:\n  - mlflow\n  - cloudpickle==1.2.1\nname: mlflow-env\n======================\n\nDoesn't seem to capture Tensorflow.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1561111222150,
        "Question_score":2,
        "Question_tags":"python|keras|scikit-learn|mlflow",
        "Question_view_count":1470,
        "Owner_creation_time":1336936830510,
        "Owner_last_access_time":1663079651823,
        "Owner_location":null,
        "Owner_reputation":948,
        "Owner_up_votes":592,
        "Owner_down_votes":1,
        "Owner_views":132,
        "Question_last_edit_time":1561122889067,
        "Answer_body":"<p>You can add extra dependencies when you save your model, for example if you have a keras step in your pipeline you can add keras &amp; tensorflow:<\/p>\n\n<pre><code>  conda_env = mlflow.sklearn.get_default_conda_env()\n  conda_env[\"dependencies\"] = ['keras==2.2.4', 'tensorflow==1.14.0'] + conda_env[\"dependencies\"]\n  mlflow.sklearn.log_model(pipeline, \"modelstorage\/model43\", conda_env = conda_env)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1572628704763,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56701139",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73562615,
        "Question_title":"MlflowException: API request (Caused by ResponseError('too many 503 error responses'))",
        "Question_body":"<p>I am using mlflow to register my model. I try to use 'Scenario 4' when artifacts load to S3 bucket from local.<\/p>\n<ol>\n<li><p>Add credentials of S3 bucket to .aws\/credentials<\/p>\n<\/li>\n<li><p>Set endpoint and mlflow URI:<\/p>\n<p>os.environ[&quot;MLFLOW_S3_ENDPOINT_URL&quot;]='https:\/\/storage.yandexcloud.net'\nos.environ[&quot;MLFLOW_TRACKING_URI&quot;]='http:\/\/:8000'<\/p>\n<\/li>\n<li><p>Log model to S3 via mlflow:<\/p>\n<p>import mlflow\nimport mlflow.sklearn\nmlflow.set_experiment(&quot;my&quot;)\n...\nmlflow.sklearn.log_model(model, artifact_path=&quot;models_mlflow&quot;)<\/p>\n<\/li>\n<\/ol>\n<p>But get error:<\/p>\n<pre><code>MlflowException: API request to http:\/\/&lt;IP&gt;:8000\/api\/2.0\/mlflow-artifacts\/artifacts\/6\/95972bcc493c4a8cbd8432fea4cc8bac\/artifacts\/models_mlflow\/model.pkl failed with exception HTTPConnectionPool(host='62.84.121.234', port=8000): Max retries exceeded with url: \/api\/2.0\/mlflow-artifacts\/artifacts\/6\/95972bcc493c4a8cbd8432fea4cc8bac\/artifacts\/models_mlflow\/model.pkl (Caused by ResponseError('too many 503 error responses'))\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1661985677113,
        "Question_score":0,
        "Question_tags":"python|amazon-s3|mlflow|mlops|yandexcloud",
        "Question_view_count":38,
        "Owner_creation_time":1396864721170,
        "Owner_last_access_time":1663931830007,
        "Owner_location":"Moscow, Russia",
        "Owner_reputation":75,
        "Owner_up_votes":105,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73562615",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58611088,
        "Question_title":"Is there a way to manage permissions at an experiment level in MLflow?",
        "Question_body":"<p>Is there a way to manage permissions at an experiment level in MLflow?  We would like to have a shared server but would like to be able to manage permissions at an experiment level - e.g. admin can view all experiments, user_group1 can manage experiment1 - perhaps different groups can see results vs post results.<\/p>\n\n<p>It looks like it is possible in databricks: <a href=\"https:\/\/docs.databricks.com\/administration-guide\/access-control\/workspace-acl.html#experiment-permissions\" rel=\"nofollow noreferrer\">https:\/\/docs.databricks.com\/administration-guide\/access-control\/workspace-acl.html#experiment-permissions<\/a>  but I can't find anything in the opensource APIdocs.<\/p>\n\n<p>Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1572364657393,
        "Question_score":4,
        "Question_tags":"mlflow",
        "Question_view_count":1567,
        "Owner_creation_time":1487332729157,
        "Owner_last_access_time":1663766140707,
        "Owner_location":null,
        "Owner_reputation":289,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58611088",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68034523,
        "Question_title":"How to download artifacts from mlflow in python",
        "Question_body":"<p>I am creating an mlflow experiment which logs a logistic regression model together with a metric and an artifact.<\/p>\n<pre><code>import mlflow\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_fscore_support\n\nwith mlflow.start_run(run_name=run_name, experiment_id=experiment_id):\n\n        logreg = LogisticRegression()\n        logreg.fit(x_train, y_train)\n        print('training over', flush=True)\n        y_pred = logreg.predict(x_test)\n        mlflow.sklearn.log_model(logreg, &quot;model&quot;)\n   \n        mlflow.log_metric(&quot;f1&quot;, precision_recall_fscore_support(y_test, y_pred, average='weighted')[2])\n        mlflow.log_artifact(x_train.to_csv('train.csv')\n<\/code><\/pre>\n<p>for some data (<code>x_train, y_train, x_test, y_test<\/code>)<\/p>\n<p>Is there any way to access the artifacts for that specific experiment_id for this run_name and read the <code>train.csv<\/code> and also read the <code>model<\/code> ?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1624016436840,
        "Question_score":3,
        "Question_tags":"python|python-3.x|mlflow",
        "Question_view_count":5182,
        "Owner_creation_time":1454338460480,
        "Owner_last_access_time":1664044608790,
        "Owner_location":null,
        "Owner_reputation":3527,
        "Owner_up_votes":352,
        "Owner_down_votes":6,
        "Owner_views":440,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There is a <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.download_artifacts\" rel=\"noreferrer\">download_artifacts function<\/a> that allows you to get access to the logged artifact:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>local_path = client.download_artifacts(run_id, &quot;train.csv&quot;, local_dir)\n<\/code><\/pre>\n<p>The model artifact could either downloaded using the same function (there should be the object called <code>model\/model.pkl<\/code> (for scikit-learn, or something else), or you can load model by run:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>loaded_model = mlflow.pyfunc.load_model(f&quot;runs:\/{run_id}\/model&quot;)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1624022186892,
        "Answer_score":5.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68034523",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72341647,
        "Question_title":"MLFlow -> ModuleNotFoundError: No module named 'sqlalchemy.future'",
        "Question_body":"<p>It seems to use MLFlow Model Registry locally, one option is to build my own backend database with SQLite.<\/p>\n<p>I've found a site, which advised to run:<\/p>\n<pre><code>mlflow server --backend-store-uri sqlite:\/\/\/mlflow.db --default-artifact-root .\/artifacts --host 0.0.0.0 --port 5000\n<\/code><\/pre>\n<p>When running the command above, I get the following error message:<\/p>\n<pre><code>2022\/05\/22 23:08:58 ERROR mlflow.cli: Error initializing backend store\n2022\/05\/22 23:08:58 ERROR mlflow.cli: No module named 'sqlalchemy.future'\nTraceback (most recent call last):\n  File &quot;\/home\/username\/.local\/lib\/python3.8\/site-packages\/mlflow\/cli.py&quot;, line 426, in server\n    initialize_backend_stores(backend_store_uri, default_artifact_root)\n  File &quot;\/home\/username\/.local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py&quot;, line 259, in initialize_backend_stores\n    _get_tracking_store(backend_store_uri, default_artifact_root)\n  File &quot;\/home\/username\/.local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py&quot;, line 244, in _get_tracking_store\n    _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\n  File &quot;\/home\/username\/.local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py&quot;, line 39, in get_store\n    return self._get_store_with_resolved_uri(resolved_store_uri, artifact_uri)\n  File &quot;\/home\/username\/.local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py&quot;, line 49, in _get_store_with_resolved_uri\n    return builder(store_uri=resolved_store_uri, artifact_uri=artifact_uri)\n  File &quot;\/home\/username\/.local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py&quot;, line 110, in _get_sqlalchemy_store\n    from mlflow.store.tracking.sqlalchemy_store import SqlAlchemyStore\n  File &quot;\/home\/username\/.local\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/sqlalchemy_store.py&quot;, line 11, in &lt;module&gt;\n    from sqlalchemy.future import select\nModuleNotFoundError: No module named 'sqlalchemy.future'\n<\/code><\/pre>\n<p>This seems odd, because if I run <code>pip freeze<\/code>, the sqlalchemy shows up, or if I do <code>from sqlalchemy.future import select<\/code> in a notebook, I get no error.<\/p>\n<p>I think this may related to using a virtual environment. The current one I'm using is in <code>\/home\/username\/folder\/mlflow\/.mlflow<\/code> but mlflow seems to be looking elsewhere for the file...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1653257749153,
        "Question_score":1,
        "Question_tags":"python|sqlalchemy|mlflow",
        "Question_view_count":274,
        "Owner_creation_time":1396288958297,
        "Owner_last_access_time":1664053410610,
        "Owner_location":null,
        "Owner_reputation":714,
        "Owner_up_votes":128,
        "Owner_down_votes":6,
        "Owner_views":251,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72341647",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71738738,
        "Question_title":"How to set custom path for databricks mlflow artifacts on s3",
        "Question_body":"<p>I've created an empty experiments from databricks experiments console and given the path for my artifacts on s3 i.e. s3:\/\/\/. When i run the scripts, the artifacts are stored at<\/p>\n<pre><code>s3:\/\/&lt;bucket&gt;\/\/&lt;32 char id&gt;\/artifacts\/model-Elasticnet\/model.pkl\n<\/code><\/pre>\n<p>I want to replace \/\/&lt;32 char id&gt;\/artifacts\/ with \/datetime\/artifacts\/ so something like<\/p>\n<pre><code>s3:\/\/&lt;bucket&gt;\/&lt;datetime&gt;\/artifacts\/model-Elasticnet\/model.pkl\n<\/code><\/pre>\n<p>Is there any way i could achieve that?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wUDcE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wUDcE.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Note: experiment_id is from databricks experiment console<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1649081553880,
        "Question_score":2,
        "Question_tags":"databricks|mlflow|aws-databricks|mlops",
        "Question_view_count":140,
        "Owner_creation_time":1526140623120,
        "Owner_last_access_time":1664008621693,
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":962,
        "Owner_up_votes":106,
        "Owner_down_votes":9,
        "Owner_views":128,
        "Question_last_edit_time":1649316095152,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71738738",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67786052,
        "Question_title":"Log Pickle files as a part of Mlflow run",
        "Question_body":"<p>I am running an MLflow experiment as a part of it I would like to log a few artifacts as a python pickle.<\/p>\n<p>Ex: Trying out different categorical encoders, so wanted to log the encoder objects as a pickle file.<\/p>\n<p>Is there a way to achieve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622538922663,
        "Question_score":3,
        "Question_tags":"python|databricks|azure-databricks|mlflow",
        "Question_view_count":1843,
        "Owner_creation_time":1411361217027,
        "Owner_last_access_time":1662192778187,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":569,
        "Owner_up_votes":41,
        "Owner_down_votes":3,
        "Owner_views":123,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There are two functions for there:<\/p>\n<ol>\n<li><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifact\" rel=\"nofollow noreferrer\">log_artifact<\/a> - to log a local file or directory as an artifact<\/li>\n<li><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifacts\" rel=\"nofollow noreferrer\">log_artifacts<\/a> - to log a contents of a local directory<\/li>\n<\/ol>\n<p>so it would be as simple as:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with mlflow.start_run():\n    mlflow.log_artifact(&quot;encoder.pickle&quot;)\n<\/code><\/pre>\n<p>And you will need to use the <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">custom MLflow model<\/a> to use that pickled file, something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow.pyfunc\n\nclass my_model(mlflow.pyfunc.PythonModel):\n    def __init__(self, encoders):\n        self.encoders = encoders\n\n    def predict(self, context, model_input):\n        _X = ...# do encoding using self.encoders.\n        return str(self.ctx.predict([_X])[0])\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1622542563552,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67786052",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72250896,
        "Question_title":"PowerShell Get request with body",
        "Question_body":"<p>I am trying <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#search-modelversions\" rel=\"nofollow noreferrer\">this api endpoint<\/a>.\nI can call this in python, no problem, like the below<\/p>\n<pre><code>get_model_versions={\n    &quot;filter&quot;:&quot;name='model_name'&quot;,\n    &quot;order_by&quot;:[&quot;version DESC&quot;],\n    &quot;max_results&quot;:1\n}\n\ninit_get = requests.get(&quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search&quot;,headers=header_read,json=get_model_versions)\n<\/code><\/pre>\n<p>However, I just can't seem to find a way to make it work in Powershell.<\/p>\n<p>First the powershell &quot;get&quot; Invoke-RestMethod does not accept a body<\/p>\n<p>and then I can't seem to find a way to append it in Powershell as a query string.<\/p>\n<p>I have tried (among other failed attempts), the following<\/p>\n<pre><code>$get_model_versions=([PSCustomObject]@{\n  filter = &quot;name=`'model_name`'&quot;\n  order_by = @(&quot;version desc&quot;)\n} | ConvertTo-Json)\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get -Body $get_model_versions\n<\/code><\/pre>\n<p>But that gives me an error that body can't be used with a get method<\/p>\n<p>trying to append it as a query string (like if I even just keep the name filter and remove the others), also fails<\/p>\n<pre><code>$searchuri= &quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search?filter=&quot;&quot;name==model_name&quot;&quot;&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get\n<\/code><\/pre>\n<p>fails with<\/p>\n<pre><code>{&quot;error_code&quot;:&quot;INVALID_PARAMETER_VALUE&quot;,&quot;message&quot;:&quot;Unsupported filter query : `\\&quot;name==model_name\\&quot;`. Unsupported operator.&quot;}\n<\/code><\/pre>\n<p>How can I mimic the same behaviour in Powershell, as I do in Python?<\/p>\n<p>EDIT 1: I did try to encode the query param (maybe I did it wrong), but here's how my failed attempt looked like<\/p>\n<pre><code>$encodedvalue = [System.Web.HttpUtility]::UrlEncode(&quot;`&quot;name='model_name'`&quot;&quot;)\n$searchuri= &quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search?filter=$encodedvalue&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get\n<\/code><\/pre>\n<p>But that too gives me<\/p>\n<pre><code>&quot;Unsupported filter query : `\\&quot;name='model_name'\\&quot;`. Unsupported operator.&quot;\n<\/code><\/pre>\n<p>I have also tried it successfully in Postman by passing a raw json body (the same as python) and when I look at the generated PowerShell code in Postman I see this<\/p>\n<pre><code>$headers = New-Object &quot;System.Collections.Generic.Dictionary[[String],[String]]&quot;\n$headers.Add(&quot;Authorization&quot;, &quot;Bearer token&quot;)\n$headers.Add(&quot;Content-Type&quot;, &quot;application\/json&quot;)\n\n$body = &quot;{\n`n    `&quot;filter`&quot;:`&quot;name='model_name'`&quot;,\n`n    `&quot;order_by`&quot;:[`&quot;version DESC`&quot;],\n`n    `&quot;max_results`&quot;:1\n`n}\n`n&quot;\n\n$response = Invoke-RestMethod 'baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search' -Method 'GET' -Headers $headers -Body $body\n$response | ConvertTo-Json\n<\/code><\/pre>\n<p>But of course that fails (if you copy that in an powershell editor and run it<\/p>\n<pre><code>Invoke-RestMethod : Cannot send a content-body with this verb-type\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1652637573657,
        "Question_score":1,
        "Question_tags":"powershell|rest|python-requests|mlflow",
        "Question_view_count":281,
        "Owner_creation_time":1428654714763,
        "Owner_last_access_time":1664012257383,
        "Owner_location":null,
        "Owner_reputation":596,
        "Owner_up_votes":53,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Question_last_edit_time":1652640935860,
        "Answer_body":"<p>Finally, after struggling for a long time, I found the answer !<\/p>\n<p>The crux is in the documentation <a href=\"https:\/\/docs.microsoft.com\/en-us\/powershell\/module\/microsoft.powershell.utility\/invoke-restmethod?view=powershell-7.2\" rel=\"nofollow noreferrer\">here<\/a>.\nEspecially this section<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/h0gwk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/h0gwk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So, if you want to pass on a body for your &quot;get&quot; method in powershell, pass it as a hashtable.<\/p>\n<p>So, finally the answer is<\/p>\n<pre><code>$query=@{&quot;filter&quot;=&quot;name='model_name'&quot;;&quot;order_by&quot;=@(&quot;version DESC&quot;); &quot;max_results&quot;=1};\n$searchuri=&quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get -Body $query\n<\/code><\/pre>\n<p>Hope this helps someone looking for something similar.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1652649592320,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72250896",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73501103,
        "Question_title":"Getting Bad request while searching run in mlflow",
        "Question_body":"<p>Training a ml model with mlflow in azure environment.<\/p>\n<pre><code>import mlflow\nfrom mlflow import MlflowClient\nfrom azureml.core import Experiment, Workspace\n\nexperiment_name = 'housing-lin-mlflow'\n\nexperiment = Experiment(ws, experiment_name)\n\nruns = mlflow.search_runs(experiment_ids=[ experiment.id ])\n\n<\/code><\/pre>\n<p>While fetching runs from search_runs getting this error :<\/p>\n<pre><code>RestException: BAD_REQUEST: For input string: &quot;5b649b3c-3b8f-497a-bb4f&quot;\n<\/code><\/pre>\n<p>MLflow version : 1.28.0\nIn Azure studio jobs have been created and successfully run.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661517215980,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-service|mlflow",
        "Question_view_count":56,
        "Owner_creation_time":1582101477803,
        "Owner_last_access_time":1663953873503,
        "Owner_location":"Delhi, India",
        "Owner_reputation":171,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":53,
        "Question_last_edit_time":1661625379892,
        "Answer_body":"<p>The bad request in MLFlow after successful running the job is because of not giving proper API permissions for the application.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Search for <strong>MLFLOW<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Scroll down<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s50AL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s50AL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Click on View API Permissions<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Under API permissions, assign the permissions according to the application running region and requirements. Checkout the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-models-mlflow\" rel=\"nofollow noreferrer\">document<\/a> for further information.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1661603882123,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73501103",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":51335594,
        "Question_title":"Error with \"mlflow ui\" when trying to run it on MS Windows",
        "Question_body":"<p>When I run <code>mlflow ui<\/code> the following error occurred:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Anaconda3\\Scripts\\gunicorn.exe\\__main__.py\", line 5, in &lt;module&gt;\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\app\\wsgiapp.py\", line 9, in &lt;module&gt;\n    from gunicorn.app.base import Application\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\app\\base.py\", line 12, in &lt;module&gt;\n    from gunicorn import util\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\util.py\", line 9, in &lt;module&gt;\n    import fcntl\nModuleNotFoundError: No module named 'fcntl'\nTraceback (most recent call last):\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Anaconda3\\Scripts\\mlflow.exe\\__main__.py\", line 9, in &lt;module&gt;\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 1066, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\cli.py\", line 131, in ui\n    mlflow.server._run_server(file_store, file_store, host, port, 1)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\server\\__init__.py\", line 48, in _run_server\n    env=env_map, stream_output=True)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\utils\\process.py\", line 38, in exec_cmd\n    raise ShellCommandException(\"Non-zero exitcode: %s\" % (exit_code))\nmlflow.utils.process.ShellCommandException: Non-zero exitcode: 1\n<\/code><\/pre>\n\n<p>I used anaconda + python 3.6.5 and I installed git and set path with <code>C:\\Program Files\\Git\\bin\\git.exe<\/code> and <code>C:\\Program Files\\Git\\cmd<\/code>.<\/p>\n\n<p>I installed <code>mlflow<\/code> whit <code>pip install mlflow<\/code> and its version is 0.2.1.<\/p>\n\n<p>I set a variable with name <code>GIT_PYTHON_GIT_EXECUTABLE<\/code> and value <code>C:\\Program Files\\Git\\bin\\git.exe<\/code> in Environment Variables. <\/p>\n\n<p>How can I solve this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1531546446273,
        "Question_score":1,
        "Question_tags":"python|windows|fcntl|mlflow",
        "Question_view_count":4688,
        "Owner_creation_time":1308552848513,
        "Owner_last_access_time":1664030809627,
        "Owner_location":null,
        "Owner_reputation":1177,
        "Owner_up_votes":24,
        "Owner_down_votes":0,
        "Owner_views":144,
        "Question_last_edit_time":1531837117032,
        "Answer_body":"<p><a href=\"https:\/\/github.com\/databricks\/mlflow\" rel=\"nofollow noreferrer\">mlflow documentation<\/a> already says that <\/p>\n\n<blockquote>\n  <p>Note 2: We <strong>do not currently support running MLflow on Windows<\/strong>.\n  Despite this, we would appreciate any contributions to make MLflow\n  work better on Windows.<\/p>\n<\/blockquote>\n\n<p>You're hitting <code>fcntl<\/code> problem: it's not available on MS Windows platform because it's a \"wrapper\" around the <a href=\"http:\/\/man7.org\/linux\/man-pages\/man2\/fcntl.2.html\" rel=\"nofollow noreferrer\">fcntl function<\/a> that's available on POSIX-compatible systems. (See <a href=\"https:\/\/stackoverflow.com\/a\/1422436\/236007\">https:\/\/stackoverflow.com\/a\/1422436\/236007<\/a> for more details.)<\/p>\n\n<p>Solving this requires modifying the source code of mlflow accordingly. <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1531837039907,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51335594",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70098779,
        "Question_title":"How to connect to MLFlow tracking server that has auth?",
        "Question_body":"<p>I want to connect to remote tracking server (<a href=\"http:\/\/123.456.78.90\" rel=\"nofollow noreferrer\">http:\/\/123.456.78.90<\/a>) that requires authentication<\/p>\n<p>When I do this:<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>import mlflow\nmlflow.set_tracking_uri(\"http:\/\/123.456.78.90\")\nmlflow.set_experiment(\"my-experiment\")<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p>I get an error<\/p>\n<p><em>MlflowException: API request to endpoint \/api\/2.0\/mlflow\/experiments\/list failed with error code 401 != 200.\nResponse body: 401 Authorization Required<\/em><\/p>\n<p>I understand that I need to log in first but I have no idea how to do it<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637767811310,
        "Question_score":1,
        "Question_tags":"authorization|tracking|mlflow",
        "Question_view_count":2102,
        "Owner_creation_time":1637766437853,
        "Owner_last_access_time":1663839694783,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":"<p><a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#logging-to-a-tracking-server\" rel=\"nofollow noreferrer\">MLflow documentation<\/a> says:<\/p>\n<blockquote>\n<p><code>MLFLOW_TRACKING_USERNAME<\/code> and <code>MLFLOW_TRACKING_PASSWORD<\/code> - username and password to use with HTTP Basic authentication. To use Basic authentication, you must set both environment variables.<\/p>\n<\/blockquote>\n<p>So you just need to set these variables in your code using <code>os.environ<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>os.environ['MLFLOW_TRACKING_USERNAME'] = 'name'\nos.environ['MLFLOW_TRACKING_PASSWORD'] = 'pass'\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1637773273483,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70098779",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73241326,
        "Question_title":"Can't see artifact ui in mlflow dashboard",
        "Question_body":"<p>mlflow server <br \/>\n--host 0.0.0.0 <br \/>\n--port 5000 <br \/>\n--backend-store-uri sqlite:\/\/\/\/tmp\/test.db <br \/>\n--artifacts-destination s3:\/\/mlflow <br \/>\n--serve-artifacts<\/p>\n<p>Using minio as S3\nAnd env. Variable as secret key &amp; access key<\/p>\n<p>#mlflow #artifactui #proxyartifact<a href=\"https:\/\/i.stack.imgur.com\/Q3h0B.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1659641244373,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":65,
        "Owner_creation_time":1542045989230,
        "Owner_last_access_time":1663926597850,
        "Owner_location":"Gandhinagar, Gujarat, India",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1660023754676,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73241326",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70377991,
        "Question_title":"Setting tracking and artifact locations",
        "Question_body":"<p>I read the documentation for setting the tracking and artifact location. For tracking URI, the options are:<\/p>\n<ol>\n<li>set the <code>MLFLOW_TRACKING_URI<\/code> in bash or the environment I use<\/li>\n<li>set the location with <code>mlflow.set_tracking_uri<\/code> inside the python code<\/li>\n<li>Start a server and then set the above parameters to reflect the server information.<\/li>\n<\/ol>\n<p>How can I set the tracking uri in MLproject? I want to use minimal external code in my project. One way I think of is to use the environment section of the MLproject file environment, like <code>[[&quot;NEW_ENV_VAR&quot;, &quot;new_var_value&quot;]<\/code> Is this correct? Or is there any other way to do it? I could find no example for this except under docker section.<\/p>\n<p>Secondly, same for the artifact registry. Can this be set somewhere in the MLproject file?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1639653097383,
        "Question_score":0,
        "Question_tags":"tracking|artifact|mlflow",
        "Question_view_count":146,
        "Owner_creation_time":1462427517847,
        "Owner_last_access_time":1646398883960,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1639662138947,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70377991",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61258979,
        "Question_title":"EKS Docker Image Pull CrashLoopBackOff",
        "Question_body":"<p>I'm trying to deploy a Docker image from ECR to my EKS. When attempting to deploy my docker image to a pod, I get the following events from a CrashLoopBackOff:<\/p>\n\n<pre><code>Events:\n  Type     Reason                  Age               From                                   Message\n  ----     ------                  ----              ----                                   -------\n  Normal   Scheduled               62s               default-scheduler                      Successfully assigned default\/mlflow-tracking-server to &lt;EC2 IP&gt;.internal\n  Normal   SuccessfulAttachVolume  60s               attachdetach-controller                AttachVolume.Attach succeeded for volume \"&lt;PVC&gt;\"\n  Normal   Pulling                 56s               kubelet, &lt;IP&gt;.ec2.internal             Pulling image \"&lt;ECR Image UI&gt;\"\n  Normal   Pulled                  56s               kubelet, &lt;IP&gt;.ec2.internal             Successfully pulled image \"&lt;ECR Image UI&gt;\"\n  Normal   Created                 7s (x4 over 56s)  kubelet, &lt;IP&gt;.ec2.internal             Created container mlflow-tracking-server\n  Normal   Pulled                  7s (x3 over 54s)  kubelet, &lt;IP&gt;.ec2.internal             Container image \"&lt;ECR Image UI&gt;\" already present on machine\n  Normal   Started                 6s (x4 over 56s)  kubelet, &lt;IP&gt;.ec2.internal             Started container mlflow-tracking-server\n  Warning  BackOff                 4s (x5 over 52s)  kubelet, &lt;IP&gt;.ec2.internal             Back-off restarting failed container\n<\/code><\/pre>\n\n<p>I don't understand why it keeps looping like this and failing. Would anyone know why this is happening?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1587067011163,
        "Question_score":1,
        "Question_tags":"docker|amazon-eks|mlflow",
        "Question_view_count":604,
        "Owner_creation_time":1417012835813,
        "Owner_last_access_time":1628782664573,
        "Owner_location":null,
        "Owner_reputation":945,
        "Owner_up_votes":35,
        "Owner_down_votes":1,
        "Owner_views":148,
        "Question_last_edit_time":null,
        "Answer_body":"<p><code>CrashLoopBackError<\/code> can be related to these possible reasons:<\/p>\n\n<ul>\n<li><p>the application inside your pod is not starting due to an error;<\/p><\/li>\n<li><p>the image your pod is based on is not present in the registry, or the\nnode where your pod has been scheduled cannot pull from the registry;<\/p><\/li>\n<li><p>some parameters of the pod has not been configured correctly.<\/p><\/li>\n<\/ul>\n\n<p>In your case it seems an application error, inside the container.\nTry to view the logs with:<\/p>\n\n<pre><code>kubectl logs &lt;your_pod&gt; -n &lt;namespace&gt;\n<\/code><\/pre>\n\n<p>For more info on how to troubleshoot this kind of error refer to:<\/p>\n\n<p><a href=\"https:\/\/pillsfromtheweb.blogspot.com\/2020\/05\/troubleshooting-kubernetes.html\" rel=\"nofollow noreferrer\">https:\/\/pillsfromtheweb.blogspot.com\/2020\/05\/troubleshooting-kubernetes.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1588674359532,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61258979",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":57987999,
        "Question_title":"Delete a run in the experiment of mlflow from the UI so the run does not exist in backend store",
        "Question_body":"<p>I found deleting a <code>run<\/code> only change the state from <code>active<\/code> to <code>deleted<\/code>, because the run is still visible in the UI if searching by <code>deleted<\/code>. <\/p>\n\n<p>Is it possible to remove a <code>run<\/code> from the UI to save the space? \nWhen removing a run, does the artifact correspond to the run is also removed?<\/p>\n\n<p>If not, can the run be removed through rest call?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1568794247973,
        "Question_score":6,
        "Question_tags":"mlflow",
        "Question_view_count":3582,
        "Owner_creation_time":1408370821673,
        "Owner_last_access_time":1663216838707,
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":2521,
        "Owner_up_votes":447,
        "Owner_down_votes":13,
        "Owner_views":197,
        "Question_last_edit_time":1568796468592,
        "Answer_body":"<p>You can't do it via the web UI but you can from a python terminal<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmlflow.delete_experiment(69)\n<\/code><\/pre>\n\n<p>Where 69 is the experiment ID<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1569793174476,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57987999",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73619538,
        "Question_title":"geting artifacts from mlflow GridSearch run",
        "Question_body":"<p>I'm running a sklearn pipeline with hyperparameter search (let's say GridSearch). Now, I am logging artifacts such as test results and whole-dataset predictions. I'd like to retrieve these artifacts but the mlflow API is getting in the way...<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmlflow.set_tracking_uri(&quot;sqlite:\/\/\/mlruns\/mlruns.db&quot;)\nmlflow.set_registry_uri(&quot;.\/mlruns\/&quot;)\n\nrun_ids = [r.run_id for r in mlflow.list_run_infos(mlflow.get_experiment_by_name(&quot;My Experiment&quot;).experiment_id)]\n<\/code><\/pre>\n<p>With the above code, I can retrieve all runs but I have no way of telling which one is a toplevel run with artifacts logged or a sub-run spawned by the GridSearch procedure.<\/p>\n<p>Is there some way of querying only for <strong>parent<\/strong> runs, so I can retrieve these csv files in order to plot the results? I can of course go to the web api and manually select the run then copy the URI for the file, but I'd like to do it programmatically instead of opening a tab and clicking things.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1662455773517,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":10,
        "Owner_creation_time":1467887362327,
        "Owner_last_access_time":1663866285120,
        "Owner_location":"Spain",
        "Owner_reputation":624,
        "Owner_up_votes":56,
        "Owner_down_votes":2,
        "Owner_views":76,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73619538",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69484727,
        "Question_title":"Pyspark: How to save and apply IndexToString to convert labels back to original values in a new predicted dataset",
        "Question_body":"<p>I am using pyspark.ml.RandomForestClassifier and one of the steps here involves <strong>StringIndexer<\/strong> on the training data target variable to convert it into labels.<\/p>\n<pre><code>indexer = StringIndexer(inputCol = target_variable_name, outputCol = 'label').fit(df)\ndf = indexer.transform(df)\n<\/code><\/pre>\n<p>After fitting the final model I am saving it using mlflow.spark.log_model(). So, when applying the model on a new dataset in future, I just load the model again and apply to the new data:<\/p>\n<pre><code>model = mlflow.sklearn.load_model(&quot;models:\/RandomForest_model\/None&quot;)\npredictions = rfModel.transform(new_data)\n<\/code><\/pre>\n<p>In the new_data the prediction will come as <strong>labels<\/strong> and not in original value. So, if I have to get the original values I have to use <strong>IndexToString<\/strong><\/p>\n<pre><code>labelConverter = IndexToString(inputCol=&quot;prediction&quot;, outputCol=&quot;predictedLabel&quot;,labels=indexer.labels)\npredictions = labelConverter.transform(predictions)\n<\/code><\/pre>\n<p>So, the question is, my model doesn't save the <strong>indexer.labels<\/strong> as only the model gets saved. How do, I save and use the indexer.labels from my training dataset on any new dataset. Can this be saved and retrived in mlflow ?<\/p>\n<p>Apologies, if Iam sounding na\u00efve here . But, getting back the original values in the new dataset is really getting me confused.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1633624197133,
        "Question_score":1,
        "Question_tags":"pyspark|databricks|random-forest|apache-spark-mllib|mlflow",
        "Question_view_count":115,
        "Owner_creation_time":1501160366927,
        "Owner_last_access_time":1663743894843,
        "Owner_location":null,
        "Owner_reputation":459,
        "Owner_up_votes":100,
        "Owner_down_votes":0,
        "Owner_views":61,
        "Question_last_edit_time":1633676810887,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69484727",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61879913,
        "Question_title":"ModuleNotFoundError: No module named 'pyspark.dbutils' while running multiple.py file\/notebook on job clusters in databricks",
        "Question_body":"<p>I am working in TravisCI, MlFlow and Databricks environment where .tavis.yml sits at git master branch and detects any change in <code>.py<\/code> file and whenever it gets updated, It will run mlflow command to run .py file in databricks environment. \nmy MLProject file looks as following:<\/p>\n\n<pre><code>name: mercury_cltv_lib\nconda_env: conda-env.yml\n\n\nentry_points:    \n  main:\n    command: \"python3 run-multiple-notebooks.py\"\n<\/code><\/pre>\n\n<p>Workflow is as following:\nTravisCI detects change in master branch-->triggers build which will run MLFlow command and it'll spin up a job cluster in databricks to run .py file from repo.<\/p>\n\n<p>It worked fine with one .py file but when I tried to run multiple notebook using dbutils, it is throwing <\/p>\n\n<pre><code>  File \"run-multiple-notebooks.py\", line 3, in &lt;module&gt;\n    from pyspark.dbutils import DBUtils\nModuleNotFoundError: No module named 'pyspark.dbutils'\n<\/code><\/pre>\n\n<p>Please find below the relevant code section from run-multiple-notebooks.py<\/p>\n\n<pre><code>  def get_spark_session():\n    from pyspark.sql import SparkSession\n    return SparkSession.builder.getOrCreate()\n\n  def get_dbutils(self, spark = None):\n    try:\n        if spark == None:\n            spark = spark\n\n        from pyspark.dbutils import DBUtils #error line\n        dbutils = DBUtils(spark) #error line\n    except ImportError:\n        import IPython\n        dbutils = IPython.get_ipython().user_ns[\"dbutils\"]\n    return dbutils\n\n  def submitNotebook(notebook):\n    print(\"Running notebook %s\" % notebook.path)\n    spark = get_spark_session()\n    dbutils = get_dbutils(spark)\n<\/code><\/pre>\n\n<p>I tried all the options and tried <\/p>\n\n<pre><code>https:\/\/stackoverflow.com\/questions\/61546680\/modulenotfounderror-no-module-named-pyspark-dbutils\n<\/code><\/pre>\n\n<p>as well. It is not working :(<\/p>\n\n<p>Can someone please suggest if there is fix for the above-mentioned error while running .py in job cluster. My code works fine inside databricks local notebook but running from outside using TravisCI and MLFlow isn't working which is must requirement for pipeline automation.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1589840102237,
        "Question_score":2,
        "Question_tags":"pyspark|travis-ci|databricks|mlflow|dbutils",
        "Question_view_count":401,
        "Owner_creation_time":1555347036127,
        "Owner_last_access_time":1663694487237,
        "Owner_location":"Minnesota, USA",
        "Owner_reputation":352,
        "Owner_up_votes":27,
        "Owner_down_votes":1,
        "Owner_views":88,
        "Question_last_edit_time":1589982476476,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61879913",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68547133,
        "Question_title":"Using pytorch_lightning.loggers.MLFlowLogger with azure machine learning studio raises exception mlflow.exceptions.RestException: BAD_REQUEST",
        "Question_body":"<p>I'm trying to locally train pytorch_lightning model and log metrics using  pytorch_lightning.loggers.MLFlowLogger.<\/p>\n<p>It was working fine until last weekend. Now training crashes with error:<\/p>\n<pre><code>mlflow.exceptions.RestException: BAD_REQUEST: Response: {'Error': {'Code': 'UserError', 'Severity': None, 'Message': 'Metric once published using sync API should always use sync API to publish following metrics', 'MessageFormat': None, 'MessageParameters': None, 'ReferenceCode': None, 'DetailsUri': None, 'Target': None, 'Details': [], 'InnerError': None, 'DebugInfo': None, 'AdditionalInfo': None}, 'Correlation': {'operation': '', 'request': ''}, 'Environment': 'northeurope', 'Location': 'northeurope', 'Time': '2021-07-27T14:06:23.7035319+00:00', 'ComponentName': 'run-history', 'error_code': 'BAD_REQUEST'}\n<\/code><\/pre>\n<p>How to fix this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1627397531183,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-studio|mlflow|pytorch-lightning",
        "Question_view_count":176,
        "Owner_creation_time":1625582525933,
        "Owner_last_access_time":1656680057217,
        "Owner_location":"Krak\u00f3w, Poland",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68547133",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73321771,
        "Question_title":"Mlflow authorization with spnego",
        "Question_body":"<p>I saw this topic about Kerberos authntication - <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/2678\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/2678<\/a> . It was in 2020 . Our team trying to do authentication with kerberos by spnego. We did spnego on nginx server and it is fine - and get code 200 when we do curl to mlflow http uri . BUT we can't do it with mlflow environment variable .<\/p>\n<p>The question is - Does mlflow has some feature to make authentication with spnego or not? Or it has just these environment variables for authentication and such methods :<\/p>\n<ul>\n<li>MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD - username and password to use with HTTP Basic authentication. To use Basic authentication, you must set both environment variables .<\/li>\n<li>MLFLOW_TRACKING_TOKEN - token to use with HTTP Bearer authentication. Basic authentication takes precedence if set.<\/li>\n<li>MLFLOW_TRACKING_INSECURE_TLS - If set to the literal true, MLflow does not verify the TLS connection, meaning it does not validate certificates or hostnames for https:\/\/ tracking URIs. This flag is not recommended for production environments. If this is set to true then MLFLOW_TRACKING_SERVER_CERT_PATH must not be set.<\/li>\n<li>MLFLOW_TRACKING_SERVER_CERT_PATH - Path to a CA bundle to use. Sets the verify param of the requests.request function (see <a href=\"https:\/\/requests.readthedocs.io\/en\/master\/api\/\" rel=\"nofollow noreferrer\">https:\/\/requests.readthedocs.io\/en\/master\/api\/<\/a>). When you use a self-signed server certificate you can use this to verify it on client side. If this is set MLFLOW_TRACKING_INSECURE_TLS must not be set (false).<\/li>\n<li>MLFLOW_TRACKING_CLIENT_CERT_PATH - Path to ssl client cert file (.pem). Sets the cert param of the requests.request function (see <a href=\"https:\/\/requests.readthedocs.io\/en\/master\/api\/\" rel=\"nofollow noreferrer\">https:\/\/requests.readthedocs.io\/en\/master\/api\/<\/a>). This can be used to use a (self-signed) client certificate.<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660225498580,
        "Question_score":1,
        "Question_tags":"authentication|kerberos|mlflow|spnego",
        "Question_view_count":25,
        "Owner_creation_time":1536337420893,
        "Owner_last_access_time":1662282588817,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73321771",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65627039,
        "Question_title":"MLflow stores tags but does not return them",
        "Question_body":"<p>I am running the below code to store tags and then to retrieve them. As you can see below, Mlflow is storing one set of tags and returning another.<\/p>\n<pre><code>import mlflow\nwith mlflow.start_run() as active_run:\n    tw = { &quot;run_id&quot;: 1}\n    mlflow.set_tags(tw)            \n    print(&quot;Tags are &quot;, active_run.data.tags)\n    print(type(active_run.data.tags))\n<\/code><\/pre>\n<p>Output<\/p>\n<pre><code>Tags are  {'mlflow.source.name': '\/media\/Space\/AI\/anaconda4\/lib\/python3.7\/site-packages\/ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'adeel'}\n<\/code><\/pre>\n<p>Looking at the stored tags through mlflow ui, I can see that the tag &quot;run_id&quot; set by the code is actually stored in the run. However, only the header information of the run seems to be getting returned by active_run.data.tags.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610100575263,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":173,
        "Owner_creation_time":1445990517173,
        "Owner_last_access_time":1663982428387,
        "Owner_location":"Sydney, New South Wales, Australia",
        "Owner_reputation":689,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":87,
        "Question_last_edit_time":1611139529420,
        "Answer_body":"<p>At the moment, you have to query your run again in MLflow to get the run with all the info that you logged. In the example below, I call <code>mlflow.get_run(&lt;run_id&gt;)<\/code> to achieve this.<\/p>\n<pre><code>import mlflow\n\n\nwith mlflow.start_run() as active_run:\n  tags = { &quot;my_tag&quot;: 1}\n  mlflow.set_tags(tags)            \n  # Keep track of the run ID of the active run\n  run_id = active_run.info.run_id\n\nrun = mlflow.get_run(run_id)\nprint(&quot;The tags are &quot;, run.data.tags)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1610128097449,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65627039",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69088149,
        "Question_title":"MlFlow: Can't find runs using api",
        "Question_body":"<p>I try get list of runs, but get empty list.<\/p>\n<p>There are my runs:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/v8uF2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/v8uF2.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But if I try get it using api:\nI expect(<a href=\"https:\/\/mlflow.org\/docs\/latest\/rest-api.html#get-experiment\" rel=\"nofollow noreferrer\">by API<\/a>) that I also watch &quot;runs&quot;, but watch &quot;experiment&quot; only<\/p>\n<pre><code>http:\/\/localhost:5000\/api\/2.0\/mlflow\/experiments\/get?experiment_id=0\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/78a3Y.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/78a3Y.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I read in doc, that &quot;This field is deprecated. Please use the \u201cSearch Runs\u201d API to fetch runs within an experiment.&quot;, Ok, I try \u201cSearch Runs\u201d<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/EExIn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EExIn.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Nothing again.<\/p>\n<p>But I try get run by id(from ui):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4eHB7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4eHB7.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I need get list of run ids by experiment id. How can I do it?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1631017253323,
        "Question_score":0,
        "Question_tags":"python|api|mlflow",
        "Question_view_count":306,
        "Owner_creation_time":1525702055273,
        "Owner_last_access_time":1664047417137,
        "Owner_location":"Moscow, Russia",
        "Owner_reputation":155,
        "Owner_up_votes":34,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69088149",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60667610,
        "Question_title":"How to deploy mlflow model with data preprocessing(text data)",
        "Question_body":"<p>I have developed keras text classification model. I have preprocessed data(tokenization). I have logged trained model successfully(mlflow.keras.log_model). I have served model using mlflow serve. Now while doing prediction on text data I need to do preprocessing using same tokenizer object used for training.\nHow to preprocess test data and get predictions from served model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1584090517703,
        "Question_score":3,
        "Question_tags":"mlflow",
        "Question_view_count":1996,
        "Owner_creation_time":1498470936987,
        "Owner_last_access_time":1614914388693,
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can log a custom python model: \n<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1584552358667,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60667610",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63806335,
        "Question_title":"How to migrate MlFlow experiments from one Databricks workspace to another with registered models?",
        "Question_body":"<p>so unfortunatly we have to redeploy our Databricks Workspace in which we use the MlFlow functonality with the Experiments and the registering of Models.<\/p>\n<p>However if you export the user folder where the eyperiment is saved with a DBC and import it into the new workspace, the Experiments are not migrated and are just missing.<\/p>\n<p>So the easiest solution did not work. The next thing I tried was to create a new experiment in the new workspace. Copy all the experiment data from the dbfs of the old workspace (with dbfs cp -r dbfs:\/databricks\/mlflow source, and then the same again to upload it to the new workspace) to the new one. And then just reference the location of the data to the experiment like in the following picture:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/emgGs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/emgGs.png\" alt=\"Create Experiment with existing path\" \/><\/a><\/p>\n<p>This is also not working, no run is visible, although the path is already existing.<\/p>\n<p>The next idea was that the registred models are the most important one so at least those should be there and accessible. For that I used the documentation here: <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/model-registry.html\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/model-registry.html<\/a>.<\/p>\n<p>With the following code you get a list of the registred models on the old workspace with the reference on the run_id and location.<\/p>\n<pre><code>from mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\nfor rm in client.list_registered_models():\n    pprint(dict(rm), indent=4)\n<\/code><\/pre>\n<p>And with this code you can add models to a model registry with a reference to the location of the artifact data (on the new workspace):<\/p>\n<pre><code># first the general model must be defined\nclient.create_registered_model(name='MyModel')\n\n# and then the run of the model you want to registre will be added to the model as version one\nclient.create_model_version( name='MyModel', run_id='9fde022012046af935fe52435840cf1', source='dbfs:\/databricks\/mlflow\/experiment_id\/run_id\/artifacts\/model')\n<\/code><\/pre>\n<p>But that did also not worked out. if you go into the Model Registry you get a message like this: <a href=\"https:\/\/i.stack.imgur.com\/Ham4y.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ham4y.png\" alt=\"error message of the registred model\" \/><\/a>.<\/p>\n<p>And I really checked, at the given path (the source) there the data is really uploaded and also a model is existing.<\/p>\n<p>Do you have any new ideas to migrate those models in Databricks?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1599634821840,
        "Question_score":4,
        "Question_tags":"migration|databricks|azure-databricks|mlflow",
        "Question_view_count":1704,
        "Owner_creation_time":1437927419340,
        "Owner_last_access_time":1655189509997,
        "Owner_location":null,
        "Owner_reputation":416,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":90,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63806335",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73469166,
        "Question_title":"Can we print the configurations on which the MLflow server has started?",
        "Question_body":"<p>I am using the following command to start the MLflow server:<\/p>\n<pre><code>mlflow server --backend-store-uri postgresql:\/\/mlflow_user:mlflow@localhost\/mlflow  --artifacts-destination &lt;S3 bucket location&gt; --serve-artifacts  -h 0.0.0.0 -p 8000\n<\/code><\/pre>\n<p>Before production deployment, we have a requirement that we need to print or fetch the under what configurations the server is running. For example, the above command uses localhost postgres connection and S3 bucket.<\/p>\n<p>Is there a way to achieve this?<\/p>\n<p>Also, how do I set the server's environment as &quot;production&quot;? So finally I should see a log like this:<\/p>\n<pre><code>[LOG] Started MLflow server:\nEnv: production\npostgres: localhost:5432\nS3: &lt;S3 bucket path&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1661326653987,
        "Question_score":0,
        "Question_tags":"machine-learning|mlflow|model-management",
        "Question_view_count":17,
        "Owner_creation_time":1484504952247,
        "Owner_last_access_time":1664042089473,
        "Owner_location":"Mumbai, India",
        "Owner_reputation":4433,
        "Owner_up_votes":121,
        "Owner_down_votes":31,
        "Owner_views":885,
        "Question_last_edit_time":1661327087603,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73469166",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72443352,
        "Question_title":"How to save summary of pyGAM using MLflow?",
        "Question_body":"<p>Getting None in return while trying to save output of pyGAM summary function in a variable or file to log it using MLflow .<\/p>\n<pre><code>gam = LinearGAM(s(0, n_splines=20) + s(1) + s(2)+ s(3)+s(4)+s(5)+s(6)+s(7)+s(8)+s(9)+s(10)+s(11)+s(12)+s(13)+s(14)+s(15)+s(16)+s(17)).fit(X_GAM, Y_GAM)\ngam.summary()\noutput = gam.summary()\ntype(output)\n<\/code><\/pre>\n<p><strong>Output:<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Ub0Gs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ub0Gs.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>NoneType<\/p>\n<p>Is there any efficient way to store pyGAM summary output using MLflow?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1653979981097,
        "Question_score":0,
        "Question_tags":"python-3.x|machine-learning|mlflow|pygam",
        "Question_view_count":28,
        "Owner_creation_time":1452602206603,
        "Owner_last_access_time":1663948894330,
        "Owner_location":"Kolkata, West Bengal, India",
        "Owner_reputation":55,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1653981428129,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72443352",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72473826,
        "Question_title":"When to use mlflow.set_tag() vs mlflow.log_params()?",
        "Question_body":"<p>I am confused about the usecase of mlflow.set_tag() vs mlflow.log_params() as both takes key and value pair. Currently, I use mlflow.set_tag() to set tags for data version, code version, etc and mlflow.log_params() to set model training parameters like loss, accuracy, optimizer, etc.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1654161812087,
        "Question_score":1,
        "Question_tags":"machine-learning|mlflow",
        "Question_view_count":77,
        "Owner_creation_time":1473408279707,
        "Owner_last_access_time":1663860023897,
        "Owner_location":"Germany",
        "Owner_reputation":620,
        "Owner_up_votes":49,
        "Owner_down_votes":1,
        "Owner_views":101,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72473826",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69573565,
        "Question_title":"How to configure artifact store of mlflow tracking service to connect to minio S3 using minio STS generated acces_key, secret_key and session_token",
        "Question_body":"<ul>\n<li><p>Minio is configured with LDAP and am generating credentials of user\nwith AssumeRoleWithLDAPIdentity using STS API (<a href=\"https:\/\/docs.min.io\/minio\/baremetal\/security\/ad-ldap-external-identity-management\/AssumeRoleWithLDAPIdentity.html#assumerolewithldapidentity\" rel=\"nofollow noreferrer\">reference<\/a>)<\/p>\n<\/li>\n<li><p>From above values, I'm setting the variables AWS_ACCESS_KEY, AWS_SECRET_KEY, AWS_SESSION_TOKEN (<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#amazon-s3-and-s3-compatible-storage\" rel=\"nofollow noreferrer\">reference<\/a>)<\/p>\n<\/li>\n<\/ul>\n<p>I'm getting error when am trying to push model to mlflow to store in minio artifact<\/p>\n<pre><code>S3UploadFailedError: Failed to upload \/tmp\/tmph68xubhm\/model\/MLmodel to mlflow\/1\/xyz\/artifacts\/model\/MLmodel: An error occurred (InvalidTokenId) when calling the PutObject operation: The security token included in the request is invalid\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1634225910190,
        "Question_score":2,
        "Question_tags":"amazon-s3|minio|mlflow|mlops",
        "Question_view_count":255,
        "Owner_creation_time":1496203946490,
        "Owner_last_access_time":1664009984060,
        "Owner_location":null,
        "Owner_reputation":838,
        "Owner_up_votes":89,
        "Owner_down_votes":2,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69573565",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72239105,
        "Question_title":"MLFlow Webhook calling Azure DevOps pipeline - retrieve body",
        "Question_body":"<p>I am using the MLFlow Webhooks , mentioned <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-registry-webhooks\" rel=\"nofollow noreferrer\">here<\/a>. I am using that to queue an Azure Devops Pipeline.<\/p>\n<p>However, I can't seem to to find a way to retrieve the payload variables inside my pipeline.<\/p>\n<p>E.g. during transition of models, according to the document, such a payload is passed<\/p>\n<pre><code>POST\n\/your\/endpoint\/for\/event\/model-versions\/stage-transition\n--data {\n  &quot;event&quot;: &quot;MODEL_VERSION_TRANSITIONED_STAGE&quot;,\n  &quot;webhook_id&quot;: &quot;c5596721253c4b429368cf6f4341b88a&quot;,\n  &quot;event_timestamp&quot;: 1589859029343,\n  &quot;model_name&quot;: &quot;Airline_Delay_SparkML&quot;,\n  &quot;version&quot;: &quot;8&quot;,\n  &quot;to_stage&quot;: &quot;Production&quot;,\n  &quot;from_stage&quot;: &quot;None&quot;,\n  &quot;text&quot;: &quot;Registered model 'someModel' version 8 transitioned from None to Production.&quot;\n}\n<\/code><\/pre>\n<p>My webhook is created like this:<\/p>\n<pre><code>mlflow_webhook_triggerDevOps={\n  &quot;events&quot;: [&quot;TRANSITION_REQUEST_CREATED&quot;, &quot;REGISTERED_MODEL_CREATED&quot;],\n  &quot;description&quot;: &quot;Integration with Azure DevOps&quot;,\n  &quot;status&quot;: &quot;ACTIVE&quot;,\n  &quot;http_url_spec&quot;: {\n                    &quot;url&quot;: &quot;https:\/\/dev.azure.com\/orgname\/ProjectName\/_apis\/build\/builds?definitionId=742&amp;api-version=6.0&quot;,\n                    &quot;authorization&quot;: &quot;Basic &quot; + base64_message\n                    }\n }\n\nmlflow_createwebhook=requests.post('https:\/\/databricksurl\/api\/2.0\/mlflow\/registry-webhooks\/create', headers=header, proxies=proxies, json=mlflow_webhook_body)\n<\/code><\/pre>\n<p>How do I then retrieve the payload variable e.g. model_name, inside my pipeline definition in Azure Devops?.<\/p>\n<p>I looked at <a href=\"https:\/\/stackoverflow.com\/questions\/50838651\/vsts-use-api-to-set-build-parameters-at-queue-time\">this post<\/a>, but I can't seem to see any payload information (like mentioned above) under the Network-payload tab (or I am not using properly).<\/p>\n<p>Right now, I can trigger the pipeline, but can't seem to find a way to retrieve the payload.<\/p>\n<p>Is it possible? Am I missing something?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1652522121283,
        "Question_score":1,
        "Question_tags":"azure-devops|databricks|webhooks|azure-databricks|mlflow",
        "Question_view_count":125,
        "Owner_creation_time":1428654714763,
        "Owner_last_access_time":1664012257383,
        "Owner_location":null,
        "Owner_reputation":596,
        "Owner_up_votes":53,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72239105",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67472983,
        "Question_title":"Can MLFlow log new metrics in a terminated run?",
        "Question_body":"<p>I would like to use MLFlow (with Python) to log time series with time interval equal to 1 day.\nMy idea would be to create a new run with a certain ID and to use function <code>log_metric<\/code> every day (say, with a cron job) with a new value. Once my run is terminated, can I &quot;reopen&quot; it and log a new metric ?\nWhat I have in mind is:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Day 1\nimport mlflow\n\ntracking_uri = &quot;my_uri&quot;\nmlflow.set_tracking_uri(tracking_uri)\nxp_id = 0\nmlflow.start_run(run_name=&quot;test&quot;, experiment_id=xp_id)\nmlflow.log_metric(&quot;test_metric&quot;, 1)\nmlflow.end_run()\n<\/code><\/pre>\n<p>And the following days:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\ndef log_daily_metric(daily_value_metric):\n  tracking_uri = &quot;my_uri&quot;\n  mlflow.set_tracking_uri(tracking_uri)\n  xp_id = 0\n  mlflow.restart_run(run_name=&quot;test&quot;, experiment_id=xp_id)  # \/!\\ function mlflow.restart does not exist\n  mlflow.log_metric(&quot;test_metric&quot;, daily_value_metric)\n  mlflow.end_run()\n<\/code><\/pre>\n<p>so that run <code>&quot;test&quot;<\/code> would have new metrics logged every day.<\/p>\n<p>Any idea to achieve this ?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1620659322073,
        "Question_score":2,
        "Question_tags":"python-3.x|time-series|mlflow",
        "Question_view_count":274,
        "Owner_creation_time":1539181664153,
        "Owner_last_access_time":1663922812787,
        "Owner_location":"Nice, France",
        "Owner_reputation":1247,
        "Owner_up_votes":690,
        "Owner_down_votes":9,
        "Owner_views":191,
        "Question_last_edit_time":1620662360136,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67472983",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70962095,
        "Question_title":"what is the difference between Duration, TT (Training Time), RunTime on ML Performance Report of mlflow",
        "Question_body":"<p>I compared the performance of machine learning algorithms by applying pycaret and k-fold on a data and reported it on mlflow. There are three time columns in the report, these are duration, TT(training time) and runtime. When I look at these times, they are all different from each other. I know the training time, but what are the other times?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1643831868733,
        "Question_score":0,
        "Question_tags":"machine-learning|classification|mlflow|pycaret",
        "Question_view_count":54,
        "Owner_creation_time":1642027182437,
        "Owner_last_access_time":1648553902727,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1643835272060,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70962095",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70792868,
        "Question_title":"Serving MLFlow artifacts through `--serve-artifacts` without passing credentials",
        "Question_body":"<p>A new version of MLFlow (1.23) provided a <code>--serve-artifacts<\/code> option (via <a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/5045\" rel=\"nofollow noreferrer\">this<\/a> pull request) along with some example code. This <em>should<\/em> allow me to simplify the rollout of a server for data scientists by only needing to give them one URL for the tracking server, rather than a URI for the tracking server, URI for the artifacts server, and a username\/password for the artifacts server. At least, that's how I understand it.<\/p>\n<p>A complication that I have is that I need to use <code>podman<\/code> instead of <code>docker<\/code> for my containers (and without relying on <code>podman-compose<\/code>). I ask that you keep those requirements in mind; I'm aware that this is an odd situation.<\/p>\n<p>What I did before this update (for MLFlow 1.22) was to create a kubernetes play yaml config, and I was successfully able to issue a <code>podman play kube ...<\/code> command to start a pod and from a different machine successfully run an experiment and save artifacts after setting the appropriate four env variables. I've been struggling with getting things working with the newest version.<\/p>\n<p>I am following the <code>docker-compose<\/code> example provided <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/mlflow_artifacts\" rel=\"nofollow noreferrer\">here<\/a>. I am trying a (hopefully) simpler approach. The following is my kubernetes play file defining a pod.<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>apiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: &quot;2022-01-14T19:07:15Z&quot;\n  labels:\n    app: mlflowpod\n  name: mlflowpod\nspec:\n  containers:\n  - name: minio\n    image: quay.io\/minio\/minio:latest\n    ports:\n    - containerPort: 9001\n      hostPort: 9001\n    - containerPort: 9000\n      hostPort: 9000\n    resources: {}\n    tty: true\n    volumeMounts:\n    - mountPath: \/data\n      name: minio-data\n    args:\n    - server\n    - \/data\n    - --console-address\n    - :9001\n\n  - name: mlflow-tracking\n    image: localhost\/mlflow:latest\n    ports:\n    - containerPort: 80\n      hostPort: 8090\n    resources: {}\n    tty: true\n    env:\n      - name: MLFLOW_S3_ENDPOINT_URL\n        value: http:\/\/127.0.0.1:9000\n      - name: AWS_ACCESS_KEY_ID\n        value: minioadmin\n      - name: AWS_SECRET_ACCESS_KEY\n        value: minioadmin\n    command: [&quot;mlflow&quot;]\n    args:\n      - server\n      - -p \n      - 80\n      - --host \n      - 0.0.0.0\n      - --backend-store-uri \n      - sqlite:\/\/\/root\/store.db\n      - --serve-artifacts\n      - --artifacts-destination \n      - s3:\/\/mlflow\n      - --default-artifact-root \n      - mlflow-artifacts:\/\n#      - http:\/\/127.0.0.1:80\/api\/2.0\/mlflow-artifacts\/artifacts\/experiments\n      - --gunicorn-opts \n      - &quot;--log-level debug&quot;\n    volumeMounts:\n    - mountPath: \/root\n      name: mlflow-data  \n\n  volumes:\n  - hostPath:\n      path: .\/minio\n      type: Directory\n    name: minio-data\n  - hostPath:\n      path: .\/mlflow\n      type: Directory\n    name: mlflow-data\nstatus: {}\n<\/code><\/pre>\n<p>I start this with <code>podman play kube mlflowpod.yaml<\/code>. On the same machine (or a different one, it doesn't matter), I have cloned and installed <code>mlflow<\/code> into a virtual environment. From that virtual environment, I set an environmental variable <code>MLFLOW_TRACKING_URI<\/code> to <code>&lt;name-of-server&gt;:8090<\/code>. I then run the <code>example.py<\/code> file in the <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/mlflow_artifacts\" rel=\"nofollow noreferrer\"><code>mlflow_artifacts<\/code><\/a> example directory. I get the following response:<\/p>\n<pre><code>....\nbotocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n<p>Which seems like the client needs the server credentials to minIO, which I thought the proxy was supposed to take care of.<\/p>\n<p>If I also provide the env variables<\/p>\n<pre><code>$env:MLFLOW_S3_ENDPOINT_URL=&quot;http:\/\/&lt;name-of-server&gt;:9000\/&quot; \n$env:AWS_ACCESS_KEY_ID=&quot;minioadmin&quot;\n$env:AWS_SECRET_ACCESS_KEY=&quot;minioadmin&quot;\n<\/code><\/pre>\n<p>Then things work. But that kind of defeats the purpose of the proxy...<\/p>\n<p>What is it about the proxy setup via kubernates play yaml and podman that is going wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1642709711853,
        "Question_score":1,
        "Question_tags":"kubernetes|mlflow|podman",
        "Question_view_count":773,
        "Owner_creation_time":1321286030420,
        "Owner_last_access_time":1642791786807,
        "Owner_location":null,
        "Owner_reputation":428,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":1642710219863,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70792868",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56088195,
        "Question_title":"mlflow can't find .py file",
        "Question_body":"<p>I'm trying to learn to use <code>mlflow<\/code> by creating a very simple project and log it.<\/p>\n\n<p>I've tried following <code>mlflow<\/code>'s example and my code runs properly when running the main.py as a normal bash command.<\/p>\n\n<p>I couldn't make it run using the <code>mlflow<\/code> CLI using project and a simple file.\nI got the following error.<\/p>\n\n<pre><code>(rlearning) yair@pc2016:~\/reinforced_learning101$ mlflow run src\/main.py \n2019\/05\/11 10:21:41 ERROR mlflow.cli: === Could not find main among entry points [] or interpret main as a runnable script. Supported script file extensions: ['.py', '.sh'] ===\n(rlearning) yair@pc2016:~\/reinforced_learning101$ mlflow run .\n2019\/05\/11 10:40:25 INFO mlflow.projects: === Created directory \/tmp\/tmpe26oernf for downloading remote URIs passed to arguments of type 'path' ===\n2019\/05\/11 10:40:25 INFO mlflow.projects: === Running command 'source activate mlflow-21497056aed7961402b515847613ed9f950fa9fc &amp;&amp; python src\/main.py 1.0' in run with ID 'ed51446de4c44903ab891d09cfe10e49' === \nbash: activate: No such file or directory\n2019\/05\/11 10:40:25 ERROR mlflow.cli: === Run (ID 'ed51446de4c44903ab891d09cfe10e49') failed ===\n\n<\/code><\/pre>\n\n<p>Needless to say my main has a <code>.py<\/code> suffix.<\/p>\n\n<p>Is there anything wrong that causes this issue?<\/p>\n\n<p>My main.py is:<\/p>\n\n<pre><code>import sys\n\nimport gym\nimport mlflow\n\n\nif __name__ == '__main__':\n    env = gym.make(\"CartPole-v0\")\n    right_percent = float(sys.argv[1]) if len(sys.argv) &gt; 1 else 1.0\n    with mlflow.start_run():\n        obs = env.reset()\n        print(env.action_space)\n        action = 1  # accelerate right\n        print(obs)\n        mlflow.log_param(\"right percent\", right_percent)\n        mlflow.log_metric(\"mean score\", 1)\n        mlflow.log_metric(\"std score\", 0)\n<\/code><\/pre>\n\n<p>conda_env.yaml<\/p>\n\n<pre><code>name: rlearning\nchannels:\n  - defaults\ndependencies:\n  - python=3.7\n  - numpy\n  - pandas\n  - tensorflow-gpu\n  - pip:\n      - mlflow\n      - gym\n<\/code><\/pre>\n\n<p>MLproject<\/p>\n\n<pre><code>name: reinforced learning\n\nconda_env: files\/config\/conda_environment.yaml\n\nentry_points:\n  main:\n    parameters:\n      right_percent: {type: float, default: 1.0}\n    command: \"python src\/main.py {right_percent}\"\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1557559963290,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":1263,
        "Owner_creation_time":1435222208620,
        "Owner_last_access_time":1662621569283,
        "Owner_location":"London, UK",
        "Owner_reputation":91,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Question_last_edit_time":1557560675392,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56088195",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58917918,
        "Question_title":"How to make predictions using a model that requires an input shape with more than two dimensions using MLflow?",
        "Question_body":"<p>I'm trying to implement a tensorflow (keras) based model into mlflow while learning how it works and if it suite our needs. I'm trying to implement the Fashion MNIST example from tensorflow website <a href=\"https:\/\/www.tensorflow.org\/tutorials\/keras\/classification?hl=it\" rel=\"nofollow noreferrer\">Here the link<\/a><\/p>\n\n<p>I was able to train and to log the model successfully into mlflow using this code:<\/p>\n\n<pre><code>import mlflow\nimport mlflow.tensorflow\nimport mlflow.keras\n\n# TensorFlow and tf.keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Helper libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(tf.__version__)\n\nfashion_mnist = keras.datasets.fashion_mnist\n\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\nclass_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\ntrain_images = train_images \/ 255.0\n\ntest_images = test_images \/ 255.0\n\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28, 28)),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(optimizer='adam',\n          loss='sparse_categorical_crossentropy',\n          metrics=['accuracy'])\n\nif __name__ == \"__main__\":\n\n    model.fit(train_images, train_labels, epochs=10)\n    test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n    print('\\nTest accuracy:', test_acc)\n\n    mlflow.log_metric(\"validation accuracy\", float(test_acc))\n    mlflow.log_metric(\"validation loss\", float(test_loss))\n    mlflow.keras.log_model(model, \n                        \"model\", \n                        registered_model_name = \"Fashion MNIST\")\n<\/code><\/pre>\n\n<p>Then I'm now serving it with the models serve subcommand<\/p>\n\n<pre><code>$ mlflow models serve -m [model_path_here] -p 1234\n<\/code><\/pre>\n\n<p>The problem is that I'm not able to make predictions:<\/p>\n\n<pre><code>fashion_mnist = keras.datasets.fashion_mnist\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\ntrain_images = train_images \/ 255.0\ntest_images = test_images \/ 255.0\nlabels = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\nurl = \"http:\/\/127.0.0.1:1234\/invocations\"\n\nto_predict = test_images[0]\n\ndata = {\n    \"data\": [to_predict.tolist()]\n}\nheaders = {'Content-type': 'application\/json', 'Accept': 'text\/plain'}\nr = requests.post(url, data=json.dumps(data), headers=headers)\nres = r.json()\n<\/code><\/pre>\n\n<p>I'm getting this error:<\/p>\n\n<pre><code>{'error_code': 'BAD_REQUEST', 'message': 'Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.', 'stack_trace': 'Traceback (most recent call last):\\n  File \"\/home\/ferama\/.local\/lib\/python3.6\/site-packages\/mlflow\/pyfunc\/scoring_server\/__init__.py\", line 196, in transformation\\n    raw_predictions = model.predict(data)\\n  File \"\/home\/ferama\/.local\/lib\/python3.6\/site-packages\/mlflow\/keras.py\", line 298, in predict\\n    predicted = pd.DataFrame(self.keras_model.predict(dataframe))\\n  File \"\/home\/ferama\/.local\/lib\/python3.6\/site-packages\/tensorflow_core\/python\/keras\/engine\/training.py\", line 909, in predict\\n    use_multiprocessing=use_multiprocessing)\\n  File \"\/home\/ferama\/.local\/lib\/python3.6\/site-packages\/tensorflow_core\/python\/keras\/engine\/training_arrays.py\", line 715, in predict\\n    x, check_steps=True, steps_name=\\'steps\\', steps=steps)\\n  File \"\/home\/ferama\/.local\/lib\/python3.6\/site-packages\/tensorflow_core\/python\/keras\/engine\/training.py\", line 2472, in _standardize_user_data\\n    exception_prefix=\\'input\\')\\n  File \"\/home\/ferama\/.local\/lib\/python3.6\/site-packages\/tensorflow_core\/python\/keras\/engine\/training_utils.py\", line 564, in standardize_input_data\\n    \\'with shape \\' + str(data_shape))\\nValueError: Error when checking input: expected flatten_input to have 3 dimensions, but got array with shape (1, 28)\\n'}\n<\/code><\/pre>\n\n<p>That code above worked fine with a one dimension model<\/p>\n\n<p>The error seems to me related to the fact that a pandas DataFrame is a two dimensional data structure and the model instead requires a three dimensional input.<\/p>\n\n<p>The latest words from the error \"...but got array with shape (1, 28)\". The input shape should be (1, 28, 28) instead<\/p>\n\n<p>There is a way to use this kind of models with mlflow? There is a way to serialize and send numpy arrays directly as input instead of pandas dataframes?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1574090895563,
        "Question_score":3,
        "Question_tags":"python|tensorflow|keras|mlflow",
        "Question_view_count":1221,
        "Owner_creation_time":1293720910407,
        "Owner_last_access_time":1641627502447,
        "Owner_location":null,
        "Owner_reputation":503,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58917918",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":64354220,
        "Question_title":"Permission denied writing artifacts to an NFS-mounted PVC",
        "Question_body":"<p>I'm attempting to write MLflow artifacts to an NFS-mounted PVC. It's a new PVC mounting at <code>\/opt\/mlflow<\/code>, but MLflow seems to have permission writing to it. The specific error I'm getting is<\/p>\n<pre><code>PermissionError: [Errno 13] Permission denied: '\/opt\/mlflow'\n<\/code><\/pre>\n<p>I ran the same deployment with an S3-backed artifact store, and that worked just fine. That was on my home computer though, and I don't have the ability to do that at work. The MLflow documentation seems to indicate that I don't need any special syntax for NFS mounts.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1602681645387,
        "Question_score":1,
        "Question_tags":"kubernetes|mlflow|kubernetes-pvc",
        "Question_view_count":432,
        "Owner_creation_time":1586788742313,
        "Owner_last_access_time":1607609300903,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1607049416232,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64354220",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62865250,
        "Question_title":"How to serve AI models in a multi-tenant environment in scale?",
        "Question_body":"<p>I have a servers cluster, each server gets real-time authentication events as requests, and returns a risk score for the incoming event, based on AI models that sits in S3.<\/p>\n<p>This cluster serves multiple customers. Each customer has its own AI model in S3.\nEach AI model file in S3 size is ~50MB in size.<\/p>\n<p><strong>The problem:<\/strong><\/p>\n<p>Let's say this cluster consists of 10 servers, and it serves 20 customers. Respectively, there are 20 AI models in S3.<\/p>\n<p>In a naive solution, each server in the cluster might end up loading all the 20 models from S3 to the server memory.\n20(servers in the cluster)*50MB(model size in S3) = 1GB.\nIt takes long time to download the model and load it to memory, and the amount of memory is limited to the memory capacity of the server.\nAnd of course - these problems get bigger with scale.<\/p>\n<p>So what are my options?\nI know that there are out of the box products for model life cycle management, such as: MlFlow, KubeFlow, ...\nDo these products have a solution to the problem I raised?<\/p>\n<p>Maybe use Redis as a cache layer?<\/p>\n<p>Maybe use Redis as a cache layer in combination with MlFlow and KubeFlow?<\/p>\n<p>Any other solution?<\/p>\n<p><strong>Limitation:<\/strong>\nI can't have sticky session between the servers in that cluster, so I can't ensure all the requests of the same customer will end up in the same server.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1594579872523,
        "Question_score":0,
        "Question_tags":"artificial-intelligence|scalability|mlflow|kubeflow",
        "Question_view_count":504,
        "Owner_creation_time":1289286902413,
        "Owner_last_access_time":1663763347283,
        "Owner_location":null,
        "Owner_reputation":1806,
        "Owner_up_votes":104,
        "Owner_down_votes":3,
        "Owner_views":148,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62865250",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68501612,
        "Question_title":"MLflow saves models to relative place instead of tracking_uri",
        "Question_body":"<p>sorry if my question is too basic, but cannot solve it.\nI am experimenting with mlflow currently and facing the following issue:<\/p>\n<p>Even if I have set the <em>tracking_uri<\/em>, the mlflow artifacts are saved to the <em>.\/mlruns\/...<\/em> folder relative to the path from where I run <code>mlfow run path\/to\/train.py<\/code> (in command line). The mlflow server searches for the artifacts following the <em>tracking_uri<\/em> (<code>mlflow server --default-artifact-root here\/comes\/the\/same\/tracking_uri<\/code>).<\/p>\n<p>Through the following example it will be clear what I mean:<\/p>\n<p>I set the following in the training script before the <code>with mlflow.start_run() as run:<\/code><\/p>\n<pre><code>mlflow.set_tracking_uri(&quot;file:\/\/\/home\/@myUser\/@SomeFolders\/mlflow_artifact_store\/mlruns\/&quot;)\n<\/code><\/pre>\n<p>My expectation would be that mlflow saves all the artifacts to the place I gave in the registry uri. Instead, it saves the artifacts relative to place from where I run <code>mlflow run path\/to\/train.py<\/code>, i.e. running the following<\/p>\n<pre><code>\/home\/@myUser\/ mlflow run path\/to\/train.py\n<\/code><\/pre>\n<p>creates the structure:<\/p>\n<pre><code>\/home\/@myUser\/mlruns\/@experimentID\/@runID\/artifacts\n\/home\/@myUser\/mlruns\/@experimentID\/@runID\/metrics\n\/home\/@myUser\/mlruns\/@experimentID\/@runID\/params\n\/home\/@myUser\/mlruns\/@experimentID\/@runID\/tags\n<\/code><\/pre>\n<p>and therefore it doesn't find the run artifacts in the tracking_uri, giving the error message:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;train.py&quot;, line 59, in &lt;module&gt;\n    with mlflow.start_run() as run:\n  File &quot;\/home\/@myUser\/miniconda3\/envs\/mlflow-ff56d6062d031d43990effc19450800e72b9830b\/lib\/python3.6\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 204, in start_run\n    active_run_obj = client.get_run(existing_run_id)\n  File &quot;\/home\/@myUser\/miniconda3\/envs\/mlflow-ff56d6062d031d43990effc19450800e72b9830b\/lib\/python3.6\/site-packages\/mlflow\/tracking\/client.py&quot;, line 151, in get_run\n    return self._tracking_client.get_run(run_id)\n  File &quot;\/home\/@myUser\/miniconda3\/envs\/mlflow-ff56d6062d031d43990effc19450800e72b9830b\/lib\/python3.6\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 57, in get_run\n    return self.store.get_run(run_id)\n  File &quot;\/home\/@myUser\/miniconda3\/envs\/mlflow-ff56d6062d031d43990effc19450800e72b9830b\/lib\/python3.6\/site-packages\/mlflow\/store\/tracking\/file_store.py&quot;, line 524, in get_run\n    run_info = self._get_run_info(run_id)\n  File &quot;\/home\/@myUser\/miniconda3\/envs\/mlflow-ff56d6062d031d43990effc19450800e72b9830b\/lib\/python3.6\/site-packages\/mlflow\/store\/tracking\/file_store.py&quot;, line 544, in _get_run_info\n    &quot;Run '%s' not found&quot; % run_uuid, databricks_pb2.RESOURCE_DOES_NOT_EXIST\nmlflow.exceptions.MlflowException: Run '788563758ece40f283bfbf8ba80ceca8' not found\n2021\/07\/23 16:54:16 ERROR mlflow.cli: === Run (ID '788563758ece40f283bfbf8ba80ceca8') failed ===\n<\/code><\/pre>\n<p>Why is that so? How can I change the place where the artifacts are stored, this directory structure is created? I have tried <code>mlflow run --storage-dir here\/comes\/the\/path<\/code>, setting the <em>tracking_uri<\/em>, <em>registry_uri<\/em>. If I run the <code>\/home\/path\/to\/tracking\/uri mlflow run path\/to\/train.py<\/code> it works, but I need to run the scripts remotely.<\/p>\n<p>My endgoal would be to change the artifact uri to an NFS drive, but even in my local computer I cannot do the trick.<\/p>\n<p>Thanks for reading it, even more thanks if you suggest a solution! :)\nHave a great day!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1627054156107,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":785,
        "Owner_creation_time":1512572031753,
        "Owner_last_access_time":1655968570680,
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68501612",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59827478,
        "Question_title":"How to get current run_id inside of mlflow.start_run()?",
        "Question_body":"<p><code>mlflow.active_run()<\/code> returns nothing so I can't just use\n<code>current_rui_id = mlflow.active_run().info.run_id<\/code><\/p>\n\n<p>I have to get <strong>run_id<\/strong> inside of this construction for being able to continue logging parameters,  metrics and artifacts inside of another block but for the same model:<\/p>\n\n<pre><code>with mlflow.start_run(run_name=\"test_ololo\"):\n\n    \"\"\" \n       fitting a model here ...\n    \"\"\"\n\n    for name, val in metrics:\n        mlflow.log_metric(name, np.float(val))\n\n    # Log our parameters into mlflow\n    for k, v in params.items():\n        mlflow.log_param(key=k, value=v)\n\n    pytorch.log_model(learn.model, f'model')\n    mlflow.log_artifact('.\/outputs\/fig.jpg')\n<\/code><\/pre>\n\n<p>I have to get current <strong>run_id<\/strong> to continue training inside the same run<\/p>\n\n<pre><code>with mlflow.start_run(run_id=\"215d3a71925a4709a9b694c45012988a\"):\n\n    \"\"\"\n       fit again\n       log_metrics\n    \"\"\"\n\n    pytorch.log_model(learn.model, f'model')\n    mlflow.log_artifact('.\/outputs\/fig2.jpg')\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1579537993127,
        "Question_score":4,
        "Question_tags":"python|mlflow",
        "Question_view_count":6042,
        "Owner_creation_time":1443627469260,
        "Owner_last_access_time":1663849445573,
        "Owner_location":null,
        "Owner_reputation":863,
        "Owner_up_votes":100,
        "Owner_down_votes":2,
        "Owner_views":22,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59827478",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67677780,
        "Question_title":"MLFlow run: Pass parameters in a file instead of key\/value pairs",
        "Question_body":"<p>Usually when running an MLProject, I would use something similar to:<\/p>\n<pre><code>mlflow run . -P alpha=0.1 -P l1_ratio=0.9\n<\/code><\/pre>\n<p>Is it possible to pass a file containing the key\/value pairs instead ? so something like:<\/p>\n<pre><code>mlflow run . --file .\/parametrs\n<\/code><\/pre>\n<p>where .\/parameters contains the key\/value pairs (like an env file or something)<\/p>\n<p>One way I thought of is to make a seperate bash script that accept the file and extracts the key\/value pairs to be included in the run command, but I wonder if there's a way more native to mlflow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1621883888327,
        "Question_score":1,
        "Question_tags":"python|machine-learning|mlflow|mlops",
        "Question_view_count":222,
        "Owner_creation_time":1540654775053,
        "Owner_last_access_time":1663859840653,
        "Owner_location":"Tunisia",
        "Owner_reputation":606,
        "Owner_up_votes":42,
        "Owner_down_votes":8,
        "Owner_views":69,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It's not supported functionality according to <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#mlflow-run\" rel=\"nofollow noreferrer\">documentation<\/a>, and <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/cli.py#L124\" rel=\"nofollow noreferrer\">source code<\/a>, so you'll need to add your own wrapper to read parameters from file &amp; pass them explicitly.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1621931875960,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67677780",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65992591,
        "Question_title":"send post request using curl to mlflow api to multiple records",
        "Question_body":"<p>I have served an mlflow model and am sending POST requests in this format:<\/p>\n<pre><code>curl -X POST -H &quot;Content-Type:application\/json; format=pandas-split&quot; \n--data '{&quot;columns&quot;:[&quot;alcohol&quot;, &quot;chlorides&quot;, &quot;citric acid&quot;, &quot;density&quot;, \n&quot;fixed acidity&quot;, &quot;free sulfur dioxide&quot;, &quot;pH&quot;, &quot;residual sugar&quot;, &quot;sulphates&quot;, \n&quot;total sulfur dioxide&quot;, &quot;volatile acidity&quot;],&quot;data&quot;:[[12.8, 0.029, 0.48, 0.98, \n6.2, 29, 3.33, 1.2, 0.39, 75, 0.66]]}' \nhttp:\/\/127.0.0.1:1234\/invocations\n<\/code><\/pre>\n<p>It is getting scored. However for my particular project, the input to rest api for scoring will always be multiple records in dataframe\/csv format instead of a single record. Can someone point me to how to achieve this ?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1612181841393,
        "Question_score":1,
        "Question_tags":"curl|post|deployment|mlflow|serving",
        "Question_view_count":407,
        "Owner_creation_time":1573739890560,
        "Owner_last_access_time":1663922252563,
        "Owner_location":null,
        "Owner_reputation":115,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65992591",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70005957,
        "Question_title":"Logging the git_sha as a parameter on Mlflow using Kedro hooks",
        "Question_body":"<p>I would like to log the git_sha parameter on Mlflow as shown in the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/02_hooks.html?highlight=run_params#add-metrics-tracking-to-your-model\" rel=\"nofollow noreferrer\">documentation<\/a>. What appears to me here, is that simply running the following portion of code should be enough to get git_sha logged in the Mlflow UI. Am I right ?<\/p>\n<pre><code>@hook_impl\n    def before_pipeline_run(self, run_params: Dict[str, Any]) -&gt; None:\n        &quot;&quot;&quot;Hook implementation to start an MLflow run\n        with the same run_id as the Kedro pipeline run.\n        &quot;&quot;&quot;\n        mlflow.start_run(run_name=run_params[&quot;run_id&quot;])\n        mlflow.log_params(run_params)\n<\/code><\/pre>\n<p>But this does not work as I get all but the git_sha parameter. And when I look at the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/_modules\/kedro\/framework\/hooks\/specs.html?highlight=run_params#\" rel=\"nofollow noreferrer\">hooks specs<\/a>, it seems that this param is not part of run_params (anymore?)<\/p>\n<p>Is there a way I could get the git sha (maybe from the context journal ?) and add it to the logged parameters ?<\/p>\n<p>Thank you in advance !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637158421443,
        "Question_score":0,
        "Question_tags":"python|mlflow|kedro|mlops",
        "Question_view_count":172,
        "Owner_creation_time":1586517832390,
        "Owner_last_access_time":1660328393330,
        "Owner_location":null,
        "Owner_reputation":127,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":1637158766987,
        "Answer_body":"<p>Whilst it's heavily encouraged to use git with Kedro it's not required and as such no part of Kedro (except <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro-starters\" rel=\"nofollow noreferrer\">kedro-starters<\/a> if we're being pedantic) is 'aware' of git.<\/p>\n<p>In your <code>before_pipeline_hook<\/code> there it is pretty easy for you to retrieve the info <a href=\"https:\/\/stackoverflow.com\/questions\/14989858\/get-the-current-git-hash-in-a-python-script\">via the techniques documented here<\/a>. It seems trivial for the whole codebase, a bit more involved if you want to say provide pipeline specific hashes.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1637159193836,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70005957",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61427012,
        "Question_title":"can I use mlflow python API to register a spark UDF & then use the UDF in Spark scala code?",
        "Question_body":"<p>I'm trying to use mlflow to do the machine learning work. I register the ML model as UDF using the following python code. The question is how can I use the UDF(test_predict) in my scala code? The reason is that our main code is in Scala. The problem is that UDF created below is a temporary UDF and SparkSession scoped. thanks!<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport mlflow\nfrom mlflow import pyfunc\nimport numpy as np\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark import SQLContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql.types import *\n\nsc=SparkContext()\nspark = SparkSession.builder.appName(\"Python UDF example\").getOrCreate()\npyfunc_udf=mlflow.pyfunc.spark_udf(spark=spark, model_uri=\".\/sk\",result_type=\"float\")\nspark.udf.register(\"test_predict\",pyfunc_udf)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1587824105457,
        "Question_score":2,
        "Question_tags":"python|apache-spark|pyspark|user-defined-functions|mlflow",
        "Question_view_count":617,
        "Owner_creation_time":1553506414607,
        "Owner_last_access_time":1614866413497,
        "Owner_location":null,
        "Owner_reputation":129,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":47,
        "Question_last_edit_time":1588065231427,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61427012",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":57980954,
        "Question_title":"when using python open() on mac, getting \"[Errno 2] no such file or directory: 'file:\/\/\/absolute\/path\/' \", path generated from MLflow",
        "Question_body":"<p>I imagine this has been answered, but I could not find.<\/p>\n\n<p>I am attempting to use:<\/p>\n\n<pre><code>with open('file:\/\/\/absolute\/path\/to\/file') as fn:\n     csv-stuff\n<\/code><\/pre>\n\n<p>however, I am getting Error : <\/p>\n\n<blockquote>\n  <p>\"[Errno 2] No such file or directory: 'file:\/\/\/absolute\/path\/to\/file'\"<\/p>\n<\/blockquote>\n\n<p>The absolute file path is being generated within the following commands from os and mlflow:<\/p>\n\n<pre><code>data_uri = os.path.join(run.info.artifact_uri, \"data\")\n<\/code><\/pre>\n\n<p>where \"data\" was logged without a artifact_path specified with:<\/p>\n\n<pre><code>mlflow.log_artifact(tempfile_path,\"data\")\n<\/code><\/pre>\n\n<p>I have been stuck on this for a few days and have not figured out the issue yet. Thanks for the help!<\/p>\n\n<p>P.S. This is my first post, feel free to tell me if I am doing something wrong. Thanks.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1568748928353,
        "Question_score":1,
        "Question_tags":"python-3.x|macos|mlflow|nosuchfileexception",
        "Question_view_count":30,
        "Owner_creation_time":1551311642383,
        "Owner_last_access_time":1643120523147,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1568750990320,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57980954",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60616879,
        "Question_title":"Logging Artifacts from MlFlow on GCS Bucket",
        "Question_body":"<p>I have a running MlFlow server on GCS VM instance. I have created a bucket to log the artifacts.\nThis is the command I'm running to start the server and for specifying bucket path-<\/p>\n\n<pre><code>mlflow server --default-artifact-root gs:\/\/gcs_bucket\/artifacts --host x.x.x.x\n<\/code><\/pre>\n\n<p>But facing this error:<\/p>\n\n<pre><code>TypeError: stat: path should be string, bytes, os.PathLike or integer, not ElasticNet\n<\/code><\/pre>\n\n<p>Note- The mlflow server is running fine with the specified host alone. The problem is in the way when I'm specifying the storage bucket path.\nI have given permission of storage api by using these commands:<\/p>\n\n<pre><code>gcloud auth application-default login\ngcloud auth login\n<\/code><\/pre>\n\n<p>Also, on printing the artifact URI, this is what I'm getting:<\/p>\n\n<pre><code>mlflow.get_artifact_uri()\n<\/code><\/pre>\n\n<p>Output:<\/p>\n\n<pre><code>gs:\/\/gcs_bucket\/artifacts\/0\/122481bf990xxxxxxxxxxxxxxxxxxxxx\/artifacts\n<\/code><\/pre>\n\n<p>So in the above path from where this is coming <code>0\/122481bf990xxxxxxxxxxxxxxxxxxxxx\/artifacts<\/code> and why it's not getting auto-created at <code>gs:\/\/gcs_bucket\/artifacts<\/code><\/p>\n\n<p>After debugging more, why it's not able to get the local path from VM:\n<a href=\"https:\/\/i.stack.imgur.com\/ubDU0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ubDU0.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And this error I'm getting on VM:<\/p>\n\n<pre><code>ARNING:root:Malformed experiment 'mlruns'. Detailed error Yaml file '.\/mlruns\/mlruns\/meta.yaml' does not exist.\nTraceback (most recent call last):\n File \"\/usr\/local\/lib\/python3.6\/dist-packages\/mlflow\/store\/tracking\/file_store.py\", line 197, in list_experiments\n   experiment = self._get_experiment(exp_id, view_type)\n File \"\/usr\/local\/lib\/python3.6\/dist-packages\/mlflow\/store\/tracking\/file_store.py\", line 256, in _get_experiment\n   meta = read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n File \"\/usr\/local\/lib\/python3.6\/dist-packages\/mlflow\/utils\/file_utils.py\", line 160, in read_yaml\n   raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\nmlflow.exceptions.MissingConfigException: Yaml file '.\/mlruns\/mlruns\/meta.yaml' does not exist.\n<\/code><\/pre>\n\n<p>Can I get a solution to this and what I'm missing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1583840420677,
        "Question_score":4,
        "Question_tags":"python|google-cloud-platform|google-cloud-storage|mlflow",
        "Question_view_count":2153,
        "Owner_creation_time":1451124057623,
        "Owner_last_access_time":1664026764720,
        "Owner_location":"India",
        "Owner_reputation":736,
        "Owner_up_votes":69,
        "Owner_down_votes":2,
        "Owner_views":234,
        "Question_last_edit_time":1583922371776,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60616879",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56791931,
        "Question_title":"Not able to save pyspark iforest model using pyspark",
        "Question_body":"<p>Using iforest as described here : <a href=\"https:\/\/github.com\/titicaca\/spark-iforest\" rel=\"nofollow noreferrer\">https:\/\/github.com\/titicaca\/spark-iforest<\/a>\nBut model.save() is throwing exception.<\/p>\n\n<p>Followed the code snippet mentioned under \"Python API\" section on mentioned git page.<\/p>\n\n<p>from pyspark.ml.feature import VectorAssembler\nimport os\nimport tempfile\nfrom pyspark_iforest.ml.iforest import *<\/p>\n\n<p>col_1:integer\ncol_2:integer\ncol_3:integer<\/p>\n\n<p>assembler = VectorAssembler(inputCols=in_cols, outputCol=\"features\")\nfeaturized = assembler.transform(df)<\/p>\n\n<p>iforest = IForest(contamination=0.5, maxDepth=2)\nmodel=iforest.fit(df)<\/p>\n\n<p>model.save(\"model_path\")\nException:\nscala.NotImplementedError: The default jsonEncode only supports string, vector and matrix. org.apache.spark.ml.param.Param must override jsonEncode for java.lang.Double.<\/p>\n\n<p>Below is the output dataframe I'm getting after executing \"model.transform(df)\". model.save() should be able to save model-files.\ncol_1:integer\ncol_2:integer\ncol_3:integer\nfeatures:udt\nanomalyScore:double\nprediction:double<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1561641462687,
        "Question_score":0,
        "Question_tags":"machine-learning|pyspark|mlflow",
        "Question_view_count":372,
        "Owner_creation_time":1433235101097,
        "Owner_last_access_time":1663928002773,
        "Owner_location":"India",
        "Owner_reputation":779,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":132,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56791931",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73368271,
        "Question_title":"Pycaret MlFlow authentication",
        "Question_body":"<p>How can I use <code>log_environment = True<\/code> in Pycaret <code>setup<\/code> with<\/p>\n<p><code> import os import mlflow mlflow.set_tracking_uri(&quot;https:\/\/dagshub.com\/BexTuychiev\/pet_pawpularity.mlflow&quot;) os.environ[&quot;MLFLOW_TRACKING_USERNAME&quot;] = &quot;MLFLOW_TRACKING_USERNAME&quot; os.environ[&quot;MLFLOW_TRACKING_PASSWORD&quot;] = &quot;MLFLOW_TRACKING_PASSWORD&quot;<\/code><\/p>\n<p>Without getting  <code>RestException: INTERNAL_ERROR: Response: {'error': 'not found'} <\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660618090423,
        "Question_score":0,
        "Question_tags":"google-colaboratory|mlflow|pycaret|dagshub",
        "Question_view_count":35,
        "Owner_creation_time":1481730456640,
        "Owner_last_access_time":1663945777720,
        "Owner_location":"Kansas",
        "Owner_reputation":675,
        "Owner_up_votes":54,
        "Owner_down_votes":0,
        "Owner_views":1816,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73368271",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73643715,
        "Question_title":"ValueError: Enum ErrorCode has no value defined for name '403' in mlflow.set_experiment()",
        "Question_body":"<p>I am trying to run some code to train a model, while logging my results to MLflow on Databricks. I keep getting the following error when I try to make a call to <code>mlflow.set_experiment()<\/code>,<\/p>\n<pre><code>    raise ValueError('Enum {} has no value defined for name {!r}'.format(\nValueError: Enum ErrorCode has no value defined for name '403'\n<\/code><\/pre>\n<p>What exactly is going on here?<\/p>\n<p>I am using Databricks Connect to run my code and the section where the error pops up looks like this,<\/p>\n<pre><code>    # set remote tracking server URI\n    mlflow.set_tracking_uri(remote_server_uri)\n\n    # create the MLflow client\n    client = MlflowClient(remote_server_uri)\n\n    # set experiment to log mlflow runs\n    mlflow.set_experiment(experiment_name)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1662611126990,
        "Question_score":1,
        "Question_tags":"python|mlflow|databricks-connect",
        "Question_view_count":68,
        "Owner_creation_time":1521856385820,
        "Owner_last_access_time":1664037995903,
        "Owner_location":"Sri Lanka",
        "Owner_reputation":820,
        "Owner_up_votes":389,
        "Owner_down_votes":1,
        "Owner_views":165,
        "Question_last_edit_time":1662625313963,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73643715",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61871515,
        "Question_title":"MLflow Artifacts Storing But Not Listing In UI",
        "Question_body":"<p>I've run into an issue using MLflow server. When I first ran the command to start an mlflow server on an ec2 instance, everything worked fine. Now, although logs and artifacts are being stored to postgres and s3, the UI is not listing the artifacts. Instead, the artifact section of the UI shows:<\/p>\n\n<pre><code>Loading Artifacts Failed\nUnable to list artifacts stored under &lt;s3-location&gt; for the current run. Please contact your tracking server administrator to notify them of this error, which can happen when the tracking server lacks permission to list artifacts under the current run's root artifact directory.\n<\/code><\/pre>\n\n<p>But when I check in s3, I see the artifact in the s3 location that the error shows. What could possibly have started causing this as it used to work not too long ago and nothing was changed on the ec2 that is hosting mlflow?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1589810994053,
        "Question_score":9,
        "Question_tags":"amazon-s3|amazon-ec2|artifact|mlflow",
        "Question_view_count":3949,
        "Owner_creation_time":1417012835813,
        "Owner_last_access_time":1628782664573,
        "Owner_location":null,
        "Owner_reputation":945,
        "Owner_up_votes":35,
        "Owner_down_votes":1,
        "Owner_views":148,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61871515",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63640567,
        "Question_title":"Error using set MLFLOW_TRACKING_URI='http:\/\/0.0.0.0:5000' for serve models",
        "Question_body":"<p>Hi i need to run a command like this<\/p>\n<pre><code>mlflow server --backend-store-uri postgresql:\/\/mlflow_user:mlflow@localhost:5433\/mlflow --default-artifact-root file:D:\/artifact_root --host 0.0.0.0 --port 5000\n<\/code><\/pre>\n<p>for start my serve and i have not problem with this but when i try to run a example\nin the route of project from github python<\/p>\n<pre><code>mlflow\/examples\/sklearn_elasticnet_diabetes\/linux\/train_diabetes.py 0.1 0.9 \n<\/code><\/pre>\n<p>i get this error<\/p>\n<pre><code>  _model_registry_store_registry.register_entrypoints()\nElasticnet model (alpha=0.100000, l1_ratio=0.900000):\n  RMSE: 71.98302888908191\n  MAE: 60.5647520017933\n  R2: 0.21655161434654602\n&lt;function get_tracking_uri at 0x0000017F3AE885E8&gt;\nurl 'http:\/\/0.0.0.0:8001'\nurl2 'http|\/\/0.0.0.0|8001'\nTraceback (most recent call last):\n  File &quot;train_diabetes.py&quot;, line 90, in &lt;module&gt;\n    mlflow.log_param(&quot;alpha&quot;, alpha)\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\tracking\\fluent.py&quot;, line 210, in log_param\n    run_id = _get_or_start_run().info.run_id\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\tracking\\fluent.py&quot;, line 508, in _get_or_start_run\n    return start_run()\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\tracking\\fluent.py&quot;, line 148, in start_run\n    active_run_obj = MlflowClient().create_run(\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\tracking\\client.py&quot;, line 44, in __init__\n    self._tracking_client = TrackingServiceClient(final_tracking_uri)\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py&quot;, line 32, in __init__       \n    self.store = utils._get_store(self.tracking_uri)\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py&quot;, line 126, in _get_store     \n    return _tracking_store_registry.get_store(store_uri, artifact_uri)\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\registry.py&quot;, line 37, in get_store    \n    return builder(store_uri=store_uri, artifact_uri=artifact_uri)\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py&quot;, line 81, in _get_file_store \n    return FileStore(store_uri, store_uri)\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py&quot;, line 100, in __init__\n    self.root_directory = local_file_uri_to_path(root_directory or _default_root_dir())\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\site-packages\\mlflow\\utils\\file_utils.py&quot;, line 387, in local_file_uri_to_path\n    return urllib.request.url2pathname(path)\n  File &quot;C:\\Users\\kevin.sanchez\\Miniconda3\\envs\\env_mlflow\\lib\\nturl2path.py&quot;, line 35, in url2pathname\n    raise OSError(error)\nOSError: Bad URL: 'http|\/\/0.0.0.0|8001'\n<\/code><\/pre>\n<p>before running the python code i run this command to set the env tracking uri for the execution set MLFLOW_TRACKING_URI='http:\/\/0.0.0.0:5000'<\/p>\n<p>i don\u00b4t know why mlflow replace the : for | i need help. Before this option worked but now it is failing<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1598646868280,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":895,
        "Owner_creation_time":1598646307000,
        "Owner_last_access_time":1603930318483,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1598647185303,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63640567",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72516775,
        "Question_title":"I expose ports in docker but it does not connect",
        "Question_body":"<p>I have a simple docker image (I post the Dockerfile at the end) and I run it with<\/p>\n<pre><code>docker run -p 8888:8888 -p 5000:5000 -v $(pwd):\/workfolder -it --rm stockpred\n<\/code><\/pre>\n<p>I am expecting to expose the ports 8888 and 5000.<\/p>\n<p>Inside the container I do:<\/p>\n<pre><code>(base) root@41131b74043f:\/workfolder# mlflow ui\n[2022-06-06 10:59:24 +0000] [26] [INFO] Starting gunicorn 20.1.0\n[2022-06-06 10:59:24 +0000] [26] [INFO] Listening at: http:\/\/127.0.0.1:5000 (26)\n[2022-06-06 10:59:24 +0000] [26] [INFO] Using worker: sync\n[2022-06-06 10:59:24 +0000] [27] [INFO] Booting worker with pid: 27\n<\/code><\/pre>\n<p>so I go and open that address in my browser but I got<\/p>\n<p>The connection was reset<\/p>\n<blockquote>\n<p>The connection to the server was reset while the page was loading.<\/p>\n<pre><code>The site could be temporarily unavailable or too busy. Try again in a few moments.\nIf you are unable to load any pages, check your computer\u2019s network connection.\nIf your computer or network is protected by a firewall or proxy, make sure that Firefox is permitted to access the Web.\n<\/code><\/pre>\n<\/blockquote>\n<p>I thought that I could open the page externally. It must be something I am missing but what am I doing wrong?<\/p>\n<pre><code>FROM continuumio\/miniconda3\n\nRUN pip install mlflow&gt;=1.18.0 \\\n    &amp;&amp; pip install numpy \\\n    &amp;&amp; pip install scipy \\\n    &amp;&amp; pip install pandas \\\n    &amp;&amp; pip install scikit-learn \\\n    &amp;&amp; pip install cloudpickle \\\n    &amp;&amp; pip install pandas_datareader==0.10.0 \\\n    &amp;&amp; pip install yfinance\n<\/code><\/pre>\n<p>EDIT:<\/p>\n<p>It worked when I did<\/p>\n<pre><code>docker run --network=&quot;host&quot; -p 8888:8888 -p 5000:5000 -v $(pwd):\/workfolder -it --rm stockpred\n<\/code><\/pre>\n<p>Notice that I did not expose the ports in the Dockerfile.<\/p>\n<p>Can someone explain me why this is working like this?<\/p>\n<p>(I also tried exposing the ports in the Dockerfile and running like originally but it didn't work)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1654513786370,
        "Question_score":0,
        "Question_tags":"docker|mlflow",
        "Question_view_count":121,
        "Owner_creation_time":1421198269333,
        "Owner_last_access_time":1664010554427,
        "Owner_location":null,
        "Owner_reputation":5585,
        "Owner_up_votes":792,
        "Owner_down_votes":53,
        "Owner_views":1350,
        "Question_last_edit_time":1654516610190,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72516775",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":57033896,
        "Question_title":"How to use MLfLow with private git repositories?",
        "Question_body":"<p>I tested <code>MLflow<\/code> experiment when the source code is stored in public a git repository. Example command looks like this<\/p>\n\n<pre><code>mlflow run  https:\/\/github.com\/amesar\/mlflow-fun.git#examples\/hello_world \\\n --experiment-id=2019 \\\n -Palpha=100 -Prun_origin=GitRun -Plog_artifact=True\n<\/code><\/pre>\n\n<p>However, when I provide an internal (private) git repository link instead of public- MLflow redirects to login url, and then execution fails like this.<\/p>\n\n<pre><code>git.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\ncmdline: git fetch -v origin\nstderr: 'fatal: unable to update url base from redirection:\nasked for: https:\/\/gitlab-master.companyname.com\/myusername\/project_name\n\/tree\/master\/models\/myclassifier\/info\/refs?service=git-upload-pack\nredirect: https:\/\/gitlab-master.company.com\/users\/sign_in'\n<\/code><\/pre>\n\n<p>Is there any way to commmunicate credentials of git account to MLflow?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1563170204117,
        "Question_score":2,
        "Question_tags":"git|gitlab|databricks|mlflow",
        "Question_view_count":1278,
        "Owner_creation_time":1401104228227,
        "Owner_last_access_time":1663944731387,
        "Owner_location":"Santa Clara, CA, USA",
        "Owner_reputation":4031,
        "Owner_up_votes":79,
        "Owner_down_votes":6,
        "Owner_views":244,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57033896",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62686181,
        "Question_title":"MLflow - How can I run python code using a REST API",
        "Question_body":"<p>I'am a newbie on MachineLearning. Just a simple question, how can I run python code using REST API?\nHere is the documentation\n<a href=\"https:\/\/mlflow.org\/docs\/latest\/rest-api.html\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/rest-api.html<\/a>\nBut there are no examples for REST API.\nI just created an experiment, but I cant create python run?\nAny examples like this? &quot;This is create just a experiment&quot;\ncurl -X POST http:\/\/localhost:5000\/api\/2.0\/preview\/mlflow\/experiments\/create -d '{&quot;name&quot;:&quot;TEST&quot;}'<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1593642122807,
        "Question_score":2,
        "Question_tags":"machine-learning|artificial-intelligence|mlflow",
        "Question_view_count":494,
        "Owner_creation_time":1593641319837,
        "Owner_last_access_time":1663184903993,
        "Owner_location":"Turkey",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1593642521280,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62686181",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56604989,
        "Question_title":"How to install mlflow using pip install",
        "Question_body":"<p>I'm working on a Window 10 machine and trying to pip install mlflow but I'm getting the following error message.<\/p>\n\n<pre><code>THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\nmlflow from https:\/\/files.pythonhosted.org\/packages\/01\/ec\/8c9448968d4662e8354b9c3a62e635f8929ed507a45af3d9fdb84be51270\/mlflow-1.0.0-py3-none-any.whl#sha256=0f2f116a377b9da538642eaf688caa0a7166ee1ede30c8734830eb9e789574b4:\n    Expected sha256 0f2f116a377b9da538642eaf688caa0a7166ee1ede30c8734830eb9e789574b4\n         Got        eb34ea16ecfe02d474ce50fd1f88aba82d56dcce9e8fdd30193ab39edf32ac9e\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1560545573400,
        "Question_score":1,
        "Question_tags":"pip|mlflow",
        "Question_view_count":365,
        "Owner_creation_time":1355343131933,
        "Owner_last_access_time":1649125560750,
        "Owner_location":null,
        "Owner_reputation":5655,
        "Owner_up_votes":73,
        "Owner_down_votes":3,
        "Owner_views":629,
        "Question_last_edit_time":1560799785056,
        "Answer_body":"<p>It is trying to check cache for packages. They were likely compiled in linux or some other OS and you are trying to install them in Windows.<\/p>\n\n<p>This should fix your issue:<\/p>\n\n<pre><code>pip install --no-cache-dir mlflow\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1560545869140,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56604989",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65744496,
        "Question_title":"MLFlow Model Registry ENDPOINT_NOT_FOUND: No API found for ERROR",
        "Question_body":"<p>I'm currently using MLFlow in Azure Databricks and trying to load a model from the Model Registry. Currently referencing the version, but will want to reference the stage 'Production' (I get the same error when referencing the stage as well)<\/p>\n<p>I keep encountering an error:<\/p>\n<pre><code>ENDPOINT_NOT_FOUND: No API found for 'POST \/mlflow\/model-versions\/get-download-uri'\n<\/code><\/pre>\n<p>My artifacts are stored in the dbfs filestore.<\/p>\n<p>I have not been able to identify why this is happening.<\/p>\n<p>Code:<\/p>\n<pre><code>from mlflow.tracking.client import MlflowClient\nfrom mlflow.entities.model_registry.model_version_status import ModelVersionStatus\n\nimport mlflow.pyfunc\n\nmodel_name = &quot;model_name&quot;\n\nmodel_version_uri = &quot;models:\/{model_name}\/4&quot;.format(model_name=model_name)\n\nprint(&quot;Loading registered model version from URI: '{model_uri}'&quot;.format(model_uri=model_version_uri))\nmodel_version_4 = mlflow.pyfunc.load_model(model_version_uri)\n\nmodel_production_uri = &quot;models:\/{model_name}\/production&quot;.format(model_name=model_name)\n\nprint(&quot;Loading registered model version from URI: '{model_uri}'&quot;.format(model_uri=model_production_uri))\nmodel_production = mlflow.pyfunc.load_model(model_production_uri)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1610750609033,
        "Question_score":1,
        "Question_tags":"azure|databricks|mlflow",
        "Question_view_count":387,
        "Owner_creation_time":1581292138330,
        "Owner_last_access_time":1614304868413,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1610964132683,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65744496",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70383800,
        "Question_title":"How can I run Tensorboard with MLFlow's logs?",
        "Question_body":"<p>I use MLFlow with autolog to keep track of my Tensorflow models:<\/p>\n<pre><code>mlflow.tensorflow.autolog(every_n_iter=1)\nwith mlflow.start_run():\n  model = ...\n  model.compile(...)\n  model.fit(...)\n<\/code><\/pre>\n<p>and then I want to use my tensorboard logs located in the artifacts.\nBut when I run:<\/p>\n<pre><code>%tensorboard --logdir=&lt;logs_path&gt;\n<\/code><\/pre>\n<p>I have the error message:\n&quot;No dashboards are active for the current data set.\nProbable causes:<\/p>\n<p>You haven\u2019t written any data to your event files.\nTensorBoard can\u2019t find your event files.&quot;<\/p>\n<p>I work on Databricks, so log_path is something like:<\/p>\n<pre><code>\/dbfs\/databricks\/mlflow-tracking\/..\n<\/code><\/pre>\n<p>Any ideas?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1639678545067,
        "Question_score":1,
        "Question_tags":"tensorflow|databricks|tensorboard|mlflow",
        "Question_view_count":501,
        "Owner_creation_time":1639677790923,
        "Owner_last_access_time":1663854761243,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70383800",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70296350,
        "Question_title":"Mlflow log_model, not able to predict with spark_udf but with python works",
        "Question_body":"<p>I was wondering to log a model on mlflow, once I do it, I'm able to predict probabilities with python loaded model but not with spark_udf. The thing is, I still need to have a preprocessing function inside the model. Here is a toy reproductible example for you to see when it fails:<\/p>\n<pre><code>import mlflow\nfrom mlflow.models.signature import infer_signature\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport numpy as np\n\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=2, n_classes=2, shuffle=True, random_state=1995)\nX, y = pd.DataFrame(X), pd.DataFrame(y,columns=[&quot;target&quot;])\n# geerate column names\nX.columns = [f&quot;col_{idx}&quot; for idx in range(len(X.columns))]\nX[&quot;categorical_column&quot;] = np.random.choice([&quot;a&quot;,&quot;b&quot;,&quot;c&quot;], size=len(X) )\n\n\ndef encode_catcolumn(X):\n  X = X.copy()\n  # replace cat values [a,b,c] for [-10,0,35] respectively\n  X['categorical_column'] = np.select([X[&quot;categorical_column&quot;] == &quot;a&quot;, X[&quot;categorical_column&quot;] == &quot;b&quot;, X[&quot;categorical_column&quot;] == &quot;c&quot;],  [-10, 0,35] ) \n  return X\n\n# with catcolumn encoded; i need to use custom encoding , we'll do this within mlflow later\nX_encoded = encode_catcolumn(X)\n\n<\/code><\/pre>\n<p>Now let's create a wrapper for the model to encode the function within the model. Please see that the function encode_catcolumn within the class and the one outside the class presented before are the same.<\/p>\n<pre><code>class SklearnModelWrapper(mlflow.pyfunc.PythonModel):\n  def __init__(self, model):\n    self.model = model\n  def encode_catcolumn(self,X):\n    X = X.copy()\n    # replace cat values [a,b,c] for [-10,0,35] respectively\n    X['categorical_column'] = np.select([X[&quot;categorical_column&quot;] == &quot;a&quot;, X[&quot;categorical_column&quot;] == &quot;b&quot;, X[&quot;categorical_column&quot;] == &quot;c&quot;],  [-10, 0,35] ) \n    return X \n  def predict(self, context, model_input):\n    # encode catvariable\n    model_input = self.encode_catcolumn(model_input)\n    # predict probabilities\n    predictions = self.model.predict_proba(model_input)[:,1]\n    return predictions\n<\/code><\/pre>\n<p>Now let's log the model<\/p>\n<pre><code>with mlflow.start_run(run_name=&quot;reproductible_example&quot;) as run:\n  clf = RandomForestClassifier()\n  clf.fit(X_encoded,y)\n  # wrappmodel with pyfunc, does the encoding inside the class \n  wrappedModel = SklearnModelWrapper(clf)\n  # When the model is deployed, this signature will be used to validate inputs.\n  mlflow.pyfunc.log_model(&quot;reproductible_example_model&quot;, python_model=wrappedModel)\n\nmodel_uuid = run.info.run_uuid\nmodel_path = f'runs:\/{model_uuid}\/reproductible_example_model'\n<\/code><\/pre>\n<p>Do the inference without spark and works perfectly:<\/p>\n<pre><code>model_uuid = run.info.run_uuid\nmodel_path = f'runs:\/{model_uuid}\/reproductible_example_model'\n# Load model as a PyFuncModel.\nloaded_model = mlflow.pyfunc.load_model(model_path)\n# predictions without spark , encodes the variables INSIDE; this WORKS\nloaded_model.predict(X)\n<\/code><\/pre>\n<p>Now do the inference with spark_udf and get an error:<\/p>\n<pre><code># create spark dataframe to test it on spark\nX_spark = spark.createDataFrame(X)\n# Load model as a Spark UDF.\nloaded_model_spark = mlflow.pyfunc.spark_udf(spark, model_uri=model_path)\n\n# Predict on a Spark DataFrame.\ncolumns = list(X_spark.columns)\n# this does not work\nX_spark.withColumn('predictions', loaded_model_spark(*columns)).collect()\n<\/code><\/pre>\n<p>The error is:<\/p>\n<pre><code>PythonException: An exception was thrown from a UDF: 'KeyError: 'categorical_column'', from &lt;command-908038&gt;, line 7. Full traceback below:\n<\/code><\/pre>\n<p>I need to some how encode the variables and preprocess within the class. Is there any solution to this or any workaround to make this code able to woork with spark?\nWhat I've tried so far:<\/p>\n<ol>\n<li>Incorporate the encode_catcolumn within a sklearn Pipeline (with a custom encoder sklearn) -&gt; Fails;<\/li>\n<li>Create a function within the sklearn wrapper class (this example) -&gt; Fails\n3 ) Use the log_model and then create a pandas_udf in order to do it with spark as well --&gt; works but that's not what I want. I would like to be able to run the model on spark with just calling .predict() method or something like that.<\/li>\n<li>When a remove the preprocessing function and do it outside the class --&gt;  this actually works but this is not what<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639081909030,
        "Question_score":0,
        "Question_tags":"apache-spark|pyspark|scikit-learn|mlflow|mlops",
        "Question_view_count":730,
        "Owner_creation_time":1588098542797,
        "Owner_last_access_time":1664026048063,
        "Owner_location":"Buenos Aires, Argentina",
        "Owner_reputation":391,
        "Owner_up_votes":44,
        "Owner_down_votes":2,
        "Owner_views":26,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70296350",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71531665,
        "Question_title":"How to get `run_id` when using MLflow Project",
        "Question_body":"<p>When using MLflow Projects (via an <code>MLproject<\/code> file) I get this message at starting time:<\/p>\n<pre><code>INFO mlflow.projects.backend.local: \n=== Running command 'source \/anaconda3\/bin\/..\/etc\/profile.d\/conda.sh &amp;&amp; \nconda activate mlflow-4736797b8261ec1b3ab764c5060cae268b4c8ffa 1&gt;&amp;2 &amp;&amp; \npython3 main.py' in run with ID 'e2f0e8c670114c5887963cd6a1ac30f9' === \n<\/code><\/pre>\n<p>I want to access the <code>run_id<\/code> shown above (<strong>e2f0e8c670114c5887963cd6a1ac30f9<\/strong>) from inside the main script.<\/p>\n<p>I expected a run to be active but:<\/p>\n<pre><code>mlflow.active_run()\n&gt; None\n<\/code><\/pre>\n<p>Initiating a run inside the main script does give me access the correct <code>run_id<\/code>, although any subsequent runs will have a different <code>run_id<\/code>.<\/p>\n<pre><code># first run inside the script - correct run_id\nwith mlflow.start_run():\n   print(mlflow.active_run().info.run_id)\n&gt; e2f0e8c670114c5887963cd6a1ac30f9\n\n# second run inside the script - wrong run_id\nwith mlflow.start_run():\n   print(mlflow.active_run().info.run_id)\n&gt; 417065241f1946b98a4abfdd920239b1\n<\/code><\/pre>\n<p>Seems like a strange behavior, and I was wondering if there's another way to access the <code>run_id<\/code> assigned at the beginning of the <code>MLproject<\/code> run?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1647628142007,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":143,
        "Owner_creation_time":1527686643970,
        "Owner_last_access_time":1663364883530,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71531665",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73808171,
        "Question_title":"What is the difference between deploying models in MLflow and Sagemaker?",
        "Question_body":"<p>I could do\n<code>mlflow model serve -m &lt;RUN_ID&gt; --p 1234 --no-conda<\/code><\/p>\n<p>and<\/p>\n<p><code>mlflow sagemaker run-local -m &lt;MODEL_PATH&gt; -p 1234<\/code><\/p>\n<p>Are they not the same anyway as both can do model serving so what's the hassle deploying it to Sagemaker?<\/p>\n<p>I'm a beginner at this so if anyone can help me out with my understanding that will be great. Thank you so much in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663806620887,
        "Question_score":0,
        "Question_tags":"rest|deployment|amazon-sagemaker|endpoint|mlflow",
        "Question_view_count":26,
        "Owner_creation_time":1487831504143,
        "Owner_last_access_time":1664005735500,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73808171",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72365934,
        "Question_title":"Running mlflow ui in AWS Sagemaker",
        "Question_body":"<p>I want to run mlflow UI in sagemaker but it simply does not work, When it outputs the http address going to it results in a &quot;this site cannot be reached&quot;<\/p>\n<p>Here is the code:<\/p>\n<pre><code>def mlflow_test(server_uri, experiment_name):\n    mlflow.set_tracking_uri(server_uri)\n    mlflow.set_experiment(experiment_name)\n    with mlflow.start_run():\n        params = {\n            &quot;n-estimators&quot;: 100,\n            &quot;min-samples-leaf&quot;: 10,\n            &quot;features&quot;: 'feature_test'\n        }\n        mlflow.log_params(params)\n        mlflow.log_metric('foo', 5)\n        mlflow.end_run()\n<\/code><\/pre>\n<p>running that code will return:<\/p>\n<pre><code>[2022-05-24 15:48:44 +0000] [27820] [INFO] Starting gunicorn 20.1.0\n[2022-05-24 15:48:44 +0000] [27820] [INFO] Listening at: http:\/\/127.0.0.1:5000 (27820)\n[2022-05-24 15:48:44 +0000] [27820] [INFO] Using worker: sync\n[2022-05-24 15:48:44 +0000] [27823] [INFO] Booting worker with pid: 27823\n<\/code><\/pre>\n<p>Going to the <a href=\"http:\/\/127.0.0.1:5000\" rel=\"nofollow noreferrer\">http:\/\/127.0.0.1:5000<\/a> link won't work. Anyone know how to get mlflow ui running in sagemaker? There's not much info on this that's at an easy to understand level. I just want to log my metrics and params in sagemaker and view them using the mlflow ui<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653407649377,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|mlflow",
        "Question_view_count":93,
        "Owner_creation_time":1615994492347,
        "Owner_last_access_time":1657796021117,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72365934",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62754017,
        "Question_title":"Save customized function inside function in MLFlow log_model",
        "Question_body":"<p>I would like to do something with MLFlow but I do not find any solution on Internet. I am working with MLFlow and R, and I want to save a regression model. The thing is that by the time I want to predict the testing data, I want to do some transformation of that data. Then I have:<\/p>\n<pre class=\"lang-r prettyprint-override\"><code>data &lt;- #some data with numeric regressors and dependent variable called 'y'\n\n# Divide into train and test\nind &lt;- sample(nrow(data), 0.8*nrow(data), replace = FALSE)\ndataTrain &lt;- data[ind,]\ndataTest &lt;- data[-ind,]\n\n# Run model in the mlflow framework\nwith(mlflow_start_run(), {\n   model &lt;- lm(y ~ ., data = dataTrain)\n   \n   predict_fun &lt;- function(model, data_to_predict){\n       data_to_predict[,3] &lt;- data_to_predict[,3]\/2\n       data_to_predict[,4] &lt;- data_to_predict[,4] + 1\n\n       return(predict(model, data_to_predict))\n       }\n\n   predictor &lt;- crate(~predict_fun(model,dataTest),model)\n\n   ### Some code to use the predictor to get the predictions and measure the accuracy as a log_metric\n   ##################\n   ##################\n   ##################\n\n   mlflow_log_model(predictor,'model')\n}\n<\/code><\/pre>\n<p>As you can notice, my prediction function not only consists in predict the new data you are evaluating, but it also makes some transformations in the third and fourth columns. All examples I saw on the web use the function predict in the <em>crate<\/em> as the default function of R.<\/p>\n<p>Once I save this model, when I run it in another notebook with some Test data, I get the error: <em>&quot;predict_fun&quot; doesn't exist<\/em>. That is because my algorithm has not saved this specific function. Do you know what can I do to save and specific prediction function that I have created instead of the default functions that are in R?<\/p>\n<p>This is not the real example I am working with, but it is an approximation of it. The fact is that I want to save extra functions apart from the model itself.<\/p>\n<p>Thank you very much!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1594030946863,
        "Question_score":1,
        "Question_tags":"r|predict|mlflow",
        "Question_view_count":140,
        "Owner_creation_time":1568015757070,
        "Owner_last_access_time":1642423173287,
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62754017",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72835208,
        "Question_title":"Can't access the blob folder but files inside it are able to download",
        "Question_body":"<p>I have azure storage where I am using containers to store blobs. I am trying to download the blob from this container. But either using python SDK or rest, I am getting error &quot;The specified blob does not exist.&quot; but when I giving the full path with the final file such as .txt or whatever instead of root folder, it is able to download it.<\/p>\n<p>For example:\nfollowing URL gives error <strong><a href=\"https:\/\/mlflowsmodeltorage.blob.core.windows.net\/mlflow-test\/110\/63e7b9f2482b45e29b8c2983fa9522ef\/artifacts\/models\" rel=\"nofollow noreferrer\">https:\/\/mlflowsmodeltorage.blob.core.windows.net\/mlflow-test\/110\/63e7b9f2482b45e29b8c2983fa9522ef\/artifacts\/models<\/a><\/strong> The specified blob does not exist.<\/p>\n<p>but the URL <strong><a href=\"https:\/\/mlflowsmodeltorage.blob.core.windows.net\/mlflow-test\/110\/63e7b9f2482b45e29b8c2983fa9522ef\/artifacts\/models\/conda.yaml\" rel=\"nofollow noreferrer\">https:\/\/mlflowsmodeltorage.blob.core.windows.net\/mlflow-test\/110\/63e7b9f2482b45e29b8c2983fa9522ef\/artifacts\/models\/conda.yaml<\/a><\/strong> able to download the file.<\/p>\n<p>Same thing happens with the python SDK. But I want to download the whole folder rather than the files inside it.<\/p>\n<p>How can I achieve it.<\/p>\n<p>Below is the code I am using to access the blob using pytohn SDK<\/p>\n<pre><code>from azure.storage.blob import BlobServiceClient\n\nSTORAGEACCOUNTURL = &quot;https:\/\/mlflowsmodeltorage.blob.core.windows.net&quot;\nSTORAGEACCOUNTKEY = &quot;xxxxxxxxxxxxxx&quot;\nCONTAINERNAME = &quot;mlflow-test&quot;\nBLOBNAME = &quot;110\/63e7b9f2482b45e29b8c2983fa9522ef\/artifacts\/models\/&quot;\n\nblob_service_client_instance = BlobServiceClient(\n    account_url=STORAGEACCOUNTURL, credential=STORAGEACCOUNTKEY,\n\n)\n\nblob_client_instance = blob_service_client_instance.get_blob_client(\n    CONTAINERNAME, BLOBNAME, snapshot=None)\n\nblob_data = blob_client_instance.download_blob()\ndata = blob_data.readall()\nprint(data)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1656715374267,
        "Question_score":0,
        "Question_tags":"azure-blob-storage|azure-storage|mlflow|azure-storage-account",
        "Question_view_count":141,
        "Owner_creation_time":1626523396117,
        "Owner_last_access_time":1664042694257,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":1656886891087,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72835208",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62296590,
        "Question_title":"MLFlow and Hydra causing crash when used together",
        "Question_body":"<p>I'm trying to utilize Hydra with MLFlow, so I wrote the bare minimum script to see if they worked together (importing etc.). Both work fine on their own, but when put together I get a weird outcome.<\/p>\n\n<p>I have the script below:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import hydra\nfrom omegaconf import DictConfig\nfrom mlflow import log_metric, log_param, log_artifact,start_run\n\n@hydra.main(config_path=\"config.yaml\")\ndef my_app(cfg : DictConfig):\n    # print(cfg.pretty())\n    # print(cfg['coordinates']['x0'])\n    log_param(\"a\",2)\n    log_metric(\"b\",3)\n\nif __name__ == \"__main__\":\n    my_app()\n<\/code><\/pre>\n\n<p>However when ran, I get the error below:<\/p>\n\n<pre><code>ilknull@nurmachine:~\/Files\/Code\/Python\/MLFlow_test$ python3 hydra_temp.py \nError in atexit._run_exitfuncs:\nTraceback (most recent call last):\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/fluent.py\", line 164, in end_run\n    MlflowClient().set_terminated(_active_run_stack[-1].info.run_id, status)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 311, in set_terminated\n    self._tracking_client.set_terminated(run_id, status, end_time)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 312, in set_terminated\n    end_time=end_time)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/file_store.py\", line 377, in update_run_info\n    run_info = self._get_run_info(run_id)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/file_store.py\", line 442, in _get_run_info\n    databricks_pb2.RESOURCE_DOES_NOT_EXIST)\nmlflow.exceptions.MlflowException: Run '9066793c02604a6783d081ed965d5eff' not found\n<\/code><\/pre>\n\n<p>Again, they work perfectly fine when used separately, but together they cause this error. Any ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591767515617,
        "Question_score":4,
        "Question_tags":"python|docker|exception|mlflow|fb-hydra",
        "Question_view_count":718,
        "Owner_creation_time":1583072555673,
        "Owner_last_access_time":1663970229957,
        "Owner_location":null,
        "Owner_reputation":301,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Thanks for reporting this. I was not aware of this issue.<\/p>\n\n<p>This is because Hydra is changing your current working directory for each run.<\/p>\n\n<p>I did some digging, this is what you can do:<\/p>\n\n<ol>\n<li>Set the MLFLOW_TRACKING_URI environment variable:<\/li>\n<\/ol>\n\n<pre><code>MLFLOW_TRACKING_URI=file:\/\/\/$(pwd)\/.mlflow  python3 hydra_temp.py\n<\/code><\/pre>\n\n<ol start=\"2\">\n<li>Call set_tracking_url() before hydra.main() starts:<\/li>\n<\/ol>\n\n<pre><code>import hydra\nfrom omegaconf import DictConfig\nfrom mlflow import log_metric, log_param, set_tracking_uri\nimport os\n\nset_tracking_uri(f\"file:\/\/\/{os.getcwd()}\/.mlflow\")\n\n@hydra.main(config_name=\"config\")\ndef my_app(cfg: DictConfig):\n    log_param(\"a\", 2)\n    log_metric(\"b\", 3)\n\n\nif __name__ == \"__main__\":\n    my_app()\n<\/code><\/pre>\n\n<ol start=\"3\">\n<li>Wait for my <a href=\"https:\/\/github.com\/facebookresearch\/hydra\/issues\/664\" rel=\"nofollow noreferrer\">new issue<\/a> to get resolved, then there will have a proper plugin to integrate with mlflow.\n(This will probably take a while).<\/li>\n<\/ol>\n\n<p>By the way, Hydra 1.0 has new support for setting environment variables:<\/p>\n\n<p>This <em>ALMOST<\/em> works:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>hydra:\n  job:\n    env_set:\n      MLFLOW_TRACKING_DIR: file:\/\/${hydra:runtime.cwd}\/.mlflow\n      MLFLOW_TRACKING_URI: file:\/\/${hydra:runtime.cwd}\/.mlflow\n<\/code><\/pre>\n\n<p>Unfortunately Hydra is cleaning up the env variables when your function exits, and MLFlow is making the final save when the process exits so the env variable is no longer set.\nMLFlow also keeps re-initializing the FileStore object used to store the experiments data. If they would have initialized it just once and reused the same object the above should would have worked.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1591773861796,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1591782103620,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62296590",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72569496,
        "Question_title":"In mlflow, is it possible to track an artifact without having mlflow store it separately?",
        "Question_body":"<p>I am new to mlflow. I am trying to track\/log some artifacts (a directory of images output by my model) such that they are affiliated with the run that generated them, and so that I can view them in the mlflow UI along with all the other tracked information.<\/p>\n<p>This directory of images is generated in a custom folder path location (with a unique identifier for each run). My goal is to point mlflow to this directory so that it can recognize that these images are artifacts to track.<\/p>\n<p>Is this possible? From my understanding, the mlflow.log_artifact() function will simply create a duplicate of this image and store it within mlflow's default artifact path (ie, something like <em>mydrive1\/mlflow\/0\/\/artifacts\/<\/em>). I do not want to create a duplicate; I want to keep these images where I originally saved them.<\/p>\n<p>Example of file tree:<br \/>\nmydrive1\/<br \/>\n--\/train.py<br \/>\n--\/mlflow\/<br \/>\n----\/0\/<br \/>\n------\/meta.yaml<br \/>\n------\/[random char sequence]<br \/>\n--------\/artifacts\/<br \/>\n--------\/meta.yaml<\/p>\n<p>mydrive2\/<br \/>\n--\/output\/<br \/>\n----\/my_experiment0\/<br \/>\n------\/images\/<br \/>\n--------\/image1.png<br \/>\n--------\/image2.png<\/p>\n<p>I have found that if I manually edit the <em>artifact_uri<\/em> variable (in the meta.yaml file of the relevant run) to point to the relevant directory of images (ie, <em>mydrive2\/my_experiment0\/images\/<\/em>), all those images will show up in the artifact viewer in the mlflow UI. Is there a way to edit the <em>artifact_uri<\/em> variable via the mlflow API (or some other principled way)?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1654839245583,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":164,
        "Owner_creation_time":1560804109483,
        "Owner_last_access_time":1664065617013,
        "Owner_location":"Tokyo, Japan",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72569496",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69227917,
        "Question_title":"Connect MLflow server to minio in local",
        "Question_body":"<p>I am trying to connect mlflow with Minio server, both are running on my local machine, I am able to connect my client code to minio by adding the below lines to the code,<\/p>\n<pre><code>os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] =&quot;xxxx&quot;\nos.environ['AWS_SECRET_ACCESS_KEY'] =&quot;xxxxxx&quot; \nos.environ['MLFLOW_TRACKING_URI'] = 'http:\/\/localhost:5000'\n<\/code><\/pre>\n<p>But the mlflow server is not getting connected to Minio. To run Mlflow server, command I use:<\/p>\n<pre><code>mlflow server -h 0.0.0.0 -p 5000 --default-artifact-root s3:\/\/mlbucket --backend-store-uri sqlite:\/\/\/mlflow.db\n<\/code><\/pre>\n<p>The mlflow server runs, but while accessing the artifacts page the server, it throws the error:<\/p>\n<pre><code>raise NoCredentialsError()\nbotocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n<p>So how can I pass the credentials of the Minio to the mlflow server command?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1631902626293,
        "Question_score":1,
        "Question_tags":"python|minio|mlflow",
        "Question_view_count":1136,
        "Owner_creation_time":1310893185210,
        "Owner_last_access_time":1663988189020,
        "Owner_location":"Thiruvananthapuram, Kerala, India",
        "Owner_reputation":2763,
        "Owner_up_votes":373,
        "Owner_down_votes":7,
        "Owner_views":851,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Just add the below environment variables:<\/p>\n<pre><code>export AWS_ACCESS_KEY_ID=&lt;your-aws-access-key-id&gt;\nexport AWS_SECRET_ACCESS_KEY = &lt;your-aws-secret-access-key&gt;\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1634743751772,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69227917",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69479488,
        "Question_title":"Hi. I am very new to MLFlow, and want to implement MLFlow project on my own ML model. However I am getting \"\"Could not find main among entry points\"\"",
        "Question_body":"<p>The full error message is as below:<\/p>\n<pre><code>ERROR mlflow.cli: === Could not find main among entry points [] or interpret main as a runnable script. Supported script file extensions: ['.py', '.sh'] ===\n\n<\/code><\/pre>\n<p>I also try the solutions suggested here <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code>, but the result is the same.<\/p>\n<p>Below I provide all the required files to run <code>MLflow<\/code> project.<\/p>\n<p>The <code>conda.yaml<\/code> file<\/p>\n<pre><code>name: lightgbm-example\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.6\n  - pip\n  - pip:\n      - mlflow&gt;=1.6.0\n      - lightgbm\n      - pandas\n      - numpy\n<\/code><\/pre>\n<p>The MLProject file<\/p>\n<pre><code>name: lightgbm-example\nconda_env: ~\/Desktop\/MLflow\/conda.yaml\nentry-points:\n    main:\n      parameters:\n        learning_rate: {type: float, default: 0.1}\n        colsample_bytree: {type: float, default: 1.0}\n        subsample: {type: float, default: 1.0} \n      command: |\n          python3 ~\/Desktop\/MLflow\/Test.py \\\n            --learning-rate={learning_rate} \\\n            --colsample-bytree={colsample_bytree} \\\n            --subsample={subsample}\n<\/code><\/pre>\n<p>My Test.py file<\/p>\n<pre><code>import pandas as pd\nimport lightgbm as lgb\nimport numpy as np\nimport mlflow\nimport mlflow.lightgbm\nimport argparse\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=&quot;LightGBM example&quot;)\n    parser.add_argument(\n        &quot;--learning-rate&quot;,\n        type=float,\n        default=0.1,\n        help=&quot;learning rate to update step size at each boosting step (default: 0.3)&quot;,\n    )\n    parser.add_argument(\n        &quot;--colsample-bytree&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of columns when constructing each tree (default: 1.0)&quot;,\n    )\n    parser.add_argument(\n        &quot;--subsample&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of the training instances (default: 1.0)&quot;,\n    )\n    return parser.parse_args()\n\ndef find_specificity(c_matrix):\n    specificity = c_matrix[1][1]\/(c_matrix[1][1]+c_matrix[0][1])\n    return specificity\n    \n    \ndef main():\n\n    args = parse_args()\n\n    df = pd.read_csv('~\/Desktop\/MLflow\/Churn_demo.csv')\n    train_df = df.sample(frac=0.8, random_state=25)\n    test_df = df.drop(train_df.index)\n\n\n        \n    train_df.drop(['subscriberid'], axis = 1, inplace = True)\n    test_df.drop(['subscriberid'], axis = 1, inplace = True)\n\n    TrainX = train_df.iloc[:,:-1]\n    TrainY = train_df.iloc[:,-1]\n\n    TestX = test_df.iloc[:,:-1]\n    TestY = test_df.iloc[:,-1]\n    \n    mlflow.lightgbm.autolog()\n    \n    dtrain = lgb.Dataset(TrainX, label=TrainY)\n    dtest = lgb.Dataset(TestX, label=TestY)\n    \n    with mlflow.start_run():\n\n        parameters = {\n            'objective': 'binary',\n            'device':'cpu',\n            'num_threads': 6,\n            'num_leaves': 127,\n            'metric' : 'binary',\n            'lambda_l2':5,\n            'max_bin': 63,\n            'bin_construct_sample_cnt' :2*1000*1000,\n            'learning_rate': args.learning_rate,\n            'colsample_bytree': args.colsample_bytree,\n            'subsample': args.subsample,\n            'verbose': 1\n        }\n\n\n\n        model = lgb.train(parameters,\n                       dtrain,\n                       valid_sets=dtest,\n                       num_boost_round=10000,\n                       early_stopping_rounds=10)\n                       \n               \n        y_proba=model.predict(TestX)\n        pred=np.where(y_proba&gt;0.25,1,0) \n        conf_matrix = confusion_matrix(TestY,pred)\n        \n        specificity = find_specificity(conf_matrix)\n        acc = accuracy_score(TestY,pred)\n        \n        mlflow.log_metric({&quot;specificity&quot; : specificity, &quot;accuracy&quot; : acc})\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n        \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633602279323,
        "Question_score":2,
        "Question_tags":"python|yaml|mlflow",
        "Question_view_count":418,
        "Owner_creation_time":1583491811220,
        "Owner_last_access_time":1663774319043,
        "Owner_location":"Baku, Azerbaijan",
        "Owner_reputation":23,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Fortunately, I have been resolved my problem. I list some solutions for the same error which can help you in the future if you face the same problem.<\/p>\n<ol>\n<li>File names. The file names should be the same suggested in MLFlow docs <code>https:\/\/mlflow.org\/ <\/code>. For example not <code>conda.yamp<\/code>, but <code>conda.yaml<\/code>, as there was such problem in <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/3856<\/code><\/li>\n<li>The <code>conda.yaml<\/code> file does not support Tab, please consider using spaces instead<\/li>\n<li>In the MLProject file name 'P' should be the upper case before MLFlow 1.4. But the later versions it does not matter as explained there <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code><\/li>\n<li>(In my case) MLProject file is space sensitive. Let the <code> https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples<\/code> GitHub examples guide you.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1633946464143,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69479488",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72454747,
        "Question_title":"Problem when loading a xgboost model from mlflow registry",
        "Question_body":"<p>I create a xgboost classifier:<\/p>\n<pre><code>   xg_reg = xgb.XGBClassifier(objective ='reg:squarederror',  learning_rate = 0.1,\n                max_depth = 20, alpha = 10, n_estimators = 50, use_label_encoder=False)\n<\/code><\/pre>\n<p>After training the model, I am logging it to the MLFLow registry:<\/p>\n<pre><code>   mlflow.xgboost.log_model(\n        xgb_model = xg_reg, \n        artifact_path = &quot;xgboost-models&quot;,\n        registered_model_name = &quot;xgb-regression-model&quot;\n    )\n<\/code><\/pre>\n<p>In the remote UI, I can see the logged model:<\/p>\n<pre><code>artifact_path: xgboost-models\nflavors:\n  python_function:\n    data: model.xgb\n    env: conda.yaml\n    loader_module: mlflow.xgboost\n    python_version: 3.7.9\n  xgboost:\n    code: null\n    data: model.xgb\n    model_class: xgboost.sklearn.XGBClassifier\n    xgb_version: 1.5.2\nmlflow_version: 1.25.1\nmodel_uuid: 5fd42554cf184d8d96afae34dbb96de2\nrun_id: acdccd9f610b4c278b624fca718f76b4\nutc_time_created: '2022-05-17 17:54:53.039242\n<\/code><\/pre>\n<p>Now, on the server side, to load the logged model:<\/p>\n<pre><code>   model = mlflow.xgboost.load_model(model_uri=model_path)\n<\/code><\/pre>\n<p>which loads OK, but the model type is<\/p>\n<blockquote>\n<p>&lt;xgboost.core.Booster object at 0x00000234DBE61D00&gt;<\/p>\n<\/blockquote>\n<p>and the predictions are numpy.float32 (eg 0.5) instead of int64 (eg 0, 1) for the original model.<\/p>\n<p>Any ideas what can be wrong? Many thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654036554297,
        "Question_score":1,
        "Question_tags":"python|xgboost|mlflow",
        "Question_view_count":163,
        "Owner_creation_time":1369252294280,
        "Owner_last_access_time":1663789810223,
        "Owner_location":null,
        "Owner_reputation":324,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It turns out this was caused by using different versions of mlflow. The model was uploaded to registry with the newest version but was loaded with a previous one. When updated the server to load it, it now works! :)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655934198727,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72454747",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68753631,
        "Question_title":"Combine MLflow projects with docker-compose",
        "Question_body":"<p>I face the following situation:<\/p>\n<p>We train our models within docker container, which is build by running a docker-compose file. I have implemented MLflow to work with docker-compose (by doing something similar to e.g. this post: <a href=\"https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039<\/a>), creating two more containers (one for the server and one for the postgresql backend).<\/p>\n<p>However, the story doesn't end here. Our goal is to implement a full ML pipeline, which includes data creation, preprocessing steps and so on. I know, that ML projects is something which helps to create such pipeline. I have seen that it is designed to work with docker images (<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/projects.html\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/projects.html<\/a>), but I don't get it, how one could use it with docker-compose.<\/p>\n<p>Could you help me in that by giving any tipps, guidelines, documentations, etc?<\/p>\n<p>Or in general, any advice, how a full machine learning pipeline could be implemented using mlflow?<\/p>\n<p>Thanks a lot!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1628755106943,
        "Question_score":1,
        "Question_tags":"machine-learning|docker-compose|pipeline|mlflow",
        "Question_view_count":1007,
        "Owner_creation_time":1512572031753,
        "Owner_last_access_time":1655968570680,
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":1628775110070,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68753631",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69689266,
        "Question_title":"How to set a tag at the experiment level in MLFlow",
        "Question_body":"<p>I can see that an experiment in MLFlow can have tags (like runs can have tags).\nI'm able to set a run's tag using <code>mlflow.set_tag<\/code>, but how do I set it for an experiment?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1635000756820,
        "Question_score":2,
        "Question_tags":"python|mlflow",
        "Question_view_count":1047,
        "Owner_creation_time":1244984040077,
        "Owner_last_access_time":1663968051750,
        "Owner_location":"New York, NY",
        "Owner_reputation":13408,
        "Owner_up_votes":306,
        "Owner_down_votes":12,
        "Owner_views":687,
        "Question_last_edit_time":1635005765867,
        "Answer_body":"<p>If you look into the Python API, the very <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html\" rel=\"nofollow noreferrer\">first example<\/a> in <code>mlflow.tracking package<\/code> that shows how to create the <code>MLflowClient<\/code> is really showing how to tag experiment using the <code>client.set_experiment_tag<\/code> function (<a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html#mlflow.tracking.MlflowClient.set_experiment_tag\" rel=\"nofollow noreferrer\">doc<\/a>):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from mlflow.tracking import MlflowClient\n\n# Create an experiment with a name that is unique and case sensitive.\nclient = MlflowClient()\nexperiment_id = client.create_experiment(&quot;Social NLP Experiments&quot;)\nclient.set_experiment_tag(experiment_id, &quot;nlp.framework&quot;, &quot;Spark NLP&quot;)\n<\/code><\/pre>\n<p>you can also set it for model version with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html#mlflow.client.MlflowClient.set_model_version_tag\" rel=\"nofollow noreferrer\">set_model_version_tag<\/a> function, and for registered model with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html#mlflow.tracking.MlflowClient.set_registered_model_tag\" rel=\"nofollow noreferrer\">set_registered_model_tag<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1635012053310,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1663958792436,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69689266",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70030903,
        "Question_title":"Error while loading MLFlow model in python 3.7",
        "Question_body":"<p>I am saving the MLFlow model using databricks. Below are the details:<\/p>\n<pre><code>artifact_path: model\ndatabricks_runtime: 8.4.x-gpu-ml-scala2.12\nflavors:\n  python_function:\n    data: data\n    env: conda.yaml\n    loader_module: mlflow.pytorch\n    pickle_module_name: mlflow.pytorch.pickle_module\n    python_version: 3.8.8\n  pytorch:\n    model_data: data\n    pytorch_version: 1.9.0+cu102\n<\/code><\/pre>\n<p><strong>I am not able to locally load the model using Python 3.7<\/strong>, whereas it works well with Python 3.9.<\/p>\n<p>Any idea what could be the possible resolution to this?<\/p>\n<p>Error Trace:<\/p>\n<pre><code>  File &quot;\/Users\/danishm\/opt\/miniconda3\/envs\/py37\/lib\/python3.7\/site-packages\/mlflow\/pytorch\/__init__.py&quot;, line 714, in load_model\n    return _load_model(path=torch_model_artifacts_path, **kwargs)\n  File &quot;\/Users\/danishm\/opt\/miniconda3\/envs\/py37\/lib\/python3.7\/site-packages\/mlflow\/pytorch\/__init__.py&quot;, line 626, in _load_model\n    return torch.load(model_path, **kwargs)\n  File &quot;\/Users\/danishm\/opt\/miniconda3\/envs\/py37\/lib\/python3.7\/site-packages\/torch\/serialization.py&quot;, line 607, in load\n    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n  File &quot;\/Users\/danishm\/opt\/miniconda3\/envs\/py37\/lib\/python3.7\/site-packages\/torch\/serialization.py&quot;, line 882, in _load\n    result = unpickler.load()\nTypeError: code() takes at most 15 arguments (16 given)\n<\/code><\/pre>\n<p>I have tried specifying the pickle module name explicitly as <code>mlflow.pytorch.load_model(ML_MODEL_PATH,pickle_module=mlflow.pytorch.pickle_module)<\/code><\/p>\n<p>But still getting the same error.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1637304792843,
        "Question_score":0,
        "Question_tags":"python-3.x|pickle|torch|mlflow",
        "Question_view_count":477,
        "Owner_creation_time":1637303950900,
        "Owner_last_access_time":1663585014760,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70030903",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69738859,
        "Question_title":"Multiple values for a single parameter in the mlflow run command",
        "Question_body":"<p>I just started learning mlflow and wanted to know how to pass multiple values to  each parameter in the mlflow run command.<\/p>\n<p>The objective is to pass a dictionary to GridSearchCV as a param_grid to perform cross validation.<\/p>\n<p>In my main code, I retrieve the command line parameters using argparse. And by adding nargs='+' in the add_argument(), I can write spaced values for each hyper parameter and then applying vars() to create the dictionary. See code below:<\/p>\n<pre><code>import argparse\n\n# Build the parameters for the command-line\nparam_names = list(RandomForestClassifier().get_params().keys())\n\n# Param types in the same order they appear in param_names by using get_params()\nparam_types = [bool, float, dict, str, int, float, int, float, float, float,\n               float, float, float, int, int, bool, int, int, bool]\n\n# Allow for only optional command-line arguments\nparser = argparse.ArgumentParser()\ngrid_group = parser.add_argument_group('param_grid_group')\nfor i, p in enumerate(param_names):\n    grid_group.add_argument(f'--{p}', type=param_types[i], nargs='+')\n#Create a param_grid to be passed to GridSearchCV\nparam_grid_unprocessed = vars(parser.parse_args())\n<\/code><\/pre>\n<p>This works well with the classic python command :<\/p>\n<pre><code>python my_code.py --max_depth 2 3 4 --n_estimators 400 600 1000\n<\/code><\/pre>\n<p>As I said, here I can just write spaced values for each hyper-parameter and the code above does the magic by grouping the values inside a list and returning the dictionary below that I can then pass to GridSearchCV :<\/p>\n<pre><code>{'max_depth':[2, 3, 4], 'n_estimators':[400, 600, 1000]}\n<\/code><\/pre>\n<p>However with the mlflow run command, I can't get it right so far as it only accepts one value for each parameter. Here's my MLproject file :<\/p>\n<pre><code>name: mlflow_project\n\nconda_env: conda.yml\n\nentry_points:\n\n  main:\n    parameters:\n      max_depth: int\n      n_estimators: int\n    command: &quot;python my_code.py --max_depth {max_depth} --n_estimators {n_estimators}&quot;\n<\/code><\/pre>\n<p>So this works :<\/p>\n<pre><code>mlflow run . -P max_depth=2 -P n_estimators=400\n<\/code><\/pre>\n<p>But not this :<\/p>\n<pre><code> mlflow run . -P max_depth=[2, 3, 4] -P n_estimators=[400, 600, 1000]\n<\/code><\/pre>\n<p>In the <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#mlflow-run\" rel=\"nofollow noreferrer\">documentation<\/a>, it seems that it's impossible to do it. So, is there is any hack to overcome this problem ?<\/p>\n<p>Thank you in advance !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635338354137,
        "Question_score":2,
        "Question_tags":"python|gridsearchcv|mlflow",
        "Question_view_count":356,
        "Owner_creation_time":1586517832390,
        "Owner_last_access_time":1660328393330,
        "Owner_location":null,
        "Owner_reputation":127,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69738859",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71568975,
        "Question_title":"Custom MLFlow scoring_server for model serving",
        "Question_body":"<p>I would like to know if MLflow currently does support any kind of customization of it's scoring_serving that would allow the ability to register new endpoints to the published Rest API.<\/p>\n<p>By default the scoring server provides \/ping and \/invocations endpoint, but i would like to include more endpoints in addition to those.<\/p>\n<p>I've seen some resources that allow that kind of behaviour using custom WSGI implementations but i would like to know if extension of the provided mlflow scoring_server is possible in any way, so the default supporty provided by mlflow generated docker images and the deployment management is not lost.<\/p>\n<p>I explored existing official and unofficial documentation, and explored existing github issues and the mlflow codebase in it's github repository.<\/p>\n<p>Also i've explored some alternatives such as using custom WSGI server configuration for starting the Rest API.<\/p>\n<p>Any kind of resource\/documentation is greatly appreciated.<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1647937847000,
        "Question_score":2,
        "Question_tags":"rest|mlflow|serving|mlops",
        "Question_view_count":96,
        "Owner_creation_time":1370003358430,
        "Owner_last_access_time":1655743101593,
        "Owner_location":"A Coru\u00f1a",
        "Owner_reputation":303,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71568975",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65887231,
        "Question_title":"Use mlflow to serve a custom python model for scoring",
        "Question_body":"<p>I am using Python code generated from an ml software with mlflow to read a dataframe, perform some table operations and output a dataframe. I am able to run the code successfully and save the new dataframe as an artifact. However I am unable to log the model using log_model because it is not a lr or classifier model where we train and fit. I want to log a model for this so that it can be served with new data and deployed with a rest API<\/p>\n<pre><code>df = pd.read_csv(r&quot;\/home\/xxxx.csv&quot;)\n\n\nwith mlflow.start_run():\n    def getPrediction(row):\n        \n        perform_some_python_operaions \n\n        return [Status_prediction, Status_0_probability, Status_1_probability]\n    columnValues = []\n    for column in columns:\n        columnValues.append([])\n\n    for index, row in df.iterrows():\n        results = getPrediction(row)\n        for n in range(len(results)):\n            columnValues[n].append(results[n])\n\n    for n in range(len(columns)):\n        df[columns[n]] = columnValues[n]\n\n    df.to_csv('dataset_statistics.csv')\n    mlflow.log_artifact('dataset_statistics.csv')\n   \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611586824463,
        "Question_score":4,
        "Question_tags":"python|deployment|mlflow|mlops",
        "Question_view_count":3026,
        "Owner_creation_time":1573739890560,
        "Owner_last_access_time":1663922252563,
        "Owner_location":null,
        "Owner_reputation":115,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>MLflow supports <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">custom models<\/a> of mlflow.pyfunc flavor.  You can create a custom  class  inherited from the <code>mlflow.pyfunc.PythonModel<\/code>, that needs to provide function <code>predict<\/code> for performing predictions, and optional <code>load_context<\/code> to load the necessary artifacts, like this (adopted from the docs):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class MyModel(mlflow.pyfunc.PythonModel):\n\n    def load_context(self, context):\n        # load your artifacts\n\n    def predict(self, context, model_input):\n        return my_predict(model_input.values)\n<\/code><\/pre>\n<p>You can log to MLflow whatever artifacts you need for your models, define Conda environment if necessary, etc.<br \/>\nThen you can use <code>save_model<\/code> with your class to save your implementation, that could be loaded with <code>load_model<\/code> and do the <code>predict<\/code> using your model:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.pyfunc.save_model(\n        path=mlflow_pyfunc_model_path, \n        python_model=MyModel(), \n        artifacts=artifacts)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(mlflow_pyfunc_model_path)\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1611592914947,
        "Answer_score":9.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":1634187940523,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65887231",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73386272,
        "Question_title":"How to log metrics to Azure ML Metrics Tab",
        "Question_body":"<p>I have the following train.py file<\/p>\n<pre><code>import argparse\nimport os\nimport numpy as np\nimport glob\n# import joblib\nimport mlflow\nimport logging\nimport azureml.core\nimport pandas as pd\nimport numpy as np\nimport pandas as pd \n\nfrom matplotlib import pyplot as plt\nfrom azureml.core import Workspace, Dataset\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.workspace import Workspace\nfrom azureml.core.dataset import Dataset\nfrom azureml.train.automl import AutoMLConfig\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nimport lightgbm as lgb\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\n\n\n# let user feed in 2 parameters, the dataset to mount or download,\n# and the regularization rate of the logistic regression model\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    &quot;--tablename&quot;, type=str, dest=&quot;tablename&quot;, help=&quot;Table name&quot;\n)\nargs = parser.parse_args()\n\ntablename = args.tablename\n\n\nsubscription_id = ''\nresource_group = 'mlplayground'\nworkspace_name = 'mlplayground'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n\ndataset = Dataset.get_by_name(workspace, name=tablename)\ndata = dataset.to_pandas_dataframe()\n\n# use mlflow autologging\nmlflow.autolog()\n\ndata.drop(['postal_code','Column1','province','region','lattitude','longitude'], axis=1, inplace=True)\none_hot_state_of_the_building=pd.get_dummies(data.state_of_the_building) \none_hot_city = pd.get_dummies(data.city_name, prefix='city')\n\n#removing categorical features \ndata.drop(['city_name','state_of_the_building'],axis=1,inplace=True)  \n\n#Merging one hot encoded features with our dataset 'data' \ndata=pd.concat([data,one_hot_city,one_hot_state_of_the_building,],axis=1) \n\ndata['pricepersqm'] = data.price \/ data.house_area\n\nx=data.drop('price',axis=1) \ny=data.price \n\nX_df = DataFrame(x, columns= data.columns)\nX_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.20)\n\n#Converting the data into proper LGB Dataset Format\nd_train=lgb.Dataset(X_train, label=y_train)\n\n\n#Declaring the parameters\nparams = {\n    'task': 'train', \n    'boosting': 'gbdt',\n    'objective': 'regression',\n    'num_leaves': 10,\n    'learning_rate': 0.01,\n    'metric': {'l2','l1'},\n    'verbose': -1\n}\n\nprint(&quot;Train a LightGBM Regression model&quot;)\nclf=lgb.train(params,d_train,1000)\n\n#model prediction on X_test\nprint(&quot;Predict the test set&quot;)\ny_pred=clf.predict(X_test)\n\n#using RMSE error metric\nmse =mean_squared_error(y_pred,y_test)\nprint(&quot;RMSE: &quot;, mse**0.5)\nmlflow.log_metric(&quot;RMSE&quot;, mse**0.5)\n<\/code><\/pre>\n<p>And then from a notebook file I use the following:<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core import Experiment\n\n# connect to your workspace\nws = Workspace.from_config()\n\nexperiment_name = &quot;get-started-with-jobsubmission-tutorial-andlightgbm&quot;\nexp = Experiment(workspace=ws, name=experiment_name)\n\n\n\nfrom azureml.core.environment import Environment\n\n# use a curated environment that has already been built for you\n\nenv = Environment.get(workspace=ws, \n                      name=&quot;AzureML-sklearn-1.0-ubuntu20.04-py38-cpu&quot;, \n                      version=1)\n\nfrom azureml.core import ScriptRunConfig\n\nargs = [&quot;--tablename&quot;, &quot;BelgiumRealEstate&quot;]\n\nsrc = ScriptRunConfig(\n    source_directory=&quot;&quot;,\n    script=&quot;train.py&quot;,\n    arguments=args,\n    compute_target=&quot;local&quot;,\n    environment=env,\n)\n\nrun = exp.submit(config=src)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>As you can see in the train.py file I am logging the RMSE, however the metric does not appear on the metrics tab.<\/p>\n<p>What should I do?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660729490553,
        "Question_score":0,
        "Question_tags":"python|azure-machine-learning-service|mlflow",
        "Question_view_count":40,
        "Owner_creation_time":1302030303093,
        "Owner_last_access_time":1663332147473,
        "Owner_location":"Brussels, B\u00e9lgica",
        "Owner_reputation":30340,
        "Owner_up_votes":1667,
        "Owner_down_votes":79,
        "Owner_views":2937,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73386272",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71001833,
        "Question_title":"Accessing Delta Lake Table in Databricks via Spark in MLflow project",
        "Question_body":"<p>I am currently accessing deltalake table from databricks notebook using spark. However now I need to access delta tables from MLflow project. MLflow spark api only allows logging and loading of SparkML models. Any idea on how can I accomplish this?<\/p>\n<p>Currently I am trying to access spark via this code in MLflow project:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nspark = pyspark.sql.SparkSession._instantiatedSession\nif spark is None:\n  # NB: If there is no existing Spark context, create a new local one.\n  # NB: We're disabling caching on the new context since we do not need it and we want to\n  # avoid overwriting cache of underlying Spark cluster when executed on a Spark Worker\n  # (e.g. as part of spark_udf).\n  spark = ( pyspark.sql.SparkSession.builder \\\n   .config(&quot;spark.python.worker.reuse&quot;, True)\n   .config(&quot;spark.databricks.io.cache.enabled&quot;, False)\n   # In Spark 3.1 and above, we need to set this conf explicitly to enable creating\n   # a SparkSession on the workers\n   .config(&quot;spark.executor.allowSparkContext&quot;, &quot;true&quot;)\n   .master(&quot;local[*]&quot;)\n   .appName(&quot;MLflow Project&quot;)\n   .getOrCreate()\n  )\n<\/code><\/pre>\n<p>But I am getting this error:<\/p>\n<pre><code>py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1644092503000,
        "Question_score":1,
        "Question_tags":"apache-spark|pyspark|databricks|delta-lake|mlflow",
        "Question_view_count":282,
        "Owner_creation_time":1477235384120,
        "Owner_last_access_time":1663896021600,
        "Owner_location":null,
        "Owner_reputation":71,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1644169913796,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71001833",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72425907,
        "Question_title":"How to log a tensorflow model with mlflow.tensorflow.log_model (error module 'tensorflow._api.v2.saved_model' has no attribute 'tag_constants')",
        "Question_body":"<p>I am trying to log a trained model with MLFlow using mlflow.tensorflow.log_model.<\/p>\n<p>After training a simple sequential tf model<\/p>\n<pre><code>history = binary_model.fit(train_ds, validation_data=val_ds, epochs=num_epochs)\n<\/code><\/pre>\n<p>I am trying to log it:<\/p>\n<pre><code>    from tensorflow.python.saved_model import signature_constants\n    tag=[tf.saved_model.tag_constants.SERVING]\n    key=signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n\n    mlflow.tensorflow.log_model(tf_saved_model_dir=saved_model_path,\n                                tf_meta_graph_tags=tag,\n                                tf_signature_def_key=key,\n                                artifact_path=&quot;tf-models&quot;,\n                                registered_model_name=model_name)\n<\/code><\/pre>\n<p>but I get the error:<\/p>\n<pre><code>    AttributeError                            Traceback (most recent call last)\n    \/var\/folders\/2k\/g7p7j2gx6v54vkwv3v401h2m0000gn\/T\/ipykernel_73638\/562549064.py in &lt;module&gt;\n          1 from tensorflow.python.saved_model import signature_constants\n    ----&gt; 2 tag=[tf.saved_model.tag_constants.SERVING]\n          3 key=signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n          4 \n          5 mlflow.tensorflow.log_model(tf_saved_model_dir=saved_model_path,\n\n    AttributeError: module 'tensorflow._api.v2.saved_model' has no attribute 'tag_constants'\n<\/code><\/pre>\n<p>Any idea how to get the tags and keys correctly from the model to log it in MLFlow?<\/p>\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1653845789630,
        "Question_score":1,
        "Question_tags":"python|tensorflow|mlflow",
        "Question_view_count":319,
        "Owner_creation_time":1369252294280,
        "Owner_last_access_time":1663789810223,
        "Owner_location":null,
        "Owner_reputation":324,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The <code>tag_constants<\/code> is in <code>tf.compat.v1.saved_model<\/code>.<\/p>\n<p>To resolve the error replace this line<\/p>\n<pre><code>tag=[tf.saved_model.tag_constants.SERVING]\n<\/code><\/pre>\n<p>with this<\/p>\n<pre><code>tag=[tf.compat.v1.saved_model.tag_constants.SERVING]\n<\/code><\/pre>\n<p>Please refer <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/compat\/v1\/saved_model\/tag_constants\" rel=\"nofollow noreferrer\">this<\/a> for more details.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655094569432,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72425907",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62271624,
        "Question_title":"Access databricks secrets in pyspark\/python job",
        "Question_body":"<p>Databricks secrets can be accessed within notebooks using dbutils, however since dbutils is not available outside notebooks how can one access secrets in pyspark\/python jobs, especially if they are run using mlflow.<\/p>\n\n<p>I have already tried <a href=\"https:\/\/stackoverflow.com\/questions\/51885332\/how-to-load-databricks-package-dbutils-in-pyspark?rq=1\">How to load databricks package dbutils in pyspark<\/a><\/p>\n\n<p>which does not work for remote jobs or mlflow project runs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591652446997,
        "Question_score":1,
        "Question_tags":"databricks|azure-databricks|mlflow",
        "Question_view_count":1369,
        "Owner_creation_time":1497525776727,
        "Owner_last_access_time":1615138462217,
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":481,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62271624",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62720044,
        "Question_title":"No Artifacts Recorded MLFlow",
        "Question_body":"<p>I am unable to store, view, and retrieve the artifacts in MLFlow. The artifact folder is empty irrespective of creating a new experiment and assign proper experiment name and location.<\/p>\n<p>Server: mlflow server --backend-store-uri mlruns\/ --default-artifact-root mlruns\/ --host 0.0.0.0 --port 5000<\/p>\n<p>Create an Experiment: mlflow.create_experiment(exp_name, artifact_location='mlruns\/')<\/p>\n<pre><code>with mlflow.start_run():\n    mlflow.log_metric(&quot;mse&quot;, float(binary))\n    mlflow.log_artifact(data_path, &quot;data&quot;)\n    # log model\n    mlflow.keras.log_model(model, &quot;models&quot;)\n<\/code><\/pre>\n<p>The code compiles and runs but does not have any artifacts recorded. It has mlflow.log-model.history file but not the model.h5<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1593796777153,
        "Question_score":4,
        "Question_tags":"python-3.x|machine-learning|keras|mlflow",
        "Question_view_count":2676,
        "Owner_creation_time":1536570133123,
        "Owner_last_access_time":1601678349240,
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":89,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62720044",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":54773795,
        "Question_title":"Accessing MLFlow UI with a folder name different than mlruns",
        "Question_body":"<p>I set the <code>tracking_uri<\/code> to a folder name different than <code>mlruns<\/code>. <\/p>\n\n<p>Is there a way I can open the <strong>MLFlow UI<\/strong> pointing to the new folder name for mlruns? <\/p>\n\n<p>I know I can rename the folder back to <code>mlruns<\/code>, which gets me access to all of my metrics and parameters for each experiment, but the artifacts are not accessible, since they were logged to a different folder name than mlruns. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1550605267453,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":1521,
        "Owner_creation_time":1550605103667,
        "Owner_last_access_time":1575604038253,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1550609420312,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54773795",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60641337,
        "Question_title":"mlflow R installation MLFLOW_PYTHON_BIN",
        "Question_body":"<p>I am trying to install mlflow in R and im getting this error message saying <\/p>\n\n<blockquote>\n  <p>mlflow::install_mlflow()\n  Error in mlflow_conda_bin() :\n    Unable to find conda binary. Is Anaconda installed?\n    If you are not using conda, you can set the environment variable MLFLOW_PYTHON_BIN to the path of yourpython executable.<\/p>\n<\/blockquote>\n\n<p>I have tried the following<\/p>\n\n<pre><code>export MLFLOW_PYTHON_BIN=\"\/usr\/bin\/python\" \nsource ~\/.bashrc\necho $MLFLOW_PYTHON_BIN  -&gt; this prints the \/usr\/bin\/python.\n<\/code><\/pre>\n\n<p>or in R,<\/p>\n\n<pre><code>sys.setenv(MLFLOW_PYTHON_BIN=\"\/usr\/bin\/python\")\nsys.getenv() -&gt; prints MLFLOW_PYTHON_BIN is set to \/usr\/bin\/python.\n<\/code><\/pre>\n\n<p>however, it still does not work<\/p>\n\n<p>I do not want to use conda environment.<\/p>\n\n<p>how to I get past this error?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1583947052940,
        "Question_score":5,
        "Question_tags":"r|mlflow|system-variable",
        "Question_view_count":1141,
        "Owner_creation_time":1539211301843,
        "Owner_last_access_time":1663982305137,
        "Owner_location":null,
        "Owner_reputation":117,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":1584403666972,
        "Answer_body":"<p>The install_mlflow command only works with conda right now, sorry about the confusing message. You can either:<\/p>\n<ul>\n<li>install conda - this is the recommended way of installing and using mlflow<\/li>\n<\/ul>\n<p>or<\/p>\n<ul>\n<li>install mlflow python package yourself via pip<\/li>\n<\/ul>\n<p>To install mlflow yourself, pip install correct (matching the the R package) python version of mlflow and set the MLFLOW_PYTHON_BIN environment variable as well as MLFLOW_BIN evn variable: e.g.<\/p>\n<pre><code>library(mlflow)\nsystem(paste(&quot;pip install -U mlflow==&quot;, mlflow:::mlflow_version(), sep=&quot;&quot;))\nSys.setenv(MLFLOW_BIN=system(&quot;which mlflow&quot;))\nSys.setenv(MLFLOW_PYTHON_BIN=system(&quot;which python&quot;))\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1584554585176,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1624202175903,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60641337",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69703225,
        "Question_title":"Can I change the port of my MLflow tracking server?",
        "Question_body":"<p>I would like to know if I can change the port of my MLflow server.<\/p>\n<p>By default it is running on port 5000, but my company's VPN only allows HTTP (port 80) and HTTPS (port 443) traffic.<\/p>\n<p>This might be a very beginner's question, but is it possible, and if yes, is there any problem on running the MLflow server on port 83 (HTTP) ?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635139632427,
        "Question_score":1,
        "Question_tags":"http|port|mlflow",
        "Question_view_count":540,
        "Owner_creation_time":1561106497313,
        "Owner_last_access_time":1661059074150,
        "Owner_location":null,
        "Owner_reputation":133,
        "Owner_up_votes":28,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes, you can do that by passing the <code>-p port_number<\/code> command-line switch when starting MLflow server (see <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#cmdoption-mlflow-server-p\" rel=\"nofollow noreferrer\">docs<\/a>). Please note, that to be able to use ports below 1024, the server needs to be run as root.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1635152056289,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69703225",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56647549,
        "Question_title":"MLflow Error while deploying the Model to local REST server",
        "Question_body":"<blockquote>\n  <p><strong>System Details:<\/strong><\/p>\n  \n  <p>Operating System: Ubuntu 19.04<\/p>\n  \n  <p>Anaconda version: 2019.03<\/p>\n  \n  <p>Python version: 3.7.3<\/p>\n  \n  <p>mlflow version: 1.0.0<\/p>\n<\/blockquote>\n\n<p><strong>Steps to Reproduce:<\/strong> <a href=\"https:\/\/mlflow.org\/docs\/latest\/tutorial.html\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tutorial.html<\/a><\/p>\n\n<p><strong>Error at line\/command:<\/strong> <code>mlflow models serve -m [path_to_model] -p 1234<\/code><\/p>\n\n<p><strong>Error:<\/strong>\nCommand 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1>&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1<\/p>\n\n<p><strong>Terminal Log:<\/strong><\/p>\n\n<pre><code>(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# mlflow models serve -m $(pwd) -p 1234\n2019\/06\/18 16:15:16 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n2019\/06\/18 16:15:17 INFO mlflow.pyfunc.backend: === Running command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app'\nbash: activate: No such file or directory\nTraceback (most recent call last):\n  File \"\/root\/anaconda3\/envs\/mlflow\/bin\/mlflow\", line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/models\/cli.py\", line 43, in serve\n    host=host)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 76, in serve\n    command_env=command_env)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 147, in _execute_in_conda_env\n    command, rc\nException: Command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1\n(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# \n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1560855399150,
        "Question_score":0,
        "Question_tags":"python-3.x|deployment|mlflow",
        "Question_view_count":1840,
        "Owner_creation_time":1504001058090,
        "Owner_last_access_time":1630952509900,
        "Owner_location":null,
        "Owner_reputation":2101,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":41,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Following the steps mentioned in the GitHub Issue <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">1507<\/a> (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/1507<\/a>) I was able to resolve this issue.<\/p>\n\n<p>In reference to this post, the \"<strong>anaconda\/bin\/<\/strong>\" directory is never added to the list of environment variables i.e. PATH variable. In order to resolve this issue, add the \"<strong>else<\/strong>\" part of conda initialize code block from ~\/.bashrc file to your PATH variable.<\/p>\n\n<pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('\/home\/atulk\/anaconda3\/bin\/conda' 'shell.bash' 'hook' 2&gt; \/dev\/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\" ]; then\n        . \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\"\n    else\n        export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n<\/code><\/pre>\n\n<p>In this case, I added <strong>export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"<\/strong> to the PATH variable. However, this is just a temporary fix until the issue is fixed in the project.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1561730574529,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56647549",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72206086,
        "Question_title":"Can't log MLflow artifacts to S3 with docker-based tracking server",
        "Question_body":"<p>I'm trying to set up a simple MLflow tracking server with docker that uses a mysql backend store and S3 bucket for artifact storage.  I'm using a simple docker-compose file to set this up on a server and supplying all of the credentials through a .env file.  When I try to run the sklearn_elasticnet_wine example from the mlflow repo here: <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/sklearn_elasticnet_wine\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/sklearn_elasticnet_wine<\/a> using<code>TRACKING_URI = &quot;http:\/\/localhost:5005<\/code> from the machine hosting my tracking server, the run fails with the following error: <code>botocore.exceptions.NoCredentialsError: Unable to locate credentials<\/code>.  I've verified that my environment variables are correct and available in my mlflow_server container. The runs show up in my backend store so the run only seems to be failing at the artifact logging step.  I'm not sure why this isn't working.  I've seen a examples of how to set up a tracking server online, including: <a href=\"https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039<\/a>.  Some use minio also but others just specify their s3 location as I have.  I'm not sure what I'm doing wrong at this point. Do I need to explicitly set the ARTIFACT_URI as well?  Should I be using Minio?  Eventually, I'll be logging runs to the server from another machine, hence the nginx container.  I'm pretty new to all of this so I'm hoping it's something really obvious and easy to fix but so far the Google has failed me.  TIA.<\/p>\n<pre><code>version: '3'\n\nservices:\n  app: \n    restart: always\n    build: .\/mlflow\n    image: mlflow_server\n    container_name: mlflow_server\n    expose:\n      - 5001\n    ports:\n      - &quot;5001:5001&quot;\n    networks:\n      - internal \n    environment:\n      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\n      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\n      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}\n      - AWS_S3_BUCKET=${AWS_S3_BUCKET}\n      - DB_USER=${DB_USER}\n      - DB_PASSWORD=${DB_PASSWORD}\n      - DB_PORT=${DB_PORT}\n      - DB_NAME=${DB_NAME}\n    command: &gt;\n      mlflow server \n      --backend-store-uri mysql+pymysql:\/\/${DB_USER}:${DB_PASSWORD}@${DB_HOST}:${DB_PORT}\/${DB_NAME} \n      --default-artifact-root s3:\/\/${AWS_S3_BUCKET}\/mlruns\/\n      --host 0.0.0.0 \n      --port 5001\n\n  nginx: \n    restart: always\n    build: .\/nginx\n    image: mlflow_nginx\n    container_name: mlflow_nginx\n    ports:\n      - &quot;5005:80&quot; \n    networks:\n      - internal \n    depends_on:\n      - app\n\nnetworks:\n  internal:\n    driver: bridge\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1652294436513,
        "Question_score":2,
        "Question_tags":"docker|amazon-s3|docker-compose|boto3|mlflow",
        "Question_view_count":620,
        "Owner_creation_time":1639614248310,
        "Owner_last_access_time":1663891212383,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1652296959876,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72206086",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72688726,
        "Question_title":"Can you edit the tags of a registered model version after the fact using mlflow api?",
        "Question_body":"<p>I am trying to use the mlflow model registry. My plan is to have a monthly scheduled retraining pipeline. I know from reading the documentation that so long as I set the model name to the same string if I save a model\/use create_model_version it will create a new version of the model. I also saw from the documentation that I can set the tags associated with a version using create_model_version as well. I want to set tags for valid_to_date and valid_from_date for each version so that if I want to go back in time and backfill predictions with the correct version <em>as of<\/em> when the data is from I am using the correct model. My initial thought was every time I create a new model version I set the valid_from_date as the date of that model version creation and the valid_to_date as 1-1-2099. Then when I train a new version edit the tag of the previous version valid_to_date from 1-1-2099 to the date it was superceded by a new version. Is that something that can be done using the mlflow python api?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655735663570,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":108,
        "Owner_creation_time":1333388796950,
        "Owner_last_access_time":1663959988533,
        "Owner_location":null,
        "Owner_reputation":335,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":43,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72688726",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59003752,
        "Question_title":"adbazureml not supported by mlflow",
        "Question_body":"<p>We've been following the latest Microsoft webinar about deploying the ML model from Azure Databricks to Azure ML using the MLFlow, and we get the following error when trying to run the experiment from Databricks notebook using the following code:<\/p>\n\n<pre><code>experimentName=\"someExperimentName\"\nmlflow.set_experiment(experimentName)\n<\/code><\/pre>\n\n<p>the error message:<\/p>\n\n<blockquote>\n  <p>UnsupportedModelRegistryStoreURIException: Unsupported URI\n  'adbazureml:\/\/westus.experiments.azureml.net\/history\/v1.0\/subscriptions\/cemrecdsap-t10us-20180830\/resourceGroups\/2f5a718e-7c56-4dd3-aa7b-03a19b70667\/providers\/Microsoft.MachineLearningServices\/workspaces\/cemrecdsap-mlservice'\n  for model registry store. Supported schemes are: ['', 'file',\n  'sqlite', 'https', 'databricks', 'postgresql', 'mysql', 'http',\n  'mssql']<\/p>\n<\/blockquote>\n\n<p>Init script we use as suggested in Microsoft MLflow webinar:\n(it was available here, but now it's removed - <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/azure-databricks\/linking\/azureml-cluster-init.sh\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/azure-databricks\/linking\/azureml-cluster-init.sh<\/a>)<\/p>\n\n<pre><code>#!\/bin\/bash\n\n############## START CONFIGURATION #################\n# Provide the required *AzureML* workspace information\nregion=\"westus\" \nsubscriptionId=\"bcb65f42-f234-4bff-91cf-9ef816cd9936\" \nresourceGroupName=\"dev-rg\"\nworkspaceName=\"myazuremlws\"\n# Optional config directory\nconfigLocation=\"\/databricks\/config.json\"\n############### END CONFIGURATION #################\n\n# Drop the workspace configuration on the cluster\nsudo touch $configLocation\nsudo echo {\\\\\"subscription_id\\\\\": \\\\\"${subscriptionId}\\\\\", \\\\\"resource_group\\\\\": \\\\\"${resourceGroupName}\\\\\", \\\\\"workspace_name\\\\\": \\\\\"${workspaceName}\\\\\"} &gt; $configLocation\n\n# Set the MLflow Tracking URI\ntrackingUri=\"adbazureml:\/\/${region}.experiments.azureml.net\/history\/v1.0\/subscriptions\/${subscriptionId}\/resourceGroups\/${resourceGroupName}\/providers\/Microsoft.MachineLearningServices\/workspaces\/${workspaceName}\"\nsudo echo export MLFLOW_TRACKING_URI=${trackingUri} &gt;&gt; \/databricks\/spark\/conf\/spark-env.sh \n<\/code><\/pre>\n\n<p>We use the latest MLFlow version, 1.4<\/p>\n\n<p>Is there a chance that the <strong>adbazureml<\/strong> protocol that was used in the webinar is not supported yet by MLFlow?\nOr we missed something else?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1574472436810,
        "Question_score":3,
        "Question_tags":"mlflow",
        "Question_view_count":430,
        "Owner_creation_time":1502581315743,
        "Owner_last_access_time":1576191685887,
        "Owner_location":"Israel",
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59003752",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69944447,
        "Question_title":"How to change the directory of mlflow logs?",
        "Question_body":"<p>I am using MLflow to log the metrics but I want to change the default saving logs directory. So, instead of writing log files besides my main file, I want to store them to <code>\/path\/outputs\/lg <\/code>. I don't know how to change it. I use it without in the <code>Model<\/code>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nfrom time import time\n\nimport mlflow\nimport numpy as np\nimport torch\nimport tqdm\n\n# from segmentation_models_pytorch.utils import metrics\nfrom AICore.emergency_landing.metrics import IoU, F1\nfrom AICore.emergency_landing.utils import AverageMeter\nfrom AICore.emergency_landing.utils import TBLogger\n\n\nclass Model:\n    def __init__(self, model, num_classes=5, ignore_index=0, optimizer=None, scheduler=None, criterion=None,\n                 device=None, epochs=30, train_loader=None, val_loader=None, tb_logger: TBLogger = None,\n                 logger=None,\n                 best_model_path=None,\n                 model_check_point_path=None,\n                 load_from_best_model=None,\n                 load_from_model_checkpoint=None,\n                 early_stopping=None,\n                 debug=False):\n\n        self.debug = debug\n\n        self.early_stopping = {\n            'init': early_stopping,\n            'changed': 0\n        }\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.criterion = criterion\n        self.device = device\n        self.epochs = epochs\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n        self.model = model.to(device)\n\n        self.tb_logger = tb_logger\n        self.logger = logger\n\n        self.best_loss = np.Inf\n\n        if not os.path.exists(best_model_path):\n            os.makedirs(best_model_path)\n        self.best_model_path = best_model_path\n\n        if not os.path.exists(model_check_point_path):\n            os.makedirs(model_check_point_path)\n        self.model_check_point_path = model_check_point_path\n\n        self.load_from_best_model = load_from_best_model\n        self.load_from_model_checkpoint = load_from_model_checkpoint\n\n        if self.load_from_best_model is not None:\n            self.load_model(path=self.load_from_best_model)\n        if self.load_from_model_checkpoint is not None:\n            self.load_model_checkpoint(path=self.load_from_model_checkpoint)\n\n        self.train_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n        self.val_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n        self.test_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n\n        self.train_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n        self.val_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n        self.test_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n\n    def metrics(self, is_train=True):\n        if is_train:\n            train_losses = AverageMeter('Training Loss', ':.4e')\n            train_iou = AverageMeter('Training iou', ':6.2f')\n            train_f_score = AverageMeter('Training F_score', ':6.2f')\n\n            return train_losses, train_iou, train_f_score\n        else:\n            val_losses = AverageMeter('Validation Loss', ':.4e')\n            val_iou = AverageMeter('Validation mean iou', ':6.2f')\n            val_f_score = AverageMeter('Validation F_score', ':6.2f')\n\n            return val_losses, val_iou, val_f_score\n\n    def fit(self):\n\n        self.logger.info(&quot;\\nStart training\\n\\n&quot;)\n        start_training_time = time()\n\n        with mlflow.start_run():\n            for e in range(self.epochs):\n                start_training_epoch_time = time()\n                self.model.train()\n                train_losses_avg, train_iou_avg, train_f_score_avg = self.metrics(is_train=True)\n                with tqdm.tqdm(self.train_loader, unit=&quot;batch&quot;) as tepoch:\n                    tepoch.set_description(f&quot;Epoch {e}&quot;)\n                    for image, target in tepoch:\n                        # Transfer Data to GPU if available\n                        image = image.to(self.device)\n                        target = target.to(self.device)\n                        # Clear the gradients\n                        self.optimizer.zero_grad()\n                        # Forward Pass\n                        # out = self.model(image)['out']\n                        # if unet == true =&gt; remove ['out']\n                        out = self.model(image)\n                        # Find the Loss\n                        loss = self.criterion(out, target)\n                        # Calculate Loss\n                        train_losses_avg.update(loss.item(), image.size(0))\n                        # Calculate gradients\n                        loss.backward()\n                        # Update Weights\n                        self.optimizer.step()\n\n                        iou = self.train_iou(out.cpu(), target.cpu()).item()\n                        train_iou_avg.update(iou)\n\n                        f1_score = self.train_f1(out.cpu(), target.cpu()).item()\n                        train_f_score_avg.update(f1_score)\n\n                        tepoch.set_postfix(loss=train_losses_avg.avg,\n                                           iou=train_iou_avg.avg,\n                                           f_score=train_f_score_avg.avg)\n                        if self.debug:\n                            break\n\n                self.tb_logger.log(log_type='criterion\/training', value=train_losses_avg.avg, epoch=e)\n                self.tb_logger.log(log_type='iou\/training', value=train_iou_avg.avg, epoch=e)\n                self.tb_logger.log(log_type='f_score\/training', value=train_f_score_avg.avg, epoch=e)\n\n                mlflow.log_metric('criterion\/training', train_losses_avg.avg, step=e)\n                mlflow.log_metric('iou\/training', train_iou_avg.avg, step=e)\n                mlflow.log_metric('f_score\/training', train_f_score_avg.avg, step=e)\n\n                end_training_epoch_time = time() - start_training_epoch_time\n                print('\\n')\n                self.logger.info(\n                    f'Training Results - [{end_training_epoch_time:.3f}s] Epoch: {e}:'\n                    f' f_score: {train_f_score_avg.avg:.3f},'\n                    f' IoU: {train_iou_avg.avg:.3f},'\n                    f' Loss: {train_losses_avg.avg:.3f}')\n\n                # validation step\n                val_loss = self.evaluation(e)\n                # apply scheduler\n                if self.scheduler:\n                    self.scheduler.step()\n                # early stopping\n                if self.early_stopping['init'] &gt;= self.early_stopping['changed']:\n                    self._early_stopping_model(val_loss=val_loss)\n                else:\n                    print(f'The model can not learn more, Early Stopping at epoch[{e}]')\n                    break\n\n                # save best model\n                if self.best_model_path is not None:\n                    self._best_model(val_loss=val_loss, path=self.best_model_path)\n\n                # model check points\n                if self.model_check_point_path is not None:\n                    self.save_model_check_points(path=self.model_check_point_path, epoch=e, net=self.model,\n                                                 optimizer=self.optimizer, loss=self.criterion,\n                                                 avg_loss=train_losses_avg.avg)\n\n                # log mlflow\n                if self.scheduler:\n                    mlflow.log_param(&quot;get_last_lr&quot;, self.scheduler.get_last_lr())\n                    mlflow.log_param(&quot;scheduler&quot;, self.scheduler.state_dict())\n\n                self.tb_logger.flush()\n                if self.debug:\n                    break\n\n            end_training_time = time() - start_training_time\n            print(f'Finished Training after {end_training_time:.3f}s')\n            self.tb_logger.close()\n\n    def evaluation(self, epoch):\n        print('Validating...')\n        start_validation_epoch_time = time()\n        self.model.eval()  # Optional when not using Model Specific layer\n        with torch.no_grad():\n            val_losses_avg, val_iou_avg, val_f_score_avg = self.metrics(is_train=False)\n            with tqdm.tqdm(self.val_loader, unit=&quot;batch&quot;) as tepoch:\n                for image, target in tepoch:\n                    # Transfer Data to GPU if available\n                    image = image.to(self.device)\n                    target = target.to(self.device)\n                    # out = self.model(image)['out']\n                    # if unet == true =&gt; remove ['out']\n                    out = self.model(image)\n                    # Find the Loss\n                    loss = self.criterion(out, target)\n                    # Calculate Loss\n                    val_losses_avg.update(loss.item(), image.size(0))\n\n                    iou = self.val_iou(out.cpu(), target.cpu()).item()\n                    val_iou_avg.update(iou)\n\n                    f1_score = self.val_f1(out.cpu(), target.cpu()).item()\n                    val_f_score_avg.update(f1_score)\n\n                    tepoch.set_postfix(loss=val_losses_avg.avg,\n                                       iou=val_iou_avg.avg,\n                                       f_score=val_f_score_avg.avg)\n                    if self.debug:\n                        break\n            print('\\n')\n            self.tb_logger.log(log_type='criterion\/validation', value=val_losses_avg.avg, epoch=epoch)\n            self.tb_logger.log(log_type='iou\/validation', value=val_iou_avg.avg, epoch=epoch)\n            self.tb_logger.log(log_type='f_score\/validation', value=val_f_score_avg.avg, epoch=epoch)\n\n            mlflow.log_metric('criterion\/validation', val_losses_avg.avg, step=epoch)\n            mlflow.log_metric('iou\/validation', val_iou_avg.avg, step=epoch)\n            mlflow.log_metric('f_score\/validation', val_f_score_avg.avg, step=epoch)\n\n            end_validation_epoch_time = time() - start_validation_epoch_time\n            self.logger.info(\n                f'validation Results - [{end_validation_epoch_time:.3f}s] Epoch: {epoch}:'\n                f' f_score: {val_f_score_avg.avg:.3f},'\n                f' IoU: {val_iou_avg.avg:.3f},'\n                f' Loss: {val_losses_avg.avg:.3f}')\n            print('\\n')\n            return val_losses_avg.avg\n\n    def _save_model(self, name, path, params):\n        torch.save(params, path)\n\n    def _early_stopping_model(self, val_loss):\n        if self.best_loss &lt; val_loss:\n            self.early_stopping['changed'] += 1\n        else:\n            self.early_stopping['changed'] = 0\n\n    def _best_model(self, val_loss, path):\n        if self.best_loss &gt; val_loss:\n            self.best_loss = val_loss\n            name = f'\/best_model_loss_{self.best_loss:.2f}'.replace('.', '_')\n            self._save_model(name, path=f'{path}\/{name}.pt', params={\n                'model_state_dict': self.model.state_dict(),\n            })\n\n            print(f'The best model is saved with criterion: {self.best_loss:.2f}')\n\n    def save_model_check_points(self, path, epoch, net, optimizer, loss, avg_loss):\n        name = f'\/model_epoch_{epoch}_loss_{avg_loss:.2f}'.replace('.', '_')\n        self._save_model(name, path=f'{path}\/{name}.pt', params={\n            'epoch': epoch,\n            'model_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'criterion': loss,\n        })\n        print(f'model checkpoint is saved at model_epoch_{epoch}_loss_{avg_loss:.2f}')\n\n    def load_model_checkpoint(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        epoch = checkpoint['epoch']\n        self.criterion = checkpoint['criterion']\n\n        return epoch\n\n    def load_model(self, path):\n        best_model = torch.load(path)\n        self.model.load_state_dict(best_model['model_state_dict'])\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1636727119830,
        "Question_score":0,
        "Question_tags":"machine-learning|deep-learning|pytorch|mlflow",
        "Question_view_count":436,
        "Owner_creation_time":1410333105327,
        "Owner_last_access_time":1662489968593,
        "Owner_location":"Turin, Metropolitan City of Turin, Italy",
        "Owner_reputation":477,
        "Owner_up_votes":133,
        "Owner_down_votes":0,
        "Owner_views":130,
        "Question_last_edit_time":1636755379243,
        "Answer_body":"<p>The solution is:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.set_tracking_uri(uri=f'file:\/\/{hydra.utils.to_absolute_path(&quot;..\/output\/mlruns&quot;)}')\nexp = mlflow.get_experiment_by_name(name='Emegency_landing')\nif not exp:\n    experiment_id = mlflow.create_experiment(name='Emegency_landing',\n                                                 artifact_location=f'file:\/\/{hydra.utils.to_absolute_path(&quot;..\/output\/mlruns&quot;)}')\nelse:\n    experiment_id = exp.experiment_id\n<\/code><\/pre>\n<p>And then you should pass the experiment Id to:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with mlflow.start_run(experiment_id=experiment_id):\n     pass \n<\/code><\/pre>\n<p>If you don't mention the <code>\/path\/mlruns<\/code>, when you run the command of <code>mlflow ui<\/code>, it will create another folder automatically named <code>mlruns<\/code>. so, pay attention to this point to have the same name as <code>mlruns<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1636732235590,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1636755540676,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69944447",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67959892,
        "Question_title":"Log experiments with MLflow to DAGsHub tracking server",
        "Question_body":"<p>I'm trying to use MLflow and log my experiments to DAGsHub's remote-tracking server but I get this error message:<\/p>\n<p><code>WARNING mlflow.utils.autologging_utils: Encountered unexpected error during tensorflow autologging: 'NoneType' object has no attribute 'info' <\/code> with a lot of HTML text.<\/p>\n<p>When I check my DAGsHub repo, no new experiment is created.<\/p>\n<p>What am I'm doing wrong?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1623598028067,
        "Question_score":3,
        "Question_tags":"mlflow",
        "Question_view_count":95,
        "Owner_creation_time":1620133604910,
        "Owner_last_access_time":1641927869990,
        "Owner_location":"New York, NY, USA",
        "Owner_reputation":53,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67959892",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72663925,
        "Question_title":"Failure on importing mlflow to Azure Databricks 7.3 LTS ML Runtime",
        "Question_body":"<p>I am having trouble trying to import mlflow to Azure databricks. I'm currently using 7.3 LTS ML Runtime, which already have mlflow==1.11.0. I am a developing data scientist and I have no clue how to solve this issue. Have already tried to reinstall and didn't suceed. Any thoughts?<\/p>\n<p>This is the error message:<\/p>\n<pre><code>Failure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (azureml-core 1.8.0.post1 (\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages), Requirement.parse('azureml-core~=1.19.0'), {'azureml-telemetry'}).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (azureml-core 1.8.0.post1 (\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages), Requirement.parse('azureml-core~=1.19.0'), {'azureml-telemetry'}).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (azureml-core 1.8.0.post1 (\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages), Requirement.parse('azureml-core~=1.19.0')).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (azureml-core 1.8.0.post1 (\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages), Requirement.parse('azureml-core~=1.19.0')).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (azureml-core 1.8.0.post1 (\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages), Requirement.parse('azureml-core~=1.19.0')).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception (msrest 0.6.18 (\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages), Requirement.parse('msrest&gt;=0.6.21'), {'azure-mgmt-containerregistry'}).\nCould not import from mlflow. Please upgrade to Mlflow 1.4.0 or higher.\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/utils.py:123: UserWarning: Failure attempting to register store for scheme &quot;adbazureml&quot;: No module named 'mlflow.store.rest_store'\n  _tracking_store_registry.register_entrypoints()\nCould not import from mlflow. Please upgrade to Mlflow 1.4.0 or higher.\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/utils.py:123: UserWarning: Failure attempting to register store for scheme &quot;azureml&quot;: No module named 'mlflow.store.rest_store'\n  _tracking_store_registry.register_entrypoints()\nCould not import from mlflow. Please upgrade to Mlflow 1.4.0 or higher.\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_model_registry\/utils.py:106: UserWarning: Failure attempting to register store for scheme &quot;azureml&quot;: No module named 'mlflow.store.rest_store'\n  _model_registry_store_registry.register_entrypoints()\nCould not import from mlflow. Please upgrade to Mlflow 1.4.0 or higher.\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages\/mlflow\/store\/artifact\/artifact_repository_registry.py:89: UserWarning: Failure attempting to register artifact repository for scheme &quot;adbazureml&quot;: No module named 'mlflow.store.rest_store'\n  _artifact_repository_registry.register_entrypoints()\nCould not import from mlflow. Please upgrade to Mlflow 1.4.0 or higher.\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages\/mlflow\/store\/artifact\/artifact_repository_registry.py:89: UserWarning: Failure attempting to register artifact repository for scheme &quot;azureml&quot;: No module named 'mlflow.store.rest_store'\n  _artifact_repository_registry.register_entrypoints()\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages\/tensorflow\/python\/data\/ops\/iterator_ops.py:546: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n  class IteratorBase(collections.Iterator, trackable.Trackable,\n\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503\/lib\/python3.7\/site-packages\/tensorflow\/python\/autograph\/utils\/testing.py:21: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n  import imp\n\npip list\n\nazure-common              1.1.28           \nazure-core                1.24.1           \nazure-graphrbac           0.61.1           \nazure-identity            1.4.1            \nazure-mgmt-authorization  0.61.0           \nazure-mgmt-containerregistry 10.0.0        \nazure-mgmt-core           1.3.1            \nazure-mgmt-keyvault       2.2.0            \nazure-mgmt-network        10.2.0           \nazure-mgmt-resource       11.0.0           \nazure-mgmt-storage        11.2.0           \nazure-storage-blob        12.4.0           \nazureml-automl-core       1.19.0           \nazureml-core              1.8.0.post1      \nazureml-dataprep          2.6.6            \nazureml-dataprep-native   26.0.0           \nazureml-dataprep-rslex    1.4.0            \nazureml-dataset-runtime   1.19.0.post1     \nazureml-mlflow            1.8.0            \nazureml-pipeline          1.19.0           \nazureml-pipeline-core     1.19.0           \nazureml-pipeline-steps    1.19.0           \nazureml-sdk               1.19.0           \nazureml-telemetry         1.19.0           \nazureml-train             1.19.0           \nazureml-train-automl-client 1.19.0         \nazureml-train-core        1.19.0           \nazureml-train-restclients-hyperdrive 1.19.0\nmlflow                    1.11.0\npip                       20.0.2\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655493728417,
        "Question_score":0,
        "Question_tags":"python|azure-databricks|mlflow",
        "Question_view_count":102,
        "Owner_creation_time":1647266372287,
        "Owner_last_access_time":1655579594010,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72663925",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72053762,
        "Question_title":"How to create seperate mlflow custom models for training and prediction?",
        "Question_body":"<p>My requirement is to create separate Mlflow custom models for training and prediction.\nI want to create training model and use those training model in prediction model<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651214518370,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":45,
        "Owner_creation_time":1651212627497,
        "Owner_last_access_time":1663746029710,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72053762",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71408963,
        "Question_title":"Getting `dtype of input object does not match expected dtype <U0` when invoking MLflow-deployed NLP model in SageMaker",
        "Question_body":"<p>I deployed a Huggingface Transformer model in SageMaker using MLflow's <code>sagemaker.deploy()<\/code>.<\/p>\n<p>When logging the model I used <code>infer_signature(np.array(test_example), loaded_model.predict(test_example))<\/code> to infer input and output signatures.<\/p>\n<p>Model is deployed successfully. When trying to query the model I get <code>ModelError<\/code> (full traceback below).<\/p>\n<p>To query the model, I am using precisely the same <code>test_example<\/code> that I used for <code>infer_signature()<\/code>:<\/p>\n<p><code>test_example = [['This is the subject', 'This is the body']]<\/code><\/p>\n<p>The only difference is that when querying the deployed model, I am not wrapping the test example in <code>np.array()<\/code> as that is not <code>json<\/code>-serializeable.<\/p>\n<p>To query the model I tried two different approaches:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nSAGEMAKER_REGION = 'us-west-2'\nMODEL_NAME = '...'\n\nclient = boto3.client(&quot;sagemaker-runtime&quot;, region_name=SAGEMAKER_REGION)\n\n# Approach 1\nclient.invoke_endpoint(\n                EndpointName=MODEL_NAME,\n                Body=json.dumps(test_example),\n                ContentType=&quot;application\/json&quot;,\n            )\n\n# Approach 2\nclient.invoke_endpoint(\n                EndpointName=MODEL_NAME,\n                Body=pd.DataFrame(test_example).to_json(orient=&quot;split&quot;),\n                ContentType=&quot;application\/json; format=pandas-split&quot;,\n            )\n<\/code><\/pre>\n<p>but they result in the same error.<\/p>\n<p>Will be grateful for your suggestions.<\/p>\n<p>Thank you!<\/p>\n<p>Note: I am using Python 3 and all <strong>strings are unicode<\/strong>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>---------------------------------------------------------------------------\nModelError                                Traceback (most recent call last)\n&lt;ipython-input-89-d09862a5f494&gt; in &lt;module&gt;\n      2                 EndpointName=MODEL_NAME,\n      3                 Body=test_example,\n----&gt; 4                 ContentType=&quot;application\/json; format=pandas-split&quot;,\n      5             )\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    393                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    394             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 395             return self._make_api_call(operation_name, kwargs)\n    396 \n    397         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    723             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)\n    724             error_class = self.exceptions.from_code(error_code)\n--&gt; 725             raise error_class(parsed_response, operation_name)\n    726         else:\n    727             return parsed_response\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message &quot;{&quot;error_code&quot;: &quot;BAD_REQUEST&quot;, &quot;message&quot;: &quot;dtype of input object does not match expected dtype &lt;U0&quot;}&quot;. See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/bec-sagemaker-model-test-app in account 543052680787 for more information.\n<\/code><\/pre>\n<p>Environment info:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>{'channels': ['defaults', 'conda-forge', 'pytorch'],\n 'dependencies': ['python=3.6.10',\n  'pip==21.3.1',\n  'pytorch=1.10.2',\n  'cudatoolkit=10.2',\n  {'pip': ['mlflow==1.22.0',\n    'transformers==4.17.0',\n    'datasets==1.18.4',\n    'cloudpickle==1.3.0']}],\n 'name': 'bert_bec_test_env'}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646826989870,
        "Question_score":0,
        "Question_tags":"amazon-web-services|nlp|amazon-sagemaker|mlflow",
        "Question_view_count":61,
        "Owner_creation_time":1490275561927,
        "Owner_last_access_time":1663878457720,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":83,
        "Owner_up_votes":30,
        "Owner_down_votes":0,
        "Owner_views":56,
        "Question_last_edit_time":1646837087963,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71408963",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70445997,
        "Question_title":"MLflow run within a docker container - Running with \"docker_env\" in MLflow project file",
        "Question_body":"<p>We are trying to develop an MLflow pipeline. We have our developing environment in a series of dockers (no local python environment &quot;whatsoever&quot;). This means that we have set up a docker container with MLflow and all requirements necessary to run pipelines. The issue we have is that when we write our MLflow project file we need to use &quot;docker_env&quot; to specify the environment. This figure illustrates what we want to achieve:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1tLuw.jpg\" rel=\"nofollow noreferrer\">MLflow run dind<\/a><\/p>\n<p>MLflow inside the docker needs to access the docker daemon\/service so that it can either use the &quot;docker-image&quot; in the MLflow project file or pull it from docker hub. We are aware of the possibility of using &quot;conda_env&quot; in the MLflow project file but wish to avoid this.<\/p>\n<p>Our question is,<\/p>\n<p>Do we need to set some sort of &quot;docker in docker&quot; solution to achieve our goal?<\/p>\n<p>Is it possible to set up the docker container in which MLflow is running so that it can access the &quot;host machine&quot; docker daemon?<\/p>\n<p>I have been all over Google and MLflow's documentation but I can seem to find anything that can guide us. Thanks a lot in advance for any help or pointers!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1640160788793,
        "Question_score":1,
        "Question_tags":"docker|mlflow|docker-in-docker",
        "Question_view_count":779,
        "Owner_creation_time":1546431264350,
        "Owner_last_access_time":1663848875057,
        "Owner_location":"Norway",
        "Owner_reputation":31,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1640184686952,
        "Answer_body":"<p>I managed to create my pipeline using docker and docker_env in MLflow. It is not necessary to run d-in-d, the &quot;sibling approach&quot; is sufficient. This approach is described here:<\/p>\n<p><a href=\"https:\/\/jpetazzo.github.io\/2015\/09\/03\/do-not-use-docker-in-docker-for-ci\/\" rel=\"nofollow noreferrer\">https:\/\/jpetazzo.github.io\/2015\/09\/03\/do-not-use-docker-in-docker-for-ci\/<\/a><\/p>\n<p>and it is the preferred method to avoid d-in-d.<\/p>\n<p>One needs to be very careful when mounting volumes within the primary and secondary docker environments: all volume mounts happen in the host machine.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1640385689187,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70445997",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73591190,
        "Question_title":"MLFlow & Conda: store envs in project dir instead of ~\/opt\/anaconda3\/envs",
        "Question_body":"<p>As per conda's <a href=\"https:\/\/docs.conda.io\/projects\/conda\/en\/latest\/user-guide\/tasks\/manage-environments.html#id3\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n<blockquote>\n<p>You can control where a conda environment lives by providing a path to a target directory when creating the environment. [...]:\n<code>conda create --prefix .\/envs jupyterlab=3.2 matplotlib=3.5 numpy=1.21<\/code><\/p>\n<\/blockquote>\n<p>Is it possible to modify how mlflow invoques <code>conda create<\/code> when generating all the components' environments, in order save those at the root of the project instead of the default <code>...\/anaconda3\/envs<\/code> ?<\/p>\n<p>Many thanks in advance for your help,<br \/>\nKind regards<br \/>\nMarc<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1662198074837,
        "Question_score":0,
        "Question_tags":"conda|mlflow|anaconda3",
        "Question_view_count":18,
        "Owner_creation_time":1598781108953,
        "Owner_last_access_time":1664084623343,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1662198174732,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73591190",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":66282143,
        "Question_title":"Sagemaker Train Job can't connect to ec2 instance",
        "Question_body":"<p>I have MLFlow server running on ec2 instance, port 5000.<\/p>\n<p>This ec2 instance has security group with opened TCP connection on port 5000 to another security group designated for SageMaker.<\/p>\n<p>ec2 instance inbound rules:\n<a href=\"https:\/\/i.stack.imgur.com\/VXwid.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VXwid.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>SageMaker outbound rules:\n<a href=\"https:\/\/i.stack.imgur.com\/ZUzek.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZUzek.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>These 2 security groups are in the same VPC<\/p>\n<p>Now, I try to run SageMaker training job with designated security group, so that the training script will log metrics to ec2 server via internal IP address. (As answered <a href=\"https:\/\/stackoverflow.com\/questions\/45416882\/aws-security-group-include-another-security-group\">here<\/a>), but connection fails<\/p>\n<p>SageMaker job init:<\/p>\n<pre><code>   role = &quot;ml_sagemaker&quot;\n   security_group_ids = ['sg-04868acca16e81183']\n   bucket = sagemaker_session.default_bucket()  \n   out_path = f&quot;s3:\/\/{bucket}\/{project_name}&quot;\n\n   estimator = PyTorch(entry_point='run_train.py',\n                       source_dir='.',\n                       sagemaker_session=sagemaker_session,\n                       instance_type=instance_type,\n                       instance_count=1,\n                       framework_version='1.5.0',\n                       py_version='py3',\n                       role=role,\n                       security_group_ids=security_group_ids,\n                       hyperparameters={},\n                       )\n   ....\n\n<\/code><\/pre>\n<p>Inside <code>run_train.py<\/code>:<\/p>\n<pre><code>import mlflow\ntracking_uri = &quot;http:\/\/172.31.77.137:5000&quot;  # &lt;- this is internal ec2 IP\nmlflow.set_tracking_uri(tracking_uri)\nmlflow.log_param(&quot;test_param&quot;, 3)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/urllib3\/util\/connection.py&quot;, line 74, in create_connection\n    sock.connect(sa)\nTimeoutError: [Errno 110] Connection timed out\n<\/code><\/pre>\n<p><strong>However<\/strong>, when when I create SageMaker Notebook instance with the same security group and the same IAM role, I am able to successfully connect to ec2 and log metrics from within the Notebook.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/YYHlO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YYHlO.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Here is SageMaker Notebook configurations:<\/p>\n<img src=\"https:\/\/i.stack.imgur.com\/bslu8.png\" width=\"300\" \/>\n<p>How can I connect to ec2 instance from SageMaker Training Job?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1613755129957,
        "Question_score":1,
        "Question_tags":"amazon-ec2|amazon-vpc|amazon-sagemaker|aws-security-group|mlflow",
        "Question_view_count":608,
        "Owner_creation_time":1438098365740,
        "Owner_last_access_time":1663853069297,
        "Owner_location":null,
        "Owner_reputation":653,
        "Owner_up_votes":216,
        "Owner_down_votes":1,
        "Owner_views":76,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66282143",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":57237388,
        "Question_title":"MLFlow project run fails during conda env creation",
        "Question_body":"<p>I am trying to get mlflow mlproject working.<\/p>\n\n<p>When i run the mlflow run with repo name<\/p>\n\n<pre><code>mlflow run  git@gitlabe2.xx.yy.zz:name\/mlflow-example.git\n<\/code><\/pre>\n\n<p>The execution fails with the below error<\/p>\n\n<pre><code>File \"\/home\/example\/miniconda\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/projects\/__init__.py\", line 265, in run\nuse_conda=use_conda, storage_dir=storage_dir, synchronous=synchronous, run_id=run_id)\nFile \"\/home\/example\/miniconda\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/projects\/__init__.py\", line 144, in _run\nconda_env_name = _get_or_create_conda_env(project.conda_env_path)\nFile \"\/home\/example\/miniconda\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/projects\/__init__.py\", line 144, in _run\nconda_env_name = _get_or_create_conda_env(project.conda_env_path)\nFile \"\/home\/example\/miniconda\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/projects\/__init__.py\", line 498, in _get_or_create_conda_env\nconda_env_path], stream_output=True)\nFile \"\/home\/example\/miniconda\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/utils\/process.py\", line 38, in exec_cmd\nraise ShellCommandException(\"Non-zero exitcode: %s\" % (exit_code))\n<\/code><\/pre>\n\n<p>Any pointers on where I should look ?<\/p>\n\n<p>The suspect the conda.yaml file has some issues especially the conda env name.\nI have different names for the environment where the project is created and where the project is being run. Does it matter ?<\/p>\n\n<p>Thanks<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1564278857600,
        "Question_score":1,
        "Question_tags":"conda|mlflow",
        "Question_view_count":917,
        "Owner_creation_time":1466129156870,
        "Owner_last_access_time":1664082712907,
        "Owner_location":"Chengdu, Sichuan, China",
        "Owner_reputation":2087,
        "Owner_up_votes":31,
        "Owner_down_votes":1,
        "Owner_views":87,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57237388",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68718719,
        "Question_title":"How can I retrive the model.pkl in the experiment in Databricks",
        "Question_body":"<p>I want to retrieve the pickle off my trained model, which I know is in the run file inside my experiments in Databricks.<\/p>\n<p>It seems that the <code>mlflow.pyfunc.load_model<\/code> can only do the <code>predict<\/code> method.<\/p>\n<p>There is an option to directly access the pickle?<\/p>\n<p>I also tried to use the path in the run using the <code>pickle.load(path)<\/code> (example of path: dbfs:\/databricks\/mlflow-tracking\/20526156406\/92f3ec23bf614c9d934dd0195\/artifacts\/model\/model.pkl).<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1628544411170,
        "Question_score":1,
        "Question_tags":"python|azure|databricks|datastore|mlflow",
        "Question_view_count":3081,
        "Owner_creation_time":1522076554450,
        "Owner_last_access_time":1663901377960,
        "Owner_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Owner_reputation":96,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I recently found the solution which can be done by the following two approaches:<\/p>\n<ol>\n<li>Use the customized predict function at the moment of saving the model (check <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">databricks<\/a> documentation for more details).<\/li>\n<\/ol>\n<p>example give by Databricks<\/p>\n<pre><code>class AddN(mlflow.pyfunc.PythonModel):\n\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input):\n        return model_input.apply(lambda column: column + self.n)\n# Construct and save the model\nmodel_path = &quot;add_n_model&quot;\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n<\/code><\/pre>\n<ol start=\"2\">\n<li>Load the model artefacts as we are downloading the artefact:<\/li>\n<\/ol>\n<pre><code>from mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\n\ntmp_path = client.download_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path='model\/model.pkl')\n\nf = open(tmp_path,'rb')\n\nmodel = pickle.load(f)\n\nf.close()\n\n \n\nclient.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;&quot;)\n\nclient.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;model&quot;)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1629748421287,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1643054905512,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68718719",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56120016,
        "Question_title":"Is it possible to specify MLflow project Environment through a Dockerfile (instead of an image)?",
        "Question_body":"<p>To my understanding, currently (May 2019) mlflow support running project in docker environment; however, it needs the docker image already been built. This leaves the docker image building to be a separate workflow. What is the suggested way to run a mlflow project from Dockerfile? <\/p>\n\n<p>Is there plans to support targeting Dockerfile natively in mlflow? What are the considerations about using image vs Dockerfile? Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1557782434943,
        "Question_score":2,
        "Question_tags":"docker|machine-learning|artificial-intelligence|databricks|mlflow",
        "Question_view_count":631,
        "Owner_creation_time":1386735502123,
        "Owner_last_access_time":1663974468663,
        "Owner_location":null,
        "Owner_reputation":3405,
        "Owner_up_votes":641,
        "Owner_down_votes":8,
        "Owner_views":1094,
        "Question_last_edit_time":1557791232507,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56120016",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73399640,
        "Question_title":"Track input transformation for keras-flavored mlflow models",
        "Question_body":"<p><strong>TL\/DR:<\/strong> How to track and serve the input transformation for keras-flavored mlflow models?<\/p>\n<p>Neural network training usually involves preprocessing steps in which<\/p>\n<ul>\n<li>continuous variables are scaled and shifted to have unit width and zero mean,<\/li>\n<li>categorical variables (integer or string) are transformed to one-hot encoding.<\/li>\n<\/ul>\n<p>When the model is applied to new data, the scaling weights and the category-to-index association needs to be known.<\/p>\n<p>In keras there are generally two options to perform preprocessing:<\/p>\n<ul>\n<li><strong>Option 1:<\/strong> Using <a href=\"https:\/\/keras.io\/guides\/preprocessing_layers\/\" rel=\"nofollow noreferrer\">preprocessing layers<\/a>, or<\/li>\n<li><strong>Option 2:<\/strong> perform the transformation before the training when the dataset is loaded.<\/li>\n<\/ul>\n<p>With <strong>Option 1<\/strong>, the transformation is part of the model and will be automatically applied when the network is used and served as a <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html\" rel=\"nofollow noreferrer\">mlflow model<\/a>.<\/p>\n<p>My question concerns <strong>Option 2<\/strong>: What is the recommended way<\/p>\n<ul>\n<li>to keep track of the input transformation in mlflow for different experiments,<\/li>\n<li>and how to apply the same transformations when the model is served, e.g. with <code>mlflow model serve<\/code>?<\/li>\n<\/ul>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1660810680010,
        "Question_score":1,
        "Question_tags":"keras|mlflow",
        "Question_view_count":49,
        "Owner_creation_time":1302064833600,
        "Owner_last_access_time":1663871200727,
        "Owner_location":null,
        "Owner_reputation":3635,
        "Owner_up_votes":460,
        "Owner_down_votes":7,
        "Owner_views":424,
        "Question_last_edit_time":1660979496129,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73399640",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":57078147,
        "Question_title":"How should I mount docker volumes in mlflow project?",
        "Question_body":"<p>I use <code>mlflow<\/code> in a docker environment as described in this <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker\" rel=\"nofollow noreferrer\">example<\/a> and I start my runs with <code>mlflow run .<\/code>.<\/p>\n\n<p>I get output like this<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>2019\/07\/17 16:08:16 INFO mlflow.projects: === Building docker image mlflow-myproject-ab8e0e4 ===\n2019\/07\/17 16:08:18 INFO mlflow.projects: === Created directory \/var\/folders\/93\/xt2vz36s7jd1fh9bkhkk9sgc0000gn\/T\/tmp1lxyqqw9 for downloading remote URIs passed to arguments of type 'path' ===\n2019\/07\/17 16:08:18 INFO mlflow.projects: === Running command 'docker run \n--rm -v \/Users\/foo\/bar\/mlruns:\/mlflow\/tmp\/mlruns -e \nMLFLOW_RUN_ID=ef21de61d8a6436b97b643e5cee64ae1 -e MLFLOW_TRACKING_URI=file:\/\/\/mlflow\/tmp\/mlruns -e MLFLOW_EXPERIMENT_ID=0 mlflow-myproject-ab8e0e4 python train.py' in run with ID 'ef21de61d8a6436b97b643e5cee64ae1' ===\n<\/code><\/pre>\n\n<p>I would like to mount a docker volume named <code>my_docker_volume<\/code> to the container\n at \nthe path <code>\/data<\/code>. So instead of the <code>docker run<\/code> shown above, I would like to\n use<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>docker run --rm --mount source=my_docker_volume,target=\/data -v \/Users\/foo\/bar\/mlruns:\/mlflow\/tmp\/mlruns -e MLFLOW_RUN_ID=ef21de61d8a6436b97b643e5cee64ae1 -e MLFLOW_TRACKING_URI=file:\/\/\/mlflow\/tmp\/mlruns -e MLFLOW_EXPERIMENT_ID=0 mlflow-myproject-ab8e0e4 python train.py\n<\/code><\/pre>\n\n<p>I see that I could in principle run it once without mounted volume and then \ncopy the <code>docker run ...<\/code> and add <code>--mount source=my_volume,target=\/data<\/code> but\n I'd rather use something like<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>mlflow run --mount source=my_docker_volume,target=\/data .\n<\/code><\/pre>\n\n<p>but this obviously doesn't work because --mount is not a parameter for \n<code>mlflow run<\/code>.\nWhat's the recommened way of mounting a docker volume then?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1563373349400,
        "Question_score":4,
        "Question_tags":"docker|mlflow",
        "Question_view_count":1301,
        "Owner_creation_time":1375955724343,
        "Owner_last_access_time":1664025245257,
        "Owner_location":"Freiburg im Breisgau, Germany",
        "Owner_reputation":191,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1564754919900,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57078147",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69551533,
        "Question_title":"Error can't get attribute Net when saving PyTorch model with MLFlow",
        "Question_body":"<p>After installing MLFlow using <a href=\"https:\/\/github.com\/artefactory\/one-click-mlflow\" rel=\"nofollow noreferrer\">one-click-mlflow<\/a> I save a pytorch model using the default command that I found in the user guide. You can find the command bellow:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.pytorch.log_model(net, artifact_path=&quot;model&quot;, pickle_module=pickle)\n<\/code><\/pre>\n<p>The neural network saved is very simple, this is basically a two layer neural network with Xavier initialization and hyperbolic tangent as activation function.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class Net(T.nn.Module):\n    \n    def __init__(self):\n        super(Net, self).__init__()\n        self.hid1 = T.nn.Linear(n_features, 10)\n        self.hid2 = T.nn.Linear(10, 10)\n        self.oupt = T.nn.Linear(10, 1)\n        T.nn.init.xavier_uniform_(self.hid1.weight) \n        T.nn.init.zeros_(self.hid1.bias)\n        T.nn.init.xavier_uniform_(self.hid2.weight)\n        T.nn.init.zeros_(self.hid2.bias)\n        T.nn.init.xavier_uniform_(self.oupt.weight)\n        T.nn.init.zeros_(self.oupt.bias)\n        \n    def forward(self, x):\n        z = T.tanh(self.hid1(x))\n        z = T.tanh(self.hid2(z))\n        z = self.oupt(z)\n        return z\n<\/code><\/pre>\n<p>Every things is runing fine in the Jupyter Notebook. I can log metrics and other artifact but when I save the model I got the following error message:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>2021\/10\/13 09:21:00 WARNING mlflow.utils.requirements_utils: Found torch version (1.9.0+cu111) contains a local version label (+cu111). MLflow logged a pip requirement for this package as 'torch==1.9.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2021\/10\/13 09:21:00 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.10.0+cu111) contains a local version label (+cu111). MLflow logged a pip requirement for this package as 'torchvision==0.10.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2021\/10\/13 09:21:01 ERROR mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: \/tmp\/tmpnl9dsoye\/model\/data, flavor: pytorch)\nTraceback (most recent call last):\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/utils\/environment.py&quot;, line 212, in infer_pip_requirements\n    return _infer_requirements(model_uri, flavor)\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/utils\/requirements_utils.py&quot;, line 263, in _infer_requirements\n    modules = _capture_imported_modules(model_uri, flavor)\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/utils\/requirements_utils.py&quot;, line 221, in _capture_imported_modules\n    _run_command(\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/utils\/requirements_utils.py&quot;, line 173, in _run_command\n    raise MlflowException(msg)\nmlflow.exceptions.MlflowException: Encountered an unexpected error while running ['\/home\/ucsky\/.virtualenv\/mymodel\/bin\/python', '\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/utils\/_capture_modules.py', '--model-path', '\/tmp\/tmpnl9dsoye\/model\/data', '--flavor', 'pytorch', '--output-file', '\/tmp\/tmplyj0w2fr\/imported_modules.txt', '--sys-path', '[&quot;\/home\/ucsky\/project\/ofi-ds-research\/incubator\/ofi-pe-fr\/notebook\/guillaume-simon\/06-modelisation-pytorch&quot;, &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/git\/ext\/gitdb&quot;, &quot;\/usr\/lib\/python39.zip&quot;, &quot;\/usr\/lib\/python3.9&quot;, &quot;\/usr\/lib\/python3.9\/lib-dynload&quot;, &quot;&quot;, &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages&quot;, &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/IPython\/extensions&quot;, &quot;\/home\/ucsky\/.ipython&quot;, &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/gitdb\/ext\/smmap&quot;]']\nexit status: 1\nstdout: \nstderr: Traceback (most recent call last):\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/utils\/_capture_modules.py&quot;, line 125, in &lt;module&gt;\n    main()\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/utils\/_capture_modules.py&quot;, line 118, in main\n    importlib.import_module(f&quot;mlflow.{flavor}&quot;)._load_pyfunc(model_path)\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/pytorch\/__init__.py&quot;, line 723, in _load_pyfunc\n    return _PyTorchWrapper(_load_model(path, **kwargs))\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/pytorch\/__init__.py&quot;, line 626, in _load_model\n    return torch.load(model_path, **kwargs)\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/torch\/serialization.py&quot;, line 607, in load\n    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/torch\/serialization.py&quot;, line 882, in _load\n    result = unpickler.load()\n  File &quot;\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/torch\/serialization.py&quot;, line 875, in find_class\n    return super().find_class(mod_name, name)\nAttributeError: Can't get attribute 'Net' on &lt;module '__main__' from '\/home\/ucsky\/.virtualenv\/mymodel\/lib\/python3.9\/site-packages\/mlflow\/utils\/_capture_modules.py'&gt;\n<\/code><\/pre>\n<p>Can somebody explain me what is wrong?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1634111089723,
        "Question_score":2,
        "Question_tags":"python|pytorch|virtualenv|mlflow|mlops",
        "Question_view_count":343,
        "Owner_creation_time":1263142631610,
        "Owner_last_access_time":1661432787853,
        "Owner_location":null,
        "Owner_reputation":382,
        "Owner_up_votes":248,
        "Owner_down_votes":3,
        "Owner_views":61,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69551533",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":64099250,
        "Question_title":"MLFlow in docker - unable to store artifacts in sftp server (atmoz)",
        "Question_body":"<p>I would like to run MLflow &quot;entirely offline&quot; using docker (i.e. no cloud storage like S3 or blob). So I followed <a href=\"https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039\" rel=\"nofollow noreferrer\">this guide<\/a> and tried to set the artifact store to the <a href=\"https:\/\/github.com\/atmoz\/sftp\" rel=\"nofollow noreferrer\">atmoz<\/a> sftp server running inside another docker container. As suggested in the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#sftp-server\" rel=\"nofollow noreferrer\">MLFlow docs<\/a>, I try to auth with host keys, however, when I try to register my artifact I receive the following error <code>pysftp.exceptions.CredentialException: No password or key specified.<\/code><\/p>\n<p>I guess, there's something wrong with my hostkey setup. I also tried to follow <a href=\"https:\/\/towardsdatascience.com\/setup-mlflow-in-production-d72aecde7fef\" rel=\"nofollow noreferrer\">this guide<\/a> (mentioned in <a href=\"https:\/\/towardsdatascience.com\/setup-mlflow-in-production-d72aecde7fef\" rel=\"nofollow noreferrer\">this question<\/a>), but unfortunately it didn't have enough details for my - limited - knowledge of containers, sftp servers and pub-priv-key setups. My docker-compose looks like this...<\/p>\n<pre><code>services:\ndb:\n    restart: always\n    image: mysql\/mysql-server:5.7.28\n    container_name: mlflow_db\n    expose:\n        - &quot;3306&quot;\n    networks:\n        - backend\n    environment:\n        - MYSQL_DATABASE=${MYSQL_DATABASE}\n        - MYSQL_USER=${MYSQL_USER}\n        - MYSQL_PASSWORD=${MYSQL_PASSWORD}\n        - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}\n    volumes:\n        - dbdata:\/var\/lib\/mysql\n\nmlflow-sftp:\n    image: atmoz\/sftp\n    container_name: mlflow-sftp\n    ports:\n        - &quot;2222:22&quot;\n    volumes:\n        - .\/storage\/sftp:\/home\/foo\/storage\n        - .\/ssh_host_ed25519_key:\/home\/foo\/.ssh\/ssh_host_ed25519_key.pub:ro\n        - .\/ssh_host_rsa_key:\/home\/foo\/.ssh\/ssh_host_rsa_key.pub:ro\n    command: foo::1001\n    networks:\n        - backend\n    \nweb:\n    restart: always\n    build: .\/mlflow\n    depends_on:\n        - mlflow-sftp\n    image: mlflow_server\n    container_name: mlflow_server\n    expose:\n        - &quot;5000&quot;\n    networks:\n        - frontend\n        - backend\n    volumes:\n        - .\/ssh_host_ed25519_key:\/root\/.ssh\/ssh_host_ed25519_key:ro\n        - .\/ssh_host_rsa_key:\/root\/.ssh\/ssh_host_rsa_key:ro\n    command: &gt;\n        bash -c &quot;sleep 3\n        &amp;&amp; ssh-keyscan -H mlflow-sftp &gt;&gt; ~\/.ssh\/known_hosts\n        &amp;&amp; mlflow server --backend-store-uri mysql+pymysql:\/\/${MYSQL_USER}:${MYSQL_PASSWORD}@db:3306\/${MYSQL_DATABASE} --default-artifact-root sftp:\/\/foo@localhost:2222\/storage --host 0.0.0.0&quot;\n    \nnginx:\n    restart: always\n    build: .\/nginx\n    image: mlflow_nginx\n    container_name: mlflow_nginx\n    ports:\n        - &quot;80:80&quot;\n    networks:\n        - frontend\n    depends_on:\n        - web\n<\/code><\/pre>\n<p>networks:\nfrontend:\ndriver: bridge\nbackend:\ndriver: bridge<\/p>\n<p>volumes:\ndbdata:<\/p>\n<p>... and in my python script I create a new mlflow experiment as follows.<\/p>\n<pre><code>remote_server_uri = &quot;http:\/\/localhost:80&quot; \nmlflow.set_tracking_uri(remote_server_uri)\nEXPERIMENT_NAME = &quot;test43&quot;\nmlflow.create_experiment(EXPERIMENT_NAME) #, artifact_location=ARTIFACT_URI)\nmlflow.set_experiment(EXPERIMENT_NAME)\nEXPERIMENT_NAME = &quot;test43&quot;\nmlflow.create_experiment(EXPERIMENT_NAME) #, artifact_location=ARTIFACT_URI)\nmlflow.set_experiment(EXPERIMENT_NAME)\nwith mlflow.start_run():\n    print(mlflow.get_artifact_uri())\n    print(mlflow.get_registry_uri())\n    lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n    lr.fit(train_x, train_y)\n\n    predicted_qualities = lr.predict(test_x)\n\n    (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n    print(&quot;Elasticnet model (alpha=%f, l1_ratio=%f):&quot; % (alpha, l1_ratio))\n    print(&quot;  RMSE: %s&quot; % rmse)\n    print(&quot;  MAE: %s&quot; % mae)\n    print(&quot;  R2: %s&quot; % r2)\n\n    mlflow.log_param(&quot;alpha&quot;, alpha)\n    mlflow.log_param(&quot;l1_ratio&quot;, l1_ratio)\n    mlflow.log_metric(&quot;rmse&quot;, rmse)\n    mlflow.log_metric(&quot;r2&quot;, r2)\n    mlflow.log_metric(&quot;mae&quot;, mae)\n\n    tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n\n    if tracking_url_type_store != &quot;file&quot;:\n        mlflow.sklearn.log_model(lr, &quot;model&quot;, registered_model_name=&quot;ElasticnetWineModel&quot;)\n    else:\n        mlflow.sklearn.log_model(lr, &quot;model&quot;)\n<\/code><\/pre>\n<p>I haven't modified the dockerfiles of the first mentioned guide i.e. you'll be able to see them <a href=\"https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039\" rel=\"nofollow noreferrer\">here<\/a>. My guess is that I messed something up with the host keys, maybe put them in a wrong directory, but after several hours of brute-force experimenting I hope someone can help me with a pointer in the right direction. Let me know if there's anything missing to reproduce the error.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1601284508343,
        "Question_score":4,
        "Question_tags":"python|docker|public-key|mlflow",
        "Question_view_count":874,
        "Owner_creation_time":1517932915310,
        "Owner_last_access_time":1662728503583,
        "Owner_location":"Germany",
        "Owner_reputation":41,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64099250",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":64462918,
        "Question_title":"How to save models in MLFlow with R and get Stages of them in Azure Databricks?",
        "Question_body":"<p>I would like to save a model in MLFlow with Azure Databricks. In Python, I can use the following code to save a model with a name automatically:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.spark.log_model(\n        model,\n        artifact_path = 'model_prueba',\n        registered_model_name = 'model_prueba'\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/tTaNw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tTaNw.png\" alt=\"Registered models window\" \/><\/a><\/p>\n<p>But I am trying to do the same with <strong>R<\/strong> with the following code:<\/p>\n<pre class=\"lang-r prettyprint-override\"><code>mlflow_log_model(\n          model,\n          artifact_path = 'model_prueba_R',\n          registered_model_name = 'model_prueba_R'\n    )\n<\/code><\/pre>\n<p>But it does not register any model in the Models section. It only saves the model with the artifact path in the run section.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/zfAza.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zfAza.png\" alt=\"Artifact location\" \/><\/a><\/p>\n<p>Anyone could tell me the way to save the model for staging automatically with code in R?<\/p>\n<p>Thank you very much!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1603281092693,
        "Question_score":1,
        "Question_tags":"python|r|model|azure-databricks|mlflow",
        "Question_view_count":201,
        "Owner_creation_time":1568015757070,
        "Owner_last_access_time":1642423173287,
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64462918",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59881297,
        "Question_title":"How to serve custom MLflow model with Docker?",
        "Question_body":"<p>We have a project following essentially this\n<a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker\" rel=\"nofollow noreferrer\">docker example<\/a> with the only difference that we created a custom model similar to <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">this<\/a> whose code lies in a directory called <code>forecast<\/code>. We succeeded in running the model with <code>mlflow run<\/code>. The problem arises when we try to serve the model. After doing <\/p>\n\n<pre><code>mlflow models build-docker -m \"runs:\/my-run-id\/my-model\" -n \"my-image-name\"\n<\/code><\/pre>\n\n<p>we fail running the container with<\/p>\n\n<pre><code>docker run -p 5001:8080 \"my-image-name\"\n<\/code><\/pre>\n\n<p>with the following error:<\/p>\n\n<pre><code>ModuleNotFoundError: No module named 'forecast'\n<\/code><\/pre>\n\n<p>It seems that the docker image is not aware of the source code defining our custom model class.\nWith Conda environnement the problem does not arise thanks to the <code>code_path<\/code> argument in <code>mlflow.pyfunc.log_model<\/code>.<\/p>\n\n<p>Our Dockerfile is very basic, with just <code>FROM continuumio\/miniconda3:4.7.12, RUN pip install {model_dependencies}<\/code>.<\/p>\n\n<p>How to let the docker image know about the source code for deserialising the model and run it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579791133580,
        "Question_score":3,
        "Question_tags":"docker|mlflow",
        "Question_view_count":2105,
        "Owner_creation_time":1429204620943,
        "Owner_last_access_time":1662844490857,
        "Owner_location":"Paris, France",
        "Owner_reputation":41,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59881297",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73570230,
        "Question_title":"How to access Mlflow running on fargate (ECS) with only VPN in\/outbound rules from sagemaker notebook instance?",
        "Question_body":"<p><strong>Context:<\/strong><\/p>\n<p>I have deployed Mlflow on ECS(Fargate) using terraform using this public <a href=\"https:\/\/github.com\/Glovo\/terraform-aws-mlflow\" rel=\"nofollow noreferrer\">git-repo<\/a>. After deploying Mlflow which was publicly accessible using the link, I made some changes in the security group and changed in\/outbound rule to the only company VPN ips, now that link is only accessible under the VPN.<\/p>\n<p><strong>Question:<\/strong><\/p>\n<p>Now I have Sagemake notebook instance and want to access that link inside the notebook and the notebook is running on AWS internet(outside Company-VPN) and I'm not able to access that link. What could be the possible solution?<\/p>\n<p>I don't want to open access of Mlflow-link publicaly to accessible form anywhere on the internet.<\/p>\n<p><strong>Running this code on notebook:<\/strong><\/p>\n<pre><code>!pip install mlflow\nimport mlflow\nmlflow.set_tracking_uri(&quot;http:\/\/mlflow-mlp-xyz-xyz.eu-west-1.elb.amazonaws.com\/&quot;)\nmlflow.get_experiment_by_name('mlpmlflowlogger')\ncurrent_experiment=dict(mlflow.get_experiment_by_name('mlpmlflowlogger'))\nprint(current_experiment)\nexperiment_id=current_experiment['experiment_id']\nprint(experiment_id)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1662039083460,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-ecs|amazon-vpc|aws-security-group|mlflow",
        "Question_view_count":31,
        "Owner_creation_time":1511960637980,
        "Owner_last_access_time":1663681247487,
        "Owner_location":"Deggendorf, Germany",
        "Owner_reputation":813,
        "Owner_up_votes":87,
        "Owner_down_votes":1,
        "Owner_views":115,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73570230",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":44053141,
        "Question_title":"Can I make Neptune talk to git?",
        "Question_body":"<p>In <a href=\"https:\/\/neptune.ml\/\" rel=\"nofollow noreferrer\">Neptune<\/a> (this machine learning experiment tracker) is it possible to make it git-aware? I mean - using <code>.gitignore<\/code> for excluded files and saving commit hashes for each run?<\/p>\n\n<p>In particular, when I review an already finished job, can I go directly to GitHub commit?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1495124614783,
        "Question_score":0,
        "Question_tags":"git|github|machine-learning|neptune",
        "Question_view_count":140,
        "Owner_creation_time":1314097464770,
        "Owner_last_access_time":1663601506563,
        "Owner_location":"Warsaw, Poland",
        "Owner_reputation":11056,
        "Owner_up_votes":1563,
        "Owner_down_votes":3,
        "Owner_views":544,
        "Question_last_edit_time":1503919276067,
        "Answer_body":"<p>Starting form version 2.0 Neptune provides integration with git, see: <a href=\"https:\/\/docs.neptune.ml\/advanced-topics\/git-integration\/\" rel=\"nofollow noreferrer\">https:\/\/docs.neptune.ml\/advanced-topics\/git-integration\/<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1503919410423,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44053141",
        "Question_exclusive_tag":"Neptune"
    },
    {
        "Question_id":72143195,
        "Question_title":"Running Neptune in Jupyter Notebook gives NameError that Neptune is not defined",
        "Question_body":"<pre><code># Create run in project (Neptune)\nrun = neptune.init(project='ssraghuvanshi1989\/GCI-01-Lung-CT-Segmentation-20220506')\n<\/code><\/pre>\n<hr \/>\n<pre><code>NameError                    Traceback (most recent call last)\nC:\\Users\\SAURAB~1\\AppData\\Local\\Temp\/ipykernel_27104\/3392926562.py in &lt;module&gt;\n      1 # Create run in project\n----&gt; 2 run = neptune.init(project='ssraghuvanshi1989\/GCI-01-Lung-CT-Segmentation-20220506')\n\nNameError: name 'neptune' is not defined\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1651847578783,
        "Question_score":0,
        "Question_tags":"python|google-colaboratory|nameerror|mlops|neptune",
        "Question_view_count":67,
        "Owner_creation_time":1423107401473,
        "Owner_last_access_time":1653027951467,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1651848652763,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72143195",
        "Question_exclusive_tag":"Neptune"
    },
    {
        "Question_id":73833935,
        "Question_title":"Running Neptune.ai in a loop",
        "Question_body":"<p>so i created a for loop so I can run various batch sizes, where each loop will open and close a neptune run. The first time runs fine, but the following runs, the accuracy doesn't record into neptune, and python does not throw an error? Can anyone think what the problem may be?<\/p>\n<pre><code>for i in range(len(percentage)):\n\n    run = neptune.init(\n        project=&quot;xxx&quot;,\n        api_token=&quot;xxx&quot;,\n    )\n\n    epochs = 600\n    batch_perc = percentage[i]\n    lr = 0.001\n    sb = 64 #round((43249*batch_perc)*0.00185)\n    params = {\n        'lr': lr,\n        'bs': sb,\n        'epochs': epochs,\n        'batch %': batch_perc\n    }\n    run['parameters'] = params\n\n    torch.manual_seed(12345)\n    td = 43249 * batch_perc\n    vd = 0.1*(43249 - td) + td\n\n    train_dataset = dataset[:round(td)]\n    val_dataset = dataset[round(td):round(vd)]\n    test_dataset = dataset[round(vd):]\n\n    print(f'Number of training graphs: {len(train_dataset)}')\n    run['train'] = len(train_dataset)\n    print(f'Number of validation graphs: {len(val_dataset)}')\n    run['val'] = len(val_dataset)\n    print(f'Number of test graphs: {len(test_dataset)}')\n    run['test'] = len(test_dataset)\n\n    train_loader = DataLoader(train_dataset, batch_size=sb, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=sb, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\n    model = GCN(hidden_channels=64).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = torch.nn.CrossEntropyLoss()\n\n    for epoch in range(1, epochs):\n        train()\n        train_acc = test(train_loader)\n        run['training\/batch\/acc'].log(train_acc)\n        val_acc = test(val_loader)\n        run['training\/batch\/val'].log(val_acc)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663978074247,
        "Question_score":0,
        "Question_tags":"python|neptune",
        "Question_view_count":27,
        "Owner_creation_time":1648215317920,
        "Owner_last_access_time":1664077689273,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73833935",
        "Question_exclusive_tag":"Neptune"
    },
    {
        "Question_id":64713492,
        "Question_title":"Is there a way to log the keras model summary to neptune?",
        "Question_body":"<p>I am working on a CNN project and I would like to log the model.summary to neptune.ai. The intention of that is to have an idea about the model parameters while comparing different models. Any help\/tips would be much appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1604660980577,
        "Question_score":1,
        "Question_tags":"python|keras|deep-learning|neptune",
        "Question_view_count":285,
        "Owner_creation_time":1604146329127,
        "Owner_last_access_time":1662133039343,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1660057709880,
        "Answer_body":"<p>You can log <code>model.summary<\/code> (assuming it's keras), like this:<\/p>\n<pre><code>neptune.init('workspace\/project')\nneptune.create_experiment()\n\nmodel = keras.Sequential(...)\nmodel.summary(print_fn=lambda x: neptune.log_text('model_summary', x))\n<\/code><\/pre>\n<p>This will log entire summary as lines of text. You can later browse it in the <em>Logs<\/em> section of the experiment. Look for tile: &quot;model_summary&quot; in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/e\/HELLO-325\/logs\" rel=\"nofollow noreferrer\">example<\/a>.<\/p>\n<p>Another option - for easier compare - is to log hyper-parameters at experiment creation, like this:<\/p>\n<pre><code># Define parameters as Python dict\nPARAMS = {'batch_size': 64,\n          'n_epochs': 100,\n          'shuffle': True,\n          'activation': 'elu'}\n\n# Pass PARAMS dict to params at experiment creation\nneptune.create_experiment(params=PARAMS)\n<\/code><\/pre>\n<p>You will have them in <em>Parameters<\/em> tab of the experiment, like in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/e\/HELLO-44\/parameters\" rel=\"nofollow noreferrer\">example<\/a>. You will be able to add each parameter as a column to the dashboard for quick compare. Look for greenish columns in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/experiments?viewId=d7f80ebe-5bfe-4d12-97c1-2b1e6184a2ed\" rel=\"nofollow noreferrer\">dashboard<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1604748950752,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64713492",
        "Question_exclusive_tag":"Neptune"
    },
    {
        "Question_id":71857090,
        "Question_title":"jsonschema 4.4.0 does not provide the extra 'isoduration'",
        "Question_body":"<p>So I'm trying to run some piece of code and keep getting the following error:<\/p>\n<pre><code>File &quot;\/opt\/conda\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py&quot;, line 770, in resolve\n    raise DistributionNotFound(req, requirers)\npkg_resources.DistributionNotFound: The 'isoduration; extra == &quot;format&quot;' distribution was not found and is required by jsonschema\n<\/code><\/pre>\n<p>However, after running<\/p>\n<pre><code>pip uninstall -y jsonschema &amp;&amp; pip install -U jsonschema &amp;&amp; pip install jsonschema[isoduration]\n<\/code><\/pre>\n<p>I get<\/p>\n<pre><code>Requirement already satisfied: jsonschema[isoduration] in \/opt\/conda\/lib\/python3.8\/site-packages (4.4.0)\n  WARNING: jsonschema 4.4.0 does not provide the extra 'isoduration'\n<\/code><\/pre>\n<p>and surely, running my code again I get the same error message as before.<\/p>\n<p><strong>I tried:<\/strong><\/p>\n<ul>\n<li><code>pip install isoduration<\/code>, but different format showed up as\nmissing<\/li>\n<li>hard removing <code>jsonschema<\/code> with <code>rm -rf ...<\/code><\/li>\n<li>installing <code>jsonschema==3.2.0<\/code> as it supposedly worked for a friend of mine<\/li>\n<\/ul>\n<p>I'm very confused with what's going on here, any help would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649850905623,
        "Question_score":1,
        "Question_tags":"python|jsonschema|kedro|neptune",
        "Question_view_count":74,
        "Owner_creation_time":1532254746377,
        "Owner_last_access_time":1656938822310,
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1649853689192,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71857090",
        "Question_exclusive_tag":"Neptune"
    },
    {
        "Question_id":44053028,
        "Question_title":"Setting job tags for Neptune",
        "Question_body":"<p>Recently I started using Neptune (via <a href=\"https:\/\/go.neptune.deepsense.io\/\" rel=\"nofollow noreferrer\">Neptune Go<\/a>) and want to have a well-organised history of experiments. How to set tags to a given experiment? (Do I do it before running it, or after?)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1495124248987,
        "Question_score":1,
        "Question_tags":"tags|neptune",
        "Question_view_count":57,
        "Owner_creation_time":1314097464770,
        "Owner_last_access_time":1663601506563,
        "Owner_location":"Warsaw, Poland",
        "Owner_reputation":11056,
        "Owner_up_votes":1563,
        "Owner_down_votes":3,
        "Owner_views":544,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There are four ways to set tags to your experiment:<\/p>\n\n<ol>\n<li>In the <code>run\/enqueue\/exec<\/code> command, i.e:<\/li>\n<\/ol>\n\n<p><code>neptune run --tags tag1 tag2 tag3 tag4<\/code><\/p>\n\n<ol start=\"2\">\n<li>In the configuration file:<\/li>\n<\/ol>\n\n<p><code>tags: [tag1, tag2, tag3, tag4]<\/code><\/p>\n\n<ol start=\"3\">\n<li>In your code:<\/li>\n<\/ol>\n\n<p><code>ctx.job.tags.append('new-tag')<\/code><\/p>\n\n<ol start=\"4\">\n<li>In the Web UI. In the experiment dashboard you have to click on \"Job Properties\" in the top left corner of the screen. Side panel will appear where you can modify job properties.<\/li>\n<\/ol>\n\n<p>So you can change tags of your experiment in every phase of your experiment execution.<\/p>\n\n<p>Sources: <\/p>\n\n<ul>\n<li><p><a href=\"http:\/\/neptune.deepsense.io\/versions\/latest\/reference-guides\/cli.html#tags\" rel=\"nofollow noreferrer\">http:\/\/neptune.deepsense.io\/versions\/latest\/reference-guides\/cli.html#tags<\/a><\/p><\/li>\n<li><p><a href=\"http:\/\/neptune.deepsense.io\/versions\/latest\/reference-guides\/job-and-experiment.html#tags\" rel=\"nofollow noreferrer\">http:\/\/neptune.deepsense.io\/versions\/latest\/reference-guides\/job-and-experiment.html#tags<\/a> <\/p><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1495176106327,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44053028",
        "Question_exclusive_tag":"Neptune"
    },
    {
        "Question_id":70322291,
        "Question_title":"Error notification not working in background job Neptune Software",
        "Question_body":"<p>For a particular server script, we are adding a background job.\nIn that ,it has an option to add error notification emails ,which is not working.  There is error in my script, which I can see in job log but not getting any notifications on email.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1639300561957,
        "Question_score":0,
        "Question_tags":"background|amazon-neptune|aws-neptune|neptune",
        "Question_view_count":36,
        "Owner_creation_time":1583493572323,
        "Owner_last_access_time":1660792659480,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1649766671956,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70322291",
        "Question_exclusive_tag":"Neptune"
    },
    {
        "Question_id":72606870,
        "Question_title":"Pass NEPTUNE_API_TOKEN environment variable via docker run command",
        "Question_body":"<p>Using the <code>docker run<\/code> command, I'm trying to pass my NEPTUNE_API_TOKEN to my container.<\/p>\n<p>My understanding is that I should use the <code>-e<\/code> flag as follows: <code>-e ENV_VAR='env_var_value'<\/code> and that might work.\nI wish, however, to use the value existing in the already-running session, as follows:<\/p>\n<pre><code>docker run -e NEPTUNE_API_TOKEN=$(NEPTUNE_API_TOKEN) &lt;my_image&gt;\n<\/code><\/pre>\n<p>However, after doing so, NEPTUNE_API_TOKEN is set to empty when checking the value inside the container.\nMy question is whether I'm doing something wrong or if this is not possible and I must provide an explicit Neptune API token as a string.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1655141431553,
        "Question_score":0,
        "Question_tags":"docker|neptune",
        "Question_view_count":42,
        "Owner_creation_time":1538410701937,
        "Owner_last_access_time":1663830248157,
        "Owner_location":"Israel",
        "Owner_reputation":735,
        "Owner_up_votes":211,
        "Owner_down_votes":5,
        "Owner_views":80,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72606870",
        "Question_exclusive_tag":"Neptune"
    },
    {
        "Question_id":56881619,
        "Question_title":"What are the pros and cons of using DVC and Pachyderm?",
        "Question_body":"<p>What are the pros and cons of using either of these?<\/p>\n\n<p><a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc<\/a><\/p>\n\n<p><a href=\"https:\/\/github.com\/pachyderm\/pachyderm\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pachyderm\/pachyderm<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1562220779043,
        "Question_score":1,
        "Question_tags":"machine-learning|version-control|data-science|dvc|pachyderm",
        "Question_view_count":1635,
        "Owner_creation_time":1562220403123,
        "Owner_last_access_time":1563409319090,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56881619",
        "Question_exclusive_tag":"Pachyderm"
    },
    {
        "Question_id":56308560,
        "Question_title":"Nodejs example for pachyderm",
        "Question_body":"<p>I am new to Pachyderm.<\/p>\n\n<p>I have a pipeline to extract, transform and then save in the db.\nEverything is already written in nodejs, docekrized.\nNow, I would like to move and use pachyderm.<\/p>\n\n<p>I tried following the python examples they provided, but creating this new pipeline always fails and the job never starts.<\/p>\n\n<p>All my code does is take the <code>\/pfs\/data<\/code> and copy it to <code>\/pfs\/out<\/code>. <\/p>\n\n<p>Here is my pipeline definition<\/p>\n\n<pre><code>{\n    \"pipeline\": {\n        \"name\": \"copy\"\n    },\n    \"transform\": {\n        \"cmd\": [\"npm\", \"start\"],\n        \"image\": \"simple-node-docker\"\n    },\n    \"input\": {\n        \"pfs\": {\n            \"repo\": \"data\",\n            \"glob\": \"\/*\"\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>All that happens is that the pipeline fails and the job never starts.<\/p>\n\n<p>Is there a way to debug on why the pipeline is failing?\nIs there something special about my docker image that needs to happen?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1558817113660,
        "Question_score":0,
        "Question_tags":"node.js|pachyderm",
        "Question_view_count":80,
        "Owner_creation_time":1401628725057,
        "Owner_last_access_time":1664053631220,
        "Owner_location":null,
        "Owner_reputation":71,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56308560",
        "Question_exclusive_tag":"Pachyderm"
    },
    {
        "Question_id":53987937,
        "Question_title":"Pachyderm pipeline does not start a job and launches an empty repo",
        "Question_body":"<p>I have a JSON configuration for my pipeline in Pachyderm:<\/p>\n\n<pre><code>{\n    \"pipeline\": {\n        \"name\": \"mopng-beneficiary-v2\"\n    },\n    \"input\": {\n        \"pfs\": {\n            \"repo\": \"mopng_beneficiary_v2\",\n            \"glob\": \"\/*\"\n        }\n    },\n    \"transform\": {\n        \"cmd\": [\"python3\", \"\/pclean_phlc9h6grzqdhm6sc0zrxjne_UdOgg.py \/pfs\/mopng_beneficiary_v2\/euoEQHIwIQTe1wXtg46fFYok.csv \/pfs\/mopng_beneficiary_v2\/\/Users\/aviralsrivastava\/Downloads\/5Feb18_master_ujjwala_latlong_dist_dno_so_v7.csv \/pfs\/mopng_beneficiary_v2\/\/Users\/aviralsrivastava\/Downloads\/ppac_master_v3_mmi_enriched_with_sanity_check.csv \/pfs\/mopng_beneficiary_v2\/Qc.csv\"],\n        \"image\": \"mopng-beneficiary-v2-image\"\n    }\n}\n<\/code><\/pre>\n\n<p>And my docker file is as follows:<\/p>\n\n<pre><code>FROM ubuntu:14.04\n\n# Install opencv and matplotlib.\nRUN apt-get update \\\n    &amp;&amp; apt-get upgrade -y \\\n    &amp;&amp; apt-get install -y unzip wget build-essential \\\n        cmake git pkg-config libswscale-dev \\\n        python3-dev python3-numpy python3-tk \\\n        libtbb2 libtbb-dev libjpeg-dev \\\n        libpng-dev libtiff-dev libjasper-dev \\\n        bpython python3-pip libfreetype6-dev \\\n    &amp;&amp; apt-get clean \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\n\nRUN sudo pip3 install matplotlib\nRUN sudo pip3 install pandas\n\n# Add our own code.\nADD pclean.py \/pclean.py\n<\/code><\/pre>\n\n<p>However, when I run my command to create the pipeline:<\/p>\n\n<pre><code>pachctl create-pipeline -f https:\/\/raw.githubusercontent.com\/avisrivastava254084\/learning-pachyderm\/master\/pipeline.json\n<\/code><\/pre>\n\n<p>The files are existing in the pfs:<\/p>\n\n<pre><code>pachctl put-file mopng_beneficiary_v2 master -f \/Users\/aviralsrivastava\/Downloads\/pclean_phlc9h6grzqdhm6sc0zrxjne_UdOgg.py\n\u279c  ~ pachctl put-file mopng_beneficiary_v2 master -f \/Users\/aviralsrivastava\/Downloads\/5Feb18_master_ujjwala_latlong_dist_dno_so_v7.csv\n\u279c  ~ pachctl put-file mopng_beneficiary_v2 master -f \/Users\/aviralsrivastava\/Downloads\/ppac_master_v3_mmi_enriched_with_sanity_check.csv\n\u279c  ~ pachctl put-file mopng_beneficiary_v2 master -f \/Users\/aviralsrivastava\/Downloads\/euoEQHIwIQTe1wXtg46fFYok.csv\n<\/code><\/pre>\n\n<p>It should be worth to note that I am getting this from the logs command(<code>pachctl get-logs --pipeline=mopng-beneficiary-v2<\/code>):<\/p>\n\n<pre><code>container \"user\" in pod \"pipeline-mopng-beneficiary-v2-v1-lnbjh\" is waiting to start: trying and failing to pull image\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1546262146783,
        "Question_score":0,
        "Question_tags":"json|docker|pachyderm",
        "Question_view_count":232,
        "Owner_creation_time":1531235190283,
        "Owner_last_access_time":1550050689970,
        "Owner_location":"delhi",
        "Owner_reputation":903,
        "Owner_up_votes":53,
        "Owner_down_votes":0,
        "Owner_views":144,
        "Question_last_edit_time":1563207497823,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53987937",
        "Question_exclusive_tag":"Pachyderm"
    },
    {
        "Question_id":53991961,
        "Question_title":"File not found even after adding the file inside docker",
        "Question_body":"<p>I have written a docker file which adds my python script inside the container:\n<code>ADD test_pclean.py \/test_pclean.py<\/code><\/p>\n\n<p>My directory structure is:<\/p>\n\n<pre><code>.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pipeline.json\n\u2514\u2500\u2500 test_pclean.py\n<\/code><\/pre>\n\n<p>My json file which acts as a configuration file for creating a pipeline in Pachyderm is as follows:<\/p>\n\n<pre><code>{\n    \"pipeline\": {\n        \"name\": \"mopng-beneficiary-v2\"\n    },\n    \"transform\": {\n        \"cmd\": [\"python3\", \"\/test_pclean.py\"],\n        \"image\": \"avisrivastava254084\/mopng-beneficiary-v2-image-7\"\n    },\n    \"input\": {\n        \"atom\": {\n            \"repo\": \"mopng_beneficiary_v2\",\n            \"glob\": \"\/*\"\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>Even though I have copied the official documentation's example, I am facing an error:\n<code>python3: can't open file '\/test_pclean.py': [Errno 2] No such file or directory<\/code><\/p>\n\n<p>My dockerfile is:<\/p>\n\n<pre><code>FROM    debian:stretch\n\n# Install opencv and matplotlib.\nRUN apt-get update \\\n    &amp;&amp; apt-get upgrade -y \\\n    &amp;&amp; apt-get install -y unzip wget build-essential \\\n        cmake git pkg-config libswscale-dev \\\n    &amp;&amp; apt-get clean \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\n\nRUN apt update\nRUN apt-get -y install python3-pip\nRUN pip3 install matplotlib\nRUN pip3 install pandas\n\nADD test_pclean.py \/test_pclean.py\nENTRYPOINT [ \"\/bin\/bash\/\" ]\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":6,
        "Question_creation_time":1546296533713,
        "Question_score":0,
        "Question_tags":"python|python-3.x|docker|pachyderm",
        "Question_view_count":430,
        "Owner_creation_time":1531235190283,
        "Owner_last_access_time":1550050689970,
        "Owner_location":"delhi",
        "Owner_reputation":903,
        "Owner_up_votes":53,
        "Owner_down_votes":0,
        "Owner_views":144,
        "Question_last_edit_time":1546423579990,
        "Answer_body":"<p>I was not changing the commits to my docker images on each build and hence, Kubernetes was using the local docker file that it had(w\/o tags and commits, it doesn't acknowledge any change). Once I started using commit with each build, Kubernetes started downloading the intended docker image.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1547720458432,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53991961",
        "Question_exclusive_tag":"Pachyderm"
    },
    {
        "Question_id":67937857,
        "Question_title":"Pachyderm deploy GCP - no such image",
        "Question_body":"<p>I'm deploying Pachyderm on GKE but when I deploy the pipeline (following the <a href=\"https:\/\/docs.pachyderm.com\/latest\/getting_started\/beginner_tutorial\/\" rel=\"nofollow noreferrer\">https:\/\/docs.pachyderm.com\/latest\/getting_started\/beginner_tutorial\/<\/a>) the Pod fails in ImagePullCrashLoopBack giving this error &quot;no such image&quot;.<\/p>\n<p>Here, the output of the command &quot;kubectl get pods&quot;:\n<a href=\"https:\/\/i.stack.imgur.com\/lpvj7.png\" rel=\"nofollow noreferrer\">screenshot<\/a><\/p>\n<p>How can I fix the deployment procedure?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1623417969943,
        "Question_score":0,
        "Question_tags":"kubernetes|google-cloud-platform|pachyderm",
        "Question_view_count":48,
        "Owner_creation_time":1415266912943,
        "Owner_last_access_time":1664046818297,
        "Owner_location":"Milano, Metropolitan City of Milan, Italy",
        "Owner_reputation":107,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As mentioned in the Slack channel of Pachyderm community, adding the flag <code>--no-expose-docker-socket<\/code> to the deploy call should solve the issue.<\/p>\n<p><code>pachctl deploy google ${BUCKET_NAME} ${STORAGE_SIZE} --dynamic-etcd-nodes=1 --no-expose-docker-socket<\/code><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1623418140676,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67937857",
        "Question_exclusive_tag":"Pachyderm"
    },
    {
        "Question_id":44269634,
        "Question_title":"How do you link Pachyderm with the correct Kubernetes context?",
        "Question_body":"<p>I have more than one Kubernetes context. When I change contexts, I have been using <code>kill -9<\/code>  to kill the port-forward in order to redo the <code>pachtctl port-forward &amp;<\/code> command. I wonder if this is the right way of doing it.<\/p>\n\n<p>In more detail:<\/p>\n\n<p>I start off being in a Kubernetes context, we'll call it context_x. I then want to change context to my local context, called minikube. I also want to see my repos for this minikube context, but when I use <code>pachctl list-repo<\/code>, it still shows context_x's Pachyderm repos. When I do <code>pachctl port-forward<\/code>, I then get an error message about the address being already in use. So I have to ps -a, then kill -9 on those port forward processes, and then do pachctl port-forward command again.<\/p>\n\n<p>An example of what I've been doing:<\/p>\n\n<pre><code>$ kubectl config use-context minikube\n$ pachctl list-repo #doesn't show minikube context's repos\n$ pachctl port-forward &amp;\n...several error messages along the lines of:\nUnable to create listener: Error listen tcp4 127.0.0.1:30650: bind: address already in use\n$ ps -a | grep forward\n33964 ttys002    0:00.51 kubectl port-forward dash-12345678-abcde 38080:8080\n33965 ttys002    0:00.51 kubectl port-forward dash-12345679-abcde 38081:8081\n37245 ttys002    0:00.12 pachctl port-forward &amp;\n37260 ttys002    0:00.20 kubectl port-forward pachd-4212312322-abcde 30650:650\n$ kill -9 37260\n$ pachctl port-forward &amp; #works as expected now\n<\/code><\/pre>\n\n<p>Also, kill -9 on the <code>pachctl port-forward<\/code> process 37245 doesn't work, it seems like I have to kill -9 on the <code>kubectl port-forward<\/code><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1496169756220,
        "Question_score":4,
        "Question_tags":"kubernetes|portforwarding|pachyderm",
        "Question_view_count":438,
        "Owner_creation_time":1327230207597,
        "Owner_last_access_time":1663187730303,
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":3322,
        "Owner_up_votes":319,
        "Owner_down_votes":5,
        "Owner_views":160,
        "Question_last_edit_time":1525391431087,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44269634",
        "Question_exclusive_tag":"Pachyderm"
    },
    {
        "Question_id":53989099,
        "Question_title":"Pachyderm pipeline unable to pull docker image",
        "Question_body":"<p>I have successfully pushed my docker image to the registry:\n<code>my-username\/image-name<\/code><\/p>\n\n<p>However, pachyderm is still unable to pull docker image(from logs):<\/p>\n\n<pre><code>container \"user\" in pod \"pipeline-mopng-beneficiary-v2-v1-b6kln\" is waiting to start: trying and failing to pull image\n<\/code><\/pre>\n\n<p>Where should I specify the image so that pachyderm is able to pull one?\nThis is my config file:<\/p>\n\n<pre><code>{\n    \"pipeline\": {\n        \"name\": \"mopng-beneficiary-v2\"\n    },\n    \"input\": {\n        \"pfs\": {\n            \"repo\": \"mopng_beneficiary_v2\",\n            \"glob\": \"\/*\"\n        }\n    },\n    \"transform\": {\n        \"cmd\": [\"python3\", \"\/pclean.py \/pfs\/mopng_beneficiary_v2\/euoEQHIwIQTe1wXtg46fFYok.csv \/pfs\/mopng_beneficiary_v2\/\/Users\/aviralsrivastava\/Downloads\/5Feb18_master_ujjwala_latlong_dist_dno_so_v7.csv \/pfs\/mopng_beneficiary_v2\/\/Users\/aviralsrivastava\/Downloads\/ppac_master_v3_mmi_enriched_with_sanity_check.csv \/pfs\/mopng_beneficiary_v2\/Qc.csv\"],\n        \"image\": \"username\/my-image\"\n    }\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1546270692627,
        "Question_score":1,
        "Question_tags":"docker|pachyderm",
        "Question_view_count":158,
        "Owner_creation_time":1531235190283,
        "Owner_last_access_time":1550050689970,
        "Owner_location":"delhi",
        "Owner_reputation":903,
        "Owner_up_votes":53,
        "Owner_down_votes":0,
        "Owner_views":144,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53989099",
        "Question_exclusive_tag":"Pachyderm"
    },
    {
        "Question_id":71430286,
        "Question_title":"Permission Denied using Google AiPlatform ModelServiceClient",
        "Question_body":"<p>I am following a guide to get a Vertex AI pipeline working:<\/p>\n<p><a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5\" rel=\"nofollow noreferrer\">https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5<\/a><\/p>\n<p>I have implemented the following custom component:<\/p>\n<pre><code>from google.cloud import aiplatform as aip\nfrom google.oauth2 import service_account\n\nproject = &quot;project-id&quot;\nregion = &quot;us-central1&quot;\ndisplay_name = &quot;lookalike_model_pipeline_1646929843&quot;\n\nmodel_name = f&quot;projects\/{project}\/locations\/{region}\/models\/{display_name}&quot;\napi_endpoint = &quot;us-central1-aiplatform.googleapis.com&quot; #europe-west2\nmodel_resource_path = model_name\nclient_options = {&quot;api_endpoint&quot;: api_endpoint}\n\n# Initialize client that will be used to create and send requests.\nclient = aip.gapic.ModelServiceClient(credentials=service_account.Credentials.from_service_account_file('..\\\\service_accounts\\\\aiplatform_sa.json'), \nclient_options=client_options)\n#get model evaluation\nresponse = client.list_model_evaluations(parent=model_name)\n<\/code><\/pre>\n<p>And I get following error:<\/p>\n<pre><code>(&lt;class 'google.api_core.exceptions.PermissionDenied'&gt;, PermissionDenied(&quot;Permission 'aiplatform.modelEvaluations.list' denied on resource '\/\/aiplatform.googleapis.com\/projects\/project-id\/locations\/us-central1\/models\/lookalike_model_pipeline_1646929843' (or it may not exist).&quot;), &lt;traceback object at 0x000002414D06B9C0&gt;)\n<\/code><\/pre>\n<p>The model definitely exists and has finished training. I have given myself admin rights in the aiplatform service account. In the guide, they do not use a service account, but uses only client_options instead. The client_option has the wrong type since it is a dict(str, str) when it should be: Optional['ClientOptions']. But this doesn't cause an error.<\/p>\n<p>My main question is: how do I get around this permission issue?<\/p>\n<p>My subquestions are:<\/p>\n<ol>\n<li>How can I use my model_name variable in a URL to get to the model?<\/li>\n<li>How can I create an Optional['ClientOptions'] object to pass as client_option<\/li>\n<li>Is there another way I can list_model_evaluations from a model that is in VertexAI, trained using automl?<\/li>\n<\/ol>\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":7,
        "Question_creation_time":1646943487337,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|model|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":936,
        "Owner_creation_time":1562706291280,
        "Owner_last_access_time":1664033431527,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1646987974512,
        "Answer_body":"<p>I tried using your code and it did not also work for me and got a different error. As @DazWilkin mentioned it is recommended to use the Cloud Client.<\/p>\n<p>I used <code>aiplatform_v1<\/code> and it worked fine. One thing I noticed is that you should always define a value for <code>client_options<\/code> so it will point to the correct endpoint. Checking the code for <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/google\/cloud\/aiplatform_v1\/services\/model_service\/client.py#L122\" rel=\"nofollow noreferrer\">ModelServiceClient<\/a>, if I'm not mistaken the endpoint defaults to <strong>&quot;aiplatform.googleapis.com&quot;<\/strong> which don't have a location prepended. AFAIK the endpoint should prepend a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/locations\" rel=\"nofollow noreferrer\">location<\/a>.<\/p>\n<p>See code below. I used AutoML models and it returns their model evaluations.<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\nfrom typing import Optional\n\ndef get_model_eval(\n        project_id: str,\n        model_id: str,\n        client_options: dict,\n        location: str = 'us-central1',\n        ):\n\n    client_model = aiplatform.services.model_service.ModelServiceClient(client_options=client_options)\n\n    model_name = f'projects\/{project_id}\/locations\/{location}\/models\/{model_id}'\n    list_eval_request = aiplatform.types.ListModelEvaluationsRequest(parent=model_name)\n    list_eval = client_model.list_model_evaluations(request=list_eval_request)\n    print(list_eval)\n\n\n\napi_endpoint = 'us-central1-aiplatform.googleapis.com'\nclient_options = {&quot;api_endpoint&quot;: api_endpoint} # api_endpoint is required for client_options\nproject_id = 'project-id'\nlocation = 'us-central1'\nmodel_id = '99999999999' # aiplatform_v1 uses the model_id\n\nget_model_eval(\n        client_options = client_options,\n        project_id = project_id,\n        location = location,\n        model_id = model_id,\n        )\n<\/code><\/pre>\n<p>This is an output snippet from my AutoML Text Classification:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/RXrxh.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RXrxh.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1646971559036,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1646998024380,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71430286",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73461369,
        "Question_title":"How should Pubsub, acting a log sink, fire a function without sending the log?",
        "Question_body":"<p>I have been using <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/08-model-monitoring.ipynb\" rel=\"nofollow noreferrer\">this<\/a> example of creating a Vertex AI monitoring job. It sends an email and have adapted it to send a Pubsub message, with @Jose Gutierrez Paliza's help.<\/p>\n<p>I have got this working, sort of. But what seems to be happening is that Pubsub pushes the log to  a function which errors.<\/p>\n<p>My log sink includes:\n<a href=\"https:\/\/i.stack.imgur.com\/00ksv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/00ksv.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>When I look at logs I see an INFO entry:<\/p>\n<pre><code>   my-fn an_id Event data: {&quot;insertId&quot;:&quot;another_id...\n<\/code><\/pre>\n<p>followed by a separate ERROR entry:<\/p>\n<pre><code>...\nValueError: The pipeline parameter insertId is not found in the pipeline job input definitions.\n<\/code><\/pre>\n<p>So I assume Pubsub is sending the log to the function which gets extraneous crap, including <code>insertId<\/code>.<\/p>\n<p>I can run the pipeline fine via Jupyter:<\/p>\n<pre><code>from google.cloud import pubsub\n\npublish_client = pubsub.PublisherClient()\ntopic = f'projects\/{PROJECT}\/topics\/{PUBSUB_TOPIC}'\ndata = {}\nmessage = json.dumps(data)\n\n_ = publish_client.publish(topic, message.encode())\n<\/code><\/pre>\n<p>So how do I the equivalent via Pubsub?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1661268443880,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-functions|google-cloud-pubsub|google-cloud-monitoring|google-cloud-vertex-ai",
        "Question_view_count":60,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73461369",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69878915,
        "Question_title":"Deploying multiple models to same endpoint in Vertex AI",
        "Question_body":"<p>Our use case is as follows:\nWe have multiple custom trained models (in the hundreds, and the number increases as we allow the user of our application to create models through the UI, which we then train and deploy on the fly) and so deploying each model to a separate endpoint is expensive as Vertex AI <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/deploy-model-console#custom-trained\" rel=\"nofollow noreferrer\">charges per node used<\/a>. Based on the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/deployment#models-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> it seems that we can deploy models of different types to the same endpoint but I am not sure how that would work. Let's say I have 2 different custom trained models deployed using custom containers for prediction to the same endpoint. Also, say I specify the traffic split to be 50% for the two models. Now, how do I send a request to a specific model? Using the python SDK, we make calls to the endpoint, like so:<\/p>\n<pre><code>from google.cloud import aiplatform\nendpoint = aiplatform.Endpoint(endpoint_id)\nprediction = endpoint.predict(instances=instances)\n\n# where endpoint_id is the id of the endpoint and instances are the observations for which a prediction is required\n<\/code><\/pre>\n<p>My understanding is that in this scenario, vertex AI will route some calls to one model and some to the other based on the traffic split. I could use the parameters field, as specified in the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#prediction\" rel=\"nofollow noreferrer\">docs<\/a>, to specify the model and then process the request accordingly in the custom prediction container, but still some calls will end up going to a model which it will not be able to process (because Vertex AI is not going to be sending all requests to all models, otherwise the traffic split wouldn't make sense). How do I then deploy multiple models to the same endpoint and make sure that every prediction request is guaranteed to be served?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_time":1636348555970,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":1271,
        "Owner_creation_time":1471292986790,
        "Owner_last_access_time":1664058170413,
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/deployment#models-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> talks about a use case where 2 models are trained on the same feature set and are sharing the ingress prediction traffic. As you have understood correctly, this does not apply to models that have been trained on different feature sets, that is, different models.<\/p>\n<p>Unfortunately, deploying different models to the same endpoint utilizing only one node is not possible in Vertex AI at the moment. There is an ongoing feature request that is being worked on. However, we cannot provide an exact ETA on when that feature will be available.<\/p>\n<p>I reproduced the multi-model setup and noticed the below points.<\/p>\n<p><strong>Traffic Splitting<\/strong><\/p>\n<blockquote>\n<p>I deployed 2 different models to the same endpoint and sent predictions to it. I set a 50-50 traffic splitting rule and saw errors that implied requests being sent to the wrong model.<\/p>\n<\/blockquote>\n<p><strong>Cost Optimization<\/strong><\/p>\n<blockquote>\n<p>When multiple models are deployed to the same endpoint, they are deployed to separate, independent nodes. So, you will still be charged for each node used. Also, node autoscaling happens at the model level, not at the endpoint level.<\/p>\n<\/blockquote>\n<p>A plausible workaround would be to pack all your models into a single container and use a custom HTTP server logic to send prediction requests to the appropriate model. This could be achieved using the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#request_requirements\" rel=\"nofollow noreferrer\"><code>parameters<\/code><\/a> field of the prediction request body. The custom logic would look something like this.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@app.post(os.environ['AIP_PREDICT_ROUTE'])\nasync def predict(request: Request):\n    body = await request.json()\n    parameters = body[&quot;parameters&quot;]\n    instances = body[&quot;instances&quot;]\n    inputs = np.asarray(instances)\n    preprocessed_inputs = _preprocessor.preprocess(inputs)\n\n    if(parameters[&quot;model_name&quot;]==&quot;random_forest&quot;):\n        print(parameters[&quot;model_name&quot;])\n        outputs = _random_forest_model.predict(preprocessed_inputs)\n    else:\n        print(parameters[&quot;model_name&quot;])\n        outputs = _decision_tree_model.predict(inputs)\n\n    return {&quot;predictions&quot;: [_class_names[class_num] for class_num in outputs]}\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1636404890009,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1636405249843,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69878915",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68456262,
        "Question_title":"GCP - Vertex AI Model - Access GCS failed",
        "Question_body":"<p>We have a Vertex AI model that was created using a custom image.\nWe are trying to access a bucket on startup but we are getting the following error:<\/p>\n<pre><code>google.api_core.exceptions.Forbidden: 403 GET https:\/\/storage.googleapis.com\/storage\/v1\/b\/...?projection=noAcl&amp;prettyPrint=false: {service account name} does not have storage.buckets.get access to the Google Cloud Storage bucket.\n<\/code><\/pre>\n<p>The problem is that I can't find the service account that is mentioned in the error to give it the right access permissions..<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626789857557,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":291,
        "Owner_creation_time":1466977784157,
        "Owner_last_access_time":1664005111393,
        "Owner_location":null,
        "Owner_reputation":117,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68456262",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73027674,
        "Question_title":"How does one move python and other types of files from one GCP notebook instance to another?",
        "Question_body":"<p>I have a Vertex AI notebook that contains a lot of python and jupyter notebook as well as pickled data files in it.  I need to move these files to another notebook.  There isn't a lot of documentation on google's help center.<\/p>\n<p>Has someone had to do this yet?  I'm new to GCP.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1658172774733,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":114,
        "Owner_creation_time":1455270976143,
        "Owner_last_access_time":1663869676733,
        "Owner_location":"Grand Rapids, MI, USA",
        "Owner_reputation":1269,
        "Owner_up_votes":134,
        "Owner_down_votes":0,
        "Owner_views":261,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Can you try these steps in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/migrate\" rel=\"nofollow noreferrer\">article<\/a>. It says you can copy your files to a <a href=\"https:\/\/cloud.google.com\/storage\/\" rel=\"nofollow noreferrer\">Google Cloud Storage Bucket<\/a> then move it to a new notebook by using gsutil tool.<\/p>\n<p>In your notebook's terminal run this code to copy an object to your Google Cloud storage bucket:<\/p>\n<pre><code>gsutil cp -R \/home\/jupyter\/* gs:\/\/BUCKET_NAMEPATH\n<\/code><\/pre>\n<p>Then open a new terminal to the target notebook and run this command to copy the directory to the notebook:<\/p>\n<pre><code>gsutil cp gs:\/\/BUCKET_NAMEPATH* \/home\/jupyter\/\n<\/code><\/pre>\n<p>Just change the <code>BUCKET_NAMEPATH<\/code> to the name of your cloud storage bucket.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1658194851972,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73027674",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68569313,
        "Question_title":"How can I run Google Cloud's \"AI Notebooks\" on a schedule automatically?",
        "Question_body":"<p>Notebooks in the Google Cloud Platform has been great for Python development in the cloud, but the last missing piece is just running existing notebooks on a schedule. There's a million different tools (Airflow, Papermill, Google Cloud Jobs, Google Cloud Scheduler, Google Cloud Cron Jobs), and as someone not as familiar with Cloud, it's really easy to get lost. Any suggestion? Thanks guys!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1627525524810,
        "Question_score":3,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":1317,
        "Owner_creation_time":1523235961717,
        "Owner_last_access_time":1642750774450,
        "Owner_location":null,
        "Owner_reputation":143,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":36,
        "Question_last_edit_time":1654245561700,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68569313",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69703127,
        "Question_title":"How to invoke custom prediction logic in Vertex AI?",
        "Question_body":"<p>Goal: serve prediction requests from a Vertex AI Endpoint by executing custom prediction logic.<\/p>\n<p>Detailed steps:\nFor example, we may already uploaded have an image_quality.pb model (developed in a non-vertex-ai pythonic environment) in a GCS bucket<\/p>\n<p>Next, we want to create a custom image inference logic by deserializing the deployed  model and serving the inference functionality in a vertex AI endpoint<\/p>\n<p>Finally, we want to pass a list of images (stored in another GCS bucket) to that endpoint.<\/p>\n<p>We also want to see the logs and metrics in tensorboard.<\/p>\n<p>Existing Vertex AI code samples provide examples for invoking model.batch_predict \/ endpoint. predict, but don't mention how to execute custom prediction code.<\/p>\n<p>It would be great if someone can provide guidelines and links to documents\/code in order to implement the above steps.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1635138837680,
        "Question_score":1,
        "Question_tags":"google-ai-platform|google-cloud-vertex-ai|google-cloud-ai-platform-pipelines",
        "Question_view_count":133,
        "Owner_creation_time":1423356129317,
        "Owner_last_access_time":1638159935947,
        "Owner_location":null,
        "Owner_reputation":191,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":37,
        "Question_last_edit_time":1635227801976,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69703127",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68370906,
        "Question_title":"Google Cloud Platform Dataset can't show image in VertexAI",
        "Question_body":"<p>Thanks All:\nWhen I use GCP-VertexAI and upload image from my computer. Next step to Browse page, all images always are loading. I can't any picture in Browse, and labing. But we can find upload image in Cloud Storage. How should I do for check my images in Browse?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/w4gFz.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626225376420,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":167,
        "Owner_creation_time":1626224549820,
        "Owner_last_access_time":1637572769890,
        "Owner_location":"Taipei, \u53f0\u7063",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68370906",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73251212,
        "Question_title":"How do I retrieve a model in Vertex AI?",
        "Question_body":"<p>I defined a training job:<\/p>\n<pre><code>job = aiplatform.AutoMLTextTrainingJob(...\n<\/code><\/pre>\n<p>then I created a model by running the job:<\/p>\n<pre><code>model = job.run(...\n<\/code><\/pre>\n<p>It worked fine but it is now the next day and the variable <code>model<\/code> was in a Jupyter notebook and no longer exists. I have tried to get it back with:<\/p>\n<pre><code>from google.cloud import aiplatform_v1beta1\n\ndef sample_get_model():\n    client = aiplatform_v1beta1.ModelServiceClient()\n\n    model_id=id_of_training_pipeline\n    name= f'projects\/{PROJECT}\/locations\/{REGION}\/models\/{model_id}'\n    \n    request = aiplatform_v1beta1.GetModelRequest(name=name)\n    response = client.get_model(request=request)\n    print(response)\n\nsample_get_model()\n<\/code><\/pre>\n<p>I have also tried the id of v1 of the model created in place of <code>id_of_training_pipeline<\/code> and I have tried <code>\/pipelines\/pipeline_id<\/code><\/p>\n<p>but I get:\n<code>E0805 15:12:36.784008212   28406 hpack_parser.cc:1234]       Error parsing metadata: error=invalid value key=content-type value=text\/html; charset=UTF-8<\/code><\/p>\n<p>(<code>PROJECT<\/code> and <code>REGION<\/code> are set correctly).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659709186783,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":45,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/samples\/aiplatform-get-model-sample#aiplatform_get_model_sample-python\" rel=\"nofollow noreferrer\">this<\/a> Google code which works.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1659712728223,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73251212",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73817659,
        "Question_title":"How to login as human labeler on GCP Vertex AI",
        "Question_body":"<p>I set up a Labeling Task in Vertex-AI, and assigned a team.\nThe manager of that team received an email to manage the <a href=\"https:\/\/datacompute.google.com\/\" rel=\"nofollow noreferrer\">https:\/\/datacompute.google.com\/<\/a> console.\nNone of the human labelers received such an email.\nWhat do they have to do to start labeling? Is there a console for them?<\/p>\n<p>Any advice would be amazing!\nThanks!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663862524327,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":8,
        "Owner_creation_time":1663862345683,
        "Owner_last_access_time":1663864771053,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73817659",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70137519,
        "Question_title":"GCP cannot create Managed Notebook on Vertex AI",
        "Question_body":"<p>Using a GCP account that started as free, but does have billing enabled, I can't create a managed notebook and get the following popup error:<\/p>\n<p>Quota exceeded for quota metric 'Create Runtime API requests' and limit 'Create Runtime API requests per minute' of service 'notebooks.googleapis.com' for consumer 'project_number:....'<\/p>\n<p>Navigating to Quotas --&gt; Notebook API --&gt; Create Runtime API requests per minute<\/p>\n<p>Edit Quota: Create Runtime API requests per minute\nCurrent limit: 0\nEnter a new quota limit between 0 and 0.<\/p>\n<p>0 doesn't work..<\/p>\n<p>Is there something that I can do, or should have done already to increase this quota?<\/p>\n<p>TIA for any help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638037173043,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":285,
        "Owner_creation_time":1285776739110,
        "Owner_last_access_time":1663774650203,
        "Owner_location":"Chicago, IL",
        "Owner_reputation":8508,
        "Owner_up_votes":347,
        "Owner_down_votes":4,
        "Owner_views":144,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Managed notebooks is still pre-GA and is currently unavailable to the projects with insufficient service usage history.<\/p>\n<p>You can wait for the GA of the service or use a project with more service usage.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1638931046247,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70137519",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70754016,
        "Question_title":"Creating a Vertex AI Workbench with a Non Organization Account and problems with constraints\/compute.vmExternalIpAccess",
        "Question_body":"<p>I'm trying to create a Vertex AI Workbench on GCP, but every time I try I get the following error:<\/p>\n<p><em>&lt;Workbench Name&gt; Constraint constraints\/compute.vmExternalIpAccess violated for project &lt;Project ID&gt;. Add instance &lt;Workbench ID&gt; to the constraint to use external IP with it.<\/em><\/p>\n<p>I went to the Organization Policies page to edit the constraint: <em>constraints\/compute.vmExternalIpAccess<\/em> and saw that is denied for all (which is odd because in the <a href=\"https:\/\/cloud.google.com\/resource-manager\/docs\/organization-policy\/org-policy-constraints\" rel=\"nofollow noreferrer\">constraints documentation<\/a> it says that is should be enabled for all by default). Now, the problem is that when I go to edit the constraint, it says that it requires this set of permissions:<\/p>\n<ul>\n<li><em>orgpolicy.policies.create<\/em><\/li>\n<li><em>orgpolicy.policies.delete<\/em><\/li>\n<li><em>orgpolicy.policies.update<\/em><\/li>\n<li><em>orgpolicy.policy.get<\/em><\/li>\n<\/ul>\n<p>which are all part of the role: <em>roles\/orgpolicy.policyAdmin<\/em> that can only be granted at an organization level, and well, I have a Non Organization Account.<\/p>\n<p>Am I missing something?<\/p>\n<p>Thank for your time!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1642501704547,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|workbench|google-cloud-vertex-ai",
        "Question_view_count":333,
        "Owner_creation_time":1353379904267,
        "Owner_last_access_time":1653505313253,
        "Owner_location":null,
        "Owner_reputation":215,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70754016",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70623713,
        "Question_title":"How can I pass parameters to a Vertex AI Platform Pipeline?",
        "Question_body":"<p>I have created a Vertex AI pipeline similar to <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/google_cloud_pipeline_components_automl_images.ipynb\" rel=\"nofollow noreferrer\">this.<\/a><\/p>\n<p>Now the pipeline has reference to a csv file. So if this csv file changes the pipeline needs to be recreated.<\/p>\n<p>Is there any way to pass a new csv as a parameter to the pipeline when it is re-run? That is without recreating the pipeline using the notebook?<\/p>\n<p>If not, is there a best practice way of auto updating the dataset, model and deployment?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641570150640,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":611,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Have a look to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/build-pipeline\" rel=\"nofollow noreferrer\">that documentation<\/a>.<\/p>\n<p>You can define your pipeline like that<\/p>\n<pre><code>...\n# Define the workflow of the pipeline.\n@kfp.dsl.pipeline(\n    name=&quot;automl-image-training-v2&quot;,\n    pipeline_root=pipeline_root_path)\ndef pipeline(project_id: str):\n...\n<\/code><\/pre>\n<p>(you have something very similar in your notebook sample)<\/p>\n<p>Then, when you invoke your pipeline, you can pass some parameter<\/p>\n<pre><code>import google.cloud.aiplatform as aip\n\njob = aip.PipelineJob(\n    display_name=&quot;automl-image-training-v2&quot;,\n    template_path=&quot;image_classif_pipeline.json&quot;,\n    pipeline_root=pipeline_root_path,\n    parameter_values={\n        'project_id': project_id\n    }\n)\n\njob.submit()\n<\/code><\/pre>\n<p>You can see the <code>project_id<\/code> a dict parameter in the parameter values, and in parameter of your pipeline function.<\/p>\n<p>Do the same for your CSV file name!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1641588471209,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70623713",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69566674,
        "Question_title":"Vertex AI endpoints timing out",
        "Question_body":"<p>I am using vertex-ai endpoints to serve a deep learning service.<\/p>\n<p>My service takes approximately 30s - 2 minutes to respond on CPU depending on the size of the input. I noticed that when the input size takes more than one minute to respond, the API fails, giving me this error:<\/p>\n<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=en&gt;\n  &lt;meta charset=utf-8&gt;\n  &lt;meta name=viewport content=&quot;initial-scale=1, minimum-scale=1, width=device-width&quot;&gt;\n  &lt;title&gt;Error 502 (Server Error)!!1&lt;\/title&gt;\n  &lt;style&gt;\n    *{margin:0;padding:0}html,code{font:15px\/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* &gt; body{background:url(\/\/www.google.com\/images\/errors\/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(\/\/www.google.com\/images\/branding\/googlelogo\/1x\/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(\/\/www.google.com\/images\/branding\/googlelogo\/2x\/googlelogo_color_150x54dp.png) no-repeat 0% 0%\/100% 100%;-moz-border-image:url(\/\/www.google.com\/images\/branding\/googlelogo\/2x\/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(\/\/www.google.com\/images\/branding\/googlelogo\/2x\/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}\n  &lt;\/style&gt;\n  &lt;a href=\/\/www.google.com\/&gt;&lt;span id=logo aria-label=Google&gt;&lt;\/span&gt;&lt;\/a&gt;\n  &lt;p&gt;&lt;b&gt;502.&lt;\/b&gt; &lt;ins&gt;That\u2019s an error.&lt;\/ins&gt;\n  &lt;p&gt;The server encountered a temporary error and could not complete your request.&lt;p&gt;Please try again in 30 seconds.  &lt;ins&gt;That\u2019s all we know.&lt;\/ins&gt;\n<\/code><\/pre>\n<p>When I retry, I keep getting the same error. Once I decrease the input size, the API starts working again. For these reasons, I believe this is a timeout issue.<\/p>\n<p>So my question is: how can I change the timeout value in vertex-ai endpoints? I read through all the documentation, and it doesn't seem to be mentioned anywhere.<\/p>\n<p>Thank you.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1634196565023,
        "Question_score":1,
        "Question_tags":"google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":473,
        "Owner_creation_time":1407150503730,
        "Owner_last_access_time":1663843156893,
        "Owner_location":null,
        "Owner_reputation":387,
        "Owner_up_votes":37,
        "Owner_down_votes":2,
        "Owner_views":41,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69566674",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70657497,
        "Question_title":"GCP's Vertex AI - Restarting notebook <NAME>: <ZONE> does not have enough resources available to fulfill the request",
        "Question_body":"<p>I'm having issues starting \/ creating new user-managed notebooks in Vertex AI &gt; Workbench, where I'll end up with this error and can't even open JupyterLabs:<\/p>\n<blockquote>\n<p>[Vertex AI] - Restarting notebook NAME: ZONE does not have enough\nresources available to fulfill the request. Retry later or try another zone in your configurations.<\/p>\n<\/blockquote>\n<p>I didn't have this problem last month when I was still under the free trial. I also noticed this error only come up when I try to install the GPU in the notebook.<\/p>\n<p>So far, I've done the following:<\/p>\n<ul>\n<li>Change payment method (got an email that asked me to update payment settings at the end of the trial last month)<\/li>\n<li>Create new notebook in different zones and cluster over the past week, all give the same error when I attach a GPU.<\/li>\n<li>In Quotas, changed 'GPU (all regions)' from 1 to 0, and then back to 1.<\/li>\n<li>In Quotas, changed 'VM instances (default)' from 24 to 48, and then back to 24.<\/li>\n<li>Created several new projects, didn't work either.<\/li>\n<\/ul>\n<p>I also created a snapshot of both the data and boot of the notebook, but don't know how to use it to recreate it in Vertex AI again.<\/p>\n<p>Have you faced a similar issue before, how did you fix it? Thanks<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1641840588547,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|gcp-ai-platform-notebook|google-cloud-vertex-ai",
        "Question_view_count":480,
        "Owner_creation_time":1624939042350,
        "Owner_last_access_time":1654691815403,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1641991292412,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70657497",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73279871,
        "Question_title":"Proper Format of Vertex AI AutoML Action Recognition Data Labels",
        "Question_body":"<p>I'm trying to build an action recognition model in Vertex AI AutoML. I've studied the documentation thoroughly, but so far my model is not able to make any decent predictions in the wild. I've made three attempts so far, and my most recent attempt had a precision-recall curve that could be described as 'respectable', but the predictions are really awful. I'll try to explain my process below as best as I can.<\/p>\n<h2>The Raw Data<\/h2>\n<p>I recorded the same action in 34 ~3 min videos, with the number of actions in each video varying between 30 and 100. The actions themselves take &lt; 1 second. I recorded the data from 4 cameras at multiple angles, and because I was moving around a lot, there was plenty of variance in each action performed. While each raw video contains only one action, there are a total of six classes of action we hope to identify.<\/p>\n<h2>First Model Attempt<\/h2>\n<p>According to the Vertex AI documentation, it's expecting time segments for the actions The annotation JSONL\/CSV documentation says as much, but somewhere else in the documentation it says it's expecting the maximal point at which the action is performed if you wish to label the videos inside the console. Anyways, I created a labeling job and my team and I labeled all the time segments for the actions in the videos. The precision-recall curve alluded to some kind of data leakage, and when we inspected the batch prediction results we discovered that it appeared that the model was training on the 0th frame of the time segment. We were careful not to include any frames that weren't part of the action, but due to the nature of the actions, they all essentially start and end at a 'neutral' spot. At seemingly random intervals in the video, multiple or all actions would be labeled, but ONLY in those spots.<\/p>\n<h2>Second Attempt<\/h2>\n<p>We took the annotation data that we had built with the labeling job and chopped up the original videos into a series of subclips. We had all the labels and the time segments, so we did this with a simple script. We did not remove any of the frames of the video, so the neutral position in the beginning and end frames were still present. The precision-recall curve again looked suspicious, but slightly better. Inference in the wild yielded the same results.<\/p>\n<h2>Third Attempt<\/h2>\n<p>After further reviewing the documentation, Vertex AI appeared to contradict itself in what it expects in the data labels:<\/p>\n<blockquote>\n<p>When the action starts appearing that you want to identify, slowly\nprogress through till you find the center or the most representative\nmoment of the action using &quot;Next frame&quot; option.<\/p>\n<\/blockquote>\n<p>To avoid spending a ton of time on another labeling task (takes us about three days), we labeled a subset of the original subclip dataset according to this information and trained a model to analyze the precision-recall curve. FINALLY something much more respectable. However, the inferences in the wild were still terrible, suffering from the same.<\/p>\n<p>My question is: <strong>do I need to annotate negative action sequences?<\/strong> In the object tracking or object detection documentation it says that adding a <code>None_of_the_above<\/code> label would help the model to identify that which it doesn't need to focus on. And again in the action recognition documentation it points out a limitation in the labeling console:<\/p>\n<blockquote>\n<p>Limitation: There's a limitation when using the VAR labeling console,\nwhich means if you want to use the labeling tool to label actions, you\nmust label all the actions in that video.<\/p>\n<\/blockquote>\n<p>I can write a script to fill in the dead space in the video as a negative action sequence, but I'd like to know what the best practice is before going down that route and spending the money to train yet another terrible model.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1659970389697,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|computer-vision|automl|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":44,
        "Owner_creation_time":1360536048187,
        "Owner_last_access_time":1663951121943,
        "Owner_location":"Miami, FL, USA",
        "Owner_reputation":45,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73279871",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68614794,
        "Question_title":"How to create MLOps vertex ai pipeline with custom sklearn code?",
        "Question_body":"<p>I'm trying to build MLOps pipeline using vertex ai but failing to deploy it<\/p>\n<pre><code>@dsl.pipeline(\n    # Default pipeline root. You can override it when submitting the pipeline.\n    pipeline_root=PIPELINE_ROOT,\n    # A name for the pipeline. Use to determine the pipeline Context.\n    name=&quot;pipeline-test-1&quot;,\n)\ndef pipeline(\nserving_container_image_uri: str = &quot;us-docker.pkg.dev\/cloud-aiplatform\/prediction\/tf2-cpu.2-3:latest&quot;\n):\n    dataset_op = get_data()\n    train_op = train_xgb_model(dataset_op.outputs[&quot;dataset_train&quot;])\n    train_knn = knn_model(dataset_op.outputs[&quot;dataset_train&quot;])\n    \n    eval_op = eval_model(\n        test_set=dataset_op.outputs[&quot;dataset_test&quot;],\n        xgb_model=train_op.outputs[&quot;model_artifact&quot;],\n        knn_model=train_knn.outputs['best_model_artifact']\n    )\n    \n    endpoint_op = gcc_aip.ModelDeployOp(\n    project=PROJECT_ID,\n    model=eval_op.outputs[&quot;model_artifacts&quot;],\n    machine_type=&quot;n1-standard-4&quot;,\n    )\n    \n    #endpoint_op.after(eval_op)\n    \ncompiler.Compiler().compile(pipeline_func=pipeline,\n        package_path='xgb_pipe.json')\n<\/code><\/pre>\n<p>gcc_aip.ModelDeployOp is throwing error that correct model id or name should be pass<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1627856634787,
        "Question_score":1,
        "Question_tags":"mlops|google-cloud-vertex-ai",
        "Question_view_count":154,
        "Owner_creation_time":1448438738667,
        "Owner_last_access_time":1655296770893,
        "Owner_location":null,
        "Owner_reputation":73,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":55,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68614794",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73141754,
        "Question_title":"Vertex AI - Endpoint Call with JSON - Invalid JSON payload received",
        "Question_body":"<p>I successfully trained and deployed a Tensorflow Recommender model on Vertex AI.<\/p>\n<p>Everything is online and to predict the output. In the notebook I do:<\/p>\n<pre><code>loaded = tf.saved_model.load(path)\nscores, titles = loaded([&quot;doctor&quot;])\n<\/code><\/pre>\n<p>That returns:<\/p>\n<pre><code>Recommendations: [b'Nelly &amp; Monsieur Arnaud (1995)'\n b'Three Lives and Only One Death (1996)' b'Critical Care (1997)']\n<\/code><\/pre>\n<p>That is, the payload (input for the neural network) must be <code>[&quot;doctor&quot;]<\/code><\/p>\n<p>Then I generate the JSON for payload (the error is here):<\/p>\n<pre><code>!echo {&quot;\\&quot;&quot;instances&quot;\\&quot;&quot; : [{&quot;\\&quot;&quot;input_1&quot;\\&quot;&quot; : {[&quot;\\&quot;&quot;doctor&quot;\\&quot;&quot;]}}]} &gt; instances0.json\n<\/code><\/pre>\n<p>And submit to the endpoint:<\/p>\n<pre><code>!curl -X POST  \\\n-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n-H &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-west1-aiplatform.googleapis.com\/v1\/projects\/my_project\/locations\/us-west1\/endpoints\/123456789:predict \\\n-d @instances0.json &gt; results.json\n<\/code><\/pre>\n<p>... as seen here: <a href=\"https:\/\/colab.research.google.com\/github\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/community\/vertex_endpoints\/tf_hub_obj_detection\/deploy_tfhub_object_detection_on_vertex_endpoints.ipynb#scrollTo=35348dd21acd\" rel=\"nofollow noreferrer\">https:\/\/colab.research.google.com\/github\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/community\/vertex_endpoints\/tf_hub_obj_detection\/deploy_tfhub_object_detection_on_vertex_endpoints.ipynb#scrollTo=35348dd21acd<\/a><\/p>\n<p>However, when I use this payload, I get error 400:<\/p>\n<pre><code>code: 400\nmessage: &quot;Invalid JSON payload received. Expected an object key or }. s&quot; : [{&quot;input_1&quot; : {[&quot;doctor&quot;]}}]} ^&quot;\nstatus: &quot;INVALID_ARGUMENT&quot;\n<\/code><\/pre>\n<p>This below don't work either:<\/p>\n<pre><code>!echo {&quot;inputs&quot;: {&quot;input_1&quot;: [&quot;doctor&quot;]}} &gt; instances0.json\n<\/code><\/pre>\n<p>Even with validated JSON Lint, it does not return the proper prediction.<\/p>\n<p>In another Stackoverflow question is suggested to remove the &quot; \\ &quot; in the payload, but this didn't work either.<\/p>\n<p>Running:<\/p>\n<pre><code>!saved_model_cli show --dir \/home\/jupyter\/model --all\n<\/code><\/pre>\n<p>I get:<\/p>\n<pre><code>MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['__saved_model_init_op']:\n  The given SavedModel SignatureDef contains the following input(s):\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['__saved_model_init_op'] tensor_info:\n        dtype: DT_INVALID\n        shape: unknown_rank\n        name: NoOp\n  Method name is: \n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['input_1'] tensor_info:\n        dtype: DT_STRING\n        shape: (-1)\n        name: serving_default_input_1:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['output_1'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 10)\n        name: StatefulPartitionedCall_1:0\n    outputs['output_2'] tensor_info:\n        dtype: DT_STRING\n        shape: (-1, 10)\n        name: StatefulPartitionedCall_1:1\n  Method name is: tensorflow\/serving\/predict\n\n\nConcrete Functions:\n  Function Name: '__call__'\n    Option #1\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: True\n    Option #2\n      Callable with:\n        Argument #1\n          queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: True\n    Option #3\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: False\n    Option #4\n      Callable with:\n        Argument #1\n          queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: False\n\n  Function Name: '_default_save_signature'\n    Option #1\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\n\n  Function Name: 'call_and_return_all_conditional_losses'\n    Option #1\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: False\n    Option #2\n      Callable with:\n        Argument #1\n          queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: True\n    Option #3\n      Callable with:\n        Argument #1\n          queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: False\n    Option #4\n      Callable with:\n        Argument #1\n          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\n        Argument #2\n          DType: NoneType\n          Value: None\n        Argument #3\n          DType: bool\n          Value: True\n<\/code><\/pre>\n<p>The point is: I'm passing an array and I'm not sure if it must be in b64 format.<\/p>\n<p>This Python code works, but returns a different result than expected:<\/p>\n<pre><code>import tensorflow as tf\nimport base64\nfrom google.protobuf import json_format\nfrom google.protobuf.struct_pb2 import Value\nimport numpy as np\nfrom google.cloud import aiplatform\nimport os\nvertex_model = tf.saved_model.load(&quot;gs:\/\/bucket\/model&quot;)\n\nserving_input = list(\n    vertex_model.signatures[&quot;serving_default&quot;].structured_input_signature[1].keys()\n)[0]\n\nprint(&quot;Serving input :&quot;, serving_input)\n\naip_endpoint_name = (\n    f&quot;projects\/my-project\/locations\/us-west1\/endpoints\/12345567&quot;\n)\nendpoint = aiplatform.Endpoint(aip_endpoint_name)\n\ndef encode_input(input):\n    return base64.b64encode(np.array(input)).decode(&quot;utf-8&quot;)\n\ninstances_list = [{serving_input: {&quot;b64&quot;: encode_input(np.array([&quot;doctor&quot;]))}}]\ninstances = [json_format.ParseDict(s, Value()) for s in instances_list]\n\nresults = endpoint.predict(instances=instances)\nprint(results.predictions[0][&quot;output_2&quot;])\n\n\n['8 1\/2 (1963)', 'Sword in the Stone, The (1963)', 'Much Ado About Nothing (1993)', 'Jumanji (1995)', 'As Good As It Gets (1997)', 'Age of Innocence, The (1993)', 'Double vie de V\u00e9ronique, La (Double Life of Veronique, The) (1991)', 'Piano, The (1993)', 'Eat Drink Man Woman (1994)', 'Bullets Over Broadway (1994)']\n<\/code><\/pre>\n<p>Any ideas on how to fix \/ encode the payload ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658940444647,
        "Question_score":0,
        "Question_tags":"json|google-cloud-platform|google-cloud-vertex-ai|google-ai-platform",
        "Question_view_count":105,
        "Owner_creation_time":1475181309097,
        "Owner_last_access_time":1664074238173,
        "Owner_location":"Brazil",
        "Owner_reputation":4242,
        "Owner_up_votes":735,
        "Owner_down_votes":15,
        "Owner_views":421,
        "Question_last_edit_time":1659415987720,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73141754",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70435505,
        "Question_title":"Vertex AI AutoML getting data about Model, Dataset, Training Job",
        "Question_body":"<p>I am using Vertex AI for AutoML Video classification and I would like to get some data that I'm seeing in Web UI (Cloud Console) (Model\/Dataset detail).\nI'm using AI platform Python SDK or REST API.<\/p>\n<p>For example Model API returns 'training videos' but not test videos (web Model detail, tab EVALUATE)<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Y7Um9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Y7Um9.png\" alt=\"Vertex AI Model evaluation\" \/><\/a><\/p>\n<p>then for example in tab Model Properties on the web I can't obtain Training time, Total items, Algorithm, Objective, Total Items<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/5Pi96.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5Pi96.png\" alt=\"Vertex AI Model properties\" \/><\/a><\/p>\n<p>For Dataset detail, I would like to get number of labeled\/unlabeled videos, labels and correspoding number<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xr8Gw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xr8Gw.png\" alt=\"Dataset detail, labels\" \/><\/a><\/p>\n<p>This is code I'm using to get the data (as component in Vertex AI Pipeline):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_metadata(project_id, region, model_id):\n    import requests\n\n    import google.auth\n    import google.cloud.aiplatform as aip\n    from google.cloud import aiplatform_v1\n    from google.protobuf import json_format\n    from google.auth.transport import requests as grequests\n\n    aip.init(project=project_id, location=region)\n    API_ENDPOINT = &quot;{}-aiplatform.googleapis.com&quot;.format(region)\n    model = aip.Model(model_id)\n    model_dict = model.to_dict()\n    model_metadata = model_dict['metadata']\n\n    model_name = model_dict['displayName']\n    model_creation_date = model_dict['createTime']\n    model_type = model_metadata['modelType']\n    number_training = model_metadata['trainingDataItemsCount']\n\n    client_options = {\n        &quot;api_endpoint&quot;: API_ENDPOINT\n    }\n    model_path = model.resource_name\n    client_model = aiplatform_v1.services.model_service.ModelServiceClient(client_options=client_options)\n    list_eval_request = aiplatform_v1.types.ListModelEvaluationsRequest(parent=model_path)\n    list_eval = client_model.list_model_evaluations(request=list_eval_request)\n\n    eval_name = ''\n    for val in list_eval:\n        eval_name = val.name\n    get_eval_request = aiplatform_v1.types.GetModelEvaluationRequest(name=eval_name)\n    model_eval = client_model.get_model_evaluation(request=get_eval_request)\n    model_eval_data = json_format.MessageToDict(model_eval._pb)\n\n    model_metrics = model_eval_data['metrics']\n    average_precision = model_metrics.get('auPrc')\n    confidence_metrics = model_metrics['confidenceMetrics']\n    confidence_threshold = -1\n    f1_score = -1\n    precision = -1\n    recall = -1\n\n    for item in confidence_metrics:\n        confidence_threshold_temp = item['confidenceThreshold']\n        if confidence_threshold_temp &gt;= 0.5:\n            confidence_threshold = confidence_threshold_temp\n            f1_score = item['f1Score']\n            precision = item['precision']\n            recall = item['recall']\n            break\n    # auc_precision = precision\n    # auc_recall = recall\n\n    credentials, _ = google.auth.default()\n    r = grequests.Request()\n    credentials.refresh(r)\n    training_pipeline_resource_name = model_dict['trainingPipeline']\n\n    training_pipeline_url = f'https:\/\/{API_ENDPOINT}\/v1beta1\/{training_pipeline_resource_name}'\n    headers = {\n        'Authorization': f'Bearer {credentials.token}'\n    }\n    r = requests.get(training_pipeline_url, headers=headers)\n    training_pipeline_detail = r.json()\n    input_data_config = training_pipeline_detail.get('inputDataConfig', {})\n    dataset_id = input_data_config.get('datasetId', '')\n    fraction_split = input_data_config.get('fractionSplit', {})\n    test_fraction = fraction_split.get('testFraction')\n    training_fraction = fraction_split.get('trainingFraction')\n    data_split = f'{training_fraction}\/{test_fraction}'\n\n    dataset = aip.VideoDataset(dataset_id)\n    dataset_resource = json_format.MessageToDict(dataset.gca_resource._pb)\n    dataset_name = dataset_resource.get('displayName')\n    dataset_creation_date = dataset_resource.get('createTime')\n    labels = dataset_resource['labels']\n    dataset_type = labels.get('aiplatform.googleapis.com\/dataset_metadata_schema')\n\n    data = {\n        'model_id': model_id,\n        'model_name': model_name,\n        'model_creation_date': model_creation_date,\n        'model_type': model_type,\n        'number_training': number_training,\n        'average_precision': average_precision,\n        'precision': precision,\n        'recall': recall,\n        'data_split': data_split,\n        'dataset_name': dataset_name,\n        'dataset_type': dataset_type,\n        'dataset_id': dataset_id,\n        'dataset_creation_date': dataset_creation_date,        \n    }\n\n<\/code><\/pre>\n<p>Also for example what I found is that on training job when I created dataset, training model via WebUI I can obtain data split (training\/testing ratio), but when I'm doing this in Vertex AI Pipelines, I'm not explicitly setting data split for AutoMLVideoTrainingJobRunOp, I can't get data split from Training job detail, so it seems that it saved only when it's explicitly set.<\/p>\n<p>Other thing I noticed is when API requests are made for Cloud Console (inspecting Chrome Dev Tools) it returns more (richer) data (items) then for public Vertex AI APIs.<\/p>\n<p>I'm not sure if this is temporary or intentional\/permanent behaviour.<\/p>\n<p>I would appreciate thoughts\/comments\/help with this.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1640089790567,
        "Question_score":0,
        "Question_tags":"google-cloud-automl|google-cloud-vertex-ai|google-cloud-ai|google-cloud-ai-platform-pipelines",
        "Question_view_count":395,
        "Owner_creation_time":1372196724860,
        "Owner_last_access_time":1664040926440,
        "Owner_location":"Prague, Czech Republic",
        "Owner_reputation":276,
        "Owner_up_votes":46,
        "Owner_down_votes":0,
        "Owner_views":47,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70435505",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72809603,
        "Question_title":"VertexAI Tabular AutoML rejecting rows containing nulls",
        "Question_body":"<p>I am trying to build a binary classifier based on a tabular dataset that is rather sparse, but training is failing with the following message:<\/p>\n<blockquote>\n<p>Training pipeline failed with error message: Too few input rows passed validation. Of 1169548 inputs, 194 were valid. At least 50% of rows must pass validation.<\/p>\n<\/blockquote>\n<p>My understanding was that tabular AutoML should be able to handle Null values, so I'm not sure what's happening here, and I would appreciate any suggestions. The <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/tabular-data\/tabular101\" rel=\"nofollow noreferrer\">documentation<\/a> explicitly mentions reviewing each column's nullability, but I don't see any way to set or check a column's nullability on the dataset tab (perhaps the documentation is out of date?). Additionally, the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/data-types-tabular#what_values_are_treated_as_null_values\" rel=\"nofollow noreferrer\">documentation<\/a> explicitly mentions that missing values are treated as null, which is how I've set up my CSV. The <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/data-types-tabular#numeric\" rel=\"nofollow noreferrer\">documentation for numeric<\/a> however does not explicitly list support for missing values, just NaN and inf.<\/p>\n<p>The dataset is 1 million rows, 34 columns, and only 189 rows are null-free. My most sparse column has data in 5,000 unique rows, with the next rarest having data in 72k and 274k rows respectively. Columns are a mix of categorical and numeric, with only a handful of columns without nulls.<\/p>\n<p>The data is stored as a CSV, and the Dataset import seems to run without issue. Generate statistics ran on the dataset, but for some reason the missing % column failed to populate. What might be the best way to address this? I'm not sure if this is a case where I need to change my null representation in the CSV, change some dataset\/training setting, or if its an AutoML bug (less likely). Thanks!<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bF1Nn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bF1Nn.png\" alt=\"Image of missing % column being blank\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1656554846450,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":122,
        "Owner_creation_time":1614739500313,
        "Owner_last_access_time":1664046580507,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1656987570560,
        "Answer_body":"<p>To allow invalid &amp; null values during training &amp; prediction, we have to explicitly set the <code>allow invalid values<\/code> flag to <code>Yes<\/code> during training as shown in the image below. You can find this setting under model training settings on the dataset page. The flag has to be set on a column by column basis.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jWDGm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jWDGm.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1657366819400,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72809603",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70003299,
        "Question_title":"Vertex AI returns a different result from the local tflite model",
        "Question_body":"<p>I uploaded my tflite model on Vertex AI and made an endpoint, and I requested inference with some input value, but it returns a different result from my local tflite model's inference result.<\/p>\n<p>The input value is float32 array(actually sampled audio data) and I used <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predict_custom_trained_model_sample.py\" rel=\"nofollow noreferrer\">this code<\/a> for the request. Although there was the same input array, the local tflite model and the model which is uploaded on Vertex AI returns quite big different result.<\/p>\n<p>Is there any possibility of distortion on the value while it transfers to the Vertex AI instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637147367180,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|google-cloud-ai",
        "Question_view_count":175,
        "Owner_creation_time":1630024461567,
        "Owner_last_access_time":1660186387690,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1637188171083,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70003299",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70175440,
        "Question_title":"Google Cloud Platform - Vertex AI - is there a way to look at a chart of training performance over time?",
        "Question_body":"<p>I'd like to know how the training performance changes over the course of the training. Is there any way to access that via Vertex AI automl service?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638302912743,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":249,
        "Owner_creation_time":1392762853817,
        "Owner_last_access_time":1663862388447,
        "Owner_location":null,
        "Owner_reputation":955,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":94,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70175440",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71979012,
        "Question_title":"Vertex AI scheduled notebooks doesn't recognize existence of folders",
        "Question_body":"<p>I have a managed jupyter notebook in Vertex AI that I want to schedule. The notebook works just fine as long as I start it manually, but as soon as it is scheduled, it fails. There are in fact many things that go wrong when scheduled, some of them are fixable. Before explaining what my trouble is, let me first give some details of the context.<\/p>\n<p>The notebook gathers information from an API for several stores and saves the data in different folders before processing it, saving csv-files to store-specific folders and to bigquery. So, in the location of the notebook, I have:<\/p>\n<ul>\n<li>The notebook<\/li>\n<li>Functions needed for the handling of data (as *.py files)<\/li>\n<li>A series of folders, some of which have subfolders which also have subfolders<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xhOWs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xhOWs.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>When I execute this manually, no problem. Everything works well and all files end up exactly where they should, as well as in different bigQuery tables.<\/p>\n<p>However, when scheduling the execution of the notebook, everything goes wrong. First, the files *.py cannot be read (as <code>import<\/code>). No problem, I added the functions in the notebook.<\/p>\n<p>Now, the following error is where I am at a loss, because I have no idea why it does work or how to fix it. The code that leads to the error is the following:<\/p>\n<pre><code>internal = &quot;https:\/\/api.************************&quot;\n\ndf_descriptions = [] \n\nstoress = internal\nresponse_stores = requests.get(storess,auth = HTTPBasicAuth(userInternal, keyInternal))\npathlib.Path(&quot;stores\/request_1.json&quot;).write_bytes(response_stores.content)\n\nfilepath = &quot;stores&quot;\n\nfiles = os.listdir(filepath)\n\nfor file in files:\n    with open(filepath + &quot;\/&quot;+file) as json_string:\n        jsonstr = json.load(json_string)\n        information = pd.json_normalize(jsonstr)\n    df_descriptions.append(information)\n\nStoreINFO = pd.concat(df_descriptions)\nStoreINFO = StoreINFO.dropna()\nStoreINFO = StoreINFO[StoreINFO['storeIdMappings'].map(lambda d: len(d)) &gt; 0]\n\ncloud_store_ids = list(set(StoreINFO.cloudStoreId))\n\nLastWeek = datetime.date.today()- timedelta(days=2)\nLastWeek =np.datetime64(LastWeek)\n<\/code><\/pre>\n<p>and the error reported is:<\/p>\n<pre><code>FileNotFoundError                         Traceback (most recent call last)\n\/tmp\/ipykernel_165\/2970402631.py in &lt;module&gt;\n      5 storess = internal\n      6 response_stores = requests.get(storess,auth = HTTPBasicAuth(userInternal, keyInternal))\n----&gt; 7 pathlib.Path(&quot;stores\/request_1.json&quot;).write_bytes(response_stores.content)\n      8 \n      9 filepath = &quot;stores&quot;\n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in write_bytes(self, data)\n   1228         # type-check for the buffer interface before truncating the file\n   1229         view = memoryview(data)\n-&gt; 1230         with self.open(mode='wb') as f:\n   1231             return f.write(view)\n   1232 \n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in open(self, mode, buffering, encoding, errors, newline)\n   1206             self._raise_closed()\n   1207         return io.open(self, mode, buffering, encoding, errors, newline,\n-&gt; 1208                        opener=self._opener)\n   1209 \n   1210     def read_bytes(self):\n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in _opener(self, name, flags, mode)\n   1061     def _opener(self, name, flags, mode=0o666):\n   1062         # A stub for the opener argument to built-in open()\n-&gt; 1063         return self._accessor.open(self, flags, mode)\n   1064 \n   1065     def _raw_open(self, flags, mode=0o777):\n\nFileNotFoundError: [Errno 2] No such file or directory: 'stores\/request_1.json'\n<\/code><\/pre>\n<p>I would gladly find another way to do this, for instance by using GCS buckets, but my issue is the existence of sub-folders. There are many stores and I do not wish to do this operation manually because some retailers for which I am doing this have over 1000 stores. My python code generates all these folders and as I understand it, this is not feasible in GCS.<\/p>\n<p>How can I solve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1650711286300,
        "Question_score":0,
        "Question_tags":"python-3.x|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":247,
        "Owner_creation_time":1442929315877,
        "Owner_last_access_time":1664024443763,
        "Owner_location":"Stockholm, Sverige",
        "Owner_reputation":4598,
        "Owner_up_votes":1855,
        "Owner_down_votes":13,
        "Owner_views":855,
        "Question_last_edit_time":1650713970783,
        "Answer_body":"<p>GCS uses a flat namespace, so folders don't actually exist, but can be simulated as given in this <a href=\"https:\/\/cloud.google.com\/storage\/docs\/folders\" rel=\"nofollow noreferrer\">documentation<\/a>.For your requirement, you can either use absolute path (starting with &quot;\/&quot; -- not relative) or create the &quot;stores&quot; <a href=\"https:\/\/docs.python.org\/3\/library\/pathlib.html#pathlib.Path.mkdir\" rel=\"nofollow noreferrer\">directory<\/a> (with &quot;mkdir&quot;). For more information you can check this <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/schedule-and-execute-notebooks-with-vertex-ai-workbench\" rel=\"nofollow noreferrer\">blog<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1651402011487,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1651402328352,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71979012",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70740897,
        "Question_title":"Specifying Machine Type in Vertex AI Pipeline",
        "Question_body":"<p>I have a pipeline component defined like this:<\/p>\n<pre><code>data_task = run_ssd_data_op(\n        labels_path=input_labels,\n        data_config=config_task.outputs[&quot;output_data_config&quot;],\n        training_config=config_task.outputs[&quot;output_training_config&quot;],\n        assets_json=dump_conversion_task.outputs[&quot;output_ssd_query&quot;]\n    )\ndata_task.execution_options.caching_strategy.max_cache_staleness = &quot;P0D&quot;\ndata_task.container.add_resource_request('cpu', cpu_request)\ndata_task.container.add_resource_request('memory', memory_request)\n<\/code><\/pre>\n<p>When I run the pipeline on VertexAI the above component runs on an E2 machine type which matches the CPU and RAM requirements.<\/p>\n<p>However, the component runs much more slowly on VertexAI than on the Kubeflow pipeline I setup using AIPlatform. I configured that cluster to use N1-highmem-32 machines for this job.<\/p>\n<p>I would like to request that this component is run on an <code>n1-highmem-32<\/code> machine, how can I do that?<\/p>\n<p>For the GPU component of the pipeline I could use the line:<\/p>\n<pre><code>training_task.add_node_selector_constraint('cloud.google.com\/gke-accelerator', 'NVIDIA_TESLA_T4').set_gpu_limit(\n        gpu_request)\n<\/code><\/pre>\n<p>What is the equivalent <code>node_selector_constraint<\/code> that I need to apply to my <code>data_task<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1642420252480,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|kubeflow|google-cloud-vertex-ai",
        "Question_view_count":603,
        "Owner_creation_time":1350304341050,
        "Owner_last_access_time":1663674484983,
        "Owner_location":"Morecambe, United Kingdom",
        "Owner_reputation":3827,
        "Owner_up_votes":145,
        "Owner_down_votes":3,
        "Owner_views":573,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70740897",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72930772,
        "Question_title":"Using GCP's Vertex AI Image Classification exported (TF SavedModel) model for prediction",
        "Question_body":"<p>I've trained an Image Classification model via Google Cloud Platform's Vertex AI framework and liked the results. Due to that I then proceeded to export it in Tensorflow SavedModel format (shows up as 'Container' export) for custom prediction because I don't like neither the slowness of Vertex's batch prediction nor the high cost of using a Vertex endpoint.<\/p>\n<p>In my python code I used<\/p>\n<pre><code>model = tensorflow.saved_model.load(model_path)\ninfer =  model.signatures[&quot;serving_default&quot;]\n<\/code><\/pre>\n<p>When I tried to inspect what <code>infer<\/code> requires I saw that its input is two parameters: <code>image_bytes<\/code> and <code>key<\/code>. Both are string-type tensors.<\/p>\n<p>This question can be broken off into several sub-questions that then make a whole:<\/p>\n<ol>\n<li>Isn't inference done on multiple data instances? If so, why is it image_bytes and not images_bytes?<\/li>\n<li>Is image_bytes just the output of <code>open(&quot;img.jpg&quot;, &quot;rb&quot;).read()<\/code>? If so, don't I have to resize it first? To what size? How do I check that?<\/li>\n<li>What is key? I have absolutely no clue or guess regarding this one's meaning.<\/li>\n<\/ol>\n<p>The documentation for GCP is paid only and so I have decided to ask for help here. I tried to search for an answer on google for multiple days but found no relevant article.\nThank you for reading and your help would be greatly appreciated and maybe even useful to future readers.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1657476634237,
        "Question_score":0,
        "Question_tags":"python|tensorflow|google-cloud-platform|image-classification|google-cloud-vertex-ai",
        "Question_view_count":118,
        "Owner_creation_time":1442687709540,
        "Owner_last_access_time":1663092654683,
        "Owner_location":null,
        "Owner_reputation":79,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Question_last_edit_time":1659779993552,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72930772",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72827960,
        "Question_title":"How to get image classification prediction from GCP AIPlatform in ruby?",
        "Question_body":"<p>I'm new with ruby and I want to use GCP AIPlatform but I'm struggeling with the payload.<\/p>\n<p>So far, I have :<\/p>\n<pre class=\"lang-rb prettyprint-override\"><code>client = ::Google::Cloud::AIPlatform::V1::PredictionService::Client.new do |config|\n  config.endpoint = &quot;#{location}-aiplatform.googleapis.com&quot;\nend\n\nimg = File.open(imgPath, 'rb') do |img|\n  'data:image\/png;base64,' + Base64.strict_encode64(img.read)\nend\n\ninstance = Instance.new(:content =&gt; img)\n\nrequest = Google::Cloud::AIPlatform::V1::PredictRequest.new(\n  endpoint: &quot;projects\/#{project}\/locations\/#{location}\/endpoints\/#{endpoint}&quot;,\n  instances: [instance]\n)\n\nresult = client.predict request\np result\n<\/code><\/pre>\n<p>Here is my proto<\/p>\n<pre><code>message Instance {\n  required bytes content = 1;\n};\n<\/code><\/pre>\n<p>But I have the following error : <code>Invalid type Instance to assign to submessage field 'instances'<\/code><\/p>\n<p>I read the documentation but for ruby SDK it's a bit light.\nThe parameters are OK, the JS example here : <a href=\"https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-image-object-detection.js\" rel=\"nofollow noreferrer\">https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-image-object-detection.js<\/a> is working with those parameters<\/p>\n<p>What am I doing wrong ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1656671602857,
        "Question_score":0,
        "Question_tags":"ruby|google-cloud-vertex-ai",
        "Question_view_count":54,
        "Owner_creation_time":1656670919183,
        "Owner_last_access_time":1659105111753,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I managed it<\/p>\n<pre class=\"lang-rb prettyprint-override\"><code>client = Google::Cloud::AIPlatform::V1::PredictionService::Client.new do |config|\n  config.endpoint = &quot;#{location}-aiplatform.googleapis.com&quot;\nend\n\nimg = File.open(imgPath, 'rb') do |img|\n  Base64.strict_encode64(img.read)\nend\n\ninstance = Google::Protobuf::Value.new(:struct_value =&gt; {:fields =&gt; {\n  :content =&gt; {:string_value =&gt; img}\n}})\nendpoint = &quot;projects\/#{project}\/locations\/#{location}\/endpoints\/#{endpoint}&quot;\n\n\nrequest = Google::Cloud::AIPlatform::V1::PredictRequest.new(\n  endpoint: endpoint,\n  instances: [instance]\n)\n\nresult = client.predict request\np result\n<\/code><\/pre>\n<p>The use of the Google::Protobuf::Value looks ugly to me but it works<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1656947266916,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72827960",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73345417,
        "Question_title":"Vertex AI Instance Out of Disk Space",
        "Question_body":"<p>I accidentally ran out of disk space on my Vertex AI instance. There's no way to connect to it by any means now. Using the jupyter failed as there's not enough disk space:\n<code>OSError: [Errno 28] No space left on device<\/code><\/p>\n<p>I tried to increase disk space for both boot and data disks using <code>gcloud compute disks resize<\/code>, but it still doesn't work despite disk space being shown as increased in the machine info panel.<\/p>\n<p>Also tried connecting through ssh but got timeouts. My guess is that it's still caused by disk space.<\/p>\n<p>So is there any ways to recover the instance without a hard reset?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1660403531073,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":71,
        "Owner_creation_time":1660402958963,
        "Owner_last_access_time":1663710025030,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1660403595092,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73345417",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71480531,
        "Question_title":"Hugging face code throwing error when running using vertex-ai in gcp",
        "Question_body":"<p>I am training a NLP Hugging face model in vertex-ai with custom image.<\/p>\n<p>The same code works in local machine.<\/p>\n<p>Here is my code and the error.<\/p>\n<pre><code>import torch\nimport numpy as np\nimport pandas as pd\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport transformers as tr\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup,BertForMaskedLM\nfrom transformers import DataCollatorForLanguageModeling\nfrom scipy.special import softmax\nimport scipy\nimport random\nimport pickle\nimport os\n\nprint(&quot;package imported completed&quot;)\n\nos.environ['TRANSFORMERS_OFFLINE']='1'\nos.environ['HF_MLFLOW_LOG_ARTIFACTS']='TRUE'\n\nprint(&quot;env setup completed&quot;)\nprint( tr.__version__)\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(&quot;Using&quot;, device)\ntorch.backends.cudnn.deterministic = True  \n\ntr.trainer_utils.set_seed(0)\n\nprint(&quot;here&quot;)\n\ntokenizer = tr.XLMRobertaTokenizer.from_pretrained(&quot;xlm-roberta-large&quot;,local_files_only=True)\nmodel = tr.XLMRobertaForMaskedLM.from_pretrained(&quot;xlm-roberta-large&quot;, return_dict=True,local_files_only=True)\n\nmodel.to(device)\nprint(&quot;Model loaded successfully&quot;)\n\ndf=pd.read_csv(&quot;gs:\/\/****bucket***\/data.csv&quot;) \nprint(&quot;read csv&quot;)\n# ,engine='openpyxl',sheet_name=&quot;master_data&quot;\ntrain_df=df.text.tolist()\nprint(len(train_df))\n\ntrain_df=list(set(train_df))\ntrain_df = [x for x in train_df if str(x) != 'nan']\ntrain_df=train_df[:50]\n\nprint(&quot;Length of training data is \\n &quot;,len(train_df))\nprint(&quot;DATA LOADED successfully&quot;)\n\n\ntrain_encodings = tokenizer(train_df, truncation=True, padding=True, max_length=512, return_tensors=&quot;pt&quot;)\nprint(&quot;encoding done&quot;)\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\nprint(&quot;data collector done&quot;)\n\nclass SEDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n        \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        return item\n\n    def __len__(self):\n        return len(self.encodings[&quot;attention_mask&quot;])\n\ntrain_data = SEDataset(train_encodings)\n\nprint(&quot;train data created&quot;)\n\ntraining_args = tr.TrainingArguments(\n    output_dir='gs:\/\/****bucket***\/results_mlm_exp1', \n    overwrite_output_dir=True,\n    num_train_epochs=2,\n    per_device_train_batch_size=4,\n#     per_device_train_batch_size\n#     per_gpu_train_batch_size\n    prediction_loss_only=True\n#     ,save_strategy=&quot;epoch&quot;\n#     ,run_name=&quot;MLM_Exp1&quot;\n    ,learning_rate=2e-5\n#     logging_dir='gs:\/\/****bucket***\/logs_mlm_exp1',            # directory for storing logs\n#     logging_steps=32000,\n)\n\ntrainer = tr.Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_data,\n)\nprint(&quot;training to start&quot;)\ntrainer.train()\nprint(&quot;model training finished&quot;)\ntrainer.save_model(&quot;gs:\/\/****bucket***\/model_mlm_exp1&quot;)\n\nprint(&quot;training finished&quot;)\n<\/code><\/pre>\n<p>The error that I get is:<\/p>\n<pre><code>None    INFO    train data created\nNone    INFO    training to start\nNone    ERROR   0%| | 0\/8 [00:00&lt;?, ?it\/s]train.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\nNone    ERROR     item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\nNone    ERROR   \/opt\/conda\/lib\/python3.7\/site-packages\/torch\/nn\/parallel\/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\nNone    ERROR     warnings.warn('Was asked to gather along dimension 0, but all '\n\/var\/sitecustomize\/sitecustomize.py INFO    None\nNone    ERROR   0%| | 0\/8 [00:09&lt;?, ?it\/s]\n<\/code><\/pre>\n<p>Most of them are warning but still my code stops with error.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1647339579240,
        "Question_score":0,
        "Question_tags":"python-3.x|nlp|huggingface-transformers|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":153,
        "Owner_creation_time":1528361086053,
        "Owner_last_access_time":1663924548837,
        "Owner_location":null,
        "Owner_reputation":1127,
        "Owner_up_votes":526,
        "Owner_down_votes":93,
        "Owner_views":283,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71480531",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69904211,
        "Question_title":"Vertex AI prediction - Autoscaling cannot set minimum node to 0",
        "Question_body":"<p>I am unclear abut Vertex AI pricing for model predictions. In the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/pricing\" rel=\"nofollow noreferrer\">documentation<\/a>, under the heading <strong>More about automatic scaling of prediction nodes<\/strong> one of the points mentioned is:<\/p>\n<blockquote>\n<p>&quot;If you choose automatic scaling, the number of nodes scales\nautomatically, and can scale down to zero for no-traffic durations&quot;<\/p>\n<\/blockquote>\n<p>The example provided in the documentation later also seems to suggest that during a period with no traffic, zero nodes are in use. However, when I create an Endpoint in Vertex AI, under the <strong>Autoscaling<\/strong> heading it says:<\/p>\n<blockquote>\n<p><em>&quot;Autoscaling: If you set a minimum and maximum, compute nodes will scale to meet traffic demand within those boundaries&quot;<\/em><\/p>\n<\/blockquote>\n<p>The  value of 0 under <em>&quot;Minimum number of compute nodes&quot;<\/em> is not allowed so you have to enter 1 or greater, and it is mentioned that:<\/p>\n<blockquote>\n<p>Default is 1. If set to 1 or more, then compute resources will\ncontinuously run even without traffic demand. This can increase cost\nbut avoid dropped requests due to node initialization.<\/p>\n<\/blockquote>\n<p>My question is, what happens when I select autoscaling by setting Minimum to 1 and Maximum to, say, 10. Does 1 node always run continuously? Or does it scale down to 0 nodes in no traffic condition as the documentation suggests.<\/p>\n<p>To test I deployed an Endpoint with Autoscaling (min and max set to 1) and then when I sent a prediction request the response was almost immediate, suggesting the node was already up. I did that again after about an hour and again the response was immediate suggesting that the node never shut down probably. Also, for high latency requirements, is having autoscale to 0 nodes, if that is indeed possible, even practical, i.e., what latency can we expect for starting up from 0 nodes?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1636487376953,
        "Question_score":4,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":894,
        "Owner_creation_time":1471292986790,
        "Owner_last_access_time":1664058170413,
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Question_last_edit_time":1636603578603,
        "Answer_body":"<p>Are you using an N1 or a non-N1 machine type? If you want to autoscale to zero, you must use non-N1 machines. See <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/overview#node_allocation_for_online_prediction\" rel=\"nofollow noreferrer\">second note<\/a> from node allocation:<\/p>\n<blockquote>\n<p>Note: Versions that use a Compute Engine (N1) machine type cannot scale down to zero nodes. They can scale down to 1 node, at minimum.<\/p>\n<\/blockquote>\n<p><em>Update<\/em>: AI Platform supports scaling to zero, while Vertex AI currently does not. From the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#scaling\" rel=\"nofollow noreferrer\">scaling<\/a> documentation, nodes can scale but there is no mention that it can scale down to zero. Here's a public <a href=\"https:\/\/issuetracker.google.com\/206042974\" rel=\"nofollow noreferrer\">feature request<\/a> for people who wants to track this issue.<\/p>\n<p>With regards to latency requirements, the actual output will vary. However, one thing to note according to the documentation is that the service may not be able to bring nodes online fast enough to keep up with large spikes of request traffic. If your traffic regularly has steep spikes, and if reliably low latency is important to your application, you may want to consider manual scaling.<\/p>\n<p>Additional Reference: <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/machine-types-online-prediction#automatic_scaling\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/machine-types-online-prediction#automatic_scaling<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1636603298292,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1636679717047,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69904211",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72014493,
        "Question_title":"Training spaCy model as a Vertex AI Pipeline \"Component\"",
        "Question_body":"<p>I am trying to <a href=\"https:\/\/spacy.io\/usage\/training\" rel=\"nofollow noreferrer\">train a spaCy model<\/a> , but turning the code into a Vertex AI Pipeline <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro?hl=en#:%7E:text=Step%201%3A%20Create%20a%20Python%20function%20based%20component\" rel=\"nofollow noreferrer\">Component<\/a>. My current code is:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;train.yaml&quot;\n)\ndef train(train_name: str, dev_name: str) -&gt; NamedTuple(&quot;output&quot;, [(&quot;model_path&quot;, str)]):\n    &quot;&quot;&quot;\n    Trains a spacy model\n    \n    Parameters:\n    ----------\n    train_name : Name of the spaCy &quot;train&quot; set, used for model training.\n    dev_name: Name of the spaCy &quot;dev&quot; set, , used for model training.\n    \n    Returns:\n    -------\n    output : Destination path of the saved model.\n    &quot;&quot;&quot;\n    import spacy\n    import subprocess\n    \n    spacy.require_gpu()  # &lt;=== IMAGE FAILS TO BE COMPILED HERE\n    \n    # NOTE: The remaining code has already been tested and proven to be functional.\n    #       It has been edited since the project is private.\n    \n    # Presets for training\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;init&quot;, &quot;fill-config&quot;, &quot;gcs\/secret_path_to_config\/base_config.cfg&quot;, &quot;config.cfg&quot;])\n\n    # Training model\n    location = &quot;gcs\/secret_model_destination_path\/TestModel&quot;\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;train&quot;, &quot;config.cfg&quot;,\n                    &quot;--output&quot;, location,\n                    &quot;--paths.train&quot;, &quot;gcs\/secret_bucket\/secret_path\/{}.spacy&quot;.format(train_name),\n                    &quot;--paths.dev&quot;, &quot;gcs\/secret_bucket\/secret_path\/{}.spacy&quot;.format(dev_name),\n                    &quot;--gpu-id&quot;, &quot;0&quot;])\n    \n    return (location,)\n<\/code><\/pre>\n<p>The Vertex AI Logs display the following as main cause of the failure:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bRotZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bRotZ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The libraries are successfully installed, and yet I feel like there is some missing library \/ setting (as I know by <a href=\"https:\/\/dev.to\/davidgerva\/spacy-3-on-a-google-cloud-compute-instance-to-train-a-ner-transformer-model-23hf\" rel=\"nofollow noreferrer\">experience<\/a>); however I don't know  how to make it &quot;Python-based Vertex AI Components Compatible&quot;. BTW, the use of GPU is <strong>mandatory<\/strong> in my code.<\/p>\n<p>Any ideas?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1650978304193,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|spacy-3|google-cloud-vertex-ai|spacy-transformers",
        "Question_view_count":234,
        "Owner_creation_time":1629385138957,
        "Owner_last_access_time":1663953209400,
        "Owner_location":null,
        "Owner_reputation":395,
        "Owner_up_votes":18,
        "Owner_down_votes":4,
        "Owner_views":38,
        "Question_last_edit_time":1651090705967,
        "Answer_body":"<p>After some rehearsals, I think I have figured out what my code was missing. Actually, the <code>train<\/code> component definition was correct (with some minor tweaks relative to what was originally posted); however <strong>the pipeline was missing the GPU definition<\/strong>. I will first include a dummy example code, which trains a NER model using spaCy, and orchestrates everything via Vertex AI Pipeline:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from kfp.v2 import compiler\nfrom kfp.v2.dsl import pipeline, component, Dataset, Input, Output, OutputPath, InputPath\nfrom datetime import datetime\nfrom google.cloud import aiplatform\nfrom typing import NamedTuple\n\n\n# Component definition\n\n@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;generate.yaml&quot;\n)\ndef generate_spacy_file(train_path: OutputPath(), dev_path: OutputPath()):\n    &quot;&quot;&quot;\n    Generates a small, dummy 'train.spacy' &amp; 'dev.spacy' file\n    \n    Returns:\n    -------\n    train_path : Relative location in GCS, for the &quot;train.spacy&quot; file.\n    dev_path: Relative location in GCS, for the &quot;dev.spacy&quot; file.\n    &quot;&quot;&quot;\n    import spacy\n    from spacy.training import Example\n    from spacy.tokens import DocBin\n\n    td = [    # Train (dummy) dataset, in 'spacy V2 presentation'\n              (&quot;Walmart is a leading e-commerce company&quot;, {&quot;entities&quot;: [(0, 7, &quot;ORG&quot;)]}),\n              (&quot;I reached Chennai yesterday.&quot;, {&quot;entities&quot;: [(19, 28, &quot;GPE&quot;)]}),\n              (&quot;I recently ordered a book from Amazon&quot;, {&quot;entities&quot;: [(24,32, &quot;ORG&quot;)]}),\n              (&quot;I was driving a BMW&quot;, {&quot;entities&quot;: [(16,19, &quot;PRODUCT&quot;)]}),\n              (&quot;I ordered this from ShopClues&quot;, {&quot;entities&quot;: [(20,29, &quot;ORG&quot;)]}),\n              (&quot;Fridge can be ordered in Amazon &quot;, {&quot;entities&quot;: [(0,6, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a new Washer&quot;, {&quot;entities&quot;: [(16,22, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a old table&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a fancy dress&quot;, {&quot;entities&quot;: [(18,23, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a camera&quot;, {&quot;entities&quot;: [(12,18, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a tent for our trip&quot;, {&quot;entities&quot;: [(12,16, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a screwdriver from our neighbour&quot;, {&quot;entities&quot;: [(12,22, &quot;PRODUCT&quot;)]}),\n              (&quot;I repaired my computer&quot;, {&quot;entities&quot;: [(15,23, &quot;PRODUCT&quot;)]}),\n              (&quot;I got my clock fixed&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n              (&quot;I got my truck fixed&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n    ]\n    \n    dd = [    # Development (dummy) dataset (CV), in 'spacy V2 presentation'\n              (&quot;Flipkart started it's journey from zero&quot;, {&quot;entities&quot;: [(0,8, &quot;ORG&quot;)]}),\n              (&quot;I recently ordered from Max&quot;, {&quot;entities&quot;: [(24,27, &quot;ORG&quot;)]}),\n              (&quot;Flipkart is recognized as leader in market&quot;,{&quot;entities&quot;: [(0,8, &quot;ORG&quot;)]}),\n              (&quot;I recently ordered from Swiggy&quot;, {&quot;entities&quot;: [(24,29, &quot;ORG&quot;)]})\n    ]\n\n    \n    # Converting Train &amp; Development datasets, from 'spaCy V2' to 'spaCy V3'\n    nlp = spacy.blank(&quot;en&quot;)\n    db_train = DocBin()\n    db_dev = DocBin()\n\n    for text, annotations in td:\n        example = Example.from_dict(nlp.make_doc(text), annotations)\n        db_train.add(example.reference)\n        \n    for text, annotations in dd:\n        example = Example.from_dict(nlp.make_doc(text), annotations)\n        db_dev.add(example.reference)\n    \n    db_train.to_disk(train_path + &quot;.spacy&quot;)  # &lt;== Obtaining and storing &quot;train.spacy&quot;\n    db_dev.to_disk(dev_path + &quot;.spacy&quot;)      # &lt;== Obtaining and storing &quot;dev.spacy&quot;\n    \n\n# ----------------------- ORIGINALLY POSTED CODE -----------------------\n\n@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;train.yaml&quot;\n)\ndef train(train_path: InputPath(), dev_path: InputPath(), output_path: OutputPath()):\n    &quot;&quot;&quot;\n    Trains a spacy model\n    \n    Parameters:\n    ----------\n    train_path : Relative location in GCS, for the &quot;train.spacy&quot; file.\n    dev_path: Relative location in GCS, for the &quot;dev.spacy&quot; file.\n    \n    Returns:\n    -------\n    output : Destination path of the saved model.\n    &quot;&quot;&quot;\n    import spacy\n    import subprocess\n    \n    spacy.require_gpu()  # &lt;=== IMAGE NOW MANAGES TO GET BUILT!\n\n    # Presets for training\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;init&quot;, &quot;fill-config&quot;, &quot;gcs\/secret_path_to_config\/base_config.cfg&quot;, &quot;config.cfg&quot;])\n\n    # Training model\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;train&quot;, &quot;config.cfg&quot;,\n                    &quot;--output&quot;, output_path,\n                    &quot;--paths.train&quot;, &quot;{}.spacy&quot;.format(train_path),\n                    &quot;--paths.dev&quot;, &quot;{}.spacy&quot;.format(dev_path),\n                    &quot;--gpu-id&quot;, &quot;0&quot;])\n\n# ----------------------------------------------------------------------\n    \n\n# Pipeline definition\n\n@pipeline(\n    pipeline_root=PIPELINE_ROOT,\n    name=&quot;spacy-dummy-pipeline&quot;,\n)\ndef spacy_pipeline():\n    &quot;&quot;&quot;\n    Builds a custom pipeline\n    &quot;&quot;&quot;\n    # Generating dummy &quot;train.spacy&quot; + &quot;dev.spacy&quot;\n    train_dev_sets = generate_spacy_file()\n    # With the output of the previous component, train a spaCy modeL    \n    model = train(\n        train_dev_sets.outputs[&quot;train_path&quot;],\n        train_dev_sets.outputs[&quot;dev_path&quot;]\n    \n    # ------ !!! THIS SECTION DOES THE TRICK !!! ------\n    ).add_node_selector_constraint(\n        label_name=&quot;cloud.google.com\/gke-accelerator&quot;,\n        value=&quot;NVIDIA_TESLA_T4&quot;\n    ).set_gpu_limit(1).set_memory_limit('32G')\n    # -------------------------------------------------\n\n# Pipeline compilation   \n\ncompiler.Compiler().compile(\n    pipeline_func=spacy_pipeline, package_path=&quot;pipeline_spacy_job.json&quot;\n)\n\n\n# Pipeline run\n\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n\nrun = aiplatform.PipelineJob(  # Include your own naming here\n    display_name=&quot;spacy-dummy-pipeline&quot;,\n    template_path=&quot;pipeline_spacy_job.json&quot;,\n    job_id=&quot;ml-pipeline-spacydummy-small-{0}&quot;.format(TIMESTAMP),\n    parameter_values={},\n    enable_caching=True,\n)\n\n\n# Pipeline gets submitted\n\nrun.submit()\n<\/code><\/pre>\n<p>Now, the explanation; according to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/machine-types\" rel=\"nofollow noreferrer\">Google<\/a>:<\/p>\n<blockquote>\n<p>By default, the component will run on as a Vertex AI CustomJob using an e2-standard-4 machine, with 4 core CPUs and 16GB memory.<\/p>\n<\/blockquote>\n<p>Therefore, when the <code>train<\/code> component gets compiled, it fails as &quot;<em>it was not seeing any GPU available as resource<\/em>&quot;; in the same link however, all the available settings for both CPU and GPU are mentioned. In my case as you can see, I set <code>train<\/code> component to run under ONE (1) <code>NVIDIA_TESLA_T4<\/code> GPU card, and I also increased my CPU memory, to 32GB. With these modifications, the resulting pipeline looks as follows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s0I31.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s0I31.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>And as you can see, it gets compiled successfully, as well as trains (and eventually obtains) a functional spaCy model. From here, you can tweak this code, to fit your own needs.<\/p>\n<p>I hope this helps to anyone who might be interested.<\/p>\n<p>Thank you.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1651093924243,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72014493",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68795098,
        "Question_title":"Error when trying to use CustomPythonPackageTrainingJobRunOp in VertexAI pipeline",
        "Question_body":"<p>I am using the google cloud pipeline component CustomPythonPackageTrainingJobRunOp in a VertexAI pipeline . I have been able to run this package successfully as a CustomTrainingJob before. I can see multiple (11) error messages in the logs but the only one that seems to make sense to me is, &quot;ValueError: too many values to unpack (expected 2) &quot; but I am unable to figure out the solution. I can add all the other error messages too if required. I am logging some messages at the start of the training code so I know the errors happen before the training code is executed. I am completely stuck on this. Links to samples where someone has used CustomPythonPackageTrainingJobRunOp in a pipeline would very helpful as well. Below is the pipeline code that I am trying to execute:<\/p>\n<pre><code>import kfp\nfrom kfp.v2 import compiler\nfrom kfp.v2.google.client import AIPlatformClient\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\n\n@kfp.dsl.pipeline(name=pipeline_name)\ndef pipeline(\n    project: str = &quot;adsfafs-321118&quot;,\n    location: str = &quot;us-central1&quot;,\n    display_name: str = &quot;vertex_pipeline&quot;,\n    python_package_gcs_uri: str = &quot;gs:\/\/vertex\/training\/training-package-3.0.tar.gz&quot;,\n    python_module_name: str = &quot;trainer.task&quot;,\n    container_uri: str = &quot;us-docker.pkg.dev\/vertex-ai\/training\/scikit-learn-cpu.0-23:latest&quot;,\n    staging_bucket: str = &quot;vertex_bucket&quot;,\n    base_output_dir: str = &quot;gs:\/\/vertex_artifacts\/custom_training\/&quot;\n):\n    \n    gcc_aip.CustomPythonPackageTrainingJobRunOp(\n        display_name=display_name,\n        python_package_gcs_uri=python_package_gcs_uri,\n        python_module=python_module_name,\n        container_uri=container_uri,\n        project=project,\n        location=location,\n        staging_bucket=staging_bucket,\n        base_output_dir=base_output_dir,\n        args = [&quot;--arg1=val1&quot;, &quot;--arg2=val2&quot;, ...]\n    )\n\n\n\ncompiler.Compiler().compile(\n    pipeline_func=pipeline, package_path=package_path\n)\n\napi_client = AIPlatformClient(project_id=project_id, region=region)\n\nresponse = api_client.create_run_from_job_spec(\n    package_path,\n    pipeline_root=pipeline_root_path\n)\n\n<\/code><\/pre>\n<p>In the documentation for CustomPythonPackageTrainingJobRunOp, the type of the argument &quot;python_module&quot; seems to be &quot;google.cloud.aiplatform.training_jobs.CustomPythonPackageTrainingJob&quot; instead of string, which seems odd. However, I tried to re-define the pipeline, where I have replaced argument python_module in CustomPythonPackageTrainingJobRunOp with a CustomPythonPackageTrainingJob object instead of a string, as below but still getting the same error:<\/p>\n<pre><code>def pipeline(\n    project: str = &quot;...&quot;,\n    location: str = &quot;...&quot;,\n    display_name: str = &quot;...&quot;,\n    python_package_gcs_uri: str = &quot;...&quot;,\n    python_module_name: str = &quot;...&quot;,\n    container_uri: str = &quot;...&quot;,\n    staging_bucket: str = &quot;...&quot;,\n    base_output_dir: str = &quot;...&quot;,\n):\n\n    job = aiplatform.CustomPythonPackageTrainingJob(\n        display_name= display_name,\n        python_package_gcs_uri=python_package_gcs_uri,\n        python_module_name=python_module_name,\n        container_uri=container_uri,\n        staging_bucket=staging_bucket\n    )\n    \n    gcc_aip.CustomPythonPackageTrainingJobRunOp(\n        display_name=display_name,\n        python_package_gcs_uri=python_package_gcs_uri,\n        python_module=job,\n        container_uri=container_uri,\n        project=project,\n        location=location,\n        base_output_dir=base_output_dir,\n        args = [&quot;--arg1=val1&quot;, &quot;--arg2=val2&quot;, ...]\n    )\n<\/code><\/pre>\n<p><strong>Edit:<\/strong><\/p>\n<p>Added the args that I was passing and had forgotten to add here.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629058946030,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|kubeflow-pipelines|google-cloud-ai|google-cloud-ai-platform-pipelines|google-cloud-vertex-ai",
        "Question_view_count":259,
        "Owner_creation_time":1471292986790,
        "Owner_last_access_time":1664058170413,
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Question_last_edit_time":1629151780472,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68795098",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73210389,
        "Question_title":"Vertex AI Executor gives NoSuchKernel",
        "Question_body":"<p>I have a simple hello-world ipynb file in a Vertex AI notebook instance that looks like this:<\/p>\n<pre><code>print(&quot;hello world&quot;)\n<\/code><\/pre>\n<p>When setting up an executor for this as shown below I receive the following error in the executor logs: <em>jupyter_client.kernelspec.NoSuchKernel: No such kernel named local-python3\nerror<\/em>\n<a href=\"https:\/\/i.stack.imgur.com\/NcuKk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NcuKk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The notebook has the following metadata<\/p>\n<pre><code>{\n    &quot;kernelspec&quot;: {\n        &quot;display_name&quot;: &quot;Python 3 (Local)&quot;,\n        &quot;language&quot;: &quot;python&quot;,\n        &quot;name&quot;: &quot;local-python3&quot;\n    },\n    &quot;language_info&quot;: {\n        &quot;codemirror_mode&quot;: {\n            &quot;name&quot;: &quot;ipython&quot;,\n            &quot;version&quot;: 3\n        },\n        &quot;file_extension&quot;: &quot;.py&quot;,\n        &quot;mimetype&quot;: &quot;text\/x-python&quot;,\n        &quot;name&quot;: &quot;python&quot;,\n        &quot;nbconvert_exporter&quot;: &quot;python&quot;,\n        &quot;pygments_lexer&quot;: &quot;ipython3&quot;,\n        &quot;version&quot;: &quot;3.7.12&quot;\n    }\n}\n<\/code><\/pre>\n<p>What would require to run this notebook successfully? I looked into the possibility of customer containers but that should be to much of a complex solution for such.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1659455503420,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":47,
        "Owner_creation_time":1587457219480,
        "Owner_last_access_time":1663764469243,
        "Owner_location":null,
        "Owner_reputation":25,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73210389",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73542706,
        "Question_title":"How to solve VertexAI prediction endpoint error?",
        "Question_body":"<p>I am trying to get predictions from an endpoint that is already created in VertexAI through UI.<\/p>\n<p>I am getting an error when I run the following code<\/p>\n<pre><code>from typing import Dict\nimport pandas as pd\nfrom google.cloud import aiplatform\nfrom google.protobuf import json_format\nfrom google.protobuf.struct_pb2 import Value\nfrom google.oauth2 import service_account\n\nkey_path = '..\/golden-tempest-xxxxx.json'\ncredentials = service_account.Credentials.from_service_account_file(key_path, scopes=[\n    &quot;https:\/\/www.googleapis.com\/auth\/cloud-platform&quot;], )\n\n\naiplatform.init(\n    project='golden-tempest-xxxxx',\n    location='us-central1',\n    credentials=credentials,\n)\n\ndef predict_tabular_classification_sample(\n    project: str,\n    endpoint_id: str,\n    instance_dict: Dict,\n    location: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n):\n    client_options = {&quot;api_endpoint&quot;: api_endpoint}\n    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n    instance = json_format.ParseDict(instance_dict, Value())\n    instances = [instance]\n    parameters_dict = {}\n    parameters = json_format.ParseDict(parameters_dict, Value())\n    endpoint = client.endpoint_path(\n        project=project, location=location, endpoint=endpoint_id\n    )\n    response = client.predict(\n        endpoint=endpoint, instances=instances, parameters=parameters\n    )\n    print(&quot;response&quot;)\n    print(&quot; deployed_model_id:&quot;, response.deployed_model_id)\n    # See gs:\/\/google-cloud-aiplatform\/schema\/predict\/prediction\/tabular_classification_1.0.0.yaml for the format of the predictions.\n    predictions = response.predictions\n    for prediction in predictions:\n        print(&quot; prediction:&quot;, dict(prediction))\n\n\ndf = pd.read_csv('..\/btc_test_classification_2.csv')\n\ndf_dict = df.to_dict('index')\n\npredict_tabular_classification_sample(\n    project=&quot;xxxxx&quot;,\n    endpoint_id=&quot;886215168080478208&quot;,\n    location=&quot;us-central1&quot;,\n    instance_dict={'instances': [df_dict[4]]}\n)\n<\/code><\/pre>\n<p><strong>Error<\/strong>:<\/p>\n<pre><code>InvalidArgument: 400 {&quot;error&quot;: &quot;Column prefix: . Error: Missing struct property: tick_count.&quot;}\n<\/code><\/pre>\n<p>My training data contains the same columns I have in test. <a href=\"https:\/\/i.stack.imgur.com\/zEQ1c.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zEQ1c.png\" alt=\"columns\" \/><\/a><\/p>\n<p>I am not sure why I am getting this error.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1661863594927,
        "Question_score":0,
        "Question_tags":"google-api|google-cloud-vertex-ai",
        "Question_view_count":54,
        "Owner_creation_time":1426332042323,
        "Owner_last_access_time":1663922259243,
        "Owner_location":null,
        "Owner_reputation":1026,
        "Owner_up_votes":236,
        "Owner_down_votes":1,
        "Owner_views":256,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73542706",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70159582,
        "Question_title":"Google Cloud Platform - Vertex AI Image classification training fails with no specific error message",
        "Question_body":"<p>I'm doing an image classification task using Vertex AI and after about 3 hours of training it fails. The error message is nondescript. It says &quot;Training pipeline failed with error message: Internal error occurred. Please retry in a few minutes. If you still experience errors, contact Vertex AI.&quot;<\/p>\n<p>It's done that for three of my models using the same image dataset (about 45k large). What could be the error here? How can I find out?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1638210471447,
        "Question_score":0,
        "Question_tags":"machine-learning|google-cloud-platform|computer-vision|google-cloud-vertex-ai",
        "Question_view_count":192,
        "Owner_creation_time":1392762853817,
        "Owner_last_access_time":1663862388447,
        "Owner_location":null,
        "Owner_reputation":955,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":94,
        "Question_last_edit_time":1638223011489,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70159582",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68532457,
        "Question_title":"How to create a Logs Router Sink when a Vertex AI training job failed (after 3 attempts)?",
        "Question_body":"<p>I am running a <code>Vertex AI custom training job<\/code> (machine learnin training using custom container) on <code>GCP<\/code>. I would like to create a <code>Pub\/Sub<\/code> message when the job failed so I can post a message on some chat like Slack. Logfile (<code>Cloud Logging)<\/code> is looking like that:<\/p>\n<pre><code>{\ninsertId: &quot;xxxxx&quot;\nlabels: {\nml.googleapis.com\/endpoint: &quot;&quot;\nml.googleapis.com\/job_state: &quot;FAILED&quot;\n}\nlogName: &quot;projects\/xxx\/logs\/ml.googleapis.com%2F1113875647681265664&quot;\nreceiveTimestamp: &quot;2021-07-09T15:05:52.702295640Z&quot;\nresource: {\nlabels: {\njob_id: &quot;1113875647681265664&quot;\nproject_id: &quot;xxx&quot;\ntask_name: &quot;service&quot;\n}\ntype: &quot;ml_job&quot;\n}\nseverity: &quot;INFO&quot;\ntextPayload: &quot;Job failed.&quot;\ntimestamp: &quot;2021-07-09T15:05:52.187968162Z&quot;\n}\n<\/code><\/pre>\n<p>I am creating a Logs Router Sink with the following query:<\/p>\n<pre><code>resource.type=&quot;ml_job&quot; AND textPayload:&quot;Job failed&quot; AND labels.&quot;ml.googleapis.com\/job_state&quot;:&quot;FAILED&quot;\n<\/code><\/pre>\n<p>The issue I am facing is that Vertex AI will retry the job 3 times before declaring the job as a failure but in the logfile the message is identical. Below you have 3 examples, only the last one that failed 3 times really failed at the end.\n<a href=\"https:\/\/i.stack.imgur.com\/NSGPb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NSGPb.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>In the logfile, I don't have any count id for example. Any idea how to solve this ? Creating a BigQuery table to keep track of the number of failure per <code>resource.labels.job_id<\/code> seems to be an overkill if I need to do that in all my project. Is there a way to do a group by <code>resource.labels.job_id<\/code> and count within Logs Router Sink ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1627312838200,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-logging|google-cloud-vertex-ai",
        "Question_view_count":216,
        "Owner_creation_time":1465222092253,
        "Owner_last_access_time":1663858783617,
        "Owner_location":"Z\u00fcrich, Switzerland",
        "Owner_reputation":1414,
        "Owner_up_votes":258,
        "Owner_down_votes":3,
        "Owner_views":478,
        "Question_last_edit_time":1627313437400,
        "Answer_body":"<p>The log sink is quite simple: provide a filter, it will publish in a PubSub topic each entry which match this filter. No group by, no count, nothing!!<\/p>\n<p>I propose you to use a combination of log-based metrics and Cloud monitoring.<\/p>\n<ol>\n<li>Firstly, create a <a href=\"https:\/\/cloud.google.com\/logging\/docs\/logs-based-metrics\" rel=\"nofollow noreferrer\">log based metrics<\/a> on your job failed log entry<\/li>\n<li>Create an <a href=\"https:\/\/cloud.google.com\/logging\/docs\/logs-based-metrics\/charts-and-alerts\" rel=\"nofollow noreferrer\">alert on this log based metrics<\/a> with the following key values<\/li>\n<\/ol>\n<ul>\n<li>Set the group by that you want, for example, the jobID (i don't know what is the relevant value for VertexAI job)<\/li>\n<li>Set an alert when the threshold is equal or above 3<\/li>\n<li>Add a notification channel and set a PubSub notification (still in beta)<\/li>\n<\/ul>\n<p>With this configuration, the alert will be posted only once in PubSub when 3 occurrences of the same jobID will occur.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1627331282972,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68532457",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73816577,
        "Question_title":"How do I ingest float from BigQuery into featurestore?",
        "Question_body":"<p>I have a <code>float<\/code> type field in BQ. I try to ingest this into a <code>double<\/code> type field in featurestore but get:<\/p>\n<pre><code>Source schema does not match the expected schema for this import. \nMissing fields in source: feature_field. Type mismatches in source: \nExpected type and mode [STRING, NULLABLE] for BQ_field, but got [FLOAT, ].\n<\/code><\/pre>\n<p>How can I import it?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663857615367,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":7,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73816577",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73362430,
        "Question_title":"Vertex AI executor with .env file",
        "Question_body":"<p>Normally when using environment variables in Docker you would specify them with ENV command in the Dockerfile or give the .env file with the --env-file option in a docker run command.<\/p>\n<p>When creating a Vertex AI notebook executor this latter option is not available while the first option requires maintenance of the Dockerfile when an env variable changes.<br \/>\nWe can specify the env variables as parameters in a .yaml when creating the executor, but that requires again an extra step editing in the process when a env variable changes.<\/p>\n<p>How can we pass the content of the .env file to the container at runtime with no extra effort in Vertex AI?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1660573840647,
        "Question_score":1,
        "Question_tags":"docker|environment-variables|google-cloud-vertex-ai",
        "Question_view_count":33,
        "Owner_creation_time":1587457219480,
        "Owner_last_access_time":1663764469243,
        "Owner_location":null,
        "Owner_reputation":25,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73362430",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69203143,
        "Question_title":"Using Tesla A100 GPU with Kubeflow Pipelines on Vertex AI",
        "Question_body":"<p>I'm using the following lines of code to specify the desired machine type and accelerator\/GPU on a Kubeflow Pipeline (KFP) that I will be running on a serverless manner through Vertex AI\/Pipelines.<\/p>\n<pre><code>op().\nset_cpu_limit(8).\nset_memory_limit(50G).\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-k80').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>and it works for other GPUs as well i.e. Pascal, Tesla, Volta cards.<\/p>\n<p>However, I can't do the same with the latest accelerator type which is the <code>Tesla A100<\/code> as it requires a special machine type, which is as least an <code>a2-highgpu-1g<\/code>.<\/p>\n<p>How do I make sure that this particular component will run on top of <code>a2-highgpu-1g<\/code> when I run it on Vertex?<\/p>\n<p>If i simply follow the method for older GPUs:<\/p>\n<pre><code>op().\nset_cpu_limit(12). # max for A2-highgpu-1g\nset_memory_limit(85G). # max for A2-highgpu-1g\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>It throws an error when run\/deployed since the machine type that is being spawned is the general type i.e. N1-Highmem-*<\/p>\n<p>Same thing happened when I did not specify the cpu and memory limits, in hope that it will automatically select the right machnie type based on the accelerator constraint.<\/p>\n<pre><code>    op().\n    add_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\n    set_gpu_limit(1)\n<\/code><\/pre>\n<p>Error:\n<code>&quot;NVIDIA_TESLA_A100&quot; is not supported for machine type &quot;n1-highmem-2&quot;,<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1631772554497,
        "Question_score":1,
        "Question_tags":"google-kubernetes-engine|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":473,
        "Owner_creation_time":1449207605093,
        "Owner_last_access_time":1663813482530,
        "Owner_location":"Manila, NCR, Philippines",
        "Owner_reputation":185,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Currently, GCP don't support A2 Machine type for normal KF Components. A potential workaround right now is to use <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-job\" rel=\"nofollow noreferrer\"><strong>GCP custom job component<\/strong><\/a> that you can explicitly specify the machine type.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1632103988632,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69203143",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68280940,
        "Question_title":"vertex ai: ResourceExhausted 429 received trailing metadata size exceeds limit",
        "Question_body":"<p>I am using google vertex AI online prediction:\nIn order to send an image it has to be in a JSON file in unit8 format which has to be less than 1.5 MB, when converting my image to uint8 it definitely exceeds 1.5MB.<\/p>\n<p>To go around this issue we can encode the unit8 file to b64, that makes the JSON file in KBs\nwhen running the prediction I get <code>Resource Exhausted: 429 received trailing metadata size exceeds limit<\/code>  Is there anyone who knows what's the problem?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1625638603360,
        "Question_score":0,
        "Question_tags":"google-cloud-ml|google-ai-platform|google-cloud-ai-platform-pipelines|google-cloud-vertex-ai",
        "Question_view_count":875,
        "Owner_creation_time":1551350379477,
        "Owner_last_access_time":1663847801140,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":1625646472767,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68280940",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69977440,
        "Question_title":"How to use kfp Artifact with sklearn?",
        "Question_body":"<p>I'm trying to develop a custom pipeline with kubeflow pipeline (kfp) components inside Vertex AI (Google Cloud Platform). The steps of the pipeline are:<\/p>\n<ol>\n<li>read data from a big query table<\/li>\n<li>create a pandas <code>DataFrame<\/code><\/li>\n<li>use the <code>DataFrame<\/code> to train a K-Means model<\/li>\n<li>deploy the model to an endpoint<\/li>\n<\/ol>\n<p>Here there is the code of the step 2. I had to use <code>Output[Artifact]<\/code> as output because <code>pd.DataFrame<\/code> type that I found <a href=\"https:\/\/stackoverflow.com\/questions\/43890844\/pythonic-type-hints-with-pandas\">here<\/a> did not work.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(base_image=&quot;python:3.9&quot;, packages_to_install=[&quot;google-cloud-bigquery&quot;,&quot;pandas&quot;,&quot;pyarrow&quot;])\ndef create_dataframe(\n    project: str,\n    region: str,\n    destination_dataset: str,\n    destination_table_name: str,\n    df: Output[Artifact],\n):\n    \n    from google.cloud import bigquery\n    \n    client = bigquery.Client(project=project, location=region)\n    dataset_ref = bigquery.DatasetReference(project, destination_dataset)\n    table_ref = dataset_ref.table(destination_table_name)\n    table = client.get_table(table_ref)\n\n    df = client.list_rows(table).to_dataframe()\n<\/code><\/pre>\n<p>Here the code of the step 3:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(base_image=&quot;python:3.9&quot;, packages_to_install=['sklearn'])\ndef kmeans_training(\n        dataset: Input[Artifact],\n        model: Output[Model],\n        num_clusters: int,\n):\n    from sklearn.cluster import KMeans\n    model = KMeans(num_clusters, random_state=220417)\n    model.fit(dataset)\n<\/code><\/pre>\n<p>The run of the pipeline is stopped due to the following error:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>TypeError: float() argument must be a string or a number, not 'Artifact'\n<\/code><\/pre>\n<p>Is it possible to convert Artifact to <code>numpy array<\/code> or <code>Dataframe<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636992603783,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|kfp",
        "Question_view_count":339,
        "Owner_creation_time":1616589293617,
        "Owner_last_access_time":1663860372103,
        "Owner_location":"Alatri, Frosinone, FR",
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69977440",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72881056,
        "Question_title":"Why does my dbt container hang in Vertex AI?",
        "Question_body":"<p>I am trying to follow <a href=\"https:\/\/datatonic.com\/insights\/dbt-vertex-ai-pipelines-google-cloud\/\" rel=\"nofollow noreferrer\">this<\/a> tutorial to run a dbt docker image as a Vertex AI component. When the pipeline runs the component just seems to sit there for ever. Is there any way of debugging the component?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657099839053,
        "Question_score":0,
        "Question_tags":"docker|dbt|google-cloud-vertex-ai|kfp",
        "Question_view_count":73,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You see the pipeline logs to get an idea regarding what is going on in the pipeline.\nFrom the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/logging\" rel=\"nofollow noreferrer\">doc<\/a><\/p>\n<blockquote>\n<p>After you define and build a pipeline, you can use Cloud Logging to create log entries to help you monitor events such as pipeline failures. You can create custom log-based metrics that send notifications when the rate of pipeline failures reaches a given threshold.<\/p>\n<\/blockquote>\n<p>You can also select a component inside Pipeline's runtime graph and then view detailed info and logs of that particular component.\n<a href=\"https:\/\/i.stack.imgur.com\/QXm02.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QXm02.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Also you can hover your cursor on the component status area(green check or grey disabled icon) to check the current status of that component.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1657175196140,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1657175516072,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72881056",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73113256,
        "Question_title":"Hyperparameter data types and scales not being validated",
        "Question_body":"<p>On past week, I was implementing some code to <a href=\"https:\/\/github.com\/explosion\/spaCy\/discussions\/11126#discussioncomment-3191163\" rel=\"nofollow noreferrer\">tune hyperparameters on a spaCy model, using Vertex AI<\/a>. From that experience, I have several questions, but since they might no be directly related to each other, I decided to open one case per each question.<\/p>\n<p>In this case, I would like to understand what is exactly going on, when I set the following hyperparameters, in some HP tuning job:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/w4C78.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/w4C78.png\" alt=\"hyperparameters\" \/><\/a><\/p>\n<p>Notice <strong>both examples have been purposedly written 'wrongly' to trigger an error but 'eerily', they don't<\/strong> (UPDATE: at least with my current understanding of the docs). I have the sensation that <em>&quot;Vertex AI does not make any validation of the inserted values, they just run whatever you write, and trigger an error only if the values don't actually make ANY sense&quot;<\/em>. Allow me to insert a couple of comments on each example:<\/p>\n<ul>\n<li><code>dropout<\/code>: This variable should be <em>&quot;scaled linearly between 0 and 1&quot;<\/em> ... However what I can see in the HP tuning jobs, are values <em>&quot;scaled linearly between 0.1 and 0.3, and nothing in the interval 0.3 to 0.5&quot;<\/em>. Now this reasoning is a bit naive, as I am not 100% sure if <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization\" rel=\"nofollow noreferrer\">this algorithm<\/a> had to do in the values selection, or <em>&quot;Google Console understood I only had the interval [0.1,0.3] to choose values from&quot;<\/em>. (UPDATE) Plus, how can a variable be &quot;discrete and linear&quot; at the same time?<\/li>\n<li><code>batch_size<\/code>: I think I know what's going on with this one, I just want to confirm: 3 categorical values (&quot;500&quot;, &quot;1000&quot; &amp; &quot;2000&quot;) are being selected &quot;as they are&quot;, since they have a SHP of &quot;UNESPECIFIED&quot;.<\/li>\n<\/ul>\n<p>(*) Notice both the HP names, as well as their values, were just &quot;examples on the spot&quot;, they don't intend to be &quot;good starting points&quot;. HP tuning initial values selection is NOT the point of this query.<\/p>\n<p>Thank you.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":7,
        "Question_creation_time":1658770653850,
        "Question_score":0,
        "Question_tags":"python-3.x|hyperparameters|google-cloud-vertex-ai|spacy-3",
        "Question_view_count":133,
        "Owner_creation_time":1629385138957,
        "Owner_last_access_time":1663953209400,
        "Owner_location":null,
        "Owner_reputation":395,
        "Owner_up_votes":18,
        "Owner_down_votes":4,
        "Owner_views":38,
        "Question_last_edit_time":1660741967556,
        "Answer_body":"<p>If the type is Categorical, then the scale type is irrelevant and ignored. If the type is DoubleValueSpec, IntegerValueSpec, or DiscreteValueSpec, then the scale type will govern which values are picked more often.<\/p>\n<p>Regarding how a variable can be both Discrete and have a scale: Discrete variables are still numeric in nature. For example, if the discrete values are <code>[1, 10, 100]<\/code>, the ScaleType will determine whether the optimization algorithm considers &quot;distance&quot; between 1 and 10 versus 10 and 100 the same (if logarithmic) or smaller (if linear).<\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_time":1660669810152,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1660850228732,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73113256",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71137452,
        "Question_title":"control over Vertex AI annotation",
        "Question_body":"<p>In the Google Vertex AI platform, the configuration of labelling the data set is decided by the vertex AI itself, ie. for example the color which is given to the bounding box while labelling is decided by vertex ai, what if I want to assign the color as my preference? I couldn't see any option to change the annotation color and also tried to feed externally labelled files to the vertex ai and it shows an error as there is no field named color;<\/p>\n<pre><code>{\n  &quot;imageGcsUri&quot;: &quot;gs:\/\/cloud-ai-platform-5100f6e6-d2e6-4966-869e-ba22a09ef85a\/bg_MAX_0002.JPG&quot;,\n  &quot;boundingBoxAnnotations&quot;: [\n    {\n      &quot;displayName&quot;: &quot;abc&quot;,\n      &quot;xMin&quot;: 0.07510431154381085,\n      &quot;xMax&quot;: 0.34492350486787204,\n      &quot;yMin&quot;: 0.1022964509394572,\n      &quot;yMax&quot;: 0.4384133611691023,\n      &quot;annotationResourceLabels&quot;: {\n        &quot;aiplatform.googleapis.com\/annotation_set_name&quot;: &quot;3765893295830466560&quot;\n      }\n    },\n    {\n      &quot;displayName&quot;: &quot;pqr&quot;,\n      &quot;xMin&quot;: 0.6801112656467315,\n      &quot;xMax&quot;: 0.9318497913769124,\n      &quot;yMin&quot;: 0.1503131524008351,\n      &quot;yMax&quot;: 0.6367432150313153,\n      &quot;annotationResourceLabels&quot;: {\n        &quot;aiplatform.googleapis.com\/annotation_set_name&quot;: &quot;3765893295830466560&quot;\n      }\n    },\n    {\n      &quot;displayName&quot;: &quot;xyz&quot;,\n      &quot;xMin&quot;: 0.15438108484005564,\n      &quot;xMax&quot;: 0.6620305980528511,\n      &quot;yMin&quot;: 0.605427974947808,\n      &quot;yMax&quot;: 0.906054279749478,\n      &quot;annotationResourceLabels&quot;: {\n        &quot;aiplatform.googleapis.com\/annotation_set_name&quot;: &quot;3765893295830466560&quot;\n      }\n    }\n  ],\n  &quot;dataItemResourceLabels&quot;: {\n    \n  }\n}\n<\/code><\/pre>\n<p>Given above is an example label generated by vertex ai and it doesn't contain any color information even though I've labelled in multiple colors.\nSo my question is how can we get control over the color feature of the labelling system in vertex AI?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1644994320227,
        "Question_score":0,
        "Question_tags":"machine-learning|google-cloud-platform|annotations|artificial-intelligence|google-cloud-vertex-ai",
        "Question_view_count":173,
        "Owner_creation_time":1626179648183,
        "Owner_last_access_time":1646630602500,
        "Owner_location":"India",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1657278784876,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71137452",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72378701,
        "Question_title":"How do I set overcommit_memory on Google Cloud notebook?",
        "Question_body":"<p>I am using a GCP Vertex managed notebook and I get a memory error which I think can be fixed by:<\/p>\n<pre><code>echo 1 &gt; \/proc\/sys\/vm\/overcommit_memory\n<\/code><\/pre>\n<p>but when I run this from a Jupyterlab terminal I am asked for a sudo password, which I do not know. What can I do?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1653485896420,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|memory-management|jupyter-lab|google-cloud-vertex-ai",
        "Question_view_count":76,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72378701",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70477987,
        "Question_title":"Vertex Ai issue when deploying a model using Java",
        "Question_body":"<p>This is what I use to deploy an Auto-ML model:<\/p>\n<pre><code>            MachineSpec machineSpec = MachineSpec.newBuilder().setMachineType(&quot;n1-standard-2&quot;).build();\n            DedicatedResources dedicatedResources =\n                    DedicatedResources.newBuilder().setMinReplicaCount(1).setMachineSpec(machineSpec).build();            \n            String model = ModelName.of(project, location, modelId).toString();\n            DeployedModel deployedModel =\n                    DeployedModel.newBuilder()\n                            .setModel(model)\n                            .setDisplayName(deployedModelDisplayName)\n                            .setDedicatedResources(dedicatedResources)\n                            .build();\n            Map&lt;String, Integer&gt; trafficSplit = new HashMap&lt;&gt;();\n            trafficSplit.put(&quot;0&quot;, 100);\n            EndpointName endpoint = EndpointName.of(project, location, endpointId);\n            OperationFuture&lt;DeployModelResponse, DeployModelOperationMetadata&gt; response =\n                    client.deployModelAsync(endpoint, deployedModel, trafficSplit);\n            response.getInitialFuture().get().getName());\n<\/code><\/pre>\n<p>The error appears when I hit this line <code>response.getInitialFuture().get().getName());<\/code><\/p>\n<p>Here is the error:\n<code>INVALID_ARGUMENT: 'dedicated_resources' is not supported for Model projects\/***\/locations\/us-central1\/models\/***<\/code><\/p>\n<p>I can deploy the model using cloud console but not programmatically using java 8. It is a new model and the endpoint is also new without any assigned model to it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_time":1640406502527,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":244,
        "Owner_creation_time":1369787017730,
        "Owner_last_access_time":1663945541287,
        "Owner_location":"Atlanta, Georgia",
        "Owner_reputation":55,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I am sorry everyone, I was implementing the wrong section of the documentation. I had to follow AutoML image, not Custom-trained one.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/a4xPR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/a4xPR.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1640820603630,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70477987",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72783902,
        "Question_title":"Is it possible to run Vertex AI Workbench on Spot machines?",
        "Question_body":"<p>I'm trying to save budget on jupyter notebooks on Google Cloud but couldn't find a way to run Vertex AI Workbench (Notebooks) on spot machines.\nWhat are my alternatives?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656408332770,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|jupyter-notebook|jupyter|google-cloud-vertex-ai",
        "Question_view_count":176,
        "Owner_creation_time":1322253579120,
        "Owner_last_access_time":1661153607010,
        "Owner_location":null,
        "Owner_reputation":73,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72783902",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70205432,
        "Question_title":"VSCode cannot see packages on a GCP VM",
        "Question_body":"<p>I have an issue where vscode when connected to a VM on GCP cannot see packages installed in <code>\/opt\/conda\/lib\/python3.7\/site-packages.<\/code> I have created the VM using Vertex AI. When I open the jupyter notebook through the UI in a the browser I can see all the packages via <code>pip3 list<\/code>. But when I am connected to the VM through SSH in vscode I cannot see the packages installed such as nltk, spacy etc. and when I try to load it gives me <code>ModuleNotFoundError<\/code>. This error does not show up when I use the jupyter notebook from the Vertex AI UI. The site-packages folder is in my system path and the python that I am using is <code>\/opt\/conda\/bin\/python3<\/code>.<\/p>\n<p>Any help is appreciated. Please do let me know if my question is clear.<\/p>\n<p>EDIT: I figured out that my packages are running on a container in the VM. Is there a way for me to access those packages via jupyter notebook in vscode?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1638473538253,
        "Question_score":0,
        "Question_tags":"visual-studio-code|google-cloud-platform|ssh|google-compute-engine|google-cloud-vertex-ai",
        "Question_view_count":230,
        "Owner_creation_time":1580840045043,
        "Owner_last_access_time":1661885424787,
        "Owner_location":null,
        "Owner_reputation":85,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":1638550660460,
        "Answer_body":"<p>Posting the answer as community wiki. As confirmed by @Abhishek, he was able to make it work by installing a docker extension on the VM then attach VS code to the container.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1638843453689,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70205432",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69925931,
        "Question_title":"Vertex AI model batch prediction failed with internal error",
        "Question_body":"<p>I have trained the AutoMl classification model on Vertex AI, unfortunately model does not work with batch predictions, whenever I try to score training dataset (same which was used for the successful model training) with batch predictions on Vertex AI I get a following error:<\/p>\n<p>&quot;Due to one or more errors, this training job was canceled on Nov 11, 2021 at 09:42AM&quot;.<\/p>\n<p>There is an option to get a details from this error and those say the following thing:<\/p>\n<p>&quot;Batch prediction job customer_value_label_cv_automl_gui encountered the following errors: INTERNAL&quot;<\/p>\n<p>Does anyone know what might be the reason for getting this kind of error? I am very surprised that the model cannot score the dataset that it was trained on. My dataset consists of 570 columns and about 300k of records. <a href=\"https:\/\/i.stack.imgur.com\/DRbjn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DRbjn.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/0MHrg.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0MHrg.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1636622783290,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-ml|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":712,
        "Owner_creation_time":1551797759387,
        "Owner_last_access_time":1648564097343,
        "Owner_location":"Krak\u00f3w, Poland",
        "Owner_reputation":77,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1636706508710,
        "Answer_body":"<p>We have been able to finally figure this out. As we were using model.batch_predict method described in the <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform.html\" rel=\"nofollow noreferrer\">official documentation<\/a> we unnecessary set the machine_type parameter. Finally, we were able to figure out that it was causing the issue, the machine was probably too weak. Once we removed this declaration this method started to use automatic resources and that solved the case. I wish Vertex AI errors were a little bit more informative because it took us a lot of trials and error to figure out.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1637235864267,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69925931",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69857932,
        "Question_title":"Specify signature name on Vertex AI Predict",
        "Question_body":"<p>I've deployed a tensorflow model in vertex AI platform using TFX Pipelines. The model have custom serving signatures but I'm strugling to specify the signature when I'm predicting.<\/p>\n<p>I've the exact same model deployed in GCP AI Platform and I'm able to specify it.<\/p>\n<p>According to the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-custom-models?authuser=5&amp;_ga=2.16305585.-680038964.1635267137#formatting-prediction-input\" rel=\"nofollow noreferrer\">vertex documentation<\/a>, we must pass a dictionary containing the Instances (List) and the Parameters (Dict) values.<\/p>\n<p>I've submitted these arguments to <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predict_custom_trained_model_sample.py\" rel=\"nofollow noreferrer\">this function<\/a>:<\/p>\n<pre><code>instances: [{&quot;argument_n&quot;: &quot;value&quot;}]\n\nparameters: {&quot;signature_name&quot;: &quot;name_of_signature&quot;}\n<\/code><\/pre>\n<p>Doesn't work, it still get the default signature of the model.<\/p>\n<p>In GCP AI Platform, I've been able to predict directly specifying in the body of the request the signature name:<\/p>\n<pre><code>response = service.projects().predict(\n        name=name,\n        body={&quot;instances&quot;: instances,\n        &quot;signature_name&quot;: &quot;name_of_signature&quot;},\n    ).execute()\n<\/code><\/pre>\n<p>@EDIT\nI've discovered that with the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.endpoints\/rawPredict\" rel=\"nofollow noreferrer\">rawPredict method<\/a> from gcloud it works.<\/p>\n<p>Here is an example:<\/p>\n<pre><code>!gcloud ai endpoints raw-predict {endpoint} --region=us-central1 \\\n--request='{&quot;signature_name&quot;:&quot;name_of_the_signature&quot;, \\\n&quot;instances&quot;: [{&quot;instance_0&quot;: [&quot;value_0&quot;], &quot;instance_1&quot;: [&quot;value_1&quot;]}]}'\n<\/code><\/pre>\n<p>Unfortunately, looking at <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/google\/cloud\/aiplatform\/models.py\" rel=\"nofollow noreferrer\">google api models code<\/a> it only have the predict method, not the raw_predict. So I don't know if it's available through python sdk right now.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1636138079960,
        "Question_score":4,
        "Question_tags":"google-cloud-platform|vertex|google-ai-platform|tfx|google-cloud-vertex-ai",
        "Question_view_count":508,
        "Owner_creation_time":1606605180560,
        "Owner_last_access_time":1664067783153,
        "Owner_location":"Brazil",
        "Owner_reputation":98,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1636203038129,
        "Answer_body":"<p>Vertex AI is a newer platform with limitations that will be improved over time. \u201csignature_name\u201d can be added to HTTP JSON Payload in <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.endpoints\/rawPredict\" rel=\"nofollow noreferrer\">RawPredictRequest<\/a> or from <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/endpoints\/raw-predict\" rel=\"nofollow noreferrer\">gcloud<\/a> as you have done but right now this is not available in regular predict requests.<\/p>\n<p><strong>Using HTTP JSON payload :<\/strong><\/p>\n<p>Example:<\/p>\n<p>input.json :<\/p>\n<pre><code>{\n   &quot;instances&quot;: [\n     [&quot;male&quot;, 29.8811345124283, 26.0, 1, &quot;S&quot;, &quot;New York, NY&quot;, 0, 0],\n     [&quot;female&quot;, 48.0, 39.6, 1, &quot;C&quot;, &quot;London \/ Paris&quot;, 0, 1]],\n \n     &quot;signature_name&quot;: &lt;string&gt;\n}\n\n<\/code><\/pre>\n<pre><code>curl \\\n-X POST \\\n-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n-H &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-aiplatform.googleapis.com\/v1\/projects\/${PROJECT_ID}\/locations\/us-central1\/endpoints\/${ENDPOINT_ID}:rawPredict \\\n-d &quot;@input.json&quot;\n\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1636439088550,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1636453108543,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69857932",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73557938,
        "Question_title":"Vertex AI: Unknown Custom Training Job Error",
        "Question_body":"<p>While attempting to use the Vertex AI via Google Console to create a custom training as follows:<\/p>\n<ol>\n<li>Select Dataset<\/li>\n<li>Select Annotation set<\/li>\n<li>Select &quot;Custom training (advanced)&quot;<\/li>\n<li>Select &quot;Train new model&quot;<\/li>\n<li>Provide Name<\/li>\n<li>Select training options<\/li>\n<li>Select &quot;Custom container&quot;<\/li>\n<li>Browse and select Docker container in Artifact Registry<\/li>\n<li>Select &quot;Dataset export directory&quot; to GCS location<\/li>\n<li>No hyperparameters<\/li>\n<li>Select &quot;Region&quot; us-central1<\/li>\n<li>Select &quot;Machine type&quot; a2-highgpu-1g<\/li>\n<li>Select &quot;Accelerator type&quot; NVIDIA_TESLA_A100<\/li>\n<li>Select &quot;Accelerator count&quot; 1<\/li>\n<li>No &quot;Prediction container&quot;<\/li>\n<\/ol>\n<p>It results in the following error message:<\/p>\n<blockquote>\n<p>Unable to start training due to the following error: Unable to parse\n`training_pipeline.training_task_inputs` into custom task `inputs`\ndefined in the file:\ngs:\/\/google-cloud-aiplatform\/schema\/trainingjob\/definition\/custom_task_1.0.0.yaml<\/p>\n<\/blockquote>\n<p>This error message does not provide enough information as to exactly what is the issue, and was wondering if anyone else know of a solution for this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1661958154070,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":33,
        "Owner_creation_time":1516951090470,
        "Owner_last_access_time":1663790014450,
        "Owner_location":null,
        "Owner_reputation":705,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Question_last_edit_time":1661967668820,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73557938",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73612214,
        "Question_title":"Vertex AI model serving - gRPC access - code pointers \/ samples",
        "Question_body":"<p>We have created an endpoint in Vertex AI. We have got the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-custom-models\" rel=\"nofollow noreferrer\">client library<\/a> route working. However, we also want to figure out the gRPC route since that is closest to the gRPC route we had with self managed TF-Serving.\nCan someone provide a code pointer for Vertex AI model serving using gRPC (preferably in Python)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662394780720,
        "Question_score":1,
        "Question_tags":"python|grpc|google-cloud-vertex-ai",
        "Question_view_count":36,
        "Owner_creation_time":1280773677817,
        "Owner_last_access_time":1662955154927,
        "Owner_location":"Jersey City, NJ",
        "Owner_reputation":4339,
        "Owner_up_votes":16,
        "Owner_down_votes":1,
        "Owner_views":479,
        "Question_last_edit_time":1662401625169,
        "Answer_body":"<p>gRPC can be used through Vertex Prediction private endpoint, but it is not yet officially supported. See sample here: <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/vertex_endpoints\/optimized_tensorflow_runtime\/tabular_optimized_online_prediction.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/vertex_endpoints\/optimized_tensorflow_runtime\/tabular_optimized_online_prediction.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1662511382056,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73612214",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71557442,
        "Question_title":"How combine results from multiple models in Google Vertex AI?",
        "Question_body":"<p>I have multiple models in Google Vertex AI and I want to create an endpoint to serve my predictions.\nI need to run aggregation algorithms, like the Voting algorithm on the output of my models.\nI have not found any ways of using the models together so that I can run the voting algorithms on the results.\nDo I have to create a new model, curl my existing models and then run my algorithms on the results?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1647864699137,
        "Question_score":1,
        "Question_tags":"google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":253,
        "Owner_creation_time":1372407778700,
        "Owner_last_access_time":1663592257430,
        "Owner_location":"Oslo, Norway",
        "Owner_reputation":134,
        "Owner_up_votes":303,
        "Owner_down_votes":0,
        "Owner_views":74,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There is no in-built provision to implement aggregation algorithms in Vertex AI. To <code>curl<\/code> results from the models then aggregate them, we would need to deploy all of them to individual endpoints. Instead, I would suggest the below method to deploy the models and the meta-model(aggregate model) to a single endpoint using <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/use-custom-container\" rel=\"nofollow noreferrer\">custom containers for prediction<\/a>. The custom container requirements can be found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>You can load the model artifacts from GCS into a custom container. If the same set of models are used (i.e) the input models to the meta-model do not change, you can package them inside the container to reduce load time. Then, a custom HTTP logic can be used to return the aggregation output like so. This is a sample custom flask server logic.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_models_from_gcs():\n    ## Pull the required model artifacts from GCS and load them here.\n    models = [model_1, model_2, model_3]\n    return models\n\ndef aggregate_predictions(predictions):\n    ## Your aggregation algorithm here\n    return aggregated_result\n\n\n@app.post(os.environ['AIP_PREDICT_ROUTE'])\nasync def predict(request: Request):\n    body = await request.json()\n    instances = body[&quot;instances&quot;]\n    inputs = np.asarray(instances)\n    preprocessed_inputs = _preprocessor.preprocess(inputs)\n\n    models = get_models_from_gcs()\n    predictions = []\n    \n    for model in models:\n        predictions.append(model.predict(preprocessed_inputs))\n\n    aggregated_result = aggregate_predictions(predictions)\n\n    return {&quot;aggregated_predictions&quot;: aggregated_result}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1647950025929,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1647950484409,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71557442",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71132848,
        "Question_title":"Google.Cloud.AIPlatform.V1 Received http2 header with status: 404",
        "Question_body":"<p>We are trying to call the Google.Cloud.AIPlatform.V1 predict API using the .Net client and keep getting the following error:   Received http2 header with status: 404<\/p>\n<p>We setup credentials using the API key and environment variable:  GOOGLE_APPLICATION_CREDENTIALS<\/p>\n<p>Here is the code to call the vertex AI predict API:<\/p>\n<pre><code>const string projectId = &quot;xxxxxx&quot;;\nconst string location = &quot;us-central1&quot;; \nconst string endpointId = &quot;xxxxxx&quot;;  \n\nPredictionServiceClient client = PredictionServiceClient.Create();\n\nvar structVal = Google.Protobuf.WellKnownTypes.Value.ForStruct(new Struct\n{\n    Fields =\n    {\n    [&quot;mimeType&quot;] = Google.Protobuf.WellKnownTypes.Value.ForString(&quot;text\/plain&quot;),\n    \/\/ Sample contents is a string constant defined in a separate file\n    [&quot;content&quot;] = Google.Protobuf.WellKnownTypes.Value.ForString(Consts.SampleContents)\n    }\n});\n\nPredictRequest req = new PredictRequest()\n{\n    EndpointAsEndpointName = EndpointName.FromProjectLocationEndpoint(projectId, location, endpointId),\n    Instances = { structVal }\n};\n\nPredictResponse response = client.Predict(req);\n<\/code><\/pre>\n<p>The full error returned:<\/p>\n<p>Status(StatusCode=&quot;Unimplemented&quot;, Detail=&quot;Received http2 header with status: 404&quot;, DebugException=&quot;Grpc.Core.Internal.CoreErrorDetailException: {&quot;created&quot;:&quot;@1644947338.412000000&quot;,&quot;description&quot;:&quot;Received http2 :status header with non-200 OK status&quot;,&quot;file&quot;:&quot;......\\src\\core\\ext\\filters\\http\\client\\http_client_filter.cc&quot;,&quot;file_line&quot;:134,&quot;grpc_message&quot;:&quot;Received http2 header with status: 404&quot;,&quot;grpc_status&quot;:12,&quot;value&quot;:&quot;404&quot;}&quot;)<\/p>\n<p>I validate the same call using CURL and was able to successfully make the call.<\/p>\n<pre><code>curl -X POST -H &quot;Authorization: Bearer XXXXX&quot; -H &quot;Content-Type: application\/json&quot; https:\/\/us-central1-aiplatform.googleapis.com\/ui\/projects\/XXXXXX\/locations\/us-central1\/endpoints\/XXXXXX:predict -d @payload.json\n<\/code><\/pre>\n<p>Any help would be greatly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644956294297,
        "Question_score":0,
        "Question_tags":"c#|google-cloud-vertex-ai",
        "Question_view_count":204,
        "Owner_creation_time":1545835825527,
        "Owner_last_access_time":1664041392653,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71132848",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68915712,
        "Question_title":"Google Auto ML taking huge time for forcasting",
        "Question_body":"<p>I have around 500 time series dataset for a period of 2.5 yrs with a granularity of 1 day for each series. This amounts to roughly 1 million data points.\nI want to forecast for 2 weeks in 1 day granularity for each of the time series. There might be correlation among these 500 time series.\nAfter ensuring that I have data for each timestamp, we are feeding these (500) time series to autoML where each time series is identified by \u201cseries identifier\u201d.\nSo, our input to the autoML (Forecasting) is timestamp, series identifier, features, and target value. I have 30 feature which are combination of categorical and numerical.\nWith this setup, if I feed to autoML, it is taking more than 20 hrs for training which is not cost effective for me.<\/p>\n<p>Please help me to optimized this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629854539447,
        "Question_score":0,
        "Question_tags":"time-series|google-cloud-automl|automl|google-cloud-vertex-ai",
        "Question_view_count":47,
        "Owner_creation_time":1497882511323,
        "Owner_last_access_time":1649034469423,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68915712",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71956892,
        "Question_title":"Training Pipeline fails after uploading model artifacts to Google Cloud Storage",
        "Question_body":"<p>Here's a snippet of my training code:<\/p>\n<pre><code>param_grid = {\n&quot;max_tokens&quot; : [100],\n&quot;max_len&quot; : [10],\n&quot;dropout&quot; : [0.1],\n}\ngs_model = GridSearchCV(KerasClassifier(build_model), param_grid, cv=3, scoring='accuracy')\ngs_model.fit(x_train, y_train, verbose = 1)\nbest_params = gs_model.best_params_\noptimized_model = build_model(max_tokens = best_params[&quot;max_tokens&quot;], max_len = best_params[&quot;max_len&quot;], dropout = best_params[&quot;dropout&quot;])\noptimized_model.fit(x_train, y_train, epochs = 3, validation_split = 0.2, callbacks = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, verbose = 1))\nmodel_name = &quot;\/tmp\/custom-model-test&quot;\noptimized_model.save(model_name)\nprint('saved model to ', model_name)\nupload_from_directory(model_name, &quot;[redacted Bucket name]&quot;, &quot;custom-model-test&quot;)\ntry: \n    upload_blob(&quot;[redacted Bucket name]&quot;, &quot;goback-custom-train\/requirements.txt&quot;, &quot;custom-model-test\/requirements.txt&quot;)\nexcept:\n    print(traceback.format_exc())\n    print('Upload failed')\n<\/code><\/pre>\n<p>Which succeeds in uploading to Google Cloud Storage. It makes use of <code>model.save<\/code> from Keras, and saves the created directory to my Bucket, along with a <code>requirements.txt<\/code> file inside it. To be clear, once the code block above is ran, a directory <code>custom-model-test\/<\/code> is created in <code>gs:\/\/[redacted Bucket name]<\/code> with contents <code>requirements.txt<\/code> and <code>tmp\/<\/code>. Inside <code>tmp\/<\/code> is <code>keras-metadata.pb<\/code>, <code>saved_model.pb<\/code>, and <code>variables\/<\/code>.<\/p>\n<p>I run this container in the following codeblock in my Kubeflow Pipeline:<\/p>\n<pre><code>training_job_run_op = gcc_aip.CustomContainerTrainingJobRunOp(\n    project = project,\n    display_name = display_name,\n    container_uri=training_container_uri,\n    model_serving_container_image_uri=model_serving_container_image_uri,\n    model_serving_container_predict_route = model_serving_container_predict_route,\n    model_serving_container_health_route = model_serving_container_health_route,\n    model_serving_container_ports = [8080],\n    service_account = &quot;[redacted service account]&quot;,\n    machine_type = &quot;n1-highmem-2&quot;,\n    accelerator_type =&quot;NVIDIA_TESLA_V100&quot;,\n    staging_bucket = BUCKET_NAME)\n<\/code><\/pre>\n<p>For some reason, after training and saving the model artifacts (the logs for the model training says it completed successfully) the pipeline fails with logs saying:<\/p>\n<pre><code>&quot; File &quot;\/opt\/python3.7\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/training_jobs.py&quot;, line 905, in _raise_failure &quot;\n&quot; raise RuntimeError(&quot;Training failed with:\\n%s&quot; % self._gca_resource.error) &quot;\n&quot;RuntimeError: Training failed with: &quot;\n&quot;code: 5\n&quot;message: &quot;There are no files under \\&quot;gs:\/\/[redacted Bucket name]\/aiplatform-custom-training-2022-04-21-14:04:46.151\/model\\&quot; to copy.&quot;\n&quot;\n<\/code><\/pre>\n<p>What's going on here? What's the fix?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1650554216863,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-storage|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":170,
        "Owner_creation_time":1649861924490,
        "Owner_last_access_time":1663770236590,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1650554522460,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71956892",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73085293,
        "Question_title":"Library is not installed on PATH - How can I install on path?",
        "Question_body":"<p>I am running this notebook in my managed notebooks environment on Google Cloud and I'm getting the following error when trying to install the packages: &quot;WARNING: The script google-oauthlib-tool is installed in '\/home\/jupyter\/.local\/bin' which is not on PATH.\nConsider adding this directory to PATH.&quot;<\/p>\n<p>Here is the python code that I'm trying to run for reference. <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/model_monitoring\/model_monitoring.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/model_monitoring\/model_monitoring.ipynb<\/a><\/p>\n<p>Any suggestions on how I can update the package installation so it is on path and resolve the error? I'm currently working on GCP user-managed notebooks on a Mac.<\/p>\n<p>Thanks so much for any tips!<\/p>\n<ul>\n<li>RE<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658518447040,
        "Question_score":2,
        "Question_tags":"python|google-cloud-platform|installation|package|google-cloud-vertex-ai",
        "Question_view_count":52,
        "Owner_creation_time":1621620820567,
        "Owner_last_access_time":1660057756750,
        "Owner_location":null,
        "Owner_reputation":29,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Open up your shell config file (likely .zshrc because the default shell on Mac is now zsh and that's the name of the zsh config file) located at your home directory in a text editor (TextEdit, etc) and add the path to the  executable.\nLike this:\nOpen the file:\n<code>open -e ~\/.zshrc<\/code>\nEdit the file:\nAdd this line at the top (may vary, check the documentation):\n<code>export PATH=&quot;\/home\/jupyter\/.local\/bin&quot;<\/code>\nThat may not work, try this:\n<code>export PATH=&quot;$PATH:\/home\/jupyter\/.local\/bin&quot;<\/code>\nYour best bet is to read the package documentation.<\/p>\n<p>After saving the config file, run <code>source ~\/.zshrc<\/code> and replace .zshrc with the config file name if it's different OR open a new terminal tab.<\/p>\n<p>What this does is tells the shell that the command exists and where to find it.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1658596861407,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73085293",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69373666,
        "Question_title":"Deployment with customer handler on Google Cloud Vertex AI",
        "Question_body":"<p>I'm trying to deploy a TorchServe instance on Google Vertex AI platform but as per their documentation (<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#response_requirements\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#response_requirements<\/a>), it requires the responses to be of the following shape:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;predictions&quot;: PREDICTIONS\n}\n<\/code><\/pre>\n<p>Where <strong>PREDICTIONS<\/strong> is an array of JSON values representing the predictions that your container has generated.<\/p>\n<p>Unfortunately, when I try to return such a shape in the <code>postprocess()<\/code> method of my custom handler, as such:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def postprocess(self, data):\n    return {\n        &quot;predictions&quot;: data\n    }\n<\/code><\/pre>\n<p>TorchServe returns:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;code&quot;: 503,\n  &quot;type&quot;: &quot;InternalServerException&quot;,\n  &quot;message&quot;: &quot;Invalid model predict output&quot;\n}\n<\/code><\/pre>\n<p>Please note that <code>data<\/code> is a list of lists, for example: [[1, 2, 1], [2, 3, 3]]. (Basically, I am generating embeddings from sentences)<\/p>\n<p>Now if I simply return <code>data<\/code> (and not a Python dictionary), it works with TorchServe but when I deploy the container on Vertex AI, it returns the following error:  <code>ModelNotFoundException<\/code>. I assumed Vertex AI throws this error since the return shape does not match what's expected (c.f. documentation).<\/p>\n<p>Did anybody successfully manage to deploy a TorchServe instance with custom handler on Vertex AI?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1632907406933,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|pytorch|google-cloud-ml|google-cloud-vertex-ai|torchserve",
        "Question_view_count":433,
        "Owner_creation_time":1300443398977,
        "Owner_last_access_time":1645461116407,
        "Owner_location":"Switzerland",
        "Owner_reputation":681,
        "Owner_up_votes":72,
        "Owner_down_votes":7,
        "Owner_views":34,
        "Question_last_edit_time":1632914906016,
        "Answer_body":"<p>Actually, making sure that the TorchServe processes correctly the input dictionary (instances) solved the issue. It seems like what's on the <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/pytorch-google-cloud-how-deploy-pytorch-models-vertex-ai\" rel=\"nofollow noreferrer\">article<\/a> did not work for me.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1633278542092,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69373666",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72741757,
        "Question_title":"Vertex AI Custom Container Training Job python SDK - google.api_core.exceptions.FailedPrecondition: 400 '",
        "Question_body":"<p>I have built a custom container which use my managed dataset on vertex to run my training code, it worked successfully when I create the training job on the Vertex AI website interface.<\/p>\n<p>But now I'm trying to create the training job from a python script using<\/p>\n<pre><code>class google.cloud.aiplatform.CustomContainerTrainingJob\n<\/code><\/pre>\n<p>I load a managed dataset that I have on vertex AI with<\/p>\n<pre><code>dataset = aiplatform.ImageDataset(dataset_id) if dataset_id else None\n<\/code><\/pre>\n<p>But when I try to run the following code:<\/p>\n<pre><code>model = job.run(\n        dataset=dataset,\n        model_display_name=model_display_name,\n        args=args,\n        replica_count=replica_count,\n        machine_type=machine_type,\n        accelerator_type=accelerator_type,\n        accelerator_count=accelerator_count,\n        training_fraction_split=training_fraction_split,\n        validation_fraction_split=validation_fraction_split,\n        test_fraction_split=test_fraction_split,\n        sync=sync,\n    )\n\n    model.wait()\n\n    print(model.display_name)\n    print(model.resource_name)\n    print(model.uri)\n    return model\n<\/code><\/pre>\n<p>I got the following error:<\/p>\n<pre><code>google.api_core.exceptions.FailedPrecondition: 400 'annotation_schema_uri' should be set in the TrainingPipeline.input_data_config for custom training or hyperparameter tuning with managed dataset.\n<\/code><\/pre>\n<p>I feel like something is wrong because when I create the job on the website I specify an export directory for the managed dataset, but I have not found where to do it here.<\/p>\n<p>Any ideas?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656062302777,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":170,
        "Owner_creation_time":1656061360900,
        "Owner_last_access_time":1659951704647,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Well I found the answer in the <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/aiplatform?hl=fr#class-googlecloudaiplatformcustomcontainertrainingjobdisplayname-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-containeruri-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-command-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerimageuri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerpredictroute-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerhealthroute-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainercommand-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerargs-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerenvironmentvariables-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerports-optionalsequenceinthttpspythonreadthedocsioenlatestlibraryfunctionshtmlint--none-modeldescription-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelinstanceschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelparametersschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelpredictionschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-project-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-location-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-credentials-optionalgoogleauthcredentialscredentialshttpsgoogleapisdevpythongoogle-authlatestreferencegoogleauthcredentialshtmlgoogleauthcredentialscredentials--none-labels-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-trainingencryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelencryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-stagingbucket-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none\" rel=\"nofollow noreferrer\">documentation<\/a>, data are automatically exported to the provided bucket thus it was not the issue. The issue was in the error ( obviously ).\nTo provide a good annotation URI, it is enough to just add a parameter to run():<\/p>\n<pre><code>annotation_schema_uri=aiplatform.schema.dataset.annotation.image.classification\n<\/code><\/pre>\n<p>image.classification was what I needed here but can be replaced by text.extraction if you do text extraction for example.<\/p>\n<p>This will pass as string value the following value which is the asked gs uri:<\/p>\n<pre><code>gs:\/\/google-cloud-aiplatform\/schema\/dataset\/annotation\/image_classification_1.0.0.yaml\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1656071944492,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1656343337227,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72741757",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72313973,
        "Question_title":"Vertex AI AutoML regression - batch predition error due to datatype mismatch",
        "Question_body":"<p>I trained a Vertex AI AutoML regression model (using the UI).\nI ran a Batch Prediction (alos with the UI) and it failed because of a datatype mismatch:\nThe Batch Prediction returned an error table in the export location in BigQuery.<\/p>\n<p>The Error :<\/p>\n<ul>\n<li><a href=\"https:\/\/i.stack.imgur.com\/1TBkT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1TBkT.png\" alt=\"List item\" \/><\/a><\/li>\n<\/ul>\n<p><strong>In the output from the Batch Prediction, DISCOUNT_PCT is indeed a STRING<\/strong>:\n<a href=\"https:\/\/i.stack.imgur.com\/hvv5t.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hvv5t.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>But in the table I loaded for the Batch Prediction, it is a NUMERIC<\/strong> (as it is in the data I used to train the model):\n<a href=\"https:\/\/i.stack.imgur.com\/RHTI2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RHTI2.png\" alt=\"enter image description here\" \/><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/q42iM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q42iM.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It looks like the Batch Prediction process somehow changed the datatype of the table I loaded. Why is this happening and how can I solve it?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1653024740080,
        "Question_score":0,
        "Question_tags":"prediction|google-cloud-ml|type-mismatch|google-cloud-vertex-ai",
        "Question_view_count":65,
        "Owner_creation_time":1534984935633,
        "Owner_last_access_time":1661490030977,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1653031324512,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72313973",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70606958,
        "Question_title":"Use a model trained by Google Cloud Vertex AI accelerated with TRT on Jetson Nano",
        "Question_body":"<p>I am trying to standardize our deployment workflow for machine vision systems. So we were thinking of the following workflow.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/lZiQs.png\" rel=\"nofollow noreferrer\">Deployment workflow<\/a><\/p>\n<p>So, we want to create the prototype for the same, so we followed the workflow. So, there is no problem with GCP operation whatsoever but when we try to export models, which we train on the <code>vertexAI<\/code> it will give three models as mentioned in the workflow which is:<\/p>\n<ol>\n<li>SaveModel<\/li>\n<li>TFLite<\/li>\n<li>TFJS<\/li>\n<\/ol>\n<p>and we try these models to convert into the ONNX model but we failed due to different errors.<\/p>\n<ol>\n<li>SaveModel - Always getting the same error with any parameter which is as follows\n<a href=\"https:\/\/i.stack.imgur.com\/HsBoa.png\" rel=\"nofollow noreferrer\">Error in savemodel<\/a>\nI tried to track the error and I identified that the model is not loading inside the TensorFlow only which is wired since it is exported from the <code>GCP vertexAI<\/code> which leverages the power of TensorFlow.<\/li>\n<li>TFLite - Successfully converted but again the problem with the <code>opset<\/code> of ONNX but with 15 <code>opset<\/code> it gets successfully converted but then NVIDIA tensorRT ONNXparser doesn't recognize the model during ONNX to TRT conversion.<\/li>\n<li>TFJS - yet not tried.<\/li>\n<\/ol>\n<p>So we are blocked here due to these problems.<\/p>\n<p>We can run these models exported directly from the <code>vertexAI<\/code> on the Jetson Nano device but the problem is <code>TF-TRT<\/code> and TensorFlow is not memory-optimized on the GPU so the system gets frozen after 3 to 4 hours of running.<\/p>\n<p>We try this workflow with google teachable machine once and it workout well all steps are working perfectly fine so I am really confused How I conclude this full workflow since it's working on a teachable machine which is created by Google and not working on vertexAI model which is again developed by same Company.<\/p>\n<p>Or am I doing Something wrong in this workflow?\nFor the background we are developing this workflow inside C++ framework for the realtime application in industrial environment.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1641471385767,
        "Question_score":1,
        "Question_tags":"tensorflow|onnx|nvidia-jetson-nano|google-cloud-vertex-ai",
        "Question_view_count":189,
        "Owner_creation_time":1641468106990,
        "Owner_last_access_time":1663914585187,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1641962349420,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70606958",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70648776,
        "Question_title":"Python click incorrectly parses arguments when called in Vertex AI Pipeline",
        "Question_body":"<p>I'm trying to run a simple Ada-boosted Decision Tree regressor on GCP Vertex AI. To parse hyperparams and other arguments I use Click for Python, a very simple CLI library. Here's the setup for my task function:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@click.command()\n@click.argument(&quot;input_path&quot;, type=str)\n@click.option(&quot;--output-path&quot;, type=str, envvar='AIP_MODEL_DIR')\n@click.option('--gcloud', is_flag=True, help='Run as if in Google Cloud Vertex AI Pipeline')\n@click.option('--grid', is_flag=True, help='Perform a grid search instead of a single run. Ignored with --gcloud')\n@click.option(&quot;--max_depth&quot;, type=int, default=4, help='Max depth of decision tree', show_default=True)\n@click.option(&quot;--n_estimators&quot;, type=int, default=50, help='Number of AdaBoost boosts', show_default=True)\ndef click_main(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators)\n\n\ndef train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    print(input_path, output_path, gcloud)\n    logger = logging.getLogger(__name__)\n    logger.info(&quot;training models from processed data&quot;)\n    ...\n<\/code><\/pre>\n<p>When I run it locally like below, Click correctly grabs the params both from console and environment and proceeds with model training (<code>AIP_MODEL_DIR<\/code> is <code>gs:\/\/(BUCKET_NAME)\/models<\/code>)<\/p>\n<pre><code>\u276f python3 -m src.models.train_model gs:\/\/(BUCKET_NAME)\/data\/processed --gcloud\n\ngs:\/\/(BUCKET_NAME)\/data\/processed gs:\/\/(BUCKET_NAME)\/models True\n\n<\/code><\/pre>\n<p>However, when I put this code on the Vertex AI Pipeline, it throws an error, namely<\/p>\n<pre><code>FileNotFoundError: b\/(BUCKET_NAME)\/o\/data%2Fprocessed%20%20--gcloud%2Fprocessed_features.csv\n<\/code><\/pre>\n<p>As it is clearly seen, Click grabs both the parameter and the <code>--gcloud<\/code> option and assigns it to <code>input_path<\/code>. The print statement before that confirms it, both by having one too many spaces and <code>--gcloud<\/code> being parsed as false.<\/p>\n<pre><code>gs:\/\/(BUCKET_NAME)\/data\/processed  --gcloud gs:\/\/(BUCKET_NAME)\/models\/1\/model\/ False\n<\/code><\/pre>\n<p>Has anyone here encountered this issue or have any idea how to solve it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641797901177,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|google-cloud-storage|python-click|google-cloud-vertex-ai",
        "Question_view_count":129,
        "Owner_creation_time":1579801831103,
        "Owner_last_access_time":1663207732260,
        "Owner_location":"Tempe, AZ, USA",
        "Owner_reputation":71,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":1641896853596,
        "Answer_body":"<p>I think is due the nature of <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/arguments\/?highlight=arguments\" rel=\"nofollow noreferrer\">arguments<\/a> and <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/options\/?highlight=options\" rel=\"nofollow noreferrer\">options<\/a>, you are mixing arguments and options although is not implicit stated in the documentation but argument will eat up the options that follow. If nargs is not allocated it will default to 1 considering everything after it follows as string which it looks like this is the case.<\/p>\n<blockquote>\n<p>nargs \u2013 the number of arguments to match. If not 1 the return value is a tuple instead of single value. The default for nargs is 1 (except if the type is a tuple, then it\u2019s the arity of the tuple).<\/p>\n<\/blockquote>\n<p>I think you should first use options followed by the argument as display on the <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/documentation\/?highlight=arguments\" rel=\"nofollow noreferrer\">documentation page<\/a>. Other approach is to group it under a command as show on this <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/commands\/\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1641810988040,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70648776",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70664460,
        "Question_title":"Is it possible to create and train natural-language google-cloud model using python sdk?",
        "Question_body":"<p>I want to create a Vertex pipeline using KFP for training natural language model, and I can't find a python API to use for creating and training model, I know that there is the option of creating the model from the console, but I am looking for a way to do it on my git repository.<\/p>\n<p>any ideas?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1641893769170,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|automl|google-cloud-vertex-ai|google-natural-language",
        "Question_view_count":13,
        "Owner_creation_time":1563265471333,
        "Owner_last_access_time":1663834240627,
        "Owner_location":"Israel",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70664460",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72692781,
        "Question_title":"Error call Google Vertex AI endpoint from a python backend",
        "Question_body":"<p>I am trying to send an http post request to my google vertex ai endpoint for prediction. Though I do set the Bearer Token in the request header, the request still fails with the below error:<\/p>\n<pre><code>{\n&quot;error&quot;: {\n    &quot;code&quot;: 401,\n    &quot;message&quot;: &quot;Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https:\/\/developers.google.com\/identity\/sign-in\/web\/devconsole-project.&quot;,\n    &quot;status&quot;: &quot;UNAUTHENTICATED&quot;,\n    &quot;details&quot;: [\n        {\n            &quot;@type&quot;: &quot;type.googleapis.com\/google.rpc.ErrorInfo&quot;,\n            &quot;reason&quot;: &quot;ACCESS_TOKEN_TYPE_UNSUPPORTED&quot;,\n            &quot;metadata&quot;: {\n                &quot;service&quot;: &quot;aiplatform.googleapis.com&quot;,\n                &quot;method&quot;: &quot;google.cloud.aiplatform.v1.PredictionService.Predict&quot;\n            }\n        }\n    ]\n}\n<\/code><\/pre>\n<p>}<\/p>\n<p>Since I am making this call from a python backend, I'm not sure if OAuth 2 as suggested in the message would be wise and applicable choice.<\/p>\n<p>The model is already deployed and endpointed test on vertex ai and it worked fine. What I am trying to do is send same prediction task via an http post request using postman and this is what failed.<\/p>\n<p>The request url looks like this:<\/p>\n<pre><code>https:\/\/[LOCATION]-aiplatform.googleapis.com\/v1\/projects\/[PROJECT ID]\/locations\/[LOCATION]\/endpoints\/[ENDPOINT ID]:predict\n<\/code><\/pre>\n<p>Where token bearer is set in the potman authorization tab and instance set in request body.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_time":1655757826930,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-endpoints|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":353,
        "Owner_creation_time":1426920929353,
        "Owner_last_access_time":1662127122187,
        "Owner_location":"Bangkok",
        "Owner_reputation":194,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":1655940147089,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72692781",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73815721,
        "Question_title":"How do you specify multiple values for the inputTensorName key in INPUTMETADATA spec in Vertex Explainable AI for a Functional API model?",
        "Question_body":"<p>I want to add explanation to my model running in Vertex AI using the Vertex AI SDK.I get a silent error when running the batch prediction using ModelBatchPredictOp, where the ModelBatchPredictOp node runs infinitely.Here is my ModelBatchPredictOp definition;<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>ModelBatchPredictOp(\n    project=project_id,\n    job_display_name=&quot;tensorflow-ex-batch-prediction-job&quot;,\n    location=project_location,\n    model=champion_model.outputs[&quot;model&quot;],\n    instances_format=&quot;csv&quot;,\n    predictions_format=&quot;jsonl&quot;,\n    gcs_source_uris=gcs_source_uris,\n    gcs_destination_output_uri_prefix=gcs_destination_output_uri_prefix,\n    machine_type=batch_prediction_machine_type,\n    starting_replica_count=batch_prediction_min_replicas,\n    max_replica_count=batch_prediction_max_replicas,\n    generate_explanation=True,\n)\n<\/code><\/pre>\n<p>I have narrowed down the issue to inputTensorName key in the INPUTMETADATA spec.The 'inputTensorName' key in the INPUTMETADATA spec takes in a string for it's value(<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/ExplanationSpec#InputMetadata\" rel=\"nofollow noreferrer\">INPUTMETADATA spec<\/a>). In my case I have a tensorflow model defined using the functional API meaning it has multiple inputs, as shown below;<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># numeric\/categorical features in Chicago trips dataset to be preprocessed\nNUM_COLS = [&quot;dayofweek&quot;, &quot;hourofday&quot;, &quot;trip_distance&quot;, &quot;trip_miles&quot;, &quot;trip_seconds&quot;]\nORD_COLS = [&quot;company&quot;]\nOHE_COLS = [&quot;payment_type&quot;]\n\ndef build_and_compile_model(dataset: Dataset, model_params: dict) -&gt; Model:\n        &quot;&quot;&quot;Build and compile model.\n\n        Args:\n            dataset (Dataset): training dataset\n            model_params (dict): model parameters\n\n        Returns:\n            model (Model): built and compiled model\n        &quot;&quot;&quot;\n\n        # create inputs (scalars with shape `()`)\n        num_ins = {\n            name: Input(shape=(), name=name, dtype=tf.float32) for name in NUM_COLS\n        }\n        ord_ins = {\n            name: Input(shape=(), name=name, dtype=tf.string) for name in ORD_COLS\n        }\n        cat_ins = {\n            name: Input(shape=(), name=name, dtype=tf.string) for name in OHE_COLS\n        }\n\n        # join all inputs and expand by 1 dimension. NOTE: this is useful when passing\n        # in scalar inputs to a model in Vertex AI batch predictions or endpoints e.g.\n        # `{&quot;instances&quot;: {&quot;input1&quot;: 1.0, &quot;input2&quot;: &quot;str&quot;}}` instead of\n        # `{&quot;instances&quot;: {&quot;input1&quot;: [1.0], &quot;input2&quot;: [&quot;str&quot;]}`\n        all_ins = {**num_ins, **ord_ins, **cat_ins}\n        exp_ins = {n: tf.expand_dims(i, axis=-1) for n, i in all_ins.items()}\n\n        # preprocess expanded inputs\n        num_encoded = [normalization(n, dataset)(exp_ins[n]) for n in NUM_COLS]\n        ord_encoded = [str_lookup(n, dataset, &quot;int&quot;)(exp_ins[n]) for n in ORD_COLS]\n        ohe_encoded = [str_lookup(n, dataset, &quot;one_hot&quot;)(exp_ins[n]) for n in OHE_COLS]\n\n        # ensure ordinal encoded layers is of type float32 (like the other layers)\n        ord_encoded = [tf.cast(x, tf.float32) for x in ord_encoded]\n\n        # concat encoded inputs and add dense layers including output layer\n        x = num_encoded + ord_encoded + ohe_encoded\n        x = Concatenate()(x)\n        for units, activation in model_params[&quot;hidden_units&quot;]:\n            x = Dense(units, activation=activation)(x)\n        x = Dense(1, name=&quot;output&quot;, activation=&quot;linear&quot;)(x)\n\n        model = Model(inputs=all_ins, outputs=x, name=&quot;nn_model&quot;)\n        model.summary()\n\n        logging.info(f&quot;Use optimizer {model_params['optimizer']}&quot;)\n        optimizer = optimizers.get(model_params[&quot;optimizer&quot;])\n        optimizer.learning_rate = model_params[&quot;learning_rate&quot;]\n\n        model.compile(\n            loss=model_params[&quot;loss_fn&quot;],\n            optimizer=optimizer,\n            metrics=model_params[&quot;metrics&quot;],\n        )\n\n        return model\n<\/code><\/pre>\n<p>As a consequence when getting the input layer, using;<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>serving_input = list(\n        loaded_model.signatures[&quot;serving_default&quot;].structured_input_signature[1].keys()\n    )[0]\n  \nINPUT_METADATA = {\n   &quot;input_tensor_name&quot;: serving_input,\n   &quot;encoding&quot;: &quot;BAG_OF_FEATURES&quot;,\n   &quot;modality&quot;: &quot;numeric&quot;,\n   &quot;index_feature_mapping&quot;: cols,\n}\n\n<\/code><\/pre>\n<p>I only get one Input layer corresponding to one of the input tensors (cols) in either NUM_COLS, ORD_COLS or OHE_COLS. This causes an infinite run when running the ModelBatchPredictOp in the prediction pipeline as only the name to one input tensor is passed as the value to the inputTensorName. Running<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>list(model.signatures[&quot;serving_default&quot;].structured_input_signature[1].keys())\n<\/code><\/pre>\n<p>returns the a list of all the input layers corresponding to the input tensor names (cols) defined in NUM_COLS, ORD_COLS and OHE_COLS.<\/p>\n<p>How do I specify the value of inputTensorName in order to capture all the input layers? Or is there a work around to assign multiple input tensor names to inputTensorName?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663854073780,
        "Question_score":1,
        "Question_tags":"python|tensorflow|google-cloud-vertex-ai|google-cloud-ai-platform-pipelines|xai",
        "Question_view_count":19,
        "Owner_creation_time":1461847578647,
        "Owner_last_access_time":1663929992527,
        "Owner_location":"Nairobi, Kenya",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1663924909350,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73815721",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72907038,
        "Question_title":"Cost of deploying a TensorFlow model in GCP?",
        "Question_body":"<p>I'm thinking of deploying a TensorFlow model using Vertex AI in GCP. I am almost sure that the cost will be directly related to the number of queries per second (QPS) because I am going to use automatic scaling. I also know that the type of machine (with GPU, TPU, etc.) will have an impact on the cost.<\/p>\n<ul>\n<li>Do you have any estimation about the cost versus the number of queries per second?<\/li>\n<li>How does the type of virtual machine changes this cost?<\/li>\n<\/ul>\n<p>The type of model is for object detection.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657258379287,
        "Question_score":0,
        "Question_tags":"tensorflow|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":74,
        "Owner_creation_time":1569457921527,
        "Owner_last_access_time":1664002308563,
        "Owner_location":"San Luis Potos\u00ed, S.L.P., M\u00e9xico",
        "Owner_reputation":41,
        "Owner_up_votes":113,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1657258776587,
        "Answer_body":"<p>Autoscaling depends on the CPU and GPU utilization which directly correlates to the QPS, as you have said. To estimate the cost based on the QPS, you can deploy a custom prediction container to a Compute Engine instance directly, then benchmark the instance by making prediction calls until the VM hits 90+ percent CPU utilization (consider GPU utilization if configured). Do this multiple times for different machine types, and determine the &quot;QPS per cost per hour&quot; of different machine types. You can re-run these experiments while benchmarking latency to find the <strong>ideal cost per QPS per your latency targets<\/strong> for your specific custom prediction container. For more information about choosing the ideal machine for your workload, refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#finding_the_ideal_machine_type\" rel=\"nofollow noreferrer\">documentation<\/a>.<\/p>\n<p>For your second question, as per the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/pricing#custom-trained_models:%7E:text=a%20specific%20job.-,Prediction%20and%20explanation,-This%20table%20provides\" rel=\"nofollow noreferrer\">Vertex AI pricing documentation<\/a> (for model deployment), cost estimation is done based on the node hours. A node hour represents the time a virtual machine spends running your prediction job or waiting in a ready state to handle prediction or explanation requests. Each type of VM offered has a specific pricing per node hour depending on the number of cores and the amount of memory. Using a VM with more resources will cost more per node hour and vice versa. To choose an ideal VM for your deployment, please follow the steps given in the first paragraph which will help you find a good trade off between cost and performance.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1657283230409,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72907038",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72640182,
        "Question_title":"Kubeflow Pipeline Training Component Failing | Unknown return type: <class 'inspect._empty'>",
        "Question_body":"<p>I am running an ML pipeline and the training component\/step (see code below) continues to fail with the following error: &quot;RuntimeError: Unknown return type: &lt;class 'inspect._empty'&gt;. Must be one of <code>str<\/code>, <code>int<\/code>, <code>float<\/code>, a subclass of <code>Artifact<\/code>, or a NamedTuple collection of these types.&quot;<\/p>\n<p>Any ideas on what might be causing the issue\/error and how to resolve it?<\/p>\n<p>Thank you!<\/p>\n<ul>\n<li>RE<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>\n@component(\n    # this component builds an xgboost classifier with xgboost\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;\n)\n\ndef build_xgb_xgboost(project: str, \n                            bq_dataset: str, \n                            test_view_name: str,\n                            bq_location: str,\n                            metrics: Output[Metrics],\n                            model: Output[Model]\n\n):\n    from google.cloud import bigquery\n    import xgboost as xgb\n    import pandas as pd\n    from xgboost import XGBRegressor\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.metrics import mean_squared_error as MSE\n    from sklearn.metrics import mean_absolute_error\n    import joblib\n    import pyarrow\n    import db_dtypes\n     \n\n    client = bigquery.Client(project=project) \n\n    view_uri = f&quot;{project}.{bq_dataset}.{test_view_name}&quot; #replace view_name with test_view_name\n    \n    build_df_for_xgboost = '''\n    SELECT * FROM `{view_uri}`\n    '''.format(view_uri = view_uri)\n\n    job_config = bigquery.QueryJobConfig()\n    df_1 = client.query(build_df_for_xgboost).to_dataframe()\n    \n    #client.query(build_df_for_xgboost, job_config=job_config).to_dataframe()  \n    \n    df = df_1.drop(['int64_field_0'], axis=1)\n    \n    def onehot_encode(df, column):\n        df = df.copy()\n        dummies = pd.get_dummies(df[column], prefix=column)\n        df = pd.concat([df, dummies], axis=1)\n        df = df.drop(column, axis=1)\n    return df\n    \n    # Binary encoding\n    df['preferred_foot'] = df['preferred_foot'].replace({'left': 0, 'right': 1})\n    \n    # One-hot encoding\n    for column in ['attacking_work_rate', 'defensive_work_rate']:\n        df = onehot_encode(df, column=column)\n    \n    # Split df into X and y\n    y = df['overall_rating']\n    X = df.drop('overall_rating', axis=1)\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n\n    #specify parameters\n    \n    #define your model \n    bst = XGBRegressor(\n    objective='reg:linear',\n    learning_rate = '.1',\n    alpha = '0.001'\n    )\n    \n    #fit your model\n    bst.fit(X_train, y_train)\n    \n    # Predict the model \n    y_pred = bst.predict(X_test)\n    rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n    mae = mean_absolute_error(y_test, y_pred)\n    \n    metrics.log_metric(&quot;RMSE&quot;, rmse)\n    metrics.log_metric(&quot;framework&quot;, &quot;xgboost&quot;)\n    metrics.log_metric(&quot;dataset_size&quot;, len(df))\n    metrics.log_metric(&quot;MAE&quot;, mae)\n    \n    dump(bst, model.path + &quot;.joblib&quot;)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655350561587,
        "Question_score":0,
        "Question_tags":"kubeflow|google-cloud-vertex-ai|kubeflow-pipelines|mlops",
        "Question_view_count":107,
        "Owner_creation_time":1621620820567,
        "Owner_last_access_time":1660057756750,
        "Owner_location":null,
        "Owner_reputation":29,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":1659256989790,
        "Answer_body":"<p>I think this might just be a bug in the version of KFP v2 SDK code you're using.<\/p>\n<p>I mostly use the stable KFPv1 methods to avoid problems.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\n\n\ndef train_xgboost_model(\n    project: str, \n    bq_dataset: str, \n    test_view_name: str,\n    bq_location: str,\n    metrics_path: OutputPath(Metrics),\n    model_path: OutputPath(Model),\n):\n    import json\n    from pathlib import Path\n\n    metrics = {\n       ...\n    }\n    Path(metrics_path).write_text(json.dumps(metrics))\n\n    dump(bst, model_path)\n\ntrain_xgboost_model_op = create_component_from_func(\n    func=train_xgboost_model,\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;,\n)\n\n<\/code><\/pre>\n<p>You can also find many examples of real-world components in this repo: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components<\/a><\/p>\n<p>including an XGBoost trainer <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py<\/a><\/p>\n<p>and a full XGBoost pipeline: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1659257407332,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72640182",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72314675,
        "Question_title":"Managing data drift when using w2vec embeddings on VertexAI",
        "Question_body":"<p>So I am looking into moving my models from GCP's AI Platform to Vertex AI, my main motivation for it being the fact that Vertex AI has automatic email notifications when your data skews or drifts (<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/model-monitoring\/using-model-monitoring\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/model-monitoring\/using-model-monitoring<\/a>).<\/p>\n<p>So if you start receiving dodgy data that doesn't resemble the training set, they send you an email telling you which features (columns) of the data you are trying to predict are drifting away from your training data.<\/p>\n<p>However, I am unsure how this would work in my case since my data is text data that has been encoded using word2vec embeddings. Therefore, my dataset has 300 columns but I don't know what feature each of the columns refers to.<\/p>\n<p>Is this sort of data drift analysis still useful in my particular case?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1653029251863,
        "Question_score":1,
        "Question_tags":"machine-learning|google-cloud-platform|word2vec|google-cloud-vertex-ai",
        "Question_view_count":133,
        "Owner_creation_time":1612345105200,
        "Owner_last_access_time":1663853054067,
        "Owner_location":"London, Reino Unido",
        "Owner_reputation":57,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72314675",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71101070,
        "Question_title":"How to properly extract endpoint id from gcp_resources of a Vertex AI pipeline on GCP?",
        "Question_body":"<p>I am using GCP Vertex AI pipeline (KFP) and using <code>google-cloud-aiplatform==1.10.0<\/code>, <code>kfp==1.8.11<\/code>, <code>google-cloud-pipeline-components==0.2.6<\/code>\nIn a component I am getting a gcp_resources <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/google-cloud\/google_cloud_pipeline_components\/proto\/README.md\" rel=\"nofollow noreferrer\">documentation<\/a> :<\/p>\n<pre><code>gcp_resources (str):\n            Serialized gcp_resources proto tracking the create endpoint's long running operation.\n<\/code><\/pre>\n<p>To extract the endpoint_id to do online prediction of my deployed model, I am doing:<\/p>\n<pre><code>from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\nfrom google.protobuf.json_format import Parse\ninput_gcp_resources = Parse(endpoint_ressource_name, GcpResources())\ngcp_resources=input_gcp_resources.resources.__getitem__(0).resource_uri.split('\/')\nendpoint_id=gcp_resources[gcp_resources.index('endpoints')+1]\n<\/code><\/pre>\n<p>Is there a better\/native way of extracting such info ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644758810230,
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":225,
        "Owner_creation_time":1465222092253,
        "Owner_last_access_time":1663858783617,
        "Owner_location":"Z\u00fcrich, Switzerland",
        "Owner_reputation":1414,
        "Owner_up_votes":258,
        "Owner_down_votes":3,
        "Owner_views":478,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In this case is the best way to extract the information. But, I recommend using the <a href=\"http:\/\/ttps:\/\/github.com\/aio-libs\/yarl\" rel=\"nofollow noreferrer\">yarl<\/a> library for complex uri to parse.<\/p>\n<p>You can see this example:<\/p>\n<pre><code>&gt;&gt;&gt; from yarl import URL\n&gt;&gt;&gt; url = URL('https:\/\/www.python.org\/~guido?arg=1#frag')\n&gt;&gt;&gt; url\nURL('https:\/\/www.python.org\/~guido?arg=1#frag')\n<\/code><\/pre>\n<p>All URL parts can be accessed by these properties.<\/p>\n<pre><code>&gt;&gt;&gt; url.scheme\n'https'\n&gt;&gt;&gt; url.host\n'www.python.org'\n&gt;&gt;&gt; url.path\n'\/~guido'\n&gt;&gt;&gt; url.query_string\n'arg=1'\n&gt;&gt;&gt; url.query\n&lt;MultiDictProxy('arg': '1')&gt;\n&gt;&gt;&gt; url.fragment\n'frag'\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1644873889256,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71101070",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72440188,
        "Question_title":"Deploy Pretrained Model Test Error: The first dimension of paddings must be the rank of inputs[4,2]",
        "Question_body":"<p>I have a successfully trained and tested a custom instance segmentation model using pixellib Mask_RCNN model.  The model runs inferences fine locally, but when I try to serve predictions using vertex ai \/ google cloud platform I cannot get the predictions to serve correctly.<\/p>\n<h2>Model Signature:<\/h2>\n<pre><code>  inputs['input_anchors'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, -1, 4)\n      name: serving_default_input_anchors:0\n  inputs['input_image'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, -1, -1, 3)\n      name: serving_default_input_image:0\n  inputs['input_image_meta'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, 14)\n      name: serving_default_input_image_meta:0\n<\/code><\/pre>\n<h2>JSON Requests - Snippet:<\/h2>\n<p>To test the model independent of any other code I use the test feature in google cloud console for vertex AI.  I enter JSON structured as follows.<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;instances&quot;: \n    [\n    {\n     &quot;input_anchors&quot;: [[[-0.35, ...], ... ]],\n     &quot;input_image&quot;: [[[[-123.7, -116.8, -103.9], ... ]]], \n     &quot;input_image_meta&quot;: [[0.0, 240.0, ....]]\n    }\n    ]\n}\n<\/code><\/pre>\n<h2>Shapes of input<\/h2>\n<p>I can confirm that the shapes of the inputs are the correct dimensions for the model signature.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>np.array(instance['input_anchors']).shape -&gt; (1,1023,4)\nnp.array(instance['input_image']).shape -&gt; (1,64,64,3)\nnp.array(instance['input_image_meta'].shape -&gt; (1,14)\n<\/code><\/pre>\n<h2>Returns the error:<\/h2>\n<p>Testing the model returns the following error.<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n &quot;error&quot;: {\n   &quot;code&quot;: 400,\n   &quot;message&quot;: &quot;{\\n    \\&quot;error\\&quot;: \\&quot;The first dimension of paddings must be the rank of inputs[4,2] [1,1,64,64,3]\\\\n\\\\t [[{{node mask_rcnn\/zero_padding2d_1\/Pad}}]]\\&quot;\\n}&quot;,\n   &quot;status&quot;: &quot;INVALID_ARGUMENT&quot;\n }\n}\n<\/code><\/pre>\n<p>The message indicates that the model is looking for a 5D input <code>[1,1,64,64,3]<\/code>, but the model signature requires a 4D input <code>(-1,-1,-1,3)<\/code>.<\/p>\n<p>Using np.expand_dims to make the input_image 5D, it results in the same error message but now asking for a 6D input <code>[1,1,1,64,64,3]<\/code>.  A 6D input yields an error message asking for a 7D input...<\/p>\n<p>If I reduce the input_image to 3D (which should definitely be incorrect) <code>[64,64,3]<\/code> I get a different error:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n &quot;error&quot;: {\n   &quot;code&quot;: 400,\n   &quot;message&quot;: &quot;{\\n    \\&quot;error\\&quot;: \\&quot;slice index 1 of dimension 0 out of bounds.\\\\n\\\\t [[{{node mask_rcnn\/roi_align_classifier\/strided_slice_8}}]]\\&quot;\\n}&quot;,\n   &quot;status&quot;: &quot;INVALID_ARGUMENT&quot;\n }\n}\n<\/code><\/pre>\n<p>Can someone help me understand if I'm structuring my inputs incorrectly for the model or if I'm running into a known bug?  It's strange that the error message always asks for +1 dimension than the dimension that I give it.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1653946027497,
        "Question_score":0,
        "Question_tags":"python|tensorflow|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":41,
        "Owner_creation_time":1457431514813,
        "Owner_last_access_time":1662026934273,
        "Owner_location":null,
        "Owner_reputation":36,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72440188",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73573487,
        "Question_title":"Vertex AI model version using Python SDK",
        "Question_body":"<p>Vertex AI offers a very interesting Model Registry that allows you to store all trained models and track all their versions.<\/p>\n<p>However, I don't manage to create new versions of the same model using the Python SDK. In particular, I have a Vertex AI Pipeline that performs: 1) data preprocessing, 2) feature engineering, 3) feature store creation, and in the end, 4) train a model with AutoML Tabular.<\/p>\n<p>The code of the Pipeline component dedicated to the point 4 is:<\/p>\n<pre><code> automl_training_electric_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n    project=project_bq,\n    model_display_name=&quot;pred-model&quot;,\n    display_name=&quot;pred-model&quot;,\n    optimization_prediction_type=&quot;classification&quot;,\n    optimization_objective=&quot;maximize-au-prc&quot;,\n    budget_milli_node_hours=1000,\n    dataset=comp5a.outputs[&quot;dataset&quot;],\n    target_column=&quot;fault&quot;,\n    location=location\n)\n<\/code><\/pre>\n<p>In the Google documentation I didn't find anything that could help me in creating new versions of the &quot;pred-model&quot;, in fact, any time I run the pipeline, Vertex AI creates a new model with the same name.<\/p>\n<p>I would like that at each training, AutoML creates a new version of the same model. E.g., v1, v2, v3.<\/p>\n<p>Here, the current situation, in which the same model is replicated and not versioned:\n<a href=\"https:\/\/i.stack.imgur.com\/zpy2n.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zpy2n.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1662054058413,
        "Question_score":3,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai|mlops",
        "Question_view_count":58,
        "Owner_creation_time":1415266912943,
        "Owner_last_access_time":1664046818297,
        "Owner_location":"Milano, Metropolitan City of Milan, Italy",
        "Owner_reputation":107,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73573487",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70070421,
        "Question_title":"Return confidence score with custom model for Vertex AI batch predictions",
        "Question_body":"<p>I uploaded a pretrained scikit learn classification model to Vertex AI and ran a batch prediction on 5 samples. It just returned a list of false predictions with no confidence score. I don't see anywhere in the SDK documentation or Google console for how to get batch predictions to include the confidence scores. Is that something Vertex AI can do?<\/p>\n<p>My intent is to automate a batch prediction pipeline using the following code.<\/p>\n<pre><code># Predict\n# &quot;csv&quot;, &quot;&quot;bigquery&quot;, &quot;tf-record&quot;, &quot;tf-record-gzip&quot;, or &quot;file-list&quot;\nbatch_prediction_job = model.batch_predict(\n    job_display_name = job_display_name,\n    gcs_source = input_path,\n    instances_format = &quot;&quot;, # jsonl, csv, bigquery, \n    gcs_destination_prefix = output_path,\n    starting_replica_count = 1,\n    max_replica_count = 10,\n    sync = True,\n)\n\nbatch_prediction_job.wait()\n\nreturn batch_prediction_job.resource_name\n<\/code><\/pre>\n<p>I tried it out in google console as a test to make sure my input data was properly formatted.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1637604275697,
        "Question_score":0,
        "Question_tags":"python|scikit-learn|google-cloud-vertex-ai",
        "Question_view_count":319,
        "Owner_creation_time":1417013182680,
        "Owner_last_access_time":1663940014890,
        "Owner_location":"Boston, MA",
        "Owner_reputation":1256,
        "Owner_up_votes":391,
        "Owner_down_votes":2,
        "Owner_views":245,
        "Question_last_edit_time":1637686316647,
        "Answer_body":"<p>I don't think so; the stock sklearn container provided by vertex doesn't provide such a score I guess. You might need to write a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/use-custom-container\" rel=\"nofollow noreferrer\">custom container<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1637647501512,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70070421",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73659986,
        "Question_title":"Vertex pipeline model training component stuck running forever because of metadata issue",
        "Question_body":"<p>I'm attempting to run a Vertex pipeline (custom model training) which I was able to run successfully in a different project. As far as I'm aware, all the pieces of infrastructure (service accounts, buckets, etc.) are identical.<\/p>\n<p>The error appears in a gray box in the pipeline UI when I click on the model training component and reads the following:<\/p>\n<pre><code>Retryable error reported. System is retrying.\ncom.google.cloud.ai.platform.common.errors.AiPlatformException: code=ABORTED, message=Specified Execution `etag`: `1662555654045` does not match server `etag`: `1662555533339`, cause=null System is retrying.\n<\/code><\/pre>\n<p>I've looked into the log explorer and found that the error logs are audit logs have the following associated tags with them:<\/p>\n<p><code>protoPayload.methodName=&quot;google.cloud.aiplatform.internal.MetadataService.RefreshLineageSubgraph&quot;<\/code><\/p>\n<p><code>protoPayload.resourceName=&quot;projects\/724306335858\/locations\/europe-west4\/metadataStores\/default<\/code><\/p>\n<p>Leading me to think that there's an issue with the Vertex Metadatastore or the way my pipeline is using it. The audit logs are automatic though, so I'm not sure.<\/p>\n<p>I've tried purging the metadata store as well as deleting it completely. I've also tried running a different model training pipeline that worked before in a different project as well but with no luck.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/P8ViW.png\" rel=\"nofollow noreferrer\">screenshot of ui<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1662715406920,
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai|mlops|gcp-ai-platform-training|custom-training",
        "Question_view_count":51,
        "Owner_creation_time":1662714633560,
        "Owner_last_access_time":1663324616117,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1662729439980,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73659986",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71214828,
        "Question_title":"Vertex AI 504 Errors in batch job - How to fix\/troubleshoot",
        "Question_body":"<p>We have a Vertex AI model that takes a relatively long time to return a prediction.<\/p>\n<p>When hitting the model endpoint with one instance, things work fine.  But batch jobs of size say 1000 instances end up with around 150 504 errors (upstream request timeout). (We actually need to send batches of 65K but I'm troubleshooting with 1000).<\/p>\n<p>I tried increasing the number of replicas assuming that the # of instances handed to the model would be (1000\/# of replicas) but that doesn't seem to be the case.<\/p>\n<p>I then read that the default batch size is 64 and so tried decreasing the batch size to 4 like this from the python code that creates the batch job:<\/p>\n<p>model_parameters = dict(batch_size=4)<\/p>\n<pre><code>def run_batch_prediction_job(vertex_config):\n\n    aiplatform.init(\n        project=vertex_config.vertex_project, location=vertex_config.location\n    )\n\n    model = aiplatform.Model(vertex_config.model_resource_name)\n\n    model_params = dict(batch_size=4)\n    batch_params = dict(\n        job_display_name=vertex_config.job_display_name,\n        gcs_source=vertex_config.gcs_source,\n        gcs_destination_prefix=vertex_config.gcs_destination,\n        machine_type=vertex_config.machine_type,\n        accelerator_count=vertex_config.accelerator_count,\n        accelerator_type=vertex_config.accelerator_type,\n        starting_replica_count=replica_count,\n        max_replica_count=replica_count,\n        sync=vertex_config.sync,\n        model_parameters=model_params\n    )\n\n    batch_prediction_job = model.batch_predict(**batch_params)\n\n    batch_prediction_job.wait()\n\n    return batch_prediction_job\n<\/code><\/pre>\n<p>I've also tried increasing the machine type to n1-high-cpu-16 and that helped somewhat but I'm not sure I understand how batches are sent to replicas?<\/p>\n<p>Is there another way to decrease the number of instances sent to the model?\nOr is there a way to increase the timeout?\nIs there log output I can use to help figure this out?\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1645492543853,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":194,
        "Owner_creation_time":1462581330170,
        "Owner_last_access_time":1663976709807,
        "Owner_location":"Berkeley, CA, United States",
        "Owner_reputation":329,
        "Owner_up_votes":101,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71214828",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73252066,
        "Question_title":"Query BQ table with Jupyter Notebook across owned projects",
        "Question_body":"<p>I have an issue about acessing data in a BigQuery table in one project using a VertexAI in another project.<\/p>\n<p>Now, I own both project and have service accounts in both project, which implies that I also have the key (credential.json in the project containing the data) which I can use to define my client:<\/p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\n\ncredentials = service_account.Credentials.from_service_account_file('credentials.json')\nproject_id = 'cloud-billing-XXXX'\nclient = bigquery.Client(credentials= credentials,project=project_id)\n<\/code><\/pre>\n<p>which should be enough to run:<\/p>\n<pre><code>%%bigquery\nSELECT * FROM `cloud-billing-XXXX.all_billing_detailed.gcp_billing_export` limit 100\n<\/code><\/pre>\n<p>but I get the error:<\/p>\n<pre><code>ERROR:\n 403 Access Denied: Table cloud-billing-XXXX:all_billing_detailed.gcp_billing_export: User does not have permission to query table cloud-billing-XXXX:all_billing_detailed.gcp_billing_export.\n<\/code><\/pre>\n<p>I can, from the project containing my notebook, query the table in BigQuery. This makes me think that the problem is a VertexAI permission issue. I read somewhere that the service account used when defining the notebook must match the service account in the project the data resides in. I tried to create a workbench notebook with the service account in the first project and it is created but when I try to open it it refuses to do so and get an error message.<\/p>\n<p>I've also tried to grant Editor and job user permissions across both project but that wouldn't work either.<\/p>\n<p>Any experiences and ideas on how to solve this would be greatly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1659713227817,
        "Question_score":1,
        "Question_tags":"google-bigquery|jupyter-notebook|service-accounts|google-cloud-vertex-ai",
        "Question_view_count":67,
        "Owner_creation_time":1442929315877,
        "Owner_last_access_time":1664024443763,
        "Owner_location":"Stockholm, Sverige",
        "Owner_reputation":4598,
        "Owner_up_votes":1855,
        "Owner_down_votes":13,
        "Owner_views":855,
        "Question_last_edit_time":null,
        "Answer_body":"<ol>\n<li>As a mentioned in the comment: add your notebook service account to the first project (which contains your BigQuery data) and grant it with <strong>Bigquery Job User<\/strong> and <strong>BigQuery Data Viewer<\/strong> permissions.<\/li>\n<li>You can query data directly in you python code (without using &quot;magic&quot; %%bigquery). Just add the next two rows:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>pct_overlap_terms_by_days_apart = client.query(&quot;SELECT * FROM `cloud-billing-XXXX.all_billing_detailed.gcp_billing_export` limit 100&quot;).to_dataframe()\npct_overlap_terms_by_days_apart.head()\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Check out table name. If table path is wrong, then you'll get the same error: <strong>403 Access Denied<\/strong>.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1659745527510,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73252066",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73368320,
        "Question_title":"Vertax AI pipeline quota",
        "Question_body":"<p>I got a custom_model_training_cpus error when runing a submitted pipeline on Vertex AI. I could not find any documents. And I am using the n1-standard-4 for the deployment machine, I do not see any issue. Any commnents would be much appriciated.<\/p>\n<blockquote>\n<p>com.google.cloud.ai.platform.common.errors.AiPlatformException: code=RESOURCE_EXHAUSTED, message=The following quota metrics exceed quota limits: aiplatform.googleapis.com\/custom_model_training_cpus, cause=null; Failed to create custom job for the task.<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TgCJD.png\" rel=\"nofollow noreferrer\">DAG flow and error message<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1660618735810,
        "Question_score":3,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":110,
        "Owner_creation_time":1660618422463,
        "Owner_last_access_time":1660874466550,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73368320",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71016472,
        "Question_title":"Vertex AI forecasting AutoML datatype mismatch",
        "Question_body":"<p>I could train the vertex AI AutoML forecating model but when I do batch prediction I get following error<\/p>\n<blockquote>\n<p>Batch prediction job batch_prediction encountered the following\nerrors:<\/p>\n<pre><code>Column &quot;sales&quot; expects type: NUMBER, the actual type is: STRING.\n<\/code><\/pre>\n<\/blockquote>\n<p>Below is a sample of test set I am passing for batch prediction in big query.<\/p>\n<p>According to the documentation for batch prediction we have to send some training\/historical data and forecasting dates. I did just that.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/J7eYT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/J7eYT.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1644227346773,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-ml|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":222,
        "Owner_creation_time":1450288149287,
        "Owner_last_access_time":1661168365350,
        "Owner_location":null,
        "Owner_reputation":500,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Question_last_edit_time":1644237395456,
        "Answer_body":"<p>Google recommend you to use the same input format for ingraining and prediction. Seams you have trained your model using a input format here the column sales were a <code>numeric<\/code> type, and now in the prediction you a using a BigQuery table with the <code>sales<\/code> column as <code>string<\/code>.<\/p>\n<p>Delete this table and import the data again defining the schema <strong>manually<\/strong>, and set sales as a numeric field, as following:<\/p>\n<pre><code>date:DATE,\nstore_product_id:STRING,\nsales:NUMERIC\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1644420672183,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71016472",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69779123,
        "Question_title":"Vertex AI Tensorboard trough user interface",
        "Question_body":"<p>I have been using the Vertex AI training service with a custom container for my own machine learning pipeline. I would like to get tensorboard logs into the experiments tab to see in real-time the metrics while the model is training.<\/p>\n<p>I was wondering if it is possible to set a custom training job in the user interface setting a <code>TENSORBOARD_INSTANCE_NAME<\/code>. It seems that this is only possible through a json-post-request.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1635592687870,
        "Question_score":0,
        "Question_tags":"tensorboard|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":222,
        "Owner_creation_time":1628607500490,
        "Owner_last_access_time":1663182539140,
        "Owner_location":"Colombia",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69779123",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72580712,
        "Question_title":"kubeflow component - why so many ways to define a component and what are the differences?",
        "Question_body":"<p>Please help understand what are the meaningful\/significant differences among different ways to create kubeflow pipeline components and the reason for having so many ways?<\/p>\n<pre><code>from kfp.components import func_to_container_op\n\n@func_to_container_op\ndef add_op(a: float, b: float) -&gt; float:\n    &quot;&quot;&quot;Returns sum of two arguments&quot;&quot;&quot;\n    return a + b\n<\/code><\/pre>\n<pre><code>from kfp.v2.dsl import component\n\n@component\ndef add_op(a: float, b: float) -&gt; float:\n    &quot;&quot;&quot;Returns sum of two arguments&quot;&quot;&quot;\n    return a + b\n<\/code><\/pre>\n<pre><code>from kfp.components import create_component_from_func\n\n@create_component_from_func\ndef add_op(a: float, b: float) -&gt; float:\n    &quot;&quot;&quot;Returns sum of two arguments&quot;&quot;&quot;\n    return a + b\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1654904685283,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":77,
        "Owner_creation_time":1416648155470,
        "Owner_last_access_time":1664057583237,
        "Owner_location":null,
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72580712",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72677822,
        "Question_title":"How do i get the output from a custom container and pass to next pipeline in Vertex AI\/Kubeflow pipeline?",
        "Question_body":"<p>I am having difficulty trying to understand how to pass a result from a container as an output artifact. I understand that we need to write the output to a file but i need some example how to do it.<\/p>\n<p><a href=\"https:\/\/www.kubeflow.org\/docs\/components\/pipelines\/sdk-v2\/component-development\/\" rel=\"nofollow noreferrer\">https:\/\/www.kubeflow.org\/docs\/components\/pipelines\/sdk-v2\/component-development\/<\/a><\/p>\n<p>This is the last part of the python container program where i save the <code>url<\/code> of model file on GCS onto <code>output.txt<\/code>.<\/p>\n<pre><code>with open('.\/output.txt', 'w') as f:\n    logging.info(f&quot;Model path url is in {'.\/output.txt'}&quot;)\n    f.write(model_path)\n<\/code><\/pre>\n<p>This is the component <code>.yaml<\/code> file<\/p>\n<pre><code>name: Dummy Model Training\ndescription: Train a dummy model and save to GCS\ninputs:\n  - name: input_url\n    description: 'Input csv url.'\n    type: String\n  - name: gcs_url\n    description: 'GCS bucket url.'\n    type: String\noutputs:\n  - name: gcs_model_path\n    description: 'Trained model path.'\n    type: String\nimplementation:\n    container:\n        image: ${CONTAINER_REGISTRY}\n        command: [\n          python, .\/app\/trainer.py,\n          --input_url, {inputValue: input_url},\n          --gcs_url, {inputValue: gcs_url},\n        ]\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1655650023283,
        "Question_score":1,
        "Question_tags":"python|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":130,
        "Owner_creation_time":1352353242573,
        "Owner_last_access_time":1663165937033,
        "Owner_location":"Singapore",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72677822",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69540618,
        "Question_title":"Kubeflow, passing Python dataframe across components?",
        "Question_body":"<p>I am writing a Kubeflow component which reads an input query and creates a <code>dataframe<\/code>, roughly as:<\/p>\n<pre><code>from kfp.v2.dsl import component \n\n@component(...)\ndef read_and_write():\n    # read the input query \n    # transform to dataframe \n    sql.to_dataframe()\n<\/code><\/pre>\n<p>I was wondering how I can pass this dataframe to the next operation in my Kubeflow pipeline.\nIs this possible? Or do I have to save the dataframe in a csv or other formats and then pass the output path of this?\nThank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634041967303,
        "Question_score":1,
        "Question_tags":"python|kubeflow|kubeflow-pipelines|tfx|google-cloud-vertex-ai",
        "Question_view_count":879,
        "Owner_creation_time":1624352292607,
        "Owner_last_access_time":1649173178490,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69540618",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72174602,
        "Question_title":"Why do I get 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS error for Vertex AI init?",
        "Question_body":"<p>I am trying to set up mlops for Vertex AI, following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/01-dataset-management.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a>. It works until, near the end, I try:<\/p>\n<pre><code>vertex_ai.init(\nproject=PROJECT,\n    location=REGION)\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code> module 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS\n<\/code><\/pre>\n<p>I am using <code>us-central1<\/code> which is supported. I wondered if maybe <code>from google.cloud import aiplatform as vertex_ai<\/code> has been changed but don't know how to find out. Any help is much appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1652110521003,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":298,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I followed the same Notebook as you, even though I didn't have any issue. What could be happening to you is that you are using an <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/releases\" rel=\"nofollow noreferrer\">older version of the library<\/a>.<\/p>\n<p>You can use the command to upgrade the library that is the following one: <code>pip3 install google-cloud-aiplatform --upgrade<\/code>.<\/p>\n<p>Sometimes this happens with the basic installation of the library; the problems could be in the <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform#installation\" rel=\"nofollow noreferrer\">dependencies, versions and indirectly permissions<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1652193787552,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72174602",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68892701,
        "Question_title":"How to structure container logs in Vertex AI?",
        "Question_body":"<p>I have a model in Vertex AI, from the logs it seems that Vertex AI has ingested the log into <code>message<\/code> field within <code>jsonPayload<\/code> field, but i would like to structure the <code>jsonPayload<\/code> field such that every key in <code>message<\/code> will be a field within <code>jsonPayload<\/code>, i.e: flatten\/extract <code>message<\/code> <a href=\"https:\/\/i.stack.imgur.com\/kpWkj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kpWkj.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1629721604810,
        "Question_score":0,
        "Question_tags":"google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":342,
        "Owner_creation_time":1608712056580,
        "Owner_last_access_time":1663849917143,
        "Owner_location":null,
        "Owner_reputation":496,
        "Owner_up_votes":29,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68892701",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69269073,
        "Question_title":"How to assign two or more time series identifier columns in Vertex AI Tabular Forecasting",
        "Question_body":"<p>I was wondering if it is possible to have more than one time series identifier column in the model? Let's assume I'd like to create a forecast at a product and store level (which the documentation suggests should be possible).<\/p>\n<p>If I select product as the series identifier, the only options I have left for store is either a covariate or an attribute and neither is applicable in this scenario.<\/p>\n<p>Would concatenating product and store and using the individual product and store code values for that concatenated ID as attributes be a solution? It doesn't feel right, but I can't see any other option - am I missing something?<\/p>\n<p>Note: I understand that this feature of Vertex AI is currently in preview and that because of that the options may be limited.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1632228068913,
        "Question_score":2,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":269,
        "Owner_creation_time":1519630645620,
        "Owner_last_access_time":1663095146407,
        "Owner_location":"Northampton, UK",
        "Owner_reputation":333,
        "Owner_up_votes":107,
        "Owner_down_votes":0,
        "Owner_views":46,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There isn't an alternate way to assign 2 or more <strong>Time Series Identifiers<\/strong> in the <strong>Forecasting Model<\/strong> on <strong>Vertex AI<\/strong>. The &quot;<strong>Forecasting model<\/strong>&quot; is in the &quot;<strong>Preview<\/strong>&quot; <a href=\"https:\/\/cloud.google.com\/products#product-launch-stages\" rel=\"nofollow noreferrer\">Product launch stage<\/a>, as you are aware, with all consequences of that fact the options are limited. Please refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/bp-tabular#data_preparation_best_practices\" rel=\"nofollow noreferrer\">doc<\/a> for more information about the best practices for data preparation to train the forecasting model.<\/p>\n<p>As a workaround, the two columns can be concatenated and assigned a Time Series Identifier on that concatenated column, as you have mentioned in the question. This way, the concatenated column carries more contextual information into the training of the model.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1632376096769,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1632482492492,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69269073",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71738221,
        "Question_title":"How to get preprocess\/postprocess steps from model created using Google Vertex AI?",
        "Question_body":"<p>A client of mine wants to run their Google Vertex AI model on NVIDIA Jetson boards using TensorRT as accelerator. The problem with this is that their model uses certain operators (DecodeJpeg) that are not supported by ONNX. I've been able to isolate the feature extrator subgraph from the model, so everything supported by ONNX is being used, while the preprocess and postprocess will be written separate from the model.<\/p>\n<p>I'm asking because I need to be provided the pre\/postprocess of the model so I could implement them separately, so is there a way to get pre\/postprocess from Google Vertex AI console?<\/p>\n<p>I've tried running a loop that rescales the image to a squared tile from 0 to 512, but none of those gave the adequate result.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1649079409933,
        "Question_score":0,
        "Question_tags":"tensorflow|onnx|tensorrt|google-cloud-vertex-ai",
        "Question_view_count":107,
        "Owner_creation_time":1616415228997,
        "Owner_last_access_time":1653485900393,
        "Owner_location":null,
        "Owner_reputation":16,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71738221",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70341414,
        "Question_title":"How to view Vertex AI Matching Engine Deployed Index logs",
        "Question_body":"<p>I have deployed an index in Vertex AI IndexEndpoint. According to the docs for <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1beta1.types.DeployedIndex\" rel=\"nofollow noreferrer\">DeployedIndex<\/a>, I have set the attribute <code>enable_access_logging<\/code> to <code>True<\/code> to enable private endpoints access logs.<\/p>\n<blockquote>\n<p><strong>enable_access_logging<\/strong><br \/>\nOptional. If true, private endpoint's access logs are sent to StackDriver Logging. These logs are like standard server access logs, containing information like timestamp and latency for each MatchRequest. Note that Stackdriver logs may incur a cost, especially if the deployed index receives a high queries per second rate (QPS). Estimate your costs before enabling this option.<\/p>\n<\/blockquote>\n<p>However, in cloud logging I only see Vertex AI audit logs and no access logs. Where can I find logs that contain information for timestamp and latency for each request?<\/p>\n<p>Deployed Index Configuration<\/p>\n<pre><code>createTime: '2021-11-24T10:59:51.975949Z'\ndeployedIndexes:\n- automaticResources:\n    maxReplicaCount: 1\n    minReplicaCount: 1\n  createTime: '2021-12-13T16:37:27.030230Z'\n  deploymentGroup: default\n  displayName: glove_brute_force_deployed_V1\n  enableAccessLogging: true\n  id: glove_brute_force_deployed_V1\n  index: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexes\/XXXXXXXXXXXXXXXXXXX\n  indexSyncTime: '2021-12-13T20:19:00.874937Z'\n  privateEndpoints:\n    matchGrpcAddress: 10.242.0.5\ndisplayName: index_endpoint_for_demo\netag: AMEw9yNMD_AR3V6LIrGln9Ye5PuWWYAOoJwxgSHs2T2Xt8iwAPv1mLOZTfaDMLFTAaBC\nname: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexEndpoints\/XXXXXXXXXXXXXXXXXXX\nnetwork: projects\/XXXXXXXXXXXX\/global\/networks\/NETWORK_ID\nupdateTime: '2021-11-24T10:59:53.271100Z'\n<\/code><\/pre>\n<p>Cloud Logging<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/RN5w9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RN5w9.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639432575870,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-logging|google-cloud-vertex-ai",
        "Question_view_count":290,
        "Owner_creation_time":1463607987530,
        "Owner_last_access_time":1651418190167,
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":143,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Question_last_edit_time":1639484817827,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70341414",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70803996,
        "Question_title":"How to set environment variables for a User-managed notebook in Vertex AI",
        "Question_body":"<p>I am trying to set some environment variables for a user managed notebook in google cloud Vertex AI. I don't want to set this from a jupyter notebook itself because I want these environment variables to be available to anyone who opens a jupyter notebook from this notebook instance. This is what I have tried so far but nothing has worked:<\/p>\n<ol>\n<li>I have an existing user managed notebook. I ssh'd into the notebook vm and then set a environment variable, <code>export TEST_VAR=TEST_VARIABLE_WAS_SET<\/code> there. However, when I open a jupyter notebook from the console and do <code>os.environ[&quot;TEST_VAR&quot;]<\/code>, it gives a key error. So, I am assuming that this has something to do with the fact that the jupyter lab session that Vertex AI starts is in a different shell session or something similar. I also tried to add the following two metadata keys to the vm, and then restarted the vm, but it did not work:<\/li>\n<\/ol>\n<p><code>gcloud compute instances add-metadata ${INSTANCE_NAME} --metadata startup-script-url=$GCS_BUCKET_NAME\/script.sh<\/code><\/p>\n<p>where script.sh is:<\/p>\n<pre><code>#!\/bin\/bash\n\nexport TEST_VAR=TEST_VARIABLE_WAS_SET\n<\/code><\/pre>\n<p>AND<\/p>\n<p><code>gcloud compute instances add-metadata ${INSTANCE_NAME} --metadata container-env-file=$GCS_BUCKET_NAME\/notebook-env.txt<\/code><\/p>\n<p>where notebook-env.txt is<\/p>\n<pre><code>TEST_VAR=TEST_VARIABLE_WAS_SET\n<\/code><\/pre>\n<ol start=\"2\">\n<li>I also tried to create a new instance of a user managed notebook from the cloud console. In that I tried to provide a script in the &quot;Select a script to run after creation&quot; and also through the &quot;Metadata&quot; option by providing, the key as <code>startup-script-url<\/code> and the value as the script location on google cloud storage. The script was the same startup script earlier.<\/li>\n<\/ol>\n<p>So, how do I achieve this, for existing user managed notebooks and when I create new ones?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1642780081007,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|jupyter-notebook|google-ai-platform|google-cloud-vertex-ai|google-notebook",
        "Question_view_count":851,
        "Owner_creation_time":1471292986790,
        "Owner_last_access_time":1664058170413,
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Question_last_edit_time":1642975658732,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70803996",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70296482,
        "Question_title":"AutoML Tables Batch Prediction- Produces an empty table in google big query",
        "Question_body":"<p>I am new to google cloud platform's Vertex AI. In Vertex AI I have created a new batch prediction and would like a Bigquery output table. However when I create the new batch prediction the output table is empty. I am not sure what the issue is. Any advise please?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6jJcD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6jJcD.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1639082614653,
        "Question_score":1,
        "Question_tags":"google-bigquery|prediction|forecasting|google-cloud-vertex-ai",
        "Question_view_count":216,
        "Owner_creation_time":1523972080007,
        "Owner_last_access_time":1661442395577,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1640052565216,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70296482",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70173096,
        "Question_title":"_InactiveRpcError while querying Vertex AI Matching Engine Index",
        "Question_body":"<p>I am following the example <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/community\/matching_engine\/matching_engine_for_indexing.ipynb\" rel=\"nofollow noreferrer\">notebook<\/a> as per GCP <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/matching-engine\/using-matching-engine#example_notebook\" rel=\"nofollow noreferrer\">docs<\/a> to test Vertex Matching Engine. I have deployed an index but while trying to query the index I am getting <code>_InactiveRpcError<\/code>. The VPC network is in <code>us-west2<\/code> with private service access enabled and the Index is deployed in <code>us-central1<\/code>. My VPC network contains the <a href=\"https:\/\/cloud.google.com\/vpc\/docs\/firewalls#more_rules_default_vpc\" rel=\"nofollow noreferrer\">pre-populated firewall rules<\/a>.<\/p>\n<p>Index<\/p>\n<pre><code>createTime: '2021-11-23T15:25:53.928606Z'\ndeployedIndexes:\n- deployedIndexId: brute_force_glove_deployed_v3\n  indexEndpoint: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexEndpoints\/XXXXXXXXXXXX\ndescription: testing python script for creating index\ndisplayName: glove_100_brute_force_20211123152551\netag: AMEw9yOVPWBOTpbAvJLllqxWMi2YurEV_sad2n13QvbIlqjOdMyiq_j20gG1ldhdZNTL\nmetadata:\n  config:\n    algorithmConfig:\n      bruteForceConfig: {}\n    dimensions: 100\n    distanceMeasureType: DOT_PRODUCT_DISTANCE\nmetadataSchemaUri: gs:\/\/google-cloud-aiplatform\/schema\/matchingengine\/metadata\/nearest_neighbor_search_1.0.0.yaml\nname: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexes\/XXXXXXXXXXXX\nupdateTime: '2021-11-23T16:04:17.993730Z'\n<\/code><\/pre>\n<p>Index-Endpoint<\/p>\n<pre><code>createTime: '2021-11-24T10:59:51.975949Z'\ndeployedIndexes:\n- automaticResources:\n    maxReplicaCount: 1\n    minReplicaCount: 1\n  createTime: '2021-11-30T15:16:12.323028Z'\n  deploymentGroup: default\n  displayName: brute_force_glove_deployed_v3\n  enableAccessLogging: true\n  id: brute_force_glove_deployed_v3\n  index: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexes\/XXXXXXXXXXXX\n  indexSyncTime: '2021-11-30T16:37:35.597200Z'\n  privateEndpoints:\n    matchGrpcAddress: 10.242.4.5\ndisplayName: index_endpoint_for_demo\netag: AMEw9yO6cuDfgpBhGVw7-NKnlS1vdFI5nnOtqVgW1ddMP-CMXM7NfGWVpqRpMRPsNCwc\nname: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexEndpoints\/XXXXXXXXXXXX\nnetwork: projects\/XXXXXXXXXXXX\/global\/networks\/XXXXXXXXXXXX\nupdateTime: '2021-11-24T10:59:53.271100Z'\n<\/code><\/pre>\n<p>Code<\/p>\n<pre><code>\nimport grpc\n\n# import the generated classes\nimport match_service_pb2\nimport match_service_pb2_grpc\n\nDEPLOYED_INDEX_SERVER_IP = '10.242.0.5'\nDEPLOYED_INDEX_ID = 'brute_force_glove_deployed_v3'\n\nquery = [-0.11333, 0.48402, 0.090771, -0.22439, 0.034206, -0.55831, 0.041849, -0.53573, 0.18809, -0.58722, 0.015313, -0.014555, 0.80842, -0.038519, 0.75348, 0.70502, -0.17863, 0.3222, 0.67575, 0.67198, 0.26044, 0.4187, -0.34122, 0.2286, -0.53529, 1.2582, -0.091543, 0.19716, -0.037454, -0.3336, 0.31399, 0.36488, 0.71263, 0.1307, -0.24654, -0.52445, -0.036091, 0.55068, 0.10017, 0.48095, 0.71104, -0.053462, 0.22325, 0.30917, -0.39926, 0.036634, -0.35431, -0.42795, 0.46444, 0.25586, 0.68257, -0.20821, 0.38433, 0.055773, -0.2539, -0.20804, 0.52522, -0.11399, -0.3253, -0.44104, 0.17528, 0.62255, 0.50237, -0.7607, -0.071786, 0.0080131, -0.13286, 0.50097, 0.18824, -0.54722, -0.42664, 0.4292, 0.14877, -0.0072514, -0.16484, -0.059798, 0.9895, -0.61738, 0.054169, 0.48424, -0.35084, -0.27053, 0.37829, 0.11503, -0.39613, 0.24266, 0.39147, -0.075256, 0.65093, -0.20822, -0.17456, 0.53571, -0.16537, 0.13582, -0.56016, 0.016964, 0.1277, 0.94071, -0.22608, -0.021106]\n\nchannel = grpc.insecure_channel(&quot;{}:10000&quot;.format(DEPLOYED_INDEX_SERVER_IP))\nstub = match_service_pb2_grpc.MatchServiceStub(channel)\n\nrequest = match_service_pb2.MatchRequest()\nrequest.deployed_index_id = DEPLOYED_INDEX_ID\nfor val in query:\n    request.float_val.append(val)\n\nresponse = stub.Match(request)\nresponse\n<\/code><\/pre>\n<p>Error<\/p>\n<pre><code>_InactiveRpcError                         Traceback (most recent call last)\n\/tmp\/ipykernel_3451\/467153318.py in &lt;module&gt;\n    108     request.float_val.append(val)\n    109 \n--&gt; 110 response = stub.Match(request)\n    111 response\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)\n    944         state, call, = self._blocking(request, timeout, metadata, credentials,\n    945                                       wait_for_ready, compression)\n--&gt; 946         return _end_unary_response_blocking(state, call, False, None)\n    947 \n    948     def with_call(self,\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)\n    847             return state.response\n    848     else:\n--&gt; 849         raise _InactiveRpcError(state)\n    850 \n    851 \n\n_InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\n    status = StatusCode.UNAVAILABLE\n    details = &quot;failed to connect to all addresses&quot;\n    debug_error_string = &quot;{&quot;created&quot;:&quot;@1638277076.941429628&quot;,&quot;description&quot;:&quot;Failed to pick subchannel&quot;,&quot;file&quot;:&quot;src\/core\/ext\/filters\/client_channel\/client_channel.cc&quot;,&quot;file_line&quot;:3093,&quot;referenced_errors&quot;:[{&quot;created&quot;:&quot;@1638277076.941428202&quot;,&quot;description&quot;:&quot;failed to connect to all addresses&quot;,&quot;file&quot;:&quot;src\/core\/lib\/transport\/error_utils.cc&quot;,&quot;file_line&quot;:163,&quot;grpc_status&quot;:14}]}&quot;\n&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638291169620,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|grpc|google-cloud-ml|grpc-python|google-cloud-vertex-ai",
        "Question_view_count":350,
        "Owner_creation_time":1463607987530,
        "Owner_last_access_time":1651418190167,
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":143,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Question_last_edit_time":1639486727367,
        "Answer_body":"<p>Currently, Matching Engine only supports Query from the same region. Can you try running the code from VM in <code>us-central1<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1638293279416,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70173096",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70670669,
        "Question_title":"How do parallel trials in GCP Vertex AI work?",
        "Question_body":"<p>When you make a hyperparameter tuning job, you can specify the number of trials to run in parallel. After that, you also select the type and count of the workers. What I don't understand is when I make two or more trials run in parallel, yet only one worker, each task is said to occupy 100% of the CPU. However, if one task occupies all of the CPU's resources, how can 2 of them run in parallel? Does GCP provision more than 1 machine?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641920680143,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":168,
        "Owner_creation_time":1579801831103,
        "Owner_last_access_time":1663207732260,
        "Owner_location":"Tempe, AZ, USA",
        "Owner_reputation":71,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":null,
        "Answer_body":"<p><strong>Parallel trials<\/strong> allows you to run the trials concurrently depending on your input on the maximum number of trials.<\/p>\n<p>You are correct with your statement &quot;<em>one worker, each task is said to occupy 100% of the CPU<\/em>&quot; and for GCP to run other tasks in parallel,<\/p>\n<blockquote>\n<p>the hyperparameter tuning service provisions multiple training processing clusters (or multiple individual machines in the case of a single-process trainer). The work pool spec that you set for your job is used for each individual training cluster.<\/p>\n<\/blockquote>\n<p>Please see <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-hyperparameter-tuning#parallel-trials\" rel=\"nofollow noreferrer\">Parallel Trials Documentation<\/a> for more details.<\/p>\n<p>And for more details about Hyperparameter Tuning, you may refer to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-hyperparameter-tuning\" rel=\"nofollow noreferrer\">Hyperparameter Tuning Documentation<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1641974198663,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70670669",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69806432,
        "Question_title":"Vertex AI Managed Notebook, get subnet\/IP",
        "Question_body":"<p>How can I find IP for vertex AI managed notebook instance? The service is differing from user managed notebooks in certain sense. The creation of an instance doesn't create a compute instance, so it's all managed by itself.<\/p>\n<p>My purpose is to whitelist the set of IPs in Mongo atlas. Set of IPs being of all the notebooks in that region. I'm using google-managed networks in this case.<\/p>\n<p>I've a few doubts here:<\/p>\n<ul>\n<li>Since within managed nb, I can change CPU consumption, will this reinstantiate a new cluster, with entirely new IP, or it will be 1 from among a group of IPs?<\/li>\n<li>Is it possible to add a custom init script?<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635836251457,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":665,
        "Owner_creation_time":1353151867410,
        "Owner_last_access_time":1663836030397,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":2513,
        "Owner_up_votes":1426,
        "Owner_down_votes":17,
        "Owner_views":251,
        "Question_last_edit_time":1636016613529,
        "Answer_body":"<p>If you want to connect to a database service on GCP, create a network (or use the default) and instantiate the notebook using this network (<code>Advanced options<\/code>) and create the white list for this entire network . It's required because the managed notebook creates a peering network on the network you will use, you can check you in <code>VPC Network<\/code> \u279e <code>VPC Network Peering<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Pd1ui.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Pd1ui.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>If you want an external IP, it will not work. Google managed notebooks <strong>does not use external ips<\/strong>, they basically access the internet via NAT gateways (does not matter if you use google or own managed networks) so you will not be able to do what you want. Move for user managed notebooks (where you can assign a fixed external ip) or white list any IP on your Mongo db service if you are not in a production environment.<\/p>\n<p>About yous doubts:<\/p>\n<blockquote>\n<p>Since within managed nb, I can change CPU consumption, will this instantiate a new cluster, with entirely new IP, or it will be 1 from among a group of IPs<\/p>\n<\/blockquote>\n<p>For the internal network it may change when you restart or recreate the notebook instance. For an external network, it does not exists and explained.<\/p>\n<blockquote>\n<p>Is it possible to add a custom init script?<\/p>\n<\/blockquote>\n<p>Basically not. But you can provide custom docker images for the notebook.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1635870534303,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1635872119870,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69806432",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69219230,
        "Question_title":"In GCP Vertex AI, why is Delete Training Pipeline REST endpoint unimplemented?",
        "Question_body":"<p>I used <a href=\"https:\/\/github.com\/googleapis\/java-aiplatform\/blob\/master\/google-cloud-aiplatform\/src\/main\/java\/com\/google\/cloud\/aiplatform\/v1\/PipelineServiceClient.java\" rel=\"nofollow noreferrer\">this code<\/a>, straight from the Javadocs, to delete a VertexAI Training Pipeline<\/p>\n<pre><code>try (PipelineServiceClient pipelineServiceClient = PipelineServiceClient.create()) {\n  TrainingPipelineName name =\n      TrainingPipelineName.of(&quot;[PROJECT]&quot;, &quot;[LOCATION]&quot;, &quot;[TRAINING_PIPELINE]&quot;);\n  pipelineServiceClient.deleteTrainingPipelineAsync(name).get();\n}\n<\/code><\/pre>\n<p>I get this error. From what I can see, this means that this API, though officially documented, is simply unimplemented. How do we delete Training Pipelines using Java?<\/p>\n<pre><code>Error in deleting \/\/aiplatform.googleapis.com\/projects\/746859988231\/locations\/us-central1\/trainingPipelines\/186468439399187392: \njava.util.concurrent.ExecutionException: \ncom.google.api.gax.rpc.UnimplementedException: io.grpc.StatusRuntimeException:\nUNIMPLEMENTED: HTTP status code 404\n...\n&lt;!DOCTYPE html&gt;\n&lt;html lang=en&gt;\n....\n  &lt;p&gt;The requested URL &lt;code&gt;\/google.cloud.aiplatform.v1.PipelineService\n\/DeleteTrainingPipeline&lt;\/code&gt; was not found on this server.  \n&lt;ins&gt;That\u2019s all we know.&lt;\/ins&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1631862868123,
        "Question_score":2,
        "Question_tags":"java|google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":259,
        "Owner_creation_time":1227171471293,
        "Owner_last_access_time":1664047108080,
        "Owner_location":"Israel",
        "Owner_reputation":17500,
        "Owner_up_votes":463,
        "Owner_down_votes":87,
        "Owner_views":1561,
        "Question_last_edit_time":1631987478696,
        "Answer_body":"<p>According to the official documentation, <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest<\/a> , the supported service URLs for this service are:<\/p>\n<pre><code>https:\/\/us-central1-aiplatform.googleapis.com\nhttps:\/\/us-east1-aiplatform.googleapis.com\nhttps:\/\/us-east4-aiplatform.googleapis.com\nhttps:\/\/us-west1-aiplatform.googleapis.com\nhttps:\/\/northamerica-northeast1-aiplatform.googleapis.com\nhttps:\/\/europe-west1-aiplatform.googleapis.com\nhttps:\/\/europe-west2-aiplatform.googleapis.com\nhttps:\/\/europe-west4-aiplatform.googleapis.com\nhttps:\/\/asia-east1-aiplatform.googleapis.com\nhttps:\/\/asia-northeast1-aiplatform.googleapis.com\nhttps:\/\/asia-northeast3-aiplatform.googleapis.com\nhttps:\/\/asia-southeast1-aiplatform.googleapis.com\nhttps:\/\/australia-southeast1-aiplatform.googleapis.com\n<\/code><\/pre>\n<hr \/>\n<pre><code>  PipelineServiceSettings pipelineServiceSettings =\n        PipelineServiceSettings.newBuilder()\n            .setEndpoint(&quot;us-central1-aiplatform.googleapis.com:443&quot;)\n            .build();\n<\/code><\/pre>\n<hr \/>\n<pre><code> try (PipelineServiceClient pipelineServiceClient =\n      PipelineServiceClient.create(pipelineServiceSettings)) {\n\n  String location = &quot;us-central1&quot;;\n  TrainingPipelineName trainingPipelineName =\n      TrainingPipelineName.of(project, location, trainingPipelineId);\n\n  OperationFuture&lt;Empty, DeleteOperationMetadata&gt; operationFuture =\n      pipelineServiceClient.deleteTrainingPipelineAsync(trainingPipelineName);\n\n  System.out.format(&quot;Operation name: %s\\n&quot;, operationFuture.getInitialFuture().get().getName());\n  System.out.println(&quot;Waiting for operation to finish...&quot;);\n  operationFuture.get(300, TimeUnit.SECONDS);\n\n  System.out.format(&quot;Deleted Training Pipeline.&quot;);\n}\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1631875853752,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1631888060710,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69219230",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73586832,
        "Question_title":"How to Access Managed Dataset in Vertex AI using Custom Container",
        "Question_body":"<p>In the google cloud documentation below:<\/p>\n<p><a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-managed-datasets#access_a_dataset_from_your_training_application\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-managed-datasets#access_a_dataset_from_your_training_application<\/a><\/p>\n<p>It says that the following environment variables are sent to the training container:<\/p>\n<pre><code>AIP_DATA_FORMAT: The format that your dataset is exported in. Possible values include: jsonl, csv, or bigquery.\nAIP_TRAINING_DATA_URI: The location that your training data is stored at.\nAIP_VALIDATION_DATA_URI: The location that your validation data is stored at.\nAIP_TEST_DATA_URI: The location that your test data is stored at.\n<\/code><\/pre>\n<p>Where each of the URI values are wildcards that annotate training, validation, and test data files in <code>.jsonl<\/code> format as such:<\/p>\n<pre><code>gs:\/\/bucket_name\/path\/training-*\ngs:\/\/bucket_name\/path\/validation-*\ngs:\/\/bucket_name\/path\/test-*\n<\/code><\/pre>\n<p><strong>Now, in your custom container that contains the python code, how do you actually access the contents of each of the files?<\/strong><\/p>\n<p>I've tried splitting the URI string using the following regex to obtain the <code>bucket_name<\/code> and the <code>prefix<\/code> info, and attempted the grab it using <code>bucket.list_blobs(delimiter='\/', prefix=prefix[:-1])<\/code> but it returns nothing when the files are definitely there. Here is a minimal example of the attempted code:<\/p>\n<pre><code>import os\nimport re\nfrom google.cloud import storage\n\naip_training_data_uri = os.environ.get('AIP_TRAINING_DATA_URI')\nmatch = re.match('gs:\/\/(.*?)\/(.*)', aip_training_data_uri)\nbucket_name, prefix = match.groups()\n\nclient = storage.Client()\nbucket = client.bucket(bucket_name)\nblobs = bucket.list_blobs(delimiter='\/', prefix=prefix[:-1]) # &quot;[:-1]&quot; to remove wildcard asterisks\n\nfor blob in blobs:\n   print(blob.download_as_string()) # This returns an empty string\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1662144278260,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":41,
        "Owner_creation_time":1516951090470,
        "Owner_last_access_time":1663790014450,
        "Owner_location":null,
        "Owner_reputation":705,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73586832",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70968460,
        "Question_title":"ModelUploadOp step failing with custom prediction container",
        "Question_body":"<p>I am currenlty trying to deploy a Vertex pipeline to achieve the following:<\/p>\n<ol>\n<li><p>Train a custom model (from a custom training python package) and dump model artifacts (trained model and data preprocessor that will be sed at prediction time). This is step is working fine as I can see new resources being created in the storage bucket.<\/p>\n<\/li>\n<li><p>Create a model resource via <code>ModelUploadOp<\/code>. This step fails for some reason when specifying <code>serving_container_environment_variables<\/code> and <code>serving_container_ports<\/code> with the error message in the <strong>errors<\/strong> section below. This is somewhat surprising as they are both needed by the prediction container and environment variables are passed as a dict as specified in the documentation.<br \/>\nThis step works just fine using <code>gcloud<\/code> commands:<\/p>\n<\/li>\n<\/ol>\n<pre class=\"lang-sh prettyprint-override\"><code>gcloud ai models upload \\\n    --region us-west1 \\\n    --display-name session_model_latest \\\n    --container-image-uri gcr.io\/and-reporting\/pred:latest \\\n    --container-env-vars=&quot;MODEL_BUCKET=ml_session_model&quot; \\\n    --container-health-route=\/\/health \\\n    --container-predict-route=\/\/predict \\\n    --container-ports=5000\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Create an endpoint.<\/li>\n<li>Deploy the model to the endpoint.<\/li>\n<\/ol>\n<p>There is clearly something that I am getting wrong with Vertex, the components <a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.2.2\/index.html\" rel=\"nofollow noreferrer\">documentation<\/a> doesn't help much in this case.<\/p>\n<h2>Pipeline<\/h2>\n<pre class=\"lang-py prettyprint-override\"><code>from datetime import datetime\n\nimport kfp\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom kfp.v2 import compiler\n\nPIPELINE_ROOT = &quot;gs:\/\/ml_model_bucket\/pipeline_root&quot;\n\n\n@kfp.dsl.pipeline(name=&quot;session-train-deploy&quot;, pipeline_root=PIPELINE_ROOT)\ndef pipeline():\n    training_op = gcc_aip.CustomPythonPackageTrainingJobRunOp(\n        project=&quot;my-project&quot;,\n        location=&quot;us-west1&quot;,\n        display_name=&quot;train_session_model&quot;,\n        model_display_name=&quot;session_model&quot;,\n        service_account=&quot;name@my-project.iam.gserviceaccount.com&quot;,\n        environment_variables={&quot;MODEL_BUCKET&quot;: &quot;ml_session_model&quot;},\n        python_module_name=&quot;trainer.train&quot;,\n        staging_bucket=&quot;gs:\/\/ml_model_bucket\/&quot;,\n        base_output_dir=&quot;gs:\/\/ml_model_bucket\/&quot;,\n        args=[\n            &quot;--gcs-data-path&quot;,\n            &quot;gs:\/\/ml_model_data\/2019-Oct_short.csv&quot;,\n            &quot;--gcs-model-path&quot;,\n            &quot;gs:\/\/ml_model_bucket\/model\/model.joblib&quot;,\n            &quot;--gcs-preproc-path&quot;,\n            &quot;gs:\/\/ml_model_bucket\/model\/preproc.pkl&quot;,\n        ],\n        container_uri=&quot;us-docker.pkg.dev\/vertex-ai\/training\/scikit-learn-cpu.0-23:latest&quot;,\n        python_package_gcs_uri=&quot;gs:\/\/ml_model_bucket\/trainer-0.0.1.tar.gz&quot;,\n        model_serving_container_image_uri=&quot;gcr.io\/my-project\/pred&quot;,\n        model_serving_container_predict_route=&quot;\/predict&quot;,\n        model_serving_container_health_route=&quot;\/health&quot;,\n        model_serving_container_ports=[5000],\n        model_serving_container_environment_variables={\n            &quot;MODEL_BUCKET&quot;: &quot;ml_model_bucket\/model&quot;\n        },\n    )\n\n    model_upload_op = gcc_aip.ModelUploadOp(\n        project=&quot;and-reporting&quot;,\n        location=&quot;us-west1&quot;,\n        display_name=&quot;session_model&quot;,\n        serving_container_image_uri=&quot;gcr.io\/my-project\/pred:latest&quot;,\n        # When passing the following 2 arguments this step fails...\n        serving_container_environment_variables={&quot;MODEL_BUCKET&quot;: &quot;ml_model_bucket\/model&quot;},\n        serving_container_ports=[5000],\n        serving_container_predict_route=&quot;\/predict&quot;,\n        serving_container_health_route=&quot;\/health&quot;,\n    )\n    model_upload_op.after(training_op)\n\n    endpoint_create_op = gcc_aip.EndpointCreateOp(\n        project=&quot;my-project&quot;,\n        location=&quot;us-west1&quot;,\n        display_name=&quot;pipeline_endpoint&quot;,\n    )\n\n    model_deploy_op = gcc_aip.ModelDeployOp(\n        model=model_upload_op.outputs[&quot;model&quot;],\n        endpoint=endpoint_create_op.outputs[&quot;endpoint&quot;],\n        deployed_model_display_name=&quot;session_model&quot;,\n        traffic_split={&quot;0&quot;: 100},\n        service_account=&quot;name@my-project.iam.gserviceaccount.com&quot;,\n    )\n    model_deploy_op.after(endpoint_create_op)\n\n\nif __name__ == &quot;__main__&quot;:\n    ts = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n    compiler.Compiler().compile(pipeline, &quot;custom_train_pipeline.json&quot;)\n    pipeline_job = aiplatform.PipelineJob(\n        display_name=&quot;session_train_and_deploy&quot;,\n        template_path=&quot;custom_train_pipeline.json&quot;,\n        job_id=f&quot;session-custom-pipeline-{ts}&quot;,\n        enable_caching=True,\n    )\n    pipeline_job.submit()\n\n<\/code><\/pre>\n<h3>Errors and notes<\/h3>\n<ol>\n<li>When specifying <code>serving_container_environment_variables<\/code> and <code>serving_container_ports<\/code> the step fails with the following error:<\/li>\n<\/ol>\n<pre><code>{'code': 400, 'message': 'Invalid JSON payload received. Unknown name &quot;MODEL_BUCKET&quot; at \\'model.container_spec.env[0]\\': Cannot find field.\\nInvalid value at \\'model.container_spec.ports[0]\\' (type.googleapis.com\/google.cloud.aiplatform.v1.Port), 5000', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com\/google.rpc.BadRequest', 'fieldViolations': [{'field': 'model.container_spec.env[0]', 'description': 'Invalid JSON payload received. Unknown name &quot;MODEL_BUCKET&quot; at \\'model.container_spec.env[0]\\': Cannot find field.'}, {'field': 'model.container_spec.ports[0]', 'description': &quot;Invalid value at 'model.container_spec.ports[0]' (type.googleapis.com\/google.cloud.aiplatform.v1.Port), 5000&quot;}]}]}\n<\/code><\/pre>\n<p>When commenting out <code>serving_container_environment_variables<\/code> and <code>serving_container_ports<\/code>  the model resource gets created but deploying it manually to the endpoint results into a failed deployment with no output logs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1643879294320,
        "Question_score":0,
        "Question_tags":"python|google-cloud-ml|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":273,
        "Owner_creation_time":1512301540763,
        "Owner_last_access_time":1663934781440,
        "Owner_location":"Milan, Italy",
        "Owner_reputation":1427,
        "Owner_up_votes":93,
        "Owner_down_votes":16,
        "Owner_views":113,
        "Question_last_edit_time":1644239389000,
        "Answer_body":"<p>After some time researching the problem I've stumbled upon <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/6848\" rel=\"nofollow noreferrer\">this<\/a> Github issue. The problem was originated by a mismatch between <a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.2.2\/index.html\" rel=\"nofollow noreferrer\"><code>google_cloud_pipeline_components<\/code><\/a> and <a href=\"https:\/\/kubernetes.io\/docs\/reference\/generated\/kubernetes-api\/v1.19\/#envvar-v1-core\" rel=\"nofollow noreferrer\"><code>kubernetes_api<\/code><\/a> docs. In this case, <code>serving_container_environment_variables<\/code> is typed as an <code>Optional[dict[str, str]]<\/code> whereas it should have been typed as a <code>Optional[list[dict[str, str]]]<\/code>. A similar mismatch can be found for <code>serving_container_ports<\/code> argument as well. Passing arguments following kubernetes documentation did the trick:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_upload_op = gcc_aip.ModelUploadOp(\n    project=&quot;my-project&quot;,\n    location=&quot;us-west1&quot;,\n    display_name=&quot;session_model&quot;,\n    serving_container_image_uri=&quot;gcr.io\/my-project\/pred:latest&quot;,\n    serving_container_environment_variables=[\n        {&quot;name&quot;: &quot;MODEL_BUCKET&quot;, &quot;value&quot;: &quot;ml_session_model&quot;}\n    ],\n    serving_container_ports=[{&quot;containerPort&quot;: 5000}],\n    serving_container_predict_route=&quot;\/predict&quot;,\n    serving_container_health_route=&quot;\/health&quot;,\n)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1643965835692,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70968460",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71158453,
        "Question_title":"module 'google.cloud.logging_v2' has no attribute 'MetricsServiceV2Client' Vertex WorkBench",
        "Question_body":"<p>I am trying to set up a locust based framework for ML load test and need to create custom metrics and logs for which the example that I am following is using 'MetricsServiceV2Client' in 'google.cloud.logging_v2' lib.\nIn the Vertex Workbench on GCP inspite being on v3.0 of the google-cloud-logging lib I am getting an issue of import<\/p>\n<p>from google.cloud import logging_v2\nfrom google.cloud.logging_v2 import MetricsServiceV2Client<\/p>\n<p>error: cannot import name 'MetricsServiceV2Client' from 'google.cloud.logging_v2' (\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/logging_v2\/<strong>init<\/strong>.py)<\/p>\n<p>Interestingly when I test the import in  google's cloud console I am able to import without any issue. What could be the issue ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645102105297,
        "Question_score":0,
        "Question_tags":"google-cloud-logging|google-cloud-vertex-ai",
        "Question_view_count":68,
        "Owner_creation_time":1615961068170,
        "Owner_last_access_time":1664041311570,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>from google.cloud.logging_v2.services.metrics_service_v2 import MetricsServiceV2Client this works !!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1645126649432,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71158453",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72094768,
        "Question_title":"How to get the status of a pipeline run within a component, running on Vertex AI?",
        "Question_body":"<p>Previously, using Kubeflow Pipelines SDK v1, the status of a pipeline could be inferred during pipeline execution by passing an Argo placeholder, <code>{{workflow.status}}<\/code>, to the component, as shown below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import kfp.dsl as dsl\n\ncomponent_1 = dsl.ContainerOp(\n    name='An example component',\n    image='eu.gcr.io\/...\/my-component-img',\n    arguments=[\n               'python3', 'main.py',\n               '--status', &quot;{{workflow.status}}&quot;\n              ]\n)\n<\/code><\/pre>\n<p>This placeholder would take the value <code>Succeeded<\/code> or <code>Failed<\/code> when passed to the component. One use-case for this would be to send a failure-warning to eg. Slack, in combination with <code>dsl.ExitHandler<\/code>.<\/p>\n<p>However, when using Pipeline SDK version 2, <code>kfp.v2<\/code>, together with Vertex AI to compile and run the pipeline the Argo placeholders no longer work, as described by <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/7614\" rel=\"nofollow noreferrer\">this open issue<\/a>. Because of this, I would need another way to check the status of the pipeline within the component. I was thinking I could use the <code>kfp.Client<\/code> <a href=\"https:\/\/kubeflow-pipelines.readthedocs.io\/en\/latest\/source\/kfp.client.html\" rel=\"nofollow noreferrer\">class<\/a>, but I'm assuming this won't work using Vertex AI, since there is no &quot;host&quot; really. Also, there seems to be supported placeholders for to pass the run id (<code>dsl.PIPELINE_JOB_ID_PLACEHOLDER<\/code>) as a placeholder, as per <a href=\"https:\/\/stackoverflow.com\/questions\/68348026\/run-id-in-kubeflow-pipelines-on-vertex-ai\">this SO post<\/a>, but I can't find anything around <code>status<\/code>.<\/p>\n<p>Any ideas how to get the status of a pipeline run within a component, running on Vertex AI?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1651551486133,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai|kubeflow-pipelines|argoproj|kfp",
        "Question_view_count":520,
        "Owner_creation_time":1562750927333,
        "Owner_last_access_time":1663926437127,
        "Owner_location":"Stockholm, Sverige",
        "Owner_reputation":803,
        "Owner_up_votes":123,
        "Owner_down_votes":7,
        "Owner_views":73,
        "Question_last_edit_time":1654867910903,
        "Answer_body":"<p>Each pipeline run is automatically logged to Google Logging, and so are also the failed pipeline runs.\nThe error logs also contain information about the pipeline and the component that failed.<\/p>\n<p>We can use this information to monitor our logs and set up an alert via email for example.<\/p>\n<p>The logs for our Vertex AI Pipeline runs we get with the following filter<\/p>\n<p>resource.type=\u201daiplatform.googleapis.com\/PipelineJob\u201d\nseverity=(ERROR OR CRITICAL OR ALERT OR EMERGENCY)<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/e4jFR.png\" rel=\"nofollow noreferrer\">Vertex AI Pipeline Logs<\/a><\/p>\n<p>Based on those logs you can set up log-based alerts <a href=\"https:\/\/cloud.google.com\/logging\/docs\/alerting\/log-based-alerts\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/logging\/docs\/alerting\/log-based-alerts<\/a>. Notifications via email, Slack, SMS, and many more are possible.<\/p>\n<p>source:\n<a href=\"https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655447541936,
        "Answer_score":1.0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72094768",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70838510,
        "Question_title":"Is it possible to request a Vertex AI endpoint from another GCP project?",
        "Question_body":"<p>I trained a model on GCP Vertex AI, and deployed it on an endpoint.<\/p>\n<p>I am able to execute predictions from a sample to my model with this python code <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python<\/a><\/p>\n<p>It works within my GCP project.<\/p>\n<p>My question is, is it possible to request this endpoint from another GCP project ? If I set a service account and set IAM role in both projects ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643047926007,
        "Question_score":0,
        "Question_tags":"python-3.x|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":417,
        "Owner_creation_time":1598873976143,
        "Owner_last_access_time":1663530379503,
        "Owner_location":"Versailles, France",
        "Owner_reputation":140,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes it is possible. For example you have Project A and Project B, assuming that Project A hosts the model.<\/p>\n<ul>\n<li><p>Add service account of Project B in Project A and provide at least <code>roles\/aiplatform.user<\/code> predefined role. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/access-control#predefined-roles\" rel=\"nofollow noreferrer\">predefined roles<\/a> and look for <code>roles\/aiplatform.user<\/code> to see complete roles it contains.<\/p>\n<\/li>\n<li><p>This role contains <strong>aiplatform.endpoints.<\/strong>* and <strong>aiplatform.batchPredictionJobs.<\/strong>* as these are the roles needed to run predictions.<\/p>\n<blockquote>\n<p>See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/iam-permissions\" rel=\"nofollow noreferrer\">IAM permissions for Vertex AI<\/a><\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Resource<\/th>\n<th>Operation<\/th>\n<th>Permissions needed<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>batchPredictionJobs<\/td>\n<td>Create a batchPredictionJob<\/td>\n<td>aiplatform.batchPredictionJobs.create (permission needed on the parent resource)<\/td>\n<\/tr>\n<tr>\n<td>endpoints<\/td>\n<td>Predict an endpoint<\/td>\n<td>aiplatform.endpoints.predict (permission needed on the endpoint resource)<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div><\/blockquote>\n<\/li>\n<\/ul>\n<p>With this set up, Project B will be able to use the model in Project A to run predictions.<\/p>\n<p>NOTE: Just make sure that the script of Project B points to the resources in Project A like <code>project_id<\/code> and <code>endpoint_id<\/code>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1643080918689,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1643081751060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70838510",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71600291,
        "Question_title":"GPU quotas increased but error on the excess of my quota",
        "Question_body":"<p>I try to make a managed notebook in Vertex AI. I requested and obtained an increase in my quota but when I create the notebook it keeps giving me error on the excess of my quota. I don't know why.<\/p>\n<p>&quot;QUOTA_EXCEEDED error message&quot;<\/p>\n<p>As required:\n<a href=\"https:\/\/i.stack.imgur.com\/M2o4f.png\" rel=\"nofollow noreferrer\">The error is about the exceed of my quotas of Nvidia GPU_P100<\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/OZxfb.png\" rel=\"nofollow noreferrer\">A you can see i have a quota of 2 GPU_P100<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1648115810510,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|gpu|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":68,
        "Owner_creation_time":1517346608423,
        "Owner_last_access_time":1664035493327,
        "Owner_location":null,
        "Owner_reputation":86,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1648135240783,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71600291",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73434003,
        "Question_title":"Read images from a bucket in GCP for ML",
        "Question_body":"<p>I have created a bucket in GCP containing my images dataset.<\/p>\n<p>The path to it is: xray-competition-bucket\/img_align_celeba<\/p>\n<p>How do I read it from GCP to Jupyter Lab in Vertex AI?<\/p>\n<p>My code is:<\/p>\n<pre><code>MAIN_PATH = '\/gcs\/xray-competition-bucket\/img_align_celeba'\n\nimage_paths = glob((MAIN_PATH + &quot;\/*.jpg&quot;))\n<\/code><\/pre>\n<p>and the result is that image_paths is an empty array.<\/p>\n<p>Note: I also tried the path gs:\/\/my_bucket\/...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1661081410723,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|jupyter-lab|bucket|google-cloud-vertex-ai",
        "Question_view_count":79,
        "Owner_creation_time":1639312590127,
        "Owner_last_access_time":1663617607140,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You will need to <a href=\"https:\/\/cloud.google.com\/storage\/docs\/downloading-objects#storage-download-object-python\" rel=\"nofollow noreferrer\">download the GCS file locally<\/a> using <code>gsutil<\/code> or the python SDK if you want to use glob. There are also libraries like <a href=\"https:\/\/gcsfs.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">GCSFS<\/a> or TensorFlow's <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/gfile\/GFile\" rel=\"nofollow noreferrer\">GFile<\/a> which offer a pythonic file-system interface for working with GCS. For example, here is <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/gfile\/glob\" rel=\"nofollow noreferrer\">GFile.glob<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1661266806083,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73434003",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71300217,
        "Question_title":"Not able to run gcloud run deploy from Vertex AI Pipelines",
        "Question_body":"<p>I have been trying to deploy a model to cloud run but I am continuously getting a permission error. Tried using all the available suggestions and answers but had no success. Finally gave the owner access to the service accounts but no success.<\/p>\n<p>screenshot of what I am trying to do.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/8AUzB.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8AUzB.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>the code<\/p>\n<pre><code>storage_client = storage.Client()\n\nbucket = storage_client.get_bucket(bucket_name)\nblobs = bucket.list_blobs(prefix=prefix)  # Get list of files\nfor blob in blobs:\n    filename = blob.name\n    if not filename.endswith(&quot;\/&quot;):\n        blob.download_to_filename(dl_dir+filename.split(&quot;\/&quot;)[-1])  # Download\n\nbucket = storage_client.get_bucket(tar_path.split(&quot;\/&quot;)[0])\nblob = bucket.blob(&quot;\/&quot;.join(tar_path.split(&quot;\/&quot;)[1:]))\nblob.download_to_filename(&quot;\/tmp\/model_serving\/model.pth.tar&quot;)\n\nos.chdir(&quot;\/tmp\/model_serving\/&quot;)\nprint(&quot;############### &quot;, os.listdir())\n\nos.system(&quot;gcloud run deploy --port 5050 --allow-unauthenticated --region us-central1 darts-rnn-from-pipeline --source .&quot;)   \n<\/code><\/pre>\n<p>error I am getting<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/tjc72.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tjc72.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I am able to run the command outside the pipeline without any errors.\nI am also able to run the storage and bq clients inside the pipeline without any errors.\nBut only this command I am not able to run.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1646077052303,
        "Question_score":1,
        "Question_tags":"python-3.x|google-cloud-platform|google-cloud-run|google-cloud-iam|google-cloud-vertex-ai",
        "Question_view_count":104,
        "Owner_creation_time":1444454434270,
        "Owner_last_access_time":1652071645660,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":140,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Question_last_edit_time":1646112654983,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71300217",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73727583,
        "Question_title":"IDtoken retrieval in Vertex AI pipeline fails randomly",
        "Question_body":"<p>In vertex AI pipelines we have created a custom component that trigger a cloud run.\nIn order to do so we need to fetch the Id Token Credentials of the running identity.\nThis work most of the time but fails randomly every 5 to 10 hours.<\/p>\n<p>here is our code :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>auth_req = google.auth.transport.requests.Request()\nid_token = google.oauth2.id_token.fetch_id_token(auth_req, url_dbt_server)\nreturn id_token\n<\/code><\/pre>\n<p>the error message is:<\/p>\n<pre><code>google.auth.exceptions.RefreshError: (&quot;Failed to retrieve http:\/\/metadata.google.internal\/computeMetadata\/v1\/instance\/service-accounts\/default\/identity?audience=&lt;Our audience&gt;&amp;format=full from the Google Compute Enginemetadata service. Status: 404 Response:\\nb'Not Found\\\\n'&quot;, &lt;google.auth.transport.requests._Response object at 0x7f8f0d78aca0&gt;)\n<\/code><\/pre>\n<p>How can I solve (or at least go around) this problem?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663229049140,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-run|google-cloud-vertex-ai",
        "Question_view_count":16,
        "Owner_creation_time":1663227968907,
        "Owner_last_access_time":1663943402967,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1663229581172,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73727583",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73130582,
        "Question_title":"How to query \/ flatten from vertex ml results saved to bigquery",
        "Question_body":"<p>This is driving me bonkers so any help greatly appreciated.<\/p>\n<p>I am using Google's Vertex ML. I have exported a batch prediction to BigQuery.<\/p>\n<p>The schema is I believe a record with repeat fields.<\/p>\n<p>So I think it would like this in JSON:<\/p>\n<pre><code>[{&quot;category&quot;:true,&quot;score&quot;:.9999},{&quot;category&quot;:false,&quot;score&quot;,.05}]\n<\/code><\/pre>\n<p>I can not figure out how to either unnest or narrow a search where a category is true.<\/p>\n<p>I need to have a flat select that has the correct category column and score value<\/p>\n<pre><code>123 | true | .9999\n123 | false | .05\n<\/code><\/pre>\n<p>or a select with a where clause to only get true values<\/p>\n<pre><code>123 | .9999\n<\/code><\/pre>\n<p>The following unnests everything but it creates four rows joining both the true and false to both the scores.<\/p>\n<pre><code>SELECT\n  row_id,\n  classes,\n  scores\nFROM\n  `database`\ncross JOIN\n  UNNEST(exported.classes) AS classes,\n  UNNEST(exported.scores) AS scores\nLIMIT\n  10\n<\/code><\/pre>\n<p>creates rows like:<\/p>\n<pre><code>123 | true | .9999\n123 | false | .9999\n123 | true | .05\n123 | false | .05\n<\/code><\/pre>\n<p>This does select the values I need but it's still a nested field...<\/p>\n<pre><code>select\nrow_id,\nclasses.classes,\nclasses.scores\nfrom (\nSELECT\n  voter_id,\n  ARRAY_CONCAT([predicted_results]) as the_results\nFROM\n  `data`\nLIMIT\n  10\n),\nunnest(the_results) as classes\n<\/code><\/pre>\n<p>creates rows like<\/p>\n<pre><code>123 | [true:.9999,false:.05]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1658876367830,
        "Question_score":0,
        "Question_tags":"sql|google-bigquery|flatten|google-cloud-vertex-ai",
        "Question_view_count":104,
        "Owner_creation_time":1282610737523,
        "Owner_last_access_time":1663981942420,
        "Owner_location":null,
        "Owner_reputation":185,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":37,
        "Question_last_edit_time":1659331589369,
        "Answer_body":"<pre><code>select \n primary_key, \n  predicted_supports.classes[SAFE_OFFSET(index)] as class,\n  predicted_supports.scores[SAFE_OFFSET(index)] as score,\nFROM `database`,\nunnest(generate_array(0,array_length(predicted_supports.classes)-1)) as index\n<\/code><\/pre>\n<p>Here is the ouput:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jVpOe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jVpOe.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1659415712443,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1659486136643,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73130582",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71763407,
        "Question_title":"Set artifact name when using kfp dsl.importer",
        "Question_body":"<p>When importing an artifact using the kfp <code>dsl.importer()<\/code> function, the imported artifact gets the default (display) name <code>artifact<\/code>. I would like to give it a custom name to make the pipeline and lineage tracking more clear. I checked the <a href=\"https:\/\/kubeflow-pipelines.readthedocs.io\/en\/latest\/source\/kfp.dsl.html#kfp.dsl.importer\" rel=\"nofollow noreferrer\">documentation<\/a>, but I can't seem to find a way to change the name of the artifact that the <code>dsl.importer()<\/code> function produces.<\/p>\n<p>Example code <code>dsl.importer()<\/code>:<\/p>\n<pre><code>    load_dataset_step = dsl.importer(\n        artifact_uri=input_data_uri,\n        artifact_class=dsl.Dataset,\n        reimport=False\n    ).set_display_name(&quot;Load Dataset&quot;)\n<\/code><\/pre>\n<p>Visualisation of the <code>dsl.importer()<\/code> step:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/b4Qx6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/b4Qx6.png\" alt=\"pipelines visualisation\" \/><\/a><\/p>\n<p>I'm making use of Google Cloud Vertex AI Pipelines.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_time":1649233510080,
        "Question_score":1,
        "Question_tags":"python|kubeflow|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":158,
        "Owner_creation_time":1353508925387,
        "Owner_last_access_time":1663854791917,
        "Owner_location":"Belgium",
        "Owner_reputation":383,
        "Owner_up_votes":100,
        "Owner_down_votes":8,
        "Owner_views":132,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71763407",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71990757,
        "Question_title":"How can GCP Automl handle overfitting?",
        "Question_body":"<p>I have created a Vertex AI AutoML image classification model. How can I assess it for overfitting? I assume I should be able to compare training vs validation accuracy but these do not seem to be <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/evaluating-automl-models\" rel=\"nofollow noreferrer\">available<\/a>.<\/p>\n<p>And if it is overfitting,can I tweak regularization parameters? Is it already doing cross validation? Anything else that can be done? (More data,early stopping, dropouts ie how can these be done?)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650821429613,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":27,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Deploy it to endpoint and test result with sample images by uploading to endpoint. If it's overfitting you can see the stats in analysis. You can increase the training sample and retrain your model again to get better result.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1650850146067,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71990757",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69658459,
        "Question_title":"Jobs-Cloud Scheduler (Google Cloud) fails to run scheduled pipelines",
        "Question_body":"<p>I'm here because I'm facing a problem with scheduled jobs in Google Cloud.\nIn Vertex AI Workbench, I created a notebook in Python 3 that creates a pipeline that trains AutoML with data from the public credit card dataset.\nIf I run the job at the end of its creation, everything works. However, if I schedule the job run <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/run-pipeline#scheduling_a_recurring_pipeline_run_using_the\" rel=\"nofollow noreferrer\">as described here<\/a> in Job Cloud Scheduler, the pipeline is enabled but the run fails.<\/p>\n<p>Here is the code that I have:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\n# import sys\nimport google.cloud.aiplatform as aip\nimport kfp\n# from kfp.v2.dsl import component\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom kfp.v2 import compiler\n# from kfp.v2.google.client import AIPlatformClient\n\nPROJECT_ID = &quot;fraud-detection-project-329506&quot;\nREGION = &quot;us-central1&quot;\n\ncredential_path = r&quot;C:\\Users\\...\\fraud-detection-project-329506-4d16889a494a.json&quot;\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n      \nBUCKET_NAME = &quot;gs:\/\/...&quot;\nSERVICE_ACCOUNT = &quot;...@fraud-detection-project-329506.iam.gserviceaccount.com&quot;\n\nAPI_ENDPOINT = &quot;{}-aiplatform.googleapis.com&quot;.format(REGION)\nPIPELINE_ROOT = &quot;{}\/dataset&quot;.format(BUCKET_NAME)\n\naip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)\n\n# file names\nTRAIN_FILE_NAME = &quot;creditcard_train.csv&quot;\nTEST_FILE_NAME = &quot;creditcard_test.csv&quot;\n\n# path for train and test dataset \ngcs_csv_path_train = f&quot;{PIPELINE_ROOT}\/{TRAIN_FILE_NAME}&quot;\ngcs_csv_path_test = f&quot;{PIPELINE_ROOT}\/{TEST_FILE_NAME}&quot;\n\n#gcs location where the output is to be written to\ngcs_destination_prefix = &quot;{}\/output&quot;.format(BUCKET_NAME)\n\n@kfp.dsl.pipeline(name=&quot;automl-tab-training-v2&quot;)\ndef pipeline(project: str = PROJECT_ID):\n    \n    # create tabular dataset\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=&quot;creditcard&quot;, gcs_source=gcs_csv_path_train\n    )\n    \n  \n    # Training with AutoML\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=&quot;train-automl-fraud-detection&quot;,\n        optimization_prediction_type=&quot;classification&quot;,\n        column_transformations=[\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Time&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V1&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V2&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V3&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V4&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V5&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V6&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V7&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V8&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V9&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V10&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V11&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V12&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V13&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V14&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V15&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V16&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V17&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V18&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V19&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V20&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V21&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V22&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V23&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V24&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V25&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V26&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V27&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V28&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Amount&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],#dataset_with_FeatEng,\n        target_column=&quot;Class&quot;,\n        budget_milli_node_hours=1000,\n    )\n    \n    # batch prediction after training\n    batchprediction_op = gcc_aip.ModelBatchPredictOp(\n        model=training_op.outputs[&quot;model&quot;],\n        job_display_name='prediction1',\n        gcs_source=gcs_csv_path_test,\n        project=project,\n        machine_type=&quot;n1-standard-2&quot;,\n        gcs_destination_prefix=gcs_destination_prefix,\n    )\n    \n\nCOMPILED_PIPELINE_PATH = r&quot;C:\\Users\\...\\tabular_classification_pipeline.json&quot;\nSCHEDULE = &quot;5 5 * * *&quot;\nDISPLAY_NAME = 'fraud_detection'\n\n# compile pipeline\ncompiler.Compiler().compile(\n    pipeline_func=pipeline,\n    package_path=COMPILED_PIPELINE_PATH,\n)\n\n# job run after its creation\njob = aip.PipelineJob(\n    display_name=DISPLAY_NAME,\n    template_path=COMPILED_PIPELINE_PATH,\n    pipeline_root=PIPELINE_ROOT,\n)\njob.run()\n\n# api_client = AIPlatformClient(project_id=PROJECT_ID, region=REGION)\n\n# schedule training\/prediction every day at a certain hour\n# api_client.create_schedule_from_job_spec(\n#    job_spec_path=COMPILED_PIPELINE_PATH,\n#    pipeline_root=PIPELINE_ROOT,\n#    schedule=SCHEDULE,\n# )\n<\/code><\/pre>\n<p>Looking at the error log, I found:<\/p>\n<pre><code>{\nhttpRequest: {\nstatus: 404\n}\ninsertId: &quot;13yj575g2rylrz9&quot;\njsonPayload: {\n@type: &quot;type.googleapis.com\/google.cloud.scheduler.logging.AttemptFinished&quot;\njobName: &quot;projects\/fraud-detection-project-329506\/locations\/us-central1\/jobs\/pipeline_pipeline_179e648c_0-11-a-a-a&quot;\nstatus: &quot;NOT_FOUND&quot;\ntargetType: &quot;HTTP&quot;\nurl: &quot;https:\/\/us-central1-fraud-detection-project-329506.cloudfunctions.net\/templated_http_request-v1&quot;\n}\nlogName: &quot;projects\/fraud-detection-project-329506\/logs\/cloudscheduler.googleapis.com%2Fexecutions&quot;\nreceiveTimestamp: &quot;2021-10-19T18:00:00.309225533Z&quot;\nresource: {\nlabels: {\njob_id: &quot;pipeline_pipeline_179e648c_0-11-a-a-a&quot;\nlocation: &quot;us-central1&quot;\nproject_id: &quot;fraud-detection-project-329506&quot;\n}\ntype: &quot;cloud_scheduler_job&quot;\n}\nseverity: &quot;ERROR&quot;\ntimestamp: &quot;2021-10-19T18:00:00.309225533Z&quot;\n}\n<\/code><\/pre>\n<p>Does it mean that I have to create the URL before running the notebook? I have no idea how to go on.\nThank you in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634804848290,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-scheduler|google-cloud-vertex-ai",
        "Question_view_count":330,
        "Owner_creation_time":1616589293617,
        "Owner_last_access_time":1663860372103,
        "Owner_location":"Alatri, Frosinone, FR",
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":1635245663990,
        "Answer_body":"<p>From the error you shared, apparently <strong>Cloud Function<\/strong> failed to create the job.<\/p>\n<pre><code>status: &quot;NOT_FOUND&quot; \ntargetType: &quot;HTTP&quot; \nurl: &quot;https:\/\/us-central1-fraud-detection-project-329506.cloudfunctions.net\/templated_http_request-v1&quot;\n<\/code><\/pre>\n<p>A possible reason from the Cloud Function side could be if <strong>Cloud Build API<\/strong> is not used in your project before or it is disabled. Can you check if it is enabled and try again? If you have enabled this API recently, wait for a few minutes for the action to propagate to the systems and retry.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1636443694643,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69658459",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69577270,
        "Question_title":"Google Cloud Vertex AI - 400 'dedicated_resources' is not supported for Model",
        "Question_body":"<p>I'm attempting to deploy a text classification model I trained with Vertex AI on the Google Cloud Platform using the Python SDK.<\/p>\n<pre><code>from google.cloud import aiplatform\n\nimport os\n\nos.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;] = &quot;&lt;key location&gt;&quot;\n\ndef create_endpoint(\n    project_id: str,\n    display_name: str,\n    location: str,\n    sync: bool = True,\n):\n    endpoint = aiplatform.Endpoint.create(\n        display_name=display_name, project=project_id, location=location,\n    )\n\n    print(endpoint.display_name)\n    print(endpoint.resource_name)\n    return endpoint\n\ndef deploy_model(project_id, location, model_id):\n    model_location = &quot;projects\/{}\/locations\/{}\/models\/{}&quot;.format(project_id, location, model_id)\n\n    print(&quot;Initializing Vertex AI&quot;)\n    aiplatform.init(project=project_id, location=location)\n\n    print(&quot;Getting model from {}&quot;.format(model_location))\n    model = aiplatform.Model(model_location)\n\n    print(&quot;Creating endpoint.&quot;)\n    endpoint = create_endpoint(project_id, &quot;{}_endpoint&quot;.format(model_id), location)\n\n    print(&quot;Deploying endpoint&quot;)\n    endpoint.deploy(\n        model,\n        machine_type=&quot;n1-standard-4&quot;,\n        min_replica_count=1,\n        max_replica_count=5,\n        accelerator_type='NVIDIA_TESLA_K80',\n        accelerator_count=1\n    )\n\n    return endpoint\n\nendpoint = deploy_model(\n    &quot;&lt;project name&gt;&quot;,\n    &quot;us-central1&quot;,\n    &quot;&lt;model id&gt;&quot;,\n)\n<\/code><\/pre>\n<p>Unfortunately, when I run this code, I receive this error after the endpoint.deploy is triggered:\n<code>google.api_core.exceptions.InvalidArgument: 400 'dedicated_resources' is not supported for Model...<\/code> followed by the model location.<\/p>\n<p>Note the places I've swapped my values with &lt;***&gt; to hide my local workspace variables.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1634245049023,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":616,
        "Owner_creation_time":1417013182680,
        "Owner_last_access_time":1663940014890,
        "Owner_location":"Boston, MA",
        "Owner_reputation":1256,
        "Owner_up_votes":391,
        "Owner_down_votes":2,
        "Owner_views":245,
        "Question_last_edit_time":1634248660769,
        "Answer_body":"<p>You encounter the error since it is not possible to assign a custom machine to a Text Classification model. Only custom models and AutoML tabular models can use custom machines as per <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#specify\" rel=\"nofollow noreferrer\">documentation<\/a>. For AutoML models aside from tabular models, Vertex AI automatically configures the machine type.<\/p>\n<blockquote>\n<p>If you want to use a custom-trained model or an AutoML tabular model\nto serve online predictions, you must specify a machine type when you\ndeploy the Model resource as a DeployedModel to an Endpoint. <strong>For other<\/strong>\n<strong>types of AutoML models, Vertex AI configures the machine types<\/strong>\n<strong>automatically.<\/strong><\/p>\n<\/blockquote>\n<p>The workaround for this is to remove the machine related parameters on your <code>endpoint.deploy()<\/code> and you should be able to deploy the model.<\/p>\n<pre><code>print(&quot;Deploying endpoint&quot;)\nendpoint.deploy(model)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1634271217372,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69577270",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72427594,
        "Question_title":"Questions on json and GCP",
        "Question_body":"<p>So I have this code to write out a json file to be connected to via endpoint. The file is pretty standard in that it has the location of how to connect to the endpoint as well as some data.<\/p>\n<pre><code>%%writefile default-pred.json\n{   PROJECT_ID:&quot;msds434-gcp&quot;,\n    REGION:&quot;us-central1&quot;,\n    ENDPOINT_ID:&quot;2857701089334001664&quot;,\n    INPUT_DATA_FILE:&quot;INPUT-JSON&quot;,\n    &quot;instances&quot;: [\n    {&quot;age&quot;: 39,\n    &quot;bill_amt_1&quot;: 47174,\n    &quot;bill_amt_2&quot;: 47974,\n    &quot;bill_amt_3&quot;: 48630,\n    &quot;bill_amt_4&quot;: 50803,\n    &quot;bill_amt_5&quot;: 30789,\n    &quot;bill_amt_6&quot;: 15874,\n    &quot;education_level&quot;: &quot;1&quot;,\n    &quot;limit_balance&quot;: 50000,\n    &quot;marital_status&quot;: &quot;2&quot;,\n    &quot;pay_0&quot;: 0,\n    &quot;pay_2&quot;:0,\n    &quot;pay_3&quot;: 0,\n    &quot;pay_4&quot;: 0,\n    &quot;pay_5&quot;: &quot;0&quot;,\n    &quot;pay_6&quot;: &quot;0&quot;,\n    &quot;pay_amt_1&quot;: 1800,\n    &quot;pay_amt_2&quot;: 2000,\n    &quot;pay_amt_3&quot;: 3000,\n    &quot;pay_amt_4&quot;: 2000,\n    &quot;pay_amt_5&quot;: 2000,\n    &quot;pay_amt_6&quot;: 2000,\n    &quot;sex&quot;: &quot;1&quot;\n    }\n  ]\n    }\n<\/code><\/pre>\n<p>Then I have this trying to connect to the file and then taking the information to connect to the end point in question. I know the information is right as it's the exact code from GCP.<\/p>\n<pre><code>!curl \\\n-X POST \\\n-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n-H &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-prediction-aiplatform.googleapis.com\/v1alpha1\/projects\/$PROJECT_ID\/locations\/$REGION\/endpoints\/$ENDPOINT_ID:predict \\\n-d &quot;@default-pred.json&quot;\n<\/code><\/pre>\n<p>So from the information I have I would expect it to parse the information I have and connect to the endpoint, but obviously I have my file wrong somehow. Any idea what it is?<\/p>\n<pre><code>{\n  &quot;error&quot;: {\n    &quot;code&quot;: 400,\n    &quot;message&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;PROJECT_ID\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;REGION\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;ENDPOINT_ID\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;INPUT_DATA_FILE\\&quot;: Cannot find field.&quot;,\n    &quot;status&quot;: &quot;INVALID_ARGUMENT&quot;,\n    &quot;details&quot;: [\n      {\n        &quot;@type&quot;: &quot;type.googleapis.com\/google.rpc.BadRequest&quot;,\n        &quot;fieldViolations&quot;: [\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;PROJECT_ID\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;REGION\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;ENDPOINT_ID\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;INPUT_DATA_FILE\\&quot;: Cannot find field.&quot;\n          }\n        ]\n      }\n    ]\n  }\n}\n<\/code><\/pre>\n<p>What am I missing here?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653862009460,
        "Question_score":0,
        "Question_tags":"json|google-cloud-platform|endpoint|google-cloud-vertex-ai",
        "Question_view_count":77,
        "Owner_creation_time":1632597456473,
        "Owner_last_access_time":1661080359213,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The data file should only include the data.<\/p>\n<p>You've included <code>PROJECT_ID<\/code>, <code>REGION<\/code>, <code>ENDPOINT<\/code> and should not.<\/p>\n<p>These need to be set in the (bash) environment <strong>before<\/strong> you issue the <code>curl<\/code> command:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>PROJECT_ID=&quot;msds434-gcp&quot;\nREGION=&quot;us-central1&quot;\nENDPOINT_ID=&quot;2857701089334001664&quot;\n\ncurl \\\n--request POST \\\n--header &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n--header &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-prediction-aiplatform.googleapis.com\/v1alpha1\/projects\/$PROJECT_ID\/locations\/$REGION\/endpoints\/$ENDPOINT_ID:predict \\\n--data &quot;@default-pred.json&quot;\n<\/code><\/pre>\n<p>The file <code>default-pred.json<\/code> should <strike>probably (I can never find this service's methods in <a href=\"https:\/\/developers.google.com\/apis-explorer\" rel=\"nofollow noreferrer\">APIs Explorer<\/a>!<\/strike>) just be:<\/p>\n<pre><code>{\n  instances&quot;: [\n    { &quot;age&quot;: 39,\n      &quot;bill_amt_1&quot;: 47174,\n      &quot;bill_amt_2&quot;: 47974,\n      &quot;bill_amt_3&quot;: 48630,\n      &quot;bill_amt_4&quot;: 50803,\n      &quot;bill_amt_5&quot;: 30789,\n      &quot;bill_amt_6&quot;: 15874,\n      &quot;education_level&quot;: &quot;1&quot;,\n      &quot;limit_balance&quot;: 50000,\n      &quot;marital_status&quot;: &quot;2&quot;,\n      &quot;pay_0&quot;: 0,\n      &quot;pay_2&quot;:0,\n      &quot;pay_3&quot;: 0,\n      &quot;pay_4&quot;: 0,\n      &quot;pay_5&quot;: &quot;0&quot;,\n      &quot;pay_6&quot;: &quot;0&quot;,\n      &quot;pay_amt_1&quot;: 1800,\n      &quot;pay_amt_2&quot;: 2000,\n      &quot;pay_amt_3&quot;: 3000,\n      &quot;pay_amt_4&quot;: 2000,\n      &quot;pay_amt_5&quot;: 2000,\n      &quot;pay_amt_6&quot;: 2000,\n      &quot;sex&quot;: &quot;1&quot;\n    }\n  ]\n}\n<\/code><\/pre>\n<p>See the documentation for the <code>aiplatform<\/code> <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/reference\/rest\/v1\/projects\/predict\" rel=\"nofollow noreferrer\"><code>predict<\/code><\/a> method as this explains this.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1653863593763,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1653864193883,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72427594",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68737342,
        "Question_title":"Model evaluation predictions from VertexAI AutoMLTabularTrainingJob",
        "Question_body":"<p>I am following python api documentation to create a AutoMLTabularTrainingJob : <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/automl-api#tabular\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/automl-api#tabular<\/a>. It trains successfully with a dataset that consists of train\/valid\/test splits. I can also get the model evaluation metrics after following this documentation : <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/evaluating-automl-models\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/evaluating-automl-models<\/a><\/p>\n<p>However, I am unable to find a way to get the raw predictions that are used to generate the model evaluation metrics. Any pointers would be much appreciated !<\/p>\n<p>Do I have to request for batch predictions again ? <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/batch-predictions#tabular\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/batch-predictions#tabular<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":8,
        "Question_creation_time":1628665036657,
        "Question_score":0,
        "Question_tags":"google-cloud-sdk|google-cloud-vertex-ai",
        "Question_view_count":53,
        "Owner_creation_time":1308505624643,
        "Owner_last_access_time":1663893908543,
        "Owner_location":null,
        "Owner_reputation":1876,
        "Owner_up_votes":165,
        "Owner_down_votes":2,
        "Owner_views":201,
        "Question_last_edit_time":1628666618203,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68737342",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70614514,
        "Question_title":"Google Cloud Vertex AI Notebook Scheduled Runs Aren't Running Code?",
        "Question_body":"<p>I've followed their <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/managed\/quickstart-schedule-execution-console\" rel=\"nofollow noreferrer\">instructions<\/a> to how to set up a managed Jupyter notebook and schedule a run, and I tossed in some pretty standard parameters and my bucket.<\/p>\n<p>After setting up the schedule, however, the run just comes out as &quot;Failed&quot;, and when I get &quot;view results&quot;, I just get my code back (with no output indication). For some reason it's just not running. Ideas?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/r0vra.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/r0vra.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>[2]<a href=\"https:\/\/i.stack.imgur.com\/ETUTY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ETUTY.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641509078660,
        "Question_score":2,
        "Question_tags":"python|google-cloud-platform|jupyter-notebook|google-cloud-vertex-ai",
        "Question_view_count":457,
        "Owner_creation_time":1523235961717,
        "Owner_last_access_time":1642750774450,
        "Owner_location":null,
        "Owner_reputation":143,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":36,
        "Question_last_edit_time":1641509284196,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70614514",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71245000,
        "Question_title":"Vertex AI Pipeline Failed Precondition",
        "Question_body":"<p>I have been following this video:\n<a href=\"https:\/\/www.youtube.com\/watch?v=1ykDWsnL2LE&amp;t=310s\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=1ykDWsnL2LE&amp;t=310s<\/a><\/p>\n<p>Code located at:\n<a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5\" rel=\"nofollow noreferrer\">https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5<\/a>\n(I have done the last two steps as per the video which isn't an issue for google_cloud_pipeline_components version: 0.1.1)<\/p>\n<p>I have created a pipeline in vertex ai which ran and used the following code to create the pipeline (from video not code extract in link above):<\/p>\n<pre><code>#run pipeline\nresponse = api_client.create_run_from_job_spec(\n    &quot;tab_classif_pipeline.json&quot;, pipeline_root = PIPELINE_ROOT,\n    parameter_values = {\n    &quot;project&quot; : PROJECT_ID,\n    &quot;display_name&quot; : DISPLAY_NAME\n    }\n)\n    \n<\/code><\/pre>\n<p>and in the GCP logs I get the following error:<\/p>\n<pre><code>&quot;google.api_core.exceptions.FailedPrecondition: 400 BigQuery Dataset location `eu` must be in the same location as the service location `us-central1`.\n<\/code><\/pre>\n<p>I get the error at the dataset_create_op stage:<\/p>\n<pre><code>    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n    project = project, display_name = display_name, bq_source = bq_source\n)\n<\/code><\/pre>\n<p>My dataset is configured in EU (the whole region) so I don't understand where us-central1 is coming from (or what the service location is?).<\/p>\n<p>Here is the all the code I have used:<\/p>\n<pre><code> PROJECT_ID = &quot;marketingtown&quot;\n BUCKET_NAME = f&quot;gs:\/\/lookalike_model&quot;\n from typing import NamedTuple\n import kfp\n from kfp import dsl\n from kfp.v2 import compiler\n from kfp.v2.dsl import (Artifact, Input, InputPath, Model, Output, \n                            OutputPath, ClassificationMetrics, \n Metrics, component)\n from kfp.v2.components.types.artifact_types import Dataset\n from kfp.v2.google.client import AIPlatformClient\n from google.cloud import aiplatform\n from google_cloud_pipeline_components import aiplatform as gcc_aip\n\n #set environment variables\n PATH = %env PATH\n %env PATH = (PATH):\/\/home\/jupyter\/.local\/bin\n REGION = &quot;europe-west2&quot;\n    \n #cloud storage path where artifact is created by pipeline\n PIPELINE_ROOT = f&quot;{BUCKET_NAME}\/pipeline_root\/&quot;\n PIPELINE_ROOT\n import time\n DISPLAY_NAME = f&quot;lookalike_model_pipeline_{str(int(time.time()))}&quot;\n print(DISPLAY_NAME)\n \n@kfp.dsl.pipeline(name = &quot;lookalike-model-training-v2&quot;, \npipeline_root = PIPELINE_ROOT)\n\ndef pipeline(\n    bq_source : str = f&quot;bq:\/\/{PROJECT_ID}.MLOp_pipeline_temp.lookalike_training_set&quot;,\n    display_name : str = DISPLAY_NAME,\n    project : str = PROJECT_ID,\n    gcp_region : str = &quot;europe-west2&quot;,\n    api_endpoint : str = &quot;europe-west2-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str : str = '{&quot;auPrc&quot; : 0.3}'\n):\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project = project, display_name = display_name, bq_source = bq_source\n    )\n    \n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        column_transformations=[\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;agentId&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;postcode&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;isMobile&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;gender&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;timeOfDay&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;sale&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;], #dataset from previous step\n        target_column=&quot;sale&quot;,\n    )\n    \n    #outputted evaluation metrics\n    model_eval_task = classification_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n    )\n    \n    #if deployment threshold is mean, deploy\n    with dsl.Condition(\n        model_eval_task.outputs[&quot;dep_decision&quot;] == &quot;true&quot;,\n        name=&quot;deploy_decision&quot;,\n    ):\n        \n    endpoint_op = gcc_aip.EndpointCreateOp(\n        project=project,\n        location=gcp_region,\n        display_name=&quot;train-automl-beans&quot;,\n    )\n        \n    #deploys model to an endpoint\n    gcc_aip.ModelDeployOp(\n        model=training_op.outputs[&quot;model&quot;],\n        endpoint=endpoint_op.outputs[&quot;endpoint&quot;],\n        min_replica_count=1,\n        max_replica_count=1,\n        machine_type=&quot;n1-standard-4&quot;,\n        )\n   \n\n     compiler.Compiler().compile(\n        pipeline_func = pipeline, package_path = &quot;tab_classif_pipeline.json&quot;\n    )\n\n    #run pipeline\n    response = api_client.create_run_from_job_spec(\n        &quot;tab_classif_pipeline.json&quot;, pipeline_root = PIPELINE_ROOT,\n        parameter_values = {\n        &quot;project&quot; : PROJECT_ID,\n        &quot;display_name&quot; : DISPLAY_NAME\n        }\n    )\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":5,
        "Question_creation_time":1645657042940,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|pipeline|kubeflow|google-cloud-vertex-ai",
        "Question_view_count":420,
        "Owner_creation_time":1562706291280,
        "Owner_last_access_time":1664033431527,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1645697298212,
        "Answer_body":"<p>I solved this issue by adding the location to the TabularDatasetCreateJob:<\/p>\n<pre><code>    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n    project=project,\n    display_name=display_name, \n    bq_source=bq_source,\n    location = gcp_region\n)\n<\/code><\/pre>\n<p>I now have the same issue with the model training job but I have learnt that a lot of the functions in the above code take a location parameter, or default to us-central1. I will update if I get any further.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1646828083056,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71245000",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70464072,
        "Question_title":"Optionally use component functions added in VertexAI python SDK",
        "Question_body":"<p>I am using vertex ai's python SDK and it's built on top of Kubeflow pipelines. In it, you supposedly can do this:<\/p>\n<pre><code>train_op = (sklearn_classification_train(\n        train_data = data_op.outputs['train_out']\n    ).\n    set_cpu_limit(training_cpu_limit).\n    set_memory_limit(training_memory_limit).\n    add_node_selector_constraint(training_node_selector).\n    set_gpu_limit(training_gpu_limit)\n)\n<\/code><\/pre>\n<p>where you can add these functions (<code>set_cpu_limit<\/code>, <code>set_memory_limit<\/code>, <code>add_node_selector<\/code>, and <code>set_gpu_limit<\/code>) onto your component. I've haven't used this syntax before.<\/p>\n<p>How I can optionally use each 'sub function' only if the variables are specified each function?<\/p>\n<p>For example, if <code>training_gpu_limit<\/code> isn't set, I don't want to execute <code>set_gpu_limit<\/code> on the component.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640272812233,
        "Question_score":0,
        "Question_tags":"python-3.x|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":51,
        "Owner_creation_time":1417013182680,
        "Owner_last_access_time":1663940014890,
        "Owner_location":"Boston, MA",
        "Owner_reputation":1256,
        "Owner_up_votes":391,
        "Owner_down_votes":2,
        "Owner_views":245,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70464072",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71598020,
        "Question_title":"Internal error on batch prediction job. Reproduction conditions are unidentified",
        "Question_body":"<p>I trained some tabular forecasting model and executed batch prediction jobs with Vertex AI, then sometimes the following error occurred 30-60 minutes after the start of the jobs.<\/p>\n<pre><code>Internal error occurred. Please retry in a few minutes. If you still experience errors, contact Vertex AI.\n<\/code><\/pre>\n<p>As the message mentioned, I retried the next day with same data and then I got the prediction job succeed.\nHowever, I have seen this error from time to time with Vertex AI.\nI cannot identify the reproduction conditions.\nI cannot find the logs of the job execution.<\/p>\n<p>What causes this error in Vertex AI?\nHow can I avoid this errror?<\/p>\n<p>Resource name:\nprojects\/832409671062\/locations\/us-central1\/batchPredictionJobs\/3813678620929425408<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1648103893343,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|google-cloud-ai-platform-pipelines",
        "Question_view_count":124,
        "Owner_creation_time":1641394229917,
        "Owner_last_access_time":1650802845457,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71598020",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72237600,
        "Question_title":"GCP Vertex AI Managed Notebook cannot use custom container",
        "Question_body":"<p>In GCP Vertex AI, I created a Managed Notebook by specifying one of our custom containers which work perfectly with User-Managed Notebook kernels.\nThe Managed Notebook starts, and Jupyter Lab seems to work without any signs of error.<\/p>\n<p>Unfortunately, if I look at the available kernels in Jupyter Lab, only the default kernels are listed but not my custom kernel.<\/p>\n<p>An activity log entry on the right shows a spinning wheel &quot;Loading kernel from [custom container]&quot; which never disappears.<br \/>\nTaking a look at the terminal,<\/p>\n<p><em>docker image ls<\/em><\/p>\n<p>does not show the custom container either; obviously, it was not even pulled to the Managed Notebook.<\/p>\n<p>If I perform<\/p>\n<p><em>docker pull [custom container]<\/em><\/p>\n<p>in the terminal, to test connectivity to the Artifact Registry then it pulls the container correctly as expected.\nHowever, the custom kernel is still not visible in Jupyter Lab (even after a notebook restart).<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1652506277780,
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":146,
        "Owner_creation_time":1652504586737,
        "Owner_last_access_time":1652551986367,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72237600",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70984360,
        "Question_title":"Vertex AI Pipeline is not using the GPU",
        "Question_body":"<p>I am building a customized pipeline with the following step:<\/p>\n<pre><code>     trainer_task = (trainer(download_task.output).set_cpu_request(&quot;16&quot;).set_memory_request(&quot;60G&quot;).\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', &quot;NVIDIA_TESLA_K80&quot;).set_gpu_limit(2))\n<\/code><\/pre>\n<p>However, when I check the number of GPU's available it says zero, and the only visible device is a CPU device. I am migrating a project from Kubeflow, and this is the first time using Vertex AI, so I am not pretty sure why this is happening.<\/p>\n<p>The involved step is component that loads a docker image from the Artifact Registry and installs Tensorflow-gpu==2.4.1.<\/p>\n<p>Am I missing something? Why is not enabling the specified GPUs?<\/p>\n<p>Any help will be highly appreciated!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1643968333550,
        "Question_score":1,
        "Question_tags":"tensorflow|google-cloud-platform|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":250,
        "Owner_creation_time":1643967668103,
        "Owner_last_access_time":1654434560203,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70984360",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73623659,
        "Question_title":"Is it possible to import custom source files into Kubeflow components?",
        "Question_body":"<p>I know that Kubeflow only modifies the container with the specified libraries to be installed. But I want to use my custom module in the training Component section of the pipeline.<\/p>\n<p>So let me clarify my case; I'm deploying a GCP Vertex AI pipeline which exists of preprocessing and training  steps. And there is also custom library that I created using some libraries like scikit. My main issue is that I want to re-use that library objects within my training step which looks like;<\/p>\n<pre><code>    packages_to_install = [\n        &quot;pandas&quot;,\n        &quot;sklearn&quot;,\n        &quot;mycustomlibrary?&quot;\n    ],\n)\ndef train_xgb_model(\n    dataset: Input[Dataset],\n    model_artifact: Output[Model]\n):\n    \n    from MyCustomLibrary import XGBClassifier\n    import pandas as pd\n    \n    data = pd.read_csv(dataset.path)\n\n    model = XGBClassifier(\n        objective=&quot;binary:logistic&quot;\n    )\n    model.fit(\n        data.drop(columns=[&quot;target&quot;]),\n        data.target,\n    )\n\n    score = model.score(\n        data.drop(columns=[&quot;target&quot;]),\n        data.target,\n    )\n\n    model_artifact.metadata[&quot;train_score&quot;] = float(score)\n    model_artifact.metadata[&quot;framework&quot;] = &quot;XGBoost&quot;\n    \n    model.save_model(model_artifact.path)``` \n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1662474254327,
        "Question_score":1,
        "Question_tags":"kubeflow|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":31,
        "Owner_creation_time":1529909367513,
        "Owner_last_access_time":1662970375840,
        "Owner_location":"\u0130stanbul, T\u00fcrkiye",
        "Owner_reputation":41,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73623659",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73300381,
        "Question_title":"Documentation on Vertex AI Feature Store online serving architecture?",
        "Question_body":"<p>There's documentation on <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/serving-online\" rel=\"nofollow noreferrer\">Vertex AI online serving<\/a>, but no mention of the underlying system being used other than <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-featurestores\" rel=\"nofollow noreferrer\">&quot;online serving nodes&quot;<\/a>. Is it <a href=\"https:\/\/cloud.google.com\/datastore\" rel=\"nofollow noreferrer\">Datastore<\/a>? Something else?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1660103073257,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":21,
        "Owner_creation_time":1386714193850,
        "Owner_last_access_time":1663802361323,
        "Owner_location":null,
        "Owner_reputation":3767,
        "Owner_up_votes":1128,
        "Owner_down_votes":10,
        "Owner_views":226,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73300381",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72073763,
        "Question_title":"GCP Vertex Pipeline - Why kfp.v2.dsl.Output as function arguments work without being provided?",
        "Question_body":"<p>Why <code> kfp.v2.dsl.Output<\/code> as function argument works without being provided?<\/p>\n<p>I am following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/training-data-analyst\/blob\/master\/courses\/machine_learning\/deepdive2\/machine_learning_in_the_enterprise\/solutions\/create_run_vertex_pipeline.ipynb\" rel=\"nofollow noreferrer\">Create and run ML pipelines with Vertex Pipelines!<\/a> Jupyter notebook example from GCP.<\/p>\n<p>The function <code>classif_model_eval_metrics<\/code> takes <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> which have no default values.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],                # No default value set, hence must be mandatory\n    metricsc: Output[ClassificationMetrics], # No default value set, hence must be mandatory\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]): \n    # Full code at the bottom.\n<\/code><\/pre>\n<p>Hence those arguments should be mandatory, but the function is called without those arguments.<\/p>\n<pre><code>    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p>The entire pipeline code is below.<\/p>\n<pre><code>@kfp.dsl.pipeline(name=&quot;automl-tab-beans-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = &quot;bq:\/\/aju-dev-demos.beans.beans1&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auRoc&quot;: 0.95}',\n):\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=display_name, bq_source=bq_source\n    )\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        column_transformations=COLUMNS,\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;Class&quot;,\n    )\n    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Why does it work and what are <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> of type <code>kfp.v2.dsl.Output<\/code>?<\/p>\n<hr \/>\n<h2>classif_model_eval_metrics function code<\/h2>\n<pre><code>from kfp.v2.dsl import (\n    Dataset, Model, Output, Input, \n    OutputPath, ClassificationMetrics, Metrics, component\n)\n\n@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]):  # Return parameter.\n    &quot;&quot;&quot;Renders evaluation metrics for an AutoML Tabular classification model.\n    Retrieves the classification model evaluation and render the ROC and confusion matrix\n    for the model. Determine whether the model is sufficiently accurate to deploy.\n    &quot;&quot;&quot;\n    import json\n    import logging\n    from google.cloud import aiplatform\n\n    # Fetch model eval info\n    def get_eval_info(client, model_name):\n        from google.protobuf.json_format import MessageToDict\n        response = client.list_model_evaluations(parent=model_name)\n        metrics_list = []\n        metrics_string_list = []\n        for evaluation in response:\n            metrics = MessageToDict(evaluation._pb.metrics)\n            metrics_str = json.dumps(metrics)\n            metrics_list.append(metrics)\n            metrics_string_list.append(metrics_str)\n        return (\n            evaluation.name,\n            metrics_list,\n            metrics_string_list,\n        )\n\n    def classification_thresholds_check(metrics_dict, thresholds_dict):\n        for k, v in thresholds_dict.items():\n            if k in [&quot;auRoc&quot;, &quot;auPrc&quot;]:  # higher is better\n                if metrics_dict[k] &lt; v:  # if under threshold, don't deploy\n                    return False\n        return True\n\n    def log_metrics(metrics_list, metricsc):\n        test_confusion_matrix = metrics_list[0][&quot;confusionMatrix&quot;]\n        logging.info(&quot;rows: %s&quot;, test_confusion_matrix[&quot;rows&quot;])\n        # log the ROC curve\n        fpr = [], tpr = [], thresholds = []\n        for item in metrics_list[0][&quot;confidenceMetrics&quot;]:\n            fpr.append(item.get(&quot;falsePositiveRate&quot;, 0.0))\n            tpr.append(item.get(&quot;recall&quot;, 0.0))\n            thresholds.append(item.get(&quot;confidenceThreshold&quot;, 0.0))\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        # log the confusion matrix\n        annotations = []\n        for item in test_confusion_matrix[&quot;annotationSpecs&quot;]:\n            annotations.append(item[&quot;displayName&quot;])\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n        # log textual metrics info as well\n        for metric in metrics_list[0].keys():\n            if metric != &quot;confidenceMetrics&quot;:\n                val_string = json.dumps(metrics_list[0][metric])\n                metrics.log_metric(metric, val_string)\n        # metrics.metadata[&quot;model_type&quot;] = &quot;AutoML Tabular classification&quot;\n\n    aiplatform.init(project=project)\n\n    client = aiplatform.gapic.ModelServiceClient(client_options={&quot;api_endpoint&quot;: api_endpoint})\n    eval_name, metrics_list, metrics_str_list = get_eval_info(\n        client, model.uri.replace(&quot;aiplatform:\/\/v1\/&quot;, &quot;&quot;)\n    )\n    log_metrics(metrics_list, metricsc)\n    thresholds_dict = json.loads(thresholds_dict_str)\n\n    return (&quot;true&quot;,) if classification_thresholds_check(metrics_list[0], thresholds_dict) else (&quot;false&quot;, )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1651375335713,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":358,
        "Owner_creation_time":1416648155470,
        "Owner_last_access_time":1664057583237,
        "Owner_location":null,
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The custom component is defined as a Python function with a <code>@kfp.v2.dsl.component<\/code> decorator.<\/p>\n<p>The <code>@component<\/code> decorator specifies three optional arguments: the base container image to use; any packages to install; and the yaml file to which to write the component specification.<\/p>\n<p>The component function, <code>classif_model_eval_metrics<\/code>, has some input parameters.  The model parameter is an input <code>kfp.v2.dsl.Model artifact<\/code>.<\/p>\n<p>The two function args, <code>metrics<\/code> and <code>metricsc<\/code>, are component Outputs, in this case of types Metrics and ClassificationMetrics. They\u2019re not explicitly passed as inputs to the component step, but rather are automatically instantiated and can be used in the component.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;,\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,  # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n)\n<\/code><\/pre>\n<p>For example, in the function below, we\u2019re calling <code>metricsc.log_roc_curve()<\/code> and <code>metricsc.log_confusion_matrix()<\/code> to render these visualizations in the Pipelines UI. These Output params become component outputs when the component is compiled, and can be consumed by other pipeline steps.<\/p>\n<pre><code>def log_metrics(metrics_list, metricsc):\n        ...\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        ...\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n<\/code><\/pre>\n<p>For more information you can refer to this <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/use-vertex-pipelines-build-automl-classification-end-end-workflow\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1652802212843,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72073763",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68156788,
        "Question_title":"Google Vertex AI AutoML - cannot specify schema for CSV Dataset",
        "Question_body":"<p>I have created Tabular datasets in Vertex AI \/ Datasets based on some CSV files. However when I try to use these datasets in AutoML for training and prediction, there is no way to specify the data types of the fields. In <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/data-types-tabular#csv\" rel=\"nofollow noreferrer\">the docs<\/a> I could not find how to do the &quot;transformations&quot;. In theory it supports the following types:<\/p>\n<ul>\n<li>Text<\/li>\n<li>Categorical<\/li>\n<li>Numeric<\/li>\n<li>Timestamp<\/li>\n<\/ul>\n<p>In case of BigQuery tables it is pretty obvious to get the data types as it is explicitely specified by the schema of the table. However in case of a CSV file sometimes it is not obvious to find out the type of a field and indeed in my case sometimes AutoML guesses incorrectly. Any ideas how to specify the data types explicitely for CSV files?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624846088910,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":443,
        "Owner_creation_time":1457019461140,
        "Owner_last_access_time":1653612331473,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1624940922616,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68156788",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69106055,
        "Question_title":"Is there anyway to train a classification model with Google's AutoML, with mixed (language and tabular) data?",
        "Question_body":"<p>I want to train a NLP classification model with Google's AutoML. The inputs of the model are tabular data and a text field which is the main field for the classification task. Without the tabular data the classification error gets huge. I'm aware that it can be done with a custom model using Keras or PyTorch. Could it be done using Google's AutoML?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1631115758097,
        "Question_score":0,
        "Question_tags":"keras|nlp|google-cloud-automl|automl|google-cloud-vertex-ai",
        "Question_view_count":46,
        "Owner_creation_time":1500240803887,
        "Owner_last_access_time":1645585994337,
        "Owner_location":"Medell\u00edn, Medellin, Antioquia, Colombia",
        "Owner_reputation":38,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":1631204087532,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69106055",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70023419,
        "Question_title":"Vertex AI - cannot create managed notebook instance",
        "Question_body":"<p>I'm following all the (easy) steps in the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/managed\/quickstart-create-console\" rel=\"nofollow noreferrer\">documentation<\/a>, but I'm stuck at clicking the &quot;CREATE&quot; button. When I click it, the process runs for a few seconds, then the button re-appears, like I never clicked it.<\/p>\n<p>If I go back to the &quot;Managed Notebooks&quot; page, no instance is present.<\/p>\n<p>Am I missing something basic? Has someone the same problem as mine?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1637252864337,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|gcp-ai-platform-notebook|google-cloud-vertex-ai",
        "Question_view_count":281,
        "Owner_creation_time":1450277518267,
        "Owner_last_access_time":1663928482250,
        "Owner_location":"Milano, MI, Italia",
        "Owner_reputation":947,
        "Owner_up_votes":625,
        "Owner_down_votes":2,
        "Owner_views":75,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70023419",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69040163,
        "Question_title":"Vertex ai custom model training for pyspark ml model",
        "Question_body":"<p>Is it possible to train a spark\/pyspark ML lib model using VertexAI custom container model building? I couldn't find any reference in the vertex ai documents regarding spark model training. For distributed processing model building only options available are PyTorch or TensorFlow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630648846963,
        "Question_score":0,
        "Question_tags":"apache-spark|pyspark|apache-spark-mllib|machine-learning-model|google-cloud-vertex-ai",
        "Question_view_count":432,
        "Owner_creation_time":1630648341403,
        "Owner_last_access_time":1663926044083,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69040163",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68465990,
        "Question_title":"How do I access AIP_STORAGE_URI in Vertex AI?",
        "Question_body":"<p>I uploaded a model with<\/p>\n<pre><code>gcloud beta ai models upload --artifact-uri\n<\/code><\/pre>\n<p>And in the docker I access <code>AIP_STORAGE_URI<\/code>.\nI see that <code>AIP_STORAGE_URI<\/code> is another Google Storage location so I try to download the files using <code>storage.Client()<\/code> but then it says that I don't have access:<\/p>\n<pre><code>google.api_core.exceptions.Forbidden: 403 GET https:\/\/storage.googleapis.com\/storage\/v1\/b\/caip-tenant-***-***-*-*-***?projection=noAcl&amp;prettyPrint=false: custom-online-prediction@**.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket\n<\/code><\/pre>\n<p>I am running this endpoint with the default service account.<\/p>\n<p><a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#artifacts\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#artifacts<\/a><\/p>\n<p>According to the above link:\n<code>The service account that your container uses by default has permission to read from this URI. <\/code><\/p>\n<p>What am I doing wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626854597743,
        "Question_score":3,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":742,
        "Owner_creation_time":1466977784157,
        "Owner_last_access_time":1664005111393,
        "Owner_location":null,
        "Owner_reputation":117,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":1626858370043,
        "Answer_body":"<p>The reason behind the error being, the default service account that Vertex AI uses has the \u201c<a href=\"https:\/\/cloud.google.com\/storage\/docs\/access-control\/iam-roles#standard-roles\" rel=\"nofollow noreferrer\">Storage Object Viewer<\/a>\u201d role which excludes the <code>storage.buckets.get<\/code> permission. At the same time, the <code>storage.Client()<\/code> part of the code makes a <code>storage.buckets.get<\/code> request to the Vertex AI managed bucket for which the default service account does not have permission to.<\/p>\n<p>To resolve the issue, I would suggest you to follow the below steps -<\/p>\n<ol>\n<li><p>Make changes in the custom code to access the bucket with the model artifacts in your project instead of using the environment variable <code>AIP_STORAGE_URI<\/code> which points to the model location in the Vertex AI managed bucket.<\/p>\n<\/li>\n<li><p>Create your own service account and grant the service account with all the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/custom-service-account\" rel=\"nofollow noreferrer\">permissions<\/a> needed by the custom code. For this specific error, a role with the <code>storage.buckets.get<\/code> permission, eg. <a href=\"https:\/\/cloud.google.com\/storage\/docs\/access-control\/iam-roles#standard-roles\" rel=\"nofollow noreferrer\">Storage Admin<\/a> (&quot;roles\/storage.admin&quot;) has to be granted to the service account.<\/p>\n<\/li>\n<li><p>Provide the newly created service account in the &quot;Service Account&quot; field when deploying the model.<\/p>\n<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1627666181500,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1661471987216,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68465990",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69936296,
        "Question_title":"Vertex AI custom container batch prediction",
        "Question_body":"<p>I have created a custom container for prediction and successfully uploaded the model to Vertex AI. I was also able to deploy the model to an endpoint and successfully request predictions from the endpoint. Within the custom container code, I use the <code>parameters<\/code> field as described <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#prediction\" rel=\"nofollow noreferrer\">here<\/a>, which I then supply later on when making an online prediction request.\nMy questions are regarding requesting batch predictions from a custom container for prediction.<\/p>\n<ol>\n<li><p>I cannot find any documentation that describes what happens when I request a batch prediction. Say, for example, I use the <code>my_model.batch_predict<\/code> function from the Python SDK and set the <code>instances_format<\/code> to &quot;csv&quot; and provide the <code>gcs_source<\/code>. Now, I have setup my custom container to expect prediction requests at <code>\/predict<\/code> as described in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements\" rel=\"nofollow noreferrer\">documentation<\/a>. Does Vertex AI make a POST request to this path, converting the cvs data into the appropriate POST body?<\/p>\n<\/li>\n<li><p>How do I specify the <code>parameters<\/code> field for batch prediction as I did for online prediction?<\/p>\n<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1636674290160,
        "Question_score":3,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":808,
        "Owner_creation_time":1471292986790,
        "Owner_last_access_time":1664058170413,
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69936296",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69904968,
        "Question_title":"How to set the run name of a PipelineJob",
        "Question_body":"<p>I have this code to start a VertexAI pipeline job:<\/p>\n<pre><code>import google.cloud.aiplatform as vertexai\n\nvertexai.init(project=PROJECT_ID,staging_bucket=PIPELINE_ROOT)\n\njob = vertexai.PipelineJob(\n    display_name='pipeline-test-1',\n    template_path='xgb_pipe.json'\n)\n\njob.run()\n<\/code><\/pre>\n<p>which works nicely, but the <code>run name<\/code> label is a random number.  How can I specify the <code>run name<\/code>?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/2Fkom.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2Fkom.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636491344913,
        "Question_score":0,
        "Question_tags":"python|google-ai-platform|google-cloud-vertex-ai|google-cloud-ai-platform-pipelines",
        "Question_view_count":302,
        "Owner_creation_time":1262684260990,
        "Owner_last_access_time":1664043379267,
        "Owner_location":"Transylvania, Romania",
        "Owner_reputation":200885,
        "Owner_up_votes":4092,
        "Owner_down_votes":148,
        "Owner_views":12738,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69904968",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72064796,
        "Question_title":"GCP Vertex AI - what is the return value of gcloud ai endpoints create defined?",
        "Question_body":"<p>Where is the return value of <code>gcloud ai endpoints create<\/code> defined and documented?<\/p>\n<p>I am following GCP coursera Vertex AI model monitoring <a href=\"https:\/\/www.coursera.org\/learn\/art-science-ml\/gradedLti\/Ru8lM\/lab-vertex-ai-model-monitoring\" rel=\"nofollow noreferrer\">Lab: Vertex AI Model Monitoring<\/a> and the task is asking to get the model endpoint ID from the <code>gcloud ai endpoints create<\/code> command output.<\/p>\n<pre><code># Deploy your model to the endpoint\nENDPOINT_NAME = &quot;churn&quot;\noutput = !gcloud --quiet beta ai endpoints create --display-name=$ENDPOINT_NAME --format=&quot;value(name)&quot;\nprint(&quot;endpoint output: &quot;, output)\n\nENDPOINT = output[-1]\nENDPOINT_ID = # TODO: Your code goes here\n<\/code><\/pre>\n<p><a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/endpoints\/create\" rel=\"nofollow noreferrer\">gcloud ai endpoints create<\/a> documentation shows no information of its return value.<\/p>\n<p>Where is it documented?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651278069133,
        "Question_score":0,
        "Question_tags":"gcloud|google-cloud-vertex-ai",
        "Question_view_count":104,
        "Owner_creation_time":1416648155470,
        "Owner_last_access_time":1664057583237,
        "Owner_location":null,
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72064796",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73830357,
        "Question_title":"issues with passing command line args to custom training job vertex AI",
        "Question_body":"<p>I am trying to run basic custom training job<\/p>\n<pre><code>job = aiplatform.CustomContainerTrainingJob(\n display_name='testjob-name',\n container_uri='gcr.io\/prj-id\/image-name:latest',\n project=project_id,\n credentials= credentials,\n staging_bucket= 'stage-bucket'\n)\n<\/code><\/pre>\n<p>using below code to run the job<\/p>\n<pre><code>job.run(\n        args=['--data_dir', '\/gcs\/bucket\/folder',\n        '--model_dir', '\/gcs\/bucket\/model',\n        '--configs', 'internal-config.yml'],\n        replica_count=1,\n        sync=True\n    )\n\n<\/code><\/pre>\n<p>Training job is getting exited with code 127 when I am passing args with job.run(). kindly help, what is the correct way to send command line arguments to custom training python script.\nThere isn\u2019t much coming up in logs either.<\/p>\n<p>Thanks in advance<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663948945487,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|custom-training",
        "Question_view_count":22,
        "Owner_creation_time":1433508703627,
        "Owner_last_access_time":1664083750830,
        "Owner_location":null,
        "Owner_reputation":130,
        "Owner_up_votes":49,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73830357",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70379395,
        "Question_title":"Vertex AI - Viewing Pipeline Output",
        "Question_body":"<p>I have followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/schedule-cloud-scheduler\" rel=\"nofollow noreferrer\">this<\/a> tutorial to create my first scheduled Vertex AI Pipeline to run every minute. The only thing it does is prints <code>&quot;Hello, &lt;any-greet-string&gt;&quot;<\/code> and also returns this same string. I can see that it is running because the last run time updates and the last run result is &quot;Success&quot; every time.<\/p>\n<p>My question is very simple: Where can I see this string printed and the output of my pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1639659363547,
        "Question_score":2,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":346,
        "Owner_creation_time":1407245826703,
        "Owner_last_access_time":1663849819650,
        "Owner_location":null,
        "Owner_reputation":745,
        "Owner_up_votes":210,
        "Owner_down_votes":6,
        "Owner_views":168,
        "Question_last_edit_time":1639726542407,
        "Answer_body":"<p>The output of the <code>print()<\/code> statements in the pipeline can be found in &quot;Cloud Logging&quot; with the appropriate filters. To check logs for each component in the pipeline, click on the respective component in the console and click &quot;<strong>VIEW LOGS<\/strong>&quot; in the right pane. A new pane with the logs will open in the pipeline page which will allow us to see the output of the component. Refer to the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I ran a sample <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#4\" rel=\"nofollow noreferrer\">pipeline<\/a> from this codelab, <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#0\" rel=\"nofollow noreferrer\">Intro to Vertex Pipelines<\/a> and below is the output for one of the <code>print()<\/code> statements in the pipeline.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<br \/>\n<h4>UPDATE:<\/h4>\n<p>Every component in a pipeline run is deployed as an individual Vertex AI custom job. Corresponding to the sample pipeline consisting 3 components, there are 3 entries in the &quot;CUSTOM JOBS&quot; section as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Therefore, to view the logs on the run level, we would need to query the log entries with the respective <code>job_id<\/code>s of the pipeline components and the <code>job_id<\/code> of the Cloud Scheduler job. The query would look like this.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.labels.job_id=(&quot;JOB_ID_1&quot; OR &quot;JOB_ID_2&quot; [OR &quot;JOB_ID_N&quot;...])\nseverity&gt;=DEFAULT\n<\/code><\/pre>\n<p>If there are no simultaneous pipeline runs, a simpler query like below can be used.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.type=(&quot;cloud_scheduler_job&quot; OR &quot;ml_job&quot;)\nseverity&gt;=DEFAULT\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1639667359983,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1639750456896,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70379395",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73300297,
        "Question_title":"Vertex AI Feature Store limits",
        "Question_body":"<p>Vertex AI has <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/quotas#featurestore\" rel=\"nofollow noreferrer\">quotas and limits<\/a>. Other than submitting very high quota requests, is there documentation on hard limits for all of the quotas? Things like &quot;online serving requests per minute&quot;, &quot;Concurrent batch jobs&quot;, and &quot;Entity types across all featurestores&quot; are pretty key constraints to know before committing to Vertex.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1660102046707,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":56,
        "Owner_creation_time":1386714193850,
        "Owner_last_access_time":1663802361323,
        "Owner_location":null,
        "Owner_reputation":3767,
        "Owner_up_votes":1128,
        "Owner_down_votes":10,
        "Owner_views":226,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73300297",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71038823,
        "Question_title":"Cannot construct an Explanation object",
        "Question_body":"<p>Trying to construct an <code>Explanation<\/code> object for a unit test, but can't seem to get it to work. Here's what I'm trying:<\/p>\n<pre><code>from google.cloud import aiplatform\n\naiplatform.compat.types.explanation_v1.Explanation(\n    attributions=aiplatform.compat.types.explanation_v1.Attribution(\n        {\n            &quot;approximation_error&quot;: 0.010399332817679649,\n            &quot;baseline_output_value&quot;: 0.9280818700790405,\n            &quot;feature_attributions&quot;: {\n                &quot;feature_1&quot;: -0.0410824716091156,\n                &quot;feature_2&quot;: 0.01155053575833639,\n            },\n            &quot;instance_output_value&quot;: 0.6717480421066284,\n            &quot;output_display_name&quot;: &quot;true&quot;,\n            &quot;output_index&quot;: [0],\n            &quot;output_name&quot;: &quot;scores&quot;,\n        }\n    )\n)\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code>&quot;.venv\/lib\/python3.7\/site-packages\/proto\/message.py&quot;, line 521, in __init__\n    super().__setattr__(&quot;_pb&quot;, self._meta.pb(**params))\nTypeError: Value must be iterable\n<\/code><\/pre>\n<p>I found <a href=\"https:\/\/github.com\/googleapis\/gapic-generator-python\/issues\/413#issuecomment-872094378\" rel=\"nofollow noreferrer\">this<\/a> on github, but I'm not sure how to apply that workaround here.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644344024883,
        "Question_score":0,
        "Question_tags":"python|protocol-buffers|google-cloud-vertex-ai",
        "Question_view_count":74,
        "Owner_creation_time":1519933306780,
        "Owner_last_access_time":1664076383160,
        "Owner_location":null,
        "Owner_reputation":3576,
        "Owner_up_votes":42,
        "Owner_down_votes":7,
        "Owner_views":203,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As the error mentioned value to be passed at <code>attributions<\/code> should be <strong>iterable<\/strong>. See <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1.types.Explanation\" rel=\"nofollow noreferrer\">Explanation attributes documentation<\/a>.<\/p>\n<p>I tried your code and placed the <code>Attribution<\/code> object in a list and the error is gone. I assigned your objects in variables just so the code is readable.<\/p>\n<p>See code and testing below:<\/p>\n<pre><code>from google.cloud import aiplatform\n\ntest = {\n            &quot;approximation_error&quot;: 0.010399332817679649,\n            &quot;baseline_output_value&quot;: 0.9280818700790405,\n            &quot;feature_attributions&quot;: {\n                &quot;feature_1&quot;: -0.0410824716091156,\n                &quot;feature_2&quot;: 0.01155053575833639,\n            },\n            &quot;instance_output_value&quot;: 0.6717480421066284,\n            &quot;output_display_name&quot;: &quot;true&quot;,\n            &quot;output_index&quot;: [0],\n            &quot;output_name&quot;: &quot;scores&quot;,\n        }\n\nattributions=aiplatform.compat.types.explanation_v1.Attribution(test)\nx  = aiplatform.compat.types.explanation_v1.Explanation(\n    attributions=[attributions]\n)\nprint(x)\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>attributions {\n  baseline_output_value: 0.9280818700790405\n  instance_output_value: 0.6717480421066284\n  feature_attributions {\n    struct_value {\n      fields {\n        key: &quot;feature_1&quot;\n        value {\n          number_value: -0.0410824716091156\n        }\n      }\n      fields {\n        key: &quot;feature_2&quot;\n        value {\n          number_value: 0.01155053575833639\n        }\n      }\n    }\n  }\n  output_index: 0\n  output_display_name: &quot;true&quot;\n  approximation_error: 0.010399332817679649\n  output_name: &quot;scores&quot;\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1644385932889,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1644387344880,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71038823",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73649262,
        "Question_title":"Create custom kernel via post-startup script in Vertex AI User Managed notebook",
        "Question_body":"<p>I am trying to use a post-startup script to create a Vertex AI User Managed Notebook whose Jupyter Lab has a dedicated virtual environment and corresponding computing kernel when first launched. I have had success creating the instance and then, as a second manual step from within the Jupyter Lab &gt; Terminal, running a bash script like so:<\/p>\n<pre><code>#!\/bin\/bash\ncd \/home\/jupyter\nmkdir -p env\ncd env\npython3 -m venv envName --system-site-packages\nsource envName\/bin\/activate\nenvName\/bin\/python3 -m pip install --upgrade pip\npython -m ipykernel install --user --name=envName\npip3 install geemap --user \npip3 install earthengine-api --user \npip3 install ipyleaflet --user \npip3 install folium --user \npip3 install voila --user \npip3 install jupyterlab_widgets\ndeactivate\njupyter labextension install --no-build @jupyter-widgets\/jupyterlab-manager jupyter-leaflet\njupyter lab build --dev-build=False --minimize=False\njupyter labextension enable @jupyter-widgets\/jupyterlab-manager\n<\/code><\/pre>\n<p>However, I have not had luck using this code as a post-startup script (being supplied through the console creation tools, as opposed to command line, thus far). When I open Jupyter Lab and look at the relevant structures, I find that there is no environment or kernel. Could someone please provide a working example that accomplishes my aim, or otherwise describe the order of build steps that one would follow?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1662640804323,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|jupyter-lab|google-cloud-vertex-ai|gcp-ai-platform-notebook",
        "Question_view_count":85,
        "Owner_creation_time":1459350905810,
        "Owner_last_access_time":1663857124693,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Post startup scripts run as root.\nWhen you run:<\/p>\n<pre><code>python -m ipykernel install --user --name=envName\n<\/code><\/pre>\n<p>Notebook is using current user which is <code>root<\/code> vs when you use Terminal, which is running as <code>jupyter<\/code> user.<\/p>\n<p>Option 1) Have 2 scripts:<\/p>\n<ul>\n<li>Script A. Contents specified in original post. Example: <code>gs:\/\/newsml-us-central1\/so73649262.sh<\/code><\/li>\n<li>Script B. Downloads script and execute it as <code>jupyter<\/code>. Example: <code>gs:\/\/newsml-us-central1\/so1.sh<\/code> and use it as post-startup script.<\/li>\n<\/ul>\n<pre><code>#!\/bin\/bash\n\nset -x\n\ngsutil cp gs:\/\/newsml-us-central1\/so73649262.sh \/home\/jupyter\nchown jupyter \/home\/jupyter\/so73649262.sh\nchmod a+x \/home\/jupyter\/so73649262.sh\nsu -c '\/home\/jupyter\/so73649262.sh' jupyter\n<\/code><\/pre>\n<p>Option 2) Create a file in bash using EOF. Write the contents into a single file and execute it as mentioned above.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1662967888203,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73649262",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73458040,
        "Question_title":"How do I enable_logging for ModelMonitoringAlertConfig in GCP?",
        "Question_body":"<p>I am trying to <code>enable_logging<\/code> in <code>ModelMonitoringAlertConfig<\/code> I have tried:<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as vertex_ai_beta\n...\n    alerting_config = vertex_ai_beta.ModelMonitoringAlertConfig(\n    enable_logging=True,\n    email_alert_config=vertex_ai_beta.ModelMonitoringAlertConfig.EmailAlertConfig(\n        user_emails=NOTIFY_EMAILS\n    )\n)\n<\/code><\/pre>\n<p>gives:<\/p>\n<pre><code> Unknown field for ModelMonitoringAlertConfig: enable_logging\n<\/code><\/pre>\n<p>but <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1.types.ModelMonitoringAlertConfig\" rel=\"nofollow noreferrer\">this<\/a> suggests it should work. What am I missing?<\/p>\n<p>(I have also tried <code>aiplatform_v1beta1<\/code>.)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661254728657,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":19,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73458040",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72988019,
        "Question_title":"Vertex AI deploy custom model to an endpoint",
        "Question_body":"<p>After I run<\/p>\n<pre><code>gcloud beta ai endpoints deploy-model (ENDPOINT : --region=REGION) --display-name=DISPLAY_NAME --model=MODEL [--accelerator=[count=COUNT],[type=TYPE]] [--deployed-model-id=DEPLOYED_MODEL_ID] [--disable-container-logging] [--enable-access-logging] [--machine-type=MACHINE_TYPE] [--max-replica-count=MAX_REPLICA_COUNT] [--min-replica-count=MIN_REPLICA_COUNT] [--service-account=SERVICE_ACCOUNT] [--traffic-split=[DEPLOYED_MODEL_ID=VALUE,\u2026]] [GCLOUD_WIDE_FLAG \u2026]\n<\/code><\/pre>\n<p>I got error &quot;(gcloud.beta.ai.endpoints.deploy-model) INVALID_ARGUMENT: AUTOMATIC_RESOURCES is not one of the supported deployment resources types for Model projects\/...\/locations\/us-central1\/models\/...<\/p>\n<p>What does this mean?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657846565487,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":63,
        "Owner_creation_time":1617317930923,
        "Owner_last_access_time":1658305735803,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72988019",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71575080,
        "Question_title":"Vertex AI Predictions using a .NET Core webapi Custom Container",
        "Question_body":"<p>I'm getting some strange behavior out of Vertex AI when calling Predictions on a custom container that is a .NET core based application.<\/p>\n<p>A little background.  I'm doing a small proof of concept to test the viability of using .NET core based Docker images as custom containers in Vertex AI.<\/p>\n<p>I have created a simple .NET core webapi that mocks out the prediction endpoint by returning what vertex-ai expects as a prediction result from the endpoint.  The Controller code is below:<\/p>\n<pre><code>[ApiController]\n[Route(&quot;api\/[controller]&quot;)]\n[Produces(&quot;application\/json&quot;)]\npublic class PredictionController : ControllerBase\n{\n    private readonly ILogger&lt;PredictionController&gt; logger;\n    public PredictionController(ILogger&lt;PredictionController&gt; logger)\n    {\n        this.logger = logger;\n    }\n\n    [HttpPost]\n    public Prediction PostPrediction(dynamic data) \n    {\n        Prediction prediction = new Prediction();\n\n        List&lt;double&gt; predictions = new List&lt;double&gt;();\n\n        predictions.Add(0.82);\n\n        prediction.Predictions = predictions;\n\n        return prediction;\n         \n    }\n}\n<\/code><\/pre>\n<p>The Dockerfile used to host the .NET core application is:<\/p>\n<pre><code>FROM mcr.microsoft.com\/dotnet\/sdk:6.0-alpine as build\nWORKDIR \/app\nCOPY . .\nRUN dotnet restore\nRUN dotnet publish -o \/app\/published-app\n\nFROM mcr.microsoft.com\/dotnet\/aspnet:6.0-alpine as runtime\nWORKDIR \/app\nCOPY --from=build \/app\/published-app \/app\nENTRYPOINT [ &quot;dotnet&quot;, &quot;\/app\/dotnet-poc.dll&quot; ]\n<\/code><\/pre>\n<p>Here is an image of hitting the dockerized .NET core app endpoint with the expected vertex-ai input and the output of the endpoint.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/YlJAj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YlJAj.png\" alt=\"docker prediction\" \/><\/a><\/p>\n<p>Following the standard documentation for deploying a model using a custom container for Vertex AI predictions, I have uploaded my docker image to Artifact Registry and imported the model using the custom container in Vertex AI.  I then deploy the model to an endpoint and run a prediction test...<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/EK1y9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EK1y9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As you can see Vertex is returning a 502 without much detail.  I cannot see any issue with the return from the prediction endpoint that should cause any error.<\/p>\n<p>Thanks for reading and looking forward to any answers that might shed light on the issue.<\/p>\n<p>Other things to note:<\/p>\n<p>Health endpoint is returning 200<\/p>\n<p>Vertex AI Endpoint is healthy<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1647964299573,
        "Question_score":0,
        "Question_tags":".net-core|google-cloud-vertex-ai",
        "Question_view_count":72,
        "Owner_creation_time":1321330022043,
        "Owner_last_access_time":1663959945940,
        "Owner_location":null,
        "Owner_reputation":174,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":40,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71575080",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68650973,
        "Question_title":"Automl SDK code with file location from bigquery but having issue while predicting",
        "Question_body":"<p>I was creating a model using GCP automl sdk module in AI Platform (using bigquery table as input for training and predicting) and predicting using batch_prediction. The issue is that the code runs fine but the output table of predictions is empty and error table has all series from prediction dataframe and addition column stating error code 3 and error is &quot;The time series has no values to predict. The time series has been excluded from predictions.&quot;.<\/p>\n<p>Code which I have used for model training:<\/p>\n<pre><code>job = aiplatform.AutoMLForecastingTrainingJob(\n    display_name='train-sdk-automl_tst1',\n    optimization_objective='minimize-mae',    \n    column_transformations=[\n        {&quot;timestamp&quot;: {&quot;column_name&quot;: &quot;Date&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Price&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Grammage&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;apparentTemperatureMax&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;apparentTemperatureMin&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Consumer_promo&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Promo_Value&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Trade_Promotion&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Holiday&quot;}},\n        {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Sales&quot;}},\n    ]\n)\n\n# This will take around an hour to run\nmy_model = job.run(\n    dataset=ds,\n    target_column='Sales',\n    time_column='Date',\n    time_series_identifier_column='SKU',\n    available_at_forecast_columns=['Date', 'Price','Grammage'\n                                   ,'apparentTemperatureMax','apparentTemperatureMin','Consumer_promo',\n                                   &quot;Promo_Value&quot;,&quot;Trade_Promotion&quot;,&quot;Holiday&quot;],\n    unavailable_at_forecast_columns=['Sales'],\n    forecast_horizon=21.0,\n    data_granularity_unit='week',\n    data_granularity_count=1,\n    weight_column=None,\n    budget_milli_node_hours=1000,\n    model_display_name='sdk_tsting_bq-forecast-model', \n    predefined_split_column_name=None\n)\n<\/code><\/pre>\n<p>Code for predictions:<\/p>\n<pre><code>BATCH_PREDICT_SOURCE = 'bq:\/\/acn-intelligent-supply-chain.scoa_ml_forecast_tool.test_data_sdk1'\nBATCH_PREDICT_DESTINATION_PREFIX = 'bq:\/\/acn-intelligent-supply-chain.scoa_ml_forecast_tool' \nmy_model.batch_predict(\n   bigquery_source=BATCH_PREDICT_SOURCE,\n   instances_format='bigquery',\n   bigquery_destination_prefix = BATCH_PREDICT_DESTINATION_PREFIX,\n   predictions_format='bigquery',\n   job_display_name='predict_sdk_tst')\n<\/code><\/pre>\n<p>Please suggest what might be going wrong here.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1628078794493,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-bigquery|sdk|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":167,
        "Owner_creation_time":1628078248313,
        "Owner_last_access_time":1637841402880,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68650973",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70916238,
        "Question_title":"AlphaFold on VertexAI - Stuck in setting up notebook for 2 hours",
        "Question_body":"<p>I am trying to run AlphaFold on VertexAI as explained <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/running-alphafold-on-vertexai\" rel=\"nofollow noreferrer\">here<\/a>. However, my instance creation is stuck in this state for roughly two hours now. There is no error message either. I am wondering if something has gone wrong or this is just the expected time it will take to setup a new instance?<\/p>\n<p>I actually tried with two different notebooks. One is the <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/raw\/main\/community-content\/alphafold_on_workbench\/AlphaFold.ipynb\" rel=\"nofollow noreferrer\">default one<\/a> linked in the above article and the other is <a href=\"https:\/\/raw.githubusercontent.com\/deepmind\/alphafold\/main\/notebooks\/AlphaFold.ipynb\" rel=\"nofollow noreferrer\">https:\/\/raw.githubusercontent.com\/deepmind\/alphafold\/main\/notebooks\/AlphaFold.ipynb<\/a><\/p>\n<p>Both are in the same state for roughly the same time.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bzK8L.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bzK8L.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643557381820,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":192,
        "Owner_creation_time":1354268805153,
        "Owner_last_access_time":1663957595997,
        "Owner_location":"Islamabad Capital Territory, Pakistan",
        "Owner_reputation":388,
        "Owner_up_votes":411,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70916238",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72439167,
        "Question_title":"Vertex AI Endpoint creation failes",
        "Question_body":"<p>When I try to deploy a google auto ml trained model to a google vertex ai endpoint (which worked 3 days ago) I suddely get an error message:<\/p>\n<p>Hello Vertex AI Customer,<\/p>\n<p>Due to an error, Vertex AI was unable to create endpoint &quot;test2&quot;.\nAdditional Details:\nOperation State: Failed with errors\nResource Name:\nprojects\/\/######\/locations\/europe-west4\/endpoints\/\/#####\nError Messages: Machine type temporarily unavailable, please deploy with a\ndifferent machine type or retry<\/p>\n<p>To view the error on Cloud Console, go back to your endpoint using\n<a href=\"https:\/\/console.cloud.google.com\/vertex-ai\/locations\/europe-west4\/models\/\/#####\/versions\/1\/deploy?authuser=1&amp;folder=&amp;organizationId=&amp;project=\/######\" rel=\"nofollow noreferrer\">https:\/\/console.cloud.google.com\/vertex-ai\/locations\/europe-west4\/models\/\/#####\/versions\/1\/deploy?authuser=1&amp;folder=&amp;organizationId=&amp;project=\/######<\/a><\/p>\n<p>Sincerely,\nThe Google Cloud AI TeamHello Vertex AI Customer,<\/p>\n<p>Does anyone have an idea why the endpoint deployment suddely does not work anymore?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1653938723073,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":217,
        "Owner_creation_time":1653938429847,
        "Owner_last_access_time":1659630685180,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72439167",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71896741,
        "Question_title":"Adding label in AutoML for text classification",
        "Question_body":"<p>I am trying to create a text dataset in a <code>Pipeline<\/code> for a text classification but I believe I am doing it the wrong way or at least I don't get it. The csv passing only contains two columns <code>message<\/code> and <code>label<\/code> which is true or false.<\/p>\n<p>Inside my pipeline I am creating dataset like this which I am not very sure how dataset is recognizing that column <code>label<\/code> is the independent variable.<\/p>\n<pre><code>dataset = gcp_aip.TextDatasetCreateOp(\n    project = project # my project id,\n    display_name = display_name # reference name,\n    gcs_source  = src_uris # path to my data in gcs,\n    import_schema_uri = aiplatform.schema.dataset.ioformat.text.single_label_classification, \n)\n<\/code><\/pre>\n<p>once created the dataset, i do training like this within the <code>Pipeline<\/code><\/p>\n<pre><code># training\nmodel = gcp_aip.AutoMLTextTrainingJobRunOp(\n    project = project,\n    display_name = display_name,\n    prediction_type = &quot;classification&quot;,\n    multi_label = False,   \n    dataset = dataset.outputs[&quot;dataset&quot;],\n)\n<\/code><\/pre>\n<p>Not sure if creation and training is doing correctly since I never specified that <code>label<\/code> is my label column and needs to use <code>message<\/code> as a feature.<\/p>\n<p>In vertex ai the dataset created look like this<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3Puts.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3Puts.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But in my training section the results from the AutML, looks like this, dont know why, label with 0% is there, which makes me doubt about the insertion of the data<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/LdSHj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LdSHj.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1650136092427,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|text|vertex|google-cloud-vertex-ai",
        "Question_view_count":158,
        "Owner_creation_time":1505748144073,
        "Owner_last_access_time":1663921056407,
        "Owner_location":null,
        "Owner_reputation":1102,
        "Owner_up_votes":108,
        "Owner_down_votes":1,
        "Owner_views":140,
        "Question_last_edit_time":1650356093116,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71896741",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73661090,
        "Question_title":"How do I give Vertex AI pipeline component permissions?",
        "Question_body":"<p>In a Vertex AI pipeline component,I try:<\/p>\n<pre><code>def my_comp(project_id: str, location: str, endpoint_id: str, endpoint: Output[Artifact]):\n    import google.cloud.aiplatform as aip\n    endpoints = aip.Endpoint.list()\n...\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code>'aiplatform.endpoints.list' denied on resource '\/\/aiplatform.googleapis.com\/projects\/...\n<\/code><\/pre>\n<p>My service account has owner permissions, and it works outside of the component. What do I need to do?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1662721061203,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":62,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This permission denied on resource issue can be resolved by using import statement:<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1663223342092,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73661090",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73351866,
        "Question_title":"Vertex AI pipelines import custom modules",
        "Question_body":"<p>I'm developing a complex pipeline in Vertex AI using Pipelines and components. I would like to import some custom modules and functions I developed for this use case.\nUnfortunately, I cannot figure out how to import those custom functions in my code without creating ad-hoc Docker images or without publishing my code on public repositories like PyPi.<\/p>\n<p>There are two pain points in pasting those custom functions' code in each component:<\/p>\n<ol>\n<li>The code becomes huge and difficult read<\/li>\n<li>The function's code completely loses the maintenability because at each small change, I have to replicate it for each component.<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1660482276650,
        "Question_score":4,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|mlops",
        "Question_view_count":132,
        "Owner_creation_time":1415266912943,
        "Owner_last_access_time":1664046818297,
        "Owner_location":"Milano, Metropolitan City of Milan, Italy",
        "Owner_reputation":107,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73351866",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72732651,
        "Question_title":"how to configure proxy in .Net application",
        "Question_body":"<p>I am facing problem while calling <strong>gcp api's (Vertex Ai &amp; Bigquery)<\/strong>. we have been using these api's from last few months. The behavior is unpredictable, sometimes we get api response successfully and sometimes its failing.<\/p>\n<p>what we noticed that the same host <strong>us-central1-aiplatform.googleapis.com<\/strong> communication is disrupted.<\/p>\n<p>Currently we are connecting with <strong>GOOGLE_APPLICATION_CREDENTIALS<\/strong>.<\/p>\n<p>I want to go through proxy. any suggestions ? or any other way I can solve this ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655997589337,
        "Question_score":1,
        "Question_tags":"c#|.net|google-cloud-platform|google-bigquery|google-cloud-vertex-ai",
        "Question_view_count":57,
        "Owner_creation_time":1462469556837,
        "Owner_last_access_time":1663877676583,
        "Owner_location":null,
        "Owner_reputation":509,
        "Owner_up_votes":71,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72732651",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72591967,
        "Question_title":"Vertex workbench - how to run BigQueryExampleGen in Jupyter notebook",
        "Question_body":"<h1>Problem<\/h1>\n<p>Tried to run <a href=\"https:\/\/www.tensorflow.org\/tfx\/api_docs\/python\/tfx\/v1\/extensions\/google_cloud_big_query\/BigQueryExampleGen\" rel=\"nofollow noreferrer\">BigQueryExampleGen<\/a> in the<\/p>\n<pre><code>InvalidUserInputError: Request missing required parameter projectId [while running 'InputToRecord\/QueryTable\/ReadFromBigQuery\/Read\/SDFBoundedSourceReader\/ParDo(SDFBoundedSourceDoFn)\/SplitAndSizeRestriction']\n<\/code><\/pre>\n<h2>Steps<\/h2>\n<p><a href=\"https:\/\/www.tensorflow.org\/tfx\/api_docs\/python\/tfx\/v1\/extensions\/google_cloud_big_query\/BigQueryExampleGen\" rel=\"nofollow noreferrer\">BigQueryExampleGen<\/a>\nSetup the GCP project and the interactive TFX context.<\/p>\n<pre><code>import os\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = &quot;path_to_credential_file&quot;\n\n\nfrom tfx.v1.extensions.google_cloud_big_query import BigQueryExampleGen\nfrom tfx.v1.components import (\n    StatisticsGen,\n    SchemaGen,\n)\nfrom tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip\ncontext = InteractiveContext(pipeline_root='.\/data\/artifacts')\n<\/code><\/pre>\n<p>Run the BigqueryExampleGen.<\/p>\n<pre><code>query = &quot;&quot;&quot;\nSELECT \n    * EXCEPT (trip_start_timestamp, ML_use)\nFROM \n    {PROJECT_ID}.public_dataset.chicago_taxitrips_prep\n&quot;&quot;&quot;.format(PROJECT_ID=PROJECT_ID)\n\nexample_gen = context.run(\n    BigQueryExampleGen(query=query)\n)\n<\/code><\/pre>\n<p>Got the error.<\/p>\n<pre><code>InvalidUserInputError: Request missing required parameter projectId [while running 'InputToRecord\/QueryTable\/ReadFromBigQuery\/Read\/SDFBoundedSourceReader\/ParDo(SDFBoundedSourceDoFn)\/SplitAndSizeRestriction']\n<\/code><\/pre>\n<h1>Data<\/h1>\n<p>See <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/01-dataset-management.ipynb\" rel=\"nofollow noreferrer\">mlops-with-vertex-ai\/01-dataset-management.ipynb<\/a> to setup the BigQuery dataset for CThe Chicago Taxi Trips dataset.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655034967900,
        "Question_score":0,
        "Question_tags":"google-bigquery|google-cloud-vertex-ai|tfx|tensorflow-extended",
        "Question_view_count":125,
        "Owner_creation_time":1416648155470,
        "Owner_last_access_time":1664057583237,
        "Owner_location":null,
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72591967",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71885282,
        "Question_title":"Constructing a Vertex AI Pipeline with a custom training container and a model serving container",
        "Question_body":"<p>I'd like to be able to train a model with a training app container that I've made and saved to my artifact registry. I want to be able to deploy a model with a flask app and with a \/predict route that can handle some logic -- not necessarily just predicting an input json. It'll also need a \/healthz route I understand. So basically I want a pipeline that performs a training job on a model training container that I make, and deploys the model with a flask app with a model serving container that I make. Looking around on Overflow, I wonder if <a href=\"https:\/\/stackoverflow.com\/questions\/68075940\/vertex-pipeline-custompythonpackagetrainingjobrunop-not-supplying-workerpoolspe\">this<\/a> question's pipeline has the correct layout I'll eventually want to have. So, something like this:<\/p>\n<pre><code>import kfp\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import component\nfrom kfp.v2.google import experimental\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\n\n@kfp.dsl.pipeline(name=pipeline_name, pipeline_root=pipeline_root_path)\ndef pipeline():\n        training_job_run_op = gcc_aip.CustomPythonPackageTrainingJobRunOp(\n            project=project_id,\n            display_name=training_job_name,\n            model_display_name=model_display_name,\n            python_package_gcs_uri=python_package_gcs_uri,\n            python_module=python_module,\n            container_uri=container_uri,\n            staging_bucket=staging_bucket,\n            model_serving_container_image_uri=model_serving_container_image_uri)\n\n        # Upload model\n        model_upload_op = gcc_aip.ModelUploadOp(\n            project=project_id,\n            display_name=model_display_name,\n            artifact_uri=output_dir,\n            serving_container_image_uri=model_serving_container_image_uri,\n        )\n        model_upload_op.after(training_job_run_op)\n\n        # Deploy model\n        model_deploy_op = gcc_aip.ModelDeployOp(\n            project=project_id,\n            model=model_upload_op.outputs[&quot;model&quot;],\n            endpoint=aiplatform.Endpoint(\n                endpoint_name='0000000000').resource_name,\n            deployed_model_display_name=model_display_name,\n            machine_type=&quot;n1-standard-2&quot;,\n            traffic_percentage=100)\n\n    compiler.Compiler().compile(pipeline_func=pipeline,\n                                package_path=pipeline_spec_path)\n<\/code><\/pre>\n<p>I'm hoping that <code>model_serving_container_image_uri<\/code> and <code>serving_container_image_uri<\/code> both refer to the URI for the model serving container I'm going to make. I've already made a training container that trains a model and saves <code>saved_model.pb<\/code> to Google Cloud Storage. Other than having a flask app that handles the prediction and health check routes and a Dockerfile that exposes a port for the flask app, what else will I need to do to ensure the model serving container works in this pipeline? Where in the code do I install the model from GCS? In the Dockerfile? How is the model serving container meant to work so that everything will go swimmingly in the construction of the pipeline? I'm having trouble finding any tutorials or examples of precisely what I'm trying to do anywhere even though this seems like a pretty common scenario.<\/p>\n<p>To that end, I attempted this with the following pipeline:<\/p>\n<pre><code>import kfp\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import component\nfrom kfp.v2.google import experimental\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\n\n@kfp.dsl.pipeline(name=pipeline_name, pipeline_root=pipeline_root_path)\ndef pipeline(\n        project: str = [redacted project ID],\n        display_name: str = &quot;custom-pipe&quot;,\n        model_display_name: str = &quot;test_model&quot;,\n        training_container_uri: str = &quot;us-central1-docker.pkg.dev\/[redacted project ID]\/custom-training-test&quot;,\n        model_serving_container_image_uri: str = &quot;us-central1-docker.pkg.dev\/[redacted project ID]\/custom-model-serving-test&quot;,\n        model_serving_container_predict_route: str = &quot;\/predict&quot;,\n        model_serving_container_health_route: str = &quot;\/healthz&quot;,\n        model_serving_container_ports: str = &quot;8080&quot;\n):\n        training_job_run_op = gcc_aip.CustomContainerTrainingJobRunOp(\n            display_name = display_name,\n            container_uri=training_container_uri,\n            model_serving_container_image_uri=model_serving_container_image_uri,\n            model_serving_container_predict_route = model_serving_container_predict_route,\n            model_serving_container_health_route = model_serving_container_health_route,\n            model_serving_container_ports = model_serving_container_ports)\n\n        # Upload model\n        model_upload_op = gcc_aip.ModelUploadOp(\n            project=project,\n            display_name=model_display_name,\n            serving_container_image_uri=model_serving_container_image_uri,\n        )\n        model_upload_op.after(training_job_run_op)\n\n        # Deploy model\n#        model_deploy_op = gcc_aip.ModelDeployOp(\n#            project=project,\n#            model=model_upload_op.outputs[&quot;model&quot;],\n#            endpoint=aiplatform.Endpoint(\n#                endpoint_name='0000000000').resource_name,\n#            deployed_model_display_name=model_display_name,\n#            machine_type=&quot;n1-standard-2&quot;,\n#            traffic_percentage=100)\n<\/code><\/pre>\n<p>Which is failing with<\/p>\n<pre><code>google.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.trainingPipelines.create' denied on resource '\/\/aiplatform.googleapis.com\/projects\/u15c36a5b7a72fabfp-tp\/locations\/us-central1' (or it may not exist).\n<\/code><\/pre>\n<p>Despite the fact that my service account has the Viewer and Kubernetes Engine Admin roles needed to work AI Platform pipelines. My training container uploads my model to Google Cloud Storage and my model serving container I've made downloads it and uses it for serving at <code>\/predict<\/code>.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1650032900060,
        "Question_score":3,
        "Question_tags":"google-cloud-platform|google-cloud-storage|google-cloud-ml|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":421,
        "Owner_creation_time":1649861924490,
        "Owner_last_access_time":1663770236590,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1650387987396,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71885282",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71813169,
        "Question_title":"Run a Vertex AI model locally",
        "Question_body":"<p>Using the Vertex AI product at GCP training was very easy, I uploaded a data set and it returned a model which is saved in a gcp bucket, I downloaded the files and the tree has these files<\/p>\n<pre><code>\u251c\u2500\u2500 environment.json\n\u251c\u2500\u2500 feature_attributions.yaml\n\u251c\u2500\u2500 final_model_structure.pb\n\u251c\u2500\u2500 instance.yaml\n\u251c\u2500\u2500 predict\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 001\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 assets\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 PVC_vocab\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 assets.extra\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 tf_serving_warmup_requests\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 saved_model.pb\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 variables\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 variables.data-00000-of-00001\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 variables.index\n\u251c\u2500\u2500 prediction_schema.yaml\n\u251c\u2500\u2500 tables_server_metadata.pb\n\u2514\u2500\u2500 transformations.pb\n<\/code><\/pre>\n<p>I would like to serve this model locally from a dockerized python application, but I don't know enough TF to do this and I am very confused about which <code>.pb<\/code> file is the actual one that has the neural network I need.<\/p>\n<p>Thanks for any tips.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1649554815260,
        "Question_score":0,
        "Question_tags":"python|docker|tensorflow|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":261,
        "Owner_creation_time":1483057875593,
        "Owner_last_access_time":1664081219423,
        "Owner_location":"St. Louis, MO, USA",
        "Owner_reputation":710,
        "Owner_up_votes":43,
        "Owner_down_votes":16,
        "Owner_views":175,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71813169",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72593351,
        "Question_title":"Jupyter Lab instance crashes with 502 error",
        "Question_body":"<p>I am using a JupyterLab virtual notebook instance from GCP Vertex AI Workbench.<\/p>\n<p>I am reading 2 billion rows of data where each row is comprised of 3 columns of\n8 bytes each.<\/p>\n<p>I am reading 100 million rows of data at a time and concatenating it to Pandas dataframe.<\/p>\n<p>All of sudden, the notebook becomes unresponsive with 502 error.<\/p>\n<p>I realize that the virtual machine crashed.<\/p>\n<p>Here is the spec to the virtual machine:\nn1-standard 64  240GB RAM\n100 GB drive<\/p>\n<p>One time, I was successful to reach 2 billion rows.\nBut all of sudden, to my dismay, it crashed with that error.<\/p>\n<p>Google doc just mentions to restart the kernel.\nThat is not so easy when it took more than 1 hour to read 2 billion rows of data.\nThis means more than 1 hour of work just got wasted.<\/p>\n<p>What is causing this error?\nWhy the error occurs so inconsistently?\nWhere is the error message for this to crash?\nOr is this an error related to pandas dataframe?\nI am creating a dataframe that have 2 billion rows.\nIf pandas cannot handle rows of this magnitude, it should simply\ncause a run time error, not crashing a virtual machine.<\/p>\n<p>Thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655046019523,
        "Question_score":1,
        "Question_tags":"pandas|google-cloud-platform|jupyter-lab|google-cloud-vertex-ai",
        "Question_view_count":146,
        "Owner_creation_time":1468198346110,
        "Owner_last_access_time":1664078356887,
        "Owner_location":"overland park, kansas",
        "Owner_reputation":932,
        "Owner_up_votes":192,
        "Owner_down_votes":10,
        "Owner_views":367,
        "Question_last_edit_time":1655255619327,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72593351",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73360734,
        "Question_title":"Programmatically enable installed extensions in Vertex AI Managed Notebook instance",
        "Question_body":"<p>I am working in JupyterLab within a Managed Notebook instance, accessed through the Vertex AI workbench, as part of a Google Cloud Project. When the instance is created, there are a number of JupyterLab extensions that are installed by default. In the web GUI, one can click the puzzle piece icon and enable\/disable all extensions with a single button click. I currently run a post-startup bash script to manage environments and module installations, and I would like to add to this script whatever commands would turn on the existing extensions. My understanding is that I can do this with<\/p>\n<pre><code># Status of extensions\njupyter labextension list\n# Enable\/disable some extension\njupyter labextension enable extensionIdentifierHere\n<\/code><\/pre>\n<p>However, when I test the enable\/disable command in an instance Terminal window, I receive, for example<\/p>\n<pre><code>[Errno 13] Permission denied: '\/opt\/conda\/etc\/jupyter\/labconfig\/page_config.json'\n<\/code><\/pre>\n<p>If I try to run this with <code>sudo<\/code>, I am asked for a password, but have no idea what that would be, given that I just built the environment and didn't set any password.<\/p>\n<p>Any insights on how to set this up, what the command(s) may be, or how else to approach this, would be appreciated.<\/p>\n<p>Potentially relevant:<\/p>\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/65950610\/not-able-to-install-jupyterlab-extensions-on-gcp-ai-platform-notebooks\">Not able to install Jupyterlab extensions on GCP AI Platform Notebooks<\/a><\/p>\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/52753205\/unable-to-sudo-to-deep-learning-image\">Unable to sudo to Deep Learning Image<\/a><\/p>\n<p><a href=\"https:\/\/jupyterlab.readthedocs.io\/en\/stable\/user\/extensions.html#enabling-and-disabling-extensions\" rel=\"nofollow noreferrer\">https:\/\/jupyterlab.readthedocs.io\/en\/stable\/user\/extensions.html#enabling-and-disabling-extensions<\/a><\/p>\n<p>Edit 1:\nAdding more detail in response to answers and comments (@gogasca, @kiranmathew). My goal is to use ipyleaft-based mapping, through the geemap and earthengine-api python modules, within the notebook. If I create a Managed Notebook instance (service account, Networks shared with me, Enable terminal, all other defaults), launch JupyterLab, open the Terminal from the Launcher, and then run a bash script that creates a venv virtual environment, exposes a custom kernel, and performs the installations, I can use geemap and ipywidgets to visualize and modify (e.g., widget sliders that change map properties) Google Earth Engine assets in a Notebook. If I try to replicate this using a Docker image, it seems to break the connection with ipyleaflet, such that when I start the instance and use a Notebook, I have access to the modules (they can be imported) but can't use ipyleaflet to do the visualization. I thought the issue was that I was not properly enabling the extensions, per the &quot;Error displaying widget: model not found&quot; error, addressed in <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/504\" rel=\"nofollow noreferrer\">this<\/a>, <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/889\" rel=\"nofollow noreferrer\">this<\/a>, <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/547\" rel=\"nofollow noreferrer\">this<\/a>, <a href=\"https:\/\/leafmap.org\/faq\/\" rel=\"nofollow noreferrer\">this<\/a>, etc. -- hence the title of my post. I tried using and modifying @TylerErickson 's Dockerfile that modifies a Google deep learning container and should handle all of this (<a href=\"https:\/\/github.com\/gee-community\/ee-jupyter-contrib\/blob\/master\/docker\/gcp_ai_deep_learning_platform\/Dockerfile\" rel=\"nofollow noreferrer\">here<\/a>), but both the original and modifications break the ipyleaflet connection when booting the Managed Notebook instance from the Docker image.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1660564868170,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|jupyter-lab|google-cloud-vertex-ai|gcp-ai-platform-notebook",
        "Question_view_count":202,
        "Owner_creation_time":1459350905810,
        "Owner_last_access_time":1663857124693,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1660731534620,
        "Answer_body":"<p>Google Managed Notebooks do not support third-party JL extensions.  Most of these extensions require a rebuild of the JupyterLab static assets bundle. This requires root access which our Managed Notebooks do not support.<\/p>\n<p>Untangling this limitation would require a significant change to the permission and security model that Managed Notebooks provides. It would also have implications for the supportability of the product itself since a user could effectively break their Managed Notebook by installing something rogue.<\/p>\n<p>I would suggest to use User Managed Notebooks.<\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_time":1660628912063,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1660634341887,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73360734",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70428593,
        "Question_title":"Vertex AI seems to think a deployed model input shape is different then when predicting locally",
        "Question_body":"<p>I'm getting what I see as strange behavior out of a deployed model in vertex ai.  I have a CNN model built with tensorflow\/keras version 2.7.  My input data is a 3 dimensional array with the follow shape (1, 570, 33).  When I pass the input data to the model locally I have a correct response.<\/p>\n<pre><code>model = keras.models.load_model('model')\nx = model.predict(input_data) # input_data is a numpy array of shape (1, 570, 33)   \nprint(x)\n[[0.1259355  0.9124526  0.65782744 0.2628207 ]]\n<\/code><\/pre>\n<p>This is a correct prediction and the model does what it is trained to do.  No problems<\/p>\n<p>When I upload the model to Vertex AI using the prebuilt Tensorflow 2.7 docker container with no extra settings (no acceleration for example) and deploy that model to an endpoint this is what I get when I call predict with the same input_data formatted for Vertex AI.<\/p>\n<pre><code>resp = client.predict(\n    endpoint=endpoint_path,\n    instances=input_data.toList(),\n    parameters=parameters,\n)\n\ninput must be 4-dimensional[1,570,33]\\n\\t [[{{function_node __inference__wrapped_model_28143}}{{node sequential\/conv2d\/BiasAdd}}]]\n<\/code><\/pre>\n<p>Here is the summary of of the model<\/p>\n<pre><code>Model: &quot;sequential&quot;\n_________________________________________________________________\nLayer (type)                Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)             (None, 570, 33, 32)       320       \n                                                                \nbatch_normalization (BatchN  (None, 570, 33, 32)      128       \normalization)                                                   \n                                                                \nactivation (Activation)     (None, 570, 33, 32)       0         \n                                                                \nconv2d_1 (Conv2D)           (None, 570, 33, 32)       9248      \n                                                                \nbatch_normalization_1 (Batc  (None, 570, 33, 32)      128       \nhNormalization)                                                 \n                                                                \nactivation_1 (Activation)   (None, 570, 33, 32)       0         \n                                                                \nconv2d_2 (Conv2D)           (None, 570, 33, 32)       9248      \n                                                                \nbatch_normalization_2 (Batc  (None, 570, 33, 32)      128       \nhNormalization)                                                 \n                                                                \nactivation_2 (Activation)   (None, 570, 33, 32)       0         \n                                                                \nconv2d_3 (Conv2D)           (None, 285, 17, 64)       18496     \n                                                                \nbatch_normalization_3 (Batc  (None, 285, 17, 64)      256       \nhNormalization)                                                 \n                                                                \nactivation_3 (Activation)   (None, 285, 17, 64)       0         \n                                                                \nconv2d_4 (Conv2D)           (None, 285, 17, 64)       36928     \n                                                                \nbatch_normalization_4 (Batc  (None, 285, 17, 64)      256       \nhNormalization)                                                 \n                                                                \nactivation_4 (Activation)   (None, 285, 17, 64)       0         \n                                                                \nconv2d_5 (Conv2D)           (None, 285, 17, 64)       36928     \n                                                                \nbatch_normalization_5 (Batc  (None, 285, 17, 64)      256       \nhNormalization)                                                 \n                                                                \nactivation_5 (Activation)   (None, 285, 17, 64)       0         \n                                                                \nconv2d_6 (Conv2D)           (None, 285, 17, 64)       36928     \n                                                                \nbatch_normalization_6 (Batc  (None, 285, 17, 64)      256       \nhNormalization)                                                 \n                                                                \nactivation_6 (Activation)   (None, 285, 17, 64)       0         \n                                                                \nconv2d_7 (Conv2D)           (None, 143, 9, 96)        55392     \n                                                                \nbatch_normalization_7 (Batc  (None, 143, 9, 96)       384       \nhNormalization)                                                 \n                                                                \nactivation_7 (Activation)   (None, 143, 9, 96)        0         \n                                                                \nconv2d_8 (Conv2D)           (None, 143, 9, 96)        83040     \n                                                                \nbatch_normalization_8 (Batc  (None, 143, 9, 96)       384       \nhNormalization)                                                 \n                                                                \nactivation_8 (Activation)   (None, 143, 9, 96)        0         \n                                                                \nconv2d_9 (Conv2D)           (None, 143, 9, 96)        83040     \n                                                                \nbatch_normalization_9 (Batc  (None, 143, 9, 96)       384       \nhNormalization)                                                 \n                                                                \nactivation_9 (Activation)   (None, 143, 9, 96)        0         \n                                                                \nconv2d_10 (Conv2D)          (None, 143, 9, 96)        83040     \n                                                                \nbatch_normalization_10 (Bat  (None, 143, 9, 96)       384       \nchNormalization)                                                \n                                                                \nactivation_10 (Activation)  (None, 143, 9, 96)        0         \n                                                                \nconv2d_11 (Conv2D)          (None, 72, 5, 128)        110720    \n                                                                \nbatch_normalization_11 (Bat  (None, 72, 5, 128)       512       \nchNormalization)                                                \n                                                                \nactivation_11 (Activation)  (None, 72, 5, 128)        0         \n                                                                \nconv2d_12 (Conv2D)          (None, 72, 5, 128)        147584    \n                                                                \nbatch_normalization_12 (Bat  (None, 72, 5, 128)       512       \nchNormalization)                                                \n                                                                \nactivation_12 (Activation)  (None, 72, 5, 128)        0         \n                                                                \nconv2d_13 (Conv2D)          (None, 72, 5, 128)        147584    \n                                                                \nbatch_normalization_13 (Bat  (None, 72, 5, 128)       512       \nchNormalization)                                                \n                                                                \nactivation_13 (Activation)  (None, 72, 5, 128)        0         \n                                                                \nconv2d_14 (Conv2D)          (None, 72, 5, 128)        147584    \n                                                                \nbatch_normalization_14 (Bat  (None, 72, 5, 128)       512       \nchNormalization)                                                \n                                                                \nactivation_14 (Activation)  (None, 72, 5, 128)        0         \n                                                                \nconv2d_15 (Conv2D)          (None, 36, 3, 160)        184480    \n                                                                \nbatch_normalization_15 (Bat  (None, 36, 3, 160)       640       \nchNormalization)                                                \n                                                                \nactivation_15 (Activation)  (None, 36, 3, 160)        0         \n                                                                \nconv2d_16 (Conv2D)          (None, 36, 3, 160)        230560    \n                                                                \nbatch_normalization_16 (Bat  (None, 36, 3, 160)       640       \nchNormalization)                                                \n                                                                \nactivation_16 (Activation)  (None, 36, 3, 160)        0         \n                                                                \nconv2d_17 (Conv2D)          (None, 36, 3, 160)        230560    \n                                                                \nbatch_normalization_17 (Bat  (None, 36, 3, 160)       640       \nchNormalization)                                                \n                                                                \nactivation_17 (Activation)  (None, 36, 3, 160)        0         \n                                                                \nconv2d_18 (Conv2D)          (None, 36, 3, 160)        230560    \n                                                                \nbatch_normalization_18 (Bat  (None, 36, 3, 160)       640       \nchNormalization)                                                \n                                                                \nactivation_18 (Activation)  (None, 36, 3, 160)        0         \n                                                                \nconv2d_19 (Conv2D)          (None, 18, 2, 192)        276672    \n                                                                \nbatch_normalization_19 (Bat  (None, 18, 2, 192)       768       \nchNormalization)                                                \n                                                                \nactivation_19 (Activation)  (None, 18, 2, 192)        0         \n                                                                \nconv2d_20 (Conv2D)          (None, 18, 2, 192)        331968    \n                                                                \nbatch_normalization_20 (Bat  (None, 18, 2, 192)       768       \nchNormalization)                                                \n                                                                \nactivation_20 (Activation)  (None, 18, 2, 192)        0         \n                                                                \nconv2d_21 (Conv2D)          (None, 18, 2, 192)        331968    \n                                                                \nbatch_normalization_21 (Bat  (None, 18, 2, 192)       768       \nchNormalization)                                                \n                                                                \nactivation_21 (Activation)  (None, 18, 2, 192)        0         \n                                                                \nconv2d_22 (Conv2D)          (None, 18, 2, 192)        331968    \n                                                                \nbatch_normalization_22 (Bat  (None, 18, 2, 192)       768       \nchNormalization)                                                \n                                                                \nactivation_22 (Activation)  (None, 18, 2, 192)        0         \n                                                                \nconv2d_23 (Conv2D)          (None, 9, 1, 224)         387296    \n                                                                \nbatch_normalization_23 (Bat  (None, 9, 1, 224)        896       \nchNormalization)                                                \n                                                                \nactivation_23 (Activation)  (None, 9, 1, 224)         0         \n                                                                \nreshape (Reshape)           (None, 9, 224)            0         \n                                                                \nmasking (Masking)           (None, 9, 224)            0         \n                                                                \nlambda (Lambda)             (None, 224)               0         \n                                                                \ndense (Dense)               (None, 4)                 900       \n                                                                \n=================================================================\nTotal params: 3,554,532\nTrainable params: 3,548,772\nNon-trainable params: 5,760\n<\/code><\/pre>\n<p>I've got a classic case of 'It works on my machine' here and could use any input or help :)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640038358063,
        "Question_score":1,
        "Question_tags":"python|numpy|tensorflow|keras|google-cloud-vertex-ai",
        "Question_view_count":178,
        "Owner_creation_time":1321330022043,
        "Owner_last_access_time":1663959945940,
        "Owner_location":null,
        "Owner_reputation":174,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":40,
        "Question_last_edit_time":1640046263672,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70428593",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71755724,
        "Question_title":"Cannot use tensorboard with Vertex AI Custom job",
        "Question_body":"<p>I'm trying to launch a custom training job using Vertex AI through <a href=\"https:\/\/github.com\/deepmind\/xmanager\" rel=\"nofollow noreferrer\">XManager<\/a>. When running Custom jobs with tensorboard enabled I get a tensorboard instance in <code>experiments -&gt; tensorboard instances<\/code> and a button on the custom job page that says <code>OPEN TENSORBOARD<\/code>. However, this leads to an empty page that says <code>Not found: TensorboardExperiment<\/code>.<\/p>\n<ul>\n<li>I observed this behaviour when running my own custom job and when running XManager's example <a href=\"https:\/\/github.com\/deepmind\/xmanager\/tree\/main\/examples\/cifar10_tensorflow\" rel=\"nofollow noreferrer\">cifar10_tensorflow<\/a>. Note that in both cases the job runs to completion without problems.<\/li>\n<li>I can visualise the logs locally via the standard tensorboard package and passing as <code>log_dir<\/code> the cloud storage directory containing the experiments logs.<\/li>\n<li>I can upload experiment logs to Vertex AI tensorboard manually using<\/li>\n<\/ul>\n<pre><code>tb-gcp-uploader --tensorboard_resource_name \\\n  TENSORBOARD_INSTANCE_NAME \\\n  --logdir=LOG_DIR \\\n  --experiment_name=TB_EXPERIMENT_NAME --one_shot=True\n<\/code><\/pre>\n<ul>\n<li>For more details check out the discussion: <a href=\"https:\/\/github.com\/deepmind\/xmanager\/issues\/15\" rel=\"nofollow noreferrer\">https:\/\/github.com\/deepmind\/xmanager\/issues\/15<\/a><\/li>\n<\/ul>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1649178281173,
        "Question_score":1,
        "Question_tags":"tensorboard|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":216,
        "Owner_creation_time":1451919318690,
        "Owner_last_access_time":1661428260770,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1649180370236,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71755724",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71941672,
        "Question_title":"How to build custom pipeline in GCP using Vertex AI",
        "Question_body":"<p>I was exploring the vertex AI AutoML feature in GCP, which lets users import datasets, train, deploy and predict ML models. My use case is to do the data pre-processing on my own (I didn't get satisfied with AutoML data preprocessing) and want to feed that data directly to a pipeline where it trains and deploys the model.\nAlso, I want to feed the new data to the dataset. It should take care of the entire pipeline (from data preprocessing to deploying the latest model).\nI want insight as to how to approach this problem?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1650465785530,
        "Question_score":2,
        "Question_tags":"machine-learning|google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai|gcp-ai-platform-training",
        "Question_view_count":264,
        "Owner_creation_time":1549305339163,
        "Owner_last_access_time":1664020879733,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":454,
        "Owner_up_votes":43,
        "Owner_down_votes":1,
        "Owner_views":79,
        "Question_last_edit_time":1650551747143,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71941672",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68998065,
        "Question_title":"Read vertex ai datasets in jupyter notebook",
        "Question_body":"<p>I am trying to create a python utility that will take dataset from vertex ai datasets and will generate statistics for that dataset. But I am unable to check the dataset using jupyter notebook. Is there any way out for this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1630410227747,
        "Question_score":3,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":737,
        "Owner_creation_time":1616070829583,
        "Owner_last_access_time":1643884535477,
        "Owner_location":null,
        "Owner_reputation":45,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If I understand correctly, you want to use <a href=\"https:\/\/cloud.google.com\/vertex-ai\" rel=\"nofollow noreferrer\">Vertex AI<\/a> dataset inside <code>Jupyter Notebook<\/code>. I don't think that this is currently possible. You are able to export <code>Vertex AI<\/code> datasets to <code>Google Cloud Storage<\/code> in JSONL format:<\/p>\n<blockquote>\n<p>Your dataset will be exported as a list of text items in JSONL format. Each row contains a Cloud Storage path, any label(s) assigned to that item, and a flag that indicates whether that item is in the training, validation, or test set.<\/p>\n<\/blockquote>\n<p>At this moment, you can use <code>BigQuery<\/code> data inside <code>Notebook<\/code> using <code>%%bigquery<\/code> like it's mentioned in <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/visualize-jupyter\" rel=\"nofollow noreferrer\">Visualizing BigQuery data in a Jupyter notebook.<\/a> or use <code>csv_read()<\/code> from machine directory or <code>GCS<\/code> like it's showed in the <a href=\"https:\/\/stackoverflow.com\/questions\/61956470\/\">How to read csv file in Google Cloud Platform jupyter notebook<\/a> thread.<\/p>\n<p>However, you can fill a <code>Feature Request<\/code> in <a href=\"https:\/\/developers.google.com\/issue-tracker\" rel=\"nofollow noreferrer\">Google Issue Tracker<\/a> to add the possibility to use <code>VertexAI<\/code> dataset directly in the <code>Jupyter Notebook<\/code> which will be considered by the <code>Google Vertex AI Team<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1631634873007,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68998065",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73033284,
        "Question_title":"How to extract model file(s) from Vertex AI?",
        "Question_body":"<p>I have been trying to import model file(s) from Vertex AI to Workbench environment. My pipeline consists of preprocessing, training and batch predictions. Sometimes the training fails due to unknown reasons yet I want the batch predictions from the latest model in that case. Is there a way to access the trained models using Python?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1658217944860,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":34,
        "Owner_creation_time":1649934777040,
        "Owner_last_access_time":1663995816517,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1658221416540,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73033284",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72481717,
        "Question_title":"Google AutoML Video Tracking Architecture",
        "Question_body":"<p>I'm developing an object tracking system using Google's Vertex AI AutoML Video Tracking. We currently have an accurate model that identifies objects per frame (as a picture) and I'm exploring models that may be able to gain further insight and accuracy by using a collection of frames (video) for the classification and tracking purposes. I want to learn more about the architecture used in the AutoML Object Tracking, but all I can find is articles hyping up the dynamic nature of the architecture. Mainly, I'm trying to answer the following 3 questions:<\/p>\n<ol>\n<li>What methods does the AutoML Object Tracking use to classify the objects and track them? Are the classifications done frame to frame, with a Euclidean distance tracker mapping objects together? Or are the objects identified and classified across multiple frames a recurrent network in space (image) and time (frame to frame). Something like a LSTM.<\/li>\n<li>What performance can object tracking in AutoML achieve that is better than their image object identification models?<\/li>\n<li>Where can I go to learn more about the model architectures on Vertex AI? It's hard to know which google publications are associated with their current platform.<\/li>\n<\/ol>\n<p>Any feedback is greatly appreciated!!!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1654200034790,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|computer-vision|google-cloud-automl|google-cloud-vertex-ai|object-tracking",
        "Question_view_count":27,
        "Owner_creation_time":1654199427997,
        "Owner_last_access_time":1654825517947,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72481717",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72746403,
        "Question_title":"How to call a Google Vertex AI endpoint with an Api Key with c# and curl",
        "Question_body":"<p>from a c# program i want to call my <strong>vertex ai endpoint<\/strong> for prediction with an <strong>api key<\/strong> (via <strong>&quot;Google Cloud&quot;\/Credentials\/API Keys&quot;<\/strong> )). I gave the api key access to Vertex AI and as a test everything else too.<\/p>\n<p>calling it with curl or c# i get the error that the service expects an &quot;OAuth 2 access token &quot; or something else.<\/p>\n<p><strong>CURL:<\/strong>\ncurl -X POST\n-H &quot;apikey=..mykey...&quot; -H &quot;Content-Type: application\/json&quot; https:\/\/.....googleapis.com\/v1\/projects\/[PROJECT_ID]\/locations\/europe-west4\/endpoints\/[ENDPOINT_ID]:predict\n-d @image.json<\/p>\n<p><strong>ERROR:<\/strong>\n&quot;error&quot;: {\n&quot;code&quot;: 401,\n&quot;message&quot;: &quot;Request is missing required authentication credential. Expected OAuth 2 access token, login cookie or other valid authentication credential. ...\n}<\/p>\n<p><strong>MY QUESTION:<\/strong>\nIs there a way to use the Apikey for authentication to vertex ai?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1656085117850,
        "Question_score":0,
        "Question_tags":"c#|authentication|curl|api-key|google-cloud-vertex-ai",
        "Question_view_count":229,
        "Owner_creation_time":1656083586047,
        "Owner_last_access_time":1662660715730,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72746403",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69721067,
        "Question_title":"GoogleAPICallError: None Unexpected state: Long-running operation had neither response nor error set",
        "Question_body":"<p>I'm new to Google Cloud Platform and I'm trying to create a Feature Store to fill with values from a csv file from Google Cloud Storage. The aim is to do that from a local notebook in Python.\nI'm basically following the code <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/official\/feature_store\/gapic-feature-store.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, making the appropriate changes since I'm working with the credit card public dataset.\nThe error that raises when I run the code is the following:<\/p>\n<pre><code>GoogleAPICallError: None Unexpected state: Long-running operation had neither response nor error set.\n<\/code><\/pre>\n<p>and it happens during the ingestion of the data from the csv file.<\/p>\n<p>Here it is the code I'm working on:<\/p>\n<pre><code>import os\nfrom datetime import datetime\nfrom google.cloud import bigquery\nfrom google.cloud import aiplatform\nfrom google.cloud.aiplatform_v1.types import feature as feature_pb2\nfrom google.cloud.aiplatform_v1.types import featurestore as featurestore_pb2\nfrom google.cloud.aiplatform_v1.types import \\\n    featurestore_service as featurestore_service_pb2\nfrom google.cloud.aiplatform_v1.types import entity_type as entity_type_pb2\nfrom google.cloud.aiplatform_v1.types import FeatureSelector, IdMatcher\n\ncredential_path = r&quot;C:\\Users\\...\\.json&quot;\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n\n## Constants\nPROJECT_ID = &quot;my-project-ID&quot;\nREGION = &quot;us-central1&quot;\nAPI_ENDPOINT = &quot;us-central1-aiplatform.googleapis.com&quot;\nINPUT_CSV_FILE = &quot;my-input-file.csv&quot;\nFEATURESTORE_ID = &quot;fraud_detection&quot;\n\n## Output dataset\nDESTINATION_DATA_SET = &quot;fraud_predictions&quot;\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\nDESTINATION_DATA_SET = &quot;{prefix}_{timestamp}&quot;.format(\n    prefix=DESTINATION_DATA_SET, timestamp=TIMESTAMP\n)\n\n## Output table. Make sure that the table does NOT already exist; \n## the BatchReadFeatureValues API cannot overwrite an existing table\nDESTINATION_TABLE_NAME = &quot;training_data&quot;\n\nDESTINATION_PATTERN = &quot;bq:\/\/{project}.{dataset}.{table}&quot;\nDESTINATION_TABLE_URI = DESTINATION_PATTERN.format(\n    project=PROJECT_ID, dataset=DESTINATION_DATA_SET, \n    table=DESTINATION_TABLE_NAME\n)\n\n## Create dataset\nclient = bigquery.Client(project=PROJECT_ID)\ndataset_id = &quot;{}.{}&quot;.format(client.project, DESTINATION_DATA_SET)\ndataset = bigquery.Dataset(dataset_id)\ndataset.location = REGION\ndataset = client.create_dataset(dataset)\nprint(&quot;Created dataset {}.{}&quot;.format(client.project, dataset.dataset_id))\n\n## Create client for CRUD and data_client for reading feature values.\nclient = aiplatform.gapic.FeaturestoreServiceClient(\n    client_options={&quot;api_endpoint&quot;: API_ENDPOINT})\ndata_client = aiplatform.gapic.FeaturestoreOnlineServingServiceClient(\n    client_options={&quot;api_endpoint&quot;: API_ENDPOINT})\nBASE_RESOURCE_PATH = client.common_location_path(PROJECT_ID, REGION)\n\n## Create featurestore (only the first time)\ncreate_lro = client.create_featurestore(\n    featurestore_service_pb2.CreateFeaturestoreRequest(\n        parent=BASE_RESOURCE_PATH,\n        featurestore_id=FEATURESTORE_ID,\n        featurestore=featurestore_pb2.Featurestore(\n            online_serving_config=featurestore_pb2.Featurestore.OnlineServingConfig(\n                fixed_node_count=1\n            ),\n        ),\n    )\n)\n\n## Wait for LRO to finish and get the LRO result.\nprint(create_lro.result())\n\nclient.get_featurestore(\n    name=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID)\n)\n\n## Create credit card entity type (only the first time)\ncc_entity_type_lro = client.create_entity_type(\n    featurestore_service_pb2.CreateEntityTypeRequest(\n        parent=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n        entity_type_id=&quot;creditcards&quot;,\n        entity_type=entity_type_pb2.EntityType(\n            description=&quot;Credit card entity&quot;,\n        ),\n    )\n)\n\n## Create fraud entity type (only the first time)\nfraud_entity_type_lro = client.create_entity_type(\n    featurestore_service_pb2.CreateEntityTypeRequest(\n        parent=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n        entity_type_id=&quot;frauds&quot;,\n        entity_type=entity_type_pb2.EntityType(\n            description=&quot;Fraud entity&quot;,\n        ),\n    )\n)\n\n## Create features for credit card type (only the first time)\nclient.batch_create_features(\n    parent=client.entity_type_path(PROJECT_ID, REGION, FEATURESTORE_ID, &quot;creditcards&quot;),\n    requests=[\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v1&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v2&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v3&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v4&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v5&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v6&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v7&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v8&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v9&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v10&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v11&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v12&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v13&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v14&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v15&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v16&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v17&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v18&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v19&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v20&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v21&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v22&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v23&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v24&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v25&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v26&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v27&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v28&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;amount&quot;,\n        ),\n    ],\n).result()\n\n## Create features for fraud type (only the first time)\nclient.batch_create_features(\n    parent=client.entity_type_path(PROJECT_ID, REGION, FEATURESTORE_ID, &quot;frauds&quot;),\n    requests=[\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;class&quot;,\n        ),\n    ],\n).result()\n\n## Import features values for credit cards\nimport_cc_request = aiplatform.gapic.ImportFeatureValuesRequest(\n    entity_type=client.entity_type_path(\n        PROJECT_ID, REGION, FEATURESTORE_ID, &quot;creditcards&quot;),\n    csv_source=aiplatform.gapic.CsvSource(gcs_source=aiplatform.gapic.GcsSource(\n        uris=[&quot;gs:\/\/fraud-detection-19102021\/dataset\/cc_details_train.csv&quot;])),\n    entity_id_field=&quot;cc_id&quot;,\n    feature_specs=[\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v1&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v2&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v3&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v4&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v5&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v6&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v7&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v8&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v9&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v10&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v11&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v12&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v13&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v14&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v15&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v16&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v17&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v18&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v19&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v20&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v21&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v22&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v23&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v24&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v25&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v26&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v27&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v28&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;amount&quot;),\n    ],\n    feature_time_field='time',\n    worker_count=1,\n)\n\n## Start to import\ningestion_lro = client.import_feature_values(import_cc_request)\n\n## Polls for the LRO status and prints when the LRO has completed\ningestion_lro.result()\n\n## Import features values for frauds\nimport_fraud_request = aiplatform.gapic.ImportFeatureValuesRequest(\n    entity_type=client.entity_type_path(\n        PROJECT_ID, REGION, FEATURESTORE_ID, &quot;frauds&quot;),\n    csv_source=aiplatform.gapic.CsvSource(gcs_source=aiplatform.gapic.GcsSource(\n        uris=[&quot;gs:\/\/fraud-detection-19102021\/dataset\/data_fraud_train.csv&quot;])),\n    entity_id_field=&quot;fraud_id&quot;,\n    feature_specs=[\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;class&quot;),\n    ],\n    feature_time_field='time',\n    worker_count=1,\n)\n\n## Start to import\ningestion_lro = client.import_feature_values(import_fraud_request)\n\n## Polls for the LRO status and prints when the LRO has completed\ningestion_lro.result()\n<\/code><\/pre>\n<p>When I check the <code>Ingestion Jobs<\/code> from the <code>Feature<\/code> section of Google Cloud Console I see that the job has finished but no values are added to my features.<\/p>\n<p>Any advice it is really precious.<\/p>\n<p>Thank you all.<\/p>\n<p><strong>EDIT 1<\/strong>\nIn the image below there is an example of the first row of the csv file I used as input (<code>cc_details_train.csv<\/code>). All the unseen features  are similar, the feature <code>class<\/code> can assume 0 or 1 values.\nThe injection job lasts about 5 minutes to import (ideally) 3000 rows, but it ends without error and without importing any value.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Z34hG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Z34hG.png\" alt=\"Rows of my csv file\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1635242363740,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-api-python-client|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":357,
        "Owner_creation_time":1616589293617,
        "Owner_last_access_time":1663860372103,
        "Owner_location":"Alatri, Frosinone, FR",
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":1636446864247,
        "Answer_body":"<p><strong>VERTEX AI recomendations when using CSV to ImportValues \/ using ImportFeatureValuesRequest<\/strong><\/p>\n<p>Its possible that when using this feature you might end not able to import any data at all. You must pay attention to the time field you are using as it must be in compliance with google time formats.<\/p>\n<ol>\n<li>feature_time_field, must follow the time constraint rule set by google which is RFC3339, ie: '2021-04-15T08:28:14Z'. You can check details about the field <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.featurestores.entityTypes\/importFeatureValues#request-body\" rel=\"nofollow noreferrer\">here<\/a> and details about timestamp format can be found <a href=\"https:\/\/developers.google.com\/protocol-buffers\/docs\/reference\/google.protobuf#timestamp\" rel=\"nofollow noreferrer\">here<\/a>.<\/li>\n<li>Other columns, fields must match is designed value. One exception is field entity_id_field, As it can be any value.<\/li>\n<\/ol>\n<p>Note: I my test i found that if i do not properly set up the time field as google recommended date format it will just not upload any feature value at all.<\/p>\n<p><em>test.csv<\/em><\/p>\n<pre><code>cc_id,time,v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15,v16,v17,v18,v19,v20,v21,v22,v23,v24,v25,v26,v27,v28,amount\n100,2021-04-15T08:28:14Z,-1.359807,-0.072781,2.534897,1.872351,2.596267,0.465238,0.923123,0.347986,0.987354,1.234657,2.128645,1.958237,0.876123,-1.712984,-0.876436,1.74699,-1.645877,-0.936121,1.456327,0.087623,1.900872,2.876234,1.874123,0.923451,0.123432,0.000012,1.212121,0.010203,1000\n<\/code><\/pre>\n<p><em>output:<\/em><\/p>\n<pre><code>imported_entity_count: 1\nimported_feature_value_count: 29\n<\/code><\/pre>\n<p><strong>About optimization and working with features<\/strong><\/p>\n<p>You can check the official documentation <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-text#single-label-classification\" rel=\"nofollow noreferrer\">here<\/a> to see the min and max amount of records recommended for processing. As a piece of advice you should only use the actual working features to run and the recommended amount of values for it.<\/p>\n<p><strong>See your running ingested job<\/strong><\/p>\n<p>Either if you use VertexUI or code to generated the ingested job. You can track its run by going into the UI to this path:<\/p>\n<pre><code>VertexAI &gt; Features &gt; View Ingested Jobs \n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1636364178672,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1638471768683,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69721067",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69316032,
        "Question_title":"Custom Container deployment in vertex ai",
        "Question_body":"<p>I am trying to deploy my custom container in vertex ai endpoint for predictions. The contents of the application are as follows.<\/p>\n<ol>\n<li>Flask - app.py<\/li>\n<\/ol>\n<pre><code>import pandas as pd\nfrom flask import Flask, jsonify,request\nimport tensorflow\nimport pre_process\nimport post_process\n\n\napp = Flask(__name__)\n\n\n@app.route('\/predict',methods=['POST'])\ndef predict():\n    req = request.json.get('instances')\n    \n    input_data = req[0]['email']\n\n    #preprocessing\n    text = pre_process.preprocess(input_data)\n    vector = pre_process.preprocess_tokenizing(text)\n\n    model = tensorflow.keras.models.load_model('model')\n\n    #predict\n    prediction = model.predict(vector)\n\n    #postprocessing\n    value = post_process.postprocess(list(prediction[0])) \n    \n    return jsonify({'output':{'doc_class':value}})\n\n\nif __name__=='__main__':\n    app.run(host='0.0.0.0')\n<\/code><\/pre>\n<ol start=\"2\">\n<li>Dockerfile<\/li>\n<\/ol>\n<pre><code>FROM python:3.7\n\nWORKDIR \/app\n\nCOPY . \/app\n\nRUN pip install --trusted-host pypi.python.org -r requirements.txt \n\n\nCMD [&quot;gunicorn&quot;, &quot;--bind&quot;, &quot;0.0.0.0:5000&quot;, &quot;app:app&quot;]\n\nEXPOSE 5050\n<\/code><\/pre>\n<ol start=\"3\">\n<li>pre_process.py<\/li>\n<\/ol>\n<pre><code>#import \nimport pandas as pd\nimport pickle\nimport re\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\ndef preprocess(text):\n    &quot;&quot;&quot;Do all the Preprocessing as shown above and\n    return a tuple contain preprocess_email,preprocess_subject,preprocess_text for that Text_data&quot;&quot;&quot;\n         \n    \n    #After you store it in the list, Replace those sentances in original text by space.\n    text = re.sub(&quot;(Subject:).+&quot;,&quot; &quot;,text,re.I)\n    \n    #Delete all the sentances where sentence starts with &quot;Write to:&quot; or &quot;From:&quot;.\n    text = re.sub(&quot;((Write to:)|(From:)).+&quot;,&quot;&quot;,text,re.I)\n    \n    #Delete all the tags like &quot;&lt; anyword &gt;&quot;\n    text = re.sub(&quot;&lt;[^&gt;&lt;]+&gt;&quot;,&quot;&quot;,text)\n    \n    #Delete all the data which are present in the brackets.\n    text = re.sub(&quot;\\([^()]+\\)&quot;,&quot;&quot;,text)\n    \n    #Remove all the newlines('\\n'), tabs('\\t'), &quot;-&quot;, &quot;&quot;.\n    text = re.sub(&quot;[\\n\\t\\\\-]+&quot;,&quot;&quot;,text)\n    \n    #Remove all the words which ends with &quot;:&quot;.\n    text = re.sub(&quot;(\\w+:)&quot;,&quot;&quot;,text)\n    \n    #Decontractions, replace words like below to full words.\n\n    lines = re.sub(r&quot;n\\'t&quot;, &quot; not&quot;, text)\n    lines = re.sub(r&quot;\\'re&quot;, &quot; are&quot;, lines)\n    lines = re.sub(r&quot;\\'s&quot;, &quot; is&quot;, lines)\n    lines = re.sub(r&quot;\\'d&quot;, &quot; would&quot;, lines)\n    lines = re.sub(r&quot;\\'ll&quot;, &quot; will&quot;, lines)\n    lines = re.sub(r&quot;\\'t&quot;, &quot; not&quot;, lines)\n    lines = re.sub(r&quot;\\'ve&quot;, &quot; have&quot;, lines)\n    lines = re.sub(r&quot;\\'m&quot;, &quot; am&quot;, lines)\n    text = lines\n    \n        #replace numbers with spaces\n    text = re.sub(&quot;\\d+&quot;,&quot; &quot;,text)\n    \n        # remove _ from the words starting and\/or ending with _\n    text = re.sub(&quot;(\\s_)|(_\\s)&quot;,&quot; &quot;,text)\n    \n        #remove 1 or 2 letter word before _\n    text = re.sub(&quot;\\w{1,2}_&quot;,&quot;&quot;,text)\n    \n        #convert all letters to lowercase and remove the words which are greater \n        #than or equal to 15 or less than or equal to 2.\n    text = text.lower()\n    \n    text =&quot; &quot;.join([i for i in text.split() if len(i)&lt;15 and len(i)&gt;2])\n    \n    #replace all letters except A-Z,a-z,_ with space\n    preprocessed_text = re.sub(&quot;\\W+&quot;,&quot; &quot;,text)\n\n    return preprocessed_text\n\ndef preprocess_tokenizing(text):\n        \n    #from tf.keras.preprocessing.text import Tokenizer\n    #from tf.keras.preprocessing.sequence import pad_sequences\n    \n    tokenizer = pickle.load(open('tokenizer.pkl','rb'))\n\n    max_length = 1019\n    tokenizer.fit_on_texts([text])\n    encoded_docs = tokenizer.texts_to_sequences([text])\n    text_padded = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n    \n    return text_padded\n<\/code><\/pre>\n<ol start=\"4\">\n<li>post_process.py<\/li>\n<\/ol>\n<pre><code>def postprocess(vector):\n    index = vector.index(max(vector))\n    classes = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n    return classes[index]\n<\/code><\/pre>\n<ol start=\"4\">\n<li>requirements.txt<\/li>\n<\/ol>\n<pre><code>gunicorn\npandas==1.3.3\nnumpy==1.19.5\nflask\nflask-cors\nh5py==3.1.0\nscikit-learn==0.24.2\ntensorflow==2.6.0\n\n<\/code><\/pre>\n<ol start=\"5\">\n<li><p>model<\/p>\n<\/li>\n<li><p>tokenizer.pkl<\/p>\n<\/li>\n<\/ol>\n<p>I am following this blog <a href=\"https:\/\/medium.com\/mlearning-ai\/serverless-prediction-at-scale-part-2-custom-container-deployment-on-vertex-ai-103a43d0a290\" rel=\"nofollow noreferrer\">vertex ai deployment<\/a> for gcloud console commands to containerise and deploy the model to endpoint.But the model is taking forever to get deployed and ultimately fails to get deployed.<\/p>\n<p>After running the container in local host, it runs as expected but it is not getting deployed into vertex ai endpoint. I don't understand whether the problem is in flask app.py or Dockerfile or whether the problem lies somewhere else.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1632490921653,
        "Question_score":3,
        "Question_tags":"flask|dockerfile|google-cloud-vertex-ai",
        "Question_view_count":629,
        "Owner_creation_time":1631092954063,
        "Owner_last_access_time":1652934780303,
        "Owner_location":null,
        "Owner_reputation":111,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I was able to resolve this issue by adding health route to http server. I added the following piece of code in my flask app.<\/p>\n<pre><code>@app.route('\/healthz')\ndef healthz():\n    return &quot;OK&quot;\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1632806171387,
        "Answer_score":4.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":1632810969572,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69316032",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72254372,
        "Question_title":"GCP's Vertex AI(AI Platform) PipelineServiceClient gives unimplemented error",
        "Question_body":"<p>When trying to list pipelines with <code>PipelineServiceClient<\/code> <code>list_pipeline_jobs<\/code> method as given <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1.services.pipeline_service.PipelineServiceClient#google_cloud_aiplatform_v1_services_pipeline_service_PipelineServiceClient_list_pipeline_jobs\" rel=\"nofollow noreferrer\">here<\/a>, I get the following error:<\/p>\n<pre><code>_InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\nstatus = StatusCode.UNIMPLEMENTED\ndetails = &quot;Received http2 header with status: 404&quot;\n...\n<\/code><\/pre>\n<p>How is the API unimplemented, how do I resolve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1652677608807,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":143,
        "Owner_creation_time":1550380195090,
        "Owner_last_access_time":1663747273577,
        "Owner_location":null,
        "Owner_reputation":434,
        "Owner_up_votes":106,
        "Owner_down_votes":34,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72254372",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73078858,
        "Question_title":"VERTEX AI, BATCH PREDICTION",
        "Question_body":"<p>Does anyone know how I can choose the option &quot;Files on Cloud Storage (file list) in the program? How can I [enter image description here choose the list format?<\/p>\n<p><img src=\"https:\/\/i.stack.imgur.com\/aIBA9.png\" alt=\"1\" \/><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1658485663377,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":57,
        "Owner_creation_time":1658484570563,
        "Owner_last_access_time":1658815412120,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1658603284180,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73078858",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70915437,
        "Question_title":"Use a model trained on image to detect objects in videos?",
        "Question_body":"<p>Using Google Vertex AI, I trained a model to detect some specific objects in images.<\/p>\n<p>Can i use this trained model to detect sames objects, but in videos ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643551863927,
        "Question_score":0,
        "Question_tags":"google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":49,
        "Owner_creation_time":1427410714920,
        "Owner_last_access_time":1647194808140,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70915437",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70940773,
        "Question_title":"Misstated quota exceed errors on managed notebooks in GCP",
        "Question_body":"<p>I am migrating some of my notebooks from the soon to be deprecated AI-Platform to the new Vertex platform in GCP. In Vertex I am using the &quot;Managed Notebooks&quot;, and all seemed to be working fine, but then suddenly I got this strange quota exceed error, when I am still far below my limits (I am currently at 4-5% of limit for the given APIs). See the error below:<\/p>\n<p>*Restarting notebook prototyping-notebook: Quota of &quot;::internal: operation &quot;projects\/1096432937575\/locations\/us-central1\/operations\/start-92b393a0-6c2e-4e11-be32-0172418d33c11643717891501967646&quot; completed with error: %!w(<em>status.Status=&amp;{{{} [] [] } 13 INTERNAL: operation name: operation-1643717891545-5d6f3e5095a3c-9aa08cdd-4bd8b9cd error code: QUOTA_EXCEEDED error message: Quota &quot; exceeded limit: 40 in region us-central1.<\/em><\/p>\n<p>I know it is still in preview, but anyone faced the same error or have any experience in how to fix this? Is there some combination of configurations that seems to be more stable than others?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":6,
        "Question_creation_time":1643719337910,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|jupyter-notebook|gcp-ai-platform-notebook|google-cloud-vertex-ai",
        "Question_view_count":129,
        "Owner_creation_time":1445972391513,
        "Owner_last_access_time":1663852274613,
        "Owner_location":null,
        "Owner_reputation":181,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":80,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70940773",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68331232,
        "Question_title":"Vertex AI batch predictions from file-list",
        "Question_body":"<p>I want to submit batch prediction job for a custom model (in my case it is torch model, but I think this is irrelevant in this case). So I read the documentation:\n<a href=\"https:\/\/i.stack.imgur.com\/WGa7T.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WGa7T.png\" alt=\"batch prediction from file-list\" \/><\/a><\/p>\n<p>But as there are no examples I cannot be sure what the schema of the json object which vertex ai will send to my model will be. Does someone have made this work ?<\/p>\n<p>My best guess is that the request will be with the following body:<\/p>\n<pre><code>{'instance' : &lt;b64-encoded-content-of-the-file&gt;}\n<\/code><\/pre>\n<p>But when I read the documentation (for other 'features' of vertex ai) I could imagine the following body as well:<\/p>\n<pre><code>{'instance': {'b64' : &lt;b64-encoded-content-of-the-file&gt;}}\n<\/code><\/pre>\n<p>Does somebody actually know ?<\/p>\n<p>Another thing I did is to make a 'fake-model' which returns the request it gets ... when I submit the batch-prediction job it actually finishes successfully but when I check the output file it is empty ... so ... I actually need help\/more time to think of other ways to decipher vertex ai docs.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625948141620,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":611,
        "Owner_creation_time":1455786472727,
        "Owner_last_access_time":1663964690547,
        "Owner_location":"Varna, Bulgaria",
        "Owner_reputation":405,
        "Owner_up_votes":36,
        "Owner_down_votes":1,
        "Owner_views":100,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Vertex AI <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#request_requirements\" rel=\"nofollow noreferrer\">custom container<\/a> should wrap a service with an endpoint (predict) for receiving a list of instances, each is a json serializable object<\/p>\n<pre><code>{'instances': [{'b64' : &lt;b64-encoded-content-of-the-file1&gt;}, {'b64' : &lt;b64-encoded-content-of-the-file1&gt;}, ...]}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1626781802423,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68331232",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71351821,
        "Question_title":"Reading File from Vertex AI and Google Cloud Storage",
        "Question_body":"<p>I am trying to set up a pipeline in GCP\/Vertex AI and am having a lot of trouble. The pipeline is being written using Kubeflow Pipelines and has many different components, one thing in particular is giving me trouble however. Eventually I want to launch this from a Cloud Function with the help of the Cloud Scheduler.<\/p>\n<p>The part that is giving me issues is fairly simple and I believe I just need some form of introduction to how I should be thinking about this setup. I simply want to read and write from files (might be .csv, .txt or similar). I imagine that the analog to the filesystem on my local machine in GCP is the Cloud Storage so this is where I have been trying to read from for the time being (please correct me if I'm wrong). The component I've built is a blatant rip-off of <a href=\"https:\/\/stackoverflow.com\/questions\/48279061\/gcs-read-a-text-file-from-google-cloud-storage-directly-into-python\">this<\/a> post and looks like this.<\/p>\n<pre><code>@component(\n    packages_to_install=[&quot;google-cloud&quot;],\n    base_image=&quot;python:3.9&quot;\n)\n\n\ndef main(\n):\n    import csv\n    from io import StringIO\n\n    from google.cloud import storage\n\n    BUCKET_NAME = &quot;gs:\/\/my_bucket&quot;\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(BUCKET_NAME)\n\n    blob = bucket.blob('test\/test.txt')\n    blob = blob.download_as_string()\n    blob = blob.decode('utf-8')\n\n    blob = StringIO(blob)  #tranform bytes to string here\n\n    names = csv.reader(blob)  #then use csv library to read the content\n    for name in names:\n        print(f&quot;First Name: {name[0]}&quot;)\n<\/code><\/pre>\n<p>The error I'm getting looks like the following:<\/p>\n<pre><code>google.api_core.exceptions.NotFound: 404 GET https:\/\/storage.googleapis.com\/storage\/v1\/b\/gs:\/\/pipeline_dev?projection=noAcl&amp;prettyPrint=false: Not Found\n<\/code><\/pre>\n<p>What's going wrong in my brain? I get the feeling that it shouldn't be this difficult to read and write files. I must be missing something fundamental? Any help is highly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1646398826330,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|google-cloud-storage|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":1298,
        "Owner_creation_time":1407245826703,
        "Owner_last_access_time":1663849819650,
        "Owner_location":null,
        "Owner_reputation":745,
        "Owner_up_votes":210,
        "Owner_down_votes":6,
        "Owner_views":168,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Try specifying bucket name w\/o a gs:\/\/. This should fix the issue. One more stackoverflow post that says the same thing: <a href=\"https:\/\/stackoverflow.com\/questions\/53436615\/cloud-storage-python-client-fails-to-retrieve-bucket\">Cloud Storage python client fails to retrieve bucket<\/a><\/p>\n<p>any storage bucket you try to access in GCP has a unique address to access it. That address starts with a gs:\/\/ always which specifies that it is a cloud storage url. Now, GCS apis are designed such that they need the bucket name only to work with it. Hence, you just pass the bucket name. If you were accessing the bucket via browser you will need the complete address to access and hence the gs:\/\/ prefix as well.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1646582537360,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71351821",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73529912,
        "Question_title":"vertex ai pipeline: label not populating on billing",
        "Question_body":"<p>I'm trying to trace how much each pipeline I run on vertex costs. I read about adding labels that lets me filter my billing report based on the <a href=\"https:\/\/cloud.google.com\/resource-manager\/docs\/creating-managing-labels\" rel=\"nofollow noreferrer\">labels<\/a>.<\/p>\n<p>It says that vertex ai is supported and the api shows the same with a labels kwarg.<\/p>\n<pre><code>job = aiplatform.PipelineJob(display_name = 'inference',\n                                    template_path = tmpdirname + '\/' + &quot;inference.json&quot;,\n                                    enable_caching = True,\n                                    project = 'project id',\n                                    location = &quot;europe-west4&quot;,    \n                                    parameter_values=params,\n                                    credentials=service_account.Credentials.from_service_account_file('service.json'),\n                                    labels={'pipeline':job_id}\n<\/code><\/pre>\n<p>The pipeline starts and runs through without issue. The label is on the job and I can within the vertex AI pipelines console filter for the job as well. On the billing dashboard it still doesn't exist, when I export the data to bigquery it doesn't exist, I can see the cost for those pipelines I ran but I cannot see those labels and filter on them.<\/p>\n<p>has anyone managed to get the label filter to work for vertex ai so that you can see the cost of a pipeline job?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661781168010,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":42,
        "Owner_creation_time":1634554256497,
        "Owner_last_access_time":1663952930450,
        "Owner_location":null,
        "Owner_reputation":9,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73529912",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68793294,
        "Question_title":"How to schedule repeated runs of a custom training job in Vertex AI",
        "Question_body":"<p>I have packaged my training code as a python package and then am able to run it as a custom training job on Vertex AI. Now, I wanted to be able to schedule this job to run, say every 2 weeks, and re-train the model. The Scheduling settings in the CustomJoBSpec allow only 2 fields, &quot;timeout&quot; and &quot;restartJobOnWorkerRestart&quot; so it's not possible using the scheduling settings in the CustomJobSpec. One way to achieve this I could think of was to create a Vertex AI pipeline with a single step using the &quot;CustomPythonPackageTrainingJobRunOp&quot; Google Cloud Pipeline Component and then scheduling the pipeline to run as I see fit. Are there better alternatives to achieve this?<\/p>\n<p><strong>Edit:<\/strong><\/p>\n<p>I was able to schedule the custom training job using Cloud Scheduler, but I found using the create_schedule_from_job_spec method in the AIPlatformClient very easy to use in the Vertex AI pipeline. The steps I took to schedule the custom job using Cloud Scheduler in gcp are as follows, <a href=\"https:\/\/cloud.google.com\/scheduler\/docs\/http-target-auth#setting_up_the_service_account\" rel=\"nofollow noreferrer\">link<\/a> to google docs:<\/p>\n<ol>\n<li>Set target type to HTTP<\/li>\n<li>For the url to specify the custom job, I followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-job#curl\" rel=\"nofollow noreferrer\">this<\/a> link to get the url<\/li>\n<li>For the authentication, under Auth header, I selected the &quot;Add OAauth token&quot;<\/li>\n<\/ol>\n<p>You also need to have a &quot;Cloud Scheduler service account&quot; with  a &quot;Cloud Scheduler Service Agent role granted to it&quot; in your project. Although the docs ay this should have been set up automatically if you enabled the Cloud Scheduler API after March 19, 2019, this was not the case for me and had to add the service account with the role manually.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1629043963683,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-cloud-ai|google-cloud-ai-platform-pipelines|google-cloud-vertex-ai",
        "Question_view_count":2669,
        "Owner_creation_time":1471292986790,
        "Owner_last_access_time":1664058170413,
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Question_last_edit_time":1629147961556,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68793294",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69267589,
        "Question_title":"Setting test\/train column Google AutoML",
        "Question_body":"<p>I have written some code to train a model using Google's AutoML in VertexAI. A nuance of my problem is that I need to set the test train column manually. The documentation for the method, set_test_train_column, says:<\/p>\n<pre><code>&quot;&quot;&quot;Sets the test\/train (ml_use) column which designates which data\nbelongs to the test and train sets. This column must be categorical.&quot;&quot;&quot;\n<\/code><\/pre>\n<p>My test\/train column is called 'set' and consists of three values, namely, 'TEST', 'TRAIN', and 'VALIDATE'. The dtype of this column is 'object' where each cell takes a string value. I have included 'VALIDATE' as this is required when setting the test\/train column when training a model in the automl section of VertexAI.<\/p>\n<p>The piece of code that implements this is:<\/p>\n<pre><code>dataset_display_name = 'dataset_1'\ntable_client.set_test_train_column(dataset_display_name=dataset_display_name,\n                                   column_spec_display_name='set')\n<\/code><\/pre>\n<p>Could someone please help me understand whether I have implemented this code correctly? Are the values in the test\/train column written correctly? Does it matter that I have not used pd.Categorical to explicitly say that the test\/train column is categorical? Should 'VALIDATE' be included in the test\/train column?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1632221342757,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":54,
        "Owner_creation_time":1562706291280,
        "Owner_last_access_time":1664033431527,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69267589",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73811793,
        "Question_title":"How do I undeploy a model from an endpoint without knowing its id in Vertex AI?",
        "Question_body":"<p>I have managed to undeploy a model from an endpoint using <code>UndeployModelRequest<\/code>:<\/p>\n<pre><code>    model_name = f'projects\/{project}\/locations\/{location}\/models\/{model_id}'\n    model_request = aiplatform_v1.types.GetModelRequest(name=model_name)\n    model_info = client_model.get_model(request=model_request)\n    deployed_models_info = model_info.deployed_models\n    deployed_model_id=model_info.deployed_models[0].deployed_model_id       \n    \n    undeploy_request = aiplatform_v1.types.UndeployModelRequest\n                       (endpoint=end_point, deployed_model_id=deployed_model_id)\n\n    client.undeploy_model(request=undeploy_request)\n<\/code><\/pre>\n<p>but all this depends on knowing <code>model_id<\/code>. I want to be able to just undeploy a model from an endpoint without knowing the model's id (there will only be one model per endpoint ever). Is that possible or can I get the model id from the endpoint somehow?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663836585407,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|kfp",
        "Question_view_count":14,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73811793",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70729582,
        "Question_title":"Batch prediction Input",
        "Question_body":"<p>I have a tensorflow model deployed on Vertex AI of Google Cloud. The model definition is:<\/p>\n<pre><code>item_model = tf.keras.Sequential([\n  tf.keras.layers.StringLookup(\n      vocabulary=item_vocab, mask_token=None),\n  tf.keras.layers.Embedding(len(item_vocab) + 1, embedding_dim)\n])\n\nuser_model = tf.keras.Sequential([\n  tf.keras.layers.StringLookup(\n      vocabulary=user_vocab, mask_token=None),\n  # We add an additional embedding to account for unknown tokens.\n  tf.keras.layers.Embedding(len(user_vocab) + 1, embedding_dim)\n])\n\n\nclass NCF_model(tf.keras.Model):\n    def __init__(self,user_model, item_model):\n        super(NCF_model, self).__init__()\n        # define all layers in init\n        \n        self.user_model = user_model\n        self.item_model  = item_model\n        self.concat_layer   = tf.keras.layers.Concatenate()\n        self.feed_forward_1 = tf.keras.layers.Dense(32,activation= 'relu')\n        self.feed_forward_2 = tf.keras.layers.Dense(64,activation= 'relu')\n        self.final = tf.keras.layers.Dense(1,activation= 'sigmoid')\n\n\n    def call(self, inputs ,training=False):\n        user_id , item_id = inputs[:,0], inputs[:,1]\n        x = self.user_model(user_id)\n        y = self.item_model(item_id)\n\n        x = self.concat_layer([x,y])\n        x = self.feed_forward_1(x)\n        x = self.feed_forward_2(x)\n        x = self.final(x)\n\n\n        return x\n<\/code><\/pre>\n<p>The model has two string inputs and it outputs a probability value.\nWhen I use the following input in the batch prediction file, I get an empty prediction file.\nSample of csv input file:<\/p>\n<pre><code>userid,itemid\nyuu,190767\nyuu,364\nyuu,154828\nyuu,72998\nyuu,130618\nyuu,183979\nyuu,588\n<\/code><\/pre>\n<p>When I use a jsonl file with the following input.<\/p>\n<pre><code>{&quot;input&quot;:[&quot;yuu&quot;, &quot;190767&quot;]}\n<\/code><\/pre>\n<p>I get the following error.<\/p>\n<pre><code>('Post request fails. Cannot get predictions. Error: Exceeded retries: Non-OK result 400 ({\\n    &quot;error&quot;: &quot;Failed to process element: 0 key: input of \\'instances\\' list. Error: INVALID_ARGUMENT: JSON object: does not have named input: input&quot;\\n}) from server, retry=3.', 1)\n<\/code><\/pre>\n<p>What seems to be going wrong with these inputs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1642332344157,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":323,
        "Owner_creation_time":1429532538823,
        "Owner_last_access_time":1664028982667,
        "Owner_location":"Gurugram, Haryana, India",
        "Owner_reputation":363,
        "Owner_up_votes":26,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70729582",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72251787,
        "Question_title":"Permission \"artifactregistry.repositories.downloadArtifacts\" denied on resource",
        "Question_body":"<p>While the artifact repository was successfully creating, running a docker push to push the image to the google artifact registry fails with a permissions error even after granting all artifact permissions to the accounting I am using on gcloud cli.<\/p>\n<p><strong>Command used to push image:<\/strong><\/p>\n<pre><code>docker push us-central1-docker.pkg.dev\/project-id\/repo-name:v2\n<\/code><\/pre>\n<p><strong>Error message:<\/strong><\/p>\n<pre><code>The push refers to repository [us-central1-docker.pkg.dev\/project-id\/repo-name]\n6f6f4a472f31: Preparing\nbc096d7549c4: Preparing\n5f70bf18a086: Preparing\n20bed28d4def: Preparing\n2a3255c6d9fb: Preparing\n3f5d38b4936d: Waiting\n7be8268e2fb0: Waiting\nb889a93a79dd: Waiting\n9d4550089a93: Waiting\na7934564e6b9: Waiting\n1b7cceb6a07c: Waiting\nb274e8788e0c: Waiting\n78658088978a: Waiting\ndenied: Permission &quot;artifactregistry.repositories.downloadArtifacts&quot; denied on resource &quot;projects\/project-id\/locations\/us-central1\/repositories\/repo-name&quot; (or it may not exist)\n\n\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1652644857243,
        "Question_score":12,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|docker-push",
        "Question_view_count":5722,
        "Owner_creation_time":1426920929353,
        "Owner_last_access_time":1662127122187,
        "Owner_location":"Bangkok",
        "Owner_reputation":194,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":1652667678670,
        "Answer_body":"<p>I was able to recreate your use case. This happens when you are trying to push an image on a <code>repository<\/code> in which its specific hostname (associated with it's repository location) is not yet  added to the credential helper configuration for authentication. You may refer to this <a href=\"https:\/\/cloud.google.com\/artifact-registry\/docs\/docker\/authentication\" rel=\"noreferrer\">Setting up authentication for Docker <\/a> as also provided by @DazWilkin in the comments for more details.<\/p>\n<p>In my example, I was trying to push an image on a repository that has a location of <code>us-east1<\/code> and got the same error since it is not yet added to the credential helper configuration.\n<a href=\"https:\/\/i.stack.imgur.com\/NQeIf.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NQeIf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>And after I ran the authentication using below command (specifically for us-east1 since it is the <code>location<\/code> of my repository), the image was successfully pushed:<\/p>\n<pre><code>gcloud auth configure-docker us-east1-docker.pkg.dev\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q2Q9x.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q2Q9x.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><em><strong>QUICK TIP<\/strong><\/em>: You may  get your authentication command specific for your repository when you open your desired repository in the <a href=\"https:\/\/console.cloud.google.com\/artifacts\" rel=\"noreferrer\">console<\/a>, and then click on the <code>SETUP INSTRUCTIONS<\/code>.\n<a href=\"https:\/\/i.stack.imgur.com\/KBjqa.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KBjqa.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1652683203067,
        "Answer_score":23.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1654468583460,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72251787",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73638322,
        "Question_title":"Vertex AI Pipelines (Kubeflow) skip step with dependent outputs on later step",
        "Question_body":"<p>I\u2019m trying to run a Vertex AI Pipelines job where I skip a certain pipeline step if the value of a certain pipeline parameter (in this case <code>do_task1<\/code>) is <code>False<\/code>. But because there is another step that runs unconditionally and expects the output of the first potentially skipped step, I get the following error, independently of do_task1 being <code>True<\/code> or <code>False<\/code>:<\/p>\n<pre><code>AssertionError: component_input_artifact: pipelineparam--task1-output_path not found. All inputs: parameters {\n  key: &quot;do_task1&quot;\n  value {\n    type: STRING\n  }\n}\nparameters {\n  key: &quot;task1_name&quot;\n  value {\n    type: STRING\n  }\n}\n<\/code><\/pre>\n<p>It seems like the compiler just cannot find the output <code>output_path<\/code> from <code>task1<\/code>. So I wonder if there is any way to have some sort of placeholders for the outputs of those steps that are under a <code>dsl.Condition<\/code> , and thus they get filled with default values unless the actual steps run and fill them with the non-default values.\nThe code below represents the problem and is easily reproducible.<\/p>\n<p>I'm using <code>google-cloud-aiplatform==1.14.0<\/code> and <code>kfp==1.8.11<\/code><\/p>\n<pre><code>from typing import NamedTuple\n\nfrom kfp import dsl\nfrom kfp.v2.dsl import Dataset, Input, OutputPath, component\nfrom kfp.v2 import compiler\n\nfrom google.cloud.aiplatform import pipeline_jobs\n\n@component(\n    base_image=&quot;python:3.9&quot;,\n    packages_to_install=[&quot;pandas&quot;]\n)\ndef task1(\n    # inputs\n    task1_name: str,\n    # outputs\n    output_path: OutputPath(&quot;Dataset&quot;),\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;output_1&quot;, str), (&quot;output_2&quot;, int)]):\n\n    import pandas as pd\n    \n    output_1 = task1_name + &quot;-processed&quot;\n    output_2 = 2\n\n    df_output_1 = pd.DataFrame({&quot;output_1&quot;: [output_1]})\n    df_output_1.to_csv(output_path, index=False)\n\n    return (output_1, output_2)\n\n@component(\n    base_image=&quot;python:3.9&quot;,\n    packages_to_install=[&quot;pandas&quot;]\n)\ndef task2(\n    # inputs\n    task1_output: Input[Dataset],\n) -&gt; str:\n\n    import pandas as pd\n\n    task1_input = pd.read_csv(task1_output.path).values[0][0]\n\n    return task1_input\n\n@dsl.pipeline(\n    pipeline_root='pipeline_root',\n    name='pipelinename',\n)\ndef pipeline(\n    do_task1: bool,\n    task1_name: str,\n):\n\n    with dsl.Condition(do_task1 == True):\n\n        task1_op = (\n            task1(\n                task1_name=task1_name,\n            )\n        )\n\n    task2_op = (\n        task2(\n            task1_output=task1_op.outputs[&quot;output_path&quot;],\n        )\n    )\n\n\nif __name__ == '__main__':\n    \n    do_task1 = True # &lt;------------ The variable to modify ---------------\n\n    # compile pipeline\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path='pipeline.json')\n\n    # create pipeline run\n    pipeline_run = pipeline_jobs.PipelineJob(\n        display_name='pipeline-display-name',\n        pipeline_root='pipelineroot',\n        job_id='pipeline-job-id',\n        template_path='pipelinename.json',\n        parameter_values={\n            'do_task1': do_task1, # pipeline compilation fails with either True or False values\n            'task1_name': 'Task 1',\n        },\n        enable_caching=False\n    )\n    \n    # execute pipeline run\n    pipeline_run.run()\n<\/code><\/pre>\n<p>Any help is much appreciated!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1662565503980,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":39,
        "Owner_creation_time":1518297015053,
        "Owner_last_access_time":1663947750400,
        "Owner_location":"Spain",
        "Owner_reputation":21,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73638322",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73224036,
        "Question_title":"Google cloud vertex AI workbench notebook stuck on \"starting\"",
        "Question_body":"<p>I'm running a Jupyter Notebook on the Vertex AI workbench and getting the error below. It loads for an hour or two, then reverts back to a stopped state. It's a managed notebook, so I can't <code>ssh<\/code> to the notebook to recover my code, which is all I want to do since there's a lot of work there I haven't backed up (I didn't expect a big Cloud service to randomly fail like this...) Any ideas on how to get this back up and running? I am happy to delete this and start a new instance onc I recover the code on this.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Cardr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Cardr.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1659540977210,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|jupyter-notebook|google-cloud-vertex-ai",
        "Question_view_count":275,
        "Owner_creation_time":1564327169643,
        "Owner_last_access_time":1662223953267,
        "Owner_location":"Cambridge, MA, USA",
        "Owner_reputation":123,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":76,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73224036",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70535237,
        "Question_title":"How to update Python on Vertex AI notebooks?",
        "Question_body":"<p>I am working in notebooks provided in the Workbench section of Vertex AI. I need an updated version of Python, but I only have access to Python 3.7 in these notebooks. I have successfully followed <a href=\"https:\/\/stackoverflow.com\/a\/62831268\/8565438\">these steps<\/a> and if I run <code>python3.8 --version<\/code> in terminal, I get <code>Python 3.8.2<\/code>, which is good, but <code>python --version<\/code> still returns <code>Python 3.7.12<\/code>. If, following <a href=\"https:\/\/stackoverflow.com\/questions\/40694528\/how-to-know-which-python-is-running-in-jupyter-notebook\">this answer<\/a> and restarting notebook's kernel, I run<\/p>\n<pre><code>from platform import python_version\nprint(python_version())\n<\/code><\/pre>\n<p>in a notebook, and I get <code>3.7.12<\/code>.<\/p>\n<p><strong>How do I get a notebook in Vertex AI supporting an up-to-date Python version?<\/strong><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1640887731833,
        "Question_score":12,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":1736,
        "Owner_creation_time":1504639661230,
        "Owner_last_access_time":1664053782077,
        "Owner_location":"Oslo, Norway",
        "Owner_reputation":5748,
        "Owner_up_votes":2565,
        "Owner_down_votes":761,
        "Owner_views":969,
        "Question_last_edit_time":1658927831667,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70535237",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71578582,
        "Question_title":"connect vertex ai endpoint through .Net",
        "Question_body":"<p>Is there any way to connected google cloud platform service vertex ai endpoint through .Net code ? I am new to gcp vertex. any help is really apricated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647981155950,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|nlp|google-cloud-vertex-ai",
        "Question_view_count":128,
        "Owner_creation_time":1462469556837,
        "Owner_last_access_time":1663877676583,
        "Owner_location":null,
        "Owner_reputation":509,
        "Owner_up_votes":71,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You could refer to <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1 documentation<\/a> for reference for dot net. You can start by:<\/p>\n<ol>\n<li>Installing the package from <a href=\"https:\/\/www.nuget.org\/packages\/Google.Cloud.AIPlatform.V1\/\" rel=\"nofollow noreferrer\">Google AI Platform nuget<\/a><\/li>\n<li>If you don't have an endpoint yet you can check out <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.EndpointServiceClient\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1.EndpointServiceClient<\/a>. You can use this class to manage endpoints like create endpoint, delete endpoint, deploy endpoint, etc.\n<ul>\n<li>Check this <a href=\"https:\/\/github.com\/googleapis\/google-cloud-dotnet\/blob\/main\/apis\/Google.Cloud.AIPlatform.V1\/Google.Cloud.AIPlatform.V1.Snippets\/IndexEndpointServiceClientSnippets.g.cs\" rel=\"nofollow noreferrer\">EndpointServiceClient code sample<\/a> for usage.<\/li>\n<\/ul>\n<\/li>\n<li>If you have an endpoint and you want to run predictions using it, you can check out <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.PredictionServiceClient\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1.PredictionServiceClient<\/a>. You can use this class to perform prediction using your endpoint.\n<ul>\n<li>Specifically <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.PredictionServiceClient#Google_Cloud_AIPlatform_V1_PredictionServiceClient_Predict_Google_Cloud_AIPlatform_V1_EndpointName_System_Collections_Generic_IEnumerable_Google_Protobuf_WellKnownTypes_Value__Google_Protobuf_WellKnownTypes_Value_Google_Api_Gax_Grpc_CallSettings_\" rel=\"nofollow noreferrer\">Predict(EndpointName, IEnumerable, Value, CallSettings)<\/a> method where it accepts an endpoint as parameter.<\/li>\n<li>Check this <a href=\"https:\/\/github.com\/googleapis\/google-cloud-dotnet\/blob\/main\/apis\/Google.Cloud.AIPlatform.V1\/Google.Cloud.AIPlatform.V1.Snippets\/PredictionServiceClientSnippets.g.cs\" rel=\"nofollow noreferrer\">PredictionServiceClient code sample<\/a> for usage.<\/li>\n<\/ul>\n<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1648004091847,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71578582",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72316498,
        "Question_title":"Authenticate Custom Training Job in Vertex AI with Service Account",
        "Question_body":"<p>I am trying to run a Custom Training Job to deploy my model in Vertex AI directly from a Jupyterlab. This Jupyterlab is instantiated from a Vertex AI Managed Notebook where I already specified the service account.<\/p>\n<p>My aim is to deploy the training script that I specify to the method <code>CustomTrainingJob<\/code> directly from the cells of my notebook. This would be equivalent to pushing an image that contains my script to <strong>container registry<\/strong> and deploying the Training Job manually from the UI of Vertex AI (in this way, by specifying the service account, I was able to corectly deploy the training job). However, I need everything to be executed from the same notebook.<\/p>\n<p>In order to specify the credentials to the <code>CustomTrainingJob<\/code> of aiplatform, I execute the following cell, where all variables are correctly set:<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>import google.auth\nfrom google.cloud import aiplatform\nfrom google.auth import impersonated_credentials\n\nsource_credentials = google.auth.default()\ntarget_credentials = impersonated_credentials.Credentials(\nsource_credentials=source_credentials,\ntarget_principal='SERVICE_ACCOUNT.iam.gserviceaccount.com',\ntarget_scopes = ['https:\/\/www.googleapis.com\/auth\/cloud-platform'])\n\naiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)\n\njob = aiplatform.CustomTrainingJob(\n    display_name=JOB_NAME,\n    script_path=SCRIPT_PATH,\n    container_uri=MODEL_TRAINING_IMAGE,\n    credentials=target_credentials\n)\n<\/code><\/pre>\n<p>When after the <code>job.run()<\/code> command is executed it seems that the credentials are not correctly set. In particular, the following error is returned:<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>\/opt\/conda\/lib\/python3.7\/site-packages\/google\/auth\/impersonated_credentials.py in _update_token(self, request)\n    254 \n    255         # Refresh our source credentials if it is not valid.\n--&gt; 256         if not self._source_credentials.valid:\n    257             self._source_credentials.refresh(request)\n    258 \n\nAttributeError: 'tuple' object has no attribute 'valid'\n<\/code><\/pre>\n<p>I also tried different ways to configure the credentials of my service account but none of them seem to work. In this case it looks like the tuple that contains the source credentials is missing the 'valid' attribute, even if the method <code>google.auth.default()<\/code> only returns two values.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1653038432043,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|service-accounts|google-cloud-vertex-ai",
        "Question_view_count":149,
        "Owner_creation_time":1648140384823,
        "Owner_last_access_time":1664054562907,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72316498",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69930186,
        "Question_title":"Google Cloud Platform Vertex AI logs not showing in custom job",
        "Question_body":"<p>I have written a python package that trains a neural network. I then package it up using the below command.<\/p>\n<pre><code>python3 setup.py sdist --formats=gztar\n<\/code><\/pre>\n<p>When I run this job through the GCP console, and manually click through all the options, I get logs from my program as expected (see example below)<\/p>\n<p><strong>Example successful logs:<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/xGFkY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xGFkY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>However, when I run the exact same job programmatically, no logs appear. Only the final error (if one occurs):<\/p>\n<p><strong>Example logs missing:<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/3oXfV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3oXfV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>In both cases, the program is running - I just cant see any of the outputs. What could the reason for this be? For reference, I have also included the code I used to programmatically start the training process:<\/p>\n<pre><code>ENTRY_POINT = &quot;projects.yaw_correction.yaw_correction&quot;\nTIMESTAMP = datetime.datetime.strftime(datetime.datetime.now(),&quot;%y%m%d_%H%M%S&quot;)\nPROJECT = &quot;yaw_correction&quot;\nGCP_PROJECT = &quot;our_gcp_project_name&quot;\nLOCATION = &quot;europe-west1&quot;\nBUCKET_NAME = &quot;our_bucket_name&quot;\nDISPLAY_NAME = &quot;Training_Job_&quot; + TIMESTAMP\nCONTAINER_URI = &quot;europe-docker.pkg.dev\/vertex-ai\/training\/pytorch-xla.1-9:latest&quot;\nMODEL_NAME = &quot;Model_&quot; + TIMESTAMP\nARGS = [f&quot;\/gcs\/fotokite-training-data\/yaw_correction\/&quot;, &quot;--cloud&quot;, &quot;--gpu&quot;]\nTENSORBOARD = &quot;projects\/&quot;our_gcp_project_name&quot;\/locations\/europe-west4\/tensorboards\/yaw_correction&quot;\n\nMACHINE_TYPE = &quot;n1-standard-4&quot;\nREPLICA_COUNT = 1\nACCELERATOR_TYPE = &quot;ACCELERATOR_TYPE_UNSPECIFIED&quot;\nACCELERATOR_COUNT = 0\nSYNC = False\n\n#Delete existing source distributions\ndef deleteDist():\n    dirpath = Path('dist')\n    if dirpath.exists() and dirpath.is_dir():\n        shutil.rmtree(dirpath)\n\n# Copy distribution to the cloud bucket storage\ndeleteDist()\nsubprocess.run(&quot;python3 setup.py sdist --formats=gztar&quot;, shell=True)\nfilename = [x for x in Path('dist').glob('*')]\nif len(filename) != 1:\n    raise Exception(&quot;More than one distribution was found&quot;)\nprint(str(filename[0]))\nPACKAGE_URI = f&quot;gs:\/\/{BUCKET_NAME}\/distributions\/&quot;\nsubprocess.run(f&quot;gsutil cp {str(filename[0])} {PACKAGE_URI}&quot;, shell=True)\nPACKAGE_URI += str(filename[0].name)\ndeleteDist()\n\n# Initialise the compute instance\naiplatform.init(project=GCP_PROJECT, location=LOCATION, staging_bucket=BUCKET_NAME)\n\n# Schedule the job\njob = aiplatform.CustomPythonPackageTrainingJob(\n    display_name=DISPLAY_NAME,\n    #script_path=&quot;trainer\/test.py&quot;,\n    python_package_gcs_uri=PACKAGE_URI,\n    python_module_name=ENTRY_POINT,\n    #requirements=['tensorflow_datasets~=4.2.0', 'SQLAlchemy~=1.4.26', 'google-cloud-secret-manager~=2.7.2', 'cloud-sql-python-connector==0.4.2', 'PyMySQL==1.0.2'],\n    container_uri=CONTAINER_URI,\n)\n\nmodel = job.run(\n    dataset=None,\n    #base_output_dir=f&quot;gs:\/\/{BUCKET_NAME}\/{PROJECT}\/Train_{TIMESTAMP}&quot;,\n    base_output_dir=f&quot;gs:\/\/{BUCKET_NAME}\/{PROJECT}\/&quot;,\n    service_account=&quot;vertex-ai-fotokite-service-acc@fotokite-cv-gcp-exploration.iam.gserviceaccount.com&quot;,\n    environment_variables=None,\n    args=ARGS,\n    replica_count=REPLICA_COUNT,\n    machine_type=MACHINE_TYPE,\n    accelerator_type=ACCELERATOR_TYPE,\n    accelerator_count=ACCELERATOR_TYPE,\n    #tensorboard=TENSORBOARD,\n    sync=SYNC\n)\nprint(model)\nprint(&quot;JOB SUBMITTED&quot;)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636641674037,
        "Question_score":2,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":436,
        "Owner_creation_time":1636640830170,
        "Owner_last_access_time":1652193537447,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1636706361063,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69930186",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73540815,
        "Question_title":"how to pass threshold to BigQuery ML model when deployed to vertex ai",
        "Question_body":"<p>I am exporting a BigQuery ML model to cloud storage and then importing the resulting tensorflow model to vertex ai.<\/p>\n<p>The deployed model uses a threshold of 0.5.<\/p>\n<p>If I were doing prediction in BigQuery, I would have used ML.predict with STRUCT(0.05 as THRESHOLD). How can I tell Vertex AI to similarly pass THRESHOLD to all input requests?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661854797090,
        "Question_score":0,
        "Question_tags":"tensorflow|google-bigquery|google-cloud-vertex-ai",
        "Question_view_count":60,
        "Owner_creation_time":1400250266617,
        "Owner_last_access_time":1663549773980,
        "Owner_location":"Seattle, WA, United States",
        "Owner_reputation":3566,
        "Owner_up_votes":39,
        "Owner_down_votes":3,
        "Owner_views":732,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73540815",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72986981,
        "Question_title":"Porting custom job from GCP AI Platform to Vertex AI - how to get state and logs of job?",
        "Question_body":"<p>I am porting custom job training from gcp AI Platform to Vertex AI.\nI am able to start a job, but can't find how to to get the status and how to stream the logs to my local client.<\/p>\n<p>For AI Platform I was using this to get the state:<\/p>\n<pre><code>from google.oauth2 import service_account\nfrom googleapiclient import discovery\nscopes = ['https:\/\/www.googleapis.com\/auth\/cloud-platform']\ncredentials = service_account.Credentials.from_service_account_file(keyFile, scopes=scopes)\nml_apis = discovery.build(&quot;ml&quot;,&quot;v1&quot;, credentials=credentials, cache_discovery=False)\nx = ml_apis.projects().jobs().get(name=&quot;projects\/%myproject%\/jobs\/&quot;+job_id).execute()  # execute http request\nreturn x['state']\n<\/code><\/pre>\n<p>And this to stream the logs:<\/p>\n<pre><code>cmd = 'gcloud ai-platform jobs stream-logs ' + job_id\n<\/code><\/pre>\n<p>This does not work for Vertex AI job. What is the replacement code?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1657835239540,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|google-ai-platform",
        "Question_view_count":62,
        "Owner_creation_time":1254829817773,
        "Owner_last_access_time":1663965241687,
        "Owner_location":"Germany",
        "Owner_reputation":2595,
        "Owner_up_votes":462,
        "Owner_down_votes":5,
        "Owner_views":357,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Can you try this command for streaming logs :<\/p>\n<pre><code>gcloud ai custom-jobs stream-logs 123 --region=europe-west4\n<\/code><\/pre>\n<p>123 is the <strong>ID<\/strong> of the custom job for this case, you can add glcoud wide flags such as --format as well.<\/p>\n<p>You can visit this <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/custom-jobs\/stream-logs\" rel=\"nofollow noreferrer\">link<\/a> for more details about this command and additional flags available.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1657854766996,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1657863259569,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72986981",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73690729,
        "Question_title":"How do I get deployed model from `ListEndpointsRequest`?",
        "Question_body":"<p>My code is:<\/p>\n<pre><code>   client = aiplatform_v1.EndpointServiceClient(client_options=options)\n    parent = client.common_location_path(project=project_id, location=location)\n    \n\n    # Initialize request argument(s)\n    request = aiplatform_v1.ListEndpointsRequest(\n        parent=parent\n    )\n\n    # Make the request\n    endpoints_pager = client.list_endpoints(request=request)\n    for endpoint in endpoints_pager.pages:\n        latest_endpoint=endpoint\n        print(endpoint.deployed_models.id)\n<\/code><\/pre>\n<p>If I print <code>endpoint<\/code> I see <code>{...deployed_models { id: ...}}<\/code>\n<code>endpoint.deployed_models.id)<\/code> doesn't work so how do I get the deployed model id?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1662992136247,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":44,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As @DazWilkin mentioned in the comments, you need to iterate through <code>deployed_models<\/code> to get the <code>id<\/code> per model. Applying this, your code should look like this:<\/p>\n<p>IMPORTANT NOTE FOR FUTURE READERS: When creating <code>client<\/code> it is needed to define the <code>api_endpoint<\/code> in <code>client_options<\/code>. If not not defined you will encounter a <strong>google.api_core.exceptions.MethodNotImplemented: 501 Received http2 header with status: 404<\/strong> error.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from google.cloud import aiplatform_v1\n\ndef sample_list_endpoints():\n\n    project_id = &quot;your-project-id&quot;\n    location = &quot;us-central1&quot;\n\n    client = aiplatform_v1.EndpointServiceClient(client_options={&quot;api_endpoint&quot;:&quot;us-central1-aiplatform.googleapis.com&quot;})\n\n    parent = client.common_location_path(project=project_id, location=location)\n    # Initialize request argument(s)\n    request = aiplatform_v1.ListEndpointsRequest(\n        parent=parent,\n    )\n\n    # Make the request\n    page_result = client.list_endpoints(request=request)\n\n    # Handle the response\n    for response in page_result:\n        for model in response.deployed_models:\n            print(model.id)\n\nsample_list_endpoints()\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1663048009750,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73690729",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72989531,
        "Question_title":"WebScrapping error in Google Cloud - Vertex AI Workbench (using Python3 & Selenium)",
        "Question_body":"<p>I am trying to use Workbench Managed Notebooks to schedule some Jupyter notebooks to run Selenium for webscrapping some pages.<\/p>\n<p>My code is below:<\/p>\n<pre><code>from get_gecko_driver import GetGeckoDriver\nget_driver = GetGeckoDriver()\nget_driver = GetGeckoDriver()\nget_driver.install()\n\nfrom selenium import webdriver\nfrom selenium.webdriver import FirefoxOptions\n\nopts = FirefoxOptions()\nopts.add_argument(&quot;--headless&quot;)\nbrowser = webdriver.Firefox(options=opts)\n<\/code><\/pre>\n<p>I get the error:<\/p>\n<pre><code>---------------------------------------------------------------------------\nSessionNotCreatedException                Traceback (most recent call last)\n\/tmp\/ipykernel_1\/3371773802.py in &lt;module&gt;\n      4 opts = FirefoxOptions()\n      5 opts.add_argument(&quot;--headless&quot;)\n----&gt; 6 browser = webdriver.Firefox(options=opts)\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/firefox\/webdriver.py in __init__(self, firefox_profile, firefox_binary, capabilities, proxy, executable_path, options, service_log_path, service_args, service, desired_capabilities, log_path, keep_alive)\n    178             command_executor=executor,\n    179             options=options,\n--&gt; 180             keep_alive=True)\n    181 \n    182         self._is_remote = False\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/webdriver.py in __init__(self, command_executor, desired_capabilities, browser_profile, proxy, keep_alive, file_detector, options)\n    275         self._authenticator_id = None\n    276         self.start_client()\n--&gt; 277         self.start_session(capabilities, browser_profile)\n    278 \n    279     def __repr__(self):\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/webdriver.py in start_session(self, capabilities, browser_profile)\n    368         w3c_caps = _make_w3c_caps(capabilities)\n    369         parameters = {&quot;capabilities&quot;: w3c_caps}\n--&gt; 370         response = self.execute(Command.NEW_SESSION, parameters)\n    371         if 'sessionId' not in response:\n    372             response = response['value']\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/webdriver.py in execute(self, driver_command, params)\n    433         response = self.command_executor.execute(driver_command, params)\n    434         if response:\n--&gt; 435             self.error_handler.check_response(response)\n    436             response['value'] = self._unwrap_value(\n    437                 response.get('value', None))\n\n~\/.local\/lib\/python3.7\/site-packages\/selenium\/webdriver\/remote\/errorhandler.py in check_response(self, response)\n    245                 alert_text = value['alert'].get('text')\n    246             raise exception_class(message, screen, stacktrace, alert_text)  # type: ignore[call-arg]  # mypy is not smart enough here\n--&gt; 247         raise exception_class(message, screen, stacktrace)\n    248 \n    249     def _value_or_default(self, obj: Mapping[_KT, _VT], key: _KT, default: _VT) -&gt; _VT:\n\nSessionNotCreatedException: Message: Expected browser binary location, but unable to find binary in default location, no 'moz:firefoxOptions.binary' capability provided, and no binary flag set on the command line\n<\/code><\/pre>\n<p>It seems that I need to setup somehow the location of Firefox, but I don't know if this is possible and how to do it (or if there are some alternatives that are easier to setup e.g. chromium).<\/p>\n<p><em>Please Note: this code is running perfectly fine when I run Jupyter notebooks in VM instances but I cannot schedule those notebooks to run automatically so I guess my only option is to go with the Vertex AI &gt; Workbench &gt; Managed Notebooks solution.<\/em><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1657864043243,
        "Question_score":0,
        "Question_tags":"python-3.x|selenium|jupyter-lab|google-cloud-vertex-ai",
        "Question_view_count":137,
        "Owner_creation_time":1623914392097,
        "Owner_last_access_time":1658465175717,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72989531",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70782169,
        "Question_title":"Vertex AI batch prediction location",
        "Question_body":"<p>When I initiate a batch prediction job on Vertex AI of google cloud, I have to specify a cloud storage bucket location. Suppose I provided the bucket location, <code>'my_bucket\/prediction\/'<\/code>, then the prediction files are stored in something like: <code>gs:\/\/my_bucket\/prediction\/prediction-test_model-2022_01_17T01_46_39_898Z<\/code>, which is a subdirectory within the bucket location I provided. The prediction files are stored within that subdirectory and are named:<\/p>\n<pre><code>prediction.results-00000-of-00002\nprediction.results-00001-of-00002\n<\/code><\/pre>\n<p>Is there any way to programmatically get the final export location from the batch prediction name, id or any other parameter as shown below in the details of the batch prediction job?\n<a href=\"https:\/\/i.stack.imgur.com\/OfNmZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OfNmZ.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1642663428603,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":554,
        "Owner_creation_time":1429532538823,
        "Owner_last_access_time":1664028982667,
        "Owner_location":"Gurugram, Haryana, India",
        "Owner_reputation":363,
        "Owner_up_votes":26,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70782169",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73403004,
        "Question_title":"Using ipyleaflet within a Vertex AI Managed Notebook running on a Docker image",
        "Question_body":"<p>TL;DR How does one get ipyleaflet to work in a Vertex AI Managed Notebook booted from a custom Docker image?<\/p>\n<p><strong>Objective<\/strong><\/p>\n<p>Following on <a href=\"https:\/\/stackoverflow.com\/questions\/73360734\/programmatically-enable-installed-extensions-in-vertex-ai-managed-notebook-insta\/73369289?noredirect=1#comment129617711_73369289\">this thread<\/a>, I am working in JupyterLab within a Managed Notebook instance, accessed through the Vertex AI workbench, as part of a Google Cloud Project. I am trying to supply a custom Docker image, such that when a Jupyter Lab notebook is launched from the running instance, it (a) contains some modules for performing and visualizing cartographic analysis, and, importantly, (b) permits visualization with ipyleaflet and associated modules.<\/p>\n<p><strong>What I've tried<\/strong><\/p>\n<p>Thus far, I have succeed in creating a Docker image (derivative of <a href=\"https:\/\/cloud.google.com\/deep-learning-containers\/docs\/choosing-container\" rel=\"nofollow noreferrer\">Google image<\/a>) that supplies, in the running Jupyter Lab, a dedicated environment (explicitly exposed kernel) with the correct modules (in particular, geemap, earthengine-api, ipyleaflet, ipywidgets). The modules are all importable and appear sound. However, so far as I can tell, supplying a custom Docker image during the build process, effectively breaks the ipyleaflet (and presumably widgets, events, etc) connection that Google's Jupyter Lab base image has if one creates a Managed Notebook <em>without<\/em> supplying a Docker image. Attempts to create map visualizations returns, &quot;Error displaying widget: model not found&quot;, discussed at <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/504\" rel=\"nofollow noreferrer\">1<\/a>, <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/889\" rel=\"nofollow noreferrer\">2<\/a>, <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/547\" rel=\"nofollow noreferrer\">3<\/a>, <a href=\"https:\/\/leafmap.org\/faq\/\" rel=\"nofollow noreferrer\">4<\/a>. In other words, if one creates a Managed Notebook <em>without<\/em> a Docker image, starts the notebook instance, launches Jupyter Lab, opens a notebook, and then uses <code>%pip install xyz<\/code> for the modules of interest, ipyleaflet-based mapping works fine. I suspect that the nuance of difference here, is that the latter method (<code>%pip install<\/code>ing from within the notebook), is being layered on top of a fully formed base Jupyter Lab container (per @gogasca's comment <a href=\"https:\/\/stackoverflow.com\/questions\/73360734\/programmatically-enable-installed-extensions-in-vertex-ai-managed-notebook-insta\/73369289?noredirect=1#comment129617711_73369289\">here<\/a>, that the Google Managed Jupyter Lab runs as a container that is not customizable).<\/p>\n<p><strong>Questions<\/strong><\/p>\n<p>So, what I would love to know is:<\/p>\n<p><strong>(1)<\/strong> How does one retain ipyleaflet (and associated modules) functionality in a Managed Notebook that is based on a user supplied Docker image?<\/p>\n<p><strong>(2)<\/strong> Is there a way to effectively replicate the <code>%pip install<\/code> approach when using a custom Docker image, such that commands specified in the Docker file are layered on top of a fully formed base Google image.<\/p>\n<p>To question (2), I suspect that when the gcloud sdk for Managed Notebooks is available (currently under the impression that this is a work in progress), it will be possible to provide a post-startup-script, as in <a href=\"https:\/\/medium.com\/@gogasca_\/ai-platform-notebooks-with-voila-c3c57d4e8e\" rel=\"nofollow noreferrer\">this example<\/a>. I am aware that there are REST and Terraform build options available that might satisfy my Managed Notebook needs. These require quite a bit more legwork though, so I am interested in simpler solutions, if they exist.<\/p>\n<p><strong>EDIT<\/strong><\/p>\n<p>Adding content for partial reproducibility. Steps to reproduce:<\/p>\n<ol>\n<li>A custom Docker image was created using the interface at <a href=\"https:\/\/shell.cloud.google.com\/\" rel=\"nofollow noreferrer\">https:\/\/shell.cloud.google.com\/<\/a>.<\/li>\n<li>Within the cloud shell, from the terminal, set gcloud configuration parameters with <code>gcloud config set project yourProjectIDHere<\/code><\/li>\n<li>Create a file named Dockerfile with the following content:<\/li>\n<\/ol>\n<blockquote>\n<pre><code>FROM python:3.7.4-buster \nENV VIRTUAL_ENV=\/env\/testEnvironment \nRUN python3 -m venv $VIRTUAL_ENV --system-site-packages \nENV PATH=&quot;$VIRTUAL_ENV\/bin:$PATH&quot; \n# Install dependencies: \nCOPY requirements.txt . \nRUN $VIRTUAL_ENV\/bin\/pip install -r requirements.txt\n<\/code><\/pre>\n<\/blockquote>\n<ol start=\"4\">\n<li>Create a file named requirements.txt with the following content<\/li>\n<\/ol>\n<blockquote>\n<pre><code>ipython\nipykernel\ngeemap\nearthengine-api\nipyleaflet\nfolium\nvoila\nipywidgets\n<\/code><\/pre>\n<\/blockquote>\n<ol start=\"5\">\n<li>From the cloud shell terminal, build your Docker image (this takes a long time to build. We will use a Google deep learning library derivative once we get our workflow sorted):\n<code>docker build . -f Dockerfile -t &quot;gcr.io\/yourProjectIDHere\/test-image:latest&quot;<\/code><\/li>\n<li>Push container to GCP Container Registry\n<code>docker push &quot;gcr.io\/yourProjectIDHere\/test-image:latest&quot;<\/code><\/li>\n<li>Move to Google cloud console and create a new Managed Notebook within your GCP.<\/li>\n<li>Select the defaults except for (a) use Service account permissions; (b) use &quot;Networks shared with me&quot; associated with the host project, including both a Network and Shared subnetwork (presuming that this isn't going to change installation components, so probably doesn't matter); (c) uncheck &quot;Enable external IPs&quot;; (d) check &quot;Enable terminal&quot;; and (e) check &quot;Provide custom docker images&quot;.<\/li>\n<li>From the &quot;Provide custom docker images&quot; dialog, select the image you just created.<\/li>\n<li>Create the Managed Notebook, and once created, open Jupyter Lab.<\/li>\n<li>The Docker image should expose a kernel in the Jupyter Lab environment. Open a new notebook using the kernel.<\/li>\n<li>Load and test modules within the notebook<\/li>\n<\/ol>\n<blockquote>\n<pre><code># Import and initialize the earthengine-api by whatever means fit your use case\nimport ee\nee.Initialize( ... )\n# Test authentication pathways\nprint(ee.Image(&quot;NASA\/NASADEM_HGT\/001&quot;).get(&quot;title&quot;).getInfo())\n# Provided you have your authentication set-up correctly...\n\n# Test import of various other modules   \nimport os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport geemap\nfrom IPython.display import display, HTML, Image\nimport ipywidgets as widgets\nfrom ipywidgets import Layout\nfrom ipywidgets import interact, interactive, fixed, interact_manual, Button, HBox, VBox\nfrom ipywidgets import HTML\nimport voila\n\n# Test inline mapping using ipyleaflet-based map     \nMap = geemap.Map(center=(-0.2557968807155925, 119.46629773460036), zoom=5)\nMap\n\n# Receive error.\n\n# This is also replicable without earthengine-api or geemap, by just trying to make a basic ipyleaflet map\nfrom ipyleaflet import Map, basemaps, basemap_to_tiles\nm = Map(basemap=basemap_to_tiles(basemaps.OpenStreetMap.Mapnik), center=(48.204793, 350.121558), zoom=3)\nm\n<\/code><\/pre>\n<\/blockquote>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1660825300527,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|jupyter-lab|google-cloud-vertex-ai|gcp-ai-platform-notebook",
        "Question_view_count":148,
        "Owner_creation_time":1459350905810,
        "Owner_last_access_time":1663857124693,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1662058794212,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73403004",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69681031,
        "Question_title":"how to concatenate the OutputPathPlaceholder with a string with Kubeflow pipelines?",
        "Question_body":"<p>I am using Kubeflow pipelines (KFP) with GCP Vertex AI pipelines. I am using <code>kfp==1.8.5<\/code> (kfp SDK) and <code>google-cloud-pipeline-components==0.1.7<\/code>. Not sure if I can find which version of Kubeflow is used on GCP.<\/p>\n<p>I am bulding a component (yaml) using python inspired form this <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3748#issuecomment-627698554\" rel=\"nofollow noreferrer\">Github issue<\/a>. I am defining an output like:<\/p>\n<pre><code>outputs=[(OutputSpec(name='drt_model', type='Model'))]\n<\/code><\/pre>\n<p>This will be a base output directory to store few artifacts on Cloud Storage like model checkpoints and model.<\/p>\n<p>I would to keep one base output directory but add sub directories depending of the artifact:<\/p>\n<ul>\n<li>&lt;output_dir_base&gt;\/model<\/li>\n<li>&lt;output_dir_base&gt;\/checkpoints<\/li>\n<li>&lt;output_dir_base&gt;\/tensorboard<\/li>\n<\/ul>\n<p>but I didn't find how to concatenate the <strong>OutputPathPlaceholder('drt_model')<\/strong> with a string like <strong>'\/model'<\/strong>.<\/p>\n<p>How can append extra folder structure like \/model or \/tensorboard to the OutputPathPlaceholder that KFP will set during run time ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1634924192977,
        "Question_score":1,
        "Question_tags":"kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":312,
        "Owner_creation_time":1465222092253,
        "Owner_last_access_time":1663858783617,
        "Owner_location":"Z\u00fcrich, Switzerland",
        "Owner_reputation":1414,
        "Owner_up_votes":258,
        "Owner_down_votes":3,
        "Owner_views":478,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I didn't realized in the first place that <code>ConcatPlaceholder<\/code> accept both Artifact and string. This is exactly what I wanted to achieve:<\/p>\n<pre><code>ConcatPlaceholder([OutputPathPlaceholder('drt_model'), '\/model'])\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1637263566352,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69681031",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71556469,
        "Question_title":"Vertex AI Endpoints - Failed to create endpoint",
        "Question_body":"<p>I'm trying to deploy a custom model to an endpoint with Vertex AI. I run the custom training and the model was correctly created in my bucket using Tensorflow 2 <code>export_saved_model<\/code> for estimators. In this bucket there is the <code>saved_model.pb<\/code> file with the folder <code>variables<\/code>.\nHowever, when I try to create an endpoint selecting the path to the saved model, the following error occurs:<\/p>\n<p><code>Failed to create endpoint &quot;endpoint_name&quot; due to error: APPLICATION_ERROR; google.cloud.ml.v1\/ModelService.CreateVersion;Field: version.deployment_uri Error: Deployment directory gs:\/\/different_bucket\/artifacts\/ is expected to contain exactly one of: [saved_model.pb, saved_model.pbtxt].<\/code><\/p>\n<p>It seems it is searching the .pb file in a bucket that is not the one I set.\nSuggestions?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1647860330843,
        "Question_score":1,
        "Question_tags":"google-cloud-endpoints|google-cloud-vertex-ai",
        "Question_view_count":345,
        "Owner_creation_time":1616589293617,
        "Owner_last_access_time":1663860372103,
        "Owner_location":"Alatri, Frosinone, FR",
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71556469",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73715783,
        "Question_title":"Why can't I access Output from Vertex pipeline kfp component?",
        "Question_body":"<p>In a Vertex AI pipeline (google_cloud_pipeline_components version: 1.0.19\nkfp version: 1.8.13), I try:<\/p>\n<pre><code>does_endpoint_exist_op = does_endpoint_exist(project=project, \n    location=location, endpoint_name_in=endpoint_name) \nendpoint_name=does_endpoint_exist_op.outputs['endpoint_name']\n<\/code><\/pre>\n<p>but this gives:<\/p>\n<pre><code>AssertionError: component_input_parameter: pipelineparam--does-endpoint-exist-endpoint_name not found.\n<\/code><\/pre>\n<p>The component is defined:<\/p>\n<pre><code>@component(\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;, \n                         &quot;google-cloud-pipeline-components==1.0.19&quot;],\n    output_component_file=&quot;does_endpoint_exist_component.yaml&quot;,\n)\n\ndef does_endpoint_exist(project: str, \n                    location: str,\n                    endpoint_name_in: str, endpoint: Output[Artifact], \n                    endpoint_name: Output[Artifact]) -&gt; str:\n<\/code><\/pre>\n<p>I can do:<\/p>\n<pre><code>endpoint_name=does_endpoint_exist_op.outputs['Output'] \n<\/code><\/pre>\n<p>OK, so why can't I access <code>endpoint_name<\/code>?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1663153516447,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|kfp",
        "Question_view_count":17,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73715783",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70610069,
        "Question_title":"Is there any parameter in any SDK for enabling access logging in GCP Vertex AI?",
        "Question_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/kV1Vb.png\" rel=\"nofollow noreferrer\">enabling access logging on the UI<\/a>seeking for some support in terms of enabling logging via the SDK(Vertex AI or AI platform or any other).Just as we enable it on the UI(pls refer attached file) &amp; other times via gcloud command like this-&gt;\ngcloud ai endpoints deploy-model ENDPOINT_ID--region=LOCATION --model=MODEL_ID --display-name=DEPLOYED_MODEL_NAME --machine-type=MACHINE_TYPE --accelerator=count=2,type=nvidia-tesla-t4 --disable-container-logging --enable-access-logging<\/p>\n<p>Does AI platform or vertex ai or any other SDK comprise of any API\/parameter which would allow us to enable access logging? If yes, could you please point in that direction?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1641485506960,
        "Question_score":0,
        "Question_tags":"logging|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":24,
        "Owner_creation_time":1632752693853,
        "Owner_last_access_time":1658209784610,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70610069",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71223654,
        "Question_title":"How to get wandb to pass arguments by position?",
        "Question_body":"<p>I am trying to explore the results of different parameter settings on my python script &quot;train.py&quot;. For that, I use a wandb sweep. Each wandb agent executes the file &quot;train.py&quot; and passes some parameters to it. As per the wandb documentation (<a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command\" rel=\"nofollow noreferrer\">https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command<\/a>), in case of e.g. two parameters &quot;param1&quot; and &quot;param2&quot; each agents starts the file with the command<\/p>\n<pre><code>\/usr\/bin\/env python train.py --param1=value1 --param2=value2\n<\/code><\/pre>\n<p>However, &quot;train.py&quot; expects<\/p>\n<pre><code>\/usr\/bin\/env python train.py value1 value2\n<\/code><\/pre>\n<p>and parses the parameter values by position. I did not write train.py and would like to not change it if possible. How can I get wandb to pass the values without &quot;--param1=&quot; in front?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645542424610,
        "Question_score":0,
        "Question_tags":"python|python-3.x|wandb",
        "Question_view_count":270,
        "Owner_creation_time":1440414980200,
        "Owner_last_access_time":1645630582560,
        "Owner_location":"Germany",
        "Owner_reputation":5,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Don't think you can get positional arguments from W&amp;B Sweeps. However, there's a little work around you can try that won't require you touching the <code>train.py<\/code> file.<\/p>\n<p>You can create an invoker file, let's call it <code>invoke.py<\/code>. Now, you can use it get rid of the keyword argument names. Something like this might work:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport subprocess\n\nif len(sys.argv[0]) &lt;= 1:\n  print(f&quot;{sys.argv[0]} program_name param0=&lt;param0&gt; param1=&lt;param1&gt; ...&quot;)\n  sys.exit(0)\n\nprogram = sys.argv[1]\nparams = sys.argv[2:]\n\nposparam = []\nfor param in params:\n  _, val = param.split(&quot;=&quot;)\n  posparam.append(val)\n\ncommand = [sys.executable, program, *posparam]\nprocess = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nout, err = process.communicate()\nsys.stdout.write(out.decode())\nsys.stdout.flush()\nsys.stderr.write(err.decode())\nsys.stderr.flush()\nsys.exit(process.returncode)\n<\/code><\/pre>\n<p>This allows you to invoke your <code>train.py<\/code> file as follows:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ python3 invoke.py \/path\/to\/train.py param0=0.001 param1=20 ...\n<\/code><\/pre>\n<p>Now to perform W&amp;B sweeps you can create a <code>command:<\/code> section (<a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command\" rel=\"nofollow noreferrer\">reference<\/a>) in your <code>sweeps.yaml<\/code> file while sweeping over the parameters <code>param0<\/code> and <code>param1<\/code>. For example:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>program: invoke.py\n...\nparameters:\n  param0:\n    distribution: uniform\n    min: 0\n    max: 1\n  param1:\n    distribution: categorical\n    values: [10, 20, 30]\ncommand:\n - ${env}\n - ${program}\n - \/path\/to\/train.py\n - ${args_no_hyphens}\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1645548706456,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1645549016652,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71223654",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71779667,
        "Question_title":"locked out of wandb local server - change user password",
        "Question_body":"<p>I am using a local weight and biases (wandb) instance running on a server with no internet connection.\nI have a user there and having no problems logging results from the server.<\/p>\n<p>However, when trying to see them in the UI it asked me to login again but unfortunately I forgot my password and reset password doesn't work with the message of <code>Error while trying to reset password<\/code>.<\/p>\n<p>I tried searching all over the documentation and found nothing to help with that.<\/p>\n<p>Any help for locally recovering my account will be appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649323903133,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":304,
        "Owner_creation_time":1608753761683,
        "Owner_last_access_time":1663850083317,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1657484089700,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71779667",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":67291062,
        "Question_title":"Control the logging frequency and contents when using wandb with HuggingFace",
        "Question_body":"<p>I am using the <code>wandb<\/code> with my HuggingFace code. I would like to log the loss and other metrics. Now I have two questions<\/p>\n<ul>\n<li>How does <code>wandb<\/code> decide when to log the loss? Is this decided by <code>logging_steps<\/code> in <code>TrainingArguments(...)<\/code>\uff1f<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>training_args = TrainingArguments(output_dir=&quot;test&quot;, \n                                  learning_rate=lr,\n                                  num_train_epochs=n_epoch,\n                                  seed=seed,\n                                  per_device_train_batch_size=2,\n                                  per_device_eval_batch_size=2,\n                                  logging_strategy=&quot;steps&quot;,\n                                  logging_steps=5,\n                                  report_to=&quot;wandb&quot;)\n<\/code><\/pre>\n<ul>\n<li>How do I make sure <code>wandb<\/code> log other metrics (for example, adding validation metrics after each epoch)? Does this happen automatically?<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619559781187,
        "Question_score":0,
        "Question_tags":"huggingface-transformers|wandb",
        "Question_view_count":385,
        "Owner_creation_time":1490778676137,
        "Owner_last_access_time":1664077488930,
        "Owner_location":null,
        "Owner_reputation":322,
        "Owner_up_votes":239,
        "Owner_down_votes":1,
        "Owner_views":109,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Correct, it is dictated by the <code>on_log<\/code> event from the Trainer, you can see it <a href=\"https:\/\/github.com\/huggingface\/transformers\/blob\/6b241e0e3bda24546d15835e5e0b48b8d1e4732c\/src\/transformers\/integrations.py#L747\" rel=\"nofollow noreferrer\">here<\/a> in WandbCallback<\/p>\n<p>Your validation metrics should be logged to W&amp;B automatically every time you validate. How often Trainer does evaluation depends on what setting is used for <code>evaluation_strategy<\/code> (and potentially <code>eval_steps<\/code> if <code>evaluation_strategy == &quot;steps&quot;<\/code>)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620162973716,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67291062",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69142852,
        "Question_title":"Weights and Biases watch log causing CUDA out of memory",
        "Question_body":"<p>I am trying to use WandB gradient visualization to debug the gradient flow in my neural net on Google Colab. Without WandB logging, the training runs without error, taking up 11Gb\/16GB on the p100 gpu. However, adding this line <code>wandb.watch(model, log='all', log_freq=3)<\/code> causes a cuda out of memory error.<\/p>\n<p><strong>How does WandB logging create extra GPU memory overhead?<\/strong><\/p>\n<p><strong>Is there some way to reduce the overhead?<\/strong><\/p>\n<p>--adding training loop code--<\/p>\n<pre><code>learning_rate = 0.001\nnum_epochs = 50\n\ndevice = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n\nmodel = MyModel()\n\nmodel = model.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nwandb.watch(model, log='all', log_freq=3)\n\nfor epoch in range(num_epochs):\n    train_running_loss = 0.0\n    train_accuracy = 0.0\n\n    model = model.train()\n\n    ## training step\n    for i, (name, output_array, input) in enumerate(trainloader):\n        \n        output_array = output_array.to(device)\n        input = input.to(device)\n        comb = torch.zeros(1,1,100,1632).to(device)\n\n        ## forward + backprop + loss\n        output = model(input, comb)\n\n        loss = my_loss(output, output_array)\n\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        ## update model params\n        optimizer.step()\n\n        train_running_loss += loss.detach().item()\n\n        temp = get_accuracy(output, output_array)\n\n        print('check 13')\n        !nvidia-smi | grep MiB | awk '{print $9 $10 $11}'\n\n        train_accuracy += temp     \n<\/code><\/pre>\n<p>-----edit-----<\/p>\n<p>I think WandB is creating an extra copy of the gradient during logging preprocessing. Here is the traceback:<\/p>\n<pre><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-11-13de83557b55&gt; in &lt;module&gt;()\n     60         get_ipython().system(&quot;nvidia-smi | grep MiB | awk '{print $9 $10 $11}'&quot;)\n     61 \n---&gt; 62         loss.backward()\n     63 \n     64         print('check 10')\n\n4 frames\n\/usr\/local\/lib\/python3.7\/dist-packages\/torch\/_tensor.py in backward(self, gradient, retain_graph, create_graph, inputs)\n    253                 create_graph=create_graph,\n    254                 inputs=inputs)\n--&gt; 255         torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n    256 \n    257     def register_hook(self, hook):\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/torch\/autograd\/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\n    147     Variable._execution_engine.run_backward(\n    148         tensors, grad_tensors_, retain_graph, create_graph, inputs,\n--&gt; 149         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n    150 \n    151 \n\n\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/wandb_torch.py in &lt;lambda&gt;(grad)\n    283             self.log_tensor_stats(grad.data, name)\n    284 \n--&gt; 285         handle = var.register_hook(lambda grad: _callback(grad, log_track))\n    286         self._hook_handles[name] = handle\n    287         return handle\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/wandb_torch.py in _callback(grad, log_track)\n    281             if not log_track_update(log_track):\n    282                 return\n--&gt; 283             self.log_tensor_stats(grad.data, name)\n    284 \n    285         handle = var.register_hook(lambda grad: _callback(grad, log_track))\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/wandb_torch.py in log_tensor_stats(self, tensor, name)\n    219         # Remove nans from tensor. There's no good way to represent that in histograms.\n    220         flat = flat[~torch.isnan(flat)]\n--&gt; 221         flat = flat[~torch.isinf(flat)]\n    222         if flat.shape == torch.Size([0]):\n    223             # Often the whole tensor is nan or inf. Just don't log it in that case.\n\nRuntimeError: CUDA out of memory. Tried to allocate 4.65 GiB (GPU 0; 15.90 GiB total capacity; 10.10 GiB already allocated; 717.75 MiB free; 14.27 GiB reserved in total by PyTorch)\n<\/code><\/pre>\n<p>---update----<\/p>\n<p>Indeed, commenting out the offending line <code>flat = flat[~torch.isinf(flat)]<\/code><\/p>\n<p>gets the logging step to just barely fit into the GPU memory.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1631361970417,
        "Question_score":2,
        "Question_tags":"pytorch|wandb",
        "Question_view_count":405,
        "Owner_creation_time":1545588089587,
        "Owner_last_access_time":1657580031330,
        "Owner_location":null,
        "Owner_reputation":131,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":1631485669880,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69142852",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73062370,
        "Question_title":"How to get a graph with the best performing runs via Sweeps (Weights & Biases)?",
        "Question_body":"<p>For my NER model I use Weights &amp; Biases sweeps for hyperparameter search. I do a grid search with about 100 runs and there are some really meaningful graphs. However, I can't figure out how to create a graph that shows about the best 10 runs in terms of f-score. Does anyone know how to do this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1658389162900,
        "Question_score":0,
        "Question_tags":"machine-learning|nlp|named-entity-recognition|wandb",
        "Question_view_count":58,
        "Owner_creation_time":1643710211767,
        "Owner_last_access_time":1663833597150,
        "Owner_location":null,
        "Owner_reputation":78,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In the sweep view, you can filter runs by certain criteria by clicking this button:\n<a href=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>There, you can add a filter to only show runs with an f1 score, or an accuracy or whatever metric you have logged higher than a certain value:\n<a href=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Of course, this won't filter for the 10 best runs, but for all runs with an accuracy of 0.9 and higher (example in picture).<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1658393400156,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73062370",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72590067,
        "Question_title":"jupyterLab Wandb does not iterative",
        "Question_body":"<p>I try to use WanDB but when i use wandb.init() there is nothing.!<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/EZfVt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EZfVt.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I am waiting a lot of time.<\/p>\n<p>However, there is nothing in window.<\/p>\n<p>This is working well in Kernel.<\/p>\n<p>please.. help me guys<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655016960480,
        "Question_score":0,
        "Question_tags":"jupyter-lab|wandb",
        "Question_view_count":40,
        "Owner_creation_time":1609152494583,
        "Owner_last_access_time":1663846781627,
        "Owner_location":"South Korea",
        "Owner_reputation":72,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I work at Weights &amp; Biases. If you're in a notebook the quickest thing you can do to get going with <code>wandb<\/code> is simply:<\/p>\n<pre><code>wandb.init(project=MY_PROJECT, entity=MY_ENTITY)\n<\/code><\/pre>\n<p>No <code>!wandb login<\/code>, <code>wandb.login()<\/code> or <code>%%wandb<\/code> needed. If you're not already logged in then <code>wandb.init<\/code> will ask you for you API key.<\/p>\n<p>(curious where you found <code>%%wandb<\/code> by the way?)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655111602567,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72590067",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71075704,
        "Question_title":"how to login wandb with another acount using colab?",
        "Question_body":"<p>For example, I have A, B acounts.<\/p>\n<p>First, I log in Google Colab with A account.\nand I want to log in wandb with B acounts. ( using !wandb login )\nis it possible??<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644556957917,
        "Question_score":0,
        "Question_tags":"authentication|google-colaboratory|wandb",
        "Question_view_count":316,
        "Owner_creation_time":1644556763937,
        "Owner_last_access_time":1649221420000,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can you the following commands to force a relogin:<\/p>\n<ul>\n<li>from terminal<\/li>\n<\/ul>\n<pre><code>wandb login --relogin\n<\/code><\/pre>\n<ul>\n<li>Using the API:<\/li>\n<\/ul>\n<pre><code>import wandb\nwandb.login(relogin=True)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1644559750243,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71075704",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":64413753,
        "Question_title":"wandb - how to get it really silent (weights and biases)",
        "Question_body":"<p>Working with Anaconda-Spyder (python 3.7), I installed the latest release of wandb (0.10.7) and try to use it with tensorflow (2.1.0) and keras (2.3.1). Since then, my console is polluted with lengthy comments due to wandb. So far I am using config and logs (not yet sweeps). It worked well for several runs BUT I cannot handle the outcome of my code that disappear in a flow of messages.<\/p>\n<p>I'd like to get rid of these messages (or find an alternative to wandb...)\nThanks in advance for your help ;-)\nHere is code to import the necessary libraries :<\/p>\n<pre><code>import os \nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  \nimport numpy as np\nimport pandas as pd \nimport tensorflow as tf \nfrom tensorflow import keras \nfrom tensorflow.keras.layers import Dropout, Dense, LSTM, Flatten, Activation from tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras import models from tensorflow.keras.models\nimport Sequential from tensorflow.keras.optimizers import Adam import\nwandb os.environ[&quot;WANDB_SILENT&quot;] = &quot;true&quot;\nwandb.init(project=&quot;project_tsa&quot;)\n<\/code><\/pre>\n<p>Later on, I define the wandb config as follows:<\/p>\n<pre><code>wandb.init(config={&quot;project_name&quot;:&quot;project_tsa&quot;,\n                   &quot;architecture&quot;: &quot;ResNet&quot;,\n                   &quot;load_weights&quot;: load_weights,\n                   &quot;epochs&quot;: epochs,\n                   &quot;batch_size&quot;: batch_size,\n                   &quot;iterations&quot;: iterat,\n                   &quot;dropout&quot;: dropout,\n                   &quot;learning_rate&quot;: learning,\n                   &quot;features&quot;: n_feature_maps,\n                   &quot;sequence&quot;: seq_length})\n<\/code><\/pre>\n<p>Eventually, I define several logs :<\/p>\n<pre><code>wandb.log({&quot;precis_pos&quot;: precis})\nwandb.log({&quot;recall_pos&quot;: recall})\nwandb.log({&quot;sortino_pos&quot;: sharpe_all[4]})\nwandb.log({&quot;sortinogain_pos&quot;: (sharpe_all[4]-sharpe_all[3])})\n<\/code><\/pre>\n<p>As soon as wandb.init is present, I automatically get ten lines of warnings :<\/p>\n<pre><code>wandb: WARNING Calling wandb.login() after wandb.init() has no effect.\n&lt;IPython.core.display.HTML object&gt;\nVBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)\u2026\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\nwandb: WARNING Calling wandb.login() after wandb.init() has no effect.\n&lt;IPython.core.display.HTML object&gt;\nVBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.0, max=1.0)\u2026\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n<\/code><\/pre>\n<p>As soon as I define config (and even worse with the logs), the code ends with more than a hundred lines of warning.... Here are just a few as example :<\/p>\n<pre><code>During handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;D:\\Anaconda3\\lib\\threading.py&quot;, line 926, in _bootstrap_inner\n    self.run()\n  File &quot;D:\\Anaconda3\\lib\\site-packages\\wandb\\filesync\\upload_job.py&quot;, line 40, in run\n    success = self.push()\n  File &quot;D:\\Anaconda3\\lib\\site-packages\\wandb\\filesync\\upload_job.py&quot;, line 88, in push\n    _, upload_headers, result = self._api.upload_urls(project, [self.save_name])\n  File &quot;D:\\Anaconda3\\lib\\site-packages\\wandb\\apis\\normalize.py&quot;, line 62, in wrapper\n    six.reraise(CommError, CommError(message, err), sys.exc_info()[2])\n  File &quot;D:\\Anaconda3\\lib\\site-packages\\six.py&quot;, line 702, in reraise\n    raise value.with_traceback(tb)\n  File &quot;D:\\Anaconda3\\lib\\site-packages\\wandb\\apis\\normalize.py&quot;, line 24, in wrapper\n    return func(*args, **kwargs)\n  File &quot;D:\\Anaconda3\\lib\\site-packages\\wandb\\internal\\internal_api.py&quot;, line 1039, in upload_urls\n    run = query_result[&quot;model&quot;][&quot;bucket&quot;]\nwandb.errors.error.CommError: 'NoneType' object is not subscriptable\n\n[SpyderKernelApp] WARNING | No such comm: 4562c8582bde4c50aedbd77151a94274\n[SpyderKernelApp] WARNING | No such comm: 197a5ff2d95849a0bd7d021f29e5f90e\n[SpyderKernelApp] WARNING | No such comm: c4ef2bdbcfb340e48c52099c8ac96dc1\n[SpyderKernelApp] WARNING | No such comm: 8898f986226043bfed836c08517299cb\n[SpyderKernelApp] WARNING | No such comm: 7fbb1deccf8c40629d95c57c6cbd2e6b\n[SpyderKernelApp] WARNING | No such comm: 1531c7eb303c4957e97426051d48441b\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603026773210,
        "Question_score":2,
        "Question_tags":"python|wandb",
        "Question_view_count":2929,
        "Owner_creation_time":1592919921400,
        "Owner_last_access_time":1663421488220,
        "Owner_location":"Brussels, Belgium",
        "Owner_reputation":59,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":1661850129076,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64413753",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71502443,
        "Question_title":"Wandb login permission denied python virtual environment",
        "Question_body":"<p>I am using a remote Slurm cluster from my university and want to get access to my wandb profile. I run my py project in a virtual env with <code>python 3.7.4<\/code>, where I could install successfully <code>wandb<\/code>.<\/p>\n<p>However, when I try to login from command line <code>python -m wandb login<\/code> I get this error<\/p>\n<pre><code>(374_env1) [userX@peregrine ~]$ python -m wandb login\nTraceback (most recent call last):\nFile &quot;\/software\/software\/Python\/3.7.4-GCCcore-8.3.0\/lib\/python3.7\/runpy.py&quot;, line 193, in _run_module_as_main\n&quot;__main__&quot;, mod_spec)\nFile &quot;\/software\/software\/Python\/3.7.4-GCCcore-8.3.0\/lib\/python3.7\/runpy.py&quot;, line 85, in _run_code\nexec(code, run_globals)\nFile &quot;\/data\/userX\/.envs\/374_env1\/lib\/python3.7\/site-packages\/wandb\/__main__.py&quot;, line 1, in &lt;module&gt;\nfrom wandb.cli import cli\nFile &quot;\/data\/userX\/.envs\/374_env1\/lib\/python3.7\/site-packages\/wandb\/cli\/cli.py&quot;, line 53, in &lt;module&gt;\ndatefmt=&quot;%Y-%m-%d %H:%M:%S&quot;,\nFile &quot;\/software\/software\/Python\/3.7.4-GCCcore-8.3.0\/lib\/python3.7\/logging\/__init__.py&quot;, line 1895, in basicConfig\nh = FileHandler(filename, mode)\nFile &quot;\/software\/software\/Python\/3.7.4-GCCcore-8.3.0\/lib\/python3.7\/logging\/__init__.py&quot;, line 1087, in __init__\nStreamHandler.__init__(self, self._open())\nFile &quot;\/software\/software\/Python\/3.7.4-GCCcore-8.3.0\/lib\/python3.7\/logging\/__init__.py&quot;, line 1116, in _open\nreturn open(self.baseFilename, self.mode, encoding=self.encoding)\nPermissionError: [Errno 13] Permission denied: '\/local\/tmp\/debug-cli.log'\n<\/code><\/pre>\n<p>I tried also to login outside of the venv but still permission denied. Is it because as a user from the cluster I don't have enough <code>-m<\/code> permissions? I also tried <code>python -u wandb login<\/code> but still permission denied. ps. I don't have <code>sudo<\/code> permission.\nAny insights?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1647454802023,
        "Question_score":0,
        "Question_tags":"python-3.x|virtualenv|wandb",
        "Question_view_count":337,
        "Owner_creation_time":1366555247053,
        "Owner_last_access_time":1661334085140,
        "Owner_location":null,
        "Owner_reputation":642,
        "Owner_up_votes":126,
        "Owner_down_votes":0,
        "Owner_views":116,
        "Question_last_edit_time":1647455744270,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71502443",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":70799787,
        "Question_title":"Can I use Wandb in Tracking my YOLOv4 Training?",
        "Question_body":"<p>I want to track my training in YOLOv4 using wandb but I can't see any tutorial on how to do it. I saw a youtube video but it was a training in YOLOv5.<\/p>\n<p>My wandb account is now logged in in the YOLOv4  training but I cant see no chart in the wandb page. It only displays this page <a href=\"https:\/\/i.stack.imgur.com\/AaAjB.png\" rel=\"nofollow noreferrer\">Wandb YOLOv4<\/a><\/p>\n<p>I want to know how exactly I can use wandb in my YOLOv4 training.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5,
        "Question_creation_time":1642760709873,
        "Question_score":0,
        "Question_tags":"yolov4|wandb",
        "Question_view_count":122,
        "Owner_creation_time":1641512289450,
        "Owner_last_access_time":1655351990127,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1642761049736,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70799787",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71916901,
        "Question_title":"WANDB Getting a run id based on tag",
        "Question_body":"<h1>Context<\/h1>\n<p>Hi!<\/p>\n<p>In <code>wandb<\/code> I can download a model based on a tag (<code>prod<\/code> for example), but I would like to also get all metrics associated to that run by using tags.<\/p>\n<p>The problem is that I don't know how to a get specific run ID based a tag.<\/p>\n<h1>Example<\/h1>\n<p>Using the code bellow we can extract a run summary metrics, but setting run IDs is setting me back.<\/p>\n<p>So if I can get run IDs based on tag or just explicitly download metrics  with another API call, like with a special sintax in <code>api.run<\/code>, that would be great! In the code example bellow I would like to use the <code>what_i_want_to_use<\/code> string to call the API instead of <code>what_i_use<\/code>.<\/p>\n<pre><code>import wandb\nfrom ast import literal_eval\napi = wandb.Api()\n\nwhat_i_use = &quot;team_name\/project_name\/runID_h3h3h4h4h4h4&quot;\n# what_i_want_to_use = &quot;team_name\/project_name\/artifact_name\/prod_tag&quot;\n\n# run is specified by &lt;entity&gt;\/&lt;project&gt;\/&lt;run_id&gt;\nrun = api.run(what_i_use)\n\n\n# save the metrics for the run to a csv file\nmetrics_dataframe = run.summary\nprint(metrics_dataframe['a_summary_metric'])\n\n<\/code><\/pre>\n<p>By running through the docs I didn't find any solution so far. Any ideias?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/mWl3I.png\" rel=\"nofollow noreferrer\">wandb public api run details<\/a><\/p>\n<p>Thanks for reading!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650314726207,
        "Question_score":0,
        "Question_tags":"python|metadata|wandb",
        "Question_view_count":645,
        "Owner_creation_time":1444675970627,
        "Owner_last_access_time":1664044672477,
        "Owner_location":null,
        "Owner_reputation":5,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It is possible to filter runs by tags as well. You can read more about it <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/public-api\/api#runs\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<pre><code>You can filter by config.*, summary_metrics.*, tags, state, entity, createdAt, etc.\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1650458488356,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71916901",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":70644326,
        "Question_title":"wandb.plot.line does not work and it just shows a table",
        "Question_body":"<p>I used this example that was provided by WandB. However, the web interface just shows a table instead of a figure.<\/p>\n<pre><code>data = [[i, random.random() + math.sin(i \/ 10)] for i in range(100)]\n        table = wandb.Table(data=data, columns=[&quot;step&quot;, &quot;height&quot;])\n        wandb.log({'line-plot1': wandb.plot.line(table, &quot;step&quot;, &quot;height&quot;)})\n<\/code><\/pre>\n<p>This is a screenshot from WandB's web interface:\n<a href=\"https:\/\/i.stack.imgur.com\/INmPU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/INmPU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Also, I have the same problem with other kinds of figures and charts that use a table.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641753400317,
        "Question_score":0,
        "Question_tags":"python|deep-learning|pytorch|wandb",
        "Question_view_count":184,
        "Owner_creation_time":1445719444550,
        "Owner_last_access_time":1664061059603,
        "Owner_location":"Iran, Canada",
        "Owner_reputation":331,
        "Owner_up_votes":113,
        "Owner_down_votes":3,
        "Owner_views":40,
        "Question_last_edit_time":null,
        "Answer_body":"<blockquote>\n<p>X-Post from the wandb forum<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744\" rel=\"nofollow noreferrer\">https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744<\/a><\/p>\n<p>If you click the section called \u201cCustom Charts\u201d above the Table, it\u2019ll show the line plot that you\u2019ve logged.<\/p>\n<p>Logging the Table also is expected behaviour, because this will allow users to interactively explore the logged data in a W&amp;B Table after logging it.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1641809697276,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70644326",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":70615413,
        "Question_title":"how to handle wandb Invalid filename characters exception on uploading image",
        "Question_body":"<p>I am using windows 10 &amp; venv &amp; python 3.9.7\nthis is my code to upload image to wandb<\/p>\n<pre><code>wandb_log[&quot;Image\/train_image&quot;] = wandb.Image('tmp.jpg')\nwandb.log(wandb_log, step)\n<\/code><\/pre>\n<p>the full directory of image is \u201cC:\\Users\\\uc774\uc900\ud601\\Documents\\Github\\terenz\\tmp.jpg\u201d\nHowever it creates this error<\/p>\n<pre><code>Media Image\/train_image is invalid. Please remove invalid filename characters\n<\/code><\/pre>\n<p>reinstalling wandb did not help to solve this problem.<br \/>\nAny suggestions? Thanks<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1641517271160,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":94,
        "Owner_creation_time":1603378831587,
        "Owner_last_access_time":1663547314893,
        "Owner_location":null,
        "Owner_reputation":161,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70615413",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71916491,
        "Question_title":"WANDB run initialization",
        "Question_body":"<p>I wanted to try using wandb to log runs of my ML experiments for a project; but I am not able to initialize the run itself.\nI tried:<\/p>\n<p><code>run = wandb.init(project=&quot;name&quot;,entity=&quot;username&quot;,name=&quot;classification&quot;)<\/code><\/p>\n<p>This results in:\nwandb: W&amp;B API key is configured (use <code>wandb login --relogin<\/code> to force relogin)<\/p>\n<p>wandb: Network error (ConnectTimeout), entering retry loop.<\/p>\n<p>wandb: Network error (ConnectTimeout), entering retry loop.<\/p>\n<p>What can I do to fix this? (I did login through the terminal before launching this cell idk what else to try)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1650311929900,
        "Question_score":0,
        "Question_tags":"python|jupyter-notebook|wandb",
        "Question_view_count":337,
        "Owner_creation_time":1650311491997,
        "Owner_last_access_time":1654197360160,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1650312801032,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71916491",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":66541985,
        "Question_title":"'WANDB_MODE' is not recognized as an internal or external command, operable program or batch file",
        "Question_body":"<p>I am trying to run the custom <a href=\"https:\/\/github.com\/ultralytics\/yolov5\" rel=\"nofollow noreferrer\">yolo model<\/a> on my data set in my local machine. I am following some reference code from the kaggle platform. Here first time I encounter the <code>wandb<\/code> frame work. while doing so I use the following to run the <code>train.py <\/code> file in my jupyter lab.<\/p>\n<pre><code>!WANDB_MODE=&quot;dryrun&quot; python train.py --img 640 --batch 16 --epochs 30 --data D:\/Anil\/Shawn_Research\/Iamge_DataSet\/VinBigData\/New_Direct\/vinbigdata.yaml --weights yolov5x.pt --cache\n<\/code><\/pre>\n<p>This work fine on the kaggle platform but in my local machine it shows following:<\/p>\n<pre><code>'WANDB_MODE' is not recognized as an internal or external command, operable program or batch file.\n<\/code><\/pre>\n<p>While reading the similar thread I realized I might making mistake related to path variable or Environment variable.\nEven I tried to get solution from the <a href=\"https:\/\/docs.wandb.ai\/library\/environment-variables\" rel=\"nofollow noreferrer\">official document<\/a> but couldn't figure out.<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615272052933,
        "Question_score":0,
        "Question_tags":"python-3.x|windows|pytorch|wandb",
        "Question_view_count":263,
        "Owner_creation_time":1490268584217,
        "Owner_last_access_time":1663920632157,
        "Owner_location":null,
        "Owner_reputation":37,
        "Owner_up_votes":101,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":1661850709696,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66541985",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73335735,
        "Question_title":"Wandb automatically logged into the wrong user -- why?",
        "Question_body":"<p>I followed the usual instructions:<\/p>\n<pre><code>pip install wandb\nwandb login\n<\/code><\/pre>\n<p>but then it never asked me for the user and thus when I pasted my key into the terminal when asked it was there in the <code>.netrc<\/code> file but it was all wrong:<\/p>\n<pre><code>(iit_term_synthesis) brandomiranda~ \u276f\n(iit_term_synthesis) brandomiranda~ \u276f wandb login\nwandb: W&amp;B API key is configured. Use `wandb login --relogin` to force relogin\n(iit_term_synthesis) brandomiranda~ \u276f wandb login --relogin\nwandb: Logging into wandb.ai. (Learn how to deploy a W&amp;B server locally: https:\/\/wandb.me\/wandb-server)\nwandb: You can find your API key in your browser here: https:\/\/wandb.ai\/authorize\nwandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\nwandb: Appending key for api.wandb.ai to your netrc file: \/Users\/brandomiranda\/.netrc\n(iit_term_synthesis) brandomiranda~ \u276f cat \/Users\/brandomiranda\/.netrc\nmachine api.wandb.ai\n  login user\n  password djkfhkjsdhfkjshdkfj...SECRET...sdhjfjhsdjkfhsdjf\n<\/code><\/pre>\n<p>fyi useufl command:<\/p>\n<pre><code>cat ~\/.netrc\n<\/code><\/pre>\n<p>how to fix this?<\/p>\n<hr \/>\n<p>seems my issue only happens in pycharm, when I run it in the terminal it works... :\/<\/p>\n<hr \/>\n<p>cross: <a href=\"https:\/\/community.wandb.ai\/t\/wandb-automatically-logeed-into-the-wrong-user-why\/2916\" rel=\"nofollow noreferrer\">https:\/\/community.wandb.ai\/t\/wandb-automatically-logeed-into-the-wrong-user-why\/2916<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1660314751363,
        "Question_score":1,
        "Question_tags":"pycharm|wandb",
        "Question_view_count":157,
        "Owner_creation_time":1340313730287,
        "Owner_last_access_time":1664073688613,
        "Owner_location":null,
        "Owner_reputation":11435,
        "Owner_up_votes":1807,
        "Owner_down_votes":299,
        "Owner_views":6472,
        "Question_last_edit_time":1660666336796,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73335735",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":67495547,
        "Question_title":"wandb logging PermissionError and OSError",
        "Question_body":"<p>Description:<\/p>\n<ul>\n<li><p>When running experiments using <code>Weights and Biases<\/code> (wandb), I\noccasionally get a <code>PermissionError<\/code> for Python's <code>logging<\/code> library\nand <code>OSError<\/code> for accessing the TLS CA cert.<\/p>\n<\/li>\n<li><p>I had the following stacktrace, repeated many times with different\ntypes of &quot;message&quot;. I can't discern the order of operations, but I'm\nguessing the cert can't be accessed and that causes the script to\ncrash, but I don't know why it only happens sometimes.<\/p>\n<\/li>\n<li><p>If it is relevant, I ran the experiments on an Ubuntu server, authenticated via Kerberos.<\/p>\n<\/li>\n<\/ul>\n<p>What I've tried:<\/p>\n<ul>\n<li>I have manually checked the CA cert, and more than half the time I can successfully run experiments. As such I don't think it's the same as <a href=\"https:\/\/stackoverflow.com\/questions\/49100986\/certbot-could-not-find-a-suitable-tls-ca-certificate-bundle-archlinux\">this<\/a> or <a href=\"https:\/\/stackoverflow.com\/questions\/46119901\/python-requests-cant-find-a-folder-with-a-certificate-when-converted-to-exe\">this<\/a>.<\/li>\n<\/ul>\n<p>Stacktrace<\/p>\n<pre><code>Message: 'handle_request: stop_status'                                                                                                                                      [854\/1967]Arguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/logging\/__init__.py&quot;, line 1085, in emit\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/logging\/__init__.py&quot;, line 1065, in flush\nPermissionError: [Errno 13] Permission denied\nCall stack:\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/threading.py&quot;, line 890, in _bootstrap\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/threading.py&quot;, line 932, in _bootstrap_inner\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py&quot;, line 54, in run\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py&quot;, line 95, in _run\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal.py&quot;, line 280, in _process\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py&quot;, line 175, in send\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py&quot;, line 183, in send_request\nMessage: 'send_request: stop_status'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py&quot;, line 24, in wrapper\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py&quot;, line 681, in check_stop_requested\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py&quot;, line 102, in __call__\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py&quot;, line 127, in execute\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/gql\/client.py&quot;, line 52, in execute\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/gql\/client.py&quot;, line 60, in _get_result\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/gql\/transport\/requests.py&quot;, line 38, in execute\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/requests\/api.py&quot;, line 119, in post\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/requests\/api.py&quot;, line 61, in request\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/requests\/sessions.py&quot;, line 530, in request\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/requests\/sessions.py&quot;, line 643, in send\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/requests\/adapters.py&quot;, line 416, in send\n  File &quot;\/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/requests\/adapters.py&quot;, line 227, in cert_verify\nOSError: Could not find a suitable TLS CA certificate bundle, invalid path: \/home\/some_user\/miniconda3\/envs\/part_ii_dev-conda\/lib\/python3.8\/site-packages\/certifi\/cacert.pem\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1620775947520,
        "Question_score":0,
        "Question_tags":"python-requests|ssl-certificate|kerberos|wandb",
        "Question_view_count":468,
        "Owner_creation_time":1519031032163,
        "Owner_last_access_time":1663673544740,
        "Owner_location":null,
        "Owner_reputation":496,
        "Owner_up_votes":47,
        "Owner_down_votes":5,
        "Owner_views":16,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67495547",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71469200,
        "Question_title":"Weights and Biases - cumulative max (highwater mark) for distributed training and sweep",
        "Question_body":"<p>I have an algorithm that I run 10 times, and return the best run by a cumulative maximum - So for each run, I return the highest validation score of the entire run. For example, this graph:\n<a href=\"https:\/\/i.stack.imgur.com\/1Iy28.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1Iy28.png\" alt=\"actual validation\" \/><\/a><\/p>\n<p>turns to this graph:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/csUOd.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/csUOd.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I ran 7 of these, and grouped them together aggregating with maximum. However, since each experiment validates at different timestep, the resulting graph is not a cumulative maximum of the entire 7 runs. That happens because at each validation point, not all runs are present:\n<a href=\"https:\/\/i.stack.imgur.com\/VhVqe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VhVqe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What I would like to have is something like this:\n<a href=\"https:\/\/i.stack.imgur.com\/wrlxx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wrlxx.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ol>\n<li>Is this achievable?<\/li>\n<li>How can I set a sweep that uses the cumulative validation of the entire experiment (not not a single trial)?<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647267422323,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":101,
        "Owner_creation_time":1553508349353,
        "Owner_last_access_time":1648824868747,
        "Owner_location":"Israel",
        "Owner_reputation":86,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71469200",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73617230,
        "Question_title":"How to avoid data averaging when logging to metric across multiple runs?",
        "Question_body":"<p>I'm trying to log data points for the same metric across multiple runs (<code>wandb.init<\/code> is called repeatedly in between each data point) and I'm unsure how to avoid the behavior seen in the attached screenshot...<\/p>\n<p>Instead of getting a line chart with multiple points, I'm getting a single data point with associated statistics. In the attached e.g., the 1st data point was generated at step 1,470 and the 2nd at step 2,940...rather than seeing two points, I'm instead getting a single point that's the average and appears at step 2,205.\n<a href=\"https:\/\/i.stack.imgur.com\/98Lln.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/98Lln.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>My hunch is that using the <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/resuming\" rel=\"nofollow noreferrer\">resume run<\/a> feature may address my problem, but even testing out this hunch is proving to be cumbersome given the constraints of the system I'm working with...<\/p>\n<p>Before I invest more time in my hypothesized solution, could someone confirm that the behavior I'm seeing is, indeed, the result of logging data to the same metric across separate runs without using the resume feature?<\/p>\n<p>If this is the case, can you confirm or deny my conception of how to use resume?<\/p>\n<p>Initial run:<\/p>\n<ol>\n<li><code>run = wandb.init()<\/code><\/li>\n<li><code>wandb_id = run.id<\/code><\/li>\n<li>cache <code>wandb_id<\/code> for successive runs<\/li>\n<\/ol>\n<p>Successive run:<\/p>\n<ol>\n<li>retrieve <code>wandb_id<\/code> from cache<\/li>\n<li><code>wandb.init(id=wandb_id, resume=&quot;must&quot;)<\/code><\/li>\n<\/ol>\n<p>Is it also acceptable \/ preferable to replace <code>1.<\/code> and <code>2.<\/code> of the initial run with:<\/p>\n<ol>\n<li><code>wandb_id = wandb.util.generate_id()<\/code><\/li>\n<li><code>wandb.init(id=wandb_id)<\/code><\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1662443319723,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":35,
        "Owner_creation_time":1352429442633,
        "Owner_last_access_time":1664036259503,
        "Owner_location":null,
        "Owner_reputation":740,
        "Owner_up_votes":46,
        "Owner_down_votes":5,
        "Owner_views":63,
        "Question_last_edit_time":null,
        "Answer_body":"<blockquote>\n<p>My hunch is that using the resume run feature may address my problem,<\/p>\n<\/blockquote>\n<p>Indeed, providing a cached <code>id<\/code> in combination with <code>resume=&quot;must&quot;<\/code> fixed the issue.\n<a href=\"https:\/\/i.stack.imgur.com\/KooZ9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KooZ9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Corresponding snippet:<\/p>\n<pre><code>import wandb\n\n# wandb run associated with evaluation after first N epochs of training.\nwandb_id = wandb.util.generate_id()\nwandb.init(id=wandb_id, project=&quot;alrichards&quot;, name=&quot;test-run-3\/job-1&quot;, group=&quot;test-run-3&quot;)\nwandb.log({&quot;mean_evaluate_loss_epoch&quot;: 20}, step=1)\nwandb.finish()\n\n# wandb run associated with evaluation after second N epochs of training.\nwandb.init(id=wandb_id, resume=&quot;must&quot;, project=&quot;alrichards&quot;, name=&quot;test-run-3\/job-2&quot;, group=&quot;test-run-3&quot;)\nwandb.log({&quot;mean_evaluate_loss_epoch&quot;: 10}, step=5)\nwandb.finish()\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1662569956687,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73617230",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69870886,
        "Question_title":"How to exit the login of wandb?Two people have two accounts on one machine...Anyway other than relogin?",
        "Question_body":"<p>There are two people using wandb api on one machine. If one forgets to relogin before running, the other one's run will be shown in the prevois one's account. I wonder if there is a way to exit the login state and then the other one will know that he\/ she hasn't login before he\/her starts running programs. IN A HURRY for help about this.\nThe senoir students in my lab ask me to try to do so but I simply don't know how and I couldn't find a answer.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636274174763,
        "Question_score":0,
        "Question_tags":"authentication|exit|conflict|wandb",
        "Question_view_count":700,
        "Owner_creation_time":1627886181520,
        "Owner_last_access_time":1642858256430,
        "Owner_location":null,
        "Owner_reputation":17,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69870886",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":64093979,
        "Question_title":"Weights and Biases: Login and network errors",
        "Question_body":"<p>I recently installed Weights and Biases (wandb) for recording the metrics of my machine learning projects. Everything worked fine when connected to wandb cloud instance or when I used a local docker image. Now, when I tried to access my local wandb instance from over the network, I started to get API error messages. However, I also noticed that wandb was trying to access my server using port 80, instead of 8080. I installed wandb client on a new cloud server and tried to access my server from there. Still, same error message shown below.<\/p>\n<p>This error happens when I use the command: <code>wandb login host=https:\/\/api.wandb.ai<\/code>\nI have tried to delete the .netrc file where the api settings are stored and re-installed wandb. Still same error. Using wandb version 0.10.2 on Ubuntu 18.04; Also, tried downgrading to version 0.8.36, no change.\nIf I try the command: <code>wandb login --relogin<\/code>, I get the same error.<\/p>\n<p>Is there some way to reset wandb so it forgets all these settings, or to resolve this issue directly?<\/p>\n<p>Many thanks<\/p>\n<p>Best Regards,<\/p>\n<p>Adeel<\/p>\n<pre><code>Retry attempt failed:\nTraceback (most recent call last):\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/connection.py&quot;, line 160, in _new_conn\n    (self._dns_host, self.port), self.timeout, **extra_kw\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/util\/connection.py&quot;, line 84, in create_connection\n    raise err\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/util\/connection.py&quot;, line 74, in create_connection\n    sock.connect(sa)\nConnectionRefusedError: [Errno 111] Connection refused\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/connectionpool.py&quot;, line 677, in urlopen\n    chunked=chunked,\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/connectionpool.py&quot;, line 392, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/http\/client.py&quot;, line 1277, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/http\/client.py&quot;, line 1323, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/http\/client.py&quot;, line 1272, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/http\/client.py&quot;, line 1032, in _send_output\n    self.send(msg)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/http\/client.py&quot;, line 972, in send\n    self.connect()\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/connection.py&quot;, line 187, in connect\n    conn = self._new_conn()\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/connection.py&quot;, line 172, in _new_conn\n    self, &quot;Failed to establish a new connection: %s&quot; % e\nurllib3.exceptions.NewConnectionError: &lt;urllib3.connection.HTTPConnection object at 0x7f0a26b54c50&gt;: Failed to establish a new connection: [Errno 111] Connection refused\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/requests\/adapters.py&quot;, line 449, in send\n    timeout=timeout\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/connectionpool.py&quot;, line 727, in urlopen\n    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/urllib3\/util\/retry.py&quot;, line 439, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='34.71.47.117', port=80): Max retries exceeded with url: \/graphql (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f0a26b54c50&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/old\/retry.py&quot;, line 96, in __call__\n    result = self._call_fn(*args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/internal\/internal_api.py&quot;, line 128, in execute\n    return self.client.execute(*args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/vendor\/gql-0.2.0\/gql\/client.py&quot;, line 52, in execute\n    result = self._get_result(document, *args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/vendor\/gql-0.2.0\/gql\/client.py&quot;, line 60, in _get_result\n    return self.transport.execute(document, *args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/vendor\/gql-0.2.0\/gql\/transport\/requests.py&quot;, line 38, in execute\n    request = requests.post(self.url, **post_args)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/requests\/api.py&quot;, line 119, in post\n    return request('post', url, data=data, json=json, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/requests\/api.py&quot;, line 61, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/requests\/sessions.py&quot;, line 530, in request\n    resp = self.send(prep, **send_kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/requests\/sessions.py&quot;, line 643, in send\n    r = adapter.send(request, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/requests\/adapters.py&quot;, line 516, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='34.71.47.117', port=80): Max retries exceeded with url: \/graphql (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f0a26b54c50&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\nwandb: Network error (ConnectionError), entering retry loop. See wandb\/debug-internal.log for full traceback.\nTraceback (most recent call last):\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/bin\/wandb&quot;, line 8, in &lt;module&gt;\n    sys.exit(cli())\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 829, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 782, in main\n    rv = self.invoke(ctx)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 610, in invoke\n    return callback(*args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/cli\/cli.py&quot;, line 72, in wrapper\n    return func(*args, **kwargs)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/cli\/cli.py&quot;, line 212, in login\n    wandb.login(relogin=relogin, key=key, anonymous=anon_mode, host=host, force=True)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_login.py&quot;, line 29, in login\n    anonymous=anonymous, key=key, relogin=relogin, host=host, force=force\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_login.py&quot;, line 128, in _login\n    apikey.write_key(settings, key)\n  File &quot;\/media\/adeel\/Space\/AI\/anaconda3\/envs\/tf2_gpu\/lib\/python3.7\/site-packages\/wandb\/lib\/apikey.py&quot;, line 223, in write_key\n    raise ValueError(&quot;API key must be 40 characters long, yours was %s&quot; % len(key))\nValueError: API key must be 40 characters long, yours was 26\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1601246511380,
        "Question_score":4,
        "Question_tags":"python|wandb",
        "Question_view_count":6277,
        "Owner_creation_time":1445990517173,
        "Owner_last_access_time":1663982428387,
        "Owner_location":"Sydney, New South Wales, Australia",
        "Owner_reputation":689,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":87,
        "Question_last_edit_time":1661849922332,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64093979",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69640534,
        "Question_title":"How to log artifacts in wandb while using saimpletransformers?",
        "Question_body":"<p>I am creating a Question Answering model using <a href=\"https:\/\/simpletransformers.ai\/docs\/qa-specifics\/\" rel=\"nofollow noreferrer\">simpletransformers<\/a>. I would also like to use wandb to track model artifacts. As I understand from <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/other\/simpletransformers\" rel=\"nofollow noreferrer\">wandb docs<\/a>, there is an integration touchpoint for simpletransformers but there is no mention of logging artifacts.<\/p>\n<p>I would like to log artifacts generated at the train, validation, and test phase such as train.json, eval.json, test.json, output\/nbest_predictions_test.json and best performing model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634705743473,
        "Question_score":0,
        "Question_tags":"nlp-question-answering|simpletransformers|wandb",
        "Question_view_count":53,
        "Owner_creation_time":1528765704783,
        "Owner_last_access_time":1635485916997,
        "Owner_location":"Ahmedabad, Gujarat, India",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Currently simpleTransformers doesn't support logging artifacts within the training\/testing scripts. But you can do it manually:<\/p>\n<pre><code>import os \n\nwith wandb.init(id=model.wandb_run_id, resume=&quot;allow&quot;, project=wandb_project) as training_run:\n    for dir in sorted(os.listdir(&quot;outputs&quot;)):\n        if &quot;checkpoint&quot; in dir:\n            artifact = wandb.Artifact(&quot;model-checkpoints&quot;, type=&quot;checkpoints&quot;)\n            artifact.add_dir(&quot;outputs&quot; + &quot;\/&quot; + dir)\n            training_run.log_artifact(artifact)\n<\/code><\/pre>\n<p>For more info, you can follow along with the W&amp;B notebook in the SimpleTransofrmer's README.md<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1634729204572,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69640534",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72951797,
        "Question_title":"Can you pass agent-specific parameters to a wandb sweep?",
        "Question_body":"<p>I would like to be able to pass a particular parameter with a different value on each agent in my wandb sweep.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1657626404050,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":42,
        "Owner_creation_time":1522603483940,
        "Owner_last_access_time":1663947359087,
        "Owner_location":"Paris, France",
        "Owner_reputation":66,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72951797",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":67696204,
        "Question_title":"AttributeError: 'NoneType' object has no attribute '_global_run_stack'",
        "Question_body":"<p><strong>Description<\/strong><\/p>\n<p>I am using PTAN library with an A3C model and I am trying to work with <strong>wandb sweep<\/strong> but I've encountered some weird problems, and I am not sure if it's a bug regarding sweep (because if I am going to use just a simple model without any threads involving is going to work properly) or I am doing something wrong.<\/p>\n<p><strong>How to reproduce<\/strong><\/p>\n<p><em><strong>training function:<\/strong><\/em><\/p>\n<pre><code>def train(conf):\n    batch = []\n    step_idx = 0\n    epoch = conf['epochs']\n    try:\n        with commune.RewardTracker(writer, stop_reward=conf['reward_bound']) as tracker:\n            with ptan.common.utils.TBMeanTracker(writer, batch_size=100) as tb_tracker:\n                while True:\n                    if step_idx == epoch:\n                        break\n                    train_entry = train_queue.get()\n                    if isinstance(train_entry, TotalReward):\n                        if tracker.reward(train_entry.reward, step_idx):\n                            break\n                        continue\n                    if isinstance(train_entry, TotalProfit):\n                        tracker.profits(train_entry.total_profit, train_entry.curr_profit, step_idx)\n                        continue\n                    step_idx += 1\n                    if step_idx % 100 == 0:\n                        torch.save(net.state_dict(), os.path.join(SAVING_FOLDER, PROJECT_NAME))\n\n                    batch.append(train_entry)\n                    if len(batch) &lt; conf['batch_size']:\n                        continue\n\n                    states_v, actions_t, vals_ref_v = commune.unpack_batch(batch, net,\n                                                                           last_val_gamma=conf['gamma'] ** conf['reward_steps'],\n                                                                           device=device)\n                    batch.clear()\n\n                    optimizer.zero_grad()\n                    logits_v, value_v = net(states_v)\n\n                    loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v)\n\n                    log_prob_v = F.log_softmax(logits_v, dim=1)\n                    adv_v = vals_ref_v - value_v.detach()\n                    log_prob_actions_v = adv_v * log_prob_v[range(conf['batch_size']), actions_t]\n                    loss_policy_v = -log_prob_actions_v.mean()\n\n                    prob_v = F.softmax(logits_v, dim=1)\n                    entropy_loss_v = conf['entropy_beta'] * (prob_v * log_prob_v).sum(dim=1).mean()\n\n                    loss_v = entropy_loss_v + loss_value_v + loss_policy_v\n                    loss_v.backward()\n                    nn_utils.clip_grad_norm_(net.parameters(), conf['clip_grad'])\n                    optimizer.step()\n\n                    tb_tracker.track(&quot;advantage&quot;, adv_v, step_idx)\n                    tb_tracker.track(&quot;values&quot;, value_v, step_idx)\n                    tb_tracker.track(&quot;batch_rewards&quot;, vals_ref_v, step_idx)\n                    tb_tracker.track(&quot;loss_entropy&quot;, entropy_loss_v, step_idx)\n                    tb_tracker.track(&quot;loss_policy&quot;, loss_policy_v, step_idx)\n                    tb_tracker.track(&quot;loss_value&quot;, loss_value_v, step_idx)\n                    tb_tracker.track(&quot;loss_total&quot;, loss_v, step_idx)\n    finally:\n        for p in data_proc_list:\n            p.terminate()\n            p.join()\n<\/code><\/pre>\n<p><strong>main function:<\/strong><\/p>\n<pre><code>if __name__ == &quot;__main__&quot;:\n    mp.set_start_method('fork')\n    device = torch.device(&quot;cuda:0&quot; if use_cuda else &quot;cpu&quot;)\n\n    with open(r'sweep_config.yaml') as file:\n        sweep_config = yaml.load(file, Loader=yaml.FullLoader)\n\n    logs_dir_name = &quot;a3c_stock&quot;\n    wandb.tensorboard.patch(root_logdir=logs_dir_name)\n\n    sweep_id = wandb.sweep(sweep_config, project=&quot;sweep_project&quot;, entity=&quot;vildnex&quot;)\n    wandb.init(config=config_default)\n\n    config = wandb.config\n\n    writer = SummaryWriter(comment=logs_dir_name)\n\n    env = make_env(config)\n    net = commune.AtariA2C(env.observation_space.shape, env.action_space.n).to(device)\n    net.share_memory()\n\n    if not os.path.isdir(SAVING_FOLDER):\n        os.mkdir(SAVING_FOLDER)\n\n    if os.path.isfile(os.path.join(SAVING_FOLDER, PROJECT_NAME)):\n        net.load_state_dict(torch.load(os.path.join(SAVING_FOLDER, PROJECT_NAME), map_location=device))\n\n    optimizer = optim.RMSprop(net.parameters(), lr=config.learning_rate, eps=1e-3)\n\n    train_queue = mp.Queue(maxsize=config.processes_count)\n    data_proc_list = []\n    dict_conf = dict(config)\n    for _ in range(config.processes_count):\n        data_proc = mp.Process(target=data_func, args=(net, device, train_queue, dict_conf))\n        data_proc.start()\n        data_proc_list.append(data_proc)\n\n    wandb.agent(sweep_id, lambda: train(dict_conf))\n<\/code><\/pre>\n<p><strong>Error message:<\/strong><\/p>\n<pre><code>Exception in thread Thread-6:\nTraceback (most recent call last):\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/agents\/pyagent.py&quot;, line 303, in _run_job\n    self._function()\n  File &quot;&lt;PATH&gt;\/RL_TraningBot\/EXPERIMENTS\/A3C_TEST.py&quot;, line 191, in &lt;lambda&gt;\n    wandb.agent(sweep_id, lambda: train(dict_conf))\n  File &quot;&lt;PATH&gt;\/RL_TraningBot\/EXPERIMENTS\/A3C_TEST.py&quot;, line 105, in train\n    tracker.profits(train_entry.total_profit, train_entry.curr_profit, step_idx)\n  File &quot;&lt;PATH&gt;\/RL_TraningBot\/EXPERIMENTS\/commune.py&quot;, line 118, in profits\n    self.writer.add_scalar(&quot;total_profit&quot;, total_profit, frame)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/torch\/utils\/tensorboard\/writer.py&quot;, line 344, in add_scalar\n    self._get_file_writer().add_summary(\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/torch\/utils\/tensorboard\/writer.py&quot;, line 250, in _get_file_writer\n    self.file_writer = FileWriter(self.log_dir, self.max_queue,\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/torch\/utils\/tensorboard\/writer.py&quot;, line 60, in __init__\n    self.event_writer = EventFileWriter(\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/integration\/tensorboard\/monkeypatch.py&quot;, line 157, in __init__\n    _notify_tensorboard_logdir(logdir, save=save, root_logdir=root_logdir_arg)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/integration\/tensorboard\/monkeypatch.py&quot;, line 167, in _notify_tensorboard_logdir\n    wandb.run._tensorboard_callback(logdir, save=save, root_logdir=root_logdir)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py&quot;, line 804, in _tensorboard_callback\n    self._backend.interface.publish_tbdata(logdir, save, root_logdir)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/interface.py&quot;, line 202, in publish_tbdata\n    self._publish(rec)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/interface.py&quot;, line 518, in _publish\n    raise Exception(&quot;The wandb backend process has shutdown&quot;)\nException: The wandb backend process has shutdown\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/lib\/python3.9\/threading.py&quot;, line 954, in _bootstrap_inner\n    self.run()\n  File &quot;\/usr\/lib\/python3.9\/threading.py&quot;, line 892, in run\n    self._target(*self._args, **self._kwargs)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/agents\/pyagent.py&quot;, line 308, in _run_job\n    wandb.finish(exit_code=1)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py&quot;, line 2374, in finish\n    wandb.run.finish(exit_code=exit_code)\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py&quot;, line 1144, in finish\n    if self._wl and len(self._wl._global_run_stack) &gt; 0:\n  File &quot;&lt;PATH&gt;\/venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_setup.py&quot;, line 234, in __getattr__\n    return getattr(self._instance, name)\nAttributeError: 'NoneType' object has no attribute '_global_run_stack'\n<\/code><\/pre>\n<p><strong>Environment<\/strong><\/p>\n<ul>\n<li>OS: Manjaro 5.21.5<\/li>\n<li>Environment: PyCharm Local<\/li>\n<li>Python Version: 3.9<\/li>\n<\/ul>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1621980760977,
        "Question_score":0,
        "Question_tags":"python|pytorch|sweeper|wandb",
        "Question_view_count":1192,
        "Owner_creation_time":1479249043857,
        "Owner_last_access_time":1664055565850,
        "Owner_location":null,
        "Owner_reputation":1383,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":177,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67696204",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71617315,
        "Question_title":"Wandb line plots only show bar charts after refresh",
        "Question_body":"<p>My weights and biases (wandb) panels (e.g. for loss) shortly show line plots (x: steps, y: loss), then refresh (showing a spinner for some time) and then only show bar charts.<\/p>\n<p>Editing such panels either shows (a) &quot;Select runs that logged eval\/loss\nto visualize data in this line chart.&quot; on the left or (b) &quot;Showing a bar chart instead of a line chart because all logged values are length one.&quot; on the right.<\/p>\n<p>Does that mean that (a) a value <code>eval\/loss<\/code> is not found for the runs, or (b) only one value is given per run? How can I change this? But why was there a real line plot shown for about a second, before the panel refreshes? Where there values dropped? Why?<\/p>\n<p>Panels shortly show line plots: <a href=\"https:\/\/i.stack.imgur.com\/MaMtL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/MaMtL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Then panels refresh and only show bar charts: <a href=\"https:\/\/i.stack.imgur.com\/5oTvP.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5oTvP.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Code<\/h2>\n<p>I am using huggingface transformers <code>TrainingArguments<\/code> with argument <code>report_to=&quot;wandb&quot;<\/code> (but with the default <code>logging_steps<\/code> of 500). I am doing 10-fold cross validation without any explicit <code>wandb.log<\/code> call within the cross validation loop. I do all of this in the <code>train()<\/code> function, which has as last command <code>wandb.finish()<\/code>. <code>train()<\/code> is called via <code>wandb.agent(sweep_id, train)<\/code> as I am using all of this within a large sweep.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1648212833057,
        "Question_score":0,
        "Question_tags":"machine-learning|pytorch|huggingface-transformers|wandb",
        "Question_view_count":231,
        "Owner_creation_time":1305196069933,
        "Owner_last_access_time":1663753090530,
        "Owner_location":"Karlsruhe, Germany",
        "Owner_reputation":6763,
        "Owner_up_votes":937,
        "Owner_down_votes":28,
        "Owner_views":709,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71617315",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71312243,
        "Question_title":"Log metrics with configuation in Pytorch Lightning using w&b",
        "Question_body":"<p>I am using PyTorch Lightning together with w&amp;b and trying associate metrics with a finite set of configurations. In the <a href=\"https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/common\/lightning_module.html\" rel=\"nofollow noreferrer\"><code>LightningModule<\/code><\/a> class I have defined the <code>test_step<\/code> as:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def test_step(self, batch, batch_idx):\n  x, y_true, config_file =  batch\n  y_pred = self.forward(x)\n  accuracy = self.accuracy(y_pred, y_true)\n  self.log(&quot;test\/accuracy&quot;, accuracy)\n<\/code><\/pre>\n<p>Assuming (for simplicity) that the batch size is 1, this will log the accuracy for 1 sample and it will be displayed as a chart in the w&amp;b dashboard.<\/p>\n<p>I would like to associate this accuracy with some configuration of the experimental environment. This configuration might include BDP factor, bandwith delay, queue_size, location, etc. I don't want to plot the configurations I just want to be able to filter or group the accuracy by some configuration value.<\/p>\n<p>The only solution I can come up with is to add these configurations as a querystring:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def test_step(self, batch, batch_idx):\n  x, y_true, config_file =  batch\n  # read values in config file\n  # ...\n\n  y_pred = self.forward(x)\n  accuracy = self.accuracy(y_pred, y_true)\n  self.log(&quot;test\/BDP=2&amp;delay=10ms&amp;queue_size=10&amp;topology=single\/accuracy&quot;, accuracy)\n<\/code><\/pre>\n<p>Is there a better solution for this that integrates my desired functionality of being able to group and filter by values like BDP?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646154863157,
        "Question_score":2,
        "Question_tags":"pytorch|metrics|pytorch-lightning|wandb",
        "Question_view_count":315,
        "Owner_creation_time":1609516642837,
        "Owner_last_access_time":1664053489570,
        "Owner_location":null,
        "Owner_reputation":2789,
        "Owner_up_votes":102,
        "Owner_down_votes":1,
        "Owner_views":304,
        "Question_last_edit_time":1646156240832,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71312243",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72555212,
        "Question_title":"wandb pytorch: top1 accuracy per class",
        "Question_body":"<p>I have 5 classes in validation set and i want to draw a graph based on top1 results per class in validation loop using wandb . I have tried a single accuracy graph based on the average of 5 classes and it works fine but i want to do a separate way like top1 accuracy for each class. I am unable to achieve, are there any way to achieve it?<\/p>\n<p><strong>Validation Loader<\/strong><\/p>\n<pre><code> val_loaders = []\n    for nuisance in val_nuisances:\n        val_loaders.append((nuisance, torch.utils.data.DataLoader(\n            datasets.ImageFolder(os.path.join(valdir, nuisance), transforms.Compose([\n                transforms.Resize(256),\n                transforms.CenterCrop(224),\n                transforms.ToTensor(),\n                normalize,\n            ])),\n            batch_size=args.batch_size, shuffle=False,\n            num_workers=args.workers, pin_memory=True,\n        )))\n\n\nval_nuisances = ['shape', 'pose', 'texture', 'context', 'weather']\n<\/code><\/pre>\n<p><strong>Validation Loop<\/strong><\/p>\n<pre><code>def validate(val_loaders, model, criterion, args):\n    overall_top1 = 0\n    for nuisance, val_loader in val_loaders:\n        batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)\n        losses = AverageMeter('Loss', ':.4e', Summary.NONE)\n        top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)\n        top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)\n        progress = ProgressMeter(\n            len(val_loader),\n            [batch_time, losses, top1, top5],\n            prefix=f'Test {nuisance}: ')\n\n        # switch to evaluate mode\n        model.eval()\n\n        with torch.no_grad():\n            end = time.time()\n            for i, (images, target) in enumerate(val_loader):\n                if args.gpu is not None:\n                    images = images.cuda(args.gpu, non_blocking=True)\n                if torch.cuda.is_available():\n                    target = target.cuda(args.gpu, non_blocking=True)\n\n                # compute output\n                output = model(images)\n                loss = criterion(output, target)\n\n                # measure accuracy and record loss\n                acc1, acc5 = accuracy(output, target, topk=(1, 5))\n                losses.update(loss.item(), images.size(0))\n                top1.update(acc1[0], images.size(0))\n                top5.update(acc5[0], images.size(0))\n\n                # measure elapsed time\n                batch_time.update(time.time() - end)\n                end = time.time()\n\n                if i % args.print_freq == 0:\n                    progress.display(i)\n\n            progress.display_summary()\n        overall_top1 += top1.avg\n    overall_top1 \/= len(val_loaders)\n    return top1.avg\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654753323610,
        "Question_score":0,
        "Question_tags":"pytorch|wandb",
        "Question_view_count":71,
        "Owner_creation_time":1420474938783,
        "Owner_last_access_time":1663917040323,
        "Owner_location":"Seoul, South Korea",
        "Owner_reputation":2201,
        "Owner_up_votes":186,
        "Owner_down_votes":3,
        "Owner_views":556,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72555212",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73109071,
        "Question_title":"How do I log a confusion matrix into Wanddb?",
        "Question_body":"<p>I'm using pytorch lightning, and at the end of each epoch, I create a confusion matrix from torchmetrics.ConfusionMatrix (see code below). I would like to log this into Wandb, but the Wandb confusion matrix logger only accepts y_targets and y_predictions. Does anyone know how to extract the updated confusion matrix y_targets and y_predictions from a confusion matrix, or alternatively give Wandb my updated confusion matrix in a way that it can be processed into eg a heatmap within wandb?<\/p>\n<pre><code>class ClassificationTask(pl.LightningModule):\n    def __init__(self, model, lr=1e-4, augmentor=augmentor):\n        super().__init__()\n        self.model = model\n        self.lr = lr\n        self.save_hyperparameters() #not being used at the moment, good to have ther in the future\n        self.augmentor=augmentor\n        \n        self.matrix = torchmetrics.ConfusionMatrix(num_classes=9)\n        \n        self.y_trues=[]\n        self.y_preds=[]\n        \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x=self.augmentor(x)#.to('cuda')\n        y_pred = self.model(x)\n        loss = F.cross_entropy(y_pred, y,)  #weights=class_weights_tensor\n        \n\n        acc = accuracy(y_pred, y)\n        metrics = {&quot;train_acc&quot;: acc, &quot;train_loss&quot;: loss}\n        self.log_dict(metrics)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        loss, acc = self._shared_eval_step(batch, batch_idx)\n        metrics = {&quot;val_acc&quot;: acc, &quot;val_loss&quot;: loss, }\n        self.log_dict(metrics)\n        return metrics\n    \n    def _shared_eval_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        acc = accuracy(y_hat, y)\n        self.matrix.update(y_hat,y)\n        return loss, acc\n    \n    def validation_epoch_end(self, outputs):\n        confusion_matrix = self.matrix.compute()\n        wandb.log({&quot;my_conf_mat_id&quot; : confusion_matrix})\n        \n    def configure_optimizers(self):\n        return torch.optim.Adam((self.model.parameters()), lr=self.lr)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658751297303,
        "Question_score":2,
        "Question_tags":"pytorch|confusion-matrix|pytorch-lightning|wandb",
        "Question_view_count":109,
        "Owner_creation_time":1626958179237,
        "Owner_last_access_time":1663922047490,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73109071",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":68903269,
        "Question_title":"logging learning rate schedule in keras via weights and biases",
        "Question_body":"<p>I am training a keras model and using a custom learning rate scheduler for the optimizer (of type tf.keras.optimizers.schedules.LearningRateSchedule), and i want to log the learning rate change via the weights&amp;biases framework.\ni couldn't find how to pass it to the WandbCallback object or log it in any way<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1629789446037,
        "Question_score":5,
        "Question_tags":"python|tensorflow|keras|deep-learning|wandb",
        "Question_view_count":968,
        "Owner_creation_time":1541621385633,
        "Owner_last_access_time":1644833613247,
        "Owner_location":"Israel",
        "Owner_reputation":59,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1629798930663,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68903269",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73244442,
        "Question_title":"HuggingFace Trainer() cannot report to wandb",
        "Question_body":"<p>I am trying to set trainer with arguments <code>report_to<\/code> to <code>wandb<\/code>, refer to <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/huggingface#getting-started-track-experiments\" rel=\"nofollow noreferrer\">this docs<\/a>\nwith config:<\/p>\n<pre><code>training_args = TrainingArguments(\n    output_dir=&quot;test_trainer&quot;,\n    evaluation_strategy=&quot;steps&quot;,\n    learning_rate=config.learning_rate,\n    num_train_epochs=config.epochs,\n    weight_decay=config.weight_decay,\n    logging_dir=config.logging_dir,\n    report_to=&quot;wandb&quot;,\n    save_total_limit=1,\n    per_device_train_batch_size=config.batch_size,\n    per_device_eval_batch_size=config.batch_size,\n    fp16=True,\n    load_best_model_at_end=True,\n    seed=42\n)\n<\/code><\/pre>\n<p>yet when I set trainer with:<\/p>\n<pre><code>trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics\n)\n<\/code><\/pre>\n<p>it shows:<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-68-b009351ab52d&gt; in &lt;module&gt;\n      4     train_dataset=train_dataset,\n      5     eval_dataset=eval_dataset,\n----&gt; 6     compute_metrics=compute_metrics\n      7 )\n\n~\/.virtualenvs\/transformers_lab\/lib\/python3.7\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\n    286                 &quot;You should subclass `Trainer` and override the `create_optimizer_and_scheduler` method.&quot;\n    287             )\n--&gt; 288         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\n    289         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\n    290         self.callback_handler = CallbackHandler(\n\n~\/.virtualenvs\/transformers_lab\/lib\/python3.7\/site-packages\/transformers\/integrations.py in get_reporting_integration_callbacks(report_to)\n    794         if integration not in INTEGRATION_TO_CALLBACK:\n    795             raise ValueError(\n--&gt; 796                 f&quot;{integration} is not supported, only {', '.join(INTEGRATION_TO_CALLBACK.keys())} are supported.&quot;\n    797             )\n    798     return [INTEGRATION_TO_CALLBACK[integration] for integration in report_to]\n\nValueError: w is not supported, only azure_ml, comet_ml, mlflow, tensorboard, wandb are supported.\n<\/code><\/pre>\n<p>Have anyone got same error before?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659671305703,
        "Question_score":0,
        "Question_tags":"python|huggingface-transformers|wandb",
        "Question_view_count":47,
        "Owner_creation_time":1587186468850,
        "Owner_last_access_time":1663918152847,
        "Owner_location":"Taipei, Taiwan R.O.C",
        "Owner_reputation":163,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":1660033763876,
        "Answer_body":"<p>Although the documentation states that the <code>report_to<\/code> parameter can receive both <code>List[str]<\/code> or <code>str<\/code> I have always used a list with 1! element for this purpose.<\/p>\n<p>Therefore, even if you report only to wandb, the solution to your problem is to replace:<\/p>\n<pre><code> report_to = 'wandb'\n<\/code><\/pre>\n<p>with<\/p>\n<pre><code>report_to = [&quot;wandb&quot;]\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1659781694627,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73244442",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":66606294,
        "Question_title":"PyTorch Lightning wants create a folder on import due to usage of wandb, which raises error on AWS Lambda",
        "Question_body":"<p>So I want to build a Docker image with PyTorch Lightning that can be used with AWS lambda. However, when the function is invoked it raises an OS Error, that claims it uses a Read-only file system and wandb.py wants to write something.<\/p>\n<p>I tried these things:<\/p>\n<ol>\n<li>Overwrite the wandb.py file of pytroch lightning, that it does not init wandb --&gt; Raises error<\/li>\n<li>Execute a python script in Dockerfile, that the files are created on docker build and exist, when invoking the lambda function --&gt; Same OS error<\/li>\n<\/ol>\n<p>Does someone know a way to skip the wandb.py?<\/p>\n<p>This is the error message:<\/p>\n<pre><code>START RequestId: ddae284d-4f32-4dc6-8160-d1fa62ba9772 Version: $LATEST\nOpenBLAS WARNING - could not determine the L2 cache size on this system, assuming 256k\n[ERROR] OSError: [Errno 30] Read-only file system: '\/home\/sbx_user1051'\nTraceback (most recent call last):\n  File &quot;\/var\/lang\/lib\/python3.8\/imp.py&quot;, line 234, in load_module\n    return load_source(name, filename, file)\n  File &quot;\/var\/lang\/lib\/python3.8\/imp.py&quot;, line 171, in load_source\n    module = _load(spec)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 702, in _load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 671, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 783, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/var\/task\/inference.py&quot;, line 5, in &lt;module&gt;\n    import pytorch_lightning as pl\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/__init__.py&quot;, line 63, in &lt;module&gt;\n    from pytorch_lightning.callbacks import Callback\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/callbacks\/__init__.py&quot;, line 25, in &lt;module&gt;\n    from pytorch_lightning.callbacks.swa import StochasticWeightAveraging\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/callbacks\/swa.py&quot;, line 26, in &lt;module&gt;\n    from pytorch_lightning.trainer.optimizers import _get_default_scheduler_config\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/__init__.py&quot;, line 18, in &lt;module&gt;\n    from pytorch_lightning.trainer.trainer import Trainer\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py&quot;, line 30, in &lt;module&gt;\n    from pytorch_lightning.loggers import LightningLoggerBase\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/__init__.py&quot;, line 31, in &lt;module&gt;\n    from pytorch_lightning.loggers.wandb import _WANDB_AVAILABLE, WandbLogger  # noqa: F401\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py&quot;, line 34, in &lt;module&gt;\n    import wandb\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/wandb\/__init__.py&quot;, line 131, in &lt;module&gt;\n    api = InternalApi()\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/wandb\/apis\/internal.py&quot;, line 17, in __init__\n    self.api = InternalApi(*args, **kwargs)\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py&quot;, line 73, in __init__\n    self._settings = Settings(\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/wandb\/old\/settings.py&quot;, line 25, in __init__\n    self._global_settings.read([Settings._global_path()])\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/wandb\/old\/settings.py&quot;, line 105, in _global_path\n    util.mkdir_exists_ok(config_dir)\n  File &quot;\/var\/lang\/lib\/python3.8\/site-packages\/wandb\/util.py&quot;, line 687, in mkdir_exists_ok\n    os.makedirs(path)\n  File &quot;\/var\/lang\/lib\/python3.8\/os.py&quot;, line 213, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File &quot;\/var\/lang\/lib\/python3.8\/os.py&quot;, line 213, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File &quot;\/var\/lang\/lib\/python3.8\/os.py&quot;, line 223, in makedirs\n    mkdir(name, mode)\nEND RequestId: ddae284d-4f32-4dc6-8160-d1fa62ba9772\nREPORT RequestId: ddae284d-4f32-4dc6-8160-d1fa62ba9772  Duration: 27000.33 ms   Billed Duration: 27001 ms   Memory Size: 10240 MB   Max Memory Used: 241 MB \nUnknown application error occurred\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615578448627,
        "Question_score":3,
        "Question_tags":"amazon-web-services|aws-lambda|pytorch|pytorch-lightning|wandb",
        "Question_view_count":202,
        "Owner_creation_time":1510177189697,
        "Owner_last_access_time":1664037019800,
        "Owner_location":"Dortmund, Deutschland",
        "Owner_reputation":1194,
        "Owner_up_votes":149,
        "Owner_down_votes":11,
        "Owner_views":119,
        "Question_last_edit_time":1661850419560,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66606294",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69425994,
        "Question_title":"is there any way to scale axis of plots in wandb?",
        "Question_body":"<p>I am working with weight and bias(wandb).<br \/>\nHowever, it logs by step. And that makes plot disturbing when comparing runs.<br \/>\nFor example, I have a run A and run B(assume that they run with same dataset).<br \/>\nrun A: 30epochs, 4 batch, 200step\/epoch<br \/>\nrun B: 30epochs, 8 batch, 100step\/epoch<\/p>\n<p>then, the plot of run A gets longer(double, in this case) in axis x when it shows with run B.<\/p>\n<p>How can I scale x axis depend to runs AFTER training?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633272788153,
        "Question_score":1,
        "Question_tags":"wandb",
        "Question_view_count":604,
        "Owner_creation_time":1603378831587,
        "Owner_last_access_time":1663547314893,
        "Owner_location":null,
        "Owner_reputation":161,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can change the x-axis used via the chart settings by clicking on the pencil icon and then selecting a different x-axis. E.g. in your case you could select &quot;epoch&quot; instead of &quot;steps&quot;. Just make sure to log &quot;epoch&quot; to your charts, something like:<\/p>\n<pre><code>steps_per_epoch = n_samples \/ batch_size\nepoch = current_step \/ steps_per_epoch\nwandb.log({&quot;epoch&quot;:epoch, ...})\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1633345375676,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69425994",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69147788,
        "Question_title":"Weights & Biases with Transformers and PyTorch?",
        "Question_body":"<p>I'm training an NLP model at work (e-commerce SEO) applying a <code>BERT<\/code> variation for portuguese language (<code>BERTimbau<\/code>) through <code>Transformers<\/code> by Hugging Face.<\/p>\n<p>I didn't used the <code>Trainer<\/code> from Transformers API. I used <code>PyTorch<\/code> to set all parameters through <code>DataLoader.utils<\/code> and <code>adamW<\/code>. I trained my model using <code>run_glue.py<\/code>.<\/p>\n<p><strong>I'm training with a VM on GCP using Jupyterlab<\/strong>. I know that I can use Weights &amp; Biases both for PyTorch and Transformers. But I don't know exactly how to set it using <code>run_glue.py<\/code>. It's my first time using Weights &amp; Biases.<\/p>\n<p><em>After preprocessing and splitting train and test through Sklearn, my code is as it follows<\/em>:<\/p>\n<pre><code>from transformers import BertTokenizer\nimport torch\n#import torchvision\nfrom torch.utils.data import Dataset, TensorDataset\nimport collections.abc as container_abcs\n\n# To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n# Constructs a BERT tokenizer. Based on WordPiece. \n# The tokenization must be performed by the tokenizer included with BERT\ntokenizer = BertTokenizer.from_pretrained('neuralmind\/bert-large-portuguese-cased', \n                                          do_lower_case=True)\n\n# Tokenize all of the sentences and map the tokens to thier word IDs. To convert all the titles from text into encoded form.\n# We will use padding and truncation because the training routine expects all tensors within a batch to have the same dimensions.\nencoded_data_train = tokenizer.batch_encode_plus(\n    df[df.data_type=='train'].text.values, \n    add_special_tokens=True,                            # Add '[CLS]' and '[SEP]'. Sequences encoded with special tokens relative to their model\n    return_attention_mask=True,                         # Return mask according to specific tokenizer defined by max_length\n    pad_to_max_length=True,                             # Pad &amp; truncate all sentences. Pad all titles to certain maximum length                  \n    max_length=128,                                     # Do not need to set max_length=256\n    return_tensors='pt'                                 # Set to use PyTorch tensors\n)\n\nencoded_data_val = tokenizer.batch_encode_plus(\n    df[df.data_type=='val'].text.values, \n    add_special_tokens=True, \n    return_attention_mask=True, \n    pad_to_max_length=True, \n    max_length=128, \n    return_tensors='pt'\n)\n\n# Split the data into input_ids, attention_masks and labels. \n# Converting the input data to the tensor , which can be feeded to the model\ninput_ids_train = encoded_data_train['input_ids']                    # Add the encoded sentence to the list.\nattention_masks_train = encoded_data_train['attention_mask']         # And its attention mask (simply differentiates padding from non-padding).\nlabels_train = torch.tensor(df[df.data_type=='train'].label.values)\n\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nlabels_val = torch.tensor(df[df.data_type=='val'].label.values)\n\n# Create training data and validation data\ndataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ndataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n<\/code><\/pre>\n<pre><code>from transformers import BertForSequenceClassification\nmodel = BertForSequenceClassification.from_pretrained(&quot;neuralmind\/bert-large-portuguese-cased&quot;,  # Select your pretrained Model\n                                                      num_labels=len(label_dict),                # Labels tp predict\n                                                      output_attentions=False,                   # Whether the model returns attentions weights. We don\u2019t really care about output_attentions. \n                                                      output_hidden_states=False)                # Whether the model returns all hidden-states. We also don\u2019t need output_hidden_states\n<\/code><\/pre>\n<pre><code>from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\nbatch_size = 32                                                              # Set your batch size according to your GPU memory      \n\ndataloader_train = DataLoader(dataset_train                                  # Use DataLoader to Optimize your model\n                              ,sampler=RandomSampler(dataset_train)          # Random Sampler from your dataset\n                              ,batch_size=batch_size)                        # If your batch_size is too high you will get a warning when you run the model \n                              #,num_workers=4                                # Number of cores\n                              #,pin_memory=True)                             # Use GPU to send your batch \n\ndataloader_validation = DataLoader(dataset_val \n                                   ,sampler=SequentialSampler(dataset_val)   # For validation the order doesn't matter. Sequential Sampler consumes less GPU.\n                                   ,batch_size=batch_size) \n                                   #,num_workers=4\n                                   #,pin_memory=True)\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n# hyperparameters\n# To construct an optimizer, we have to give it an iterable containing the parameters to optimize. \n# Then, we can specify optimizer-specific options such as the learning rate, epsilon, etc.\noptimizer = AdamW(model.parameters(),  # AdamW is a class from the huggingface library (as opposed to pytorch) \n                  lr=2e-5,             # args.learning_rate - default is 5e-5\n                  eps=1e-8)            # args.adam_epsilon  - default is 1e-8\n\n\n# Number of training epochs. The BERT authors recommend between 2 and 4. \nepochs = 2\n\n# Create the learning rate scheduler that decreases linearly from the initial learning rate set in the optimizer to 0, \n# after a warmup period during which it increases linearly from 0 to the initial learning rate set in the optimizer.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=0,                               # Default value in run_glue.py                           \n                                            num_training_steps=len(dataloader_train)*epochs)  # Total number of training steps is [number of batches] x [number of epochs]. \n                                                                                              # Note that this is not the same as the number of training samples).\n<\/code><\/pre>\n<pre><code>from sklearn.metrics import f1_score\n\ndef f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average='weighted')\n\ndef accuracy_per_class(preds, labels):\n    label_dict_inverse = {v: k for k, v in label_dict.items()}\n    \n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n\n    for label in np.unique(labels_flat):\n        y_preds = preds_flat[labels_flat==label]\n        y_true = labels_flat[labels_flat==label]\n        print(f'Class: {label_dict_inverse[label]}')\n        print(f'Accuracy: {len(y_preds[y_preds==label])}\/{len(y_true)}\\n')\n\n<\/code><\/pre>\n<p>And here follows <code>run_glue.py<\/code>:<\/p>\n<pre><code>import random\nfrom tqdm import tqdm\nimport torch\nimport numpy as np\n# from tqdm.notebook import trange, tqdm\n\n'''\nThis training code is based on the 'run_glue.py' script here:\nhttps:\/\/github.com\/huggingface\/transformers\/blob\/5bfcd0485ece086ebcbed2d008813037968a9e58\/examples\/run_glue.py#L128\n'''\n\n# Just right before the actual usage select your hardware\ndevice = torch.device('cuda') # or cpu\nmodel = model.to(device)      # send your model to your hardware\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 17\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# We'll store a number of quantities such as training and validation loss, validation and timings.\ndef evaluate(dataloader_val):\n    '''\n    Put the model in evaluation mode--the dropout layers behave differently\n    during evaluation.\n    '''\n    \n    model.eval()\n    \n    loss_val_total = 0 # Tracking variables \n\n    predictions, true_vals = [], []\n    \n    for batch in dataloader_val:\n        '''\n         Unpack this training batch from our dataloader.         \n         As we unpack the batch, we'll also copy each tensor to the GPU using the \n         `to` method.\n        \n         `batch` contains three pytorch tensors:\n           [0]: input ids \n           [1]: attention masks\n           [2]: labels \n        '''\n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }\n        \n        '''\n        Tell pytorch not to bother with constructing the compute graph during\n        the forward pass, since this is only needed for backprop (training).\n        '''\n        with torch.no_grad():        \n            outputs = model(**inputs)\n            \n        '''\n        Perform a forward pass (evaluate the model on this training batch).\n        The documentation for this `model` function is here: \n        https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers.BertForSequenceClassification\n        This will return the loss (rather than the model output) \n        because we have provided the `labels`.\n        It returns different numbers of parameters depending on what arguments\n        arge given and what flags are set. For our useage here, it returns\n        the loss (because we provided labels) and the &quot;logits&quot;--the model\n        outputs prior to activation.\n        '''\n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item() # Accumulate the validation loss.\n\n                \n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n    \n    loss_val_avg = loss_val_total\/len(dataloader_val) # Calculate the average loss over all of the batches.\n\n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n            \n    return loss_val_avg, predictions, true_vals\n\n    # ========================================\n    #               Training\n    # ========================================\n\n# For each epoch...\nfor epoch in tqdm(range(1, epochs+1)):    \n    '''\n    Put the model into training mode. Don't be mislead--the call to \n    `train` just changes the *mode*, it doesn't *perform* the training.\n    `dropout` and `batchnorm` layers behave differently during training\n    vs. test (source: https:\/\/stackoverflow.com\/questions\/51433378\/what-does-model-train-do-in-pytorch)\n    '''\n    \n    model.train()        # Put the model into training mode.\n\n    \n    loss_train_total = 0 # Reset the total loss for this epoch.\n\n    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n    for batch in progress_bar:\n\n        model.zero_grad()        \n        '''\n        Always clear any previously calculated gradients before performing a\n        backward pass. PyTorch doesn't do this automatically because \n        accumulating the gradients is &quot;convenient while training RNNs&quot;. \n        (source: https:\/\/stackoverflow.com\/questions\/48001598\/why-do-we-need-to-call-zero-grad-in-pytorch)\n        '''\n        \n        batch = tuple(b.to(device) for b in batch)\n        '''\n        Unpack this training batch from our dataloader. \n        \n         As we unpack the batch, we'll also copy each tensor to the GPU using the \n         `to` method.\n        \n         `batch` contains three pytorch tensors:\n           [0]: input ids \n           [1]: attention masks\n           [2]: labels\n         '''\n        \n        inputs = {'input_ids':      batch[0], #.to(device)\n                  'attention_mask': batch[1], #.to(device)\n                  'labels':         batch[2], #.to(device)\n                 }       \n\n        outputs = model(**inputs)\n        \n\n        loss = outputs[0] # The call to `model` always returns a tuple, so we need to pull the loss value out of the tuple.\n        loss_train_total += loss.item()         # Accumulate the training loss over all of the batches so that we can\n                                                # calculate the average loss at the end. `loss` is a Tensor containing a\n                                                # single value; the `.item()` function just returns the Python value \n                                                # from the tensor.\n        loss.backward() # Perform a backward pass to calculate the gradients.\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    # Clip the norm of the gradients to 1.0.\n                                                                   # This is to help prevent the &quot;exploding gradients&quot; problem.\n                                                                   # modified based on their gradients, the learning rate, etc.\n                \n        optimizer.step()        # Update parameters and take a step using the computed gradient.\n                                # The optimizer dictates the &quot;update rule&quot;--how the parameters are\n                                # modified based on their gradients, the learning rate, etc.\n                \n        scheduler.step()        # Update the learning rate.\n\n        \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()\/len(batch))})\n         \n        \n    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model') # Save Model\n        \n    tqdm.write(f'\\nEpoch {epoch}') # Show running epoch\n    \n    loss_train_avg = loss_train_total\/len(dataloader_train) # Calculate the average loss over all of the batches.\n           \n    tqdm.write(f'Training loss: {loss_train_avg}') # Show loss average\n    \n        # ========================================\n        #               Validation\n        # ========================================\n        # After the completion of each training epoch, measure our performance on\n        # our validation set.\n    \n    # Record all statistics from this epoch.\n    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n    val_f1 = f1_score_func(predictions, true_vals)\n    tqdm.write(f'Validation loss: {val_loss}')\n    tqdm.write(f'F1 Score (Weighted): {val_f1}')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1631409889517,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|pytorch|huggingface-transformers|wandb",
        "Question_view_count":427,
        "Owner_creation_time":1618668550253,
        "Owner_last_access_time":1663440294737,
        "Owner_location":"S\u00e3o Paulo - Brazil",
        "Owner_reputation":113,
        "Owner_up_votes":99,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":1661850382607,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69147788",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":67202711,
        "Question_title":"How to get multiple lines exported to wandb",
        "Question_body":"<p>I am using the library weights and  biases. My model outputs a curve (a time series). I'd like to see how this curve changes throughout training. So, I'd need some kind of slider where I can select epoch and it shows me the curve for that epoch. It could be something very similar to what it's done with histograms (it shows an image of the histograms across epochs and when you hover it display the histogram corresponding to that epoch). Is there a way to do this or something similar using <code>wandb<\/code>?<\/p>\n<p>Currently my code looks like this:<\/p>\n<pre><code>for epoch in range(epochs):\n   output = model(input)\n   #output is shape (37,40) (lenght 40 and I have 37 samples)\n   #it's enough to plot the first sample\n   xs = torch.arange(40).unsqueeze(dim=1)\n   ys = output[0,:].unsqueeze(dim=1)\n   wandb.log({&quot;line&quot;: wandb.plot.line_series(xs=xs, ys=ys,title=&quot;Out&quot;)}, step=epoch)\n<\/code><\/pre>\n<p>I'd appreciate any help! Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619035214083,
        "Question_score":1,
        "Question_tags":"python|pytorch|wandb",
        "Question_view_count":840,
        "Owner_creation_time":1577734207070,
        "Owner_last_access_time":1663602351910,
        "Owner_location":null,
        "Owner_reputation":422,
        "Owner_up_votes":72,
        "Owner_down_votes":3,
        "Owner_views":66,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use <code>wandb.log()<\/code> with matplotlib. Create your plot using matplotlib:<\/p>\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(0, 50)\nfor i in range(1, 4):\n    fig, ax = plt.subplots()\n    y = x ** i\n    ax.plot(x, y)\n    wandb.log({'chart': ax})\n<\/code><\/pre>\n<p>Then when you look on your wandb dashboard for the run, you will see the plot rendered as plotly plot. Click the gear in the upper left hand corner to see a slider that lets you slide over training steps and see the plot at each step.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620426900467,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67202711",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71990430,
        "Question_title":"Wandb throws Permission denied error although I am logged in",
        "Question_body":"<p>I am using the cleanrl library, in particular the script <a href=\"https:\/\/github.com\/vwxyzjn\/cleanrl\/blob\/master\/cleanrl\/dqn_atari.py\" rel=\"nofollow noreferrer\">dqn_atari.py<\/a>  dqn_atari.py where  I followed the  <a href=\"https:\/\/docs.cleanrl.dev\/advanced\/resume-training\/\" rel=\"nofollow noreferrer\">instructions<\/a> in order to save and load the target and Q-network.<\/p>\n<p>I am running it locally within a conda environment.<\/p>\n<p>I haven't loaded something before, so the error may be due to my wandb configuration. The error is &quot;wandb: ERROR Permission denied to access wandb_entity\/wandb_project_name\/project_id&quot; and appears on line:<\/p>\n<pre><code>model = run.file(&quot;agent.pt&quot;)\n<\/code><\/pre>\n<p>The full  output is:<\/p>\n<pre><code>wandb: Currently logged in as: elena (use `wandb login --relogin` to force relogin)\n    wandb: Tracking run with wandb version 0.12.15\nwandb: Run data is saved locally in \/home\/elena\/workspace\/playground\/cleanrl\/wandb\/run-20220424_180429-2moec0qp\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Resuming run BreakoutNoFrameskip-v4__dqn-save__1__1650816268\nwandb: \u2b50 View project at https:\/\/wandb.ai\/elena\/test\nwandb:  View run at https:\/\/wandb.ai\/elena\/test\/runs\/2moec0qp\nA.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n[Powered by Stella]\n\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/stable_baselines3\/common\/buffers.py:219: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 28.24GB &gt; 8.35GB\n  warnings.warn(\nwandb: ERROR Permission denied to access elena\/test\/2moec0qp\nTraceback (most recent call last):\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py&quot;, line 102, in __call__\n    result = self._call_fn(*args, **kwargs)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/apis\/public.py&quot;, line 2428, in download\n    util.download_file_from_url(path, self.url, Api().api_key)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/util.py&quot;, line 1197, in download_file_from_url\n    response.raise_for_status()\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/requests\/models.py&quot;, line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https:\/\/api.wandb.ai\/files\/elena\/test\/2moec0qp\/td_network.pt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py&quot;, line 22, in wrapper\n    return func(*args, **kwargs)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py&quot;, line 159, in wrapped_fn\n    return retrier(*args, **kargs)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py&quot;, line 118, in __call__\n    if not check_retry_fn(e):\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/util.py&quot;, line 877, in no_retry_auth\n    raise CommError(f&quot;Permission denied to access {wandb.run.path}&quot;)\nwandb.errors.CommError: Permission denied to access elena\/test\/2moec0qp\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;cleanrl\/dqn_atari_save.py&quot;, line 184, in &lt;module&gt;\n    model.download(f&quot;models\/{args.exp_name}\/&quot;)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py&quot;, line 58, in wrapper\n    raise CommError(message, err).with_traceback(sys.exc_info()[2])\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py&quot;, line 22, in wrapper\n    return func(*args, **kwargs)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py&quot;, line 159, in wrapped_fn\n    return retrier(*args, **kargs)\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py&quot;, line 118, in __call__\n    if not check_retry_fn(e):\n  File &quot;\/home\/elena\/anaconda3\/envs\/cleanrl\/lib\/python3.8\/site-packages\/wandb\/util.py&quot;, line 877, in no_retry_auth\n    raise CommError(f&quot;Permission denied to access {wandb.run.path}&quot;)\nwandb.errors.CommError: Permission denied to access elena\/test\/2moec0qp\nwandb: Waiting for W&amp;B process to finish... (failed 1). Press Control-C to abort syncing.\nwandb:                                                                                \nwandb: \nwandb: Run history:\nwandb: global_step \u2581\nwandb: \nwandb: Run summary:\nwandb: charts\/episodic_return 0\nwandb:         charts\/epsilon 0.01\nwandb:          charts\/update 1969\nwandb:            global_step 0\nwandb: \nwandb: Synced BreakoutNoFrameskip-v4__dqn-save__1__1650816268: https:\/\/wandb.ai\/elena\/test\/runs\/2moec0qp\nwandb: Synced 3 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\nwandb: Find logs at: .\/wandb\/run-20220424_180429-2moec0qp\/logs\n<\/code><\/pre>\n<p>So as you can see I am logged in and I can see the files under &quot;https:\/\/wandb.ai\/elena\/test\/runs\/2moec0qp?workspace=user-elena&quot;. Something that drew my attention was &quot;requests.exceptions.HTTPError: 404 Client Error: Not Found for url: <a href=\"https:\/\/api.wandb.ai\/files\/elena\/test\/2moec0qp\/agent.pt%22\" rel=\"nofollow noreferrer\">https:\/\/api.wandb.ai\/files\/elena\/test\/2moec0qp\/agent.pt&quot;<\/a>. This path indeed looks different from the https path, but maybe this is not the issue?\nAny ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650818668313,
        "Question_score":2,
        "Question_tags":"python|wandb",
        "Question_view_count":596,
        "Owner_creation_time":1418505278743,
        "Owner_last_access_time":1663932736703,
        "Owner_location":null,
        "Owner_reputation":319,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71990430",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71476873,
        "Question_title":"How to log additional single variable with wandb and huggingface transformers",
        "Question_body":"<p>I am using Huggingface's Transformers Trainer object and I really like the support it has for wandb.<\/p>\n<p>For my use case, I have subclassed the Trainer and, in addition to the values that are logged by default, I would like to keep track of one additional variable that gets updated at each training step.<\/p>\n<p>What is the easiest way to add tracking for a single variable with wandb?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_time":1647316549707,
        "Question_score":1,
        "Question_tags":"python|huggingface-transformers|wandb",
        "Question_view_count":169,
        "Owner_creation_time":1484868229953,
        "Owner_last_access_time":1654795122843,
        "Owner_location":null,
        "Owner_reputation":507,
        "Owner_up_votes":61,
        "Owner_down_votes":0,
        "Owner_views":50,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71476873",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":68087139,
        "Question_title":"trainer.train() in Kaggle: StdinNotImplementedError: getpass was called, but this frontend does not support input requests",
        "Question_body":"<p>When saving a version in Kaggle, I get <strong>StdinNotImplementedError: getpass was called, but this frontend does not support input requests<\/strong> whenever I use the Transformers.Trainer class. The general code I use:<\/p>\n<pre><code>from transformers import Trainer, TrainingArguments\ntraining_args = TrainingArguments(params)\ntrainer = Trainer(params)\ntrainer.train()\n<\/code><\/pre>\n<p>And the specific cell I am running now:<\/p>\n<pre><code>from transformers import Trainer, TrainingArguments,EarlyStoppingCallback\n\nearly_stopping = EarlyStoppingCallback()\n\ntraining_args = TrainingArguments(\n    output_dir=OUT_FINETUNED_MODEL_PATH,          \n    num_train_epochs=20,              \n    per_device_train_batch_size=16,  \n    per_device_eval_batch_size=16,   \n    warmup_steps=0,                \n    weight_decay=0.01,               \n    logging_dir='.\/logs',            \n    logging_steps=100,\n    evaluation_strategy=&quot;steps&quot;,\n    eval_steps=100,\n    load_best_model_at_end=True,\n    metric_for_best_model=&quot;eval_loss&quot;,\n    greater_is_better=False\n    \n)\n\ntrainer = Trainer(\n    model=model,                         \n    args=training_args,                  \n    train_dataset=train_dataset,         \n    eval_dataset=val_dataset,             \n    callbacks=[early_stopping]\n\n)\n\ntrainer.train()\n<\/code><\/pre>\n<p>When trainer.train() is called, I get the error below, which I do not get if I train with native PyTorch. I understood that the error arises since I am asked to input a password, but no password is asked when using native PyTorch code, nor when using the same code with trainer.train() on Google Colab.\nAny solution would be ok, like:<\/p>\n<ol>\n<li>Avoid being asked the password.<\/li>\n<li>Enable input requests when saving a notebook on Kaggle. After that, if I understood correctly, I would need to go to <a href=\"https:\/\/wandb.ai\/authorize\" rel=\"nofollow noreferrer\">https:\/\/wandb.ai\/authorize<\/a> (after having created an account) and copy the generated key to console. However, I do not understand why wandb should be necessary since I never explicitly used it so far.<\/li>\n<\/ol>\n<pre><code>wandb: You can find your API key in your browser here: https:\/\/wandb.ai\/authorize\nTraceback (most recent call last):\n  File &quot;\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py&quot;, line 741, in init\n    wi.setup(kwargs)\n  File &quot;\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py&quot;, line 155, in setup\n    wandb_login._login(anonymous=anonymous, force=force, _disable_warning=True)\n  File &quot;\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_login.py&quot;, line 210, in _login\n    wlogin.prompt_api_key()\n  File &quot;\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_login.py&quot;, line 144, in prompt_api_key\n    no_create=self._settings.force,\n  File &quot;\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/apikey.py&quot;, line 135, in prompt_api_key\n    key = input_callback(api_ask).strip()\n  File &quot;\/opt\/conda\/lib\/python3.7\/site-packages\/ipykernel\/kernelbase.py&quot;, line 825, in getpass\n    &quot;getpass was called, but this frontend does not support input requests.&quot;\nIPython.core.error.StdinNotImplementedError: getpass was called, but this frontend does not support input requests.\nwandb: ERROR Abnormal program exit\n---------------------------------------------------------------------------\nStdinNotImplementedError                  Traceback (most recent call last)\n\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\n    740         wi = _WandbInit()\n--&gt; 741         wi.setup(kwargs)\n    742         except_exit = wi.settings._except_exit\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py in setup(self, kwargs)\n    154         if not settings._offline and not settings._noop:\n--&gt; 155             wandb_login._login(anonymous=anonymous, force=force, _disable_warning=True)\n    156 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_login.py in _login(anonymous, key, relogin, host, force, _backend, _silent, _disable_warning)\n    209     if not key:\n--&gt; 210         wlogin.prompt_api_key()\n    211 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_login.py in prompt_api_key(self)\n    143             no_offline=self._settings.force,\n--&gt; 144             no_create=self._settings.force,\n    145         )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/apikey.py in prompt_api_key(settings, api, input_callback, browser_callback, no_offline, no_create, local)\n    134             )\n--&gt; 135             key = input_callback(api_ask).strip()\n    136         write_key(settings, key, api=api)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/ipykernel\/kernelbase.py in getpass(self, prompt, stream)\n    824             raise StdinNotImplementedError(\n--&gt; 825                 &quot;getpass was called, but this frontend does not support input requests.&quot;\n    826             )\n\nStdinNotImplementedError: getpass was called, but this frontend does not support input requests.\n\nThe above exception was the direct cause of the following exception:\n\nException                                 Traceback (most recent call last)\n&lt;ipython-input-82-4d1046ab80b8&gt; in &lt;module&gt;\n     42     )\n     43 \n---&gt; 44     trainer.train()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/transformers\/trainer.py in train(self, resume_from_checkpoint, trial, **kwargs)\n   1067         model.zero_grad()\n   1068 \n-&gt; 1069         self.control = self.callback_handler.on_train_begin(self.args, self.state, self.control)\n   1070 \n   1071         # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/transformers\/trainer_callback.py in on_train_begin(self, args, state, control)\n    338     def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n    339         control.should_training_stop = False\n--&gt; 340         return self.call_event(&quot;on_train_begin&quot;, args, state, control)\n    341 \n    342     def on_train_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/transformers\/trainer_callback.py in call_event(self, event, args, state, control, **kwargs)\n    386                 train_dataloader=self.train_dataloader,\n    387                 eval_dataloader=self.eval_dataloader,\n--&gt; 388                 **kwargs,\n    389             )\n    390             # A Callback can skip the return of `control` if it doesn't change it.\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/transformers\/integrations.py in on_train_begin(self, args, state, control, model, **kwargs)\n    627             self._wandb.finish()\n    628         if not self._initialized:\n--&gt; 629             self.setup(args, state, model, **kwargs)\n    630 \n    631     def on_train_end(self, args, state, control, model=None, tokenizer=None, **kwargs):\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/transformers\/integrations.py in setup(self, args, state, model, **kwargs)\n    604                     project=os.getenv(&quot;WANDB_PROJECT&quot;, &quot;huggingface&quot;),\n    605                     name=run_name,\n--&gt; 606                     **init_args,\n    607                 )\n    608             # add config parameters (run may have been created manually)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\n    779             if except_exit:\n    780                 os._exit(-1)\n--&gt; 781             six.raise_from(Exception(&quot;problem&quot;), error_seen)\n    782     return run\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/six.py in raise_from(value, from_value)\n\nException: problem\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624378186880,
        "Question_score":2,
        "Question_tags":"huggingface-transformers|kaggle|getpass|wandb",
        "Question_view_count":440,
        "Owner_creation_time":1624376028980,
        "Owner_last_access_time":1642694800790,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68087139",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69008133,
        "Question_title":"How to plot confidence intervals for different training samples",
        "Question_body":"<p>I am working on running training with different divisions of a training set. The plots that I get (using wandb) are fine, but not quite informative in my opinion and high in variance.\n<a href=\"https:\/\/i.stack.imgur.com\/kux5o.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kux5o.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Is there a way to plot the mean of the plots, and then confidence intervals around it? Something similar to the picture below. Alternatively, is there a way to plot variance during training?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/veoWc.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/veoWc.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630473410597,
        "Question_score":1,
        "Question_tags":"python|visualization|confidence-interval|variance|wandb",
        "Question_view_count":232,
        "Owner_creation_time":1570797361193,
        "Owner_last_access_time":1652428368017,
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":84,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69008133",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":70637798,
        "Question_title":"wandb: artifact.add_reference() option to add specific (not current) versionId or ETag to stop the need for re-upload to s3?",
        "Question_body":"<p>I feel like this should be possible, but I looked through the wandb SDK code and I can't find an easy\/logical way to do it. It <em>might<\/em> be possible to hack it by modifying the manifest entries at some point later (but maybe before the artifact is logged to wandb as then the manifest and the entries might be locked)? I saw things like this in the SDK code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>version = manifest_entry.extra.get(&quot;versionID&quot;)\netag = manifest_entry.extra.get(&quot;etag&quot;)\n<\/code><\/pre>\n<p>So, I figure we can probably edit those?<\/p>\n<h2>UPDATE<\/h2>\n<p>So, I tried to hack it together with something like this and it works but it feels wrong:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport wandb\nimport boto3\nfrom wandb.util import md5_file\n\nENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nPROJECT = os.environ.get(&quot;WANDB_PROJECT&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\napi = api = wandb.Api(overrides={&quot;entity&quot;: ENTITY, &quot;project&quot;: ENTITY})\nrun = wandb.init(entity=ENTITY, project=PROJECT, job_type=&quot;test upload&quot;)\nfile = &quot;admin2Codes.txt&quot;  # &quot;admin1CodesASCII.txt&quot; # (both already on s3 with a couple versions)\nartifact = wandb.Artifact(&quot;test_data&quot;, type=&quot;dataset&quot;)\n\n# modify one of the local files so it has a new md5hash etc.\nwith open(file, &quot;a&quot;) as f:\n    f.write(&quot;new_line_1\\n&quot;)\n\n# upload local file to s3\nlocal_file_path = file\ns3_url = f&quot;s3:\/\/bucket\/prefix\/{file}&quot;\ns3_url_arr = s3_url.replace(&quot;s3:\/\/&quot;, &quot;&quot;).split(&quot;\/&quot;)\ns3_bucket = s3_url_arr[0]\nkey = &quot;\/&quot;.join(s3_url_arr[1:])\n\ns3_client = boto3.client(&quot;s3&quot;)\nfile_digest = md5_file(local_file_path)\ns3_client.upload_file(\n    local_file_path,\n    s3_bucket,\n    key,\n    # save the md5_digest in metadata,\n    # can be used later to only upload new files to s3,\n    # as AWS doesn't digest the file consistently in the E-tag\n    ExtraArgs={&quot;Metadata&quot;: {&quot;md5_digest&quot;: file_digest}},\n)\nhead_response = s3_client.head_object(Bucket=s3_bucket, Key=key)\nversion_id: str = head_response[&quot;VersionId&quot;]\nprint(version_id)\n\n# upload a link\/ref to this s3 object in wandb:\nartifact.add_reference(s3_dir)\n# at this point we might be able to modify the artifact._manifest.entries and each entry.extra.get(&quot;etag&quot;) etc.?\nprint([(name, entry.extra) for name, entry in artifact._manifest.entries.items()])\n# set these to an older version on s3 that we know we want (rather than latest) - do this via wandb public API:\ndataset_v2 = api.artifact(f&quot;{ENTITY}\/{PROJECT}\/test_data:v2&quot;, type=&quot;dataset&quot;)\n# artifact._manifest.add_entry(dataset_v2.manifest.entries[&quot;admin1CodesASCII.txt&quot;])\nartifact._manifest.entries[&quot;admin1CodesASCII.txt&quot;] = dataset_v2.manifest.entries[\n    &quot;admin1CodesASCII.txt&quot;\n]\n# verify that it did change:\nprint([(name, entry.extra) for name, entry in artifact._manifest.entries.items()])\n\nrun.log_artifact(artifact)  # at this point the manifest is locked I believe?\nartifact.wait()  # wait for upload to finish (blocking - but should be very quick given it is just an s3 link)\nprint(artifact.name)\nrun_id = run.id\nrun.finish()\ncurr_run = api.run(f&quot;{ENTITY}\/{PROJECT}\/{run_id}&quot;)\nused_artifacts = curr_run.used_artifacts()\nlogged_artifacts = curr_run.logged_artifacts()\n<\/code><\/pre>\n<p>Am I on the right track here? I guess the other workaround is to make a copy on s3 (so that older version is the latest again) but I wanted to avoid this as the 1 file that I want to use an old version of is a large NLP model and the only files I want to change are small config.json files etc. (so seems very wasteful to upload all files again).<\/p>\n<p>I was also wondering if when I copy an old version of an object back into the same key in the bucket if that creates a real copy or just like a pointer to the same underlying object. Neither boto3 nor AWS documentation makes that clear - although it seems like it is a proper copy.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641691804600,
        "Question_score":0,
        "Question_tags":"python|amazon-s3|boto3|wandb",
        "Question_view_count":121,
        "Owner_creation_time":1526368885180,
        "Owner_last_access_time":1663954559473,
        "Owner_location":"Munich, Germany",
        "Owner_reputation":3944,
        "Owner_up_votes":574,
        "Owner_down_votes":47,
        "Owner_views":217,
        "Question_last_edit_time":1643119769043,
        "Answer_body":"<p>I think I found the correct way to do it now:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport wandb\nimport boto3\nfrom wandb.util import md5_file\n\nENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nPROJECT = os.environ.get(&quot;WANDB_PROJECT&quot;)\n\n\ndef wandb_update_only_some_files_in_artifact(\n    existing_artifact_name: str,\n    new_s3_file_urls: list[str],\n    entity: str = ENTITY,\n    project: str = PROJECT,\n) -&gt; Artifact:\n    &quot;&quot;&quot;If you want to just update a config.json file for example,\n    but the rest of the artifact can remain the same, then you can\n    use this functions like so:\n    wandb_update_only_some_files_in_artifact(\n        &quot;old_artifact:v3&quot;,\n        [&quot;s3:\/\/bucket\/prefix\/config.json&quot;],\n    )\n    and then all the other files like model.bin will be the same as in v3,\n    even if there was a v4 or v5 in between (as the v3 VersionIds are used)\n\n    Args:\n        existing_artifact_name (str): name with version like &quot;old_artifact:v3&quot;\n        new_s3_file_urls (list[str]): files that should be updated\n        entity (str, optional): wandb entity. Defaults to ENTITY.\n        project (str, optional): wandb project. Defaults to PROJECT.\n\n    Returns:\n        Artifact: the new artifact object\n    &quot;&quot;&quot;\n    api = wandb.Api(overrides={&quot;entity&quot;: entity, &quot;project&quot;: project})\n    old_artifact = api.artifact(existing_artifact_name)\n    old_artifact_name = re.sub(r&quot;:v\\d+$&quot;, &quot;&quot;, old_artifact.name)\n    with wandb.init(entity=entity, project=project) as run:\n        new_artifact = wandb.Artifact(old_artifact_name, type=old_artifact.type)\n\n        s3_file_names = [s3_url.split(&quot;\/&quot;)[-1] for s3_url in new_s3_file_urls]\n        # add the new ones:\n        for s3_url, filename in zip(new_s3_file_urls, s3_file_names):\n            new_artifact.add_reference(s3_url, filename)\n        # add the old ones:\n        for filename, entry in old_artifact.manifest.entries.items():\n            if filename in s3_file_names:\n                continue\n            new_artifact.add_reference(entry, filename)\n            # this also works but feels hackier:\n            # new_artifact._manifest.entries[filename] = entry\n\n        run.log_artifact(new_artifact)\n        new_artifact.wait()  # wait for upload to finish (blocking - but should be very quick given it is just an s3 link)\n        print(new_artifact.name)\n        print(run.id)\n    return new_artifact\n\n\n# usage:\nlocal_file_path = &quot;config.json&quot; # modified file\ns3_url = &quot;s3:\/\/bucket\/prefix\/config.json&quot;\ns3_url_arr = s3_url.replace(&quot;s3:\/\/&quot;, &quot;&quot;).split(&quot;\/&quot;)\ns3_bucket = s3_url_arr[0]\nkey = &quot;\/&quot;.join(s3_url_arr[1:])\n\ns3_client = boto3.client(&quot;s3&quot;)\nfile_digest = md5_file(local_file_path)\ns3_client.upload_file(\n    local_file_path,\n    s3_bucket,\n    key,\n    # save the md5_digest in metadata,\n    # can be used later to only upload new files to s3,\n    # as AWS doesn't digest the file consistently in the E-tag\n    ExtraArgs={&quot;Metadata&quot;: {&quot;md5_digest&quot;: file_digest}},\n)\n\nwandb_update_only_some_files_in_artifact(\n    &quot;old_artifact:v3&quot;,\n    [&quot;s3:\/\/bucket\/prefix\/config.json&quot;],\n)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1643119052816,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1643119813376,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70637798",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72079835,
        "Question_title":"Pytorch lightning callback for switching dataloader_idx",
        "Question_body":"<p>Is there a callback or something similar used when incrementing the dataloader index? The reason is that I have defined multiple dataloaders and I would like to start a new run with weight and biases for each dataloader.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651433528283,
        "Question_score":0,
        "Question_tags":"pytorch-lightning|wandb",
        "Question_view_count":146,
        "Owner_creation_time":1609516642837,
        "Owner_last_access_time":1664053489570,
        "Owner_location":null,
        "Owner_reputation":2789,
        "Owner_up_votes":102,
        "Owner_down_votes":1,
        "Owner_views":304,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72079835",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71944710,
        "Question_title":"Multiple lines on same plot with incremental logging - wandb",
        "Question_body":"<p>I am using weights &amp; biases (wandb). I want to group multiple plots into one while using incremental logging, any way to do that?<\/p>\n<p>Say we have 10 metrics, I can add them to the project incrementally, gradually building 10 graphs:<\/p>\n<pre><code>import wandb\nimport math\n\nN_STEPS = 100\n\nwandb.init(project=&quot;someProject&quot;, name=&quot;testMultipleLines&quot;)\nfor epoch in range(N_STEPS):\n    log = {}\n    log['main_metric'] = epoch \/ N_STEPS  # some main metric\n\n    # some other metrics I want to have all on 1 plot\n    other_metrics = {}\n    for j in range(10):\n        other_metrics[f'metric_{j}'] = math.sin(j * math.pi * (epoch \/ N_STEPS))\n    log['other_metrics'] = other_metrics\n\n    wandb.log(log)\n<\/code><\/pre>\n<p>This by default gets presented on the wandb interface as 11 different plots. How can they be grouped through the API (without using the web interface) such that <code>main_metric<\/code> is on one figure and all <code>other_metrics<\/code> are bunched together on a second figure?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1650480171990,
        "Question_score":6,
        "Question_tags":"python|machine-learning|plot|wandb",
        "Question_view_count":275,
        "Owner_creation_time":1457253216340,
        "Owner_last_access_time":1663905208120,
        "Owner_location":null,
        "Owner_reputation":667,
        "Owner_up_votes":100,
        "Owner_down_votes":1,
        "Owner_views":22,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71944710",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72972876,
        "Question_title":"YOLOv5 Evolution Results Not Reproducible wandb",
        "Question_body":"<p>I am running YOLOv5 in a sagemaker notebook.\nThe 10 epoch runs are using the following notebook script making use of the --evolve flag for hyperparameters.<\/p>\n<pre><code>!export WANDB_RUN_GROUP=&quot;evolution&quot; &amp;&amp; python .\/deepsea-yolov5\/yolov5\/train.py\n--img=640\n--data=.\/deepsea-yolov5\/opt\/ml\/custom_config.yaml\n--batch=2\n--weights=yolov5s.pt\n--cfg=.\/deepsea-yolov5\/yolov5\/models\/yolov5s.yaml\n--project=&quot;902005-vaa&quot;\n--cache\n--epochs=10\n--evolve=30\n<\/code><\/pre>\n<p>Evolution runs only output one point on the graph at the end of 10 epochs and the outputted hyperparameters do not show reproducible results when running in a 50 epoch run. The blue 50 epoch line showcases using the optimal hyperparameters which should intersect with the highest 10 epoch run, but it doesn't reach anywhere close.\n<a href=\"https:\/\/i.stack.imgur.com\/sW0KX.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sW0KX.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>After finding the optimal hyperparameters I ran a 50 epoch run using those parameters using the following command.<\/p>\n<pre><code>!export WANDB_RUN_GROUP=&quot;hyperparam&quot; &amp;&amp; python .\/deepsea-yolov5\/yolov5\/train.py\n--img=640\n--data=.\/deepsea-yolov5\/opt\/ml\/custom_config.yaml\n--batch=2\n--weights=yolov5s.pt\n--cfg=.\/deepsea-yolov5\/yolov5\/models\/yolov5s.yaml\n--hyp=.\/deepsea-yolov5\/opt\/ml\/input\/data\/hyp.scratch-low.yaml\n--project=&quot;902005-vaa&quot;\n--cache\n--epochs=50\n<\/code><\/pre>\n<p>However as shown in the picture above, the runs do not intersect with the best-performing hyperparameter run.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1657748030047,
        "Question_score":0,
        "Question_tags":"python|yolov5|wandb",
        "Question_view_count":63,
        "Owner_creation_time":1498005510443,
        "Owner_last_access_time":1663651022003,
        "Owner_location":"California, USA",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72972876",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72001154,
        "Question_title":"How to prevent Weights & Biases from saving best model parameters",
        "Question_body":"<p>I am using Weights &amp; Biases (<a href=\"https:\/\/wandb.ai\/\" rel=\"nofollow noreferrer\">link<\/a>) to manage hyperparameter optimization and log the results. I am training using Keras with a Tensorflow backend, and I am using the out-of-the-box logging functionality of Weights &amp; Biases, in which I run<\/p>\n<pre><code>wandb.init(project='project_name', entity='username', config=config)\n<\/code><\/pre>\n<p>and then add a <code>WandbCallback()<\/code> to the callbacks of <code>classifier.fit()<\/code>. By default, Weights &amp; Biases appears to save the model parameters (i.e., the model's weights and biases) and store them in the cloud. This eats up my account's storage quota, and it is unnecessary --- I only care about tracking the model loss\/accuracy as a function of the hyperparameters.<\/p>\n<p>Is it possible for me to train a model and log the loss and accuracy using Weights &amp; Biases, but not store the model parameters in the cloud? How can I do this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650896835733,
        "Question_score":3,
        "Question_tags":"python|tensorflow|machine-learning|keras|wandb",
        "Question_view_count":204,
        "Owner_creation_time":1514243890900,
        "Owner_last_access_time":1663708014960,
        "Owner_location":null,
        "Owner_reputation":167,
        "Owner_up_votes":113,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In order to not save the trained model weights during hyperparam optimization you do something like this:<\/p>\n<pre><code>classifier.fit(..., callbacks=[WandbCallback(.., save_model=False)]\n<\/code><\/pre>\n<p>This will only track the metrics (train\/validation loss\/acc, etc.).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1650898428827,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72001154",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":68952727,
        "Question_title":"wandb: get a list of all artifact collections and all aliases of those artifacts",
        "Question_body":"<p>The wandb documentation doesn't seem to explain how to do this - but it should be a fairly common use case I'd imagine?<\/p>\n<p>I achieved mostly (but not completely) what I wanted like this, but it seems a bit clunky? I'd have expected to have an <code>self.aliases<\/code> property on the <code>ArtifactCollection<\/code> instances?<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>ENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\ndef get_model_artifacts(key=None):\n    wandb.login(key=key if key is not None else API_KEY)\n    api = wandb.Api(overrides={&quot;entity&quot;: ENTITY})\n    model_names = [\n        i\n        for i in api.artifact_type(\n            type_name=&quot;models&quot;, project=&quot;train&quot;\n        ).collections()\n    ]\n    for model in model_names:\n        artifact = api.artifact(&quot;train\/&quot; + model.name + &quot;:latest&quot;)\n        model._attrs.update(artifact._attrs)\n        model._attrs[&quot;metadata&quot;] = json.loads(model._attrs[&quot;metadata&quot;])\n        model.aliases = [x[&quot;alias&quot;] for x in model._attrs[&quot;aliases&quot;]]\n    return model_names\n<\/code><\/pre>\n<p>I guess I could possibly look into writing a custom graph-ql query if needed or just use this clunky method.<\/p>\n<p>Am I missing something? Is there a cleaner way to do this?<\/p>\n<p>The one thing this clunky method is missing is any old aliases - it only shows the latest model and then any aliases of that (let's say &quot;latest&quot; and also &quot;v4&quot; etc.) - not sure how this would\/should be displayed but I'd have hoped to be able to get old aliases as well (i.e. aliases that point to old versions of the artifact). Although, this is less important.<\/p>\n<p><strong>EDIT<\/strong> - after a few hours looking through their sdk code, I have this (still not that happy with how clunky it is):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>ENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\ndef get_model_artifacts(key=None):\n    wandb.login(key=key if key is not None else API_KEY)\n    api = wandb.Api(overrides={&quot;entity&quot;: ENTITY})\n    model_artifacts = [\n        a\n        for a in api.artifact_type(\n            type_name=&quot;models&quot;, project=&quot;train&quot;\n        ).collections()\n    ]\n\n    def get_alias_tuple(artifact_version):\n        version = None\n        aliases = []\n        for a in artifact_version._attrs[&quot;aliases&quot;]:\n            if re.match(r&quot;^v\\d+$&quot;, a[&quot;alias&quot;]):\n                version = a[&quot;alias&quot;]\n            else:\n                aliases.append(a[&quot;alias&quot;])\n        return version, aliases\n\n    for model in model_artifacts:\n        # artifact = api.artifact(&quot;train\/&quot; + model.name + &quot;:latest&quot;)\n        # model._attrs.update(artifact._attrs)\n        # model._attrs[&quot;metadata&quot;] = json.loads(model._attrs[&quot;metadata&quot;])\n        versions = model.versions()\n        version_dict = dict(get_alias_tuple(version) for version in versions)\n        model.version_dict = version_dict\n        model.aliases = [\n            x for key, val in model.version_dict.items() for x in [key] + val\n        ]\n    return model_artifacts\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630064062593,
        "Question_score":1,
        "Question_tags":"python|wandb",
        "Question_view_count":363,
        "Owner_creation_time":1526368885180,
        "Owner_last_access_time":1663954559473,
        "Owner_location":"Munich, Germany",
        "Owner_reputation":3944,
        "Owner_up_votes":574,
        "Owner_down_votes":47,
        "Owner_views":217,
        "Question_last_edit_time":1630357543656,
        "Answer_body":"<p>I'm Annirudh. I'm an engineer at W&amp;B who helped build artifacts. Your solution is really close, but by using the <code>latest<\/code> alias when fetching the artifact we're only going to be considering the aliases from that one artifact instead of all the versions. You could get around that by looping over the versions:<\/p>\n<pre><code>api = wandb.Api()\ncollections = [\n    coll for coll in api.artifact_type(type_name=TYPE, project=PROJECT).collections()\n]\n\n\naliases = set()\nfor coll in collections:\n    for artifact in coll.versions():\n        aliases.update(artifact.aliases)\n\nprint(collections)\nprint(aliases)\n<\/code><\/pre>\n<p>Currently, the documentation is spare on collections but we're polishing them up in the public API and will release some docs around it shortly. These APIs aren't quite release ready yet -- so apologies for the rough edges.<\/p>\n<p>Please feel free to reach out to me directly in the future if you have any other questions regarding artifacts. Always happy to help.<\/p>",
        "Answer_comment_count":14.0,
        "Answer_creation_time":1630122133812,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68952727",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73517766,
        "Question_title":"How to recieve metrics for Object Detection while using pytorch and Weights & Biases?",
        "Question_body":"<p>I have been training and fine tuning few models for detection task on a custom dataset,\nI would like to plot relevant metrics such as mean Average Precision (taking into account the predicted bounding box location and the enclosed object's classification).<\/p>\n<p>I'm using Pytorch and have started using <code>Weights &amp; Biases<\/code> (<a href=\"https:\/\/youtu.be\/G7GH0SeNBMA?list=PLD80i8An1OEGajeVo15ohAQYF1Ttle0lk\" rel=\"nofollow noreferrer\">Weights &amp; Biases integrated with pytorch<\/a>)<\/p>\n<p>For avoiding inventing the wheel, I have used some files from here:<\/p>\n<p><a href=\"https:\/\/github.com\/pytorch\/vision\/tree\/main\/references\/detection\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pytorch\/vision\/tree\/main\/references\/detection<\/a><\/p>\n<p><code>engine.py<\/code> - holds the <strong>train_one_epoch()<\/strong> function<\/p>\n<p><code>cocoeval.py<\/code> - holds the <strong>summarize()<\/strong> function<\/p>\n<p>Now I would like to log those metrics to a <code>Weights &amp; Biases<\/code>,\nso I'll we able to get more clear view and intuition about the fine-tuning phase,but I'm not sure where is the proper place to put the logger invocation.\ncan somebody please assist me?<\/p>\n<pre><code>wandb.watch()\n<\/code><\/pre>\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661681279513,
        "Question_score":0,
        "Question_tags":"image-processing|pytorch|torch|wandb",
        "Question_view_count":33,
        "Owner_creation_time":1287433738400,
        "Owner_last_access_time":1664033141413,
        "Owner_location":null,
        "Owner_reputation":610,
        "Owner_up_votes":289,
        "Owner_down_votes":1,
        "Owner_views":130,
        "Question_last_edit_time":1661681944403,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73517766",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71227518,
        "Question_title":"Connecting Julia to Weights & Biases over Python",
        "Question_body":"<p>I am trying to use weights&amp;biases for my models written in Julia. I am using <code>WeightsAndBiasLogger.jl<\/code> and try to test their demo code:<\/p>\n<pre><code>using Logging, WeightsAndBiasLogger\n\nargs = (n_epochs=1_000, lr=1e-3)\nlogger = WBLogger(project=&quot;sample-project&quot;)\nconfig!(logger, args)\n\nwith(logger) do\n    loss = 0\n    for i in 1:args.n_epochs\n        loss += randn() * args.lr\n        @info &quot;train&quot; i=i loss=loss\n    end\nend\n<\/code><\/pre>\n<p>I receive an error: <strong>&quot;ArgumentError: ref of NULL PyObject&quot;<\/strong> (considering the line: logger = WBLogger(project=&quot;sample-project&quot;)\n)<\/p>\n<p>Then I tried to fix this with the following command:<\/p>\n<pre><code>using Logging, WeightsAndBiasLogger, PyCall\n\nargs = (n_epochs=1_000, lr=1e-3)\n\nconst logger = PyNULL()\nfunction __init__()\n    copy!(logger, WBLogger(project=&quot;sample-project&quot;))\nend\n\nconfig!(logger, args)\n\nwith(logger) do\n    loss = 0\n    for i in 1:args.n_epochs\n        loss += randn() * args.lr\n        @info &quot;train&quot; i=i loss=loss\n    end\nend\n<\/code><\/pre>\n<p>It creates the <code>logger<\/code> object, but now the error is:<\/p>\n<p><strong>MethodError: no method matching config!(::PyObject, ::NamedTuple{(:n_epochs, :lr), Tuple{Int64, Float64}})\nClosest candidates are: config!(!Matched::WBLogger, ::Any; kwargs...)<\/strong> (this consider the line: config!()...<\/p>\n<p>So, does anyone know how to solve the issue? Obviously, I am new to Julia, thus I apologize if asking something very stupid. In addition, if you know a better solution to integrate Julia into W&amp;B or any good alternatives, I would be glad to hear it.<\/p>\n<p>PS: Julia ver 1.7.2<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1645559620073,
        "Question_score":2,
        "Question_tags":"python|julia|pycall|wandb",
        "Question_view_count":102,
        "Owner_creation_time":1535938696457,
        "Owner_last_access_time":1664053164283,
        "Owner_location":"Roskilde, Denmark",
        "Owner_reputation":1443,
        "Owner_up_votes":339,
        "Owner_down_votes":26,
        "Owner_views":122,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71227518",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72731861,
        "Question_title":"Hyperparameter tunning with wandb - CommError: Sweep user not valid when trying to initial the sweep",
        "Question_body":"<p>I'mt rying to use wandb for hyperparameter tunning as described in <a href=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/pytorch\/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb?authuser=1#scrollTo=hoAi-idR1DQk\" rel=\"nofollow noreferrer\">this notebook<\/a> (but using my dataframe and trying to do it on random forest regressor instead).<\/p>\n<p>I'm trying to initial the sweep but I get the error:<\/p>\n<pre><code>sweep_configuration = {\n    &quot;name&quot;: &quot;test-project&quot;,\n    &quot;method&quot;: &quot;random&quot;,\n    &quot;entity&quot;:&quot;my_name&quot;\u05ea\n    &quot;metric&quot;: {\n        &quot;name&quot;: &quot;loss&quot;,\n        &quot;goal&quot;: &quot;minimize&quot;\n    }\n    \n}\n\nparameters_dict = {\n    'n_estimators': {\n        'values': [100,200,300]\n        },\n    'max_depth': {\n        'values': [4,7,10,14]\n        },\n    'min_samples_split': {\n          'values': [2,4,8]\n        },\n    \n    'min_samples_leaf': {\n          'values': [2,4,8]\n        },\n    \n    \n    'max_features': {\n          'values': [1,7,10]\n        },\n\n    }\n\nsweep_configuration['parameters'] = parameters_dict\n\nsweep_id = wandb.sweep(sweep_configuration)\n\n\n<\/code><\/pre>\n<blockquote>\n<p>400 response executing GraphQL. {&quot;errors&quot;:[{&quot;message&quot;:&quot;Sweep user not\nvalid&quot;,&quot;path&quot;:[&quot;upsertSweep&quot;]}],&quot;data&quot;:{&quot;upsertSweep&quot;:null}} wandb:\nERROR Error while calling W&amp;B API: Sweep user not valid (&lt;Response\n[400]&gt;)<br \/>\nCommError: Sweep user not valid<\/p>\n<\/blockquote>\n<p>My end goal : to inital the sweep<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655994282873,
        "Question_score":1,
        "Question_tags":"python|hyperparameters|mlops|wandb",
        "Question_view_count":182,
        "Owner_creation_time":1572256318027,
        "Owner_last_access_time":1663676968963,
        "Owner_location":"Israel",
        "Owner_reputation":1387,
        "Owner_up_votes":955,
        "Owner_down_votes":16,
        "Owner_views":224,
        "Question_last_edit_time":1656234648443,
        "Answer_body":"<p>Two things to try:<\/p>\n<ul>\n<li><p>Like in the notebook, you should pass <code>project=&quot;your-project-name&quot;<\/code> like <code>wandb.sweep(sweep_configuration, project=&quot;your-project-name&quot;)<\/code><\/p>\n<\/li>\n<li><p>Have you logged in to W&amp;B (using <code>wandb.login()<\/code>)?<\/p>\n<\/li>\n<\/ul>\n<p>Finally, once you've successfully created the sweep, you should pass the <code>sweep_id<\/code> and your function (here <code>train<\/code>) like:\n<code>wandb.agent(sweep_id, train, count=5)<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1656329528480,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72731861",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73320449,
        "Question_title":"Plotting the confuison matrix into wandb (pytorch)",
        "Question_body":"<p>I'm training a model and I'm trying to add a confusion matrix, which would be displayed in my <code>wandb<\/code>, but I got lost a bit. Basically, the matrix works; I can print it, but it's not loaded into <code>wandb<\/code>. Everything should be OK, except it's not. Can you please help me? I'm new to all this. Thanks a lot!<\/p>\n<p><strong>the code<\/strong><\/p>\n<pre><code>    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs))\n        print('-' * 10)\n\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  \n            else:\n                model.eval()   \n\n            running_loss = 0.0\n            running_corrects = 0\n\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n                from sklearn.metrics import f1_score\n                f1_score = f1_score(labels.cpu().data, preds.cpu(), average=None)\n                wandb.log({'F1 score' : f1_score})\n\n                nb_classes = 7\n\n                confusion_matrix = torch.zeros(nb_classes, nb_classes)\n                with torch.no_grad():\n                    for i, (inputs, classes) in enumerate(dataloaders['val']):\n                        inputs = inputs.to(device)\n                        classes = classes.to(device)\n                        outputs = model_ft(inputs)\n                        _, preds = torch.max(outputs, 1)\n                    \n                    for t, p in zip(classes.view(-1), preds.view(-1)):\n                        confusion_matrix[t.long(), p.long()] += 1\n              wandb.log({'matrix' : confusion_matrix})\n                           \n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n            wandb.log({'epoch loss': epoch_loss,\n                    'epoch acc': epoch_acc})\n            \n            data = [[i, random.random() + math.sin(i \/ 10)] for i in range(100)]\n            table = wandb.Table(data=data, columns=[&quot;step&quot;, &quot;height&quot;])\n            wandb.log({'line-plot1': wandb.plot.line(table, &quot;step&quot;, &quot;height&quot;)})\n\n        \n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc, f1_score))\n\n            if phase == 'val' and epoch_acc &gt; best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n        \n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n    print('f1_score: {}'.format(f1_score))\n   \n    model.load_state_dict(best_model_wts)\n    return model\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660219911593,
        "Question_score":0,
        "Question_tags":"python|pytorch|conv-neural-network|transfer-learning|wandb",
        "Question_view_count":69,
        "Owner_creation_time":1659890757880,
        "Owner_last_access_time":1660739315393,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1660228199172,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73320449",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":70573652,
        "Question_title":"Model stopped training once I introduced << report_to = 'wandb' >> in TrainingArguments",
        "Question_body":"<p>I am downloading the model <a href=\"https:\/\/huggingface.co\/microsoft\/Multilingual-MiniLM-L12-H384\/tree\/main\" rel=\"nofollow noreferrer\">https:\/\/huggingface.co\/microsoft\/Multilingual-MiniLM-L12-H384\/tree\/main<\/a> microsoft\/Multilingual-MiniLM-L12-H384 and then using it.<\/p>\n<p>Transformer Version: '4.11.3'<\/p>\n<p>I have written the below code:<\/p>\n<pre><code>import wandb\nwandb.login()\n%env WANDB_LOG_MODEL=true\n\nmodel = tr.BertForSequenceClassification.from_pretrained(&quot;\/home\/pc\/minilm_model&quot;,num_labels=2)\nmodel.to(device)\n\nprint(&quot;hello&quot;)\n\ntraining_args = tr.TrainingArguments(\nreport_to = 'wandb',\noutput_dir='\/home\/pc\/proj\/results2', # output directory\nnum_train_epochs=10, # total number of training epochs\nper_device_train_batch_size=16, # batch size per device during training\nper_device_eval_batch_size=32, # batch size for evaluation\nlearning_rate=2e-5,\nwarmup_steps=1000, # number of warmup steps for learning rate scheduler\nweight_decay=0.01, # strength of weight decay\nlogging_dir='.\/logs', # directory for storing logs\nlogging_steps=1000,\nevaluation_strategy=&quot;epoch&quot;,\nsave_strategy=&quot;no&quot;\n)\n\nprint(&quot;hello&quot;)\n\ntrainer = tr.Trainer(\nmodel=model, # the instantiated  Transformers model to be trained\nargs=training_args, # training arguments, defined above\ntrain_dataset=train_data, # training dataset\neval_dataset=val_data, # evaluation dataset\ncompute_metrics=compute_metrics\n)\n\n<\/code><\/pre>\n<p>After Executing this:<\/p>\n<p>The model stuck at this point:<\/p>\n<p>***** Running training *****<\/p>\n<pre><code>Num examples = 12981\n Num Epochs = 20\n Instantaneous batch size per device = 16\n Total train batch size (w. parallel, distributed &amp; accumulation) = 32\n Gradient Accumulation steps = 1\n Total optimization steps = 8120\nAutomatic Weights &amp; Biases logging enabled, to disable set os.environ[&quot;WANDB_DISABLED&quot;] = &quot;true&quot;\n\n<\/code><\/pre>\n<p><strong>What could be the possible solution?<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641265169473,
        "Question_score":0,
        "Question_tags":"python-3.x|nlp|huggingface-transformers|wandb",
        "Question_view_count":171,
        "Owner_creation_time":1528361086053,
        "Owner_last_access_time":1663924548837,
        "Owner_location":null,
        "Owner_reputation":1127,
        "Owner_up_votes":526,
        "Owner_down_votes":93,
        "Owner_views":283,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70573652",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":67160576,
        "Question_title":"YoloV5 killed at first epoch",
        "Question_body":"<p>I'm using a virtual machine on Windows 10 with this config:<\/p>\n<pre><code>Memory 7.8 GiB\nProcessor Intel\u00ae Core\u2122 i5-6600K CPU @ 3.50GHz \u00d7 3\nGraphics llvmpipe (LLVM 11.0.0, 256 bits)\nDisk Capcity 80.5 GB\nOS Ubuntu 20.10 64 Bit\nVirtualization Oracle\n<\/code><\/pre>\n<p>I installed docker for Ubuntu as described in <a href=\"https:\/\/docs.docker.com\/engine\/install\/ubuntu\/\" rel=\"nofollow noreferrer\">the official documentation<\/a>.<br>\nI pulled the docker image as described on the <a href=\"https:\/\/github.com\/ultralytics\/yolov5\/wiki\/Docker-Quickstart\" rel=\"nofollow noreferrer\">yolo github section for docker<\/a>.<br>\nSince I have no NVIDIA GPU I could not install a driver or CUDA.\nI pulled the aquarium from <a href=\"https:\/\/public.roboflow.com\/object-detection\/aquarium\" rel=\"nofollow noreferrer\">roboflow<\/a> and installed it on a folde aquarium.\nI ran this command to start the image and have my aquarium folder mounted<\/p>\n<pre><code>sudo docker run --ipc=host -it -v &quot;$(pwd)&quot;\/Desktop\/yolo\/aquarium:\/usr\/src\/app\/aquarium ultralytics\/yolov5:latest\n<\/code><\/pre>\n<p>And was greeted with this banner<\/p>\n<blockquote>\n<h1>=============\n== PyTorch ==<\/h1>\n<p>NVIDIA Release 21.03 (build 21060478) PyTorch Version 1.9.0a0+df837d0<\/p>\n<p>Container image Copyright (c) 2021, NVIDIA CORPORATION.  All rights\nreserved.<\/p>\n<p>Copyright (c) 2014-2021 Facebook Inc. Copyright (c) 2011-2014 Idiap\nResearch Institute (Ronan Collobert) Copyright (c) 2012-2014 Deepmind\nTechnologies    (Koray Kavukcuoglu) Copyright (c) 2011-2012 NEC\nLaboratories America (Koray Kavukcuoglu) Copyright (c) 2011-2013 NYU<br \/>\n(Clement Farabet) Copyright (c) 2006-2010 NEC Laboratories America\n(Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston) Copyright\n(c) 2006      Idiap Research Institute (Samy Bengio) Copyright (c)\n2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio,\nJohnny Mariethoz) Copyright (c) 2015      Google Inc. Copyright (c)\n2015      Yangqing Jia Copyright (c) 2013-2016 The Caffe contributors\nAll rights reserved.<\/p>\n<p>NVIDIA Deep Learning Profiler (dlprof) Copyright (c) 2021, NVIDIA\nCORPORATION.  All rights reserved.<\/p>\n<p>Various files include modifications (c) NVIDIA CORPORATION.  All\nrights reserved.<\/p>\n<p>This container image and its contents are governed by the NVIDIA Deep\nLearning Container License. By pulling and using the container, you\naccept the terms and conditions of this license:\n<a href=\"https:\/\/developer.nvidia.com\/ngc\/nvidia-deep-learning-container-license\" rel=\"nofollow noreferrer\">https:\/\/developer.nvidia.com\/ngc\/nvidia-deep-learning-container-license<\/a><\/p>\n<p>WARNING: The NVIDIA Driver was not detected.  GPU functionality will\nnot be available.    Use 'nvidia-docker run' to start this container;\nsee    <a href=\"https:\/\/github.com\/NVIDIA\/nvidia-docker\/wiki\/nvidia-docker\" rel=\"nofollow noreferrer\">https:\/\/github.com\/NVIDIA\/nvidia-docker\/wiki\/nvidia-docker<\/a> .<\/p>\n<p>NOTE: MOFED driver for multi-node communication was not detected.\nMulti-node communication performance may be reduced.<\/p>\n<\/blockquote>\n<p>So no error there.<br>\nI installed pip and with pip wandb I added wandb. I used <code>wandb login<\/code> and set my API key.<br><br>\nI ran following command:<\/p>\n<pre><code># python train.py --img 640 --batch 16 --epochs 10 --data .\/aquarium\/data.yaml --weights yolov5s.pt --project ip5 --name aquarium5 --nosave --cache\n<\/code><\/pre>\n<p>And received this output:<\/p>\n<pre><code>github: skipping check (Docker image)\nYOLOv5  v5.0-14-g238583b torch 1.9.0a0+df837d0 CPU\n\nNamespace(adam=False, artifact_alias='latest', batch_size=16, bbox_interval=-1, bucket='', cache_images=True, cfg='', data='.\/aquarium\/data.yaml', device='', entity=None, epochs=10, evolve=False, exist_ok=False, global_rank=-1, hyp='data\/hyp.scratch.yaml', image_weights=False, img_size=[640, 640], label_smoothing=0.0, linear_lr=False, local_rank=-1, multi_scale=False, name='aquarium5', noautoanchor=False, nosave=True, notest=False, project='ip5', quad=False, rect=False, resume=False, save_dir='ip5\/aquarium5', save_period=-1, single_cls=False, sync_bn=False, total_batch_size=16, upload_dataset=False, weights='yolov5s.pt', workers=8, world_size=1)\ntensorboard: Start with 'tensorboard --logdir ip5', view at http:\/\/localhost:6006\/\nhyperparameters: lr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0\nwandb: Currently logged in as: pebs (use `wandb login --relogin` to force relogin)\nwandb: Tracking run with wandb version 0.10.26\nwandb: Syncing run aquarium5\nwandb: \u2b50\ufe0f View project at https:\/\/wandb.ai\/pebs\/ip5\nwandb:  View run at https:\/\/wandb.ai\/pebs\/ip5\/runs\/1c2j80ii\nwandb: Run data is saved locally in \/usr\/src\/app\/wandb\/run-20210419_102642-1c2j80ii\nwandb: Run `wandb offline` to turn off syncing.\n\nOverriding model.yaml nc=80 with nc=7\n\n                 from  n    params  module                                  arguments                     \n  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n 24      [17, 20, 23]  1     32364  models.yolo.Detect                      [7, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n[W NNPACK.cpp:80] Could not initialize NNPACK! Reason: Unsupported hardware.\nModel Summary: 283 layers, 7079724 parameters, 7079724 gradients, 16.4 GFLOPS\n\nTransferred 356\/362 items from yolov5s.pt\nScaled weight_decay = 0.0005\nOptimizer groups: 62 .bias, 62 conv.weight, 59 other\ntrain: Scanning '\/usr\/src\/app\/aquarium\/train\/labels.cache' images and labels... 448 found, 0 missing, 1 empty, 0 corrupted: 100%|\u2588| 448\/448 [00:00&lt;?, ?\ntrain: Caching images (0.4GB): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 448\/448 [00:01&lt;00:00, 313.77it\/s]\nval: Scanning '\/usr\/src\/app\/aquarium\/valid\/labels.cache' images and labels... 127 found, 0 missing, 0 empty, 0 corrupted: 100%|\u2588| 127\/127 [00:00&lt;?, ?it\nval: Caching images (0.1GB): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 127\/127 [00:00&lt;00:00, 141.31it\/s]\nPlotting labels... \n\nautoanchor: Analyzing anchors... anchors\/target = 5.17, Best Possible Recall (BPR) = 0.9997\nImage sizes 640 train, 640 test\nUsing 3 dataloader workers\nLogging results to ip5\/aquarium5\nStarting training for 10 epochs...\n\n     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n  0%|                                                                                                                           | 0\/28 [00:00&lt;?, ?it\/s]Killed\nroot@cf40a6498016:~# \/opt\/conda\/lib\/python3.8\/multiprocessing\/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n<\/code><\/pre>\n<p>From this output I would think that there were 0 epochs completed.<br>\nMy data.yaml contains this code:<\/p>\n<pre><code>train: \/usr\/src\/app\/aquarium\/train\/images\nval: \/usr\/src\/app\/aquarium\/valid\/images\n\nnc: 7\nnames: ['fish', 'jellyfish', 'penguin', 'puffin', 'shark', 'starfish', 'stingray']\n<\/code><\/pre>\n<p><a href=\"https:\/\/wandb.ai\/\" rel=\"nofollow noreferrer\">wandb.ai<\/a> does not display any metrics, but I have the files config.yaml, requirements.txt, wandb-metadata.json and wandb-summary.json.<\/p>\n<p>Why am I not getting any output?<br>\nHas there in fact be no training at all?<br>\nIf there was a training, how can I use my model?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1618829350100,
        "Question_score":2,
        "Question_tags":"python|docker|pytorch|yolov5|wandb",
        "Question_view_count":1939,
        "Owner_creation_time":1430930915943,
        "Owner_last_access_time":1661487827213,
        "Owner_location":"Switzerland",
        "Owner_reputation":2006,
        "Owner_up_votes":255,
        "Owner_down_votes":4,
        "Owner_views":230,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67160576",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71191185,
        "Question_title":"Pip installations on Colab from local",
        "Question_body":"<p>I'd like to use wandb on Colab, and I've installed it through pip on the command line. However, the import isn't recognized on Colab, so I have to run <code>!pip install wandb<\/code> each time.<\/p>\n<p>How can I install <code>wandb<\/code> locally so that I don't have to install it on the Colab notebook each time?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_time":1645328187297,
        "Question_score":0,
        "Question_tags":"python|pip|google-colaboratory|wandb",
        "Question_view_count":516,
        "Owner_creation_time":1558237713157,
        "Owner_last_access_time":1663947795227,
        "Owner_location":null,
        "Owner_reputation":421,
        "Owner_up_votes":20,
        "Owner_down_votes":5,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71191185",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73068169,
        "Question_title":"wandb : move runs from a blocked entity",
        "Question_body":"<p>I accidentally moved several runs from my own user account to a team entity.\nUnfortunatly, this team entity had a restriction on the quantity of experiments tracked, and it now appears as blocked. I have this error message :<\/p>\n<blockquote>\n<p>Your organization is over the limit of 250 tracked hours. Please upgrade your plan to keep using W&amp;B.<\/p>\n<\/blockquote>\n<p>I can't find a way to move back these runs to my free account, does anyone know how to do this ?\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658414318473,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":19,
        "Owner_creation_time":1598425704360,
        "Owner_last_access_time":1658749438907,
        "Owner_location":"Toulouse, France",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73068169",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71432453,
        "Question_title":"getting aligned val_loss and train_loss plots for each epoch using WandB rather than separate plots",
        "Question_body":"<p>I have the following code for logging the train and val loss in each epoch using WandB API. I am not sure though why I am not getting val loss and train loss in the same epoch. Any idea how that could be fixed?<\/p>\n<pre><code>wandb.log({&quot;train loss&quot;: train_epoch_loss,\n           &quot;val loss&quot;: val_epoch_loss,\n           &quot;epoch&quot;: epoch})\n\nwandb.log({&quot;train acc&quot;: train_epoch_acc,\n           &quot;val acc&quot;: val_epoch_acc,\n           &quot;epoch&quot;: epoch})\n\nwandb.log({&quot;best val acc&quot;: best_acc, &quot;epoch&quot;: epoch})\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/w6JBF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/w6JBF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As you see, val loss vs epochs and train loss vs epochs are two completely separate entities while I would like to have both of them in one plot in WandB.\n<a href=\"https:\/\/i.stack.imgur.com\/cqSkl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cqSkl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZSMye.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZSMye.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646959611130,
        "Question_score":0,
        "Question_tags":"python|machine-learning|deep-learning|wandb",
        "Question_view_count":463,
        "Owner_creation_time":1369335377410,
        "Owner_last_access_time":1656440556490,
        "Owner_location":"Boston, MA, United States",
        "Owner_reputation":31183,
        "Owner_up_votes":4284,
        "Owner_down_votes":32,
        "Owner_views":18708,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71432453",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69920078,
        "Question_title":"What does \"save_graph\" keyword in WandbCallback mean?",
        "Question_body":"<p>I'm using Weights and Biases to track my deep learning models. To monitor everything I use the <code>WandbCallback<\/code> in <code>.fit<\/code>.\nIn the <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/integrations\/keras\/wandbcallback\" rel=\"nofollow noreferrer\">WandbCallback documentation<\/a> there is the keyword <code>save_graph<\/code> which defaults to True. The description is very brief and I wondered what that saved graph is and what it's for? Is saving a graph a costly operation? For what is it needed? (like does it complement something else, like saving the best model?)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636577514790,
        "Question_score":0,
        "Question_tags":"tensorflow2.0|tf.keras|wandb",
        "Question_view_count":60,
        "Owner_creation_time":1600299419450,
        "Owner_last_access_time":1662833034530,
        "Owner_location":"Germany",
        "Owner_reputation":62,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>That is used to create log a <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/data-types\/graph\" rel=\"nofollow noreferrer\">wandb.Graph<\/a> of the model. This class is typically used for saving and diplaying neural net models. It represents the graph as an array of nodes and edges. The nodes can have labels that can be visualized by wandb. Here's an example of the graph that it produces: <a href=\"https:\/\/wandb.ai\/l2k2\/keras-examples\/runs\/ieqy2e9h\/model\" rel=\"nofollow noreferrer\">https:\/\/wandb.ai\/l2k2\/keras-examples\/runs\/ieqy2e9h\/model<\/a><\/p>\n<p>Here's the code that does that within the callback. <a href=\"https:\/\/github.com\/wandb\/client\/blob\/1609f82c84e2244ed8fe62c746099d2094bd746a\/wandb\/integration\/keras\/keras.py#L552\" rel=\"nofollow noreferrer\">https:\/\/github.com\/wandb\/client\/blob\/1609f82c84e2244ed8fe62c746099d2094bd746a\/wandb\/integration\/keras\/keras.py#L552<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1636642289972,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69920078",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71134673,
        "Question_title":"python ray tune unable to stop trial or experiment",
        "Question_body":"<p>I am trying to make ray tune with wandb stop the experiment under certain conditions.<\/p>\n<ul>\n<li>stop all experiment if any trial raises an Exception (so i can fix the code and resume)<\/li>\n<li>stop if my score gets -999<\/li>\n<li>stop if the variable <code>varcannotbezero<\/code> gets 0<\/li>\n<\/ul>\n<p><strong>The following things i tried all failed in achieving desired behavior:<\/strong><\/p>\n<ul>\n<li>stop={&quot;score&quot;:-999,&quot;varcannotbezero&quot;:0}<\/li>\n<li>max_failures=0<\/li>\n<li>defining a Stoper class did also not work<\/li>\n<\/ul>\n<pre><code>class RayStopper(Stopper):\n    def __init__(self):\n        self._start = time.time()\n        #self._deadline = 300\n    def __call__(self, trial_id, result):\n        self.score=result[&quot;score&quot;]\n        self.varcannotbezero=result[&quot;varcannotbezero&quot;]\n        return False\n    def stop_all(self):\n        if self.score==-999 or self.varcannotbezero==0:\n            return True\n        else:\n            return False\n<\/code><\/pre>\n<p>Ray tune just continues to run<\/p>\n<pre><code>    wandb_project=&quot;ABC&quot;\n    wandb_api_key=&quot;KEY&quot;\n    ray.init(configure_logging=False)\n\n    if current_best_params is None:\n        algo = HyperOptSearch()\n    else:\n        algo = HyperOptSearch(points_to_evaluate=current_best_params,n_initial_points=n_initial_points)\n    algo = ConcurrencyLimiter(algo, max_concurrent=1)\n\n    scheduler = AsyncHyperBandScheduler()\n    analysis = tune.run(\n        tune_obj,\n        name=&quot;Name&quot;,\n        resources_per_trial={&quot;cpu&quot;: 1},\n        search_alg=algo,\n        scheduler=scheduler,\n        metric=&quot;score&quot;,\n        mode=&quot;max&quot;,\n        num_samples=10,\n        stop={&quot;score&quot;:-999,&quot;varcannotbezero&quot;:0},\n        max_failures=0,\n        config=config,\n        callbacks=[WandbLoggerCallback(project=wandb_project,entity=&quot;mycompany&quot;,api_key=wandb_api_key,log_config=True)],\n        local_dir=local_dir,\n        resume=&quot;AUTO&quot;,\n        verbose=0\n    )\n\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1644967685327,
        "Question_score":0,
        "Question_tags":"optimization|hyperparameters|ray|ray-tune|wandb",
        "Question_view_count":284,
        "Owner_creation_time":1300741959900,
        "Owner_last_access_time":1664027459750,
        "Owner_location":null,
        "Owner_reputation":2390,
        "Owner_up_votes":56,
        "Owner_down_votes":1,
        "Owner_views":308,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71134673",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73165796,
        "Question_title":"Wandb first run start time is delayed",
        "Question_body":"<p>I wanted to compare the execution speeds of three data types. The runs were organized in sequence of <code>Original<\/code>, <code>DictList<\/code>, <code>DataFrame<\/code>. So <code>Original<\/code> was the first run. The x-axis is set as <code>Relative Time (Process)<\/code><\/p>\n<p>Problem is that each runs starting time is all different! How can I make sure that they all start at time 0?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PotUf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PotUf.png\" alt=\"Loss Graph\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rwBf4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rwBf4.png\" alt=\"Elapsed Steps Graph\" \/><\/a><\/p>\n<p>I can't post the entire code. I'll post every point where <code>wandb<\/code> API is called.<\/p>\n<pre><code>import wandb\nif __name__ == &quot;__main__&quot;:\n    wandb.login()\n    datatypes = {&quot;Original&quot;: Original, &quot;DictList&quot;: DictList, &quot;DataFrame&quot;: DataFrame}\n    for type_name, datatype in datatypes.items():\n        wandb_run = wandb.init(project=&quot;Compare&quot;, name=type_name, reinit=True)\n        with wandb_run:\n            # Initialize RL training session\n            storage = datatype()\n            # Run RL training session\n            # Log (loss, elapsed_steps, etc.)    \n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1659093160443,
        "Question_score":0,
        "Question_tags":"pytorch|wandb",
        "Question_view_count":13,
        "Owner_creation_time":1502879704843,
        "Owner_last_access_time":1664071330100,
        "Owner_location":"South Korea",
        "Owner_reputation":1248,
        "Owner_up_votes":800,
        "Owner_down_votes":23,
        "Owner_views":221,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73165796",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71654404,
        "Question_title":"How to fix Wandb API giving error 400 when deleting artifacts?",
        "Question_body":"<p>I accidentally logged way too much to wandb and would like to delete some artifacts. I've tried the following script, but I get an error 400 whenever I run it:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import wandb\n\nwandb.login(key=KEY)\n\napi = wandb.Api(overrides={&quot;project&quot;: PROJECT, &quot;entity&quot;: ENTITY})\n\nfor run in api.runs():\n    files = sorted([f for f in run.logged_artifacts()], key=lambda f: f.updated_at)\n    print(&quot;Total files:&quot;, len(files))\n    print(&quot;Last file:&quot;, files[-1].name)\n    print(&quot;Last file date:&quot;, files[-1].updated_at)\n    for f in files[:-1]:\n        if &quot;.tar&quot; in f.name:\n            # also tried just f.delete()\n            a = api.artifact(f&quot;{PROJECT}\/{f.name}&quot;)\n            print(&quot;Deleting {}&quot;.format(f.name))\n            a.delete()\n<\/code><\/pre>\n<p>All I get is <code>requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https:\/\/api.wandb.ai\/graphql<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648506655603,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":349,
        "Owner_creation_time":1349927421380,
        "Owner_last_access_time":1664055989120,
        "Owner_location":null,
        "Owner_reputation":2248,
        "Owner_up_votes":69,
        "Owner_down_votes":9,
        "Owner_views":403,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71654404",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73642527,
        "Question_title":"How to install wandb on a docker image for arm?",
        "Question_body":"<p>My docker building failed at the <code>RUN <\/code><\/p>\n<p>with:<\/p>\n<pre><code>(meta_learning) brandomiranda~ \u276f docker build -f ~\/iit-term-synthesis\/Dockerfile_arm -t brandojazz\/iit-term-synthesis:test_arm ~\/iit-term-synthesis\/\n\n[+] Building 184.7s (20\/28)\n =&gt; [internal] load build definition from Dockerfile_arm                                                                                           0.0s\n =&gt; =&gt; transferring dockerfile: 41B                                                                                                                0.0s\n =&gt; [internal] load .dockerignore                                                                                                                  0.0s\n =&gt; =&gt; transferring context: 2B                                                                                                                    0.0s\n =&gt; [internal] load metadata for docker.io\/continuumio\/miniconda3:latest                                                                           0.0s\n =&gt; [ 1\/24] FROM docker.io\/continuumio\/miniconda3                                                                                                  0.0s\n =&gt; https:\/\/api.github.com\/repos\/IBM\/pycoq\/git\/refs\/heads\/main                                                                                     0.3s\n =&gt; CACHED [ 2\/24] RUN apt-get update   &amp;&amp; apt-get install -y --no-install-recommends     ssh     git     m4     libgmp-dev     opam     wget      0.0s\n =&gt; CACHED [ 3\/24] RUN useradd -m bot                                                                                                              0.0s\n =&gt; CACHED [ 4\/24] WORKDIR \/home\/bot                                                                                                               0.0s\n =&gt; CACHED [ 5\/24] ADD https:\/\/api.github.com\/repos\/IBM\/pycoq\/git\/refs\/heads\/main version.json                                                     0.0s\n =&gt; CACHED [ 6\/24] RUN opam init --disable-sandboxing                                                                                              0.0s\n =&gt; CACHED [ 7\/24] RUN opam switch create ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1 ocaml-variants.4.07.1+flambda                     0.0s\n =&gt; CACHED [ 8\/24] RUN opam switch ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1                                                          0.0s\n =&gt; CACHED [ 9\/24] RUN eval $(opam env)                                                                                                            0.0s\n =&gt; CACHED [10\/24] RUN opam repo add coq-released https:\/\/coq.inria.fr\/opam\/released                                                               0.0s\n =&gt; CACHED [11\/24] RUN opam repo --all-switches add --set-default coq-released https:\/\/coq.inria.fr\/opam\/released                                  0.0s\n =&gt; CACHED [12\/24] RUN opam update --all                                                                                                           0.0s\n =&gt; CACHED [13\/24] RUN opam pin add -y coq 8.11.0                                                                                                  0.0s\n =&gt; [14\/24] RUN opam install -y coq-serapi                                                                                                       176.3s\n =&gt; [15\/24] RUN eval $(opam env)                                                                                                                   0.2s\n =&gt; ERROR [16\/24] RUN pip install wandb --upgrade                                                                                                  8.0s\n------\n &gt; [16\/24] RUN pip install wandb --upgrade:\n#20 0.351 Defaulting to user installation because normal site-packages is not writeable\n#20 0.637 Collecting wandb\n#20 0.986   Downloading wandb-0.13.2-py2.py3-none-any.whl (1.8 MB)\n#20 1.365 Requirement already satisfied: setuptools in \/opt\/conda\/lib\/python3.9\/site-packages (from wandb) (61.2.0)\n#20 1.366 Requirement already satisfied: six&gt;=1.13.0 in \/opt\/conda\/lib\/python3.9\/site-packages (from wandb) (1.16.0)\n#20 1.409 Collecting promise&lt;3,&gt;=2.0\n#20 1.472   Downloading promise-2.3.tar.gz (19 kB)\n#20 2.087 Collecting PyYAML\n#20 2.154   Downloading PyYAML-6.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (731 kB)\n#20 2.431 Collecting protobuf&lt;4.0dev,&gt;=3.12.0\n#20 2.492   Downloading protobuf-3.20.1-cp39-cp39-manylinux2014_aarch64.whl (917 kB)\n#20 2.648 Collecting setproctitle\n#20 2.706   Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (30 kB)\n#20 2.763 Collecting Click!=8.0.0,&gt;=7.0\n#20 2.818   Downloading click-8.1.3-py3-none-any.whl (96 kB)\n#20 2.902 Collecting sentry-sdk&gt;=1.0.0\n#20 2.962   Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n#20 3.112 Collecting psutil&gt;=5.0.0\n#20 3.172   Downloading psutil-5.9.2.tar.gz (479 kB)\n#20 3.871 Collecting pathtools\n#20 3.937   Downloading pathtools-0.1.2.tar.gz (11 kB)\n#20 4.431 Collecting shortuuid&gt;=0.5.0\n#20 4.509   Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n#20 4.512 Requirement already satisfied: requests&lt;3,&gt;=2.0.0 in \/opt\/conda\/lib\/python3.9\/site-packages (from wandb) (2.27.1)\n#20 4.568 Collecting docker-pycreds&gt;=0.4.0\n#20 4.636   Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n#20 4.695 Collecting GitPython&gt;=1.0.0\n#20 4.781   Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n#20 4.834 Collecting gitdb&lt;5,&gt;=4.0.1\n#20 4.892   Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n#20 4.934 Collecting smmap&lt;6,&gt;=3.0.1\n#20 4.992   Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n#20 5.005 Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (1.26.8)\n#20 5.005 Requirement already satisfied: certifi&gt;=2017.4.17 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (2021.10.8)\n#20 5.006 Requirement already satisfied: idna&lt;4,&gt;=2.5 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (3.3)\n#20 5.006 Requirement already satisfied: charset-normalizer~=2.0.0 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (2.0.4)\n#20 5.075 Collecting urllib3&lt;1.27,&gt;=1.21.1\n#20 5.135   Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n#20 5.172 Building wheels for collected packages: promise, psutil, pathtools\n#20 5.172   Building wheel for promise (setup.py): started\n#20 5.851   Building wheel for promise (setup.py): finished with status 'done'\n#20 5.852   Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21503 sha256=6de0373376d2a8e995959e6173507e13cba502c79b648b5884b1eac45d1ec9ae\n#20 5.852   Stored in directory: \/home\/bot\/.cache\/pip\/wheels\/e1\/e8\/83\/ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n#20 5.854   Building wheel for psutil (setup.py): started\n#20 6.226   Building wheel for psutil (setup.py): finished with status 'error'\n#20 6.226   ERROR: Command errored out with exit status 1:\n#20 6.226    command: \/opt\/conda\/bin\/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' bdist_wheel -d \/tmp\/pip-wheel-4y62c4eb\n#20 6.226        cwd: \/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/\n#20 6.226   Complete output (45 lines):\n#20 6.226   running bdist_wheel\n#20 6.226   running build\n#20 6.226   running build_py\n#20 6.226   creating build\n#20 6.226   creating build\/lib.linux-aarch64-3.9\n#20 6.226   creating build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psosx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psbsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_common.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_pswindows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psposix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_compat.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_pslinux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_pssunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psaix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   creating build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/__main__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_process.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_aix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_misc.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_bsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_linux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/runner.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_connections.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_unicode.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_windows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_contracts.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_sunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_testutils.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_osx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_memleaks.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_posix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_system.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   running build_ext\n#20 6.226   building 'psutil._psutil_linux' extension\n#20 6.226   creating build\/temp.linux-aarch64-3.9\n#20 6.226   creating build\/temp.linux-aarch64-3.9\/psutil\n#20 6.226   gcc -pthread -B \/opt\/conda\/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -I\/opt\/conda\/include -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -fPIC -DPSUTIL_POSIX=1 -DPSUTIL_SIZEOF_PID_T=4 -DPSUTIL_VERSION=592 -DPSUTIL_LINUX=1 -I\/opt\/conda\/include\/python3.9 -c psutil\/_psutil_common.c -o build\/temp.linux-aarch64-3.9\/psutil\/_psutil_common.o\n#20 6.226   gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 6.226   gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 6.226   gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 6.226   gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 6.226   error: command '\/usr\/bin\/gcc' failed with exit code 1\n#20 6.226   ----------------------------------------\n#20 6.226   ERROR: Failed building wheel for psutil\n#20 6.226   Running setup.py clean for psutil\n#20 6.550   Building wheel for pathtools (setup.py): started\n#20 7.135   Building wheel for pathtools (setup.py): finished with status 'done'\n#20 7.135   Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=8e205a0f68c9c7a3c0107d1cc40d94f1d2843c78270217378dcbe98212958b82\n#20 7.135   Stored in directory: \/home\/bot\/.cache\/pip\/wheels\/b7\/0a\/67\/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n#20 7.136 Successfully built promise pathtools\n#20 7.136 Failed to build psutil\n#20 7.195 Installing collected packages: smmap, urllib3, gitdb, shortuuid, setproctitle, sentry-sdk, PyYAML, psutil, protobuf, promise, pathtools, GitPython, docker-pycreds, Click, wandb\n#20 7.262   WARNING: The script shortuuid is installed in '\/home\/bot\/.local\/bin' which is not on PATH.\n#20 7.262   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n#20 7.345     Running setup.py install for psutil: started\n#20 7.727     Running setup.py install for psutil: finished with status 'error'\n#20 7.727     ERROR: Command errored out with exit status 1:\n#20 7.727      command: \/opt\/conda\/bin\/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-gb2y421d\/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers \/home\/bot\/.local\/include\/python3.9\/psutil\n#20 7.727          cwd: \/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/\n#20 7.727     Complete output (47 lines):\n#20 7.727     running install\n#20 7.727     \/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/command\/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n#20 7.727       warnings.warn(\n#20 7.727     running build\n#20 7.727     running build_py\n#20 7.727     creating build\n#20 7.727     creating build\/lib.linux-aarch64-3.9\n#20 7.727     creating build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psosx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psbsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_common.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_pswindows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psposix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_compat.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_pslinux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_pssunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psaix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     creating build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/__main__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_process.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_aix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_misc.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_bsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_linux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/runner.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_connections.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_unicode.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_windows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_contracts.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_sunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_testutils.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_osx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_memleaks.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_posix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_system.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     running build_ext\n#20 7.727     building 'psutil._psutil_linux' extension\n#20 7.727     creating build\/temp.linux-aarch64-3.9\n#20 7.727     creating build\/temp.linux-aarch64-3.9\/psutil\n#20 7.727     gcc -pthread -B \/opt\/conda\/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -I\/opt\/conda\/include -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -fPIC -DPSUTIL_POSIX=1 -DPSUTIL_SIZEOF_PID_T=4 -DPSUTIL_VERSION=592 -DPSUTIL_LINUX=1 -I\/opt\/conda\/include\/python3.9 -c psutil\/_psutil_common.c -o build\/temp.linux-aarch64-3.9\/psutil\/_psutil_common.o\n#20 7.727     gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 7.727     gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 7.727     gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 7.727     gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 7.727     error: command '\/usr\/bin\/gcc' failed with exit code 1\n#20 7.727     ----------------------------------------\n#20 7.728 ERROR: Command errored out with exit status 1: \/opt\/conda\/bin\/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-gb2y421d\/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers \/home\/bot\/.local\/include\/python3.9\/psutil Check the logs for full command output.\n------\nexecutor failed running [\/bin\/sh -c pip install wandb --upgrade]: exit code: 1\n<\/code><\/pre>\n<p>why?<\/p>\n<p>Docker file so far:<\/p>\n<pre><code>FROM continuumio\/miniconda3\n\nRUN apt-get update \\\n  &amp;&amp; apt-get install -y --no-install-recommends \\\n    ssh \\\n    git \\\n    m4 \\\n    libgmp-dev \\\n    opam \\\n    wget \\\n    ca-certificates \\\n    rsync \\\n    strace\n\nRUN useradd -m bot\nWORKDIR \/home\/bot\nUSER bot\n\n## https:\/\/stackoverflow.com\/questions\/73642349\/how-to-have-miniconda-work-properly-with-docker-especially-naming-my-conda-en\n#RUN wget https:\/\/repo.anaconda.com\/miniconda\/Miniconda3-latest-Linux-x86_64.sh  \\\n#    &amp;&amp; bash Miniconda3-latest-Linux-x86_64.sh -b -f\n#ENV PATH=&quot;\/home\/bot\/miniconda3\/bin:${PATH}&quot;\n#RUN conda create -n pycoq python=3.9 -y\n## somehow this &quot;works&quot; but conda isn't fully aware of this. Fix later?\n#ENV PATH=&quot;\/home\/bot\/miniconda3\/envs\/pycoq\/bin:${PATH}&quot;\n\nADD https:\/\/api.github.com\/repos\/IBM\/pycoq\/git\/refs\/heads\/main version.json\n\n# -- setup opam like VP's PyCoq\nRUN opam init --disable-sandboxing\n# compiler + '_' + coq_serapi + '.' + coq_serapi_pin\nRUN opam switch create ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1 ocaml-variants.4.07.1+flambda\nRUN opam switch ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1\nRUN eval $(opam env)\n\nRUN opam repo add coq-released https:\/\/coq.inria.fr\/opam\/released\n# RUN opam pin add -y coq 8.11.0\n# ['opam', 'repo', '--all-switches', 'add', '--set-default', 'coq-released', 'https:\/\/coq.inria.fr\/opam\/released']\nRUN opam repo --all-switches add --set-default coq-released https:\/\/coq.inria.fr\/opam\/released\nRUN opam update --all\nRUN opam pin add -y coq 8.11.0\n\n#RUN opam install -y --switch ocaml-variants.4.07.1+flambda_coq-serapi_coq-serapi_8.11.0+0.11.1 coq-serapi 8.11.0+0.11.1\nRUN opam install -y coq-serapi\n\nRUN eval $(opam env)\n\n# makes sure depedencies for pycoq are installed once already in the docker image\nENV WANDB_API_KEY=&quot;SECRET&quot;\nRUN pip install wandb --upgrade\n<\/code><\/pre>\n<p><a href=\"https:\/\/community.wandb.ai\/t\/how-to-install-wandb-on-a-docker-image-for-arm\/3080\" rel=\"nofollow noreferrer\">https:\/\/community.wandb.ai\/t\/how-to-install-wandb-on-a-docker-image-for-arm\/3080<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1662595635817,
        "Question_score":0,
        "Question_tags":"python|linux|docker|anaconda|wandb",
        "Question_view_count":49,
        "Owner_creation_time":1340313730287,
        "Owner_last_access_time":1664073688613,
        "Owner_location":null,
        "Owner_reputation":11435,
        "Owner_up_votes":1807,
        "Owner_down_votes":299,
        "Owner_views":6472,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73642527",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71744288,
        "Question_title":"wandb getting logged without initiating",
        "Question_body":"<p>I do not want to use wandb. I don't even have an account. I am simply following <a href=\"https:\/\/colab.research.google.com\/github\/huggingface\/notebooks\/blob\/master\/examples\/summarization.ipynb#scrollTo=UmvbnJ9JIrJd\" rel=\"nofollow noreferrer\">this notebook<\/a> for finetuning. I am not running the 2nd and 3 cells because I do not want to push the model to the hub.<\/p>\n<p>However, when I do trainer.train() I get the following error : <a href=\"https:\/\/i.stack.imgur.com\/VPIZm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VPIZm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I don't understand where wandb.log is being called.\nI even tried os.environ[&quot;WANDB_DISABLED&quot;]  = &quot;true&quot; but I still get the error.\nPlease help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649110110203,
        "Question_score":0,
        "Question_tags":"huggingface-transformers|huggingface-tokenizers|fine-tune|wandb|huggingface",
        "Question_view_count":289,
        "Owner_creation_time":1499867951607,
        "Owner_last_access_time":1661234383987,
        "Owner_location":null,
        "Owner_reputation":307,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Question_last_edit_time":null,
        "Answer_body":"<p>posting the same message as <a href=\"https:\/\/github.com\/huggingface\/transformers\/issues\/16594\" rel=\"nofollow noreferrer\">over on <code>transformers<\/code><\/a>:<\/p>\n<hr \/>\n<p>You can turn off all external logger logging, including wandb logging by passing <code>report_to=&quot;none&quot;<\/code> in your <code>Seq2SeqTrainingArguments<\/code>.<\/p>\n<p>You might have noticed the following warning when setting up your TrainingArguments:<\/p>\n<pre><code>The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-)\n<\/code><\/pre>\n<p>Right now the default is to run all loggers that you have installed, so maybe you installed wandb on your machine since the last time you ran the script?<\/p>\n<p>If you would like to log with wandb, best practice would already be to start setting <code>report_to=&quot;wandb&quot;<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1649156563120,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71744288",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72166449,
        "Question_title":"Weights and Biases error: The wandb backend process has shutdown",
        "Question_body":"<p>running the colab linked below, I get the following error:<\/p>\n<p>&quot;The wandb backend process has shutdown&quot;<\/p>\n<p>I see nothing suspicious in the way the colab uses wandb and I couldn't find anyone with the same problem. Any help is greatly appreciated. I am using the latest version of wandb in colab.<\/p>\n<p>This is where I set up wandb:<\/p>\n<pre><code>if WANDB:\n    wandb.login()\n<\/code><\/pre>\n<p>and this is the part where I get the error:<\/p>\n<pre><code>#setup wandb if we're using it\n\nif WANDB:\n    experiment_name = os.environ.get(&quot;EXPERIMENT_NAME&quot;)\n    group = experiment_name if experiment_name != &quot;none&quot; else wandb.util.generate_id()\n\ncv_scores = []\noof_data_frame = pd.DataFrame()\nfor fold in range(1, config.folds + 1):\n    print(f&quot;Fold {fold}\/{config.folds}&quot;, end=&quot;\\n&quot;*2)\n    fold_directory = os.path.join(config.output_directory, f&quot;fold_{fold}&quot;)    \n    make_directory(fold_directory)\n    model_path = os.path.join(fold_directory, &quot;model.pth&quot;)\n    model_config_path = os.path.join(fold_directory, &quot;model_config.json&quot;)\n    checkpoints_directory = os.path.join(fold_directory, &quot;checkpoints\/&quot;)\n    make_directory(checkpoints_directory)\n    \n    #Data collators are objects that will form a batch by using a list of dataset elements as input.\n    collator = Collator(tokenizer=tokenizer, max_length=config.max_length)\n    \n    train_fold = train[~train[&quot;fold&quot;].isin([fold])]\n    train_dataset = Dataset(texts=train_fold[&quot;anchor&quot;].values, \n                            pair_texts=train_fold[&quot;target&quot;].values,\n                            contexts=train_fold[&quot;title&quot;].values,\n                            targets=train_fold[&quot;score&quot;].values, \n                            max_length=config.max_length,\n                            sep=tokenizer.sep_token,\n                            tokenizer=tokenizer)\n    \n    train_loader = DataLoader(dataset=train_dataset, \n                              batch_size=config.batch_size, \n                              num_workers=config.num_workers,\n                              pin_memory=config.pin_memory,\n                              collate_fn=collator,\n                              shuffle=True, \n                              drop_last=False)\n    \n    print(f&quot;Train samples: {len(train_dataset)}&quot;)\n    \n    validation_fold = train[train[&quot;fold&quot;].isin([fold])]\n    validation_dataset = Dataset(texts=validation_fold[&quot;anchor&quot;].values, \n                                 pair_texts=validation_fold[&quot;target&quot;].values,\n                                 contexts=validation_fold[&quot;title&quot;].values,\n                                 targets=validation_fold[&quot;score&quot;].values,\n                                 max_length=config.max_length,\n                                 sep=tokenizer.sep_token,\n                                 tokenizer=tokenizer)\n    \n    validation_loader = DataLoader(dataset=validation_dataset, \n                                   batch_size=config.batch_size*2, \n                                   num_workers=config.num_workers,\n                                   pin_memory=config.pin_memory,\n                                   collate_fn=collator,\n                                   shuffle=True, \n                                   drop_last=False)\n    \n    print(f&quot;Validation samples: {len(validation_dataset)}&quot;)\n\n\n    model = Model(**config.model)\n    \n    if not os.path.exists(model_config_path): \n        model.config.to_json_file(model_config_path)\n    \n    model_parameters = model.parameters()\n    optimizer = get_optimizer(**config.optimizer, model_parameters=model_parameters)\n    \n    training_steps = len(train_loader) * config.epochs\n    \n    if &quot;scheduler&quot; in config:\n        config.scheduler.parameters.num_training_steps = training_steps\n        config.scheduler.parameters.num_warmup_steps = training_steps * config.get(&quot;warmup&quot;, 0)\n        scheduler = get_scheduler(**config.scheduler, optimizer=optimizer, from_transformers=True)\n    else:\n        scheduler = None\n        \n    model_checkpoint = ModelCheckpoint(mode=&quot;min&quot;, \n                                       delta=config.delta, \n                                       directory=checkpoints_directory, \n                                       overwriting=True, \n                                       filename_format=&quot;checkpoint.pth&quot;, \n                                       num_candidates=1)\n\n\n    if WANDB:\n        wandb.init()\n        #wandb.init(group=group, name=f&quot;fold_{fold}&quot;, config=config)\n    \n    (train_loss, train_metrics), (validation_loss, validation_metrics, validation_outputs) = training_loop(model=model, \n                                                                                                           optimizer=optimizer, \n                                                                                                           scheduler=scheduler,\n                                                                                                           scheduling_after=config.scheduling_after,\n                                                                                                           train_loader=train_loader,\n                                                                                                           validation_loader=validation_loader,\n                                                                                                           epochs=config.epochs, \n                                                                                                           gradient_accumulation_steps=config.gradient_accumulation_steps, \n                                                                                                           gradient_scaling=config.gradient_scaling, \n                                                                                                           gradient_norm=config.gradient_norm, \n                                                                                                           validation_steps=config.validation_steps, \n                                                                                                           amp=config.amp,\n                                                                                                           debug=config.debug, \n                                                                                                           verbose=config.verbose, \n                                                                                                           device=config.device, \n                                                                                                           recalculate_metrics_at_end=True, \n                                                                                                           return_validation_outputs=True, \n                                                                                                           logger=&quot;tqdm&quot;)\n    \n    if WANDB:\n        wandb.finish()\n    \n    if config.save_model:\n        model_state = model.state_dict()\n        torch.save(model_state, model_path)\n        print(f&quot;Model's path: {model_path}&quot;)\n    \n    validation_fold[&quot;prediction&quot;] = validation_outputs.to(&quot;cpu&quot;).numpy()\n    oof_data_frame = pd.concat([oof_data_frame, validation_fold])\n    \n    cv_monitor_value = validation_loss if config.cv_monitor_value == &quot;loss&quot; else validation_metrics[config.cv_monitor_value]\n    cv_scores.append(cv_monitor_value)\n    \n    del model, optimizer, validation_outputs, train_fold, validation_fold\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    print(end=&quot;\\n&quot;*6)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1652063587210,
        "Question_score":6,
        "Question_tags":"python|wandb",
        "Question_view_count":1610,
        "Owner_creation_time":1622105241727,
        "Owner_last_access_time":1661260334760,
        "Owner_location":"Berlin, Deutschland",
        "Owner_reputation":81,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72166449",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73412851,
        "Question_title":"What is the meaning of 'config = wandb.config'?",
        "Question_body":"<p>I try to do the settings for a sweep for my Logistic regression model. I read the tutorials of wandb and cannot understand how to make the configurations and especially the meaning of <code>config=wandb.config<\/code> in the tutorials. I would really appreciate it if someone gave me a good explanation of the steps. Here is what I've done:<\/p>\n<pre><code>sweep_config = {\n    'method': 'grid'\n}\n\nmetric = {\n    'name': 'f1-score',\n    'goal': 'maximize'\n}\n\nsweep_config['metric'] = metric\n\nparameters = {\n    'penalty': {\n        'values': ['l2']\n    },\n    'C': {\n        'values': [0.01, 0.1, 1.0, 10.0, 100.0]\n    }\n}\n\nsweep_config['parameters'] = parameters\n<\/code><\/pre>\n<p>Then I create the yaml file:<\/p>\n<pre><code>stream = open('config.yaml', 'w')\nyaml.dump(sweep_config, stream) \n<\/code><\/pre>\n<p>Then it's time for training:<\/p>\n<pre><code>with wandb.init(project=WANDB_PROJECT_NAME):\n    config = wandb.config\n    \n    features = pd.read_csv('data\/x_features.csv')\n    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n\n    X_features = features = vectorizer.fit_transform(features['lemmatized_reason'])\n\n    y_labels = pd.read_csv('data\/y_labels.csv')\n\n    split_data = train_test_split(X_features, y_labels, train_size = 0.85, test_size = 0.15, stratify=y_labels)\n    features_train, labels_train = split_data[0], split_data[2]\n    features_test, labels_test = split_data[1], split_data[3]\n    \n    config = wandb.config\n    log_reg = LogisticRegression(\n        penalty=config.penalty,\n        C = config.C\n    )\n    \n    log_reg.fit(features_train, labels_train)\n    \n    labels_pred = log_reg.predict(features_test)\n    labels_proba = log_reg.predict_proba(features_test)\n    labels=list(map(str,y_labels['label'].unique()))\n    \n    # Visualize single plot\n    cm = wandb.sklearn.plot_confusion_matrix(labels_test, labels_pred, labels)\n    \n    score_f1 = f1_score(labels_test, labels_pred, average='weighted')\n    \n    sm = wandb.sklearn.plot_summary_metrics(\n    log_reg, features_train, labels_train, features_test, labels_test)\n    \n    roc = wandb.sklearn.plot_roc(labels_test, labels_proba)\n    \n    wandb.log({\n        &quot;f1-weighted-log-regression-tfidf-skf&quot;: score_f1, \n        &quot;roc-log-regression-tfidf-skf&quot;: roc, \n        &quot;conf-mat-logistic-regression-tfidf-skf&quot;: cm,\n        &quot;summary-metrics-logistic-regression-tfidf-skf&quot;: sm\n        })\n<\/code><\/pre>\n<p>And finally sweep_id and agent outside of <code>with<\/code> statement:<\/p>\n<pre><code>sweep_id = wandb.sweep(sweep_config, project=&quot;multiple-classifiers&quot;)\nwandb.agent(sweep_id)\n<\/code><\/pre>\n<p>There is something major I am missing here with this config thing, that I just cannot understand.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660891849347,
        "Question_score":0,
        "Question_tags":"python|wandb",
        "Question_view_count":61,
        "Owner_creation_time":1557474244067,
        "Owner_last_access_time":1663941664767,
        "Owner_location":null,
        "Owner_reputation":626,
        "Owner_up_votes":120,
        "Owner_down_votes":3,
        "Owner_views":140,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I work at Weights &amp; Biases. With wandb Sweeps, the idea is that wandb needs to be able to change the hyperparameters in the sweep.<\/p>\n<p>The below section where the hyperparameters are passed to <code>LogisticRegression<\/code> could also be re-written<\/p>\n<pre><code>config = wandb.config\nlog_reg = LogisticRegression(\n    penalty=config.penalty,\n    C = config.C\n)\n<\/code><\/pre>\n<p>like this:<\/p>\n<pre><code>log_reg = LogisticRegression(\n    penalty=wandb.config.penalty,\n    C = wandb.config.C\n)\n<\/code><\/pre>\n<p>However, I think you're missing defining a train function or train script, which needs to also be passed to wandb. With out it, your example above won't work.<\/p>\n<p>Below is a minimal example that should help. Hopefully the <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\" rel=\"nofollow noreferrer\">sweeps documentation<\/a> can also help.<\/p>\n<pre><code>import numpy as np \nimport random\nimport wandb\n\n#  Step 1: Define sweep config\nsweep_configuration = {\n    'method': 'random',\n    'name': 'sweep',\n    'metric': {'goal': 'maximize', 'name': 'val_acc'},\n    'parameters': \n    {\n        'batch_size': {'values': [16, 32, 64]},\n        'epochs': {'values': [5, 10, 15]},\n        'lr': {'max': 0.1, 'min': 0.0001}\n     }\n}\n\n#  Step 2: Initialize sweep by passing in config\nsweep_id = wandb.sweep(sweep_configuration)\n\ndef train_one_epoch(epoch, lr, bs): \n  acc = 0.25 + ((epoch\/30) +  (random.random()\/10))\n  loss = 0.2 + (1 - ((epoch-1)\/10 +  random.random()\/5))\n  return acc, loss\n\ndef evaluate_one_epoch(epoch): \n  acc = 0.1 + ((epoch\/20) +  (random.random()\/10))\n  loss = 0.25 + (1 - ((epoch-1)\/10 +  random.random()\/6))\n  return acc, loss\n\ndef train():\n    run = wandb.init()\n\n    #  Step 3: Use hyperparameter values from `wandb.config`\n    lr  =  wandb.config.lr\n    bs = wandb.config.batch_size\n    epochs = wandb.config.epochs\n\n    for epoch in np.arange(1, epochs):\n      train_acc, train_loss = train_one_epoch(epoch, lr, bs)\n      val_acc, val_loss = evaluate_one_epoch(epoch)\n\n      wandb.log({\n        'epoch': epoch, \n        'train_acc': train_acc,\n        'train_loss': train_loss, \n        'val_acc': val_acc, \n        'val_loss': val_loss\n      })\n\n#  Step 4: Launch sweep by making a call to `wandb.agent`\nwandb.agent(sweep_id, function=train, count=4)\n<\/code><\/pre>\n<p>Finally, can you share the link where you found the code above? Maybe we need to update some examples :)<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1660907847087,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73412851",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":65392269,
        "Question_title":"access logged values during runtime",
        "Question_body":"<p>How can I retrieve a logged value from wandb before the run was finished?<\/p>\n<pre><code>import os\nimport wandb\nwandb.init(project='someproject')\n\n\ndef loss_a():\n    # do_stuff and log:\n    wandb.log({&quot;loss_a&quot;: 1.0})\n    \ndef loss_b():\n    # do_stuff and log:\n    wandb.log({&quot;loss_b&quot;: 2.0})\n\nfor epoch in range(2):\n    loss_a()\n    loss_b()\n    \n    # somehow retrieve loss_a and loss_b and print them here:\n    print(f'loss_a={??}, loss_b={??}')\n\n<\/code><\/pre>\n<p>After run was finished I can find it with <code>wandb.Api<\/code> to get <code>run.history<\/code>. But it seems that before run was fininshed, accessing <code>run.history<\/code> doesn't work.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1608551470203,
        "Question_score":0,
        "Question_tags":"machine-learning|wandb",
        "Question_view_count":302,
        "Owner_creation_time":1304429242790,
        "Owner_last_access_time":1663848455777,
        "Owner_location":null,
        "Owner_reputation":3201,
        "Owner_up_votes":946,
        "Owner_down_votes":3,
        "Owner_views":100,
        "Question_last_edit_time":1661850170529,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65392269",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71195699,
        "Question_title":"wandb [SSL: WRONG_VERSION_NUMBER] error only thrown in Pycharm using python 3.8",
        "Question_body":"<p>I log the metrics of the training results on the wandb online server. This was working without any problems, till the beginning of this week. Since then i am suddenly unable to connect to the wandb online server and loggin the metrics isn't working anymore. I on Windows 10 using PyCharm with python version 3.8.<\/p>\n<p>The following exception is thrown:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\connectionpool.py&quot;, line 696, in urlopen\n    self._prepare_proxy(conn)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\connectionpool.py&quot;, line 964, in _prepare_proxy\n    conn.connect()\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\connection.py&quot;, line 364, in connect\n    conn = self._connect_tls_proxy(hostname, conn)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\connection.py&quot;, line 501, in _connect_tls_proxy\n    socket = ssl_wrap_socket(\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\util\\ssl_.py&quot;, line 453, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\util\\ssl_.py&quot;, line 495, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\ssl.py&quot;, line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\ssl.py&quot;, line 1040, in _create\n    self.do_handshake()\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\ssl.py&quot;, line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLError: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\requests\\adapters.py&quot;, line 439, in send\n    resp = conn.urlopen(\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\connectionpool.py&quot;, line 755, in urlopen\n    retries = retries.increment(\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\urllib3\\util\\retry.py&quot;, line 574, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.wandb.ai', port=443): Max retries exceeded with url: \/graphql (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\wandb\\sdk\\lib\\retry.py&quot;, line 102, in __call__\n    result = self._call_fn(*args, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\wandb\\sdk\\internal\\internal_api.py&quot;, line 132, in execute\n    return self.client.execute(*args, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\gql\\client.py&quot;, line 52, in execute\n    result = self._get_result(document, *args, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\gql\\client.py&quot;, line 60, in _get_result\n    return self.transport.execute(document, *args, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\gql\\transport\\requests.py&quot;, line 38, in execute\n    request = requests.post(self.url, **post_args)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\requests\\api.py&quot;, line 117, in post\n    return request('post', url, data=data, json=json, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\requests\\api.py&quot;, line 61, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\requests\\sessions.py&quot;, line 542, in request\n    resp = self.send(prep, **send_kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\requests\\sessions.py&quot;, line 655, in send\n    r = adapter.send(request, **kwargs)\n  File &quot;C:\\Users\\Miniconda3\\envs\\smartvision\\lib\\site-packages\\requests\\adapters.py&quot;, line 514, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='api.wandb.ai', port=443): Max retries exceeded with url: \/graphql (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1131)')))\nwandb: Network error (SSLError), entering retry loop.\nwandb: W&amp;B API key is configured (use `wandb login --relogin` to force relogin)\nwandb: Network error (SSLError), entering retry loop.\n<\/code><\/pre>\n<p>Strange thing is, if i execute the run configuration used in pycharm as a command from it's built in terminal it works fine. Since the same virtual environment is used on the built in terminal i don't understand why this exceptions is thrown if using the run configuration in Pycharm. What am i missing here?<\/p>\n<p>Any help would be appreciated.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_time":1645369500377,
        "Question_score":1,
        "Question_tags":"python|python-3.x|ssl|pycharm|wandb",
        "Question_view_count":259,
        "Owner_creation_time":1644055951303,
        "Owner_last_access_time":1651943996327,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71195699",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71257152,
        "Question_title":"How to show wandb training progress from run folder",
        "Question_body":"<p>After training neural networks with wandb as the logger, I received a link to show the training results and a folder named &quot;run-...&quot;, I assume that is the logging of the training process. Now I don't have that link, how to show the wandb training process from run folder?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645730259610,
        "Question_score":0,
        "Question_tags":"python|machine-learning|deep-learning|pytorch|wandb",
        "Question_view_count":86,
        "Owner_creation_time":1504450759857,
        "Owner_last_access_time":1660667476990,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":83,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":1645730901420,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71257152",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":63469762,
        "Question_title":"Weights&Biases Sweep Keras K-Fold Validation",
        "Question_body":"<p>I'm using Weights&amp;Biases Cloud-based sweeps with Keras.\nSo first i create a new Sweep within a W&amp;B Project with a config like following:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>description: LSTM Model\nmethod: random\nmetric:\n  goal: maximize\n  name: val_accuracy\nname: LSTM-Sweep\nparameters:\n  batch_size:\n    distribution: int_uniform\n    max: 128\n    min: 32\n  epochs:\n    distribution: constant\n    value: 200\n  node_size1:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size2:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size3:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size4:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size5:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  num_layers:\n    distribution: categorical\n    values:\n    - 1\n    - 2\n    - 3\n  optimizer:\n    distribution: categorical\n    values:\n    - Adam\n    - Adamax\n    - Adagrad\n  path:\n    distribution: constant\n    value: &quot;.\/path\/to\/data\/&quot;\nprogram: sweep.py\nproject: SLR\n<\/code><\/pre>\n<p>My <code>sweep.py<\/code> file looks something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># imports\ninit = wandb.init(project=&quot;my-project&quot;, reinit=True)\nconfig = wandb.config\n\ndef main():\n    skfold = StratifiedKFold(n_splits=5, \n    shuffle=True, random_state=7)\n    cvscores = []\n    group_id = wandb.util.generate_id()\n    X,y = # load data\n    i = 0\n    for train, test in skfold.split(X,y):\n        i=i+1\n        run = wandb.init(group=group_id, reinit=True, name=group_id+&quot;#&quot;+str(i))\n        model = # build model\n        model.fit([...], WandBCallback())\n        cvscores.append([...])\n        wandb.join()\n\nif __name__ == &quot;__main__&quot;:\n    main()\n<\/code><\/pre>\n<p>Starting this with the <code>wandb agent<\/code> command within the folder of <code>sweep.py<\/code>.<\/p>\n<p>What i experienced with this setup is, that with the first wandb.init() call a new run is initialized. Okay, i could just remove that. But when calling wandb.init() for the second time it seems to lose track of the sweep it is running in. Online an empty run is listed in the sweep (because of the first wandb.init() call), all other runs are listed inside the project, but not in the sweep.<\/p>\n<p>My goal is to have a run for each fold of the k-Fold cross-validation. At least i thought this would be the right way of doing this.\nIs there a different approach to combine sweeps with keras k-fold cross validation?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597757510213,
        "Question_score":2,
        "Question_tags":"python|keras|k-fold|wandb",
        "Question_view_count":1043,
        "Owner_creation_time":1486549300030,
        "Owner_last_access_time":1620240688720,
        "Owner_location":"Germany",
        "Owner_reputation":45,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1661849871016,
        "Answer_body":"<p>We put together an example of how to accomplish k-fold cross validation:<\/p>\n<p><a href=\"https:\/\/github.com\/wandb\/examples\/tree\/master\/examples\/wandb-sweeps\/sweeps-cross-validation\" rel=\"nofollow noreferrer\">https:\/\/github.com\/wandb\/examples\/tree\/master\/examples\/wandb-sweeps\/sweeps-cross-validation<\/a><\/p>\n<p>The solution requires some contortions for the wandb library to spawn multiple jobs on behalf of a launched sweep job.<\/p>\n<p>The basic idea is:<\/p>\n<ul>\n<li>The agent requests a new set of parameters from the cloud hosted parameter server.  This is the run called <code>sweep_run<\/code> in the main function.<\/li>\n<li>Send information about what the folds should process over a multiprocessing queue to waiting processes<\/li>\n<li>Each spawned process logs to their own run, organized with group and job_type to enable auto-grouping in the UI<\/li>\n<li>When the process is finished, it sends the primary metric over a queue to the parent sweep run<\/li>\n<li>The sweep run reads metrics from the child runs and logs it to the sweep run so that the sweep can use that result to impact future parameter choices and\/or hyperband early termination optimizations<\/li>\n<\/ul>\n<p>Example visualizations of the sweep and k-fold grouping can be seen here:<\/p>\n<ul>\n<li>Sweep: <a href=\"https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/sweeps\/vp0fsvku\" rel=\"nofollow noreferrer\">https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/sweeps\/vp0fsvku<\/a><\/li>\n<li>K-fold Grouping: <a href=\"https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/groups\/vp0fsvku\" rel=\"nofollow noreferrer\">https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/groups\/vp0fsvku<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1598032113043,
        "Answer_score":6.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1599772735680,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63469762",
        "Question_exclusive_tag":"Weights & Biases"
    }
]