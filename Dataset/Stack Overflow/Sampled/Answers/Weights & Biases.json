[
    {
        "Question_id":70644326,
        "Question_title":"wandb.plot.line does not work and it just shows a table",
        "Question_body":"<p>I used this example that was provided by WandB. However, the web interface just shows a table instead of a figure.<\/p>\n<pre><code>data = [[i, random.random() + math.sin(i \/ 10)] for i in range(100)]\n        table = wandb.Table(data=data, columns=[&quot;step&quot;, &quot;height&quot;])\n        wandb.log({'line-plot1': wandb.plot.line(table, &quot;step&quot;, &quot;height&quot;)})\n<\/code><\/pre>\n<p>This is a screenshot from WandB's web interface:\n<a href=\"https:\/\/i.stack.imgur.com\/INmPU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/INmPU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Also, I have the same problem with other kinds of figures and charts that use a table.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-09 18:36:40.317 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|deep-learning|pytorch|wandb",
        "Question_view_count":184,
        "Owner_creation_date":"2015-10-24 20:44:04.55 UTC",
        "Owner_last_access_date":"2022-09-24 23:10:59.603 UTC",
        "Owner_reputation":331,
        "Owner_up_votes":113,
        "Owner_down_votes":3,
        "Owner_views":40,
        "Answer_body":"<blockquote>\n<p>X-Post from the wandb forum<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744\" rel=\"nofollow noreferrer\">https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744<\/a><\/p>\n<p>If you click the section called \u201cCustom Charts\u201d above the Table, it\u2019ll show the line plot that you\u2019ve logged.<\/p>\n<p>Logging the Table also is expected behaviour, because this will allow users to interactively explore the logged data in a W&amp;B Table after logging it.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-10 10:14:57.277 UTC",
        "Answer_score":0.0,
        "Owner_location":"Iran, Canada",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70644326",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":67202711,
        "Question_title":"How to get multiple lines exported to wandb",
        "Question_body":"<p>I am using the library weights and  biases. My model outputs a curve (a time series). I'd like to see how this curve changes throughout training. So, I'd need some kind of slider where I can select epoch and it shows me the curve for that epoch. It could be something very similar to what it's done with histograms (it shows an image of the histograms across epochs and when you hover it display the histogram corresponding to that epoch). Is there a way to do this or something similar using <code>wandb<\/code>?<\/p>\n<p>Currently my code looks like this:<\/p>\n<pre><code>for epoch in range(epochs):\n   output = model(input)\n   #output is shape (37,40) (lenght 40 and I have 37 samples)\n   #it's enough to plot the first sample\n   xs = torch.arange(40).unsqueeze(dim=1)\n   ys = output[0,:].unsqueeze(dim=1)\n   wandb.log({&quot;line&quot;: wandb.plot.line_series(xs=xs, ys=ys,title=&quot;Out&quot;)}, step=epoch)\n<\/code><\/pre>\n<p>I'd appreciate any help! Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-21 20:00:14.083 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|pytorch|wandb",
        "Question_view_count":840,
        "Owner_creation_date":"2019-12-30 19:30:07.07 UTC",
        "Owner_last_access_date":"2022-09-19 15:45:51.91 UTC",
        "Owner_reputation":422,
        "Owner_up_votes":72,
        "Owner_down_votes":3,
        "Owner_views":66,
        "Answer_body":"<p>You can use <code>wandb.log()<\/code> with matplotlib. Create your plot using matplotlib:<\/p>\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(0, 50)\nfor i in range(1, 4):\n    fig, ax = plt.subplots()\n    y = x ** i\n    ax.plot(x, y)\n    wandb.log({'chart': ax})\n<\/code><\/pre>\n<p>Then when you look on your wandb dashboard for the run, you will see the plot rendered as plotly plot. Click the gear in the upper left hand corner to see a slider that lets you slide over training steps and see the plot at each step.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-05-07 22:35:00.467 UTC",
        "Answer_score":3.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67202711",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69640534,
        "Question_title":"How to log artifacts in wandb while using saimpletransformers?",
        "Question_body":"<p>I am creating a Question Answering model using <a href=\"https:\/\/simpletransformers.ai\/docs\/qa-specifics\/\" rel=\"nofollow noreferrer\">simpletransformers<\/a>. I would also like to use wandb to track model artifacts. As I understand from <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/other\/simpletransformers\" rel=\"nofollow noreferrer\">wandb docs<\/a>, there is an integration touchpoint for simpletransformers but there is no mention of logging artifacts.<\/p>\n<p>I would like to log artifacts generated at the train, validation, and test phase such as train.json, eval.json, test.json, output\/nbest_predictions_test.json and best performing model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-20 04:55:43.473 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"nlp-question-answering|simpletransformers|wandb",
        "Question_view_count":53,
        "Owner_creation_date":"2018-06-12 01:08:24.783 UTC",
        "Owner_last_access_date":"2021-10-29 05:38:36.997 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":"<p>Currently simpleTransformers doesn't support logging artifacts within the training\/testing scripts. But you can do it manually:<\/p>\n<pre><code>import os \n\nwith wandb.init(id=model.wandb_run_id, resume=&quot;allow&quot;, project=wandb_project) as training_run:\n    for dir in sorted(os.listdir(&quot;outputs&quot;)):\n        if &quot;checkpoint&quot; in dir:\n            artifact = wandb.Artifact(&quot;model-checkpoints&quot;, type=&quot;checkpoints&quot;)\n            artifact.add_dir(&quot;outputs&quot; + &quot;\/&quot; + dir)\n            training_run.log_artifact(artifact)\n<\/code><\/pre>\n<p>For more info, you can follow along with the W&amp;B notebook in the SimpleTransofrmer's README.md<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-10-20 11:26:44.573 UTC",
        "Answer_score":0.0,
        "Owner_location":"Ahmedabad, Gujarat, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69640534",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71916901,
        "Question_title":"WANDB Getting a run id based on tag",
        "Question_body":"<h1>Context<\/h1>\n<p>Hi!<\/p>\n<p>In <code>wandb<\/code> I can download a model based on a tag (<code>prod<\/code> for example), but I would like to also get all metrics associated to that run by using tags.<\/p>\n<p>The problem is that I don't know how to a get specific run ID based a tag.<\/p>\n<h1>Example<\/h1>\n<p>Using the code bellow we can extract a run summary metrics, but setting run IDs is setting me back.<\/p>\n<p>So if I can get run IDs based on tag or just explicitly download metrics  with another API call, like with a special sintax in <code>api.run<\/code>, that would be great! In the code example bellow I would like to use the <code>what_i_want_to_use<\/code> string to call the API instead of <code>what_i_use<\/code>.<\/p>\n<pre><code>import wandb\nfrom ast import literal_eval\napi = wandb.Api()\n\nwhat_i_use = &quot;team_name\/project_name\/runID_h3h3h4h4h4h4&quot;\n# what_i_want_to_use = &quot;team_name\/project_name\/artifact_name\/prod_tag&quot;\n\n# run is specified by &lt;entity&gt;\/&lt;project&gt;\/&lt;run_id&gt;\nrun = api.run(what_i_use)\n\n\n# save the metrics for the run to a csv file\nmetrics_dataframe = run.summary\nprint(metrics_dataframe['a_summary_metric'])\n\n<\/code><\/pre>\n<p>By running through the docs I didn't find any solution so far. Any ideias?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/mWl3I.png\" rel=\"nofollow noreferrer\">wandb public api run details<\/a><\/p>\n<p>Thanks for reading!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-18 20:45:26.207 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|metadata|wandb",
        "Question_view_count":645,
        "Owner_creation_date":"2015-10-12 18:52:50.627 UTC",
        "Owner_last_access_date":"2022-09-24 18:37:52.477 UTC",
        "Owner_reputation":5,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>It is possible to filter runs by tags as well. You can read more about it <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/public-api\/api#runs\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<pre><code>You can filter by config.*, summary_metrics.*, tags, state, entity, createdAt, etc.\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-04-20 12:41:28.357 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71916901",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":70637798,
        "Question_title":"wandb: artifact.add_reference() option to add specific (not current) versionId or ETag to stop the need for re-upload to s3?",
        "Question_body":"<p>I feel like this should be possible, but I looked through the wandb SDK code and I can't find an easy\/logical way to do it. It <em>might<\/em> be possible to hack it by modifying the manifest entries at some point later (but maybe before the artifact is logged to wandb as then the manifest and the entries might be locked)? I saw things like this in the SDK code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>version = manifest_entry.extra.get(&quot;versionID&quot;)\netag = manifest_entry.extra.get(&quot;etag&quot;)\n<\/code><\/pre>\n<p>So, I figure we can probably edit those?<\/p>\n<h2>UPDATE<\/h2>\n<p>So, I tried to hack it together with something like this and it works but it feels wrong:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport wandb\nimport boto3\nfrom wandb.util import md5_file\n\nENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nPROJECT = os.environ.get(&quot;WANDB_PROJECT&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\napi = api = wandb.Api(overrides={&quot;entity&quot;: ENTITY, &quot;project&quot;: ENTITY})\nrun = wandb.init(entity=ENTITY, project=PROJECT, job_type=&quot;test upload&quot;)\nfile = &quot;admin2Codes.txt&quot;  # &quot;admin1CodesASCII.txt&quot; # (both already on s3 with a couple versions)\nartifact = wandb.Artifact(&quot;test_data&quot;, type=&quot;dataset&quot;)\n\n# modify one of the local files so it has a new md5hash etc.\nwith open(file, &quot;a&quot;) as f:\n    f.write(&quot;new_line_1\\n&quot;)\n\n# upload local file to s3\nlocal_file_path = file\ns3_url = f&quot;s3:\/\/bucket\/prefix\/{file}&quot;\ns3_url_arr = s3_url.replace(&quot;s3:\/\/&quot;, &quot;&quot;).split(&quot;\/&quot;)\ns3_bucket = s3_url_arr[0]\nkey = &quot;\/&quot;.join(s3_url_arr[1:])\n\ns3_client = boto3.client(&quot;s3&quot;)\nfile_digest = md5_file(local_file_path)\ns3_client.upload_file(\n    local_file_path,\n    s3_bucket,\n    key,\n    # save the md5_digest in metadata,\n    # can be used later to only upload new files to s3,\n    # as AWS doesn't digest the file consistently in the E-tag\n    ExtraArgs={&quot;Metadata&quot;: {&quot;md5_digest&quot;: file_digest}},\n)\nhead_response = s3_client.head_object(Bucket=s3_bucket, Key=key)\nversion_id: str = head_response[&quot;VersionId&quot;]\nprint(version_id)\n\n# upload a link\/ref to this s3 object in wandb:\nartifact.add_reference(s3_dir)\n# at this point we might be able to modify the artifact._manifest.entries and each entry.extra.get(&quot;etag&quot;) etc.?\nprint([(name, entry.extra) for name, entry in artifact._manifest.entries.items()])\n# set these to an older version on s3 that we know we want (rather than latest) - do this via wandb public API:\ndataset_v2 = api.artifact(f&quot;{ENTITY}\/{PROJECT}\/test_data:v2&quot;, type=&quot;dataset&quot;)\n# artifact._manifest.add_entry(dataset_v2.manifest.entries[&quot;admin1CodesASCII.txt&quot;])\nartifact._manifest.entries[&quot;admin1CodesASCII.txt&quot;] = dataset_v2.manifest.entries[\n    &quot;admin1CodesASCII.txt&quot;\n]\n# verify that it did change:\nprint([(name, entry.extra) for name, entry in artifact._manifest.entries.items()])\n\nrun.log_artifact(artifact)  # at this point the manifest is locked I believe?\nartifact.wait()  # wait for upload to finish (blocking - but should be very quick given it is just an s3 link)\nprint(artifact.name)\nrun_id = run.id\nrun.finish()\ncurr_run = api.run(f&quot;{ENTITY}\/{PROJECT}\/{run_id}&quot;)\nused_artifacts = curr_run.used_artifacts()\nlogged_artifacts = curr_run.logged_artifacts()\n<\/code><\/pre>\n<p>Am I on the right track here? I guess the other workaround is to make a copy on s3 (so that older version is the latest again) but I wanted to avoid this as the 1 file that I want to use an old version of is a large NLP model and the only files I want to change are small config.json files etc. (so seems very wasteful to upload all files again).<\/p>\n<p>I was also wondering if when I copy an old version of an object back into the same key in the bucket if that creates a real copy or just like a pointer to the same underlying object. Neither boto3 nor AWS documentation makes that clear - although it seems like it is a proper copy.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-09 01:30:04.6 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-01-25 14:09:29.043 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-s3|boto3|wandb",
        "Question_view_count":121,
        "Owner_creation_date":"2018-05-15 07:21:25.18 UTC",
        "Owner_last_access_date":"2022-09-23 17:35:59.473 UTC",
        "Owner_reputation":3944,
        "Owner_up_votes":574,
        "Owner_down_votes":47,
        "Owner_views":217,
        "Answer_body":"<p>I think I found the correct way to do it now:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport wandb\nimport boto3\nfrom wandb.util import md5_file\n\nENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nPROJECT = os.environ.get(&quot;WANDB_PROJECT&quot;)\n\n\ndef wandb_update_only_some_files_in_artifact(\n    existing_artifact_name: str,\n    new_s3_file_urls: list[str],\n    entity: str = ENTITY,\n    project: str = PROJECT,\n) -&gt; Artifact:\n    &quot;&quot;&quot;If you want to just update a config.json file for example,\n    but the rest of the artifact can remain the same, then you can\n    use this functions like so:\n    wandb_update_only_some_files_in_artifact(\n        &quot;old_artifact:v3&quot;,\n        [&quot;s3:\/\/bucket\/prefix\/config.json&quot;],\n    )\n    and then all the other files like model.bin will be the same as in v3,\n    even if there was a v4 or v5 in between (as the v3 VersionIds are used)\n\n    Args:\n        existing_artifact_name (str): name with version like &quot;old_artifact:v3&quot;\n        new_s3_file_urls (list[str]): files that should be updated\n        entity (str, optional): wandb entity. Defaults to ENTITY.\n        project (str, optional): wandb project. Defaults to PROJECT.\n\n    Returns:\n        Artifact: the new artifact object\n    &quot;&quot;&quot;\n    api = wandb.Api(overrides={&quot;entity&quot;: entity, &quot;project&quot;: project})\n    old_artifact = api.artifact(existing_artifact_name)\n    old_artifact_name = re.sub(r&quot;:v\\d+$&quot;, &quot;&quot;, old_artifact.name)\n    with wandb.init(entity=entity, project=project) as run:\n        new_artifact = wandb.Artifact(old_artifact_name, type=old_artifact.type)\n\n        s3_file_names = [s3_url.split(&quot;\/&quot;)[-1] for s3_url in new_s3_file_urls]\n        # add the new ones:\n        for s3_url, filename in zip(new_s3_file_urls, s3_file_names):\n            new_artifact.add_reference(s3_url, filename)\n        # add the old ones:\n        for filename, entry in old_artifact.manifest.entries.items():\n            if filename in s3_file_names:\n                continue\n            new_artifact.add_reference(entry, filename)\n            # this also works but feels hackier:\n            # new_artifact._manifest.entries[filename] = entry\n\n        run.log_artifact(new_artifact)\n        new_artifact.wait()  # wait for upload to finish (blocking - but should be very quick given it is just an s3 link)\n        print(new_artifact.name)\n        print(run.id)\n    return new_artifact\n\n\n# usage:\nlocal_file_path = &quot;config.json&quot; # modified file\ns3_url = &quot;s3:\/\/bucket\/prefix\/config.json&quot;\ns3_url_arr = s3_url.replace(&quot;s3:\/\/&quot;, &quot;&quot;).split(&quot;\/&quot;)\ns3_bucket = s3_url_arr[0]\nkey = &quot;\/&quot;.join(s3_url_arr[1:])\n\ns3_client = boto3.client(&quot;s3&quot;)\nfile_digest = md5_file(local_file_path)\ns3_client.upload_file(\n    local_file_path,\n    s3_bucket,\n    key,\n    # save the md5_digest in metadata,\n    # can be used later to only upload new files to s3,\n    # as AWS doesn't digest the file consistently in the E-tag\n    ExtraArgs={&quot;Metadata&quot;: {&quot;md5_digest&quot;: file_digest}},\n)\n\nwandb_update_only_some_files_in_artifact(\n    &quot;old_artifact:v3&quot;,\n    [&quot;s3:\/\/bucket\/prefix\/config.json&quot;],\n)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-25 13:57:32.817 UTC",
        "Answer_score":0.0,
        "Owner_location":"Munich, Germany",
        "Answer_last_edit_date":"2022-01-25 14:10:13.377 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70637798",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71075704,
        "Question_title":"how to login wandb with another acount using colab?",
        "Question_body":"<p>For example, I have A, B acounts.<\/p>\n<p>First, I log in Google Colab with A account.\nand I want to log in wandb with B acounts. ( using !wandb login )\nis it possible??<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-11 05:22:37.917 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"authentication|google-colaboratory|wandb",
        "Question_view_count":316,
        "Owner_creation_date":"2022-02-11 05:19:23.937 UTC",
        "Owner_last_access_date":"2022-04-06 05:03:40 UTC",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>You can you the following commands to force a relogin:<\/p>\n<ul>\n<li>from terminal<\/li>\n<\/ul>\n<pre><code>wandb login --relogin\n<\/code><\/pre>\n<ul>\n<li>Using the API:<\/li>\n<\/ul>\n<pre><code>import wandb\nwandb.login(relogin=True)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-02-11 06:09:10.243 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71075704",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72731861,
        "Question_title":"Hyperparameter tunning with wandb - CommError: Sweep user not valid when trying to initial the sweep",
        "Question_body":"<p>I'mt rying to use wandb for hyperparameter tunning as described in <a href=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/pytorch\/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb?authuser=1#scrollTo=hoAi-idR1DQk\" rel=\"nofollow noreferrer\">this notebook<\/a> (but using my dataframe and trying to do it on random forest regressor instead).<\/p>\n<p>I'm trying to initial the sweep but I get the error:<\/p>\n<pre><code>sweep_configuration = {\n    &quot;name&quot;: &quot;test-project&quot;,\n    &quot;method&quot;: &quot;random&quot;,\n    &quot;entity&quot;:&quot;my_name&quot;\u05ea\n    &quot;metric&quot;: {\n        &quot;name&quot;: &quot;loss&quot;,\n        &quot;goal&quot;: &quot;minimize&quot;\n    }\n    \n}\n\nparameters_dict = {\n    'n_estimators': {\n        'values': [100,200,300]\n        },\n    'max_depth': {\n        'values': [4,7,10,14]\n        },\n    'min_samples_split': {\n          'values': [2,4,8]\n        },\n    \n    'min_samples_leaf': {\n          'values': [2,4,8]\n        },\n    \n    \n    'max_features': {\n          'values': [1,7,10]\n        },\n\n    }\n\nsweep_configuration['parameters'] = parameters_dict\n\nsweep_id = wandb.sweep(sweep_configuration)\n\n\n<\/code><\/pre>\n<blockquote>\n<p>400 response executing GraphQL. {&quot;errors&quot;:[{&quot;message&quot;:&quot;Sweep user not\nvalid&quot;,&quot;path&quot;:[&quot;upsertSweep&quot;]}],&quot;data&quot;:{&quot;upsertSweep&quot;:null}} wandb:\nERROR Error while calling W&amp;B API: Sweep user not valid (&lt;Response\n[400]&gt;)<br \/>\nCommError: Sweep user not valid<\/p>\n<\/blockquote>\n<p>My end goal : to inital the sweep<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-23 14:24:42.873 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-06-26 09:10:48.443 UTC",
        "Question_score":1,
        "Question_tags":"python|hyperparameters|mlops|wandb",
        "Question_view_count":182,
        "Owner_creation_date":"2019-10-28 09:51:58.027 UTC",
        "Owner_last_access_date":"2022-09-20 12:29:28.963 UTC",
        "Owner_reputation":1387,
        "Owner_up_votes":955,
        "Owner_down_votes":16,
        "Owner_views":224,
        "Answer_body":"<p>Two things to try:<\/p>\n<ul>\n<li><p>Like in the notebook, you should pass <code>project=&quot;your-project-name&quot;<\/code> like <code>wandb.sweep(sweep_configuration, project=&quot;your-project-name&quot;)<\/code><\/p>\n<\/li>\n<li><p>Have you logged in to W&amp;B (using <code>wandb.login()<\/code>)?<\/p>\n<\/li>\n<\/ul>\n<p>Finally, once you've successfully created the sweep, you should pass the <code>sweep_id<\/code> and your function (here <code>train<\/code>) like:\n<code>wandb.agent(sweep_id, train, count=5)<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-27 11:32:08.48 UTC",
        "Answer_score":1.0,
        "Owner_location":"Israel",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72731861",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72729259,
        "Question_title":"wandb.Table raises error: AssertionError: columns argument expects a `list` object",
        "Question_body":"<p>I'm very beginner with wandb , so this is very basic question.\nI have dataframe which has my x features and y values.\nI'm tryin to follow <a href=\"https:\/\/docs.wandb.ai\/examples\" rel=\"nofollow noreferrer\">this tutorial<\/a>  to train model from my pandas dataframe . However, when I try to create wandb table from my pandas dataframe, I get an error:<\/p>\n<pre><code>\nwandb.init(project='my-xgb', config={'lr': 0.01})\n\n#the log didn't work  so I haven't run it at the moment (the log 'loss') \n#wandb.log({'loss': loss, ...})\n\n\n# Create a W&amp;B Table with your pandas dataframe\ntable = wandb.Table(df1)\n<\/code><\/pre>\n<blockquote>\n<p>AssertionError: columns argument expects a <code>list<\/code> object<\/p>\n<\/blockquote>\n<p>I have no idea why is this happen, and why it excpect a list. In the tutorial it doesn't look like the dataframe is list.<\/p>\n<p>My end goal - to be able to create wandb table.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-23 11:18:13.28 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|pandas|dataframe|wandb",
        "Question_view_count":63,
        "Owner_creation_date":"2019-10-28 09:51:58.027 UTC",
        "Owner_last_access_date":"2022-09-20 12:29:28.963 UTC",
        "Owner_reputation":1387,
        "Owner_up_votes":955,
        "Owner_down_votes":16,
        "Owner_views":224,
        "Answer_body":"<p><strong>Short answer<\/strong>: <code>table = wandb.Table(dataframe=my_df)<\/code>.<\/p>\n<p>The explanation of your specific case is at the bottom.<\/p>\n<hr \/>\n<p><strong>Minimal example<\/strong> of using <code>wandb.Table<\/code> with a DataFrame:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import wandb\nimport pandas as pd\n\niris_path = 'https:\/\/raw.githubusercontent.com\/mwaskom\/seaborn-data\/master\/iris.csv'\niris = pd.read_csv(iris_path)\ntable = wandb.Table(dataframe=iris)\nwandb.log({'dataframe_in_table': table})\n<\/code><\/pre>\n<p>(Here the dataset is called the Iris dataset that consists of &quot;3 different types of irises\u2019 (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray&quot;)<\/p>\n<p>There are two ways of creating W&amp;B <code>Table<\/code>s according to <a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/log-tables#create-tables\" rel=\"nofollow noreferrer\">the official documentation<\/a>:<\/p>\n<ul>\n<li><strong>List of Rows<\/strong>: Log named columns and rows of data. For example: <code>wandb.Table(columns=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], data=[[&quot;1a&quot;, &quot;1b&quot;, &quot;1c&quot;], [&quot;2a&quot;, &quot;2b&quot;, &quot;2c&quot;]])<\/code> generates a table with two rows and three columns.<\/li>\n<li><strong>Pandas DataFrame<\/strong>: Log a DataFrame using <code>wandb.Table(dataframe=my_df)<\/code>. Column names will be extracted from the DataFrame.<\/li>\n<\/ul>\n<hr \/>\n<p><strong>Explanation<\/strong>: Why <code>table = wandb.Table(my_df)<\/code> gives error &quot;columns argument expects a <code>list<\/code> object&quot;? Because <code>wandb.Table<\/code>'s init function looks like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def __init__(\n        self,\n        columns=None,\n        data=None,\n        rows=None,\n        dataframe=None,\n        dtype=None,\n        optional=True,\n        allow_mixed_types=False,\n    ):\n<\/code><\/pre>\n<p>If one passes a DataFrame without telling it's a DataFrame, <code>wandb.Table<\/code> will assume the argument is <code>columns<\/code>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-06-23 11:28:00.023 UTC",
        "Answer_score":2.0,
        "Owner_location":"Israel",
        "Answer_last_edit_date":"2022-06-23 12:04:19.24 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72729259",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69920078,
        "Question_title":"What does \"save_graph\" keyword in WandbCallback mean?",
        "Question_body":"<p>I'm using Weights and Biases to track my deep learning models. To monitor everything I use the <code>WandbCallback<\/code> in <code>.fit<\/code>.\nIn the <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/integrations\/keras\/wandbcallback\" rel=\"nofollow noreferrer\">WandbCallback documentation<\/a> there is the keyword <code>save_graph<\/code> which defaults to True. The description is very brief and I wondered what that saved graph is and what it's for? Is saving a graph a costly operation? For what is it needed? (like does it complement something else, like saving the best model?)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-10 20:51:54.79 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow2.0|tf.keras|wandb",
        "Question_view_count":60,
        "Owner_creation_date":"2020-09-16 23:36:59.45 UTC",
        "Owner_last_access_date":"2022-09-10 18:03:54.53 UTC",
        "Owner_reputation":62,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>That is used to create log a <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/data-types\/graph\" rel=\"nofollow noreferrer\">wandb.Graph<\/a> of the model. This class is typically used for saving and diplaying neural net models. It represents the graph as an array of nodes and edges. The nodes can have labels that can be visualized by wandb. Here's an example of the graph that it produces: <a href=\"https:\/\/wandb.ai\/l2k2\/keras-examples\/runs\/ieqy2e9h\/model\" rel=\"nofollow noreferrer\">https:\/\/wandb.ai\/l2k2\/keras-examples\/runs\/ieqy2e9h\/model<\/a><\/p>\n<p>Here's the code that does that within the callback. <a href=\"https:\/\/github.com\/wandb\/client\/blob\/1609f82c84e2244ed8fe62c746099d2094bd746a\/wandb\/integration\/keras\/keras.py#L552\" rel=\"nofollow noreferrer\">https:\/\/github.com\/wandb\/client\/blob\/1609f82c84e2244ed8fe62c746099d2094bd746a\/wandb\/integration\/keras\/keras.py#L552<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-11-11 14:51:29.973 UTC",
        "Answer_score":1.0,
        "Owner_location":"Germany",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69920078",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72001154,
        "Question_title":"How to prevent Weights & Biases from saving best model parameters",
        "Question_body":"<p>I am using Weights &amp; Biases (<a href=\"https:\/\/wandb.ai\/\" rel=\"nofollow noreferrer\">link<\/a>) to manage hyperparameter optimization and log the results. I am training using Keras with a Tensorflow backend, and I am using the out-of-the-box logging functionality of Weights &amp; Biases, in which I run<\/p>\n<pre><code>wandb.init(project='project_name', entity='username', config=config)\n<\/code><\/pre>\n<p>and then add a <code>WandbCallback()<\/code> to the callbacks of <code>classifier.fit()<\/code>. By default, Weights &amp; Biases appears to save the model parameters (i.e., the model's weights and biases) and store them in the cloud. This eats up my account's storage quota, and it is unnecessary --- I only care about tracking the model loss\/accuracy as a function of the hyperparameters.<\/p>\n<p>Is it possible for me to train a model and log the loss and accuracy using Weights &amp; Biases, but not store the model parameters in the cloud? How can I do this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-25 14:27:15.733 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"python|tensorflow|machine-learning|keras|wandb",
        "Question_view_count":204,
        "Owner_creation_date":"2017-12-25 23:18:10.9 UTC",
        "Owner_last_access_date":"2022-09-20 21:06:54.96 UTC",
        "Owner_reputation":167,
        "Owner_up_votes":113,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":"<p>In order to not save the trained model weights during hyperparam optimization you do something like this:<\/p>\n<pre><code>classifier.fit(..., callbacks=[WandbCallback(.., save_model=False)]\n<\/code><\/pre>\n<p>This will only track the metrics (train\/validation loss\/acc, etc.).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-04-25 14:53:48.827 UTC",
        "Answer_score":4.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72001154",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73617230,
        "Question_title":"How to avoid data averaging when logging to metric across multiple runs?",
        "Question_body":"<p>I'm trying to log data points for the same metric across multiple runs (<code>wandb.init<\/code> is called repeatedly in between each data point) and I'm unsure how to avoid the behavior seen in the attached screenshot...<\/p>\n<p>Instead of getting a line chart with multiple points, I'm getting a single data point with associated statistics. In the attached e.g., the 1st data point was generated at step 1,470 and the 2nd at step 2,940...rather than seeing two points, I'm instead getting a single point that's the average and appears at step 2,205.\n<a href=\"https:\/\/i.stack.imgur.com\/98Lln.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/98Lln.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>My hunch is that using the <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/resuming\" rel=\"nofollow noreferrer\">resume run<\/a> feature may address my problem, but even testing out this hunch is proving to be cumbersome given the constraints of the system I'm working with...<\/p>\n<p>Before I invest more time in my hypothesized solution, could someone confirm that the behavior I'm seeing is, indeed, the result of logging data to the same metric across separate runs without using the resume feature?<\/p>\n<p>If this is the case, can you confirm or deny my conception of how to use resume?<\/p>\n<p>Initial run:<\/p>\n<ol>\n<li><code>run = wandb.init()<\/code><\/li>\n<li><code>wandb_id = run.id<\/code><\/li>\n<li>cache <code>wandb_id<\/code> for successive runs<\/li>\n<\/ol>\n<p>Successive run:<\/p>\n<ol>\n<li>retrieve <code>wandb_id<\/code> from cache<\/li>\n<li><code>wandb.init(id=wandb_id, resume=&quot;must&quot;)<\/code><\/li>\n<\/ol>\n<p>Is it also acceptable \/ preferable to replace <code>1.<\/code> and <code>2.<\/code> of the initial run with:<\/p>\n<ol>\n<li><code>wandb_id = wandb.util.generate_id()<\/code><\/li>\n<li><code>wandb.init(id=wandb_id)<\/code><\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-06 05:48:39.723 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"wandb",
        "Question_view_count":35,
        "Owner_creation_date":"2012-11-09 02:50:42.633 UTC",
        "Owner_last_access_date":"2022-09-24 16:17:39.503 UTC",
        "Owner_reputation":740,
        "Owner_up_votes":46,
        "Owner_down_votes":5,
        "Owner_views":63,
        "Answer_body":"<blockquote>\n<p>My hunch is that using the resume run feature may address my problem,<\/p>\n<\/blockquote>\n<p>Indeed, providing a cached <code>id<\/code> in combination with <code>resume=&quot;must&quot;<\/code> fixed the issue.\n<a href=\"https:\/\/i.stack.imgur.com\/KooZ9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KooZ9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Corresponding snippet:<\/p>\n<pre><code>import wandb\n\n# wandb run associated with evaluation after first N epochs of training.\nwandb_id = wandb.util.generate_id()\nwandb.init(id=wandb_id, project=&quot;alrichards&quot;, name=&quot;test-run-3\/job-1&quot;, group=&quot;test-run-3&quot;)\nwandb.log({&quot;mean_evaluate_loss_epoch&quot;: 20}, step=1)\nwandb.finish()\n\n# wandb run associated with evaluation after second N epochs of training.\nwandb.init(id=wandb_id, resume=&quot;must&quot;, project=&quot;alrichards&quot;, name=&quot;test-run-3\/job-2&quot;, group=&quot;test-run-3&quot;)\nwandb.log({&quot;mean_evaluate_loss_epoch&quot;: 10}, step=5)\nwandb.finish()\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-07 16:59:16.687 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73617230",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71926953,
        "Question_title":"Wandb website for Huggingface Trainer shows plots and logs only for the first model",
        "Question_body":"<p>I am finetuning multiple models using for loop as follows.<\/p>\n<pre><code>for file in os.listdir(args.data_dir):\n    finetune(args, file)\n<\/code><\/pre>\n<p>BUT <code>wandb<\/code> website shows plots and logs only for the first file i.e., <code>file1<\/code> in <code>data_dir<\/code> although it is training and saving models for other files. It feels very strange behavior.<\/p>\n<pre><code>wandb: Synced bertweet-base-finetuned-file1: https:\/\/wandb.ai\/***\/huggingface\/runs\/***\n<\/code><\/pre>\n<p>This is a small snippet of <strong>finetuning<\/strong> code with Huggingface:<\/p>\n<pre><code>def finetune(args, file):\n    training_args = TrainingArguments(\n        output_dir=f'{model_name}-finetuned-{file}',\n        overwrite_output_dir=True,\n        evaluation_strategy='no',\n        num_train_epochs=args.epochs,\n        learning_rate=args.lr,\n        weight_decay=args.decay,\n        per_device_train_batch_size=args.batch_size,\n        per_device_eval_batch_size=args.batch_size,\n        fp16=True, # mixed-precision training to boost speed\n        save_strategy='no',\n        seed=args.seed,\n        dataloader_num_workers=4,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset['train'],\n        eval_dataset=None,\n        data_collator=data_collator,\n    )\n    trainer.train()\n    trainer.save_model()\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-19 14:48:41.377 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-04-19 22:08:37.733 UTC",
        "Question_score":0,
        "Question_tags":"python|huggingface-transformers|wandb",
        "Question_view_count":178,
        "Owner_creation_date":"2014-02-13 12:57:24.357 UTC",
        "Owner_last_access_date":"2022-09-24 20:22:35.01 UTC",
        "Owner_reputation":1020,
        "Owner_up_votes":278,
        "Owner_down_votes":17,
        "Owner_views":206,
        "Answer_body":"<p><code>wandb.init(reinit=True)<\/code> and <code>run.finish()<\/code> helped me to log the models <strong>separately<\/strong> on wandb website.<\/p>\n<p>The working code looks like below:<\/p>\n<pre><code>\nfor file in os.listdir(args.data_dir):\n    finetune(args, file)\n\nimport wandb\ndef finetune(args, file):\n    run = wandb.init(reinit=True)\n    ...\n    run.finish()\n<\/code><\/pre>\n<p>Reference: <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script\" rel=\"nofollow noreferrer\">https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-04-20 15:50:53.183 UTC",
        "Answer_score":0.0,
        "Owner_location":"Chicago, IL, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71926953",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":63469762,
        "Question_title":"Weights&Biases Sweep Keras K-Fold Validation",
        "Question_body":"<p>I'm using Weights&amp;Biases Cloud-based sweeps with Keras.\nSo first i create a new Sweep within a W&amp;B Project with a config like following:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>description: LSTM Model\nmethod: random\nmetric:\n  goal: maximize\n  name: val_accuracy\nname: LSTM-Sweep\nparameters:\n  batch_size:\n    distribution: int_uniform\n    max: 128\n    min: 32\n  epochs:\n    distribution: constant\n    value: 200\n  node_size1:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size2:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size3:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size4:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size5:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  num_layers:\n    distribution: categorical\n    values:\n    - 1\n    - 2\n    - 3\n  optimizer:\n    distribution: categorical\n    values:\n    - Adam\n    - Adamax\n    - Adagrad\n  path:\n    distribution: constant\n    value: &quot;.\/path\/to\/data\/&quot;\nprogram: sweep.py\nproject: SLR\n<\/code><\/pre>\n<p>My <code>sweep.py<\/code> file looks something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># imports\ninit = wandb.init(project=&quot;my-project&quot;, reinit=True)\nconfig = wandb.config\n\ndef main():\n    skfold = StratifiedKFold(n_splits=5, \n    shuffle=True, random_state=7)\n    cvscores = []\n    group_id = wandb.util.generate_id()\n    X,y = # load data\n    i = 0\n    for train, test in skfold.split(X,y):\n        i=i+1\n        run = wandb.init(group=group_id, reinit=True, name=group_id+&quot;#&quot;+str(i))\n        model = # build model\n        model.fit([...], WandBCallback())\n        cvscores.append([...])\n        wandb.join()\n\nif __name__ == &quot;__main__&quot;:\n    main()\n<\/code><\/pre>\n<p>Starting this with the <code>wandb agent<\/code> command within the folder of <code>sweep.py<\/code>.<\/p>\n<p>What i experienced with this setup is, that with the first wandb.init() call a new run is initialized. Okay, i could just remove that. But when calling wandb.init() for the second time it seems to lose track of the sweep it is running in. Online an empty run is listed in the sweep (because of the first wandb.init() call), all other runs are listed inside the project, but not in the sweep.<\/p>\n<p>My goal is to have a run for each fold of the k-Fold cross-validation. At least i thought this would be the right way of doing this.\nIs there a different approach to combine sweeps with keras k-fold cross validation?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-18 13:31:50.213 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-08-30 08:57:51.017 UTC",
        "Question_score":2,
        "Question_tags":"python|keras|k-fold|wandb",
        "Question_view_count":1043,
        "Owner_creation_date":"2017-02-08 10:21:40.03 UTC",
        "Owner_last_access_date":"2021-05-05 18:51:28.72 UTC",
        "Owner_reputation":45,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":"<p>We put together an example of how to accomplish k-fold cross validation:<\/p>\n<p><a href=\"https:\/\/github.com\/wandb\/examples\/tree\/master\/examples\/wandb-sweeps\/sweeps-cross-validation\" rel=\"nofollow noreferrer\">https:\/\/github.com\/wandb\/examples\/tree\/master\/examples\/wandb-sweeps\/sweeps-cross-validation<\/a><\/p>\n<p>The solution requires some contortions for the wandb library to spawn multiple jobs on behalf of a launched sweep job.<\/p>\n<p>The basic idea is:<\/p>\n<ul>\n<li>The agent requests a new set of parameters from the cloud hosted parameter server.  This is the run called <code>sweep_run<\/code> in the main function.<\/li>\n<li>Send information about what the folds should process over a multiprocessing queue to waiting processes<\/li>\n<li>Each spawned process logs to their own run, organized with group and job_type to enable auto-grouping in the UI<\/li>\n<li>When the process is finished, it sends the primary metric over a queue to the parent sweep run<\/li>\n<li>The sweep run reads metrics from the child runs and logs it to the sweep run so that the sweep can use that result to impact future parameter choices and\/or hyperband early termination optimizations<\/li>\n<\/ul>\n<p>Example visualizations of the sweep and k-fold grouping can be seen here:<\/p>\n<ul>\n<li>Sweep: <a href=\"https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/sweeps\/vp0fsvku\" rel=\"nofollow noreferrer\">https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/sweeps\/vp0fsvku<\/a><\/li>\n<li>K-fold Grouping: <a href=\"https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/groups\/vp0fsvku\" rel=\"nofollow noreferrer\">https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/groups\/vp0fsvku<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-08-21 17:48:33.043 UTC",
        "Answer_score":6.0,
        "Owner_location":"Germany",
        "Answer_last_edit_date":"2020-09-10 21:18:55.68 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63469762",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71223654,
        "Question_title":"How to get wandb to pass arguments by position?",
        "Question_body":"<p>I am trying to explore the results of different parameter settings on my python script &quot;train.py&quot;. For that, I use a wandb sweep. Each wandb agent executes the file &quot;train.py&quot; and passes some parameters to it. As per the wandb documentation (<a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command\" rel=\"nofollow noreferrer\">https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command<\/a>), in case of e.g. two parameters &quot;param1&quot; and &quot;param2&quot; each agents starts the file with the command<\/p>\n<pre><code>\/usr\/bin\/env python train.py --param1=value1 --param2=value2\n<\/code><\/pre>\n<p>However, &quot;train.py&quot; expects<\/p>\n<pre><code>\/usr\/bin\/env python train.py value1 value2\n<\/code><\/pre>\n<p>and parses the parameter values by position. I did not write train.py and would like to not change it if possible. How can I get wandb to pass the values without &quot;--param1=&quot; in front?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-22 15:07:04.61 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|python-3.x|wandb",
        "Question_view_count":270,
        "Owner_creation_date":"2015-08-24 11:16:20.2 UTC",
        "Owner_last_access_date":"2022-02-23 15:36:22.56 UTC",
        "Owner_reputation":5,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":"<p>Don't think you can get positional arguments from W&amp;B Sweeps. However, there's a little work around you can try that won't require you touching the <code>train.py<\/code> file.<\/p>\n<p>You can create an invoker file, let's call it <code>invoke.py<\/code>. Now, you can use it get rid of the keyword argument names. Something like this might work:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport subprocess\n\nif len(sys.argv[0]) &lt;= 1:\n  print(f&quot;{sys.argv[0]} program_name param0=&lt;param0&gt; param1=&lt;param1&gt; ...&quot;)\n  sys.exit(0)\n\nprogram = sys.argv[1]\nparams = sys.argv[2:]\n\nposparam = []\nfor param in params:\n  _, val = param.split(&quot;=&quot;)\n  posparam.append(val)\n\ncommand = [sys.executable, program, *posparam]\nprocess = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nout, err = process.communicate()\nsys.stdout.write(out.decode())\nsys.stdout.flush()\nsys.stderr.write(err.decode())\nsys.stderr.flush()\nsys.exit(process.returncode)\n<\/code><\/pre>\n<p>This allows you to invoke your <code>train.py<\/code> file as follows:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ python3 invoke.py \/path\/to\/train.py param0=0.001 param1=20 ...\n<\/code><\/pre>\n<p>Now to perform W&amp;B sweeps you can create a <code>command:<\/code> section (<a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command\" rel=\"nofollow noreferrer\">reference<\/a>) in your <code>sweeps.yaml<\/code> file while sweeping over the parameters <code>param0<\/code> and <code>param1<\/code>. For example:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>program: invoke.py\n...\nparameters:\n  param0:\n    distribution: uniform\n    min: 0\n    max: 1\n  param1:\n    distribution: categorical\n    values: [10, 20, 30]\ncommand:\n - ${env}\n - ${program}\n - \/path\/to\/train.py\n - ${args_no_hyphens}\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2022-02-22 16:51:46.457 UTC",
        "Answer_score":1.0,
        "Owner_location":"Germany",
        "Answer_last_edit_date":"2022-02-22 16:56:56.653 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71223654",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73062370,
        "Question_title":"How to get a graph with the best performing runs via Sweeps (Weights & Biases)?",
        "Question_body":"<p>For my NER model I use Weights &amp; Biases sweeps for hyperparameter search. I do a grid search with about 100 runs and there are some really meaningful graphs. However, I can't figure out how to create a graph that shows about the best 10 runs in terms of f-score. Does anyone know how to do this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2022-07-21 07:39:22.9 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"machine-learning|nlp|named-entity-recognition|wandb",
        "Question_view_count":58,
        "Owner_creation_date":"2022-02-01 10:10:11.767 UTC",
        "Owner_last_access_date":"2022-09-22 07:59:57.15 UTC",
        "Owner_reputation":78,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>In the sweep view, you can filter runs by certain criteria by clicking this button:\n<a href=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>There, you can add a filter to only show runs with an f1 score, or an accuracy or whatever metric you have logged higher than a certain value:\n<a href=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Of course, this won't filter for the 10 best runs, but for all runs with an accuracy of 0.9 and higher (example in picture).<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-07-21 08:50:00.157 UTC",
        "Answer_score":3.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73062370",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73412851,
        "Question_title":"What is the meaning of 'config = wandb.config'?",
        "Question_body":"<p>I try to do the settings for a sweep for my Logistic regression model. I read the tutorials of wandb and cannot understand how to make the configurations and especially the meaning of <code>config=wandb.config<\/code> in the tutorials. I would really appreciate it if someone gave me a good explanation of the steps. Here is what I've done:<\/p>\n<pre><code>sweep_config = {\n    'method': 'grid'\n}\n\nmetric = {\n    'name': 'f1-score',\n    'goal': 'maximize'\n}\n\nsweep_config['metric'] = metric\n\nparameters = {\n    'penalty': {\n        'values': ['l2']\n    },\n    'C': {\n        'values': [0.01, 0.1, 1.0, 10.0, 100.0]\n    }\n}\n\nsweep_config['parameters'] = parameters\n<\/code><\/pre>\n<p>Then I create the yaml file:<\/p>\n<pre><code>stream = open('config.yaml', 'w')\nyaml.dump(sweep_config, stream) \n<\/code><\/pre>\n<p>Then it's time for training:<\/p>\n<pre><code>with wandb.init(project=WANDB_PROJECT_NAME):\n    config = wandb.config\n    \n    features = pd.read_csv('data\/x_features.csv')\n    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n\n    X_features = features = vectorizer.fit_transform(features['lemmatized_reason'])\n\n    y_labels = pd.read_csv('data\/y_labels.csv')\n\n    split_data = train_test_split(X_features, y_labels, train_size = 0.85, test_size = 0.15, stratify=y_labels)\n    features_train, labels_train = split_data[0], split_data[2]\n    features_test, labels_test = split_data[1], split_data[3]\n    \n    config = wandb.config\n    log_reg = LogisticRegression(\n        penalty=config.penalty,\n        C = config.C\n    )\n    \n    log_reg.fit(features_train, labels_train)\n    \n    labels_pred = log_reg.predict(features_test)\n    labels_proba = log_reg.predict_proba(features_test)\n    labels=list(map(str,y_labels['label'].unique()))\n    \n    # Visualize single plot\n    cm = wandb.sklearn.plot_confusion_matrix(labels_test, labels_pred, labels)\n    \n    score_f1 = f1_score(labels_test, labels_pred, average='weighted')\n    \n    sm = wandb.sklearn.plot_summary_metrics(\n    log_reg, features_train, labels_train, features_test, labels_test)\n    \n    roc = wandb.sklearn.plot_roc(labels_test, labels_proba)\n    \n    wandb.log({\n        &quot;f1-weighted-log-regression-tfidf-skf&quot;: score_f1, \n        &quot;roc-log-regression-tfidf-skf&quot;: roc, \n        &quot;conf-mat-logistic-regression-tfidf-skf&quot;: cm,\n        &quot;summary-metrics-logistic-regression-tfidf-skf&quot;: sm\n        })\n<\/code><\/pre>\n<p>And finally sweep_id and agent outside of <code>with<\/code> statement:<\/p>\n<pre><code>sweep_id = wandb.sweep(sweep_config, project=&quot;multiple-classifiers&quot;)\nwandb.agent(sweep_id)\n<\/code><\/pre>\n<p>There is something major I am missing here with this config thing, that I just cannot understand.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-19 06:50:49.347 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|wandb",
        "Question_view_count":61,
        "Owner_creation_date":"2019-05-10 07:44:04.067 UTC",
        "Owner_last_access_date":"2022-09-23 14:01:04.767 UTC",
        "Owner_reputation":626,
        "Owner_up_votes":120,
        "Owner_down_votes":3,
        "Owner_views":140,
        "Answer_body":"<p>I work at Weights &amp; Biases. With wandb Sweeps, the idea is that wandb needs to be able to change the hyperparameters in the sweep.<\/p>\n<p>The below section where the hyperparameters are passed to <code>LogisticRegression<\/code> could also be re-written<\/p>\n<pre><code>config = wandb.config\nlog_reg = LogisticRegression(\n    penalty=config.penalty,\n    C = config.C\n)\n<\/code><\/pre>\n<p>like this:<\/p>\n<pre><code>log_reg = LogisticRegression(\n    penalty=wandb.config.penalty,\n    C = wandb.config.C\n)\n<\/code><\/pre>\n<p>However, I think you're missing defining a train function or train script, which needs to also be passed to wandb. With out it, your example above won't work.<\/p>\n<p>Below is a minimal example that should help. Hopefully the <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\" rel=\"nofollow noreferrer\">sweeps documentation<\/a> can also help.<\/p>\n<pre><code>import numpy as np \nimport random\nimport wandb\n\n#  Step 1: Define sweep config\nsweep_configuration = {\n    'method': 'random',\n    'name': 'sweep',\n    'metric': {'goal': 'maximize', 'name': 'val_acc'},\n    'parameters': \n    {\n        'batch_size': {'values': [16, 32, 64]},\n        'epochs': {'values': [5, 10, 15]},\n        'lr': {'max': 0.1, 'min': 0.0001}\n     }\n}\n\n#  Step 2: Initialize sweep by passing in config\nsweep_id = wandb.sweep(sweep_configuration)\n\ndef train_one_epoch(epoch, lr, bs): \n  acc = 0.25 + ((epoch\/30) +  (random.random()\/10))\n  loss = 0.2 + (1 - ((epoch-1)\/10 +  random.random()\/5))\n  return acc, loss\n\ndef evaluate_one_epoch(epoch): \n  acc = 0.1 + ((epoch\/20) +  (random.random()\/10))\n  loss = 0.25 + (1 - ((epoch-1)\/10 +  random.random()\/6))\n  return acc, loss\n\ndef train():\n    run = wandb.init()\n\n    #  Step 3: Use hyperparameter values from `wandb.config`\n    lr  =  wandb.config.lr\n    bs = wandb.config.batch_size\n    epochs = wandb.config.epochs\n\n    for epoch in np.arange(1, epochs):\n      train_acc, train_loss = train_one_epoch(epoch, lr, bs)\n      val_acc, val_loss = evaluate_one_epoch(epoch)\n\n      wandb.log({\n        'epoch': epoch, \n        'train_acc': train_acc,\n        'train_loss': train_loss, \n        'val_acc': val_acc, \n        'val_loss': val_loss\n      })\n\n#  Step 4: Launch sweep by making a call to `wandb.agent`\nwandb.agent(sweep_id, function=train, count=4)\n<\/code><\/pre>\n<p>Finally, can you share the link where you found the code above? Maybe we need to update some examples :)<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2022-08-19 11:17:27.087 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73412851",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72590067,
        "Question_title":"jupyterLab Wandb does not iterative",
        "Question_body":"<p>I try to use WanDB but when i use wandb.init() there is nothing.!<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/EZfVt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EZfVt.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I am waiting a lot of time.<\/p>\n<p>However, there is nothing in window.<\/p>\n<p>This is working well in Kernel.<\/p>\n<p>please.. help me guys<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-12 06:56:00.48 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"jupyter-lab|wandb",
        "Question_view_count":40,
        "Owner_creation_date":"2020-12-28 10:48:14.583 UTC",
        "Owner_last_access_date":"2022-09-22 11:39:41.627 UTC",
        "Owner_reputation":72,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":"<p>I work at Weights &amp; Biases. If you're in a notebook the quickest thing you can do to get going with <code>wandb<\/code> is simply:<\/p>\n<pre><code>wandb.init(project=MY_PROJECT, entity=MY_ENTITY)\n<\/code><\/pre>\n<p>No <code>!wandb login<\/code>, <code>wandb.login()<\/code> or <code>%%wandb<\/code> needed. If you're not already logged in then <code>wandb.init<\/code> will ask you for you API key.<\/p>\n<p>(curious where you found <code>%%wandb<\/code> by the way?)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-13 09:13:22.567 UTC",
        "Answer_score":0.0,
        "Owner_location":"South Korea",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72590067",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73244442,
        "Question_title":"HuggingFace Trainer() cannot report to wandb",
        "Question_body":"<p>I am trying to set trainer with arguments <code>report_to<\/code> to <code>wandb<\/code>, refer to <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/huggingface#getting-started-track-experiments\" rel=\"nofollow noreferrer\">this docs<\/a>\nwith config:<\/p>\n<pre><code>training_args = TrainingArguments(\n    output_dir=&quot;test_trainer&quot;,\n    evaluation_strategy=&quot;steps&quot;,\n    learning_rate=config.learning_rate,\n    num_train_epochs=config.epochs,\n    weight_decay=config.weight_decay,\n    logging_dir=config.logging_dir,\n    report_to=&quot;wandb&quot;,\n    save_total_limit=1,\n    per_device_train_batch_size=config.batch_size,\n    per_device_eval_batch_size=config.batch_size,\n    fp16=True,\n    load_best_model_at_end=True,\n    seed=42\n)\n<\/code><\/pre>\n<p>yet when I set trainer with:<\/p>\n<pre><code>trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics\n)\n<\/code><\/pre>\n<p>it shows:<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-68-b009351ab52d&gt; in &lt;module&gt;\n      4     train_dataset=train_dataset,\n      5     eval_dataset=eval_dataset,\n----&gt; 6     compute_metrics=compute_metrics\n      7 )\n\n~\/.virtualenvs\/transformers_lab\/lib\/python3.7\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\n    286                 &quot;You should subclass `Trainer` and override the `create_optimizer_and_scheduler` method.&quot;\n    287             )\n--&gt; 288         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\n    289         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\n    290         self.callback_handler = CallbackHandler(\n\n~\/.virtualenvs\/transformers_lab\/lib\/python3.7\/site-packages\/transformers\/integrations.py in get_reporting_integration_callbacks(report_to)\n    794         if integration not in INTEGRATION_TO_CALLBACK:\n    795             raise ValueError(\n--&gt; 796                 f&quot;{integration} is not supported, only {', '.join(INTEGRATION_TO_CALLBACK.keys())} are supported.&quot;\n    797             )\n    798     return [INTEGRATION_TO_CALLBACK[integration] for integration in report_to]\n\nValueError: w is not supported, only azure_ml, comet_ml, mlflow, tensorboard, wandb are supported.\n<\/code><\/pre>\n<p>Have anyone got same error before?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-05 03:48:25.703 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-08-09 08:29:23.877 UTC",
        "Question_score":0,
        "Question_tags":"python|huggingface-transformers|wandb",
        "Question_view_count":47,
        "Owner_creation_date":"2020-04-18 05:07:48.85 UTC",
        "Owner_last_access_date":"2022-09-23 07:29:12.847 UTC",
        "Owner_reputation":163,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":"<p>Although the documentation states that the <code>report_to<\/code> parameter can receive both <code>List[str]<\/code> or <code>str<\/code> I have always used a list with 1! element for this purpose.<\/p>\n<p>Therefore, even if you report only to wandb, the solution to your problem is to replace:<\/p>\n<pre><code> report_to = 'wandb'\n<\/code><\/pre>\n<p>with<\/p>\n<pre><code>report_to = [&quot;wandb&quot;]\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-08-06 10:28:14.627 UTC",
        "Answer_score":1.0,
        "Owner_location":"Taipei, Taiwan R.O.C",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73244442",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":67291062,
        "Question_title":"Control the logging frequency and contents when using wandb with HuggingFace",
        "Question_body":"<p>I am using the <code>wandb<\/code> with my HuggingFace code. I would like to log the loss and other metrics. Now I have two questions<\/p>\n<ul>\n<li>How does <code>wandb<\/code> decide when to log the loss? Is this decided by <code>logging_steps<\/code> in <code>TrainingArguments(...)<\/code>\uff1f<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>training_args = TrainingArguments(output_dir=&quot;test&quot;, \n                                  learning_rate=lr,\n                                  num_train_epochs=n_epoch,\n                                  seed=seed,\n                                  per_device_train_batch_size=2,\n                                  per_device_eval_batch_size=2,\n                                  logging_strategy=&quot;steps&quot;,\n                                  logging_steps=5,\n                                  report_to=&quot;wandb&quot;)\n<\/code><\/pre>\n<ul>\n<li>How do I make sure <code>wandb<\/code> log other metrics (for example, adding validation metrics after each epoch)? Does this happen automatically?<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-27 21:43:01.187 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"huggingface-transformers|wandb",
        "Question_view_count":385,
        "Owner_creation_date":"2017-03-29 09:11:16.137 UTC",
        "Owner_last_access_date":"2022-09-25 03:44:48.93 UTC",
        "Owner_reputation":322,
        "Owner_up_votes":239,
        "Owner_down_votes":1,
        "Owner_views":109,
        "Answer_body":"<p>Correct, it is dictated by the <code>on_log<\/code> event from the Trainer, you can see it <a href=\"https:\/\/github.com\/huggingface\/transformers\/blob\/6b241e0e3bda24546d15835e5e0b48b8d1e4732c\/src\/transformers\/integrations.py#L747\" rel=\"nofollow noreferrer\">here<\/a> in WandbCallback<\/p>\n<p>Your validation metrics should be logged to W&amp;B automatically every time you validate. How often Trainer does evaluation depends on what setting is used for <code>evaluation_strategy<\/code> (and potentially <code>eval_steps<\/code> if <code>evaluation_strategy == &quot;steps&quot;<\/code>)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-05-04 21:16:13.717 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67291062",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71744288,
        "Question_title":"wandb getting logged without initiating",
        "Question_body":"<p>I do not want to use wandb. I don't even have an account. I am simply following <a href=\"https:\/\/colab.research.google.com\/github\/huggingface\/notebooks\/blob\/master\/examples\/summarization.ipynb#scrollTo=UmvbnJ9JIrJd\" rel=\"nofollow noreferrer\">this notebook<\/a> for finetuning. I am not running the 2nd and 3 cells because I do not want to push the model to the hub.<\/p>\n<p>However, when I do trainer.train() I get the following error : <a href=\"https:\/\/i.stack.imgur.com\/VPIZm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VPIZm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I don't understand where wandb.log is being called.\nI even tried os.environ[&quot;WANDB_DISABLED&quot;]  = &quot;true&quot; but I still get the error.\nPlease help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-04 22:08:30.203 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"huggingface-transformers|huggingface-tokenizers|fine-tune|wandb|huggingface",
        "Question_view_count":289,
        "Owner_creation_date":"2017-07-12 13:59:11.607 UTC",
        "Owner_last_access_date":"2022-08-23 05:59:43.987 UTC",
        "Owner_reputation":307,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Answer_body":"<p>posting the same message as <a href=\"https:\/\/github.com\/huggingface\/transformers\/issues\/16594\" rel=\"nofollow noreferrer\">over on <code>transformers<\/code><\/a>:<\/p>\n<hr \/>\n<p>You can turn off all external logger logging, including wandb logging by passing <code>report_to=&quot;none&quot;<\/code> in your <code>Seq2SeqTrainingArguments<\/code>.<\/p>\n<p>You might have noticed the following warning when setting up your TrainingArguments:<\/p>\n<pre><code>The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-)\n<\/code><\/pre>\n<p>Right now the default is to run all loggers that you have installed, so maybe you installed wandb on your machine since the last time you ran the script?<\/p>\n<p>If you would like to log with wandb, best practice would already be to start setting <code>report_to=&quot;wandb&quot;<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-04-05 11:02:43.12 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71744288",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":68952727,
        "Question_title":"wandb: get a list of all artifact collections and all aliases of those artifacts",
        "Question_body":"<p>The wandb documentation doesn't seem to explain how to do this - but it should be a fairly common use case I'd imagine?<\/p>\n<p>I achieved mostly (but not completely) what I wanted like this, but it seems a bit clunky? I'd have expected to have an <code>self.aliases<\/code> property on the <code>ArtifactCollection<\/code> instances?<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>ENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\ndef get_model_artifacts(key=None):\n    wandb.login(key=key if key is not None else API_KEY)\n    api = wandb.Api(overrides={&quot;entity&quot;: ENTITY})\n    model_names = [\n        i\n        for i in api.artifact_type(\n            type_name=&quot;models&quot;, project=&quot;train&quot;\n        ).collections()\n    ]\n    for model in model_names:\n        artifact = api.artifact(&quot;train\/&quot; + model.name + &quot;:latest&quot;)\n        model._attrs.update(artifact._attrs)\n        model._attrs[&quot;metadata&quot;] = json.loads(model._attrs[&quot;metadata&quot;])\n        model.aliases = [x[&quot;alias&quot;] for x in model._attrs[&quot;aliases&quot;]]\n    return model_names\n<\/code><\/pre>\n<p>I guess I could possibly look into writing a custom graph-ql query if needed or just use this clunky method.<\/p>\n<p>Am I missing something? Is there a cleaner way to do this?<\/p>\n<p>The one thing this clunky method is missing is any old aliases - it only shows the latest model and then any aliases of that (let's say &quot;latest&quot; and also &quot;v4&quot; etc.) - not sure how this would\/should be displayed but I'd have hoped to be able to get old aliases as well (i.e. aliases that point to old versions of the artifact). Although, this is less important.<\/p>\n<p><strong>EDIT<\/strong> - after a few hours looking through their sdk code, I have this (still not that happy with how clunky it is):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>ENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\ndef get_model_artifacts(key=None):\n    wandb.login(key=key if key is not None else API_KEY)\n    api = wandb.Api(overrides={&quot;entity&quot;: ENTITY})\n    model_artifacts = [\n        a\n        for a in api.artifact_type(\n            type_name=&quot;models&quot;, project=&quot;train&quot;\n        ).collections()\n    ]\n\n    def get_alias_tuple(artifact_version):\n        version = None\n        aliases = []\n        for a in artifact_version._attrs[&quot;aliases&quot;]:\n            if re.match(r&quot;^v\\d+$&quot;, a[&quot;alias&quot;]):\n                version = a[&quot;alias&quot;]\n            else:\n                aliases.append(a[&quot;alias&quot;])\n        return version, aliases\n\n    for model in model_artifacts:\n        # artifact = api.artifact(&quot;train\/&quot; + model.name + &quot;:latest&quot;)\n        # model._attrs.update(artifact._attrs)\n        # model._attrs[&quot;metadata&quot;] = json.loads(model._attrs[&quot;metadata&quot;])\n        versions = model.versions()\n        version_dict = dict(get_alias_tuple(version) for version in versions)\n        model.version_dict = version_dict\n        model.aliases = [\n            x for key, val in model.version_dict.items() for x in [key] + val\n        ]\n    return model_artifacts\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-27 11:34:22.593 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-08-30 21:05:43.657 UTC",
        "Question_score":1,
        "Question_tags":"python|wandb",
        "Question_view_count":363,
        "Owner_creation_date":"2018-05-15 07:21:25.18 UTC",
        "Owner_last_access_date":"2022-09-23 17:35:59.473 UTC",
        "Owner_reputation":3944,
        "Owner_up_votes":574,
        "Owner_down_votes":47,
        "Owner_views":217,
        "Answer_body":"<p>I'm Annirudh. I'm an engineer at W&amp;B who helped build artifacts. Your solution is really close, but by using the <code>latest<\/code> alias when fetching the artifact we're only going to be considering the aliases from that one artifact instead of all the versions. You could get around that by looping over the versions:<\/p>\n<pre><code>api = wandb.Api()\ncollections = [\n    coll for coll in api.artifact_type(type_name=TYPE, project=PROJECT).collections()\n]\n\n\naliases = set()\nfor coll in collections:\n    for artifact in coll.versions():\n        aliases.update(artifact.aliases)\n\nprint(collections)\nprint(aliases)\n<\/code><\/pre>\n<p>Currently, the documentation is spare on collections but we're polishing them up in the public API and will release some docs around it shortly. These APIs aren't quite release ready yet -- so apologies for the rough edges.<\/p>\n<p>Please feel free to reach out to me directly in the future if you have any other questions regarding artifacts. Always happy to help.<\/p>",
        "Answer_comment_count":14.0,
        "Answer_creation_date":"2021-08-28 03:42:13.813 UTC",
        "Answer_score":2.0,
        "Owner_location":"Munich, Germany",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68952727",
        "Question_exclusive_tag":"Weights & Biases"
    }
]