[
    {
        "Question_id":53125108,
        "Question_title":"How do I download files within a Sagemaker notebook instance programatically?",
        "Question_body":"<p>We have a notebook instance within Sagemaker which contains many Jupyter Python scripts. I'd like to write a program which downloads these various scripts each day (i.e. so that I could back them up). Unfortunately I don't see any reference to this in the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/index.html\" rel=\"nofollow noreferrer\">AWS CLI API<\/a>.<\/p>\n\n<p>Is this achievable? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2018-11-02 19:59:19.393 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|aws-lambda|aws-cli|amazon-sagemaker",
        "Question_view_count":8552,
        "Owner_creation_date":"2011-03-10 10:26:00.99 UTC",
        "Owner_last_access_date":"2022-09-24 23:25:14.187 UTC",
        "Owner_reputation":2563,
        "Owner_up_votes":121,
        "Owner_down_votes":0,
        "Owner_views":167,
        "Answer_body":"<p>It's not exactly that you want, but looks like <a href=\"https:\/\/en.wikipedia.org\/wiki\/Version_control\" rel=\"nofollow noreferrer\">VCS<\/a> can fit your needs. You can use Github(if you already use it) or CodeCommit(free privat repos) Details and additional ways like <code>sync<\/code> target <code>dir<\/code> with <code>S3<\/code> bucket - <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/how-to-use-common-workflows-on-amazon-sagemaker-notebook-instances\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/how-to-use-common-workflows-on-amazon-sagemaker-notebook-instances\/<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-11-12 17:02:25.47 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2018-11-13 16:12:30.813 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53125108",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73685446,
        "Question_title":"No package aclocal available",
        "Question_body":"<p>I am trying to install a package <code>aclocal<\/code> on <code>Amazon SageMmaker<\/code> (which is a requirement for <code>teseract<\/code>) using the command <code>sudo yum install aclocal<\/code><\/p>\n<p>But it gives me the following error.<\/p>\n<p><code>No package aclocal available<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-09-12 06:52:07.117 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":27,
        "Owner_creation_date":"2018-04-25 04:30:27.117 UTC",
        "Owner_last_access_date":"2022-09-24 04:51:14.263 UTC",
        "Owner_reputation":154,
        "Owner_up_votes":304,
        "Owner_down_votes":0,
        "Owner_views":27,
        "Answer_body":"<p>To install <code>tesseract<\/code> in SageMaker you can simply follow the instructions here: <a href=\"https:\/\/tesseract-ocr.github.io\/tessdoc\/Compiling.html#linux\" rel=\"nofollow noreferrer\">https:\/\/tesseract-ocr.github.io\/tessdoc\/Compiling.html#linux<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-14 05:53:44.987 UTC",
        "Answer_score":1.0,
        "Owner_location":"Pune, Maharashtra, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73685446",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50032795,
        "Question_title":"prevent access to s3 buckets for sagemaker users",
        "Question_body":"<p>I'm trying to add an IAM user for using sagemaker. I used the <code>AmazonSageMakerFullAccess<\/code> policy. But when I log in as this user I can see all of the s3 buckets of the root account and download files from them.<\/p>\n<p>The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html\" rel=\"nofollow noreferrer\">sagemaker documentation<\/a> states<\/p>\n<blockquote>\n<p>When attaching the AmazonSageMakerFullAccess policy to a role, you must do one of the following to allow Amazon SageMaker to access your S3 bucket:<\/p>\n<p>Include the string &quot;SageMaker&quot; or &quot;sagemaker&quot; in the name of the bucket where you store training data, or the model artifacts resulting from model training, or both.<\/p>\n<p>Include the string &quot;SageMaker&quot; or &quot;sagemaker&quot; in the object name of the training data object(s).<\/p>\n<p>Tag the S3 object with &quot;sagemaker=true&quot;. The key and value are case sensitive. For more information, see Object Tagging in the Amazon Simple Storage Service Developer Guide.<\/p>\n<p>Add a bucket policy that allows access for the execution role. For more information, see Using Bucket Policies and User Policies in the Amazon Simple Storage Service Developer Guide.<\/p>\n<\/blockquote>\n<p>This seems to be inaccurate the user can access s3 buckets lacking <code>sagemaker<\/code> in the name. How do I limit the access?<\/p>\n<p>the full policy is below<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;sagemaker:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;ecr:GetAuthorizationToken&quot;,\n                &quot;ecr:GetDownloadUrlForLayer&quot;,\n                &quot;ecr:BatchGetImage&quot;,\n                &quot;ecr:BatchCheckLayerAvailability&quot;,\n                &quot;cloudwatch:PutMetricData&quot;,\n                &quot;cloudwatch:PutMetricAlarm&quot;,\n                &quot;cloudwatch:DescribeAlarms&quot;,\n                &quot;cloudwatch:DeleteAlarms&quot;,\n                &quot;ec2:CreateNetworkInterface&quot;,\n                &quot;ec2:CreateNetworkInterfacePermission&quot;,\n                &quot;ec2:DeleteNetworkInterface&quot;,\n                &quot;ec2:DeleteNetworkInterfacePermission&quot;,\n                &quot;ec2:DescribeNetworkInterfaces&quot;,\n                &quot;ec2:DescribeVpcs&quot;,\n                &quot;ec2:DescribeDhcpOptions&quot;,\n                &quot;ec2:DescribeSubnets&quot;,\n                &quot;ec2:DescribeSecurityGroups&quot;,\n                &quot;application-autoscaling:DeleteScalingPolicy&quot;,\n                &quot;application-autoscaling:DeleteScheduledAction&quot;,\n                &quot;application-autoscaling:DeregisterScalableTarget&quot;,\n                &quot;application-autoscaling:DescribeScalableTargets&quot;,\n                &quot;application-autoscaling:DescribeScalingActivities&quot;,\n                &quot;application-autoscaling:DescribeScalingPolicies&quot;,\n                &quot;application-autoscaling:DescribeScheduledActions&quot;,\n                &quot;application-autoscaling:PutScalingPolicy&quot;,\n                &quot;application-autoscaling:PutScheduledAction&quot;,\n                &quot;application-autoscaling:RegisterScalableTarget&quot;,\n                &quot;logs:CreateLogGroup&quot;,\n                &quot;logs:CreateLogStream&quot;,\n                &quot;logs:DescribeLogStreams&quot;,\n                &quot;logs:GetLogEvents&quot;,\n                &quot;logs:PutLogEvents&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:GetObject&quot;,\n                &quot;s3:PutObject&quot;,\n                &quot;s3:DeleteObject&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::*SageMaker*&quot;,\n                &quot;arn:aws:s3:::*Sagemaker*&quot;,\n                &quot;arn:aws:s3:::*sagemaker*&quot;\n            ]\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:CreateBucket&quot;,\n                &quot;s3:GetBucketLocation&quot;,\n                &quot;s3:ListBucket&quot;,\n                &quot;s3:ListAllMyBuckets&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:GetObject&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringEqualsIgnoreCase&quot;: {\n                    &quot;s3:ExistingObjectTag\/SageMaker&quot;: &quot;true&quot;\n                }\n            }\n        },\n        {\n            &quot;Action&quot;: &quot;iam:CreateServiceLinkedRole&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:iam::*:role\/aws-service-role\/sagemaker.application-autoscaling.amazonaws.com\/AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringLike&quot;: {\n                    &quot;iam:AWSServiceName&quot;: &quot;sagemaker.application-autoscaling.amazonaws.com&quot;\n                }\n            }\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;iam:PassRole&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringEquals&quot;: {\n                    &quot;iam:PassedToService&quot;: &quot;sagemaker.amazonaws.com&quot;\n                }\n            }\n        }\n    ]\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2018-04-25 23:43:37.877 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-06-20 09:12:55.06 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-iam|amazon-sagemaker",
        "Question_view_count":1703,
        "Owner_creation_date":"2011-02-23 18:00:07.147 UTC",
        "Owner_last_access_date":"2022-09-24 18:44:13.9 UTC",
        "Owner_reputation":9271,
        "Owner_up_votes":2074,
        "Owner_down_votes":44,
        "Owner_views":1819,
        "Answer_body":"<p>looks like the sagemaker notebook wizard has you create a role that has limited s3 access. If I add this and the default <code>AmazonSageMakerFullAccess<\/code> the user is properly restricted. <a href=\"https:\/\/i.stack.imgur.com\/9BjRD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9BjRD.png\" alt=\"Amazon make sagemaker role\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/IM7WW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IM7WW.png\" alt=\"choose iam roles\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-04-30 23:01:07.447 UTC",
        "Answer_score":0.0,
        "Owner_location":"New York, NY, United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50032795",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72909085,
        "Question_title":"Sagemaker creates output folders but no model.tar.gz after successful completion of the Training Job",
        "Question_body":"<p>I am running a <em>Training Job<\/em> using the Sagemaker API. The code for configuring the estimator looks as follows (I shrinked the full path names a bit):<\/p>\n<pre><code>s3_input = &quot;s3:\/\/sagemaker-studio-****\/training-inputs&quot;.format(bucket)\ns3_images = &quot;s3:\/\/sagemaker-studio-****\/dataset&quot;\ns3_labels = &quot;s3:\/\/sagemaker-studio-****\/labels&quot;\ns3_output = 's3:\/\/sagemaker-studio-****\/output'.format(bucket)\n\ncfg='{}\/input\/models\/'.format(s3_input)\nweights='{}\/input\/data\/weights\/'.format(s3_input)\noutpath='{}\/'.format(s3_output)\nimages='{}\/'.format(s3_images)\nlabels='{}\/'.format(s3_labels)\n\nhyperparameters = {\n    &quot;epochs&quot;: 1,\n    &quot;batch-size&quot;: 2\n}\n\ninputs = {\n    &quot;cfg&quot;: TrainingInput(cfg),\n    &quot;images&quot;: TrainingInput(images),\n    &quot;weights&quot;: TrainingInput(weights),\n    &quot;labels&quot;: TrainingInput(labels)\n}\n\nestimator = PyTorch(\n    entry_point='train.py',\n    source_dir='s3:\/\/sagemaker-studio-****\/input\/input.tar.gz',\n    image_uri=container,\n    role=get_execution_role(),\n    instance_count=1,\n    instance_type='ml.g4dn.xlarge',\n    input_mode='File',\n    output_path=outpath,\n    train_output=outpath,\n    base_job_name='visualsearch',\n    hyperparameters=hyperparameters,\n    framework_version='1.9',\n    py_version='py38'\n)\n\nestimator.fit(inputs)\n<\/code><\/pre>\n<p>Everything runs fine and I get the success message:<\/p>\n<pre><code>Results saved to #033[1mruns\/train\/exp#033[0m\n2022-07-08 08:38:35,766 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n2022-07-08 08:38:35,766 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n2022-07-08 08:38:35,767 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n\n2022-07-08 08:39:08 Uploading - Uploading generated training model\n2022-07-08 08:39:08 Completed - Training job completed\nProfilerReport-1657268881: IssuesFound\nTraining seconds: 558\nBillable seconds: 558\nCPU times: user 1.34 s, sys: 146 ms, total: 1.48 s\nWall time: 11min 20s\n<\/code><\/pre>\n<p>When I call <code>estimator.model_data<\/code> I get a path poiting to a model.tar.gz file <code>s3:\/\/sagemaker-studio-****\/output\/...\/model.tar.gz<\/code><\/p>\n<p>Sagemaker generated subfoldes into the output folder (which in turn contain a lot of json files and other artifacts):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/WymlH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WymlH.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But the file <code>model.tar.gz<\/code> is missing. This file is nowhere to be found. Is there anything I need to change or to add, in order to obtain my model?<\/p>\n<p>Any help is much appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-08 09:03:14.073 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-s3|pytorch|amazon-sagemaker",
        "Question_view_count":94,
        "Owner_creation_date":"2021-12-27 13:35:09.92 UTC",
        "Owner_last_access_date":"2022-09-21 09:31:13.63 UTC",
        "Owner_reputation":215,
        "Owner_up_votes":40,
        "Owner_down_votes":0,
        "Owner_views":46,
        "Answer_body":"<p>you need to make sure to store your model output to the right location inside the training container. Sagemaker will upload everything that is stored in the MODEL_DIR directory. You can find the location in the ENV of the training job:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_dir = os.environ.get(&quot;SM_MODEL_DIR&quot;)\n<\/code><\/pre>\n<p>Normally it is set to <code>opt\/ml\/model<\/code><\/p>\n<p>Ref:<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_model_dir\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_model_dir<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-output.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-output.html<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-08 09:39:48.6 UTC",
        "Answer_score":2.0,
        "Owner_location":"Zedtwitz, Germany",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72909085",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57479389,
        "Question_title":"Query a table\/database in Athena from a Notebook instance",
        "Question_body":"<p>I have developed different Athena Workgroups for different teams so that I can separate their queries and their query results. The users would like to query the tables available to them from their notebook instances (JupyterLab). I am having difficulty finding code which successfully covers the requirement of querying a table from the user's specific workgroup. I have only found code that will query the table from the primary workgroup. <\/p>\n\n<p>The code I have currently used is added below. <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from pyathena import connect\nimport pandas as pd\nconn = connect(s3_staging_dir='&lt;ATHENA QUERY RESULTS LOCATION&gt;',\nregion_name='&lt;YOUR REGION, for example, us-west-2&gt;')\n\n\ndf = pd.read_sql(\"SELECT * FROM &lt;DATABASE-NAME&gt;.&lt;YOUR TABLE NAME&gt; limit 8;\", conn)\ndf\n<\/code><\/pre>\n\n<p>This code does not work as the users only have access to perform queries from their specific workgroups hence get errors when this code is run. It also does not cover the requirement of separating the user's queries in user specific workgroups. <\/p>\n\n<p>Any suggestions on how I can add alter the code so that I can run the queries within a specific workgroup from the notebook instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2019-08-13 13:59:03.61 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|jupyter-notebook|amazon-athena|amazon-sagemaker|pyathena",
        "Question_view_count":5367,
        "Owner_creation_date":"2014-09-27 17:28:16.9 UTC",
        "Owner_last_access_date":"2020-02-19 09:16:01.12 UTC",
        "Owner_reputation":105,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":40,
        "Answer_body":"<p>Documentation of <code>pyathena<\/code> is not super extensive, but after looking into source code we can see that <code>connect<\/code> simply creates instance of <code>Connection<\/code> class.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def connect(*args, **kwargs):\n    from pyathena.connection import Connection\n    return Connection(*args, **kwargs)\n<\/code><\/pre>\n\n<p>Now, after looking into signature of <code>Connection.__init__<\/code> on <a href=\"https:\/\/github.com\/laughingman7743\/PyAthena\/blob\/master\/pyathena\/connection.py\" rel=\"nofollow noreferrer\">GitHub<\/a> we can see parameter <code>work_group=None<\/code> which name in the same way as one of the parameters for <code>start_query_execution<\/code> from the <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/athena.html#Athena.Client.start_query_execution\" rel=\"nofollow noreferrer\">official<\/a> AWS Python API <code>boto3<\/code>. Here is what their documentation say about it:<\/p>\n\n<blockquote>\n  <p>WorkGroup (string) -- The name of the workgroup in which the query is being started.<\/p>\n<\/blockquote>\n\n<p>After following through usages and imports in <code>Connection<\/code> we endup with <a href=\"https:\/\/github.com\/laughingman7743\/PyAthena\/blob\/master\/pyathena\/common.py\" rel=\"nofollow noreferrer\">BaseCursor<\/a> class that under the hood makes a call to <code>start_query_execution<\/code> while unpacking a dictionary with parameters assembled by <code>BaseCursor._build_start_query_execution_request<\/code> method. That is excatly where we can see familar syntax for submitting queries to AWS Athena, in particular the following part:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>if self._work_group or work_group:\n    request.update({\n        'WorkGroup': work_group if work_group else self._work_group\n    })\n<\/code><\/pre>\n\n<p>So this should do a trick for your case:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom pyathena import connect\n\n\nconn = connect(\n    s3_staging_dir='&lt;ATHENA QUERY RESULTS LOCATION&gt;',\n    region_name='&lt;YOUR REGION, for example, us-west-2&gt;',\n    work_group='&lt;USER SPECIFIC WORKGROUP&gt;'\n)\n\ndf = pd.read_sql(\"SELECT * FROM &lt;DATABASE-NAME&gt;.&lt;YOUR TABLE NAME&gt; limit 8;\", conn)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-08-16 13:56:13.253 UTC",
        "Answer_score":3.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2019-08-16 14:44:04.207 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57479389",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59717227,
        "Question_title":"AWS SageMaker - submit button is not working with custom template",
        "Question_body":"<p>When I create a new job on AWS SageMaker, using my custom template with crowd form (see attached sample) the SUBMIT button is not working and is not even clickable. Is there anyway to make this work? Haven`t see a good response on AWS support.<\/p>\n\n<pre><code>$('#submitButton').onclick = function() {\n   $('crowd-form').submit(); \n};\n\n\n &lt;body&gt;\n    &lt;h2 id=\"hit\"&gt;test&lt;\/h2&gt;\n        &lt;canvas id=\"canvas\" width=1210 height=687&gt;&lt;\/canvas&gt;    \n        &lt;crowd-button id=\"submitButton3\"&gt;Test button&lt;\/crowd-button&gt;\n\n    &lt;crowd-form&gt;\n\n        &lt;input type=\"hidden\" name=\"path0\" id=\"input0123\" value=\"{{task.input.metadata.images.path0}}\" \/&gt;\n        &lt;crowd-input label=\"Please input the character you see in the image\" max-length=\"1\" name=\"workerInput0\"&gt;&lt;\/crowd-input&gt;\n\n        &lt;crowd-button id=\"submitButto3223n\"&gt;Submit123&lt;\/crowd-button&gt;\n\n    &lt;\/div&gt;&lt;\/div&gt;\n\n    &lt;crowd-button id=\"submitButton\"&gt;Submit123&lt;\/crowd-button&gt;\n\n    &lt;\/crowd-form&gt;\n    &lt;crowd-button id=\"submitButton1\"&gt;Submit1232&lt;\/crowd-button&gt;\n\n    &lt;script src=\"http:\/\/code.jquery.com\/jquery-1.11.0.min.js\"&gt;&lt;\/script&gt;\n &lt;\/body&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_date":"2020-01-13 13:04:26.227 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-01-14 08:12:53.557 UTC",
        "Question_score":2,
        "Question_tags":"amazon-sagemaker|mechanicalturk",
        "Question_view_count":917,
        "Owner_creation_date":"2018-12-09 09:05:30.127 UTC",
        "Owner_last_access_date":"2020-04-30 07:25:44.867 UTC",
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":"<p>There are few issues with you code snippet.<\/p>\n<p>Here are the links to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-ui-template-reference.html\" rel=\"nofollow noreferrer\">SageMaker's HTML Reference<\/a> and <a href=\"https:\/\/awsfeed.com\/whats-new\/machine-learning\/build-a-custom-data-labeling-workflow-with-amazon-sagemaker-ground-truth\/\" rel=\"nofollow noreferrer\">Example for building custom Labeling template<\/a><\/p>\n<p>First remove all those submit buttons (<code>&lt;crowd-button&gt;<\/code> elements) and the <code>onClick<\/code> event handler. From here you have two options use default SageMaker submit button or create your own in the template.<\/p>\n<h2>Use SageMaker's Submit Button<\/h2>\n<p>Leave out submit buttons (<code>crowd-button<\/code>) and SageMaker will automatically append one inside <code>crowd-form<\/code>. According to documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-ui-template-crowd-form.html\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<h2>Use custom Submit Button<\/h2>\n<p>In this case you need to:<\/p>\n<ol>\n<li>Prevent SageMaker adding button by including <code>crowd-button<\/code> <strong>inside<\/strong> the <code>crowd-form<\/code> element and setting <code>style=&quot;display: none;<\/code><\/li>\n<li>Add your own Submit button elsewhere on the template and add <code>onclick<\/code> even handler that will execute <code>form.submit()<\/code><\/li>\n<\/ol>\n<p>Here is the working example of the template (taken from the Example mentioned above).<\/p>\n<pre><code>&lt;script src=&quot;https:\/\/assets.crowd.aws\/crowd-html-elements.js&quot;&gt;&lt;\/script&gt;\n\n&lt;link rel=&quot;stylesheet&quot; href=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/css\/1.3fc3007b.chunk.css&quot;&gt;\n&lt;link rel=&quot;stylesheet&quot; href=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/css\/main.9504782e.chunk.css&quot;&gt;\n\n&lt;div id='document-text' style=&quot;display: none;&quot;&gt;\n  {{ task.input.text }}\n&lt;\/div&gt;\n&lt;div id='document-image' style=&quot;display: none;&quot;&gt;\n        {{ task.input.taskObject | grant_read_access }}\n&lt;\/div&gt;\n&lt;div id=&quot;metadata&quot; style=&quot;display: none;&quot;&gt;\n  {{ task.input.metadata }}\n&lt;\/div&gt;\n\n&lt;crowd-form&gt;\n    &lt;input name=&quot;annotations&quot; id=&quot;annotations&quot; type=&quot;hidden&quot;&gt;\n\n     &lt;!-- Prevent crowd-form from creating its own button --&gt;\n    &lt;crowd-button form-action=&quot;submit&quot; style=&quot;display: none;&quot;&gt;&lt;\/crowd-button&gt;\n&lt;\/crowd-form&gt;\n\n&lt;!-- Custom annotation user interface is rendered here --&gt;\n&lt;div id=&quot;root&quot;&gt;&lt;\/div&gt;\n\n&lt;crowd-button id=&quot;submitButton&quot;&gt;Submit&lt;\/crowd-button&gt;\n\n&lt;script&gt;\n    document.querySelector('crowd-form').onsubmit = function() {\n        document.getElementById('annotations').value = JSON.stringify(JSON.parse(document.querySelector('pre').innerText));\n    };\n\n    document.getElementById('submitButton').onclick = function() {\n        document.querySelector('crowd-form').submit();\n    };\n&lt;\/script&gt;\n\n&lt;script src=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/js\/1.3e5a6849.chunk.js&quot;&gt;&lt;\/script&gt;\n&lt;script src=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/js\/main.96e12312.chunk.js&quot;&gt;&lt;\/script&gt;\n&lt;script src=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/js\/runtime~main.229c360f.js&quot;&gt;&lt;\/script&gt;\n<\/code><\/pre>\n<p>Code source<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-01-16 15:42:48.977 UTC",
        "Answer_score":2.0,
        "Owner_location":"Lviv, Lviv Oblast, Ukraine",
        "Answer_last_edit_date":"2020-06-20 09:12:55.06 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59717227",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54171261,
        "Question_title":"ClientError: train channel is not specified with AWS object_detection_augmented_manifest_training using ground truth images",
        "Question_body":"<p>I have completed a labelling job in AWS ground truth and started working on the notebook template for object detection.<\/p>\n\n<p>I have 2 manifests which has 293 labeled images for birds in a train and validation set like this:<\/p>\n\n<pre><code>{\"source-ref\":\"s3:\/\/XXXXXXX\/Train\/Blackbird_1.JPG\",\"Bird-Label-Train\":{\"workerId\":XXXXXXXX,\"imageSource\":{\"s3Uri\":\"s3:\/\/XXXXXXX\/Train\/Blackbird_1.JPG\"},\"boxesInfo\":{\"annotatedResult\":{\"boundingBoxes\":[{\"width\":1612,\"top\":841,\"label\":\"Blackbird\",\"left\":1276,\"height\":757}],\"inputImageProperties\":{\"width\":3872,\"height\":2592}}}},\"Bird-Label-Train-metadata\":{\"type\":\"groundtruth\/custom\",\"job-name\":\"bird-label-train\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-01-16T17:28:23+0000\"}}\n<\/code><\/pre>\n\n<p>Below are the parameters I am using for the notebook instance:<\/p>\n\n<pre><code>training_params = \\\n{\n    \"AlgorithmSpecification\": {\n        \"TrainingImage\": training_image, # NB. This is one of the named constants defined in the first cell.\n        \"TrainingInputMode\": \"Pipe\"\n    },\n    \"RoleArn\": role,\n    \"OutputDataConfig\": {\n        \"S3OutputPath\": s3_output_path\n    },\n    \"ResourceConfig\": {\n        \"InstanceCount\": 1,   \n        \"InstanceType\": \"ml.p3.2xlarge\",\n        \"VolumeSizeInGB\": 5\n    },\n    \"TrainingJobName\": job_name,\n    \"HyperParameters\": { # NB. These hyperparameters are at the user's discretion and are beyond the scope of this demo.\n         \"base_network\": \"resnet-50\",\n         \"use_pretrained_model\": \"1\",\n         \"num_classes\": \"1\",\n         \"mini_batch_size\": \"16\",\n         \"epochs\": \"5\",\n         \"learning_rate\": \"0.001\",\n         \"lr_scheduler_step\": \"3,6\",\n         \"lr_scheduler_factor\": \"0.1\",\n         \"optimizer\": \"rmsprop\",\n         \"momentum\": \"0.9\",\n         \"weight_decay\": \"0.0005\",\n         \"overlap_threshold\": \"0.5\",\n         \"nms_threshold\": \"0.45\",\n         \"image_shape\": \"300\",\n         \"label_width\": \"350\",\n         \"num_training_samples\": str(num_training_samples)\n    },\n    \"StoppingCondition\": {\n        \"MaxRuntimeInSeconds\": 86400\n    },\n \"InputDataConfig\": [\n    {\n        \"ChannelName\": \"train\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                \"S3Uri\": s3_train_data_path,\n                \"S3DataDistributionType\": \"FullyReplicated\",\n                \"AttributeNames\": [\"source-ref\",\"Bird-Label-Train\"] # NB. This must correspond to the JSON field names in your augmented manifest.\n            }\n        },\n        \"ContentType\": \"image\/jpeg\",\n        \"RecordWrapperType\": \"None\",\n        \"CompressionType\": \"None\"\n    },\n    {\n        \"ChannelName\": \"validation\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                \"S3Uri\": s3_validation_data_path,\n                \"S3DataDistributionType\": \"FullyReplicated\",\n                \"AttributeNames\": [\"source-ref\",\"Bird-Label\"] # NB. This must correspond to the JSON field names in your augmented manifest.\n            }\n        },\n        \"ContentType\": \"image\/jpeg\",\n        \"RecordWrapperType\": \"None\",\n        \"CompressionType\": \"None\"\n    }\n]\n<\/code><\/pre>\n\n<p>I would end up with this being printed after running my ml.p3.2xlarge instance:<\/p>\n\n<pre><code>InProgress Starting\nInProgress Starting\nInProgress Starting\nInProgress Training\nFailed Failed\n<\/code><\/pre>\n\n<p>Followed by this error message: \n<strong>'ClientError: train channel is not specified.'<\/strong><\/p>\n\n<p>Does anyone have any thoughts for how I can get this running with no errors? Any help is much apreciated!<\/p>\n\n<p><strong>Successful run:<\/strong> Below is the paramaters that were used, along with the Augmented Manifest JSON Objects for a successful run.<\/p>\n\n<pre><code>training_params = \\\n{\n    \"AlgorithmSpecification\": {\n        \"TrainingImage\": training_image, # NB. This is one of the named constants defined in the first cell.\n        \"TrainingInputMode\": \"Pipe\"\n    },\n    \"RoleArn\": role,\n    \"OutputDataConfig\": {\n        \"S3OutputPath\": s3_output_path\n    },\n    \"ResourceConfig\": {\n        \"InstanceCount\": 1,   \n        \"InstanceType\": \"ml.p3.2xlarge\",\n        \"VolumeSizeInGB\": 50\n    },\n    \"TrainingJobName\": job_name,\n    \"HyperParameters\": { # NB. These hyperparameters are at the user's discretion and are beyond the scope of this demo.\n         \"base_network\": \"resnet-50\",\n         \"use_pretrained_model\": \"1\",\n         \"num_classes\": \"3\",\n         \"mini_batch_size\": \"1\",\n         \"epochs\": \"5\",\n         \"learning_rate\": \"0.001\",\n         \"lr_scheduler_step\": \"3,6\",\n         \"lr_scheduler_factor\": \"0.1\",\n         \"optimizer\": \"rmsprop\",\n         \"momentum\": \"0.9\",\n         \"weight_decay\": \"0.0005\",\n         \"overlap_threshold\": \"0.5\",\n         \"nms_threshold\": \"0.45\",\n         \"image_shape\": \"300\",\n         \"label_width\": \"350\",\n         \"num_training_samples\": str(num_training_samples)\n    },\n    \"StoppingCondition\": {\n        \"MaxRuntimeInSeconds\": 86400\n    },\n    \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                    \"S3Uri\": s3_train_data_path,\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                    \"AttributeNames\": attribute_names # NB. This must correspond to the JSON field names in your **TRAIN** augmented manifest.\n                }\n            },\n            \"ContentType\": \"application\/x-recordio\",\n            \"RecordWrapperType\": \"RecordIO\",\n            \"CompressionType\": \"None\"\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                    \"S3Uri\": s3_validation_data_path,\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                    \"AttributeNames\": [\"source-ref\",\"ValidateBird\"] # NB. This must correspond to the JSON field names in your **VALIDATION** augmented manifest.\n                }\n            },\n            \"ContentType\": \"application\/x-recordio\",\n            \"RecordWrapperType\": \"RecordIO\",\n            \"CompressionType\": \"None\"\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>Training Augmented Manifest File generated during the running of the training job<\/p>\n\n<pre><code>Line 1\n{\"source-ref\":\"s3:\/\/XXXXX\/Train\/Blackbird_1.JPG\",\"TrainBird\":{\"annotations\":[{\"class_id\":0,\"width\":1613,\"top\":840,\"height\":766,\"left\":1293}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"TrainBird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:21:29.829003\",\"type\":\"groundtruth\/object-detection\"}}\n\n\nLine 2\n{\"source-ref\":\"s3:\/\/xxxxx\/Train\/Blackbird_2.JPG\",\"TrainBird\":{\"annotations\":[{\"class_id\":0,\"width\":897,\"top\":665,\"height\":1601,\"left\":1598}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"TrainBird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:22:34.502274\",\"type\":\"groundtruth\/object-detection\"}}\n\n\nLine 3\n{\"source-ref\":\"s3:\/\/XXXXX\/Train\/Blackbird_3.JPG\",\"TrainBird\":{\"annotations\":[{\"class_id\":0,\"width\":1040,\"top\":509,\"height\":1695,\"left\":1548}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"TrainBird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:20:26.660164\",\"type\":\"groundtruth\/object-detection\"}}\n<\/code><\/pre>\n\n<p>I then unzip the model.tar file to get the following files:hyperparams.JSON, model_algo_1-0000.params and model_algo_1-symbol<\/p>\n\n<p>hyperparams.JSON looks like this:<\/p>\n\n<pre><code>{\"label_width\": \"350\", \"early_stopping_min_epochs\": \"10\", \"epochs\": \"5\", \"overlap_threshold\": \"0.5\", \"lr_scheduler_factor\": \"0.1\", \"_num_kv_servers\": \"auto\", \"weight_decay\": \"0.0005\", \"mini_batch_size\": \"1\", \"use_pretrained_model\": \"1\", \"freeze_layer_pattern\": \"\", \"lr_scheduler_step\": \"3,6\", \"early_stopping\": \"False\", \"early_stopping_patience\": \"5\", \"momentum\": \"0.9\", \"num_training_samples\": \"11\", \"optimizer\": \"rmsprop\", \"_tuning_objective_metric\": \"\", \"early_stopping_tolerance\": \"0.0\", \"learning_rate\": \"0.001\", \"kv_store\": \"device\", \"nms_threshold\": \"0.45\", \"num_classes\": \"1\", \"base_network\": \"resnet-50\", \"nms_topk\": \"400\", \"_kvstore\": \"device\", \"image_shape\": \"300\"}\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-13 17:15:04.507 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2019-02-24 10:45:41.75 UTC",
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-sagemaker|hyperparameters",
        "Question_view_count":1312,
        "Owner_creation_date":"2019-01-13 16:58:44.313 UTC",
        "Owner_last_access_date":"2019-08-05 19:10:01.83 UTC",
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>Thank you again for your help. All of which were valid in helping me get further. Having received a response on the AWS forum pages, I finally got it working.<\/p>\n\n<p>I understood that my JSON was slightly different to the augmented manifest training guide. Having gone back to basics, I created another labelling job, but used the 'Bounding Box' type as opposed to the 'Custom - Bounding box template'. My output matched what was expected. This ran with no errors!<\/p>\n\n<p>As my purpose was to have multiple labels, I was able to edit the files and mapping of my output manifests, which also worked!<\/p>\n\n<p>i.e.<\/p>\n\n<pre><code>{\"source-ref\":\"s3:\/\/xxxxx\/Blackbird_15.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":0,\"width\":2023,\"top\":665,\"height\":1421,\"left\":1312}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.174131\",\"type\":\"groundtruth\/object-detection\"}}\n{\"source-ref\":\"s3:\/\/xxxx\/Pigeon_19.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":2,\"width\":784,\"top\":634,\"height\":1657,\"left\":1306}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"2\":\"Pigeon\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.074809\",\"type\":\"groundtruth\/object-detection\"}} \n<\/code><\/pre>\n\n<p>The original mapping was 0:'Bird' for all images through the labelling job.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_creation_date":"2019-02-10 12:22:10.897 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54171261",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73232032,
        "Question_title":"Start execution of existing SageMaker pipeline using Python SDK",
        "Question_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/run-pipeline.html#run-pipeline-prereq\" rel=\"nofollow noreferrer\">SageMaker documentatin<\/a> explains how to run a pipeline, but it assumes I have just <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html\" rel=\"nofollow noreferrer\">defined it<\/a> and I have the object <code>pipeline<\/code> available.<\/p>\n<p>How can I run an <strong>existing<\/strong> pipeline with <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"nofollow noreferrer\">Python SDK<\/a>?<\/p>\n<p>I know how to read a pipeline with AWS CLI (i.e. <code>aws sagemaker describe-pipeline --pipeline-name foo<\/code>). Can the same be done with Python code? Then I would have <code>pipeline<\/code> object ready to use.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-04 07:37:15.513 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-08-04 07:42:19.403 UTC",
        "Question_score":0,
        "Question_tags":"aws-sdk|amazon-sagemaker",
        "Question_view_count":106,
        "Owner_creation_date":"2008-08-01 18:28:24.817 UTC",
        "Owner_last_access_date":"2022-09-24 15:32:27.99 UTC",
        "Owner_reputation":16694,
        "Owner_up_votes":3004,
        "Owner_down_votes":154,
        "Owner_views":3155,
        "Answer_body":"<p>If the Pipeline has been created, you can use the Python Boto3 SDK to make the <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.start_pipeline_execution\" rel=\"nofollow noreferrer\"><code>StartPipelineExecution<\/code><\/a> API call.<\/p>\n<pre><code>response = client.start_pipeline_execution(\n    PipelineName='string',\n    PipelineExecutionDisplayName='string',\n    PipelineParameters=[\n        {\n            'Name': 'string',\n            'Value': 'string'\n        },\n    ],\n    PipelineExecutionDescription='string',\n    ClientRequestToken='string',\n    ParallelismConfiguration={\n        'MaxParallelExecutionSteps': 123\n    }\n)\n<\/code><\/pre>\n<p>If you prefer AWS CLI, the most basic call is:<\/p>\n<pre><code>aws sagemaker start-pipeline-execution --pipeline-name &lt;name-of-the-pipeline&gt;\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-04 15:50:56.873 UTC",
        "Answer_score":1.0,
        "Owner_location":"Poland",
        "Answer_last_edit_date":"2022-08-11 10:07:51.007 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73232032",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55020390,
        "Question_title":"How do I start an AWS Sagemaker training job with GPU access in my docker container?",
        "Question_body":"<p>I have some python code that trains a Neural Network using tensorflow. <\/p>\n\n<p>I've created a docker image based on a tensorflow\/tensorflow:latest-gpu-py3 image that runs my python script.\nWhen I start an EC2 p2.xlarge instance I can run my docker container using the command<\/p>\n\n<pre><code>docker run --runtime=nvidia cnn-userpattern train\n<\/code><\/pre>\n\n<p>and the container with my code runs with no errors and uses the host GPU. <\/p>\n\n<p>The problem is, when I try to run the same container in an AWS Sagemaker training job with instance ml.p2.xlarge (I also tried with ml.p3.2xlarge), the algorithm fails with error code:<\/p>\n\n<blockquote>\n  <p>ImportError: libcuda.so.1: cannot open shared object file: No such file or directory<\/p>\n<\/blockquote>\n\n<p>Now I know what that error code means. It means that the runtime environment of the docker host is not set to \"nvidia\". The AWS documentation says that the command used to run the docker image is always<\/p>\n\n<pre><code>docker run image train\n<\/code><\/pre>\n\n<p>which would work if the default runtime is set to \"nvidia\" in the docker\/deamon.json. Is there any way to edit the host deamon.json or tell docker in the Dockerfile to use \"--runtime=nvidia\"?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-03-06 10:05:52.687 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|docker|tensorflow|gpu|amazon-sagemaker",
        "Question_view_count":1858,
        "Owner_creation_date":"2013-06-01 11:17:36.943 UTC",
        "Owner_last_access_date":"2022-06-24 11:54:43.47 UTC",
        "Owner_reputation":344,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Answer_body":"<p>With some help of the AWS support service we were able to find the problem.\nThe docker image I used to run my code on was, as I said tensorflow\/tensorflow:latest-gpu-py3 (available on <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a>)<\/p>\n\n<p>the \"latest\" tag refers to version 1.12.0 at this time. The problem was not my own, but with this version of the docker image. <\/p>\n\n<p>If I base my docker image on tensorflow\/tensorflow:1.10.1-gpu-py3, it runs as it should and uses the GPU fully. <\/p>\n\n<p>Apparently the default runtime is set to \"nvidia\" in the docker\/deamon.json on all GPU instances of AWS sagemaker.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-03-07 13:17:35.197 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55020390",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72039147,
        "Question_title":"Is there a way to include custom Regression Metrics in ModelQualityMonitor in AWS sagemaker?",
        "Question_body":"<p>I have successfully initialized a ModelQualityMonitor object.\nThen I created a monitoring schedule using the CreateMonitoringSchedule API! In the background sagemaker runs two processing jobs which merges the ground truth data with the collected endpoint data and then analyzes and creates the predefined regression metrics:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html<\/a><\/p>\n<p>Unfortunately, I am missing the MAPE (Mean Absolute Percentage Error) in the metrics, and would like to create this with in the future (also in CloudWatch).<\/p>\n<p>Sagemaker provides the following functionalities:<\/p>\n<ul>\n<li>Preprocessing and Postprocessing:\nIn addition to using the built-in mechanisms, you can extend the code with the preprocessing and postprocessing scripts.<\/li>\n<li>Bring Your Own Containers:\nAmazon SageMaker Model Monitor provides a prebuilt container with ability to analyze the data captured from endpoints for tabular datasets. If you would like to bring your own container, Model Monitor provides extension points which you can leverage.<\/li>\n<li>CloudWatch Metrics for Bring Your Own Containers<\/li>\n<\/ul>\n<p>Those points are documented on this site: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-custom-monitoring-schedules.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-custom-monitoring-schedules.html<\/a><\/p>\n<p>How exactly can I achieve my target of including MAPE with the above points?<\/p>\n<p>Here is a code snippet of my current implementation:<\/p>\n<pre><code>from sagemaker.model_monitor.model_monitoring import ModelQualityMonitor\nfrom sagemaker.model_monitor import EndpointInput\nfrom sagemaker.model_monitor.dataset_format import DatasetFormat\n\n# Create the model quality monitoring object\nMQM = ModelQualityMonitor(\n    role=role,\n    instance_count=1,\n    instance_type=&quot;ml.m5.large&quot;,\n    volume_size_in_gb=20,\n    max_runtime_in_seconds=1800,\n    sagemaker_session=sagemaker_session,\n)\n\n# suggest a baseline\njob = MQM.suggest_baseline(\n    job_name=baseline_job_name,\n    baseline_dataset=&quot;.\/baseline.csv&quot;,\n    dataset_format=DatasetFormat.csv(header=True),\n    output_s3_uri=baseline_results_uri,\n    problem_type=&quot;Regression&quot;,\n    inference_attribute=&quot;predicted_price&quot;,\n    ground_truth_attribute=&quot;price&quot;,\n)\njob.wait(logs=False)\nbaseline_job = MQM.latest_baselining_job\n\n# create a monitoring schedule\nendpointInput = EndpointInput(\n    endpoint_name=&quot;dev-TestEndpoint&quot;,\n    destination=&quot;\/opt\/ml\/processing\/input_data&quot;,\n    inference_attribute=&quot;$.data.predicted_price&quot;\n)\nMQM.create_monitoring_schedule(\n    monitor_schedule_name=&quot;DS-Schedule&quot;,\n    endpoint_input=endpointInput,\n    output_s3_uri=baseline_results_uri,\n    constraints=baseline_job.suggested_constraints(),\n    problem_type=&quot;Regression&quot;,\n    ground_truth_input=ground_truth_upload_path,\n    schedule_cron_expression=&quot;cron(0 * ? * * *)&quot;, # hourly\n    enable_cloudwatch_metrics=True\n)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-28 06:43:38.337 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|boto3|monitoring|amazon-sagemaker",
        "Question_view_count":165,
        "Owner_creation_date":"2021-02-18 15:25:28.947 UTC",
        "Owner_last_access_date":"2022-09-23 10:35:08.913 UTC",
        "Owner_reputation":160,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":"<p>Amazon SageMaker model monitor only supports metrics that are defined <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html\" rel=\"nofollow noreferrer\">here<\/a> out of the box.\nIf you need to include another metric such as MAPE (Mean Absolute Percentage Error) in your case, you will have to rely on BYOC approach, note that with this approach you cannot &quot;add&quot; a metric to the available list, unfortunately you will have to implement the entire suite of metrics yourself. I understand this is not ideal for customers, I'd encourage you to reach out to your AWS account manager to create a request to add MAPE (Mean Absolute Percentage Error) as a supported metric in the long run. I've made a note of it as well and will rely it back to the team.<\/p>\n<p>In the meantime, you can find examples on how to BYOC <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/detect-nlp-data-drift-using-custom-amazon-sagemaker-model-monitor\/\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>I work for AWS but my opinions are my own.<\/p>\n<p>Thanks,\nRaghu<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-04-28 18:40:15.177 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72039147",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":47625056,
        "Question_title":"Amazon Machine Learning and SageMaker algorithms",
        "Question_body":"<p>1) According to <a href=\"http:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/learning-algorithm.html\" rel=\"nofollow noreferrer\">http:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/learning-algorithm.html<\/a> Amazon ML uses SGD. However I can't find how many hidden layers are used in the neural network?<\/p>\n\n<p>2) Can someone confirm that SageMaker would be able to do what Amazon ML does? i.e. SageMaker is more powerful than Amazon ML?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2017-12-04 00:55:11.92 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2017-12-13 05:18:34.293 UTC",
        "Question_score":5,
        "Question_tags":"amazon-web-services|amazon-machine-learning|amazon-sagemaker",
        "Question_view_count":2794,
        "Owner_creation_date":"2010-01-25 07:55:40.363 UTC",
        "Owner_last_access_date":"2022-09-22 04:51:33.373 UTC",
        "Owner_reputation":2211,
        "Owner_up_votes":166,
        "Owner_down_votes":10,
        "Owner_views":176,
        "Answer_body":"<p>I'm not sure about Amazon ML but SageMaker uses the docker containers listed here for the built-in training: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html<\/a><\/p>\n\n<p>So, in general, anything you can do with Amazon ML you should be able to do with SageMaker (although Amazon ML has a pretty sweet schema editor).<\/p>\n\n<p>You can check out each of those containers to dive deep on how it all works.<\/p>\n\n<p>You can find an exhaustive list of available algorithms in SageMaker here:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html<\/a><\/p>\n\n<p>For now, as of December 2017, these algorithms are all available:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner.html\" rel=\"noreferrer\">Linear Learner<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/fact-machines.html\" rel=\"noreferrer\">Factorization Machines<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"noreferrer\">XGBoost Algorithm<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"noreferrer\">Image Classification Algorithm<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/seq-2-seq.html\" rel=\"noreferrer\">Amazon SageMaker Sequence2Sequence<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/k-means.html\" rel=\"noreferrer\">K-Means Algorithm<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pca.html\" rel=\"noreferrer\">Principal Component Analysis (PCA)<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lda.html\" rel=\"noreferrer\">Latent Dirichlet Allocation (LDA)<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ntm.html\" rel=\"noreferrer\">Neural Topic Model (NTM)<\/a><\/li>\n<\/ul>\n\n<p>The general SageMaker SDK interface to these algorithms looks something like this:<\/p>\n\n<pre><code>from sagemaker import KMeans\nkmeans = KMeans(role=\"SageMakerRole\",\n                train_instance_count=2,\n                train_instance_type='ml.c4.8xlarge',\n                data_location=\"s3:\/\/training_data\/\",\n                output_path=\"s3:\/\/model_artifacts\/\",\n                k=10)\n<\/code><\/pre>\n\n<p>The libraries here: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples<\/a>\nand here: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a> are particularly useful for playing with SageMaker.<\/p>\n\n<p>You can also make use of Spark with SageMaker the Spark library here: <a href=\"https:\/\/github.com\/aws\/sagemaker-spark\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-spark<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2017-12-05 11:00:55.643 UTC",
        "Answer_score":7.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2017-12-06 15:55:02.21 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47625056",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66817781,
        "Question_title":"What are the differences between AWS sagemaker and sagemaker_pyspark?",
        "Question_body":"<p>I'm currently running a quick Machine Learning proof of concept on AWS with SageMaker, and I've come across two libraries: <code>sagemaker<\/code> and <code>sagemaker_pyspark<\/code>. I would like to work with distributed data. My questions are:<\/p>\n<ol>\n<li><p>Is using <code>sagemaker<\/code> the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented <code>sagemaker_pyspark<\/code>? Based on this assumption, I do not understand what it would offer regarding using <code>scikit-learn<\/code> on a SageMaker notebook (in terms of computing capabilities).<\/p>\n<\/li>\n<li><p>Is it normal for something like <code>model = xgboost_estimator.fit(training_data)<\/code> to take 4 minutes to run with <code>sagemaker_pyspark<\/code> for a small set of test data? I see that what it does below is to train the model and also create an Endpoint to be able to offer its predictive services, and I assume that this endpoint is deployed on an EC2 instance that is created and started at the moment. Correct me if I'm wrong. I assume this from how the estimator is defined:<\/p>\n<\/li>\n<\/ol>\n<pre><code>from sagemaker import get_execution_role\nfrom sagemaker_pyspark.algorithms import XGBoostSageMakerEstimator\n\n\nxgboost_estimator = XGBoostSageMakerEstimator (\n    trainingInstanceType = &quot;ml.m4.xlarge&quot;,\n    trainingInstanceCount = 1,\n    endpointInstanceType = &quot;ml.m4.xlarge&quot;,\n    endpointInitialInstanceCount = 1,\n    sagemakerRole = IAMRole(get_execution_role())\n)\n\nxgboost_estimator.setNumRound(1)\n<\/code><\/pre>\n<p>If so, is there a way to reuse the same endpoint with different training jobs so that I don't have to wait for a new endpoint to be created each time?<\/p>\n<ol start=\"3\">\n<li><p>Does <code>sagemaker_pyspark<\/code> support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/p>\n<\/li>\n<li><p>Do you know if <code>sagemaker_pyspark<\/code> can perform hyperparameter optimization? From what I see, <code>sagemaker<\/code> offers the <code>HyperparameterTuner<\/code> class, but I can't find anything like it in <code>sagemaker_pyspark<\/code>. I suppose it is a more recent library and there is still a lot of functionality to implement.<\/p>\n<\/li>\n<li><p>I am a bit confused about the concept of <code>entry_point<\/code> and <code>container<\/code>\/<code>image_name<\/code> (both possible input arguments for the <code>Estimator<\/code> object from the <code>sagemaker<\/code> library): can you deploy models with and without containers? why would you use model containers? Do you always need to define the model externally with the <code>entry_point<\/code> script? It is also confusing that the class <code>AlgorithmEstimator<\/code> allows the input argument <code>algorithm_arn<\/code>; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/li>\n<li><p>I see the <code>sagemaker<\/code> library offers SageMaker Pipelines, which seem to be very handy for deploying properly structured ML workflows. However, I don't think this is available with <code>sagemaker_pyspark<\/code>, so in that case, I would rather create my workflows with a combination of Step Functions (to orchestrate the entire thing), Glue processes (for ETL, preprocessing and feature\/target engineering) and SageMaker processes using <code>sagemaker_pyspark<\/code>.<\/p>\n<\/li>\n<li><p>I also found out that <code>sagemaker<\/code> has the <code>sagemaker.sparkml.model.SparkMLModel<\/code> object. What is the difference between this and what <code>sagemaker_pyspark<\/code> offers?<\/p>\n<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-26 13:23:12.75 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-04-14 11:03:24.67 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|pyspark|aws-glue|amazon-sagemaker|aws-step-functions",
        "Question_view_count":242,
        "Owner_creation_date":"2013-08-09 12:45:12.53 UTC",
        "Owner_last_access_date":"2021-04-28 18:42:48.283 UTC",
        "Owner_reputation":135,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":75,
        "Answer_body":"<p><code>sagemaker<\/code> is the SageMaker Python SDK. It calls SageMaker-related AWS service APIs on your behalf. You don't need to use it, but it can make life easier<\/p>\n<blockquote>\n<ol>\n<li>Is using sagemaker the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented sagemaker_pyspark?<\/li>\n<\/ol>\n<\/blockquote>\n<p>No. You can run distributed training jobs using <code>sagemaker<\/code> (see <code>instance_count<\/code> parameter)<\/p>\n<p><code>sagemaker_pyspark<\/code> facilitates calling SageMaker-related AWS service APIs from Spark. Use it if you want to use SageMaker services from Spark<\/p>\n<blockquote>\n<ol start=\"2\">\n<li>Is it normal for something like model = xgboost_estimator.fit(training_data) to take 4 minutes to run with sagemaker_pyspark for a small set of test data?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, it takes a few minutes for an EC2 instance to spin-up. Use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">Local Mode<\/a> if you want to iterate more quickly locally. Note: Local Mode won't work with SageMaker built-in algorithms, but you can prototype with (non AWS) XGBoost\/SciKit-Learn<\/p>\n<blockquote>\n<ol start=\"3\">\n<li>Does sagemaker_pyspark support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, but you'd probably want to extend <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/api.html#sagemakerestimator\" rel=\"nofollow noreferrer\">SageMakerEstimator<\/a>. Here you can provide the <code>trainingImage<\/code> URI<\/p>\n<blockquote>\n<ol start=\"4\">\n<li>Do you know if sagemaker_pyspark can perform hyperparameter optimization?<\/li>\n<\/ol>\n<\/blockquote>\n<p>It does not appear so. It'd probably be easier just to do this from SageMaker itself though<\/p>\n<blockquote>\n<p>can you deploy models with and without containers?<\/p>\n<\/blockquote>\n<p>You can certainly host your own models any way you want. But if you want to use SageMaker model inference hosting, then containers are required<\/p>\n<blockquote>\n<p>why would you use model containers?<\/p>\n<\/blockquote>\n<blockquote>\n<p>Do you always need to define the model externally with the entry_point script?<\/p>\n<\/blockquote>\n<p>The whole Docker thing makes bundling dependencies easier, and also makes things language\/runtime-neutral. SageMaker doesn't care if your algorithm is in Python or Java or Fortran. But it needs to know how to &quot;run&quot; it, so you tell it a working directory and a command to run. This is the entry point<\/p>\n<blockquote>\n<p>It is also confusing that the class AlgorithmEstimator allows the input argument algorithm_arn; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/blockquote>\n<p>Please clarify which &quot;three&quot; you are referring to<\/p>\n<p>6 is not a question, so no answer required :)<\/p>\n<blockquote>\n<ol start=\"7\">\n<li>What is the difference between this and what sagemaker_pyspark offers?<\/li>\n<\/ol>\n<\/blockquote>\n<p>sagemaker_pyspark lets you call SageMaker services from Spark, whereas SparkML Serving lets you use Spark ML services from SageMaker<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-03-30 15:54:38.127 UTC",
        "Answer_score":1.0,
        "Owner_location":"Australia",
        "Answer_last_edit_date":"2021-10-14 17:03:02.413 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66817781",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73042521,
        "Question_title":"How to use scikit learn model from inside sagemaker 'model.tar.gz' file?",
        "Question_body":"<p>New to Sagemaker..<\/p>\n<p>Trained a &quot;linear-learner&quot; classification model using the Sagemaker API, and it saved a &quot;model.tar.gz&quot; file in my s3 path. From what I understand SM just used an image of a scikit logreg model.<\/p>\n<p>Finally, I'd like to gain access to the model object itself, so I unpacked the &quot;model.tar.gz&quot; file only to find another file called &quot;model_algo-1&quot; with no extension.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DPUMO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DPUMO.png\" alt=\"contents of unknown file\" \/><\/a><\/p>\n<p>Can anyone tell me how I can find the &quot;real&quot; modeling object without using the inference\/Endpoint delpoy API provided by Sagemaker? There are some things I want to look at manually.<\/p>\n<p>Thanks,\nCraig<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-19 19:28:20.593 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":42,
        "Owner_creation_date":"2016-09-22 14:43:34.617 UTC",
        "Owner_last_access_date":"2022-09-22 14:50:58.827 UTC",
        "Owner_reputation":129,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>Linear-Learner is a built in algorithm written using MX-net and the binary is also MXNET compatible. You can't use this model outside of SageMaker as there is no open source implementation for this.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-21 23:57:06.9 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73042521",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62737008,
        "Question_title":"How to get Sagemaker Batch Transform Job status printed out in my python notebook?",
        "Question_body":"<p>I am running a python notebook which is initiating a Batch Transform Job in Sagemaker. However, I want to also print the status &quot;Failed&quot;, &quot;In Progress&quot; and &quot;Completed&quot; once the job is complete running. As of now, I am only able to start the Batch Transform Job (rf=random forest) but I am not certain how to get the job status print outs. Can someone help with that given my script below?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/SU0ln.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/SU0ln.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<pre><code>rf_transformer = rf.transformer(\n                                instance_count,\n                                instance_type,\n                                strategy=strategy,\n                                output_path=output_path,\n                                max_payload=max_payload)\n\nrf_transformer.transform(\n                                str('s3:\/\/batch_scoring\/rf_output),\n                                content_type='text\/csv',\n                                compression_type='Gzip'\n                         )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-05 04:59:20.127 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python-3.x|amazon-s3|amazon-sagemaker",
        "Question_view_count":612,
        "Owner_creation_date":"2020-06-03 22:21:17.74 UTC",
        "Owner_last_access_date":"2020-09-13 17:40:31.793 UTC",
        "Owner_reputation":91,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Answer_body":"<p>You can do it with:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>job_name = rf_transformer.latest_transform_job.name\nrf_transformer.sagemaker_session.describe_transform_job(job_name)['TransformJobStatus']\n<\/code><\/pre>\n<p>You can also use the AWS SDK directly, if you wish:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nsagemaker_client = boto3.client('sagemaker')\nsagemaker_client.describe_transform_job(job_name)['TransformJobStatus']\n<\/code><\/pre>\n<p>API documentation: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeTransformJob.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-07 20:37:11.75 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62737008",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71120471,
        "Question_title":"How to determine size of images available in aws?",
        "Question_body":"<p>I'm using one of the images listed here <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md<\/a>, to create an sagemaker endpoint, but I keep getting &quot;failed reason: Image size 15136109518 is greater that suppported size 1073741824&quot; .<\/p>\n<p>is there a way to find out the size of images provided <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md<\/a> or any aws managed images?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-15 02:29:08.237 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|amazon-ecr",
        "Question_view_count":153,
        "Owner_creation_date":"2020-05-30 00:10:41.983 UTC",
        "Owner_last_access_date":"2022-09-24 19:51:20.543 UTC",
        "Owner_reputation":525,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":98,
        "Answer_body":"<p>I suspect you are trying to deploy a serverless endpoint provisioned with 1GB of memory. As discussed <a href=\"https:\/\/repost.aws\/questions\/QU35dVp2D9SKKUnnVYGw9Z7A\/how-to-check-determine-image-container-size-for-aws-managed-images\" rel=\"nofollow noreferrer\">here<\/a> &quot;You can increase the memory size of your endpoint with the MemorySizeInMB parameter, more info in this documentation: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config%22\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config&quot;<\/a><\/p>\n<p>In order to view the uncompressed size of an image you can use the following example command:<\/p>\n<pre><code>$ docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-training:1.15.2-cpu-py27-ubuntu18.04\n\n$ docker inspect -f &quot;{{ .Size }}&quot; 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-training:1.15.2-cpu-py27-ubuntu18.04\n<\/code><\/pre>\n<p>Kindly also note that you will need to provision enough memory to accommodate your model as well. Please see this <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html#serverless-endpoints-how-it-works-memory\" rel=\"nofollow noreferrer\">link<\/a> for more information.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-22 22:30:51.34 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71120471",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56353814,
        "Question_title":"Batch transform job results in \"InternalServerError\" with data file >100MB",
        "Question_body":"<p>I'm using Sagemaker in order to perform binary classification on time series, each sample being a numpy array of shape [24,11] (24h, 11features). I used a tensorflow model in script mode, my script being very similar to the one I used as reference:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py<\/a><\/p>\n\n<p>The training reported success and I was able to deploy a model for batch transformation. The transform job works fine when I input just a few samples (say, [10,24,11]), but it returns an <code>InternalServerError<\/code> when I input more samples for prediction (for example, [30000, 24, 11], which size is >100MB).<\/p>\n\n<p>Here is the error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-6-0c46f7563389&gt; in &lt;module&gt;()\n     32 \n     33 # Then wait until transform job is completed\n---&gt; 34 tf_transformer.wait()\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    133     def wait(self):\n    134         self._ensure_last_transform_job()\n--&gt; 135         self.latest_transform_job.wait()\n    136 \n    137     def _ensure_last_transform_job(self):\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    207 \n    208     def wait(self):\n--&gt; 209         self.sagemaker_session.wait_for_transform_job(self.job_name)\n    210 \n    211     @staticmethod\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_transform_job(self, job, poll)\n    893         \"\"\"\n    894         desc = _wait_until(lambda: _transform_job_status(self.sagemaker_client, job), poll)\n--&gt; 895         self._check_job_status(job, desc, 'TransformJobStatus')\n    896         return desc\n    897 \n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n    915             reason = desc.get('FailureReason', '(No reason provided)')\n    916             job_type = status_key_name.replace('JobStatus', ' job')\n--&gt; 917             raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))\n    918 \n    919     def wait_for_endpoint(self, endpoint, poll=5):\n\nValueError: Error for Transform job Tensorflow-batch-transform-2019-05-29-02-56-00-477: Failed Reason: InternalServerError: We encountered an internal error.  Please try again.\n\n<\/code><\/pre>\n\n<p>I tried to use both SingleRecord and MultiRecord parameters when deploying the model but the result was the same, so I decided to keep MultiRecord. My transformer looks like that:<\/p>\n\n<pre><code>transformer = tf_estimator.transformer(\n    instance_count=1, \n    instance_type='ml.m4.xlarge',\n    max_payload = 100,\n    assemble_with = 'Line',\n    strategy='MultiRecord'\n)\n<\/code><\/pre>\n\n<p>At first I was using a json file as input for the transform job, and it threw the error : <\/p>\n\n<pre><code>Too much data for max payload size\n<\/code><\/pre>\n\n<p>So next I tried the jsonlines format (the .npy format is not supported as far as I understand), thinking that jsonlines could get split by Line and thus avoid the size error, but that's where I got the <code>InternalServerError<\/code>. Here is the related code:<\/p>\n\n<pre><code>#Convert test_x to jsonlines and save\ntest_x_list = test_x.tolist()\nfile_path ='data_cnn_test\/test_x.jsonl'\nfile_name='test_x.jsonl'\n\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)    \n\ninput_key = 'batch_transform_tf\/input\/{}'.format(file_name)\noutput_key = 'batch_transform_tf\/output'\ntest_input_location = 's3:\/\/{}\/{}'.format(bucket, input_key)\ntest_output_location = 's3:\/\/{}\/{}'.format(bucket, output_key)\n\ns3.upload_file(file_path, bucket, input_key)\n\n# Initialize the transformer object\ntf_transformer = sagemaker.transformer.Transformer(\n    base_transform_job_name='Tensorflow-batch-transform',\n    model_name='sagemaker-tensorflow-scriptmode-2019-05-29-02-46-36-162',\n    instance_count=1,\n    instance_type='ml.c4.2xlarge',\n    output_path=test_output_location,\n    assemble_with = 'Line'\n    )\n\n# Start the transform job\ntf_transformer.transform(test_input_location, content_type='application\/jsonlines', split_type='Line')\n<\/code><\/pre>\n\n<p>The list named test_x_list has a shape [30000, 24, 11], which corresponds to 30000 samples so I would like to return 30000 predictions.<\/p>\n\n<p>I suspect my jsonlines file isn't being split by Line and is of course too big to be processed in one batch, which throws the error, but I don't understand why it doesn't get split correctly. I am using the default output_fn and input_fn (I did not re-write those functions in my script).<\/p>\n\n<p>Any insight on what I could be doing wrong would be greatly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-29 05:43:39.707 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|tensorflow|amazon-sagemaker",
        "Question_view_count":1826,
        "Owner_creation_date":"2019-05-29 03:08:01.007 UTC",
        "Owner_last_access_date":"2021-06-28 08:28:46.13 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>I assume this is a duplicate of this AWS Forum post: <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0<\/a><\/p>\n\n<p>Anyway, for completeness I'll answer here as well.<\/p>\n\n<p>The issue is that you are serializing your dataset incorrectly when converting it into jsonlines:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)   \n<\/code><\/pre>\n\n<p>What the above is doing is creating a very large single-line containing your full dataset which is too big for single inference call to consume.<\/p>\n\n<p>I suggest you change your code to make each line a single sample so that inference can take place on individual samples instead of the whole dataset:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    for sample in test_x_list:\n        writer.write(sample)\n<\/code><\/pre>\n\n<p>If one sample at a time is too slow you can also play around with the <code>max_concurrent_transforms<\/code>, <code>strategy<\/code>, and <code>max_payload<\/code> parameters to be able to batch the data as well as run concurrent transforms if your algorithm can run in parallel - also, of course, you can split the data into multiple files and run transformations with more than just one node. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html<\/a> for additional detail on what these parameters do.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-05-31 19:12:00.12 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56353814",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51488308,
        "Question_title":"Failing to read data from s3 to a spark dataframe in Sagemaker",
        "Question_body":"<p>I'm trying to read a csv file on an s3 bucket (for which the sagemaker notebook has full access to) into a spark dataframe however I am hitting the following issue where <code>sagemaker-spark_2.11-spark_2.2.0-1.1.1.jar<\/code> can't be found. Any tips on how to resolve this is appreciate!<\/p>\n\n<pre><code>bucket = \"mybucket\"\nprefix = \"folder\/file.csv\"\ndf = spark.read.csv(\"s3:\/\/{}\/{}\/\".format(bucket,prefix))\n\nPy4JJavaError: An error occurred while calling o388.csv.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Error reading configuration file\nat java.util.ServiceLoader.fail(ServiceLoader.java:232)\nat java.util.ServiceLoader.parse(ServiceLoader.java:309)\nat java.util.ServiceLoader.access$200(ServiceLoader.java:185)\nat java.util.ServiceLoader$LazyIterator.hasNextService(ServiceLoader.java:357)\nat java.util.ServiceLoader$LazyIterator.hasNext(ServiceLoader.java:393)\nat java.util.ServiceLoader$1.hasNext(ServiceLoader.java:474)\nat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)\nat scala.collection.Iterator$class.foreach(Iterator.scala:893)\nat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\nat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\nat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\nat scala.collection.TraversableLike$class.filterImpl(TraversableLike.scala:247)\nat scala.collection.TraversableLike$class.filter(TraversableLike.scala:259)\nat scala.collection.AbstractTraversable.filter(Traversable.scala:104)\nat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:614)\nat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:190)\nat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:596)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:498)\nat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\nat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\nat py4j.Gateway.invoke(Gateway.java:282)\nat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\nat py4j.commands.CallCommand.execute(CallCommand.java:79)\nat py4j.GatewayConnection.run(GatewayConnection.java:238)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.FileNotFoundException: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker_pyspark\/jars\/sagemaker-spark_2.11-spark_2.2.0-1.1.1.jar (No such file or directory)\n    at java.util.zip.ZipFile.open(Native Method)\n    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:219)\n    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:149)\n    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:166)\n    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:103)\n    at sun.net.www.protocol.jar.URLJarFile.&lt;init&gt;(URLJarFile.java:93)\n    at sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:69)\n    at sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:84)\n    at sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:122)\n    at sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:150)\n    at java.net.URL.openStream(URL.java:1045)\n    at java.util.ServiceLoader.parse(ServiceLoader.java:304)\n    ... 26 more\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2018-07-23 23:08:14.417 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2018-07-23 23:18:41.18 UTC",
        "Question_score":0,
        "Question_tags":"amazon-s3|pyspark|amazon-sagemaker",
        "Question_view_count":2283,
        "Owner_creation_date":"2015-10-11 00:47:36.12 UTC",
        "Owner_last_access_date":"2022-09-22 16:32:40.907 UTC",
        "Owner_reputation":973,
        "Owner_up_votes":486,
        "Owner_down_votes":15,
        "Owner_views":82,
        "Answer_body":"<p>(Making comment to the original question as answer)<\/p>\n\n<p>It looks like a jupyter kernel issue. I had a similar issue and I used <code>Sparkmagic (pyspark)<\/code> kernel instead of <code>Sparkmagic (pyspark3)<\/code> and it is working fine. Follow instructions on this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/\" rel=\"nofollow noreferrer\">blog<\/a> and see if it helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-07-25 04:43:38.793 UTC",
        "Answer_score":1.0,
        "Owner_location":"Los Angeles, CA, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51488308",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66887340,
        "Question_title":"How do you clear the persistent storage for a notebook instance on AWS SageMaker?",
        "Question_body":"<p>So I'm running into the following error on AWS SageMaker when trying to save:<\/p>\n<blockquote>\n<p>Unexpected error while saving file: untitled.ipynb [Errno 28] No space left on device<\/p>\n<\/blockquote>\n<p>If I remove my notebook, create a new identical one and run it, everything works fine. However, I'm suspecting the Jupyter checkpoint takes up too much space if I save the notebook while it's running and therefore I'm running out of space. Sadly, getting more storage is not an option for me, so I'm wondering if there's any command I can use to clear the storage before running my notebook?<\/p>\n<p>More specifically, clearing the persistent storage in the beginning and at the end of the training process.<\/p>\n<p>I have googled like a maniac but there is no suggestion aside from &quot;just increase the amount of storage bro&quot; and that's why I'm asking the question here.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-31 11:51:06.313 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|jupyter-notebook|storage|amazon-sagemaker",
        "Question_view_count":973,
        "Owner_creation_date":"2017-09-02 17:20:57.017 UTC",
        "Owner_last_access_date":"2022-09-23 08:31:46.523 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>If you don't want your data to be persistent across multiple notebook runs, just store them in <code>\/tmp<\/code> which is not persistent. You have at least 10GB. More details <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/howitworks-create-ws.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2021-03-31 13:45:19.367 UTC",
        "Answer_score":0.0,
        "Owner_location":"Sweden",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66887340",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50669991,
        "Question_title":"AWS SageMaker is not authorized to perform: ecr:CreateRepository on resource: *",
        "Question_body":"<p>I am creating my own Docker image so that I can use my own models in AWS SageMaker. I sucessfully created a Docker image using command line inside the Jupyter Notebook in SageMaker ml.t2.medium instance using a customized Dockerfile:<\/p>\n\n<pre><code>REPOSITORY            TAG                 IMAGE ID            CREATED             SIZE\nsklearn               latest              01234212345        6 minutes ago       1.23GB\n<\/code><\/pre>\n\n<p>But when I run in Jupyter:<\/p>\n\n<pre><code>! aws ecr create-repository --repository-name sklearn\n<\/code><\/pre>\n\n<p>I get the following error:<\/p>\n\n<pre><code>An error occurred (AccessDeniedException) when calling the CreateRepository operation: User: arn:aws:sts::1234567:assumed-role\/AmazonSageMaker-ExecutionRole-12345\/SageMaker is not authorized to perform: ecr:CreateRepository on resource: *\n<\/code><\/pre>\n\n<p>I already set up SageMaker, EC2, EC2ContainerService permissions and the following policy for EC2Container but I still get the same error.<\/p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"sagemaker:*\",\n        \"ec2:*\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n<\/code><\/pre>\n\n<p>Any idea on how I can solve this issue?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2018-06-03 19:06:16.997 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-06-03 19:46:45.113 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|docker|scikit-learn|dockerfile|amazon-sagemaker",
        "Question_view_count":4509,
        "Owner_creation_date":"2016-09-29 20:35:09.097 UTC",
        "Owner_last_access_date":"2022-09-25 02:50:38.173 UTC",
        "Owner_reputation":4242,
        "Owner_up_votes":735,
        "Owner_down_votes":15,
        "Owner_views":421,
        "Answer_body":"<p>I solved the problem. We must set a permission at SageMaker Execution Role as following:<\/p>\n\n<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"ecr:*\"            ],\n        \"Resource\": \"*\"\n    }\n]}\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-06-04 15:31:01.857 UTC",
        "Answer_score":5.0,
        "Owner_location":"Brazil",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50669991",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57213293,
        "Question_title":"ClientError: lst should at least has three parts, but only has 1 parts for",
        "Question_body":"<p>I use SageMaker for research purpose in my study, hope somebody can help me out.\nError I get\nClientError: lst should at least has three parts, but only has 1 parts for '1 0 class_iphone6splus\/i6 (1).jpg'<\/p>\n\n<p>It is possible to create my own training job using the SageMaker GUI only?\nCause I'm totally new to AWS...\nThe built in algorithm that I wan to use is image classification. <\/p>\n\n<p>I have 400 images in JPG format for the datasets. Those images are from two different phone models which is iPhone 6s plus and iPhone7plus so that the system will classify them into two different classes. Both 200 each.<\/p>\n\n<p>S3 bucket\nIn train folder, I have two different folder to store those images which are class_iphone6splus and \nclass_iphone7plus 200 each for one class. The .lst file which are created by own use notepad++ name as data.lst are put in these two folder with the images cause I not sure where to put it.\nWhile in validation folder, I also store the same 400 images into another class_iphone6splus and \nclass_iphone7plus folder seperately by their class.<\/p>\n\n<p>Things store in .lst file<br>\nExample altogether is 400lines<br>\n1 0 class_iphone6splus\/i6 (1).jpg<br>\nuntil<br>\n200 0 class_iphone6splus\/i6 (200).jpg  <\/p>\n\n<p>201 1 class_iphone7plus\/i7 (1).jpg<br>\nuntil<br>\n400 1 class_iphone7plus\/i7 (200).jpg  <\/p>\n\n<p>Should I create two different folder in the bucket to store the .lst file which are train_lst folder &amp; validation_lst folder. These two folders also should contains 400 images?  <\/p>\n\n<p>Resource configuration:<br>\nInstance type: ml.p2.xlarge<br>\nInstance count: 1<br>\nAdditional storage: 5GB  <\/p>\n\n<p>Hyperparameters:<br>\nnum_classes:2<br>\nnum_training_samples:400<br>\nothers parameters used default value by system.  <\/p>\n\n<p>Input data configuration:<br>\nI set 4 channels which are:  <\/p>\n\n<p>1) train\nS3 location: s3:\/\/datasets-for-testing\/train<\/p>\n\n<p>2) validation\nS3 location: s3:\/\/datasets-for-testing\/validation<\/p>\n\n<p>3) train_lst\nS3 location: s3:\/\/datasets-for-testing\/train<\/p>\n\n<p>4) validation_lst\nS3 location: s3:\/\/datasets-for-testing\/validation<\/p>\n\n<p>Input mode:file\nContent type: application\/jpeg or use application\/x-image will be better<\/p>\n\n<p>S3 output path\ns3:\/\/datasets-for-testing\/output<\/p>\n\n<p>These are all the configurations I choose before click on the 'Create training job'.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-07-26 04:39:26.253 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-07-26 07:30:09.647 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|image-processing|amazon-sagemaker",
        "Question_view_count":239,
        "Owner_creation_date":"2019-07-26 04:23:33.777 UTC",
        "Owner_last_access_date":"2019-08-22 08:53:13.567 UTC",
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>I create a training job that you have specified and had got the same error. To resolve the error <strong>ClientError: lst should at least has three parts, but only has 1 parts for<\/strong>, make sure that the file <strong>.lst<\/strong> is well-formatted with tab-separated like this:<\/p>\n\n<pre><code>5      1   iphone\/iphone7_1.jpg\n1000   0   iphone\/iphone6_1.jpg\n22     1   iphone\/iphone7_2.jpg\n<\/code><\/pre>\n\n<p>I used  <code>nano<\/code> on <strong>MAC OS X<\/strong> to validate the tab-separated format.<\/p>",
        "Answer_comment_count":9.0,
        "Answer_creation_date":"2019-07-26 18:27:55.663 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57213293",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51365850,
        "Question_title":"how to deploy a xgboost model on amazon sagemaker?",
        "Question_body":"<p>Is there a way to deploy a xgboost model trained locally using amazon sagemaker? I only saw tutorial talking about both training and deploying model with amazon sagemaker.\nThanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-07-16 15:58:06.157 UTC",
        "Question_favorite_count":0.0,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"python|amazon-web-services|xgboost|amazon-sagemaker",
        "Question_view_count":1641,
        "Owner_creation_date":"2018-06-28 13:47:43.837 UTC",
        "Owner_last_access_date":"2018-07-31 15:13:18.053 UTC",
        "Owner_reputation":53,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":"<p>This <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/d5681a07611ae29567355b60b2f22500b561218b\/advanced_functionality\/xgboost_bring_your_own_model\/xgboost_bring_your_own_model.ipynb\" rel=\"nofollow noreferrer\">example notebook<\/a> is good starting point showing how to use a pre-existing scikit-learn xgboost model with the Amazon SageMaker to create a hosted endpoint for that model.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-07-16 22:05:53.543 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51365850",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53717800,
        "Question_title":"Configuring GPU in aws sagemaker with keras and tensorflow as backend",
        "Question_body":"<p>I am a newbie to aws sagemaker.\nI am trying to setup a model in aws sagemaker using keras with GPU support.\nThe docker base image used to infer the model is given below<\/p>\n\n<pre><code>FROM tensorflow\/tensorflow:1.10.0-gpu-py3\n\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends nginx curl\n...\n<\/code><\/pre>\n\n<p>This is the keras code I'm using to check if a GPU is identified by keras in flask.<\/p>\n\n<pre><code>import keras\n@app.route('\/ping', methods=['GET'])\ndef ping():\n\n    keras.backend.tensorflow_backend._get_available_gpus()\n\n    return flask.Response(response='\\n', status=200,mimetype='application\/json')\n<\/code><\/pre>\n\n<p>When I spin up a notebook instance in a sagemaker using the GPU the keras code shows available GPUs.\nSo, in order to access GPU in the inference phase(model) do I need to install any additional libraries in the docker file apart from the tensorflow GPU base image?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-12-11 05:18:03.17 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"tensorflow|keras|amazon-sagemaker",
        "Question_view_count":1958,
        "Owner_creation_date":"2018-12-11 04:49:59.113 UTC",
        "Owner_last_access_date":"2022-09-25 04:40:42.797 UTC",
        "Owner_reputation":57,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<p>You shouldn't need to install anything else. Keras relies on TensorFlow for GPU detection and configuration.<\/p>\n\n<p>The only thing worth noting is how to use multiple GPUs during training. I'd recommend passing 'gpu_count' as an hyper parameter, and setting things up like so:<\/p>\n\n<pre><code>from keras.utils import multi_gpu_model\nmodel = Sequential()\nmodel.add(...)\n...\nif gpu_count &gt; 1:\n    model = multi_gpu_model(model, gpus=gpu_count)\nmodel.compile(...)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-12-19 09:02:47.393 UTC",
        "Answer_score":5.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53717800",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69099627,
        "Question_title":"what causes an unpickling stack underflow when trying to serialize a succesfully generated SageMaker model",
        "Question_body":"<p>I am currently working on setting up a pipeline in Amazon Sagemaker. For that I set up an xgboost-estimator and trained it on my dataset. The training job runs as expected and the freshly trained model is saved to the specified output bucket. Later I want to reimport the model, which is done by getting the mode.tar.gz from the output bucket, extracting the model and serializing the binary via pickle.<\/p>\n<pre><code># download the model artifact from AWS S3\n!aws s3 cp s3:\/\/my-bucket\/output\/sagemaker-xgboost-2021-09-06-12-19-41-306\/output\/model.tar.gz .\n\n# opens the downloaded model artifcat and loads it as 'model' variable\nmodel_path = &quot;model.tar.gz&quot;\nwith tarfile.open(model_path) as tar:\n    tar.extractall(path=&quot;.&quot;)\n\nmodel = pkl.load(open(&quot;xgboost-model&quot;, &quot;rb&quot;))\n<\/code><\/pre>\n<p>Whenever I try to tun this I receive an unpickling stack underflow:<\/p>\n<pre><code>---------------------------------------------------------------------------\nUnpicklingError                           Traceback (most recent call last)\n&lt;ipython-input-9-b88a7424f790&gt; in &lt;module&gt;\n     10     tar.extractall(path=&quot;.&quot;)\n     11 \n---&gt; 12 model = pkl.load(open(&quot;xgboost-model&quot;, &quot;rb&quot;))\n     13 \n\nUnpicklingError: unpickling stack underflow\n<\/code><\/pre>\n<p>So far I retrained the model to see, if the error occurs with a different model file and it does. I also downloaded the model.tar.gz and validated it via gunzip. When extracting the binary file xgboost-model is extracted correctly, I just can't pickle it. Every occurence of the error I found on stackoverflow points at a damaged file, but this one is generated directly by SageMaker and I do note perform any transformation on it, but extracting it from the model.tar.gz. Reloading a model like this seems to be quite a common use case, referring to the documentation and different tutorials.\nLocally I receive the same error with the downloaded file. I tried to step directly into pickle for debugging it but couldn't make much sense of it. The complete error stack looks like this:<\/p>\n<pre><code>Exception has occurred: UnpicklingError       (note: full exception trace is shown but execution is paused at: _run_module_as_main)\nunpickling stack underflow\n  File &quot;\/sagemaker_model.py&quot;, line 10, in &lt;module&gt;\n    model = pkl.load(open('xgboost-model', 'rb'))\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 87, in _run_code\n    exec(code, run_globals)\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 87, in _run_code\n    exec(code, run_globals)\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 197, in _run_module_as_main (Current frame)\n    return _run_code(code, main_globals, None,\n<\/code><\/pre>\n<p>What could cause this issue and at which step during the process could I apply changes to fix or workaround the problem.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":4,
        "Question_creation_date":"2021-09-08 08:32:46.887 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":null,
        "Question_score":5,
        "Question_tags":"python-3.x|pickle|amazon-sagemaker",
        "Question_view_count":1747,
        "Owner_creation_date":"2019-02-17 11:00:47.147 UTC",
        "Owner_last_access_date":"2022-09-23 14:32:52.187 UTC",
        "Owner_reputation":71,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>The issue rooted in the model version used for the xgboost framework. from 1.3.0 on the default output changed from pickle to json and the sagemaker documentation does not seem to have been updated accordingly. So if you want to read the model via<\/p>\n<pre><code>    tar.extractall(path=&quot;.&quot;)\n\nmodel = pkl.load(open(&quot;xgboost-model&quot;, &quot;rb&quot;))\n<\/code><\/pre>\n<p>as described in the sagemaker docs, you need to import the XGBOOST framework with with a former version, e.g. 1.2.1.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-09-24 10:32:40.17 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69099627",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54314876,
        "Question_title":"AWS Sagemaker SKlearn entry point allow multiple script",
        "Question_body":"<p>I am trying to follow the tutorial <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb\" rel=\"noreferrer\">here<\/a> to implement a custom inference pipeline for feature preprocessing. It uses the python sklearn sdk to bring in custom preprocessing pipeline from a script. For example:<\/p>\n\n<pre><code>from sagemaker.sklearn.estimator import SKLearn\n\nscript_path = 'preprocessing.py'\n\nsklearn_preprocessor = SKLearn(\n    entry_point=script_path,\n    role=role,\n    train_instance_type=\"ml.c4.xlarge\",\n    sagemaker_session=sagemaker_session)\n<\/code><\/pre>\n\n<p>However I can't find a way to send multiple files. The reason I need multiple files is because I have a custom class used in the sklearn pipeline needs to be imported from a custom module. Without importing,  it raises error <code>AttributeError: module '__main__' has no attribute 'CustomClassName'<\/code> when having the custom class in the same preprocessing.py file due to the way pickle works (at least I think it's related to pickle). <\/p>\n\n<p>Anyone know if sending multiple files is even possible?<\/p>\n\n<p>Newbie to Sagemaker, thanks!!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-22 19:06:18.373 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":8,
        "Question_tags":"python|machine-learning|amazon-sagemaker",
        "Question_view_count":2019,
        "Owner_creation_date":"2016-02-09 19:59:23.123 UTC",
        "Owner_last_access_date":"2022-09-19 20:09:05.473 UTC",
        "Owner_reputation":85,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>There's a source_dir parameter which will \"lift\" a directory of files to the container and put it on your import path.<\/p>\n\n<p>You're entrypoint script should be put there to and referenced from that location.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-01-23 13:43:41.873 UTC",
        "Answer_score":4.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54314876",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66911321,
        "Question_title":"Upgrading Python version for running and creating custom container for Sagemaker Endpoint",
        "Question_body":"<p>[UPDATED] We are currently working on creating a Multi-Arm Bandit model for sign up optimization using the Build Your Own workflow that can be found here (basically substituting the model for our own):<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own<\/a><\/p>\n<p>Our project directory is set up as:\n<a href=\"https:\/\/i.stack.imgur.com\/QwaIQ.png\" rel=\"nofollow noreferrer\">Project Directory<\/a><\/p>\n<p>The issue is that I added some code including the dataclasses library that is only available since Python 3.7, and our project seems to keep using 3.6, causing a failure when running the Cloud Formation set up. The error in our Cloudwatch Logs is:<\/p>\n<pre><code>2021-03-31T11:04:11.077-05:00 Copy\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker\n    worker.init_process()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process\n    self.load_wsgi()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load\n    return self.load_wsgiapp()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/util.py&quot;, line 359, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/usr\/lib\/python3.6\/importlib\/__init__.py&quot;, line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 678, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/opt\/program\/wsgi.py&quot;, line 1, in &lt;module&gt;\n    import predictor as myapp\n  File &quot;\/opt\/program\/predictor.py&quot;, line 9, in &lt;module&gt;\n    from model_contents.model import MultiArmBandit, BanditParameters\n  File &quot;\/opt\/program\/model_contents\/model.py&quot;, line 7, in &lt;module&gt;\n    from dataclasses import dataclass, field, asdict\nTraceback (most recent call last): File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker worker.init_process() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process self.load_wsgi() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi self.wsgi = self.app.wsgi() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi self.callable = self.load() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load return self.load_wsgiapp() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp return util.import_app(self.app_uri) File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/util.py&quot;, line 359, in import_app mod = importlib.import_module(module) File &quot;\/usr\/lib\/python3.6\/importlib\/__init__.py&quot;, line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 678, in exec_module File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed File &quot;\/opt\/program\/wsgi.py&quot;, line 1, in &lt;module&gt; import predictor as myapp File &quot;\/opt\/program\/predictor.py&quot;, line 9, in &lt;module&gt; from model_contents.model import MultiArmBandit, BanditParameters File &quot;\/opt\/program\/model_contents\/model.py&quot;, line 7, in &lt;module&gt; from dataclasses import dataclass, field, asdict\n\n    2021-03-31T11:04:11.077-05:00\n\nCopy\nModuleNotFoundError: No module named 'dataclasses'\nModuleNotFoundError: No module named 'dataclasses'\n<\/code><\/pre>\n<p>Our updated Dockerfile is:<\/p>\n<pre><code># This is a Python 3 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:18.04\n\n# Retrieves information about what packages can be installed\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python3-pip \\\n         python3.8 \\\n         python3-setuptools \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Set python 3.8 as default\nRUN update-alternatives --install \/usr\/bin\/python python \/usr\/bin\/python3.8 1\nRUN update-alternatives --install \/usr\/bin\/python3 python3 \/usr\/bin\/python3.8 1\n\n# Here we get all python packages.\nRUN pip --no-cache-dir install numpy boto3 flask gunicorn\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# model_output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=&quot;\/opt\/program:${PATH}&quot;\nENV PYTHONPATH \/model_contents\n\n# Set up the program in the image\nCOPY bandit\/ \/opt\/program\/\nWORKDIR \/opt\/program\/\n\nRUN chmod +x \/opt\/program\/serve &amp;&amp; chmod +x \/opt\/program\/train\nLABEL git_tag=$GIT_TAG\n<\/code><\/pre>\n<p>I'm not sure if the nginx.conf file defaults to Py 3.6 so I want to make sure that it's not a big hassle to upgrade to Py 3.7 or 3.8 without many changes.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-04-01 20:44:18.56 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-04-02 17:15:27.407 UTC",
        "Question_score":1,
        "Question_tags":"python-3.x|docker|ubuntu|amazon-cloudformation|amazon-sagemaker",
        "Question_view_count":1398,
        "Owner_creation_date":"2018-05-14 09:05:19.837 UTC",
        "Owner_last_access_date":"2022-07-01 15:18:50.603 UTC",
        "Owner_reputation":33,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Answer_body":"<p>You can update the Dockerfile after it install Python3.8 using <code>apt-get<\/code> with the following <code>RUN<\/code> commands<\/p>\n<pre><code>RUN update-alternatives --install \/usr\/bin\/python python \/usr\/bin\/python3.8 1\nRUN update-alternatives --install \/usr\/bin\/python3 python3 \/usr\/bin\/python3.8 1\n<\/code><\/pre>\n<p>The first <code>RUN<\/code> command will link <code>\/usr\/bin\/python<\/code> to <code>\/usr\/bin\/python3.8<\/code> and the second one will link <code>\/usr\/bin\/python3<\/code> to <code>\/usr\/bin\/python3.8<\/code><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-04-01 21:13:30.733 UTC",
        "Answer_score":1.0,
        "Owner_location":"Madrid, Spain",
        "Answer_last_edit_date":"2021-04-01 21:20:29.39 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66911321",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62002183,
        "Question_title":"Use tensorboard with object detection API in sagemaker",
        "Question_body":"<p>with <a href=\"https:\/\/github.com\/svpino\/tensorflow-object-detection-sagemaker\" rel=\"nofollow noreferrer\">this<\/a> I successfully created a training job on sagemaker using the Tensorflow Object Detection API in a docker container. Now I'd like to monitor the training job using sagemaker, but cannot find anything explaining how to do it. I don't use a sagemaker notebook.\nI think I can do it by saving the logs into a S3 bucket and point there a local tensorboard instance .. but don't know how to tell the tensorflow object detection API where to save the logs (is there any command line argument for this ?).\nSomething like <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/keras_script_mode_pipe_mode_horovod\/tensorflow_keras_CIFAR10.ipynb\" rel=\"nofollow noreferrer\">this<\/a>, but the script <code>generate_tensorboard_command.py<\/code> fails because my training job don't have the <code>sagemaker_submit_directory<\/code> parameter..<\/p>\n<p>The fact is when I start the training job nothing is created on my s3 until the job finish and upload everything. There should be a way tell tensorflow where to save the logs (s3) during the training, hopefully without modifying the API source code..<\/p>\n<p><strong>Edit<\/strong><\/p>\n<p>I can finally make it works with the accepted solution (tensorflow natively supports read\/write to s3), there are however additional steps to do:<\/p>\n<ol>\n<li>Disable network isolation in the training job configuration<\/li>\n<li>Provide credentials to the docker image to write to S3 bucket<\/li>\n<\/ol>\n<p>The only thing is that Tensorflow continuously polls filesystem (i.e. looking for an updated model in serving mode) and this cause useless requests to S3, that you will have to pay (together with a buch of errors in the console). I opened a new question <a href=\"https:\/\/stackoverflow.com\/q\/64969198\/4267439\">here<\/a> for this. At least it works.<\/p>\n<p><strong>Edit 2<\/strong><\/p>\n<p>I was wrong, TF just write logs, is not polling so it's an expected behavior and the extra costs are minimal.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-05-25 12:12:39.343 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-11-25 16:53:18.013 UTC",
        "Question_score":2,
        "Question_tags":"object-detection|tensorboard|amazon-sagemaker|object-detection-api|tensorflow-model-garden",
        "Question_view_count":311,
        "Owner_creation_date":"2014-11-18 21:32:30.293 UTC",
        "Owner_last_access_date":"2022-09-24 17:06:59.437 UTC",
        "Owner_reputation":2302,
        "Owner_up_votes":51,
        "Owner_down_votes":4,
        "Owner_views":227,
        "Answer_body":"<p>Looking through the example you posted, it looks as though the <code>model_dir<\/code> passed to the TensorFlow Object Detection package is configured to <code>\/opt\/ml\/model<\/code>:<\/p>\n<pre><code># These are the paths to where SageMaker mounts interesting things in your container.\nprefix = '\/opt\/ml\/'\ninput_path = os.path.join(prefix, 'input\/data')\noutput_path = os.path.join(prefix, 'output')\nmodel_path = os.path.join(prefix, 'model')\nparam_path = os.path.join(prefix, 'input\/config\/hyperparameters.json')\n<\/code><\/pre>\n<p>During the training process, tensorboard logs will be written to <code>\/opt\/ml\/model<\/code>, and then uploaded to s3 as a final model artifact AFTER training: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html<\/a>.<\/p>\n<p>You <em>might<\/em> be able to side-step the SageMaker artifact upload step and point the <code>model_dir<\/code> of TensorFlow Object Detection API directly at an s3 location during training:<\/p>\n<pre><code>model_path = &quot;s3:\/\/your-bucket\/path\/here\n<\/code><\/pre>\n<p>This means that the TensorFlow library within the SageMaker job is directly writing to S3 instead of the filesystem inside of it's container. Assuming the underlying TensorFlow Object Detection code can write directly to S3 (something you'll have to verify), you should be able to see the tensorboard logs and checkpoints there in realtime.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-07 16:06:22.057 UTC",
        "Answer_score":1.0,
        "Owner_location":"Jesi, Italy",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62002183",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51858379,
        "Question_title":"how to link s3 bucket to sagemaker notebook",
        "Question_body":"<p>I am trying to link my s3 bucket to a notebook instance, however i am not able to:<\/p>\n\n<p>Here is how much I know:<\/p>\n\n<pre><code>from sagemaker import get_execution_role\n\nrole = get_execution_role\nbucket = 'atwinebankloadrisk'\ndatalocation = 'atwinebankloadrisk'\n\ndata_location = 's3:\/\/{}\/'.format(bucket)\noutput_location = 's3:\/\/{}\/'.format(bucket)\n<\/code><\/pre>\n\n<p>to call the data from the bucket:<\/p>\n\n<pre><code>df_test = pd.read_csv(data_location\/'application_test.csv')\ndf_train = pd.read_csv('.\/application_train.csv')\ndf_bureau = pd.read_csv('.\/bureau_balance.csv')\n<\/code><\/pre>\n\n<p>However I keep getting errors and unable to proceed.\nI haven't found answers that can assist much.<\/p>\n\n<p>PS: I am new to this AWS<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":1,
        "Question_creation_date":"2018-08-15 12:05:35.12 UTC",
        "Question_favorite_count":3.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-s3|amazon-sagemaker",
        "Question_view_count":13863,
        "Owner_creation_date":"2017-09-14 09:04:01.387 UTC",
        "Owner_last_access_date":"2022-09-21 09:28:36.2 UTC",
        "Owner_reputation":45,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Answer_body":"<p>You can load S3 Data into AWS SageMaker Notebook by using the sample code below. Do make sure the Amazon SageMaker role has policy attached to it to have access to S3. <\/p>\n\n<p>[1] <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html<\/a> <\/p>\n\n<pre><code>import boto3 \nimport botocore \nimport pandas as pd \nfrom sagemaker import get_execution_role \n\nrole = get_execution_role() \n\nbucket = 'Your_bucket_name' \ndata_key = your_data_file.csv' \ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key) \n\npd.read_csv(data_location) \n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-08-21 17:43:00.927 UTC",
        "Answer_score":5.0,
        "Owner_location":"Kampala, Uganda",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51858379",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60037376,
        "Question_title":"Can't use Keras CSVLogger callbacks in Sagemaker script mode. It fails to write the log file on S3 ( error - No such file or directory )",
        "Question_body":"<p>I have this script where I want to get the callbacks to a separate CSV file in sagemaker custom script docker container. But when I try to run in local mode, it fails giving the following error. I have a hyper-parameter tuning job(HPO) to run and this keeps giving me errors. I need to get this local mode run correctly before doing the HPO. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/de522.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/de522.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>In the notebook I use the following code.<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='lstm_model.py', \n                          role=role,\n                          code_location=custom_code_upload_location,\n                          output_path=model_artifact_location+'\/',\n                          train_instance_count=1, \n                          train_instance_type='local',\n                          framework_version='1.12', \n                          py_version='py3',\n                          script_mode=True,\n                          hyperparameters={'epochs': 1},\n                          base_job_name='hpo-lstm-local-test'\n                         )\n\ntf_estimator.fit({'training': training_input_path, 'validation': validation_input_path})\n<\/code><\/pre>\n\n<p>In my <strong>lstm_model.py<\/strong> script the following code is used.<\/p>\n\n<pre><code>lgdir = os.path.join(model_dir, 'callbacks_log.csv')\ncsv_logger = CSVLogger(lgdir, append=True)\n\nregressor.fit(x_train, y_train, batch_size=batch_size,\n              validation_data=(x_val, y_val), \n              epochs=epochs,\n              verbose=2,\n              callbacks=[csv_logger]\n              )\n<\/code><\/pre>\n\n<p>I tried creating a file before hand like shown below using tensorflow backend. But it doesn't create a file. ( K : tensorflow Backend, tf: tensorflow )<\/p>\n\n<pre><code>filename = tf.Variable(lgdir , tf.string)\ncontent = tf.Variable(\"\", tf.string)\nsess = K.get_session()\ntf.io.write_file(filename, content)\n<\/code><\/pre>\n\n<p>I can't use any other packages like pandas to create the file as the TensorFlow docker container in SageMaker for custom scripts doesn't provide them. They give only a limited amount of packages. <\/p>\n\n<p>Is there a way I can write the csv file to the S3 bucket location, before the fit method try to write the callback. Or is that the solution to the problem? I am not sure. <\/p>\n\n<p>If you can even suggest other suggestions to get callbacks, I would even accept that answer. But it should be worth the effort. <\/p>\n\n<p>This docker image is really narrowing the scope. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-03 10:31:01.867 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow|keras|amazon-sagemaker",
        "Question_view_count":412,
        "Owner_creation_date":"2018-01-28 13:47:46.417 UTC",
        "Owner_last_access_date":"2022-09-23 19:02:42.52 UTC",
        "Owner_reputation":65,
        "Owner_up_votes":89,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":"<p>Well for starters, you can always make your own docker image using the Tensorflow image as a base. I work in Tensorflow 2.0 so this will be slightly different for you but here is an example of my image pattern:<\/p>\n\n<pre><code># Downloads the TensorFlow library used to run the Python script\nFROM tensorflow\/tensorflow:2.0.0a0 # you would use the equivalent for your TF version\n\n# Contains the common functionality necessary to create a container compatible with Amazon SageMaker\nRUN pip install sagemaker-containers -q \n\n# Wandb allows us to customize and centralize logging while maintaining open-source agility\nRUN pip install wandb -q # here you would install pandas\n\n# Copies the training code inside the container to the design pattern created by the Tensorflow estimator\n# here you could copy over a callbacks csv\nCOPY mnist-2.py \/opt\/ml\/code\/mnist-2.py \nCOPY callbacks.py \/opt\/ml\/code\/callbacks.py \nCOPY wandb_setup.sh \/opt\/ml\/code\/wandb_setup.sh\n\n# Set the login script as the entry point\nENV SAGEMAKER_PROGRAM wandb_setup.sh # here you would instead launch lstm_model.py\n<\/code><\/pre>\n\n<p>I believe you are looking for a pattern similar to this, but I prefer to log all of my model data using <a href=\"https:\/\/www.wandb.com\/\" rel=\"nofollow noreferrer\">Weights and Biases<\/a>. They're a little out of data on their SageMaker integration but I'm actually in the midst of writing an updated tutorial for them. It should certainly be finished this month and include logging and comparing runs from hyperparameter tuning jobs<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-03-10 17:15:47.413 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60037376",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57074382,
        "Question_title":"How to get the public ip of amazon sagemaker's notebook instance? Is it possible?",
        "Question_body":"<p>Is it possible to get the public-ip of an amazon <code>sagemaker<\/code> notebook instance?<\/p>\n\n<p>I was wondering if I can ssh into it using the public ip for remote debugging purposes.<\/p>\n\n<p>I tried getting the public ip using the below curl command<\/p>\n\n<pre><code>$curl http:\/\/169.254.169.254\/latest\/meta-data\n<\/code><\/pre>\n\n<p>This just lists the local ip and not the public ip.<\/p>\n\n<p>I also tried the below command.<\/p>\n\n<pre><code>$curl ifconfig.me\n<\/code><\/pre>\n\n<p>This returns an ip address like <code>13.232.96.15<\/code>. If I try ssh into this it doesnt work.<\/p>\n\n<p>Is there any other way we can do this?<\/p>\n\n<p>Note : The ssh port 22 is open already in the security group<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-17 11:03:06.39 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":6,
        "Question_tags":"python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":4461,
        "Owner_creation_date":"2016-02-10 09:50:46.15 UTC",
        "Owner_last_access_date":"2022-09-23 10:06:18.677 UTC",
        "Owner_reputation":1347,
        "Owner_up_votes":1531,
        "Owner_down_votes":8,
        "Owner_views":217,
        "Answer_body":"<p>I don't think you can ssh to notebook instances. You can either use open them from the console, or grab the url with an API, re: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/howitworks-access-ws.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/howitworks-access-ws.html<\/a><\/p>\n\n<p>If you need a terminal, then you can open one from Jupyter.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-07-17 23:22:54.44 UTC",
        "Answer_score":5.0,
        "Owner_location":"Bangalore, Karnataka, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57074382",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55132599,
        "Question_title":"Difference in usecases for AWS Sagemaker vs Databricks?",
        "Question_body":"<p>I was looking at Databricks because it integrates with AWS services like Kinesis, but it looks to me like SageMaker is a direct competitor to Databricks? We are heavily using AWS, is there any reason to add DataBricks into the stack or odes SageMaker fill the same role?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-03-13 00:23:26.6 UTC",
        "Question_favorite_count":3.0,
        "Question_last_edit_date":null,
        "Question_score":9,
        "Question_tags":"apache-spark|pyspark|databricks|amazon-sagemaker",
        "Question_view_count":11894,
        "Owner_creation_date":"2015-01-15 17:43:03.7 UTC",
        "Owner_last_access_date":"2022-08-23 22:54:25.603 UTC",
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Answer_body":"<p>SageMaker is a great tool for deployment, it simplifies a lot of processes configuring containers, you only need to write 2-3 lines to deploy the model as an endpoint and use it.  SageMaker also provides the dev platform (Jupyter Notebook) which supports Python and Scala (sparkmagic kernal) developing, and i managed installing external scala kernel in jupyter notebook. Overall, SageMaker provides end-to-end ML services. Databricks has unbeatable Notebook environment for Spark development. <\/p>\n\n<p>Conclusion<\/p>\n\n<ol>\n<li><p>Databricks is a better platform for Big data(scala, pyspark) Developing.(unbeatable notebook environment)<\/p><\/li>\n<li><p>SageMaker is better for Deployment. and if you are not working on big data, SageMaker is a perfect choice working with (Jupyter notebook + Sklearn + Mature containers + Super easy deployment). <\/p><\/li>\n<li><p>SageMaker provides \"real time inference\", very easy to build and deploy, very impressive. you can check the official SageMaker Github.\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline<\/a><\/p><\/li>\n<\/ol>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2019-03-20 21:40:34.27 UTC",
        "Answer_score":14.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55132599",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54268970,
        "Question_title":"Wrong dense layer output shape after moving from TF 1.12 to 1.10",
        "Question_body":"<p>I'm migrating from Tensorflow 1.12 to Tensorflow 1.10 (Collaboratory -> AWS sagemaker), the code seems to be working fine in Tensorflow 1.12 but in 1.10 i get an error <code>ValueError: Error when checking target: expected dense to have 2 dimensions, but got array with shape (52692,)<\/code><\/p>\n\n<p>Input example - strings with no whitespaces: <\/p>\n\n<pre><code>[\"testAbc\", \"aaDD\", \"roam\"]\n<\/code><\/pre>\n\n<p>which I preprocess by changing small letters into 1, capital letters 2, digits - 3, '-' - 4, '_' - 5 and padding so they are equal length with 0s<\/p>\n\n<p>and 4 labels a - 0, b - 1, c - 2, d - 3<\/p>\n\n<p>Assuming max length for each word is 10 (in my code it's 20):<\/p>\n\n<p>features - [[1 1 1 1 2 1 1 0 0 0][1 1 2 2 0 0 0 0 0 0][1 1 1 1 0 0 0 0 0 0]]<\/p>\n\n<p>labels - [1, 1, 2, 3]<\/p>\n\n<p>expected output: [a: 0%, b: 0%, c: 1%, d: 99%] (example)<\/p>\n\n<pre><code>model = keras.Sequential()\nmodel.add(\n    keras.layers.Embedding(6, 8, input_length=maxFeatureLen))\nmodel.add(keras.layers.LSTM(12))\nmodel.add(keras.layers.Dense(4, activation=tf.nn.softmax))\nmodel.compile(tf.train.AdamOptimizer(0.001), loss=\"sparse_categorical_crossentropy\")\nmodel.fit(train[\"featuresVec\"],\n            train[\"labelsVec\"],\n            epochs=1,\n            verbose=1,\n            callbacks=[],\n            validation_data=(evale[\"featuresVec\"], evale[\"labelsVec\"],),\n            validation_steps=evale[\"count\"],\n            steps_per_epoch=train[\"count\"])\n<\/code><\/pre>\n\n<p>Shapes of train and evale - 2D arrays<\/p>\n\n<pre><code>train[\"featuresVec\"]=\n[[1 2 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n [1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [2 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n\nevale[\"featuresVec\"]=\n[[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n [1 1 1 1 1 1 2 1 1 1 1 1 0 0 0 0 0 0 0 0]\n [1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 0]\n [1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 0 0]\n [1 1 1 1 1 2 1 1 1 1 1 1 0 0 0 0 0 0 0 0]]\n\ntrain[\"labelsVec\"] = [1 0 0 0 2]\nevale[\"labelsVec\"] = [0 1 1 1 1]\n<\/code><\/pre>\n\n<p>Shapes:<\/p>\n\n<pre><code>train[\"featuresVec\"] = [52692, 20]\nevale[\"featuresVec\"] = [28916, 20]\ntrain[\"labelsVec\"] = [52692]\nevale[\"labelsVec\"] = [28916]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_date":"2019-01-19 16:12:07.393 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2019-01-19 16:38:55.247 UTC",
        "Question_score":0,
        "Question_tags":"python|tensorflow|keras|amazon-sagemaker",
        "Question_view_count":120,
        "Owner_creation_date":"2018-11-13 21:54:35.153 UTC",
        "Owner_last_access_date":"2019-04-13 20:25:53.21 UTC",
        "Owner_reputation":91,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":27,
        "Answer_body":"<p>Probably your labels vector needs to be of shape <code>(batch_size, 1)<\/code> instead of just <code>(batch_size,)<\/code>. <\/p>\n\n<p><strong>Note:<\/strong> Since you are using <code>sparse_categorical_crossentropy<\/code> as loss function instead of <code>categorical_crossentropy<\/code>, it is correct to not one-hot encode the labels. <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-01-19 17:02:45.403 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54268970",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68251533,
        "Question_title":"Why is AWS Sagemaker notebook instance designed to only persist data under ~\/Sagemaker?",
        "Question_body":"<p>In my current job we use AWS managed notebooks on Sagemaker EC2. I am largely okay with the user experience but the lack of data persistency outside <code>~\/Sagemaker<\/code> has been quite inconvenient. Every time should the instance need restarting, I'd lose all the settings and python packages. Wonder why AWS would make this particular decision for Sagemaker. Have used Google Cloud's AI platform before and it does not have such settings and my configurations would always persist.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-05 06:23:56.78 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":266,
        "Owner_creation_date":"2014-08-22 21:51:20.997 UTC",
        "Owner_last_access_date":"2022-09-25 01:52:01.297 UTC",
        "Owner_reputation":2758,
        "Owner_up_votes":603,
        "Owner_down_votes":2,
        "Owner_views":122,
        "Answer_body":"<p>I faced a similar issue on other AWS services. Usually for managed services AWS uses read-only containers approach and leave just one folder of the filesystem for read\/write that persist across the stop\/restart cycle.\nReguarding the packages installation the seems to be to install your custom environment on the notebook instance's Amazon EBS volume, as described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-07-05 12:24:52.607 UTC",
        "Answer_score":1.0,
        "Owner_location":"Vancouver, BC, Canada",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68251533",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51933366,
        "Question_title":"Version management in SageMaker",
        "Question_body":"<p>We're currently working with 3 employees in the same notebook-instance, however, since this is a shared workspace this makes version management extra difficult. Is it possible to link aws credentials to your git account from within SageMaker? Or are there any other ways recommended for version management? <\/p>\n\n<p>Right now we're using a single git account for committing the code from within jupyter terminal. <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-08-20 14:51:45.277 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":836,
        "Owner_creation_date":"2018-08-20 14:37:26.323 UTC",
        "Owner_last_access_date":"2020-09-25 18:34:56.833 UTC",
        "Owner_reputation":33,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>The situation has changed : Git is now <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-notebooks-now-support-git-integration-for-increased-persistence-collaboration-and-reproducibility\/\" rel=\"nofollow noreferrer\">available<\/a> in SageMaker<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-11-30 10:32:45.693 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51933366",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57273357,
        "Question_title":"Empty dictionary on AnnotationConsolidation lambda event for aws Sagemaker",
        "Question_body":"<p>I am starting to use aws sagemaker on the development of my machine learning model and I'm trying to build a lambda function to process the responses of a sagemaker labeling job. I already created my own lambda function but when I try to read the event contents I can see that the event dict is completely empty, so I'm not getting any data to read.<\/p>\n\n<p>I have already given enough permissions to the role of the lambda function. Including:\n- AmazonS3FullAccess.\n- AmazonSagemakerFullAccess.\n- AWSLambdaBasicExecutionRole<\/p>\n\n<p>I've tried using this code for the Post-annotation Lambda (adapted for python 3.6):<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step2-demo1.html#sms-custom-templates-step2-demo1-post-annotation\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step2-demo1.html#sms-custom-templates-step2-demo1-post-annotation<\/a><\/p>\n\n<p>As well as this one in this git repository:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py<\/a><\/p>\n\n<p>But none of them seemed to work.<\/p>\n\n<p>For creating the labeling job I'm using boto3's functions for sagemaker:\n<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_labeling_job\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_labeling_job<\/a><\/p>\n\n<p>This is the code i have for creating the labeling job:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def create_labeling_job(client,bucket_name ,labeling_job_name, manifest_uri, output_path):\n\n    print(\"Creating labeling job with name: %s\"%(labeling_job_name))\n\n    response = client.create_labeling_job(\n        LabelingJobName=labeling_job_name,\n        LabelAttributeName='annotations',\n        InputConfig={\n            'DataSource': {\n                'S3DataSource': {\n                    'ManifestS3Uri': manifest_uri\n                }\n            },\n            'DataAttributes': {\n                'ContentClassifiers': [\n                    'FreeOfAdultContent',\n                ]\n            }\n        },\n        OutputConfig={\n            'S3OutputPath': output_path\n        },\n        RoleArn='arn:aws:myrolearn',\n        LabelCategoryConfigS3Uri='s3:\/\/'+bucket_name+'\/config.json',\n        StoppingConditions={\n            'MaxPercentageOfInputDatasetLabeled': 100,\n        },\n        LabelingJobAlgorithmsConfig={\n            'LabelingJobAlgorithmSpecificationArn': 'arn:image-classification'\n        },\n        HumanTaskConfig={\n            'WorkteamArn': 'arn:my-private-workforce-arn',\n            'UiConfig': {\n                'UiTemplateS3Uri':'s3:\/\/'+bucket_name+'\/templatefile'\n            },\n            'PreHumanTaskLambdaArn': 'arn:aws:lambda:us-east-1:432418664414:function:PRE-BoundingBox',\n            'TaskTitle': 'Title',\n            'TaskDescription': 'Description',\n            'NumberOfHumanWorkersPerDataObject': 1,\n            'TaskTimeLimitInSeconds': 600,\n            'AnnotationConsolidationConfig': {\n                'AnnotationConsolidationLambdaArn': 'arn:aws:my-custom-post-annotation-lambda'\n            }\n        }\n    )\n\n    return response\n<\/code><\/pre>\n\n<p>And this is the one i have for the lambda function:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>    print(\"Received event: \" + json.dumps(event, indent=2))\n    print(\"event: %s\"%(event))\n    print(\"context: %s\"%(context))\n    print(\"event headers: %s\"%(event[\"headers\"]))\n\n    parsed_url = urlparse(event['payload']['s3Uri']);\n    print(\"parsed_url: \",parsed_url)\n\n    labeling_job_arn = event[\"labelingJobArn\"]\n    label_attribute_name = event[\"labelAttributeName\"]\n\n    label_categories = None\n    if \"label_categories\" in event:\n        label_categories = event[\"labelCategories\"]\n        print(\" Label Categories are : \" + label_categories)\n\n    payload = event[\"payload\"]\n    role_arn = event[\"roleArn\"]\n\n    output_config = None # Output s3 location. You can choose to write your annotation to this location\n    if \"outputConfig\" in event:\n        output_config = event[\"outputConfig\"]\n\n    # If you specified a KMS key in your labeling job, you can use the key to write\n    # consolidated_output to s3 location specified in outputConfig.\n    kms_key_id = None\n    if \"kmsKeyId\" in event:\n        kms_key_id = event[\"kmsKeyId\"]\n\n    # Create s3 client object\n    s3_client = S3Client(role_arn, kms_key_id)\n\n    # Perform consolidation\n    return do_consolidation(labeling_job_arn, payload, label_attribute_name, s3_client)\n<\/code><\/pre>\n\n<p>I've tried debugging the event object with:<\/p>\n\n<pre><code>    print(\"Received event: \" + json.dumps(event, indent=2))\n<\/code><\/pre>\n\n<p>But it just prints an empty dictionary: <code>Received event: {}<\/code><\/p>\n\n<p>I expect the output to be something like:<\/p>\n\n<pre><code>    #Content of an example event:\n    {\n        \"version\": \"2018-10-16\",\n        \"labelingJobArn\": &lt;labelingJobArn&gt;,\n        \"labelCategories\": [&lt;string&gt;],  # If you created labeling job using aws console, labelCategories will be null\n        \"labelAttributeName\": &lt;string&gt;,\n        \"roleArn\" : \"string\",\n        \"payload\": {\n            \"s3Uri\": &lt;string&gt;\n        }\n        \"outputConfig\":\"s3:\/\/&lt;consolidated_output configured for labeling job&gt;\"\n    }\n<\/code><\/pre>\n\n<p>Lastly, when I try yo get the labeling job ARN with:<\/p>\n\n<pre><code>    labeling_job_arn = event[\"labelingJobArn\"]\n<\/code><\/pre>\n\n<p>I just get a KeyError (which makes sense because the dictionary is empty).<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-30 13:51:41.657 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-12-04 12:23:09.053 UTC",
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|aws-lambda|python-3.6|amazon-sagemaker",
        "Question_view_count":846,
        "Owner_creation_date":"2018-05-04 16:04:40.547 UTC",
        "Owner_last_access_date":"2022-09-05 11:01:25.78 UTC",
        "Owner_reputation":81,
        "Owner_up_votes":54,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":"<p>I found the problem, I needed to add the ARN of the role used by my Lamda function as a Trusted Entity on the Role used for the Sagemaker Labeling Job.<\/p>\n\n<p>I just went to <code>Roles &gt; MySagemakerExecutionRole &gt; Trust Relationships<\/code> and added:<\/p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": [\n          \"arn:aws:iam::xxxxxxxxx:role\/My-Lambda-Role\",\n           ...\n        ],\n        \"Service\": [\n          \"lambda.amazonaws.com\",\n          \"sagemaker.amazonaws.com\",\n           ...\n        ]\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n<\/code><\/pre>\n\n<p>This made it work for me.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-08-02 10:28:09.013 UTC",
        "Answer_score":0.0,
        "Owner_location":"Madrid, Spain",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57273357",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58992447,
        "Question_title":"AWS SageMaker notebook list tables using boto3 and PySpark",
        "Question_body":"<p>Having some difficulty executing the following code in AWS SageMaker. It is supposed to just list all of the tables in DynamoDB.<\/p>\n\n<pre><code>import boto3\nresource = boto3.resource('dynamodb', region_name='xxxx')\nresponse  = resource.tables.all()\nfor r in response:\n    print(r.name)\n<\/code><\/pre>\n\n<p>If the SageMaker notebook kernel is set to \"conda_python3\" the code executes fine and the tables are listed out in the notebook as expected (this happens pretty much instantly).<\/p>\n\n<p>However, if I set the kernel to \"Sparkmagic (PySpark)\" the same code infinitely runs and doesn't output the table list at all.<\/p>\n\n<p>Does anyone know why this would happen for the PySpark kernel but not for the conda3 kernel? Ideally I need to run this code as part of a bigger script that relies on PySpark, so would like to get it working with PySpark.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-11-22 10:38:28.777 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-11-22 10:48:20.06 UTC",
        "Question_score":1,
        "Question_tags":"pyspark|amazon-dynamodb|boto3|amazon-sagemaker",
        "Question_view_count":440,
        "Owner_creation_date":"2013-11-18 17:24:50.587 UTC",
        "Owner_last_access_date":"2022-09-23 11:35:05.217 UTC",
        "Owner_reputation":1012,
        "Owner_up_votes":1106,
        "Owner_down_votes":1,
        "Owner_views":102,
        "Answer_body":"<p>Figured out what the issue was, you need to end an endpoint to tour VPC for DyanmoDB.<\/p>\n\n<p>To do this navigate to:<\/p>\n\n<ol>\n<li>AWS VPC<\/li>\n<li>Endpoints<\/li>\n<li>Create Endpoint<\/li>\n<li>Select the dynamodb service (will be type Gateway)<\/li>\n<li>Select the VPC your Notebook is using<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-11-22 13:47:30.807 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58992447",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54193723,
        "Question_title":"Size of image for prediction with SageMaker object detection?",
        "Question_body":"<p>I'm using the AWS SageMaker \"built in\" object detection algorithm (SSD) and we've trained it on a series of annotated 512x512 images (image_shape=512).  We've deployed an endpoint and when using it for prediction we're getting mixed results.  <\/p>\n\n<p>If the image we use for prediciton is around that 512x512 size we're getting great accuracy and good results.  If the image is significantly larger (e.g. 8000x10000) we get either wildly inaccurate, or no results.  If I manually resize those large images to 512x512pixels the features we're looking for are no longer discernable to the eye.  Which suggests that if my endpoint is resizing images, then that would explain why the model is struggling.<\/p>\n\n<p><strong>Note:<\/strong> Although the size in pexels is large, my images are basically line drawings on a white background. They have very little color and large patches of solid white, so they compress very well.  I'm mot running into the 6Mb request size limit.<\/p>\n\n<p>So, my questions are:<\/p>\n\n<ol>\n<li>Does training the model at image_shape=512 mean my prediction images should also be that same size?<\/li>\n<li>Is there a generally accepted method for doing object detection on very large images?  I can envisage how I might chop the image into smaller tiles then feed each tile to my model, but if there's something \"out of the box\" that will do it for me, then that'd save some effort.<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-01-15 06:28:29.27 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"aws-sdk|object-detection|amazon-sagemaker",
        "Question_view_count":771,
        "Owner_creation_date":"2008-11-18 05:09:29.4 UTC",
        "Owner_last_access_date":"2022-09-21 04:09:53.41 UTC",
        "Owner_reputation":5789,
        "Owner_up_votes":532,
        "Owner_down_votes":9,
        "Owner_views":464,
        "Answer_body":"<p>Your understanding is correct. The endpoint resizes images based on the parameter <code>image_shape<\/code>. To answer your questions:<\/p>\n\n<ol>\n<li>As long as the scale of objects (i.e., expansion of pixels) in the resized images are similar between training and prediction data, the trained model should work.<\/li>\n<li>Cropping is one option. Another method is to train separate models for large and small images as David suggested.<\/li>\n<\/ol>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-01-15 20:32:38.317 UTC",
        "Answer_score":1.0,
        "Owner_location":"Adelaide, Australia",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54193723",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73383302,
        "Question_title":"how does input_fn, predict_fn and output_fn work in aws sagemaker script mode?",
        "Question_body":"<p>i am trying to understand how input_fn, predict_fn and outout_fn work? I am able to understand what they are, but I am not able to understand  how they are called (invoked), can anyone help me understand the same<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-08-17 05:33:10.293 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":39,
        "Owner_creation_date":"2019-11-12 07:17:01.663 UTC",
        "Owner_last_access_date":"2022-09-25 05:05:06.423 UTC",
        "Owner_reputation":17,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":"<p>When your endpoint comes up, <code>model_fn<\/code> is invoked so that your model is loaded. When you invoke the endpoint, <code>input_fn<\/code> is called so that your input payload is parsed, immediately after that, <code>predict_fn<\/code> is called so that a prediction is generated, and then <code>output_fn<\/code> is called to parse the prediction before returning it to the caller.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-24 17:54:09.82 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73383302",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70835006,
        "Question_title":"How to integrate spark.ml pipeline fitting and hyperparameter optimisation in AWS Sagemaker?",
        "Question_body":"<p>Here is a high-level picture of what I am trying to achieve: I want to train a LightGBM model with spark as a compute <a href=\"https:\/\/microsoft.github.io\/SynapseML\/docs\/features\/lightgbm\/about\/\" rel=\"nofollow noreferrer\">backend<\/a>, all in SageMaker using their Training Job api.\nTo clarify:<\/p>\n<ol>\n<li>I have to use LightGBM in general, there is no option here.<\/li>\n<li>The reason I need to use spark compute backend is because the training with the current dataset does not fit in memory anymore.<\/li>\n<li>I want to use SageMaker Training job setting so I could use SM Hyperparameter optimisation job to find the best hyperparameters for LightGBM. While LightGBM spark interface itself does offer some hyperparameter tuning capabilities, it does not offer Bayesian HP tuning.<\/li>\n<\/ol>\n<p>Now, I know the general approach to running custom training in SM: build a container in a certain way, and then just pull it from ECR and kick-off a training job\/hyperparameter tuning job through <code>sagemaker.Estimator<\/code> <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html\" rel=\"nofollow noreferrer\">API<\/a>. Now, in this case SM would handle resource provisioning for you, would create an instance and so on. What I am confused about is that essentially, to use spark compute backend, I would need to have an EMR cluster running, so the SDK would have to handle that as well. However, I do not see how this is possible with the API above.<\/p>\n<p>Now, there is also that thing called <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">Sagemaker Pyspark SDK<\/a>. However, the provided <code>SageMakerEstimator<\/code> API from that package does not support on-the-fly cluster configuration either.<\/p>\n<p>Does anyone know a way how to run a Sagemaker training job that would use an EMR cluster so that later the same job could be used for hyperparameter tuning activities?<\/p>\n<p>One way I see is to run an EMR cluster in the background, and then just create a regular SM estimator job that would connect to the EMR cluster and do the training, essentially running a spark driver program in SM Estimator job.<\/p>\n<p>Has anyone done anything similar in the past?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-24 13:57:22.193 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"pyspark|amazon-sagemaker|apache-spark-ml",
        "Question_view_count":196,
        "Owner_creation_date":"2013-01-03 17:13:19.987 UTC",
        "Owner_last_access_date":"2022-09-20 21:44:43.16 UTC",
        "Owner_reputation":2171,
        "Owner_up_votes":162,
        "Owner_down_votes":19,
        "Owner_views":126,
        "Answer_body":"<p>Thanks for your questions. Here are answers:<\/p>\n<ul>\n<li><p><strong>SageMaker PySpark SDK<\/strong> <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/<\/a> does the opposite of what you want: being able to call a non-spark (or spark) SageMaker job from a Spark environment. Not sure that's what you need here.<\/p>\n<\/li>\n<li><p><strong>Running Spark in SageMaker jobs<\/strong>. While you can use SageMaker Notebooks to connect to a remote EMR cluster for interactive coding, you do not need EMR to run Spark in SageMaker jobs (Training and Processing). You have 2 options:<\/p>\n<ul>\n<li><p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_processing.html#pysparkprocessor\" rel=\"nofollow noreferrer\">SageMaker Processing has a built-in Spark Container<\/a>, which is easy to use but unfortunately not connected to SageMaker Model Tuning (that works with Training only). If you use this, you will have to find and use a third-party, external parameter search library ; for example <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-distributed-hyperparameter-and-neural-architecture-tuning-jobs-with-syne-tune\/\" rel=\"nofollow noreferrer\">Syne Tune<\/a> from AWS itself (that supports bayesian optimization)<\/p>\n<\/li>\n<li><p>SageMaker Training can run custom docker-based jobs, on one or multiple machines. If you can fit your Spark code within SageMaker Training spec, then you will be able to use SageMaker Model Tuning to tune your Spark code. However there is no framework container for Spark on SageMaker Training, so you would have to build your own, and I am not aware of any examples. Maybe you could get inspiration from the <a href=\"https:\/\/github.com\/aws\/sagemaker-spark-container\" rel=\"nofollow noreferrer\">Processing container code here<\/a> to build a custom Training container<\/p>\n<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>Your idea of using the Training job as a client to launch an EMR cluster is good and should work (if SM has the right permissions), and will indeed allow you to use SM Model Tuning. I'd recommend:<\/p>\n<ul>\n<li>each SM job to create a new transient cluster (auto-terminate after step) to keep costs low and avoid tuning results to be polluted by inter-job contention that could arise if running everything on the same cluster.<\/li>\n<li>use the cheapest possible instance type for the SM estimator, because it will need to stay up during all duration of your EMR experiment to collect and print your final metric (accuracy, duration, cost...)<\/li>\n<\/ul>\n<p>In the same spirit, I once used SageMaker Training myself to launch Batch Transform jobs for the sole purpose of leveraging the bayesian search API to find an inference configuration that minimizes cost.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-02-25 12:57:03.447 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70835006",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55987935,
        "Question_title":"How do I pull the pre-built docker images for SageMaker?",
        "Question_body":"<p>I'm trying to pull the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pre-built-docker-containers-frameworks.html\" rel=\"noreferrer\">pre-built docker images<\/a> for SageMaker. I am able to successfully <code>docker login<\/code> to ECR (my AWS credentials). When I try to pull the image I get the standard <code>no basic auth credentials<\/code>.<\/p>\n\n<p>Maybe I'm misunderstanding... I assumed those ECR URLs were public.<\/p>\n\n<pre><code>$(aws ecr get-login --region us-west-2 --no-include-email)\n\ndocker pull 246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-scikit-learn\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-05 00:33:55.31 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2019-05-06 20:45:42.127 UTC",
        "Question_score":5,
        "Question_tags":"amazon-web-services|docker|amazon-sagemaker|amazon-ecr",
        "Question_view_count":2738,
        "Owner_creation_date":"2012-07-25 17:32:50.417 UTC",
        "Owner_last_access_date":"2022-08-24 18:33:32.027 UTC",
        "Owner_reputation":1325,
        "Owner_up_votes":303,
        "Owner_down_votes":10,
        "Owner_views":131,
        "Answer_body":"<p>Could you show your ECR login command and pull command in the question?<\/p>\n\n<p>For SageMaker pre-built image 520713654638.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.3.0-cpu-py3<\/p>\n\n<p>What I do is:<\/p>\n\n<ol>\n<li>Log in ECR<\/li>\n<\/ol>\n\n<p><code>$(aws ecr get-login --no-include-email --registry-ids 520713654638 --region us-west-2)<\/code><\/p>\n\n<ol start=\"2\">\n<li>Pull the image<\/li>\n<\/ol>\n\n<p><code>docker pull 520713654638.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.3.0-cpu-py3<\/code><\/p>\n\n<p>These images are public readable so you can pull them from any AWS account. I guess the reason you failed is that you did not specify --registry-ids in your login. But it's better if you can provide your scripts for others to identify what's wrong.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-05-06 20:21:30.813 UTC",
        "Answer_score":3.0,
        "Owner_location":"California",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55987935",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50675708,
        "Question_title":"Sagemaker Predict on local instance, JSON Error",
        "Question_body":"<p>I'm trying to make transfer learning method on MXNet on Sagemaker instance. Train and serve start locally without any problem and I'm using that python code to predict:<\/p>\n\n<pre><code>def predict_mx(net, fname):\n    with open(fname, 'rb') as f:\n      img = image.imdecode(f.read())\n      plt.imshow(img.asnumpy())\n      plt.show()\n    data = transform(img, -1, test_augs)\n    plt.imshow(data.transpose((1,2,0)).asnumpy()\/255)\n    plt.show()\n    data = data.expand_dims(axis=0)\n    return net.predict(data.asnumpy().tolist())\n<\/code><\/pre>\n\n<p>I checked <code>data.asnumpy().tolist()<\/code> that is ok and pyplot draw images (firts is the original image, the second is the resized image). But <code>net.predict<\/code> raise an error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nJSONDecodeError                           Traceback (most recent call last)\n&lt;ipython-input-171-ea0f1f5bdc72&gt; in &lt;module&gt;()\n----&gt; 1 predict_mx(predictor.predict, '.\/data2\/burgers-imgnet\/00103785.jpg')\n\n&lt;ipython-input-170-150a72b14997&gt; in predict_mx(net, fname)\n     30     plt.show()\n     31     data = data.expand_dims(axis=0)\n---&gt; 32     return net(data.asnumpy().tolist())\n     33 \n\n~\/Projects\/Lab\/ML\/AWS\/v\/lib64\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data)\n     89         if self.deserializer is not None:\n     90             # It's the deserializer's responsibility to close the stream\n---&gt; 91             return self.deserializer(response_body, response['ContentType'])\n     92         data = response_body.read()\n     93         response_body.close()\n\n~\/Projects\/Lab\/ML\/AWS\/v\/lib64\/python3.6\/site-packages\/sagemaker\/predictor.py in __call__(self, stream, content_type)\n    290         \"\"\"\n    291         try:\n--&gt; 292             return json.load(codecs.getreader('utf-8')(stream))\n    293         finally:\n    294             stream.close()\n\n\/usr\/lib64\/python3.6\/json\/__init__.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    297         cls=cls, object_hook=object_hook,\n    298         parse_float=parse_float, parse_int=parse_int,\n--&gt; 299         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n    300 \n    301 \n\n\/usr\/lib64\/python3.6\/json\/__init__.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    352             parse_int is None and parse_float is None and\n    353             parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 354         return _default_decoder.decode(s)\n    355     if cls is None:\n    356         cls = JSONDecoder\n\n\/usr\/lib64\/python3.6\/json\/decoder.py in decode(self, s, _w)\n    337 \n    338         \"\"\"\n--&gt; 339         obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n    340         end = _w(s, end).end()\n    341         if end != len(s):\n\n\/usr\/lib64\/python3.6\/json\/decoder.py in raw_decode(self, s, idx)\n    355             obj, end = self.scan_once(s, idx)\n    356         except StopIteration as err:\n--&gt; 357             raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n    358         return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n<\/code><\/pre>\n\n<p>I tried to json.dumps my data, and there is no problem with that.<\/p>\n\n<p>Note that I didn't deployed the service on AWS yet, I want to be able to test the model and prediction locally before to make a larger train and to serve it later.<\/p>\n\n<p>Thanks for your help<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_date":"2018-06-04 07:48:56.343 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|mxnet|amazon-sagemaker",
        "Question_view_count":1358,
        "Owner_creation_date":"2012-06-21 12:10:16.09 UTC",
        "Owner_last_access_date":"2022-09-23 12:52:12.623 UTC",
        "Owner_reputation":2835,
        "Owner_up_votes":79,
        "Owner_down_votes":7,
        "Owner_views":281,
        "Answer_body":"<p>The call to <strong>net.predict<\/strong> is working fine. <\/p>\n\n<p>It seems that you are using the SageMaker Python SDK <strong>predict_fn<\/strong> for hosting. After the <strong>predict_fn<\/strong> is invoked, the MXNet container will try to serialize your prediction to JSON before sending it back to the client. You can see code that does that here: <a href=\"https:\/\/github.com\/aws\/sagemaker-mxnet-container\/blob\/master\/src\/mxnet_container\/serve\/transformer.py#L132\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-mxnet-container\/blob\/master\/src\/mxnet_container\/serve\/transformer.py#L132<\/a><\/p>\n\n<p>The container is failing to serialize because <strong>net.predict<\/strong> does not return a serializable object. You can solve this issue by returning a list instead:<\/p>\n\n<pre><code>return net.predict(data.asnumpy().tolist()).asnumpy().tolist()\n<\/code><\/pre>\n\n<p>Another alternative is to use a <strong>transform_fn<\/strong> instead of <strong>prediction_fn<\/strong> so you can handle the output serialization yourself. You can see an example of a <strong>transform_fn<\/strong> here <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/e93eff66626c0ab1f292048451c4c3ac7c39a121\/examples\/cli\/host\/script.py#L41\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/e93eff66626c0ab1f292048451c4c3ac7c39a121\/examples\/cli\/host\/script.py#L41<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-07-14 16:02:52.427 UTC",
        "Answer_score":1.0,
        "Owner_location":"Laval, France",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50675708",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63248562,
        "Question_title":"How to handle a .csv input for use in Tensorflow Serving batch transform?",
        "Question_body":"<p><strong>Information:<\/strong>\nI am loading an existing trained model.tar.gz from an S3 bucket, and want to perform a batch transform with a .csv containing the input data. The data.csv is structured in such a way that reading it into a pandas DataFrame gives me rows of complete prediction inputs.<\/p>\nNotes:\n<ul>\n<li>This is done on Amazon Sagemaker using the Python SDK<\/li>\n<li>BATCH_TRANSFORM_INPUT is the path to data.csv.<\/li>\n<li>I'm able to load the contents inside model.tar.gz and use them for inference on my local machine using tensorflow, and the logs show <code>2020-08-04 13:35:01.123557: I tensorflow_serving\/core\/loader_harness.cc:87] Successfully loaded servable version {name: model version: 1}<\/code>so the model seems to have been trained and saved properly.<\/li>\n<li>The data.csv is in the exact same format as the training data, which means one row per &quot;prediction&quot; where all columns in that row represents the different features.<\/li>\n<li>Changing the argument strategy to 'MultiRecord' gives the same error<\/li>\n<li>[path in s3] is a substitute for the real path as i don't want to reveal any bucket information.<\/li>\n<li>TensorFlow ModelServer: 2.0.0+dev.sha.ab786af<\/li>\n<li>TensorFlow Library: 2.0.2<\/li>\n<\/ul>\n<p>Where 1-5 are features, the file data.csv looks like:<\/p>\n<pre><code>+------+-------------------------+---------+----------+---------+----------+----------+\n| UNIT | TS                      | 1       | 2        | 3       | 4        | 5        |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:01:00.000 | 1.81766 | 0.178043 | 1.33607 | 25.42162 | 12.85445 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:02:00.000 | 1.81673 | 0.178168 | 1.30159 | 25.48204 | 12.87305 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:03:00.000 | 1.8155  | 0.176242 | 1.38399 | 25.35309 | 12.47222 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:04:00.000 | 1.81530 | 0.176398 | 1.39781 | 25.18216 | 12.16837 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:05:00.000 | 1.81505 | 0.151682 | 1.38451 | 25.22351 | 12.41623 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n<\/code><\/pre>\n<p>inference.py currently looks like:<\/p>\n<pre><code>def input_handler(data, context):\n    import pandas as pd\n    if context.request_content_type == 'text\/csv':\n        payload = pd.read_csv(data)\n        instance = [{&quot;dataset&quot;: payload}]\n        return json.dumps({&quot;instances&quot;: instance})\n    else:\n        _return_error(416, 'Unsupported content type &quot;{}&quot;'.format(context.request_content_type or 'Unknown'))\n<\/code><\/pre>\n<h3>The problem:<\/h3>\n<p>When the following code runs in my jupyter Notebook:<\/p>\n<pre><code>sagemaker_model = Model(model_data = '[path in s3]\/savedmodel\/model.tar.gz'),  \n                        sagemaker_session=sagemaker_session,\n                        role = role,\n                        framework_version='2.0',\n                        entry_point = os.path.join('training', 'inference.py')\n                        )\n\ntf_serving_transformer = sagemaker_model.transformer(instance_count=1,\n                                                     instance_type='ml.p2.xlarge',\n                                                     max_payload=1,\n                                                     output_path=BATCH_TRANSFORM_OUTPUT_DIR,\n                                                     strategy='SingleRecord')\n\n\ntf_serving_transformer.transform(data=BATCH_TRANSFORM_INPUT, data_type='S3Prefix', content_type='text\/csv')\ntf_serving_transformer.wait()\n<\/code><\/pre>\n<p>The model seems to get loaded, but I end up with the following error:\n<code>2020-08-04T09:54:27.415:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=1, BatchStrategy=SINGLE_RECORD 2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv: ClientError: 400 2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv:  2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv: Message: 2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv: { &quot;error&quot;: &quot;Failed to process element: 0 of 'instances' list. Error: Invalid argument: JSON Value: \\&quot;\\&quot; Type: String is not of expected type: float&quot; } <\/code><\/p>\n<p>Error more clearly:<\/p>\n<p><strong>ClientError: 400\nMessage: {&quot;error&quot;: &quot;Failed to process element: 0 of 'instances' list. Error: Invalid argument: JSON Value: &quot;&quot; Type: String is not of expected type: float&quot;}<\/strong><\/p>\n<p>If i understand this error correctly, something is wrong with the way my data is structured, so that sagemaker fails to deliver the input data to the TFS model. I suppose there is some &quot;input handling&quot; missing in my inference.py. Maybe the csv data has to somehow be translated into a compatible JSON, for TFS to use it? What exactly has to be done in input_handler() ?<\/p>\n<p>I appreciate all help, and am sorry for this confusing case. If there is any additional information needed, please ask and I'll gladly provide what I can.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2020-08-04 14:00:53.107 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2020-08-06 06:51:39.177 UTC",
        "Question_score":2,
        "Question_tags":"csv|amazon-s3|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":774,
        "Owner_creation_date":"2020-08-04 13:47:33.443 UTC",
        "Owner_last_access_date":"2020-08-19 12:14:42.567 UTC",
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p><strong>Solution:<\/strong> The problem was solved by saving the dataframe as .csv using the arguments header=False, index=False. This makes the saved csv not include the dataframe indexing labels. TFS accepted a clean .csv with only float values (without labels). I assume the error message <em>Invalid argument: JSON Value: &quot;&quot; Type: String is not of expected type: float<\/em> refers to the first cell in the csv, which if the csv was exported with labels is just an empty cell. When it got an empty string instead of a float value it got confused.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-08-06 06:51:08.183 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63248562",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57379173,
        "Question_title":"Where do I store my model's training data, artifacts, etc?",
        "Question_body":"<p>I'm trying to build and push a custom ML model with docker to Amazon SageMaker. I know things are supposed to follow the general structure of being in opt\/ml. But there's no such bucket in Amazon S3??? Am I supposed to create this directory within my container before I build and push the image to AWS? I just have no idea where to put my training data, etc.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-06 15:15:46.377 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1022,
        "Owner_creation_date":"2014-05-05 00:56:45.187 UTC",
        "Owner_last_access_date":"2022-05-04 22:46:57.89 UTC",
        "Owner_reputation":145,
        "Owner_up_votes":35,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":"<p>SageMaker is automating the deployment of the Docker image with your code using the convention of channel->local-folder. Everything that you define with a channel in your <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig\" rel=\"nofollow noreferrer\">input data configuration<\/a>, will be copied to the local Docker file system under <em>\/opt\/ml\/<\/em> folder, using the name of the channel as the name of the sub-folder.<\/p>\n\n<pre><code>{\n\"train\" : {\"ContentType\":  \"trainingContentType\", \n           \"TrainingInputMode\": \"File\", \n           \"S3DistributionType\": \"FullyReplicated\", \n           \"RecordWrapperType\": \"None\"},\n\"evaluation\" : {\"ContentType\":  \"evalContentType\", \n                \"TrainingInputMode\": \"File\", \n                \"S3DistributionType\": \"FullyReplicated\", \n                \"RecordWrapperType\": \"None\"},\n\"validation\" : {\"TrainingInputMode\": \"File\", \n                \"S3DistributionType\": \"FullyReplicated\", \n                \"RecordWrapperType\": \"None\"}\n} \n<\/code><\/pre>\n\n<p>to:<\/p>\n\n<pre><code>\/opt\/ml\/input\/data\/training\n\/opt\/ml\/input\/data\/validation\n\/opt\/ml\/input\/data\/testing\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-08-07 08:46:16.867 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57379173",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57212696,
        "Question_title":"Gluonnlp installation not found on Sagemaker jupyter notebook",
        "Question_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/I8c93.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/I8c93.png\" alt=\"enter image description here\"><\/a>I am attempting to install Gluonnlp to a sagemaker jupyter notebook. Im using the command <code>!sudo pip3 install gluonnlp<\/code> to install.  Which is successful.  However on import I get <code>ModuleNotFoundError: No module named 'gluonnlp'<\/code><\/p>\n\n<p>I got the same issue when attempting to install mxnet with pip in the same notebook.  It was resolved when I conda installed mxnet instead.  However conda install has not been working for gluonnlp as it cannot find the package.  I can't seem to find a way to conda install gluonnlp.  Any suggestions would be highly appreciated.<\/p>\n\n<p>Here are some of the commands I have tried<\/p>\n\n<p><code>!sudo pip3 install gluonnlp<\/code><\/p>\n\n<p><code>!conda install gluonnlp<\/code> --> Anaconda cant find the package in any channels<\/p>\n\n<pre><code>!conda install pip --y\n!sudo pip3 install gluonnlp\n\n!sudo pip3 install gluonnlp\n\n!conda install -c conda-forge gluonnlp --y\n<\/code><\/pre>\n\n<p>All these commands on my import \nimport warnings<\/p>\n\n<pre><code>warnings.filterwarnings('ignore')\n\nimport io\nimport random\nimport numpy as np\nimport mxnet as mx\nimport gluonnlp as nlp\nfrom bert import data, model\n<\/code><\/pre>\n\n<p>result in the error<\/p>\n\n<pre><code>ModuleNotFoundError: No module named 'gluonnlp'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_date":"2019-07-26 03:17:10.61 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-07-29 15:12:07.093 UTC",
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|jupyter-notebook|amazon-sagemaker|gluon",
        "Question_view_count":2015,
        "Owner_creation_date":"2018-07-17 15:14:49.147 UTC",
        "Owner_last_access_date":"2021-11-04 23:15:34.717 UTC",
        "Owner_reputation":425,
        "Owner_up_votes":12,
        "Owner_down_votes":1,
        "Owner_views":92,
        "Answer_body":"<p>this is as simple as creating a Jupyter notebook using the 'conda_mxnet_p36' kernel, and adding a cell containing:<\/p>\n\n<pre><code>!pip install gluonnlp\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2019-07-28 12:10:13.18 UTC",
        "Answer_score":0.0,
        "Owner_location":"Berkeley, CA, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57212696",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51710241,
        "Question_title":"Using numpy.ndarray type (multilabel) for labels in Sagemaker RecordIO format?",
        "Question_body":"<p>I am trying to write a numpy.ndarray as the labels for Amazon Sagemaker's conversion tool: write_numpy_to_dense_tensor(). It converts a numpy array of  features and labels to a RecordIO for better use of Sagemaker algorithms.<\/p>\n\n<p>However, if I try to pass a multilabel output for the labels, I get an error stating it can only be a vector (i.e. a scalar for every feature row).<\/p>\n\n<p>Is there any way of having multiple values in the label? This is useful for multidimensional regressions which can be achieved with XGBoost, Random Forests, Neural Networks, etc.<\/p>\n\n<p><strong>Code<\/strong><\/p>\n\n<pre><code>import sagemaker.amazon.common as smac\nprint(\"Types: {}, {}\".format(type(X_train), type(y_train)))\nprint(\"X_train shape: {}\".format(X_train.shape))\nprint(\"y_train shape: {}\".format(y_train.shape))\nf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(f, X_train.astype('float32'), y_train.astype('float32'))\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong><\/p>\n\n<pre><code>Types: &lt;class 'numpy.ndarray'&gt;, &lt;class 'numpy.ndarray'&gt;\nX_train shape: (9919, 2684)\ny_train shape: (9919, 20)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-14-fc1033b7e309&gt; in &lt;module&gt;()\n      3 print(\"y_train shape: {}\".format(y_train.shape))\n      4 f = io.BytesIO()\n----&gt; 5 smac.write_numpy_to_dense_tensor(f, X_train.astype('float32'), y_train.astype('float32'))\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/amazon\/common.py in write_numpy_to_dense_tensor(file, array, labels)\n     94     if labels is not None:\n     95         if not len(labels.shape) == 1:\n---&gt; 96             raise ValueError(\"Labels must be a Vector\")\n     97         if labels.shape[0] not in array.shape:\n     98             raise ValueError(\"Label shape {} not compatible with array shape {}\".format(\n\nValueError: Labels must be a Vector\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-08-06 14:43:44.69 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|mxnet|amazon-sagemaker",
        "Question_view_count":683,
        "Owner_creation_date":"2015-11-28 15:29:16.76 UTC",
        "Owner_last_access_date":"2022-09-15 00:33:36.597 UTC",
        "Owner_reputation":454,
        "Owner_up_votes":87,
        "Owner_down_votes":1,
        "Owner_views":44,
        "Answer_body":"<p>Tom, XGBoost does not support RecordIO format. It only supports csv and libsvm. Also, the algorithm itself doesn\u2019t natively support multi-label. But there are a couple of ways around it: <a href=\"https:\/\/stackoverflow.com\/questions\/40916939\/xg-boost-for-multilabel-classification\">Xg boost for multilabel classification?<\/a><\/p>\n\n<p>Random Cut Forest does not support multiple labels either. If more than one label is provided it picks up the first only.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-08-13 18:50:28.337 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51710241",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71467176,
        "Question_title":"How can I verify that my training job is reading the augmented manifest file?",
        "Question_body":"<p>Apologies for the long post.<\/p>\n<p>Originally, I had data in one location on an S3 bucket and used to train deep learning image classification models on this data using the typical 'File' mode and passing the S3 uri where the data is stored as training input. To try and accelerate training, I wanted to switch to using:<\/p>\n<ol>\n<li>Pipe mode, to stream data and not download all the data at the beginning of the training, starting training faster and saving disk space.<\/li>\n<li>Augmented Manifest File coupled with 1., so that I don't have to place my data in a single location on S3, so I avoid moving data around when I train models.<\/li>\n<\/ol>\n<p>I was making my script similar to <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=934156#934156\" rel=\"nofollow noreferrer\">the one in this example<\/a>. I printed the steps done when parsing the data, however I noticed that the data might not have been read because when printing it shows the following:<\/p>\n<pre><code>step 1 Tensor(&quot;ParseSingleExample\/ParseExample\/ParseExampleV2:0&quot;, shape=(), dtype=string)\nstep 2 Tensor(&quot;DecodePng:0&quot;, shape=(None, None, 3), dtype=uint8)\nstep 3 Tensor(&quot;Cast:0&quot;, shape=(None, None, 3), dtype=float32)\n<\/code><\/pre>\n<p>I guess the image is not being read\/found since the shape is <code>[None, None, 3]<\/code> when it should be <code>[224, 224, 3]<\/code>, so maybe the problem is from the Augmented Manifest file?<\/p>\n<p>Here's an example of how my Augmented Manifest file is written:<\/p>\n<pre><code>{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image1.png&quot;, &quot;label&quot;: 1}\n{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image2.png&quot;, &quot;label&quot;: 2}\n{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image3.png&quot;, &quot;label&quot;: 3}\n<\/code><\/pre>\n<p>Some other details I should probably mention:<\/p>\n<ol>\n<li>When I create the Training Input I pass <code>'content_type': 'application\/x-recordio', 'record_wrapping': 'RecordIO'<\/code>, even though my data are in .png format, but I assumed that as the augmented manifest file is read the data get wrapped in the RecordIO format.<\/li>\n<li>Following my first point, I pass <code>PipeModeDataset(channel=channel, record_format='RecordIO')<\/code>, so also not sure about the RecordIO thing.<\/li>\n<\/ol>\n<p>There isn't an actual error that is raised, just when I start fitting the model nothing happens, it keeps on running but nothing actually runs so I'm trying to find the issue.<\/p>\n<hr \/>\n<p>EDIT: It now reads the shape correctly, but there's still the issue where it enters the .fit method and does nothing, just keeps running without doing anything. Find part of the script below.<\/p>\n<pre><code>def train_input_fn(train_channel):\n    &quot;&quot;&quot;Returns input function that feeds the model during training&quot;&quot;&quot;\n    return _input_fn(train_channel)\n\ndef _input_fn(channel):\n    &quot;&quot;&quot;\n        Returns a Dataset which reads from a SageMaker PipeMode channel.\n    &quot;&quot;&quot;\n    \n    features = {\n        'image-ref': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([3], tf.int64),\n    }\n \n    def combine(records):\n        return records[0], records[1]\n \n    def parse(record):\n        \n        parsed = tf.io.parse_single_example(record, features)\n        \n                 \n\n        image = tf.io.decode_png(parsed[&quot;image-ref&quot;], channels=3, dtype=tf.uint8)\n        image = tf.reshape(image, [224, 224, 3])\n        \n        lbl = parsed['label']\n        print(image, lbl)\n        return (image, lbl)\n \n    ds = PipeModeDataset(channel=channel, record_format='RecordIO')\n    ds = ds.map(parse, num_parallel_calls=AUTOTUNE)\n    ds = ds.prefetch(AUTOTUNE)\n \n    return ds\n\ndef model(dataset):\n    &quot;&quot;&quot;Generate a simple model&quot;&quot;&quot;\n    inputs = Input(shape=(224, 224, 3))\n    prediction_layer = Dense(2, activation = 'softmax')\n\n\n    x = inputs\n    x = tf.keras.applications.mobilenet.MobileNet(include_top=False, input_shape=(224,224,3), weights='imagenet')(x)\n    outputs = prediction_layer(x)\n    rec_model = tf.keras.Model(inputs, outputs)    \n    \n    rec_model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n        metrics=['accuracy']\n    )\n    \n    \n    rec_model.fit(\n        dataset\n    )\n\n    return rec_model\n\ndef main(params):\n    \n    epochs = params['epochs']\n    train_channel = params['train_channel']\n    record_format = params['record_format']\n    batch_size = params['batch_size']\n        \n    train_spec = train_input_fn(train_channel)\n    model_classifier = model(train_spec)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-14 11:41:25.31 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2022-03-14 14:41:37.627 UTC",
        "Question_score":0,
        "Question_tags":"python|tensorflow|amazon-s3|manifest|amazon-sagemaker",
        "Question_view_count":127,
        "Owner_creation_date":"2019-12-10 22:23:16.283 UTC",
        "Owner_last_access_date":"2022-08-11 09:37:11.827 UTC",
        "Owner_reputation":15,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>From <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<blockquote>\n<p>A PipeModeDataset can read TFRecord, RecordIO, or text line records.<\/p>\n<\/blockquote>\n<p>While your'e trying to read binary (PNG) files. I don't see a relevant <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions\/tree\/tf-2\/src\/pipemode_op\/RecordReader\" rel=\"nofollow noreferrer\">record reader here<\/a> to help you do that.<br \/>\nYou could build your own format pipe implementation like shown <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">here<\/a>, but it's considerably more effort.<\/p>\n<p>Alternatively, you mentioned your files are scattered in different folders, but if your files common path contains less than 2M files, you could use <a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/10\/amazon-sagemaker-fast-file-mode\/\" rel=\"nofollow noreferrer\">FastFile mode<\/a> to <strong>stream<\/strong> data. Currently, FastFile only supports an S3 Prefix, so you won't be able to use a manifest.<\/p>\n<p>Also see this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/choose-the-best-data-source-for-your-amazon-sagemaker-training-job\/\" rel=\"nofollow noreferrer\">general pros\/cons discussion of the different available storage and input types available in SageMaker<\/a>.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2022-03-18 12:38:20.133 UTC",
        "Answer_score":3.0,
        "Owner_location":"Beirut, Lebanon",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71467176",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60644771,
        "Question_title":"How to parse AWS Sagemaker SM_USER_ARGS with argparse into an argparse Namespace?",
        "Question_body":"<p>AWS Sagemaker uses <code>SM_USER_ARGS<\/code> (as documented <a href=\"https:\/\/github.com\/aws\/sagemaker-containers#sm-user-args\" rel=\"nofollow noreferrer\">here<\/a>) as an environment variable in which it contains a string (list) of arguments as they are passed by the user. So the environment variable value looks like this: <code>'[\"--test_size\",\"0.2\",\"--random_seed\",\"42\", \"--not_optmize\"]'<\/code>.<\/p>\n\n<p>With <code>json.loads()<\/code> I am capable of transforming that string into a python list. Although, I want to create an abstract module that returns an <strong>argparse Namespace<\/strong> in a way that rest of the code remains intact whether I run it locally or in the AWS Sagemaker service.<\/p>\n\n<p>So, basically, what I want is a code that receives the input <code>[\"--test_size\",\"0.2\",\"--random_seed\",\"42\", \"--not_optmize\"]<\/code> and output <code>Namespace(test_size=0.2, random_seed='42', not_optmize=True, &lt;other_arguments&gt;... ])<\/code>.<\/p>\n\n<p>Does python <strong>argparse<\/strong> package helps me with that? I am trying to figure out a way that I do not need to re implement the argparse parser.<\/p>\n\n<p>Here is an example, I have this config.ini file:<\/p>\n\n<pre><code>[Docker]\nhome_dir = \/opt\nSM_MODEL_DIR = %(home_dir)s\/ml\/model\nSM_CHANNELS = [\"training\"]\nSM_NUM_GPUS = 1\nSM_NUM_CPUS =\nSM_LOG_LEVEL = 20\nSM_USER_ARGS = [\"--test_size\",\"0.2\",\"--random_seed\",\"42\"]\nSM_INPUT_DIR = %(home_dir)s\/ml\/input\nSM_INPUT_CONFIG_DIR = %(home_dir)s\/ml\/input\/config\nSM_OUTPUT_DIR = %(home_dir)s\/ml\/output\nSM_OUTPUT_INTERMEDIATE_DIR = %(home_dir)s\/ml\/output\/intermediate\n<\/code><\/pre>\n\n<p>I have this Argparser class:<\/p>\n\n<pre><code>import argparse\nimport configparser\nimport datetime\nimport json\nimport multiprocessing\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict\n\nfrom .files import JsonFile, YAMLFile\n\n\nclass ArgParser(ABC):\n\n    @abstractmethod\n    def get_arguments(self) -&gt; Dict[str, Any]:\n        pass\n\n\nclass AWSArgParser(ArgParser):\n\n    def __init__(self):\n        configuration_file_path = 'config.ini'\n\n        self.environment = \"Sagemaker\" \\\n            if os.environ.get(\"SM_MODEL_DIR\", False) \\\n            else os.environ.get(\"ENVIRON\", \"Default\")\n\n        config = configparser.ConfigParser()\n        config.read(configuration_file_path)\n        if self.environment == \"Local\":\n            config[self.environment][\"home_dir\"] = str(pathlib.Path(__file__).parent.absolute())\n        if self.environment != 'Sagemaker':\n            config[self.environment][\"SM_NUM_CPUS\"] = str(multiprocessing.cpu_count())\n\n        for key, value in config[self.environment].items():\n            os.environ[key.upper()] = value\n\n        self.parser = argparse.ArgumentParser()\n        # AWS Sagemaker default environmental arguments\n        self.parser.add_argument(\n            '--model_dir',\n            type=str,\n            default=os.environ['SM_MODEL_DIR'],\n        )\n        self.parser.add_argument(\n            '--channel_names',\n            default=json.loads(os.environ['SM_CHANNELS']),\n        )\n        self.parser.add_argument(\n            '--num_gpus',\n            type=int,\n            default=os.environ['SM_NUM_GPUS'],\n        )\n        self.parser.add_argument(\n            '--num_cpus',\n            type=int,\n            default=os.environ['SM_NUM_CPUS'],\n        )\n        self.parser.add_argument(\n            '--user_args',\n            default=json.loads(os.environ['SM_USER_ARGS']),\n        )\n        self.parser.add_argument(\n            '--input_dir',\n            type=str,\n            default=os.environ['SM_INPUT_DIR'],\n        )\n        self.parser.add_argument(\n            '--input_config_dir',\n            type=Path,\n            default=os.environ['SM_INPUT_CONFIG_DIR'],\n        )\n        self.parser.add_argument(\n            '--output_dir',\n            type=Path,\n            default=os.environ['SM_OUTPUT_DIR'],\n        )\n\n        # Extra arguments\n        self.run_tag = datetime.datetime \\\n            .fromtimestamp(time.time()) \\\n            .strftime('%Y-%m-%d-%H-%M-%S')\n        self.parser.add_argument(\n            '--run_tag',\n            default=self.run_tag,\n            type=str,\n            help=f\"Run tag (default: 'datetime.fromtimestamp')\",\n        )\n        self.parser.add_argument(\n            '--environment',\n            type=str,\n            default=self.environment,\n        )\n\n        self.args = self.parser.parse_args()\n\n    def get_arguments(self) -&gt; Dict[str, Any]:\n        &lt;parse self.args.user_args&gt;\n\n        return self.args\n<\/code><\/pre>\n\n<p>then I have my <code>train<\/code> script:<\/p>\n\n<pre><code>from utils.arg_parser import AWSArgParser\n\nif __name__ == '__main__':\n    logger.info(f\"Begin train.py\")\n\n    if os.environ[\"ENVIRON\"] == \"Sagemaker\":\n        arg_parser = AWSArgParser()\n        args = arg_parser.get_arguments()\n    else:\n        args = &lt;normal local parse&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_date":"2020-03-11 21:40:23.9 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-03-11 22:13:11.167 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|argparse|amazon-sagemaker",
        "Question_view_count":782,
        "Owner_creation_date":"2016-02-14 10:09:50.637 UTC",
        "Owner_last_access_date":"2022-09-21 20:43:14.553 UTC",
        "Owner_reputation":903,
        "Owner_up_votes":463,
        "Owner_down_votes":28,
        "Owner_views":94,
        "Answer_body":"<p>Following <a href=\"https:\/\/stackoverflow.com\/users\/1126841\/chepner\">@chepner<\/a>'s comment an example solution would be something like this:<\/p>\n\n<p>config.ini file:<\/p>\n\n<pre><code>[Docker]\nhome_dir = \/opt\nSM_MODEL_DIR = %(home_dir)s\/ml\/model\nSM_CHANNELS = [\"training\"]\nSM_NUM_GPUS = 1\nSM_NUM_CPUS =\nSM_LOG_LEVEL = 20\nSM_USER_ARGS = [\"--test_size\",\"0.2\",\"--random_seed\",\"42\", \"--not_optimize\"]\nSM_INPUT_DIR = %(home_dir)s\/ml\/input\nSM_INPUT_CONFIG_DIR = %(home_dir)s\/ml\/input\/config\nSM_OUTPUT_DIR = %(home_dir)s\/ml\/output\nSM_OUTPUT_INTERMEDIATE_DIR = %(home_dir)s\/ml\/output\/intermediate\n<\/code><\/pre>\n\n<p>A <code>TrainArgParser<\/code> class like this:<\/p>\n\n<pre><code>class ArgParser(ABC):\n\n    @abstractmethod\n    def get_arguments(self) -&gt; Dict[str, Any]:\n        pass\n\n\nclass TrainArgParser(ArgParser):\n\n    def __init__(self):\n        configuration_file_path = 'config.ini'\n\n        self.environment = \"Sagemaker\" \\\n            if os.environ.get(\"SM_MODEL_DIR\", False) \\\n            else os.environ.get(\"ENVIRON\", \"Default\")\n\n        config = configparser.ConfigParser()\n        config.read(configuration_file_path)\n        if self.environment == \"Local\":\n            config[self.environment][\"home_dir\"] = str(pathlib.Path(__file__).parent.absolute())\n        if self.environment != 'Sagemaker':\n            config[self.environment][\"SM_NUM_CPUS\"] = str(multiprocessing.cpu_count())\n\n        for key, value in config[self.environment].items():\n            os.environ[key.upper()] = value\n\n        self.parser = argparse.ArgumentParser()\n        # AWS Sagemaker default environmental arguments\n        self.parser.add_argument(\n            '--model_dir',\n            type=str,\n            default=os.environ['SM_MODEL_DIR'],\n        )\n        self.parser.add_argument(\n            '--channel_names',\n            default=json.loads(os.environ['SM_CHANNELS']),\n        )\n        self.parser.add_argument(\n            '--num_gpus',\n            type=int,\n            default=os.environ['SM_NUM_GPUS'],\n        )\n        self.parser.add_argument(\n            '--num_cpus',\n            type=int,\n            default=os.environ['SM_NUM_CPUS'],\n        )\n        self.parser.add_argument(\n            '--user_args',\n            default=json.loads(os.environ['SM_USER_ARGS']),\n        )\n        self.parser.add_argument(\n            '--input_dir',\n            type=str,\n            default=os.environ['SM_INPUT_DIR'],\n        )\n        self.parser.add_argument(\n            '--input_config_dir',\n            type=Path,\n            default=os.environ['SM_INPUT_CONFIG_DIR'],\n        )\n        self.parser.add_argument(\n            '--output_dir',\n            type=Path,\n            default=os.environ['SM_OUTPUT_DIR'],\n        )\n\n        # Extra arguments\n        self.run_tag = datetime.datetime \\\n            .fromtimestamp(time.time()) \\\n            .strftime('%Y-%m-%d-%H-%M-%S')\n        self.parser.add_argument(\n            '--run_tag',\n            default=self.run_tag,\n            type=str,\n            help=f\"Run tag (default: 'datetime.fromtimestamp')\",\n        )\n        self.parser.add_argument(\n            '--environment',\n            type=str,\n            default=self.environment,\n        )\n\n        self.args = self.parser.parse_args()\n\n    def get_arguments(self) -&gt; Dict[str, Any]:\n        # Not in AWS Sagemaker arguments\n        self.parser.add_argument(\n            '--test_size',\n            default=0.2,\n            type=float,\n            help=\"Test dataset size (default: '0.2')\",\n        )\n        self.parser.add_argument(\n            '--random_seed',\n            default=42,\n            type=int,\n            help=\"Random number for initialization (default: '42')\",\n        )\n        self.parser.add_argument(\n            '--secrets',\n            type=YAMLFile.parse_string,\n            default='',\n            help=\"An yaml formated string (default: '')\"\n        )\n        self.parser.add_argument(\n            '--bucket_name',\n            type=str,\n            default='',\n            help=\"Bucket name of a remote storage (default: '')\"\n        )\n        self.args = self.parser.parse_args(self.args.user_args)\n\n        return self.args\n<\/code><\/pre>\n\n<p>and a entry_script for <code>train<\/code> would start like this:<\/p>\n\n<pre><code>#!\/usr\/bin\/env python\n\nfrom utils.arg_parser import TrainArgParser\n\nif __name__ == '__main__':\n    logger.info(f\"Begin train.py\")\n\n    arg_parser = TrainArgParser()\n    args = arg_parser.get_arguments()\n    print(args)\n<\/code><\/pre>\n\n<p>This should output something like this:<\/p>\n\n<pre><code>Namespace(bucket_name='', channel_names=['training'], environment='Docker', input_config_dir=PosixPath('\/opt\/ml\/input\/config'), input_dir='\/opt\/ml\/input', model_dir='\/opt\/ml\/model', num_cpus=8, num_gpus=1, output_dir=PosixPath('\/opt\/ml\/output'), random_seed=42, run_tag='2020-03-11-22-18-21', secrets={}, test_size=0.2, user_args=['--test_size', '0.2', '--random_seed', '42'])\n<\/code><\/pre>\n\n<p>But that is useless if AWS Sagemaker treats <code>SM_USER_ARGS<\/code> and <code>SM_HPS<\/code> as the same thing. :(<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-03-11 22:30:47.987 UTC",
        "Answer_score":0.0,
        "Owner_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60644771",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65091602,
        "Question_title":"Is it possible to use more than 50 Labels in AWS Ground Truth",
        "Question_body":"<p>Is it possible to use more than 50 labels with AWS Ground Truth?<\/p>\n<p>For example <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-bounding-box.html\" rel=\"nofollow noreferrer\">here<\/a> are 3 labels:<\/p>\n<ul>\n<li>bird<\/li>\n<li>plane<\/li>\n<li>kite<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/GII4X.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/GII4X.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It shows that only 50 labels can be created. Is it possible to create more than 50 labels via AWS-CLI or any other API?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-12-01 13:40:24.587 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|labeling|amazon-ground-truth",
        "Question_view_count":149,
        "Owner_creation_date":"2017-11-07 14:18:51.503 UTC",
        "Owner_last_access_date":"2022-09-24 17:52:28.29 UTC",
        "Owner_reputation":5537,
        "Owner_up_votes":1253,
        "Owner_down_votes":7,
        "Owner_views":215,
        "Answer_body":"<p>No, according to the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-text-classification-multilabel.html\" rel=\"nofollow noreferrer\">documentation<\/a> the maximum is 50.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-12-01 14:14:57.01 UTC",
        "Answer_score":1.0,
        "Owner_location":"M\u00fcnchen, Deutschland",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65091602",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60405600,
        "Question_title":"How to make inference on local PC with the model trained on AWS SageMaker by using the built-in algorithm Semantic Segmentation?",
        "Question_body":"<p>Similar to the issue of <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/200\" rel=\"nofollow noreferrer\">The trained model can be deployed on the other platform without dependency of sagemaker or aws service?<\/a>.<\/p>\n\n<p>I have trained a model on AWS SageMaker by using the built-in algorithm <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/semantic-segmentation.html\" rel=\"nofollow noreferrer\">Semantic Segmentation<\/a>. This trained model named as <code>model.tar.gz<\/code> is stored on S3. So I want to download this file from S3 and then use it to make inference on my local PC without using AWS SageMaker anymore. Since the built-in algorithm Semantic Segmentation is built using the <a href=\"https:\/\/github.com\/dmlc\/gluon-cv\" rel=\"nofollow noreferrer\">MXNet Gluon framework and the Gluon CV toolkit<\/a>, so I try to refer the documentation of <a href=\"https:\/\/mxnet.apache.org\/\" rel=\"nofollow noreferrer\">mxnet<\/a> and <a href=\"https:\/\/gluon-cv.mxnet.io\/\" rel=\"nofollow noreferrer\">gluon-cv<\/a> to make inference on local PC.<\/p>\n\n<p>It's easy to download this file from S3, and then I unzip this file to get three files:<\/p>\n\n<ol>\n<li><strong>hyperparams.json<\/strong>: includes the parameters for network architecture, data inputs, and training. Refer to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/segmentation-hyperparameters.html\" rel=\"nofollow noreferrer\">Semantic Segmentation Hyperparameters<\/a>.<\/li>\n<li><strong>model_algo-1<\/strong><\/li>\n<li><strong>model_best.params<\/strong><\/li>\n<\/ol>\n\n<p>Both <strong>model_algo-1<\/strong> and <strong>model_best.params<\/strong> are the trained models, and I think it's the output from <code>net.save_parameters<\/code> (Refer to <a href=\"https:\/\/beta.mxnet.io\/guide\/getting-started\/crash-course\/4-train.html\" rel=\"nofollow noreferrer\">Train the neural network<\/a>). I can also load them with the function <code>mxnet.ndarray.load<\/code>.<\/p>\n\n<p>Refer to <a href=\"https:\/\/beta.mxnet.io\/guide\/getting-started\/crash-course\/5-predict.html\" rel=\"nofollow noreferrer\">Predict with a pre-trained model<\/a>. I found there are two necessary things:<\/p>\n\n<ol>\n<li>Reconstruct the network for making inference.<\/li>\n<li>Load the trained parameters.<\/li>\n<\/ol>\n\n<p>As for reconstructing the network for making inference, since I have used PSPNet from training, so I can use the class <code>gluoncv.model_zoo.PSPNet<\/code> to reconstruct the network. And I know how to use some services of AWS SageMaker, for example batch transform jobs, to make inference. I want to reproduce it on my local PC. If I use the class <code>gluoncv.model_zoo.PSPNet<\/code> to reconstruct the network, I can't make sure whether the parameters for this network are same those used on AWS SageMaker while making inference. Because I can't see the image <code>501404015308.dkr.ecr.ap-northeast-1.amazonaws.com\/semantic-segmentation:latest<\/code> in detail. <\/p>\n\n<p>As for loading the trained parameters, I can use the <code>load_parameters<\/code>. But as for <strong>model_algo-1<\/strong> and <strong>model_best.params<\/strong>, I don't know which one I should use.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-26 01:40:49.783 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-02-26 01:52:24.47 UTC",
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|mxnet|gluon",
        "Question_view_count":351,
        "Owner_creation_date":"2016-09-04 05:43:41.507 UTC",
        "Owner_last_access_date":"2022-09-16 07:40:33.96 UTC",
        "Owner_reputation":61,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>The following code works well for me.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import mxnet as mx\nfrom mxnet import image\nfrom gluoncv.data.transforms.presets.segmentation import test_transform\nimport gluoncv\n\n# use cpu\nctx = mx.cpu(0)\n\n# load test image\nimg = image.imread('.\/img\/IMG_4015.jpg')\nimg = test_transform(img, ctx)\nimg = img.astype('float32')\n\n# reconstruct the PSP network model\nmodel = gluoncv.model_zoo.PSPNet(2)\n\n# load the trained model\nmodel.load_parameters('.\/model\/model_algo-1')\n\n# make inference\noutput = model.predict(img)\npredict = mx.nd.squeeze(mx.nd.argmax(output, 1)).asnumpy()\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-03-02 05:15:37.777 UTC",
        "Answer_score":0.0,
        "Owner_location":"Tokyo, Japan",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60405600",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50049928,
        "Question_title":"Sagemaker image classification: Best way to perform inference on many images in S3?",
        "Question_body":"<p>I trained a model with the built-in RESnet18 docker image, and now I want to deploy the model to an endpoint and classify ~ 1 million images. I have all my training, validation, and test images stored on S3 in RecordIO format (converted with <a href=\"https:\/\/mxnet.incubator.apache.org\/tutorials\/basic\/data.html?highlight=im2rec\" rel=\"nofollow noreferrer\">im2rec.py<\/a>). According to the <a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>The Amazon SageMaker Image Classification algorithm supports both RecordIO (application\/x-recordio) and image (application\/x-image) content types for training. The algorithm supports only\u00a0application\/x-image\u00a0for inference.<\/p>\n<\/blockquote>\n\n<p>So I cannot perform inference on my training data in RecordIO format. To overcome this I copied all the raw .jpg images (~ 2GB) onto my Sagemaker Jupyter Notebook instance and performed inference one at a time in the following way:<\/p>\n\n<pre><code>img_list = os.listdir('temp_data') # list of all ~1,000,000 images\n\nfor im in img_list:\n    with open('temp_data\/'+im, 'rb') as f:\n        payload = f.read()\n        payload = bytearray(payload)\n    response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n                                       ContentType='application\/x-image', \n                                       Body=payload)\n\n    etc...\n<\/code><\/pre>\n\n<p>Needless to say, transferring all the data onto my Notebook instance took a long time and I would prefer not having to do that before running inference. Why does the SageMaker Image Classification not support RecordIO for inference? And more importantly, what is the best way to run inference on many images without having to move them from S3?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-04-26 18:39:35.17 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-s3|resnet|amazon-sagemaker",
        "Question_view_count":2600,
        "Owner_creation_date":"2016-09-22 05:01:46.39 UTC",
        "Owner_last_access_date":"2021-07-08 05:36:01.31 UTC",
        "Owner_reputation":832,
        "Owner_up_votes":41,
        "Owner_down_votes":0,
        "Owner_views":48,
        "Answer_body":"<p>The RecordIO format is designed to pack a large number of images into a single file, so I don't think it would work well for predicting single images.<\/p>\n\n<p>When it comes to prediction, you definitely don't have to copy images to a notebook instance or to S3. You just have to load them from anywhere and inline them in your prediction requests.<\/p>\n\n<p><strong>If you want HTTP-based prediction, here are your options:<\/strong><\/p>\n\n<p>1) Use the SageMaker SDK Predictor.predict() API on any machine (as long as it has proper AWS credentials) <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a><\/p>\n\n<p>2) Use the AWS Python SDK (aka boto3) API invoke_endpoint() on any machine (as long as it has proper AWS credentials)<\/p>\n\n<p>You can even build a simple service to perform pre-processing or post-processing with Lambda. Here's an example: <a href=\"https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033<\/a><\/p>\n\n<p><strong>If you want batch prediction:<\/strong>\n the simplest way is to retrieve the trained model from SageMaker, write a few lines of ad-hoc MXNet code to load it and run all your predictions. Here's an example: <a href=\"https:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html\" rel=\"nofollow noreferrer\">https:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html<\/a><\/p>\n\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-07-12 04:22:35.823 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50049928",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63024900,
        "Question_title":"How to train and deploy model in script mode on Sagemaker without using jupyter notebook instance (serverless)?",
        "Question_body":"<p>I have been using a jupyter notebook instance to spin up a training job (on separate instance) and deploy the endpoint (on another instance). I am using sagemaker tensorflow APIs for this as shown below:<\/p>\n<pre><code># create Tensorflow object and provide and entry point script\ntf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',\n                      train_instance_count=1, train_instance_type='ml.p2.xlarge',\n                      framework_version='1.12', py_version='py3')\n\n# train model on data on s3 and save model artifacts to s3\ntf_estimator.fit('s3:\/\/bucket\/path\/to\/training\/data')\n\n# deploy model on another instance using checkpoints saved on S3\npredictor = estimator.deploy(initial_instance_count=1,\n                         instance_type='ml.c5.xlarge',\n                         endpoint_type='tensorflow-serving')\n<\/code><\/pre>\n<p>I have been doing all of these steps through a jupyter notebook instance. What AWS services I can use to get rid off the dependency of jupyter notebook instance and automate these tasks of training and deploying the model in serverless fashion?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-22 00:19:39.513 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|tensorflow|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":758,
        "Owner_creation_date":"2013-09-04 04:27:22.847 UTC",
        "Owner_last_access_date":"2022-09-22 00:59:12.557 UTC",
        "Owner_reputation":4616,
        "Owner_up_votes":751,
        "Owner_down_votes":5,
        "Owner_views":592,
        "Answer_body":"<p>I recommend <code>AWS Step Functions<\/code>.  Been using it to schedule <code>SageMaker Batch Transform<\/code> and preprocessing jobs since it integrates with <code>CloudWatch<\/code> event rules.  It can also train models, perform hpo tuning, and integrates with <code>lambda<\/code>.  There is a SageMaker\/Step Functions SDK as well as you can use Step Functions directly by creating state machines. Some examples and documentation:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/11\/introducing-aws-step-functions-data-science-sdk-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/11\/introducing-aws-step-functions-data-science-sdk-amazon-sagemaker\/<\/a><\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/connect-sagemaker.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/connect-sagemaker.html<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-07-24 21:06:01.857 UTC",
        "Answer_score":2.0,
        "Owner_location":"Pune, India",
        "Answer_last_edit_date":"2020-07-24 21:17:13.193 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63024900",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71340893,
        "Question_title":"When I get a prediction from sagemaker endpoint, what does the endpoint do?",
        "Question_body":"<p>In sagemaker, the docs talk about inference scripts requiring to have 4 specific functions. When we get a prediction, the python SDK sends a request to the endpoint.<\/p>\n<p>Then the inference script runs. But I cannot find where in the SDK the inference script is run.<\/p>\n<p>When I navigate through the sdk code the <code>Predictor.predict()<\/code> method calls the sagemaker session to post a request to the endpoint and get a response. That is the final step in the sdk. Sagemaker is obviously doing something when it receives that request.<\/p>\n<p>What is the code that it runs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-03 16:58:59.29 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":405,
        "Owner_creation_date":"2020-01-13 16:18:39.743 UTC",
        "Owner_last_access_date":"2022-09-23 12:14:46.19 UTC",
        "Owner_reputation":1012,
        "Owner_up_votes":91,
        "Owner_down_votes":96,
        "Owner_views":66,
        "Answer_body":"<p>The endpoint is essentially a Flask web server running in a Docker container<\/p>\n<p>If it's a scikit-learn image, when you invoke the endpoint, it loads your script from S3, then...<\/p>\n<p>It calls <code>input_fn(request_body: bytearray, content_type) -&gt; np.ndarray<\/code> to parse the <code>request_body<\/code> into a numpy array<\/p>\n<p>Then it calls your <code>model_fn(model_dir: str) -&gt; object<\/code> function to load the model from <code>model_dir<\/code> and return the model<\/p>\n<p>Then it calls <code>predict_fn(input_object: np.ndarray, model: object) -&gt; np.array<\/code>, which calls your <code>model.predict()<\/code> function and returns the prediction<\/p>\n<p>Then it calls <code>output_fn(prediction: np.array, accept: str)<\/code> to take the result from <code>predict_fn<\/code> and encode it to the <code>accept<\/code> type<\/p>\n<p>You don't need to implement all of these functions yourself, as there are defaults<\/p>\n<p>You <strong>do<\/strong> need to implement <code>model_fn<\/code><\/p>\n<p>You only need to implement <code>input_fn<\/code> if you have non numeric data<\/p>\n<p>You only need to implement <code>predict_fn<\/code> if your model uses something other than <code>.predict()<\/code><\/p>\n<p>You can see how the default function implementations work <a href=\"https:\/\/github.com\/aws\/sagemaker-scikit-learn-container\/blob\/master\/src\/sagemaker_sklearn_container\/serving.py\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-03-03 17:38:04.32 UTC",
        "Answer_score":2.0,
        "Owner_location":"Ireland",
        "Answer_last_edit_date":"2022-03-08 17:40:23.947 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71340893",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69853177,
        "Question_title":"Docker-compose wouldn't start on Sagemaker's Notebook instance",
        "Question_body":"<p>Docker-compose seems to have stopped working on Sagemaker Notebook instances. When running <code>docker-compose up<\/code> I encounter the following error:<\/p>\n<pre><code>During handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/bin\/docker-compose&quot;, line 8, in &lt;module&gt;\n    sys.exit(main())\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/main.py&quot;, line 81, in main\n    command_func()\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/main.py&quot;, line 200, in perform_command\n    project = project_from_options('.', options)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/command.py&quot;, line 70, in project_from_options\n    enabled_profiles=get_profiles_from_options(options, environment)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/command.py&quot;, line 153, in get_project\n    verbose=verbose, version=api_version, context=context, environment=environment\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/docker_client.py&quot;, line 43, in get_client\n    environment=environment, tls_version=get_tls_version(environment)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/docker_client.py&quot;, line 170, in docker_client\n    client = APIClient(use_ssh_client=not use_paramiko_ssh, **kwargs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/docker\/api\/client.py&quot;, line 197, in __init__\n    self._version = self._retrieve_server_version()\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/docker\/api\/client.py&quot;, line 222, in _retrieve_server_version\n    'Error while fetching server API version: {0}'.format(e)\ndocker.errors.DockerException: Error while fetching server API version: Timeout value connect was Timeout(connect=60, read=60, total=None), but it must be an int, float or None\n<\/code><\/pre>\n<p>I can start Docker containers as usual.<\/p>\n<pre><code>sh-4.2$ docker version\nClient:\n Version:           20.10.7\n API version:       1.41\n Go version:        go1.15.14\n Git commit:        f0df350\n Built:             Tue Sep 28 19:55:40 2021\n OS\/Arch:           linux\/amd64\n Context:           default\n Experimental:      true\n\nServer:\n Engine:\n  Version:          20.10.7\n  API version:      1.41 (minimum version 1.12)\n  Go version:       go1.15.14\n  Git commit:       b0f5bc3\n  Built:            Tue Sep 28 19:57:35 2021\n  OS\/Arch:          linux\/amd64\n  Experimental:     false\n containerd:\n  Version:          1.4.6\n  GitCommit:        d71fcd7d8303cbf684402823e425e9dd2e99285d\n runc:\n  Version:          1.0.0\n  GitCommit:        %runc_commit\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n<\/code><\/pre>\n<p>But <code>docker-compose<\/code> wouldn't work...<\/p>\n<pre><code>sh-4.2$ docker-compose version\ndocker-compose version 1.29.2, build unknown\ndocker-py version: 5.0.0\nCPython version: 3.6.13\nOpenSSL version: OpenSSL 1.1.1l  24 Aug 2021\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-05 12:22:23.82 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-11-05 12:36:51.803 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|docker|docker-compose|amazon-sagemaker",
        "Question_view_count":312,
        "Owner_creation_date":"2013-11-15 15:40:39.387 UTC",
        "Owner_last_access_date":"2022-09-24 12:00:52.63 UTC",
        "Owner_reputation":2470,
        "Owner_up_votes":910,
        "Owner_down_votes":11,
        "Owner_views":285,
        "Answer_body":"<p>For those of you who (might) have encountered the same issue, here's the fix:<\/p>\n<p>1). Install the newest version of docker-compose:<\/p>\n<pre><code>sh-4.2$ sudo curl -L &quot;https:\/\/github.com\/docker\/compose\/releases\/download\/1.29.2\/docker-compose-$(uname -s)-$(uname -m)&quot; -o \/usr\/local\/bin\/docker-compose\nsh-4.2$ sudo chmod +x \/usr\/local\/bin\/docker-compose\n<\/code><\/pre>\n<p>2). Change your <code>PATH<\/code> accordingly (since docker-compose is installed using <code>conda<\/code> and is picked up first) or use <code>\/usr\/local\/bin\/docker-compose<\/code> onwards:<\/p>\n<pre><code>sh-4.2$ PATH=\/usr\/local\/bin:$PATH\nsh-4.2$ docker-compose version\ndocker-compose version 1.29.2, build 5becea4c\ndocker-py version: 5.0.0\nCPython version: 3.7.10\nOpenSSL version: OpenSSL 1.1.0l  10 Sep 2019\n<\/code><\/pre>\n<p>Perhaps, the issue is related to this:<\/p>\n<blockquote>\n<p>On August 9, 2021 the Jupyter Notebook and Jupyter Lab open source software projects announced 2 security concerns that could impact Amazon Sagemaker Notebook Instance customers.<\/p>\n<p>Sagemaker has deployed updates to address these concerns, and we recommend customers with existing notebook sessions to stop and restart their notebook instance(s) to benefit from these updates. Notebook instances launched after August 10, 2021, when updates were deployed, are not impacted by this issue and do not need to be restarted.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-11-05 12:22:23.82 UTC",
        "Answer_score":0.0,
        "Owner_location":"Ljubljana, Slovenia",
        "Answer_last_edit_date":"2021-11-06 07:21:15.063 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69853177",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71611419,
        "Question_title":"Mixing shell variables and python variables in IPython '!command'",
        "Question_body":"<p>Trying to figure out whether this behaviour on IPython (v7.12.0, on Amazon SageMaker) is a bug or I'm missing some proper way \/ documented constraint...<\/p>\n<p>Say I have some Python variables like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>NODE_VER = &quot;v16.14.2&quot;\nNODE_DISTRO = &quot;linux-x64&quot;\n<\/code><\/pre>\n<p>These commands both work fine in a notebook:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>!echo $PATH\n# Shows **contents of system path**\n!echo \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:\n# Shows \/usr\/local\/lib\/nodejs\/node-v16.14.2-linux-x64\/bin\n<\/code><\/pre>\n<p>...But this does not:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>!echo \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:$PATH\n# Shows:\n# \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:**contents of system path**\n<\/code><\/pre>\n<p>I've tried a couple of combinations of e.g. using <code>$NODE_VER<\/code> syntax instead (which produces <code>node--\/<\/code> instead of <code>node-{NODE_VER}-{NODE_DISTRO}\/<\/code>, but seems like any combination using both shell variables (PATH) and Python variables (NODE_VER\/NODE_DISTRO) fails.<\/p>\n<p>Can anybody help me understand why and how to work around it?<\/p>\n<p>My end goal, as you might have guessed already, is to actually add this folder to the PATH rather than just echoing it - something like:<\/p>\n<pre><code>!export PATH=\/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:$PATH\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-03-25 02:12:04.543 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"jupyter-notebook|ipython|jupyter-lab|amazon-sagemaker",
        "Question_view_count":160,
        "Owner_creation_date":"2020-04-19 07:33:10.603 UTC",
        "Owner_last_access_date":"2022-09-20 07:39:49.537 UTC",
        "Owner_reputation":473,
        "Owner_up_votes":26,
        "Owner_down_votes":0,
        "Owner_views":37,
        "Answer_body":"<p><a href=\"https:\/\/stackoverflow.com\/questions\/69194172\/how-to-reference-both-a-python-and-environment-variable-in-jupyter-bash-magic\">How to reference both a python and environment variable in jupyter bash magic?<\/a><\/p>\n<p>Try<\/p>\n<pre><code>!echo \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:$$PATH\n<\/code><\/pre>\n<p><code>$$PATH<\/code> forces it to use the system variable rather than try to find a Python\/local one.<\/p>\n<p>Various examples:<\/p>\n<pre><code>In [130]: foo = 'foo*.txt'\nIn [131]: HOME = 'myvar'\nIn [132]: !echo $foo\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt\nIn [133]: !echo $foo $HOME\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt myvar\nIn [134]: !echo $foo $$HOME\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt \/home\/paul\nIn [135]: !echo $foo $PWD\n\/home\/paul\/mypy\nIn [136]: !echo $foo $$PWD\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt \/home\/paul\/mypy\nIn [137]: !echo {foo} $PWD\n{foo} \/home\/paul\/mypy\nIn [138]: !echo {foo} $$PWD\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt \/home\/paul\/mypy\n<\/code><\/pre>\n<p>Any variable not locally defined forces the behavior you see:<\/p>\n<pre><code>In [139]: !echo $abc\n\nIn [140]: !echo {foo} $abc\n{foo}\n<\/code><\/pre>\n<p>It may put the substitution in a <code>try\/except<\/code> block, and &quot;give up&quot; if there's any <code>NameError<\/code>.<\/p>\n<p>This substitution can occur in most of the magics, not just <code>!<\/code>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-03-25 04:55:37.557 UTC",
        "Answer_score":3.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2022-03-25 07:08:28.5 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71611419",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56024351,
        "Question_title":"Beginners guide to Sagemaker",
        "Question_body":"<p>I have followed an Amazon tutorial for using SageMaker and have used it to create the model in the tutorial (<a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a>).<\/p>\n\n<p>This is my first time using SageMaker, so my question may be stupid.<\/p>\n\n<p>How do you actually view the model that it has created? I want to be able to see a) the final formula created with the parameters etc. b) graphs of plotted factors etc. as if I was reviewing a GLM for example.<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-07 14:00:04.32 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":748,
        "Owner_creation_date":"2019-04-04 17:09:23.22 UTC",
        "Owner_last_access_date":"2020-10-12 11:57:36.66 UTC",
        "Owner_reputation":327,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Answer_body":"<p>If you followed the SageMaker tutorial you must have trained an XGBoost model. SageMaker places the model artifacts in a bucket that you own, check the output S3 location in the AWS SageMaker console. <\/p>\n\n<p>For more information about XGBoost you can check the AWS SageMaker documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#xgboost-sample-notebooks\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#xgboost-sample-notebooks<\/a> and the example notebooks, e.g. <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone.ipynb<\/a><\/p>\n\n<p>To consume the XGBoost artifact generated by SageMaker, check out the official documentation, which contains the following code:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># SageMaker XGBoost uses the Python pickle module to serialize\/deserialize \n# the model, which can be used for saving\/loading the model.\n# To use a model trained with SageMaker XGBoost in open source XGBoost\n# Use the following Python code:\n\nimport pickle as pkl \nmodel = pkl.load(open(model_file_path, 'rb'))\n# prediction with test data\npred = model.predict(dtest)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-06-05 00:19:29.323 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56024351",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63114905,
        "Question_title":"Sagemaker tensorflow endpoint not calling the input_handler when being invoked for a prediction",
        "Question_body":"<p>I'm deploying a <code>tensorflow.serving<\/code> endpoint with a custom <code>inference.py<\/code> script via the <code>entry point<\/code> parameter<\/p>\n<pre><code>model = Model(role='xxx',\n              framework_version='2.2.0',\n              entry_point='inference.py',\n              model_data='xxx')\n\npredictor = model.deploy(instance_type='xxx',\n                         initial_instance_count=1,\n                         endpoint_name='xxx')\n<\/code><\/pre>\n<p>inference.py constains an <code>input_handler<\/code> and an <code>output_handler<\/code> functions, but when i call predict with:<\/p>\n<pre><code>model = Predictor(endpoint_name='xxx')\nurl = 'xxx'\n\ninput = {\n    'instances': [url]\n}\n\npredictions = model.predict(input)\n<\/code><\/pre>\n<p>I'm getting the following <code>error<\/code>:<\/p>\n<p><em>botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;{&quot;error&quot;: &quot;Failed to process element: 0 of 'instances' list. Error: Invalid argument: JSON Value: &quot;xxx&quot; Type: String is not of expected type: float&quot; }&quot;<\/em><\/p>\n<p>It seems the function is never calling the <code>input_handler<\/code> function in inference.py script. Do you know why this might be happening?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2020-07-27 12:01:33.46 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2020-07-28 10:24:44.943 UTC",
        "Question_score":2,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":474,
        "Owner_creation_date":"2014-06-14 14:25:34.69 UTC",
        "Owner_last_access_date":"2022-09-25 03:04:43.037 UTC",
        "Owner_reputation":5998,
        "Owner_up_votes":2638,
        "Owner_down_votes":56,
        "Owner_views":426,
        "Answer_body":"<p>Found the problem thanks to AWS support:<\/p>\n<p>I was creating an endpoint that already had an endpoint configuration with the same name and the new configuration wasn't being utilized.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-07-28 16:11:01.83 UTC",
        "Answer_score":0.0,
        "Owner_location":"Porto, Portugal",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63114905",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66692579,
        "Question_title":"AWS SageMaker: PermissionError: Access Denied - Reading data from S3 bucket",
        "Question_body":"<p>I am using AWS SageMaker. I already used it before and I had no problems reading data from an S3 bucket.\nSo, I set up a new notebook instance and id this:<\/p>\n<pre><code>from sagemaker import get_execution_role\nrole = get_execution_role()\n\nbucket='my-bucket'\n\ndata_key = 'myfile.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n\ndf = pd.read_csv(data_location)\n<\/code><\/pre>\n<p>What I got is this:<\/p>\n<pre><code>PermissionError: Access Denied\n<\/code><\/pre>\n<p>Note: I checked the IAM Roles and also the policies and it seems to me that I have all the necessary rights to access the S3 bucket (AmazonS3FullAccess etc. are granted). What is different from the situation before is that my data is encrypted. Is there something I have to set up besides the roles?<\/p>\n<p>Edit:<\/p>\n<p>The role I use consist of three policies. These are<\/p>\n<ul>\n<li>AmazonS3FullAccess<\/li>\n<li>AmazonSageMakerFullAccess<\/li>\n<\/ul>\n<p>and an Execution Role where I added kms:encrypt and kms:decrypt. It looks like this one:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;xyz&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:PutObject&quot;,\n                &quot;s3:GetObject&quot;,\n                &quot;s3:ListBucket&quot;,\n                &quot;s3:DeleteObject&quot;,\n                &quot;kms:Encrypt&quot;,\n                &quot;kms:Decrypt&quot;\n            ],\n            &quot;Resource&quot;: &quot;arn:aws:s3:::*&quot;\n        }\n    ]\n}\n<\/code><\/pre>\n<p>Is there something missing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_date":"2021-03-18 13:56:00.093 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-03-18 14:20:30.863 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|error-handling|amazon-sagemaker",
        "Question_view_count":1327,
        "Owner_creation_date":"2019-05-29 11:58:00.073 UTC",
        "Owner_last_access_date":"2022-09-23 09:10:17.557 UTC",
        "Owner_reputation":1166,
        "Owner_up_votes":555,
        "Owner_down_votes":1,
        "Owner_views":248,
        "Answer_body":"<p>You need to add (or modify) an IAM policy to grant access to the key the bucket uses for its encryption:<\/p>\n<pre><code>{\n  &quot;Sid&quot;: &quot;KMSAccess&quot;,\n  &quot;Action&quot;: [\n    &quot;kms:Decrypt&quot;\n  ],\n  &quot;Effect&quot;: &quot;Allow&quot;,\n  &quot;Resource&quot;: &quot;arn:aws:kms:example-region-1:123456789098:key\/111aa2bb-333c-4d44-5555-a111bb2c33dd&quot;\n}\n<\/code><\/pre>\n<p>Alternatively you can change the key policy of the KMS key directly to grant the sagemaker role access directly. <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/s3-bucket-access-default-encryption\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/s3-bucket-access-default-encryption\/<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-03-18 15:24:06.907 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66692579",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72266041,
        "Question_title":"Loading a spacy .tar.gz model artifact from s3 Sagemaker",
        "Question_body":"<p>I have a pretrained spacy model on a local folder that I can easily read with <code>m = spacy.load(&quot;path\/model\/&quot;)<\/code><\/p>\n<p>But now I have to upload it as a .tar.gz file to use as a Sagemaker model artifact.\nHow can I read this .tar.gz file?<\/p>\n<p>Ideally I want to read the unzipped folder from memory. Without extracting all to disk and then reading it again<\/p>\n<p>My question is almost a duplicate of this one <a href=\"https:\/\/stackoverflow.com\/questions\/49274650\/directly-load-spacy-model-from-packaged-tar-gz-file\">Directly load spacy model from packaged tar.gz file<\/a>. But the answers don't explain how to untar unzip the folder into memory<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-16 21:56:01.967 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"model|spacy|amazon-sagemaker|artifact|tar.gz",
        "Question_view_count":184,
        "Owner_creation_date":"2014-05-23 18:31:01.95 UTC",
        "Owner_last_access_date":"2022-09-21 00:06:48.06 UTC",
        "Owner_reputation":135,
        "Owner_up_votes":163,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":"<p>Turns out Sagemaker already decompress the <code>.tar.gz<\/code> file automatically.\nSo I can just read the folder exactly like before.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-05-17 16:02:38.077 UTC",
        "Answer_score":0.0,
        "Owner_location":"Rio de Janeiro, Brazil",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72266041",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73654460,
        "Question_title":"how to use sagemaker inside pyspark",
        "Question_body":"<p>I have a simple requirement, I need to run sagemaker prediction inside a spark job<\/p>\n<p>am trying to run the below<\/p>\n<pre><code>ENDPOINT_NAME = &quot;MY-ENDPOINT_NAME&quot;\nfrom sagemaker_pyspark import SageMakerModel\nfrom sagemaker_pyspark import EndpointCreationPolicy\nfrom sagemaker_pyspark.transformation.serializers import ProtobufRequestRowSerializer\nfrom sagemaker_pyspark.transformation.deserializers import ProtobufResponseRowDeserializer\n\nattachedModel = SageMakerModel(\n    existingEndpointName=ENDPOINT_NAME,\n    endpointCreationPolicy=EndpointCreationPolicy.DO_NOT_CREATE,\n    endpointInstanceType=None,  # Required\n    endpointInitialInstanceCount=None,  # Required\n    requestRowSerializer=ProtobufRequestRowSerializer(\n        featuresColumnName=&quot;featureCol&quot;\n    ),  # Optional: already default value\n    responseRowDeserializer= ProtobufResponseRowDeserializer(schema=ouput_schema),\n)\n\ntransformedData2 = attachedModel.transform(df)\ntransformedData2.show()\n<\/code><\/pre>\n<p>I get the following error <code>TypeError: 'JavaPackage' object is not callable<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-08 19:57:27.127 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|apache-spark|amazon-sagemaker",
        "Question_view_count":18,
        "Owner_creation_date":"2012-04-26 13:33:06.71 UTC",
        "Owner_last_access_date":"2022-09-25 00:14:52.287 UTC",
        "Owner_reputation":1972,
        "Owner_up_votes":701,
        "Owner_down_votes":11,
        "Owner_views":547,
        "Answer_body":"<p>this was solved by ...<\/p>\n<pre><code>classpath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars())\nconf = SparkConf() \\\n    .set(&quot;spark.driver.extraClassPath&quot;, classpath)\nsc = SparkContext(conf=conf)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-08 21:33:47.94 UTC",
        "Answer_score":0.0,
        "Owner_location":"Egypt",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73654460",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51968742,
        "Question_title":"AWS sagemaker invokeEndpoint model internal error",
        "Question_body":"<p>I am trying to send a request on a model on sagemaker using .NET. The code I am using is: <\/p>\n\n<pre><code>var data = File.ReadAllBytes(@\"C:\\path\\file.csv\");\nvar credentials = new Amazon.Runtime.BasicAWSCredentials(\"\",\"\");\nvar awsClient = new AmazonSageMakerRuntimeClient(credentials, RegionEndpoint.EUCentral1);\nvar request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest\n{\n    EndpointName = \"EndpointName\",\n    ContentType = \"text\/csv\",\n    Body = new MemoryStream(data),\n};\n\nvar response = awsClient.InvokeEndpoint(request);\nvar predictions = Encoding.UTF8.GetString(response.Body.ToArray());\n<\/code><\/pre>\n\n<p>the error that I am getting on <code>awsClient.InvokeEndpoint(request)<\/code><\/p>\n\n<p>is:<\/p>\n\n<blockquote>\n  <p>Amazon.SageMakerRuntime.Model.ModelErrorException: 'The service\n  returned an error with Error Code ModelError and HTTP Body:\n  {\"ErrorCode\":\"INTERNAL_FAILURE_FROM_MODEL\",\"LogStreamArn\":\"arn:aws:logs:eu-central-1:xxxxxxxx:log-group:\/aws\/sagemaker\/Endpoints\/myEndpoint\",\"Message\":\"Received\n  server error (500) from model with message \\\"\\\". See\n  \"https:\/\/ url_to_logs_on_amazon\"\n  in account xxxxxxxxxxx for more\n  information.\",\"OriginalMessage\":\"\",\"OriginalStatusCode\":500}'<\/p>\n<\/blockquote>\n\n<p>the url that the error message suggests for more information does not help at all.<\/p>\n\n<p>I believe that it is a data format issue but I was not able to find a solution.<\/p>\n\n<p>Does anyone has encountered this behavior before?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2018-08-22 14:08:26.973 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"c#|.net|amazon-web-services|amazon-sagemaker",
        "Question_view_count":4385,
        "Owner_creation_date":"2017-05-25 12:29:46.88 UTC",
        "Owner_last_access_date":"2020-06-17 08:32:20.743 UTC",
        "Owner_reputation":51,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":"<p>The problem relied on the data format as suspected. In my case all I had to do is send the data as a json serialized string array and use <code>ContentType = application\/json<\/code> because the python function running on the endpoint which is responsible for sending the data to the predictor was only accepting json strings. <\/p>\n\n<p>Another way to solve this issues is to modify the python function which is responsible for the input handling to accept all content types and modify the data in a way that the predictor will understand.<\/p>\n\n<p>example of working code for my case:<\/p>\n\n<pre><code>        var data = new string[] { \"this movie was extremely good .\", \"the plot was very boring .\" };\n        var serializedData = JsonConvert.SerializeObject(data);\n\n        var credentials = new Amazon.Runtime.BasicAWSCredentials(\"\",\"\");\n        var awsClient = new AmazonSageMakerRuntimeClient(credentials, RegionEndpoint.EUCentral1);\n        var request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest\n        {\n            EndpointName = \"endpoint\",\n            ContentType = \"application\/json\",\n            Body = new MemoryStream(Encoding.ASCII.GetBytes(serializedData)),\n        };\n\n        var response = awsClient.InvokeEndpoint(request);\n        var predictions = Encoding.UTF8.GetString(response.Body.ToArray());\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-08-24 10:02:05.6 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51968742",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50281188,
        "Question_title":"Sagemaker Java client generate IOrecord",
        "Question_body":"<p>I am trying to build a training set for Sagemaker using the Linear Learner algorithm. This algorithm supports recordIO wrapped protobuf and csv as format for the training data. As the training data is generated using spark I am having issues to generate a csv file from a dataframe (this seem broken for now), so I am trying to use protobuf. <\/p>\n\n<p>I managed to create a binary file for the training dataset using Protostuff which is a library that allows to generate protobuf messages from POJO objects. The problem is when triggering the training job I receive that message from SageMaker:\nClientError: No training data processed. Either the training channel is empty or the mini-batch size is too high. Verify that training data contains non-empty files and the mini-batch size is less than the number of records per training host.<\/p>\n\n<p>The training file is certainly not null. I suspect the way I generate the training data to be incorrect as I am able to train models using the libsvm format. Is there a way to generate IOrecord using the Sagemaker java client ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-05-10 20:39:10.483 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":222,
        "Owner_creation_date":"2015-04-08 00:54:56.053 UTC",
        "Owner_last_access_date":"2019-10-03 00:46:15.877 UTC",
        "Owner_reputation":35,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":"<p>Answering my own question. It was an issue in the algorithm configuration. I reduced mini batch size and it worked fine.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-05-18 14:17:53.37 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50281188",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67123040,
        "Question_title":"How to tell programmatically that an AWS Step Function execution has been completed?",
        "Question_body":"<p>I am triggering a Step Function execution via a Python cell in a SageMaker Notebook, like this:<\/p>\n<pre><code>state_machine_arn = 'arn:aws:states:us-west-1:1234567891:stateMachine:alexanderMyPackageStateMachineE3411O13-A1vQWERTP9q9'\nsfn = boto3.client('stepfunctions')\n..\nsfn.start_execution(**kwargs)  # Non Blocking Call\nrun_arn = response['executionArn']\nprint(f&quot;Started run {run_name}. ARN is {run_arn}.&quot;)\n<\/code><\/pre>\n<p>and then in order to check that the execution (which might take hours to complete depending on the input) has been completed, before I start doing some custom post-analysis on the results, I manually execute a cell with:<\/p>\n<pre><code>response = sfn.list_executions(\n    stateMachineArn=state_machine_arn,\n    maxResults=1\n)\nprint(response)\n<\/code><\/pre>\n<p>where I can see from the output the status of the execution, e.g. <code>'status': 'RUNNING'<\/code>.<\/p>\n<p>How can I automate this, i.e. trigger the Step Function and continue the execution on my post-analysis custom logic only after the execution has finished? Is there for example a blocking call to start the execution, or a callback method I could use?<\/p>\n<p>I can think of putting a sleep method, so that the Python Notebook cell would periodically call <code>list_executions()<\/code> and check the status, and only when the execution is completed, continue to rest of the code. I can statistically determine the sleep period, but I was wondering if there is a simpler\/more accurate way.<\/p>\n<hr \/>\n<p>PS: Related: <a href=\"https:\/\/stackoverflow.com\/questions\/46878423\/how-to-avoid-simultaneous-execution-in-aws-step-function\">How to avoid simultaneous execution in aws step function<\/a>, however I would like to avoid creating any new AWS resource, just for this, I would like to do everything from within the Notebook.<\/p>\n<p>PPS: I cannot make any change to <code>MyPackage<\/code> and the Step Function definition.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-04-16 09:53:03.027 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-04-16 14:57:49.787 UTC",
        "Question_score":4,
        "Question_tags":"python|amazon-web-services|asynchronous|amazon-sagemaker|aws-step-functions",
        "Question_view_count":1216,
        "Owner_creation_date":"2013-05-22 21:25:42.213 UTC",
        "Owner_last_access_date":"2022-09-21 16:01:33.95 UTC",
        "Owner_reputation":70285,
        "Owner_up_votes":7595,
        "Owner_down_votes":12100,
        "Owner_views":13121,
        "Answer_body":"<p>Based on the comments.<\/p>\n<p>If no new resources are to be created (no CloudWatch Event rules, lambda functions) nor any changes to existing Step Function are allowed, then <strong>pooling iteratively<\/strong> <code>list_executions<\/code> would be the best solution.<\/p>\n<p>AWS CLI and boto3 have implemented similar solutions (not for Step Functions), but for some other services. They are called <code>waiters<\/code> (e.g. <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/ec2.html#waiters\" rel=\"nofollow noreferrer\">ec2 waiters<\/a>). So basically you would have to create your own <strong>waiter for Step Function<\/strong>, as AWS does not provide one for that. AWS uses <strong>15 seconds<\/strong> sleep time from what I recall for its waiters.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-04-16 10:18:25.233 UTC",
        "Answer_score":2.0,
        "Owner_location":"London, UK",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67123040",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73781018,
        "Question_title":"How to pass region to the SKLearnProcessor - botocore.exceptions.NoRegionError: You must specify a region",
        "Question_body":"<p>I'm using the following code to do a sklearn transformation job in sagemaker:<\/p>\n<pre><code>region = boto3.session.Session().region_name\nrole = sagemaker.get_execution_role()\nsklearn_processor = SKLearnProcessor(\n    framework_version=&quot;1.0-1&quot;, role=role,\n    instance_type=&quot;ml.m5.xlarge&quot;, instance_count=1,\n    # sagemaker_session = Session()\n)\nout_path = os.path.join(bucket, prefix, f'test_transform\/data.csv')\nsklearn_processor.run(\n    code=&quot;preprocess.py&quot;,\n    inputs = [\n        ProcessingInput(source = 'my_package\/', destination = '\/opt\/ml\/processing\/input\/code\/my_package\/')\n    ],\n    outputs=[\n        ProcessingOutput(output_name=&quot;test_transform_data&quot;, \n                         source = '\/opt\/ml\/processing\/output\/test_transform',\n                         destination = out_path),\n    ],\n    arguments=[&quot;--time-slot-minutes&quot;, &quot;30min&quot;]\n)\n<\/code><\/pre>\n<p>Within the above code, it's running preprocess.py, and (within) preprocess.py loads the data from snowflake database using the credentials saved in aws secretsmanager:<\/p>\n<pre><code>region = boto3.Session().region_name\nsecrets_client = boto3.client(service_name='secretsmanager', region_name=region)\n<\/code><\/pre>\n<p>So here's where error happen: first line above returns region as None, so the the second line of code raises <code>botocore.exceptions.NoRegionError: You must specify a region<\/code><\/p>\n<p>In this case, how can I pass the region to SKLearnProcessor or is there any other way to make the code working within the processing job instance?<\/p>\n<p>FYI:\nthe source of input <code>'my_package\/'<\/code> is in the structure below to install packages and include py dependencies used in <code>preprocess.py<\/code><\/p>\n<pre><code>\u251c\u2500\u2500 my_package\n\u2502   \u251c\u2500\u2500 file1.py\n\u2502   \u251c\u2500\u2500 file2.py\n\u2502   \u2514\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 preprocess.py\n<\/code><\/pre>\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-20 02:27:02.13 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|containers|amazon-sagemaker",
        "Question_view_count":20,
        "Owner_creation_date":"2016-08-26 09:07:46.917 UTC",
        "Owner_last_access_date":"2022-09-25 04:38:07.563 UTC",
        "Owner_reputation":505,
        "Owner_up_votes":53,
        "Owner_down_votes":19,
        "Owner_views":52,
        "Answer_body":"<p>set following in the code preprocess.py solved the issue:<\/p>\n<pre><code>os.environ['AWS_DEFAULT_REGION'] = 'us-west-2' \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-20 12:53:03.9 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73781018",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63805114,
        "Question_title":"Uploading 1000s of files to AWS Notebook Instance",
        "Question_body":"<p>When I open AWS Notebook Instance-&gt; Jupyter Notebook. It gives me a storage (probably called an S3 bucket). I created a folder there and tried to upload 1000s of data. However, it asks me to manually click on the upload button next to every single file. Is it possible to upload that data much easier way?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-09 05:11:40.94 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":382,
        "Owner_creation_date":"2013-05-26 03:45:19.747 UTC",
        "Owner_last_access_date":"2022-09-22 01:29:51.95 UTC",
        "Owner_reputation":311,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Answer_body":"<p>You could use the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/userguide\/cli-services-s3-commands.html#using-s3-commands-managing-objects-move\" rel=\"nofollow noreferrer\">AWS-CLI<\/a> or the <a href=\"https:\/\/docs.aws.amazon.com\/AWSJavaScriptSDK\/latest\/AWS\/S3.html\" rel=\"nofollow noreferrer\">AWS-S3 SDK<\/a> (JS in this example).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-09-09 06:21:38.017 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63805114",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70632239,
        "Question_title":"cloud 9 and sagemaker - hyper parameter optimisation",
        "Question_body":"<p>I have done quite a few google searches but have not found a clear answer to the following use case. Basically, I would rather use cloud 9 (most of the time) as my IDE rather than Jupyter. What I am confused\/not sure about is, how I could executed long running jobs like (Bayesian) hyper parameter optimisation from there. Can I use Sagemaker capabilities? Should I use docker and deploy to ECR (looking for the cheapest-ish option)? Any pointers w.r.t. to this particular issue would be very much appreciated. Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-08 12:20:16.603 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-01-08 22:11:23.347 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|aws-cloud9",
        "Question_view_count":31,
        "Owner_creation_date":"2010-03-01 10:53:04.443 UTC",
        "Owner_last_access_date":"2022-09-24 18:56:19.313 UTC",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Answer_body":"<p>You could use whatever IDE you choose (including your laptop).<br \/>\nSaegMaker tuning job (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex.html\" rel=\"nofollow noreferrer\">example<\/a>) is <strong>asynchronous<\/strong>, so you can safely close your IDE after launching it. You can <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-monitor.html\" rel=\"nofollow noreferrer\">monitor the job the AWS web console,<\/a> or with a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeHyperParameterTuningJob.html\" rel=\"nofollow noreferrer\">DescribeHyperParameterTuningJob API call<\/a>.<\/p>\n<p>You can launch TensorFlow, PyTorch, XGBoost, Scikit-learn, and other <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/frameworks.html\" rel=\"nofollow noreferrer\">popular ML frameworks<\/a>, using one of the built-in framework containers, avoiding the extra work of bringing your own container.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-01-09 13:45:20.213 UTC",
        "Answer_score":1.0,
        "Owner_location":"Somewhere",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70632239",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67863816,
        "Question_title":"semantic content recommendation system with Amazon SageMaker, storing in S3",
        "Question_body":"<p>I am fairly new to AWS and Sagemaker and have decided to follow some of the tutorials Amazon has to familiarize myself with it. I've been following this one (<a href=\"https:\/\/aws.amazon.com\/getting-started\/hands-on\/semantic-content-recommendation-system-amazon-sagemaker\/5\/\" rel=\"nofollow noreferrer\">tutorial<\/a>) and I've realized that it's an older tutorial using Sagemaker v1. I've been able to look up and change whatever is needed for the tutorial to work in v2 but I became stuck at this part for storing the training data in a S3 bucket to deploy the model.<\/p>\n<pre><code>import io\nimport sagemaker.amazon.common as smac\n\nprint('train_features shape = ', predictions.shape)\nprint('train_labels shape = ', labels.shape)\nbuf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, predictions, labels)\nbuf.seek(0)\n\nbucket = BUCKET\nprefix = PREFIX\nkey = 'knn\/train'\nfname = os.path.join(prefix, key)\nprint(fname)\nboto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\ns3_train_data = 's3:\/\/{}\/{}\/{}'.format(bucket, prefix, key)\nprint('uploaded training data location: {}'.format(s3_train_data))\n<\/code><\/pre>\n<p>It returns this error<\/p>\n<pre><code>NameError Traceback (most recent call \nlast)\n&lt;ipython-input-20-9e52dd949332&gt; in &lt;module&gt;\n 3\n 4\n----&gt; 5 print('train_features shape = ', predictions.shape)\n 6 print('train_labels shape = ', labels.shape)\n 7 buf = io.BytesIO()\nNameError: name 'predictions' is not defined\n<\/code><\/pre>\n<p>I'm curious as to why this would have worked in Sagemaker v1 and not v2 if predictions is not defined and if anyone could point me in the right direction for correcting this.<\/p>\n<p>Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-06 21:22:45.19 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-06-07 04:41:25.867 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":76,
        "Owner_creation_date":"2021-06-06 21:09:27.55 UTC",
        "Owner_last_access_date":"2021-08-05 17:01:20.663 UTC",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>It looks like they've left some of the code out, or changed the terminology and left in predictions by accident. predictions is an object that is defined on this page <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-test-model.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-test-model.html<\/a><\/p>\n<p>You'll have to work out what predictions is in your case.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-06-07 02:39:17.14 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67863816",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73645084,
        "Question_title":"Create Hugging Face Transformers Tokenizer using Amazon SageMaker in a distributed way",
        "Question_body":"<p>I am using the SageMaker HuggingFace Processor to create a custom tokenizer on a large volume of text data.\nIs there a way to make this job data distributed - meaning read partitions of data across nodes and train the tokenizer leveraging multiple CPUs\/GPUs.<\/p>\n<p>At the moment, providing more nodes to the processing cluster merely replicates the tokenization process (basically duplicates the process of creation), which is redundant. You can primarily only scale vertically.<\/p>\n<p>Any insights into this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-08 07:17:04.647 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-09-08 09:02:38.727 UTC",
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|huggingface-transformers|huggingface-tokenizers",
        "Question_view_count":27,
        "Owner_creation_date":"2022-09-08 07:14:26.503 UTC",
        "Owner_last_access_date":"2022-09-23 21:03:19.637 UTC",
        "Owner_reputation":48,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>Considering the following example code for\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/processing-job-frameworks-hugging-face.html\" rel=\"nofollow noreferrer\">HuggingFaceProcessor<\/a>:<\/p>\n<p>If you have 100 large files in S3 and use a ProcessingInput with\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ProcessingS3Input.html#:%7E:text=S3DataDistributionType\" rel=\"nofollow noreferrer\">s3_data_distribution_type<\/a>=&quot;ShardedByS3Key&quot; (instead of FullyReplicated), the objects in your S3 prefix will be sharded and distributed to your instances.<\/p>\n<p>For example, if you have 100 large files and want to filter records from them using HuggingFace on 5 instances, the s3_data_distribution_type=&quot;ShardedByS3Key&quot; will put 20 objects on each instance, and each instance can read the files from its own path, filter out records, and write (uniquely named) files to the output paths, and SageMaker Processing will put the filtered files in S3.<\/p>\n<p>However, if your filtering criteria is stateful or depends on doing a full pass over the dataset first (such as: filtering outliers based on mean and standard deviation on a feature - in case of using SKLean Processor for example): you'll need to pass that information in to the job so each instance can know how to filter. To send information to the instances launched, you have to use the\u00a0<code>\/opt\/ml\/config\/resourceconfig.json<\/code>\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-your-own-processing-container.html#byoc-config\" rel=\"nofollow noreferrer\">file<\/a>:<\/p>\n<p><code>{ &quot;current_host&quot;: &quot;algo-1&quot;, &quot;hosts&quot;: [&quot;algo-1&quot;,&quot;algo-2&quot;,&quot;algo-3&quot;] }<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-08 16:08:29.533 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73645084",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51817494,
        "Question_title":"deploy a simple function to amazon sagemaker",
        "Question_body":"<p>I have been playing with Amazon Sagemaker. They have amazing sample notebooks in different areas. However, for testing purposes, I want to create an endpoint that returns the result from a function. From what I have seen so far, my understanding is that we can deploy only models but I would like to clarify it.<\/p>\n\n<p>Let's say I want to invoke the endpoint and it should give me the square of the input value. So, I will first create a function:<\/p>\n\n<pre><code>def my_square(x):\n    return x**2\n<\/code><\/pre>\n\n<p>Can we deploy this simple function in Amazon Sagemaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-08-13 07:45:57.657 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-08-15 21:26:19.667 UTC",
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":216,
        "Owner_creation_date":"2015-04-16 01:27:21.93 UTC",
        "Owner_last_access_date":"2022-09-21 00:22:16.033 UTC",
        "Owner_reputation":2282,
        "Owner_up_votes":123,
        "Owner_down_votes":10,
        "Owner_views":264,
        "Answer_body":"<p>Yes this is possible but it will need some overhead:\nYou can pass your own docker images for training and inference to sagemaker.<\/p>\n\n<p>Inside this containers you can do anything you want including return your <code>my_square<\/code> function. Keep in mind that you have to write your own flask microservice including proxy and wsgi server(if needed).<\/p>\n\n<p>In my opinion <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">this example<\/a> is the most helpfull one.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-08-13 09:39:42.207 UTC",
        "Answer_score":3.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51817494",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58194899,
        "Question_title":"AWS SageMaker SKLearn entry point in a subdirectory?",
        "Question_body":"<p>Can I specify SageMaker estimator's entry point script to be in a subdirectory? So far, it fails for me. Here is what I want to do:<\/p>\n\n<pre><code>sklearn = SKLearn(\n    entry_point=\"RandomForest\/my_script.py\",\n    source_dir=\"..\/\",\n    hyperparameters={...\n<\/code><\/pre>\n\n<p>I want to do this so I don't have to break my directory structure. I have some modules, which I use in several sagemaker projects, and each project lives in its own directory:<\/p>\n\n<pre><code>my_git_repo\/\n\n  RandomForest\/\n    my_script.py\n    my_sagemaker_notebook.ipynb\n\n  TensorFlow\/\n    my_script.py\n    my_other_sagemaker_notebook.ipynb\n\nmodule_imported_in_both_scripts.py\n<\/code><\/pre>\n\n<p>If I try to run this, SageMaker fails because it seems to parse the name of the entry point script to make a module name out of it, and it does not do a good job:<\/p>\n\n<pre><code>\/usr\/bin\/python3 -m RandomForest\/my_script --bootstrap True --case nf_2 --max_features 0.5 --min_impurity_decrease 5.323785009485933e-06 --model_name model --n_estimators 455 --oob_score True\n\n...\n\n\/usr\/bin\/python3: No module named RandomForest\/my_script\n\n<\/code><\/pre>\n\n<p>Anyone knows a way around this other than putting <code>my_script.py<\/code> in the <code>source_dir<\/code>?<\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/54314876\/aws-sagemaker-sklearn-entry-point-allow-multiple-script\">Related to this question<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-02 03:39:06.7 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|scikit-learn|amazon-sagemaker",
        "Question_view_count":843,
        "Owner_creation_date":"2013-05-08 18:53:12.833 UTC",
        "Owner_last_access_date":"2022-09-23 20:31:17.31 UTC",
        "Owner_reputation":1245,
        "Owner_up_votes":1495,
        "Owner_down_votes":0,
        "Owner_views":109,
        "Answer_body":"<p>Unfortunately, this is a gap in functionality. There is some related work in <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/pull\/941\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/pull\/941<\/a> which should also solve this issue, but for now, you do need to put <code>my_script.py<\/code> in <code>source_dir<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-10-24 17:57:09.097 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58194899",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57032981,
        "Question_title":"how long does converting to amazon protobuf record using .record_set() take to complete",
        "Question_body":"<p>I am trying to convert a numpy array to an amazon protobuf record using <code>sagemaker.amazon.amazon_estimator.AmazonAlgorithmEstimatorBase.record_set()<\/code> However, this is taking a really long time. <\/p>\n\n<p>I'm wondering how the function actually performs and how long it should take<\/p>\n\n<pre><code>from sagemaker import LinearLearner\nimport numpy as np\n\nmodel=LinearLearner(role=get_execution_role(),\n                             train_instance_count=len(train_features),\n                             train_instance_type='ml.t2.medium',\n                             predictor_type='binary_classifier',\n                                )\n<\/code><\/pre>\n\n<pre><code>numpy_array = np.array([[7.4727994e-01 9.5506465e-01 7.6940370e-01 8.2015032e-01 1.8113719e-01\n  7.8720862e-01 2.9677063e-01 2.6711187e-01 7.9498607e-01 4.4924998e-01\n  4.9533784e-01 2.6846960e-01 7.0506859e-01 4.1573554e-01 6.5843487e-01\n  3.2448095e-01 4.3870610e-01 7.2739214e-01 6.0914969e-01 5.5108833e-01\n  5.8835250e-01 5.5872935e-01 4.4392920e-01 6.8353373e-01 4.7664520e-01\n  5.6887656e-01 4.7034043e-01 4.1631639e-01 3.1357434e-01 5.5933639e-04]\n [5.7815754e-01 9.5828843e-01 7.7824914e-01 8.3188844e-01 2.3287645e-01\n  7.7196079e-01 2.5512937e-01 2.7032304e-01 7.8349811e-01 5.0130588e-01\n  4.8345023e-01 3.8397798e-01 5.9922373e-01 4.7720599e-01 6.7832541e-01\n  2.7788603e-01 4.6435007e-01 7.6100332e-01 7.7771670e-01 5.1536995e-01\n  5.8536130e-01 5.6407303e-01 5.0898582e-01 6.7815554e-01 3.0614817e-01\n  5.7353836e-01 3.8981739e-01 4.1474316e-01 3.1389123e-01 3.5031504e-04]]) \n<\/code><\/pre>\n\n<pre><code>record=model.record_set(numpy_array)\n<\/code><\/pre>\n\n<h2>Expected output<\/h2>\n\n<p>I expect the variable record to container a record ready for training with linearlearning model<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-15 03:36:03.097 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|linear-regression|amazon-sagemaker",
        "Question_view_count":119,
        "Owner_creation_date":"2018-01-07 10:20:53.68 UTC",
        "Owner_last_access_date":"2022-09-24 11:32:05.34 UTC",
        "Owner_reputation":1848,
        "Owner_up_votes":1616,
        "Owner_down_votes":1,
        "Owner_views":206,
        "Answer_body":"<p>I believe this is the problem:<\/p>\n\n<pre><code>train_instance_count=len(train_features)\n<\/code><\/pre>\n\n<p>This parameter is about infrastructure (how many SageMaker instances you want to train on), not about features. You should set it to 1.<\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker import LinearLearner\nimport numpy as np\n\nmodel=LinearLearner(role=sagemaker.get_execution_role(),\n                             train_instance_count=1,\n                             train_instance_type='ml.t2.medium',\n                             predictor_type='binary_classifier')\n\nnumpy_array = np.array(...)\n\nrecord=model.record_set(numpy_array)\n# This takes &lt;100 ms on my t3 notebook instance\n\nprint(record)\n\n(&lt;class 'sagemaker.amazon.amazon_estimator.RecordSet'&gt;, {'s3_data':\n's3:\/\/sagemaker-eu-west-1-123456789012\/sagemaker-record-sets\/LinearLearner-\n2019-07-18-09-48-21-639\/.amazon.manifest', 'feature_dim': 30, 'num_records': 2,\n's3_data_type': 'ManifestFile', 'channel': 'train'})\n<\/code><\/pre>\n\n<p>The manifest file lists the protobuf-encoded file(s):<\/p>\n\n<pre><code>[{\"prefix\": \"s3:\/\/sagemaker-eu-west-1-123456789012\/sagemaker-record-sets\/LinearLearner-2019-07-18-09-48-21-639\/\"}, \"matrix_0.pbr\"]\n<\/code><\/pre>\n\n<p>You can now use it for the training channel when you call fit(), re: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-07-18 10:01:56.137 UTC",
        "Answer_score":1.0,
        "Owner_location":"Bangkok",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57032981",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60163614,
        "Question_title":"What is correct input for mxnet's linear learner in AWS SageMaker?",
        "Question_body":"<p>I am trying to create a simple linear learner in AWS SageMaker with MXNet. I have never worked with SageMaker or MXNet previously. Fitting the model gives runtime error as follows and shuts the instance:<\/p>\n\n<blockquote>\n  <p>UnexpectedStatusException: Error for Training job\n  linear-learner-2020-02-11-06-13-22-712: Failed. Reason: ClientError:\n  Unable to read data channel 'train'. Requested content-type is\n  'application\/x-recordio-protobuf'. Please verify the data matches the\n  requested content-type. (caused by MXNetError)<\/p>\n<\/blockquote>\n\n<p>I think that the data should be converted to protobuf format before passing as training data. Could someone please explain to me what is the correct format for MXNet models? What is the best way to convert a simple data frame into protobuf?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-11 07:08:26.08 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker|mxnet",
        "Question_view_count":617,
        "Owner_creation_date":"2017-03-30 09:42:34.4 UTC",
        "Owner_last_access_date":"2022-09-24 11:15:23.747 UTC",
        "Owner_reputation":834,
        "Owner_up_votes":118,
        "Owner_down_votes":11,
        "Owner_views":91,
        "Answer_body":"<p><a href=\"https:\/\/github.com\/awslabs\/fraud-detection-using-machine-learning\/blob\/master\/source\/notebooks\/sagemaker_fraud_detection.ipynb\" rel=\"nofollow noreferrer\">This end-to-end demo<\/a> shows usage of Linear Learner from input data pre-processed in <code>pandas<\/code> dataframes and then converted to protobuf using the SDK. But note that:<\/p>\n\n<ul>\n<li>There is no need to use protobuf, you can also pass csv data with the target variable on the first column of the files, as <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner.html#ll-input_output\" rel=\"nofollow noreferrer\">indicated here<\/a>.<\/li>\n<li>There is no need to know MXNet in order to use the SageMaker Linear Learner, just use the SDK of your choice, bring data to S3, and orchestrate training and inference :)<\/li>\n<\/ul>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-02-11 09:00:05.43 UTC",
        "Answer_score":2.0,
        "Owner_location":"Ahmedabad, Gujarat, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60163614",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60156370,
        "Question_title":"how SageMaker to access s3 bucket data",
        "Question_body":"<p>I was using the code <code>pd.read_json('s3:\/\/example2020\/kaggle.json')<\/code> to access S3 bucket data, but it threw the error of <code>FileNotFoundError: example2020\/kaggle.json<\/code>. <\/p>\n\n<p>The methods I tried:<\/p>\n\n<p><strong>[Region]<\/strong>\nThe s3 bucket is in Ohio region while the SageMaker notebook instance is in Singapore. Not sure if this matters. I tried to recreate a s3 bucket in Singapore region but I still cannot access it and got the same file not found error. <\/p>\n\n<p><strong>[IAM Role]<\/strong>\nI checked the permission of IAM-SageMaker Execution role\n<a href=\"https:\/\/i.stack.imgur.com\/st4AR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/st4AR.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-10 18:31:36.75 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-s3|amazon-sagemaker",
        "Question_view_count":1386,
        "Owner_creation_date":"2018-02-13 14:44:57.007 UTC",
        "Owner_last_access_date":"2022-09-23 01:39:29.63 UTC",
        "Owner_reputation":896,
        "Owner_up_votes":1273,
        "Owner_down_votes":3,
        "Owner_views":177,
        "Answer_body":"<p>The problem is still IAM permission. <\/p>\n\n<p>I created a new notebook instance and a new IAM role. You would be asked how to access s3 bucket. I chose <code>all s3 bucket<\/code>. Then the problem solved. \n<a href=\"https:\/\/i.stack.imgur.com\/B0qOO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/B0qOO.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><br>\n<br>\n<strong>[Solution]<\/strong>\nIn Resource tab, check whether bucket name is general.\n <a href=\"https:\/\/i.stack.imgur.com\/LL6Fw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LL6Fw.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>If you changed old IAM and it is not working, you can create a new IAM role. And attach this role to the notebook.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-02-10 18:31:36.75 UTC",
        "Answer_score":2.0,
        "Owner_location":"Australia",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60156370",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60801292,
        "Question_title":"View Neptune Graph Schema using Jupyter notebook",
        "Question_body":"<p>Is there a way to view the schema of a graph in a Neptune cluster using Jupyter Notebook? <\/p>\n\n<p>Like you would do a \"select * from tablename limit 10\" in an RDS using SQL, similarly is there a way to get a sense of the graph data through Jupyter Notebook?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2020-03-22 15:36:34.693 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2021-11-23 23:00:19.89 UTC",
        "Question_score":2,
        "Question_tags":"gremlin|amazon-sagemaker|amazon-neptune|gremlinpython|graph-notebook",
        "Question_view_count":831,
        "Owner_creation_date":"2020-03-22 15:31:06.787 UTC",
        "Owner_last_access_date":"2022-09-17 07:01:39.723 UTC",
        "Owner_reputation":99,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":"<p>It depends on how large your graph is as to how well this will perform but you can get a sense of the type of nodes and edges you have using something like the example below. From the tags you used I assume you are using Gremlin:<\/p>\n\n<pre><code>g.V().groupCount().by(label)\ng.E().groupCount().by(label)\n<\/code><\/pre>\n\n<p>If you have a very large graph try putting something like <code>limit(100000)<\/code> before the <code>groupCount<\/code> step.<\/p>\n\n<p>If you are using a programming language like Python (with gremlin python installed) then you will need to add a <code>next()<\/code> terminal step to the queries as in:<\/p>\n\n<pre><code>g.V().groupCount().by(label).next()\ng.E().groupCount().by(label).next()\n<\/code><\/pre>\n\n<p>Having found the labels and distribution of the labels you could use one of them to explore some properties. Let's imagine there is a label called \"person\".<\/p>\n\n<pre><code>g.V().hasLabel('person').limit(10).valueMap().toList()\n<\/code><\/pre>\n\n<p>Remember with Gremlin property graphs vertices with the same label may not necessarily have all the same properties so it's good to look at more than one vertex to get a sense for that as well.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-03-22 18:40:12.783 UTC",
        "Answer_score":4.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60801292",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56497428,
        "Question_title":"Use images in s3 with SageMaker without .lst files",
        "Question_body":"<p>I am trying to create (what I thought was) a simple image classification pipeline between s3 and SageMaker.<\/p>\n\n<p>Images are stored in an s3 bucket with their class labels in their file names currently, e.g.<\/p>\n\n<p><strong>My-s3-bucket-dir<\/strong><\/p>\n\n<pre><code>cat-1.jpg\ndog-1.jpg\ncat-2.jpg\n..\n<\/code><\/pre>\n\n<p>I've been trying to leverage several related example .py scripts, but most seem to be download data sets already in .rec format or containing special manifest or annotation files I don't have.<\/p>\n\n<p>All I want is to pass the images from s3 to the SageMaker image classification algorithm that's located in the same region, IAM account, etc. I suppose this means I need a <code>.lst<\/code> file<\/p>\n\n<p>When I try to manually create the <code>.lst<\/code> it doesn't seem to like it and it also takes too long doing manual work to be a good practice.<\/p>\n\n<p>How can I automatically generate the <code>.lst<\/code> file (or otherwise send the images\/classes for training)? <\/p>\n\n<p>Things I read made it sound like <code>im2rec.py<\/code> was a solution, but I don't see how. The example I'm working with now is <\/p>\n\n<p><code>Image-classification-fulltraining-highlevel.ipynb<\/code><\/p>\n\n<p>but it seems to download the data as <code>.rec<\/code>, <\/p>\n\n<pre><code>download('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-train.rec')\ndownload('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-val.rec')\n<\/code><\/pre>\n\n<p>which just skips working with the .jpeg files. I found another that converts them to <code>.rec<\/code> but again it has essentially the <code>.lst<\/code> already as <code>.json<\/code> and just converts it.<\/p>\n\n<p>I have mostly been working in a Python Jupyter notebook within the AWS console (in my browser) but I have also tried using their GUI. <\/p>\n\n<p>How can I simply and automatically generate the <code>.lst<\/code> or otherwise get the data\/class info into SageMaker without manually creating a <code>.lst<\/code> file?<\/p>\n\n<p><strong><em>Update<\/em><\/strong><\/p>\n\n<p>It looks like im2py can't be run against s3. You'd have to completely download everything from all s3 buckets into the notebook's storage...<\/p>\n\n<blockquote>\n  <p>Please note that [...] im2rec.py is running locally,\n  therefore cannot take input from the S3 bucket. To generate the list\n  file, you need to download the data and then use the im2rec tool. - AWS SageMaker Team<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-06-07 15:37:31.153 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-06-10 15:50:49.817 UTC",
        "Question_score":2,
        "Question_tags":"python-3.x|amazon-s3|computer-vision|amazon-sagemaker",
        "Question_view_count":937,
        "Owner_creation_date":"2014-05-05 14:48:58.467 UTC",
        "Owner_last_access_date":"2022-09-24 18:32:51.257 UTC",
        "Owner_reputation":21413,
        "Owner_up_votes":2163,
        "Owner_down_votes":516,
        "Owner_views":6465,
        "Answer_body":"<p>There are 3 options to provide annotated data to the Image Classification algo: (1) packing labels in recordIO files, (2) storing labels in a JSON manifest file (\"augmented manifest\" option), (3) storing labels in a list file. All options are documented here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html<\/a>.<\/p>\n\n<p>Augmented Manifest and .lst files option are quick to do since they just require you to create an annotation file with a usually quick <code>for<\/code> loop for example. RecordIO requires you to use <code>im2rec.py<\/code> tool, which is a little more work.<\/p>\n\n<p>Using .lst files is <strong>another option<\/strong> that is reasonably easy: you just need to create annotation them with a quick for loop, like this:<\/p>\n\n<pre><code># assuming train_index, train_class, train_pics store the pic index, class and path\n\nwith open('train.lst', 'a') as file:\n    for index, cl, pic in zip(train_index, train_class, train_pics):\n        file.write(str(index) + '\\t' + str(cl) + '\\t' + pic + '\\n')\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-06-07 22:58:18.333 UTC",
        "Answer_score":2.0,
        "Owner_location":"Columbia, MD, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56497428",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58177548,
        "Question_title":"SageMaker Managed Spot Training with Object Detection algorithm",
        "Question_body":"<p>I'm trying to train an object detection model starting from an existing model using the new Managed Spot Training feature,  The paramters used when creating my Estimator are as follows:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>od_model = sagemaker.estimator.Estimator(get_image_uri(sagemaker.Session().boto_region_name, 'object-detection', repo_version=\"latest\"),\n                                         Config['role'],\n                                         train_instance_count = 1,\n                                         train_instance_type = 'ml.p3.16xlarge',\n                                         train_volume_size = 50,\n                                         train_max_run = (48 * 60 * 60),\n                                         train_use_spot_instances = True,\n                                         train_max_wait = (72 * 60 * 60),\n                                         input_mode = 'File',\n                                         checkpoint_s3_uri = Config['train_checkpoint_uri'],\n                                         output_path = Config['s3_output_location'],\n                                         sagemaker_session = sagemaker.Session()\n                                         )\n<\/code><\/pre>\n\n<p>(The references to <code>Config<\/code> in the above are a config data structure I'm using to extract\/centralise some parameters)<\/p>\n\n<p>When I run the above, I get the following exception:<\/p>\n\n<blockquote>\n  <p>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: MaxWaitTimeInSeconds above 3600 is not supported for the given algorithm.<\/p>\n<\/blockquote>\n\n<p>If I change <code>train_max_wait<\/code> to 3600 I get this exception instead:<\/p>\n\n<blockquote>\n  <p>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: Invalid MaxWaitTimeInSeconds. It must be present and be greater than or equal to MaxRuntimeInSeconds<\/p>\n<\/blockquote>\n\n<p>However changing <code>max_run_time<\/code> to 3600 or less isn't going to work for me as I expect this model to take several days to train (large data set), in fact a single epoch takes more than an hour.<\/p>\n\n<p>The <a href=\"https:\/\/aws.amazon.com\/blogs\/aws\/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs\/\" rel=\"nofollow noreferrer\">AWS blog post on Managed Spot Training<\/a> say that <code>MaxWaitTimeInSeconds<\/code> is limited to an 60 minutes for:<\/p>\n\n<blockquote>\n  <p>For built-in algorithms and AWS Marketplace algorithms that don\u2019t use checkpointing, we\u2019re enforcing a maximum training time of 60 minutes (MaxWaitTimeInSeconds parameter).<\/p>\n<\/blockquote>\n\n<p>Earlier, the same blog post says:<\/p>\n\n<blockquote>\n  <p>Built-in algorithms: computer vision algorithms support checkpointing (Object Detection, Semantic Segmentation, and very soon Image Classification).<\/p>\n<\/blockquote>\n\n<p>So I don't think it's that my algorithm doesn't support Checkpointing.  In fact that blog post uses object detection and max run times of 48 hours.  So I don't think it's an algorithm limitation.<\/p>\n\n<p>As you can see above, I've set up a S3 URL for the checkpoints.  The S3 bucket does exist, and the training container has access to it (it's the same bucket that the training data and model outputs are placed, and I had no problems with access to those before turning on spot training.<\/p>\n\n<p>My boto and sagemaker libraries are current versions:<\/p>\n\n<pre><code>boto3 (1.9.239)\nbotocore (1.12.239)\nsagemaker (1.42.3)\n<\/code><\/pre>\n\n<p>As best I can tell from reading various docs, I've got everything set up correctly.  My use case is almost exactly what's described in the blog post linked above, but I'm using the SageMaker Python SDK instead of the console.<\/p>\n\n<p>I'd really like to try Managed Spot Training to save some money, as I have a very long training run coming up.  But limiting timeouts to an hour isn't going to work for my use case.  Any suggestions?<\/p>\n\n<p><strong>Update:<\/strong>  If I comment out the <code>train_use_spot_instances<\/code> and <code>train_max_wait<\/code> options to train on regular on-demand instances my training job is created successfully.  If I then try to use the console to clone the job and turn on Spot instances on the clone I get the same ValidationException.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-01 02:59:26.77 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2019-10-01 03:26:23.487 UTC",
        "Question_score":4,
        "Question_tags":"python|amazon-web-services|object-detection|amazon-sagemaker",
        "Question_view_count":1234,
        "Owner_creation_date":"2008-11-18 05:09:29.4 UTC",
        "Owner_last_access_date":"2022-09-21 04:09:53.41 UTC",
        "Owner_reputation":5789,
        "Owner_up_votes":532,
        "Owner_down_votes":9,
        "Owner_views":464,
        "Answer_body":"<p>I ran my script again today and it worked fine, no <code>botocore.exceptions.ClientError<\/code> exceptions.  Given that this issue affected both the Python SDK for Sagemaker and the console, I suspect it might have been an issue with the backend API and not my client code.<\/p>\n<p>Either way, it's working now.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-10-07 23:51:14.473 UTC",
        "Answer_score":1.0,
        "Owner_location":"Adelaide, Australia",
        "Answer_last_edit_date":"2022-03-03 01:41:35.95 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58177548",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59801874,
        "Question_title":"When do you specify the Target variable in a SageMaker Training job?",
        "Question_body":"<p>I'm trying to create a Machine Learning algorithm following this tutorial : <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-console.html\" rel=\"nofollow noreferrer\">Get Started with Amazon SageMaker<\/a><\/p>\n\n<p>Unless I missed something in the tutorial, I didn't find any steps where we specify the target variable. Can someone explain where \/ when we specify our target variable when creating an ML model using SageMaker built-in algorithms? <\/p>\n\n<p>Thanks a lot! <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2020-01-18 15:14:40.36 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":715,
        "Owner_creation_date":"2019-07-02 08:23:28.543 UTC",
        "Owner_last_access_date":"2022-09-14 07:41:26.633 UTC",
        "Owner_reputation":895,
        "Owner_up_votes":29,
        "Owner_down_votes":1,
        "Owner_views":53,
        "Answer_body":"<p>It depends on the scientific paradigm you're using in SageMaker :)<\/p>\n\n<ul>\n<li>SageMaker Built-in algorithms all have their input specification,\ndescribed in their respective documentation. For example, for\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner.html#ll-input_output\" rel=\"nofollow noreferrer\">SageMaker Linear Learner<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#InputOutput-XGBoost\" rel=\"nofollow noreferrer\">SageMaker XGBoost<\/a> the target is assumed\nto be the first column.<\/li>\n<li>With custom code, such as Bring-Your-Own-Docker or SageMaker Framework containers (for Sklearn, TF, PyTorch, MXNet) since you are the one writing the code you can write any sort of logic, and the target can be any column of your dataset.<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-01-20 09:33:25.583 UTC",
        "Answer_score":4.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59801874",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62007961,
        "Question_title":"Where does entry_point script is stored in custom Sagemaker Framework training job container?",
        "Question_body":"<p>I am trying to create my own custom Sagemaker Framework that runs a custom python script to train a ML model using the entry_point parameter.<\/p>\n\n<p>Following the Python SDK documentation (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html<\/a>), I wrote the simplest code to run a training job just to see how it behaves and how Sagemaker Framework works.<\/p>\n\n<p>My problem is that I don't know how to properly build my Docker container in order to run the entry_point script.<\/p>\n\n<p>I added the <code>train.py<\/code> script into the container that only logs the folders and files paths as well as the variables in the containers environment.<\/p>\n\n<p>I was able to run the training job, but I couldn't find any reference of the entry_point script neither in environment variable nor the files in the container.<\/p>\n\n<p>Here is the code I used:<\/p>\n\n<ul>\n<li><strong>Custom Sagemaker Framework Class:<\/strong><\/li>\n<\/ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.estimator import Framework\n\nclass Doc2VecEstimator(Framework):\n    def create_model():\n        pass\n<\/code><\/pre>\n\n<ul>\n<li><strong>train.py:<\/strong><\/li>\n<\/ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>import argparse\nimport os\nfrom datetime import datetime\n\n\ndef log(*_args):\n    print('[log-{}]'.format(datetime.now().isoformat()), *_args)\n\n\ndef listdir_rec(path):\n    ls = os.listdir(path)\n    print(path, ls)\n\n    for ls_path in ls:\n        if os.path.isdir(os.path.join(path, ls_path)):\n            listdir_rec(os.path.join(path, ls_path))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--epochs', type=int, default=5)\n    parser.add_argument('--debug_size', type=int, default=None)\n\n    # # I commented the lines bellow since I haven't configured the environment variables in my container\n    #     # Sagemaker specific arguments. Defaults are set in the environment variables.\n    #     parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n    #     parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    #     parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n\n    args, _ = parser.parse_known_args()\n\n    log('Received arguments {}'.format(args))\n\n    log(os.environ)\n\n    listdir_rec('.')\n\n<\/code><\/pre>\n\n<ul>\n<li><strong>Dockerfile:<\/strong><\/li>\n<\/ul>\n\n<pre class=\"lang-sh prettyprint-override\"><code>FROM ubuntu:18.04\n\nRUN apt-get -y update \\\n    &amp;&amp; \\\n    apt-get install -y --no-install-recommends \\\n        wget \\\n        python3 \\\n        python3-pip \\\n        nginx \\\n        ca-certificates \\\n    &amp;&amp; \\\n    rm -rf \/var\/lib\/apt\/lists\/*\n\nRUN pip3 install --upgrade pip setuptools \\\n    &amp;&amp; \\\n    pip3 install \\\n        numpy \\\n        scipy \\\n        scikit-learn \\\n        pandas \\\n        flask \\\n        gevent \\\n        gunicorn \\\n        joblib \\\n        pyAthena \\\n        pandarallel \\\n        nltk \\\n        gensim \\\n    &amp;&amp; \\\n    rm -rf \/root\/.cache\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\n\nCOPY train.py \/train.py\n\nENTRYPOINT [\"python3\", \"-u\", \"train.py\"]\n<\/code><\/pre>\n\n<ul>\n<li><strong>Training Job Execution Script:<\/strong><\/li>\n<\/ul>\n\n<pre><code>framework = Doc2VecEstimator(\n    image_name=image,\n    entry_point='train_doc2vec_model.py',\n    output_path='s3:\/\/{bucket_prefix}'.format(bucket_prefix=bucket_prefix),\n\n    train_instance_count=1,\n    train_instance_type='ml.m5.xlarge',\n    train_volume_size=5,\n\n    role=role,\n    sagemaker_session=sagemaker_session,\n    base_job_name='gensim-doc2vec-train-100-epochs-test',\n\n    hyperparameters={\n        'epochs': '100',\n        'debug_size': '100',\n    },\n)\n\nframework.fit(s3_input_data_path, wait=True)\n<\/code><\/pre>\n\n<p>I haven't found a way to make the training job to run the <code>train_doc2vec_model.py<\/code>. So how do I create my own custom Framework class\/container?<\/p>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-05-25 17:52:12.65 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-05-25 20:07:41.657 UTC",
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|machine-learning|dockerfile|amazon-sagemaker",
        "Question_view_count":747,
        "Owner_creation_date":"2020-04-23 13:34:34.873 UTC",
        "Owner_last_access_date":"2022-04-27 17:55:09.487 UTC",
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":"<p>SageMaker team created a <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\" rel=\"nofollow noreferrer\">python package <code>sagemaker-training<\/code><\/a> to install in your docker so that your customer container will be able to handle external <code>entry_point<\/code> scripts.\nSee here for an example using Catboost that does what you want to do :)<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/sagemaker-byo-catboost-container-demo\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-byo-catboost-container-demo<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-05-25 19:39:18.777 UTC",
        "Answer_score":3.0,
        "Owner_location":"S\u00e3o Paulo, SP, Brasil",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62007961",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72097417,
        "Question_title":"Segfault using htop on AWS Sagemaker pytorch-1.10-cpu-py38 app",
        "Question_body":"<p>I am trying to launch the htop command in the Pytorch 1.10 - Python 3.8 CPU optimized AWS Sagemaker container. This works fine in other images I have used till now, but in this one, the command fails with a segfault:<\/p>\n<pre><code>htop \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nSegmentation fault (core dumped)\n<\/code><\/pre>\n<p>More info :<\/p>\n<pre><code>htop --version\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop 2.2.0 - (C) 2004-2019 Hisham Muhammad\nReleased under the GNU GPL.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-03 09:33:22.183 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|conda|amazon-sagemaker|htop",
        "Question_view_count":147,
        "Owner_creation_date":"2019-03-01 09:06:03.127 UTC",
        "Owner_last_access_date":"2022-08-11 13:18:40.803 UTC",
        "Owner_reputation":494,
        "Owner_up_votes":133,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Answer_body":"<p>I fixed this with<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code># Note: add sudo if needed:\nln -fs \/lib\/x86_64-linux-gnu\/libncursesw.so.6 \/opt\/conda\/lib\/libncursesw.so.6\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-17 09:56:35.693 UTC",
        "Answer_score":1.0,
        "Owner_location":"Paris, France",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72097417",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63128111,
        "Question_title":"Sagemaker Object2Vec training samples per second",
        "Question_body":"<p>I am using Sagemaker Object2Vec to train on data of size 2GB.<\/p>\n<p>ml.p2.xlarge instance took 12 hours to train the data on 4 epochs going at the speed of 5000 samples\/sec.<\/p>\n<p>Now, I am using a higher level instance ml.p2.16xlarge and it only trains at 400 samples\/sec with this in the logs<\/p>\n<pre><code>[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:739: only 114 out of 240 GPU pairs are enabled direct access. It may affect the performance. You can set MXNET_ENABLE_GPU_P2P=0 to turn it off\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .vvvvvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: v.vvvvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vv.vvvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvv.vvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvv.vvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvv.vvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvvv.vv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvvvv.v.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvvvvv........\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: ..........vvvvvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........v.vvvvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vv.vvvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvv.vvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvvv.vv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvvvv.v\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvvvvv.\n<\/code><\/pre>\n<p>There are about 50 million samples.<\/p>\n<p>What can I do to correct this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-28 06:18:24.483 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":59,
        "Owner_creation_date":"2017-08-30 20:07:37.673 UTC",
        "Owner_last_access_date":"2021-06-03 23:26:03.367 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>2 ideas:<\/p>\n<ol>\n<li>Before increasing the GPU count, grow batch size so that a single\nGPU is as busy as possible<\/li>\n<li>Use P3 instances instead of P2. P3 is more recent, has more memory, more CUDA cores, faster memory bandwidth and NVLink inter-GPU connections. Though it's more\nexpensive by hour, your total training cost may be much smaller if\nproperly tuned<\/li>\n<\/ol>\n<p>Also, if your problem involves sparse updates, meaning if just a small fraction of all tokens appear in a given mini-batch, you can try using <code>token_embedding_storage_type = 'row_sparse'<\/code>, which I think refers to using sparse gradient updates like described here <a href=\"https:\/\/medium.com\/apache-mxnet\/learning-embeddings-for-music-recommendation-with-mxnets-sparse-api-5698f4d7d8\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/apache-mxnet\/learning-embeddings-for-music-recommendation-with-mxnets-sparse-api-5698f4d7d8<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-07-28 11:58:39.41 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63128111",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53182436,
        "Question_title":"SageMaker: visualizing training statistics",
        "Question_body":"<p>If I send a TensorFlow training job to a SageMaker instance, what is the typical way to view training progress? Can I access TensorBoard for this launched EC2 instance? Is there some other alternative? What I'm looking for specifically are things like graphs of current training epoch and mAP.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-11-07 01:18:04.34 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow|data-visualization|tensorboard|amazon-sagemaker",
        "Question_view_count":415,
        "Owner_creation_date":"2013-02-20 05:47:52.693 UTC",
        "Owner_last_access_date":"2022-09-23 20:45:28.4 UTC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Answer_body":"<p>you can now specify metrics(metricName, Regex) that you want to track by using AWS management console or Amazon SageMaker Python SDK APIs. After the model training starts, Amazon SageMaker will automatically monitor and stream the specified metrics in real time to the Amazon CloudWatch console for visualizing time-series curves. <\/p>\n\n<p>Ref: \n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_MetricDefinition.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_MetricDefinition.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-11-23 22:07:36.183 UTC",
        "Answer_score":3.0,
        "Owner_location":"NYC",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53182436",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72419908,
        "Question_title":"How to match input\/output with sagemaker batch transform?",
        "Question_body":"<p>I'm using sagemaker batch transform, with json input files. see below for sample input\/output files. i have custom inference code below, and i'm using json.dumps to return prediction, but it's not returning json. I tried to use =&gt;    &quot;DataProcessing&quot;: {&quot;JoinSource&quot;: &quot;string&quot;,  }, to match input and output. but i'm getting error that &quot;unable to marshall ...&quot; . I think because , the output_fn is returning array of list or just list and not json , that is why it is unable to match input with output.any suggestions on how should i return the data?<\/p>\n<p>infernce code<\/p>\n<pre><code>def model_fn(model_dir):\n...\ndef input_fn(data, content_type):\n...\ndef predict_fn(data, model):\n...\ndef output_fn(prediction, accept):\n    if accept == &quot;application\/json&quot;:\n        return json.dumps(prediction), mimetype=accept)\n    raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))\n<\/code><\/pre>\n<p>input file<\/p>\n<pre><code>{&quot;data&quot; : &quot;input line  one&quot; }\n{&quot;data&quot; : &quot;input line  two&quot; }\n....\n<\/code><\/pre>\n<p>output file<\/p>\n<pre><code>[&quot;output line  one&quot; ]\n[&quot;output line  two&quot; ]\n<\/code><\/pre>\n<pre><code>{\n   &quot;BatchStrategy&quot;: SingleRecord,\n   &quot;DataProcessing&quot;: { \n      &quot;JoinSource&quot;: &quot;string&quot;,\n   },\n   &quot;MaxConcurrentTransforms&quot;: 3,\n   &quot;MaxPayloadInMB&quot;: 6,\n   &quot;ModelClientConfig&quot;: { \n      &quot;InvocationsMaxRetries&quot;: 1,\n      &quot;InvocationsTimeoutInSeconds&quot;: 3600\n   },\n   &quot;ModelName&quot;: &quot;some-model&quot;,\n   &quot;TransformInput&quot;: { \n      &quot;ContentType&quot;: &quot;string&quot;,\n      &quot;DataSource&quot;: { \n         &quot;S3DataSource&quot;: { \n            &quot;S3DataType&quot;: &quot;string&quot;,\n            &quot;S3Uri&quot;: &quot;s3:\/\/bucket-sample&quot;\n         }\n      },\n      &quot;SplitType&quot;: &quot;Line&quot;\n   },\n   &quot;TransformJobName&quot;: &quot;transform-job&quot;\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-28 23:26:47.103 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":358,
        "Owner_creation_date":"2020-05-30 00:10:41.983 UTC",
        "Owner_last_access_date":"2022-09-24 19:51:20.543 UTC",
        "Owner_reputation":525,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":98,
        "Answer_body":"<p><code>json.dumps<\/code> will not convert your array to a dict structure and serialize it to a JSON String.<\/p>\n<p>What data type is <code>prediction<\/code> ? Have you tested making sure <code>prediction<\/code> is a dict?<\/p>\n<p>You can confirm the data type by adding <code>print(type(prediction))<\/code> to see the data type in the CloudWatch Logs.<\/p>\n<p>If prediction is a <code>list<\/code> you can test the following:<\/p>\n<pre><code>def output_fn(prediction, accept):\n    if accept == &quot;application\/json&quot;:\n\n        my_dict = {'output': prediction}\n        return json.dumps(my_dict), mimetype=accept)\n\n    raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))\n<\/code><\/pre>\n<p><code>DataProcessing<\/code> and <code>JoinSource<\/code> are used to associate the data that is relevant to the prediction results in the output. It is not meant to be used to match the input and output format.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-06-02 00:17:48.707 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72419908",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61191412,
        "Question_title":"Unable to install toc2 notebook extension for AWS Sagemaker Instance (Lifecycle Configurations)",
        "Question_body":"<p>There's probably something very obvious I'm missing or Sagemaker just doesn't support these kinds of extensions, but I've been trying to enable toc2 (Table of Contents) jupyter extension for my Sagemaker notebook via lifecycle configurations, but for whatever reason it still isn't showing up.<\/p>\n\n<p>I built my script out combining a sample AWS script and a quick article on the usual ways of enabling extensions:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-nb-extension\/on-start.sh\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-nb-extension\/on-start.sh<\/a><\/p>\n\n<p><a href=\"https:\/\/towardsdatascience.com\/jupyter-notebook-extensions-517fa69d2231\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/jupyter-notebook-extensions-517fa69d2231<\/a><\/p>\n\n<pre><code>#!\/bin\/bash\n\nset -e\nsudo -u ec2-user -i &lt;&lt;EOF\n\n--Activate notebook environment\nsource activate JupyterSystemEnv\n\n--Install extensions\npip install jupyter_contrib_nbextensions &amp;&amp; jupyter contrib\nnbextension install\njupyter nbextension enable toc2 --py --sys-prefix\n\nsource deactivate\n\n\nEOF\n<\/code><\/pre>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_date":"2020-04-13 15:39:56.013 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|jupyter-notebook|amazon-sagemaker|tableofcontents",
        "Question_view_count":1027,
        "Owner_creation_date":"2016-03-03 06:30:06.313 UTC",
        "Owner_last_access_date":"2022-09-23 17:07:37.883 UTC",
        "Owner_reputation":757,
        "Owner_up_votes":83,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Answer_body":"<p>Answering my question, looks like I was just missing the line <code>jupyter contrib nbextension install --user<\/code> to copy the JS\/CSS files into Jupyter's search directory and some config updates (<a href=\"https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions\" rel=\"nofollow noreferrer\">https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions<\/a>).<\/p>\n<p>Corrected statement<\/p>\n<pre><code>#!\/bin\/bash\n\nset -e\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\nsource \/home\/ec2-user\/anaconda3\/bin\/activate JupyterSystemEnv\n\npip install jupyter_contrib_nbextensions\njupyter contrib nbextension install --user\njupyter nbextension enable toc2\/main\n\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\n\n\nEOF\n\n##Below may be unnecessary, but other user needed to run to see success\ninitctl restart jupyter-server --no-wait\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-01-15 22:07:51.693 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2021-03-10 05:07:20.85 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61191412",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61181955,
        "Question_title":"Is there any limits of saving result on S3 from sagemaker Processing?",
        "Question_body":"<p>\u203b I used google translation, if you have any question, let me know!<\/p>\n\n<p>I am trying to run python script with huge 4 data, using sagemaker processing. And my current situation are as follows:<\/p>\n\n<ul>\n<li>can run this script with 3 data<\/li>\n<li>can't run the script with only 1 data (the biggest, the same structure with others)<\/li>\n<li>as for all of 4 data, the script has finished (so, I suspected this error in S3, ie. when copying sagemaker result to S3)<\/li>\n<\/ul>\n\n<p>The error I got is this InternalServerError.<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"sagemaker_train_and_predict.py\", line 56, in &lt;module&gt;\n    outputs=outputs\n  File \"{xxx}\/sagemaker_constructor.py\", line 39, in run\n    outputs=outputs\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 408, in run\n    self.latest_job.wait(logs=logs)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 723, in wait\n    self.sagemaker_session.logs_for_processing_job(self.job_name, wait=True)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 3111, in logs_for_processing_job\n    self._check_job_status(job_name, description, \"ProcessingJobStatus\")\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 2615, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Processing job sagemaker-vm-train-and-predict-2020-04-12-04-15-40-655: Failed. Reason: InternalServerError: We encountered an internal error.  Please try again.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-13 05:22:28.657 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":95,
        "Owner_creation_date":"2020-04-13 05:07:12.8 UTC",
        "Owner_last_access_date":"2020-06-30 10:24:46.19 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":"<p>There may be some issue transferring the output data to S3 if the output is generated at a high rate and size is too large. <\/p>\n\n<p>You can 1) try to slow down writing the output a bit or 2) call S3 from your algorithm container to upload the output directly using boto client (<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html<\/a>).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-04-14 17:50:13.193 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61181955",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54408673,
        "Question_title":"Getting sagemaker container locally",
        "Question_body":"<p>When I try to run sagemaker locally for tensorflow in script mode. It seems like I cannot pull the docker container. I have ran the code below from a sagemaker notebook instance and everything ran fine. But when running it on my machine it doesn't work.<\/p>\n\n<p>How can I download the container, so I can debug things locally?<\/p>\n\n<pre><code>import os\n\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\n\n\nhyperparameters = {}\nrole = 'arn:aws:iam::xxxxxxxx:role\/yyyyyyy'\nestimator = TensorFlow(\n    entry_point='train.py',\n    source_dir='.',\n    train_instance_type='local',\n    train_instance_count=1,\n    hyperparameters=hyperparameters,\n    role=role,\n    py_version='py3',\n    framework_version='1.12.0',\n    script_mode=True)\n\nestimator.fit()\n<\/code><\/pre>\n\n<p>I get this output<\/p>\n\n<pre><code>INFO:sagemaker:Creating training-job with name: sagemaker-tensorflow-\nscriptmode-2019-01-28-18-51-57-787\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nError response from daemon: pull access denied for 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode, repository does not exist or may require 'docker login'\n\nsubprocess.CalledProcessError: Command 'docker pull 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.12.0-cpu-py3' returned non-zero exit status 1.\n<\/code><\/pre>\n\n<p>The warning looks like the output you get when using the docker login stuff <a href=\"https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/Registries.html\" rel=\"nofollow noreferrer\">here<\/a>. If I follow these steps to register to the directory with tensorflow container it says login success<\/p>\n\n<pre><code>Invoke-Expression -Command (aws ecr get-login --no-include-email --registry-ids 520713654638 --region eu-west-2)\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\n<\/code><\/pre>\n\n<p>But then I still cannot pull it<\/p>\n\n<pre><code>docker pull 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.11.0-cpu-py3\nError response from daemon: pull access denied for 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode, repository does not exist or may require 'docker login'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-28 19:06:41.737 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|docker|amazon-sagemaker",
        "Question_view_count":2235,
        "Owner_creation_date":"2012-11-06 13:00:33.663 UTC",
        "Owner_last_access_date":"2022-09-24 09:30:20.603 UTC",
        "Owner_reputation":893,
        "Owner_up_votes":112,
        "Owner_down_votes":2,
        "Owner_views":185,
        "Answer_body":"<p>the same sequence works for me locally : 'aws ecr get-login', 'docker login', 'docker pull'. <\/p>\n\n<p>Does your local IAM user have sufficient credentials to pull from ECR? The 'AmazonEC2ContainerRegistryReadOnly' policy should be enough: <a href=\"https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/ecr_managed_policies.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/ecr_managed_policies.html<\/a><\/p>\n\n<p>Alternatively, you can grab the container from Github and build it: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2019-01-29 08:30:03.277 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54408673",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69693666,
        "Question_title":"How to Deploy ML Recommender System on AWS",
        "Question_body":"<p>I'm dabbling with ML and was able to take a tutorial and get it to work for my needs.  It's a simple recommender system using TfidfVectorizer and linear_kernel.  I run into a problem with how I go about deploying it through Sagemaker with an end point.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel \nimport json\nimport csv\n\nwith open('data\/big_data.json') as json_file:\n    data = json.load(json_file)\n\nds = pd.DataFrame(data)\n\ntf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\ntfidf_matrix = tf.fit_transform(ds['content'])\ncosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n\nresults = {}\n\nfor idx, row in ds.iterrows():\n    similar_indices = cosine_similarities[idx].argsort()[:-100:-1]\n    similar_items = [(cosine_similarities[idx][i], ds['id'][i]) for i in similar_indices]\n\n    results[row['id']] = similar_items[1:]\n\ndef item(id):\n    return ds.loc[ds['id'] == id]['id'].tolist()[0]\n\ndef recommend(item_id, num):\n    print(&quot;Recommending &quot; + str(num) + &quot; products similar to &quot; + item(item_id) + &quot;...&quot;)\n    print(&quot;-------&quot;)\n    recs = results[item_id][:num]\n    for rec in recs:\n        print(&quot;Recommended: &quot; + item(rec[1]) + &quot; (score:&quot; + str(rec[0]) + &quot;)&quot;)\n\nrecommend(item_id='129035', num=5)\n<\/code><\/pre>\n<p>As a starting point I'm not sure if the output from <code>tf.fit_transform(ds['content'])<\/code> is considered the model or the output from <code>linear_kernel(tfidf_matrix, tfidf_matrix)<\/code>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-24 03:32:29.497 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-10-25 14:48:35.2 UTC",
        "Question_score":0,
        "Question_tags":"machine-learning|amazon-sagemaker|tfidfvectorizer",
        "Question_view_count":63,
        "Owner_creation_date":"2021-10-24 03:12:09.02 UTC",
        "Owner_last_access_date":"2022-04-19 16:38:57.657 UTC",
        "Owner_reputation":53,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Answer_body":"<p>I came to the conclusion that I didn't need to deploy this through SageMaker.  Since the final linear_kernel output was a Dictionary I could do quick ID lookups to find correlations.<\/p>\n<p>I have it working on AWS with API Gateway\/Lambda, DynamoDB and an EC2 server to collect, process and plug the data into DynamoDB for fast lookups.  No expensive SageMaker endpoint needed.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-11-05 01:24:36.147 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69693666",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73397959,
        "Question_title":"Question regarding sagemaker pipeline example on AWS?",
        "Question_body":"<p>In the sagemaker pipeline example shown here<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html<\/a><\/p>\n<p>I see two lines joining from AbaloneProcess to AbaloneTrain and AbaloneEval respectively. However, based on the code, I would expect it to actually be connected from AbaloneTrain only, then to AbaloneEval in a single path. Can somebody explain to me what is actually happening here because I am struggling to wrap my head around this. Much appreciated and apologies for the inconvenience in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-18 05:42:35.963 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":16,
        "Owner_creation_date":"2022-02-16 03:15:56.94 UTC",
        "Owner_last_access_date":"2022-09-23 15:06:17.55 UTC",
        "Owner_reputation":53,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>The connections are indicated data dependencies between steps, not only the order of execution. This is what you see AbaloneProcess connected to AbaloneEval, since the output of AbaloneProcess is used in AbaloneEval.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-24 17:56:04.683 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73397959",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49675637,
        "Question_title":"How to pass a request to sagemaker using postman",
        "Question_body":"<p>I've trained a model on sagemaker and have created the endpoint. I'm trying to invoke the endpoint using postman. But when training the model and even after that, I have not specified any header for the training data. I'm at a loss as to how to create payload while sending a post request to sagemaker<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-04-05 15:03:12.383 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":null,
        "Question_score":8,
        "Question_tags":"web-services|amazon-web-services|postman|amazon-sagemaker",
        "Question_view_count":6746,
        "Owner_creation_date":"2014-09-17 16:42:55.307 UTC",
        "Owner_last_access_date":"2022-09-22 07:49:15.917 UTC",
        "Owner_reputation":1124,
        "Owner_up_votes":156,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Answer_body":"<p>Once the endpoint is created, you can invoke it as any other restful service, with credentials and payload. <\/p>\n\n<p>I am guessing, there could be two places where might be stuck. \nOne could be, sending an actual PostMan Request with all the headers and everything. \nNewer version of Postman has AWS Signature as one of the Authorization types. You can use that to invoke the service. There are no other spacial headers required. Note that there is a bug in Postman still open (<a href=\"https:\/\/github.com\/postmanlabs\/postman-app-support\/issues\/1663\" rel=\"noreferrer\">issue-1663<\/a>) that only affects if you are a AWS federated account. Individual accounts should not be affected by this issue. <\/p>\n\n<p>Or, you could be stuck at the actual payload. When you invoke the SageMaker endpoint, the payload is passed as is to the model. If you want to preprocess the input before feeding it to the model, you'd have to implement an input_fn method and specify that when instantiating the model. <\/p>\n\n<p>You might also be able to invoke SageMaker endpoint using AWS SDK boto3 as follows <\/p>\n\n<pre><code>import boto3\nruntime= boto3.client('runtime.sagemaker')\n\npayload = getImageData()\n\n\nresult  = runtime.invoke_endpoint(\n    EndpointName='my_endpoint_name',\n    Body=payload,\n    ContentType='image\/jpeg'\n)\n<\/code><\/pre>\n\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-04-05 16:37:35.433 UTC",
        "Answer_score":11.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49675637",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61464960,
        "Question_title":"SageMaker multimodel and RandomCutForest",
        "Question_body":"<p>I am trying to invoke a MultiModel Endpoint with a RandomCutForest Model. I receive error though, 'Error loading model'. I can invoke the endpoint with models given from the examples.\nAm I missing something e.g. limitations on what models I can use? <\/p>\n\n<p>For MultiModel inspiration I am using below:<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb<\/a><\/p>\n  \n  <p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/<\/a><\/p>\n<\/blockquote>\n\n<p>I am trying to deploy the outputted 'model.tar.gz' from below RCF example in the MultiModel endpoint:<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb<\/a><\/p>\n<\/blockquote>\n\n<pre><code>model_name = 'model'\nfull_model_name = '{}.tar.gz'.format(model_name)\nfeatures = data\n\nbody = ','.join(map(str, features)) + '\\n'\nresponse = runtime_sm_client.invoke_endpoint(\n                    EndpointName=endpoint_name,\n                    ContentType='text\/csv',\n                    TargetModel=full_model_name,\n                    Body=body)\nprint(response)\n<\/code><\/pre>\n\n<p><strong>Cloudwatch log Error:<\/strong><\/p>\n\n<pre><code>&gt; 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Error loading model: Unable\n&gt; to load model: invalid load key, '{'. [17:28:59]\n&gt; \/workspace\/src\/learner.cc:334: Check failed: fi-&gt;Read(&amp;mparam_,\n&gt; sizeof(mparam_)) == sizeof(mparam_) (25 vs. 136) : BoostLearner: wrong\n&gt; model format 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Stack trace: 2020-04-27\n&gt; 17:28:59,005 [INFO ] W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (0)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x24)\n&gt; [0x7f37ce1cacb4] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9 com.amazonaws.ml.mms.wlm.WorkerThread\n&gt; - Backend response time: 0 2020-04-27 17:28:59,005 [INFO ] W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (1)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(xgboost::LearnerImpl::Load(dmlc::Stream*)+0x4b5)\n&gt; [0x7f37ce266985] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (2)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(XGBoosterLoadModel+0x37)\n&gt; [0x7f37ce1bf417] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (3)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c)\n&gt; [0x7f37ee993ec0] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (4)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d)\n&gt; [0x7f37ee99387d] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (5)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce)\n&gt; [0x7f37eeba91de] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (6)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12c14)\n&gt; [0x7f37eeba9c14] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (7)\n&gt; \/miniconda3\/bin\/python(_PyObject_FastCallKeywords+0x48b)\n&gt; [0x563d71b4218b] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (8)\n&gt; \/miniconda3\/bin\/python(_PyEval_EvalFrameDefault+0x52cf)\n&gt; [0x563d71b91e8f] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  2020-04-27 17:28:59,005\n&gt; [WARN ] W-9003-b39b888fb4a3fa6cf83bb34a9\n&gt; com.amazonaws.ml.mms.wlm.WorkerThread - Backend worker thread\n&gt; exception. java.lang.IllegalArgumentException: reasonPhrase contains\n&gt; one of the following prohibited characters: \\r\\n: Unable to load\n&gt; model: Unable to load model: invalid load key, '{'. [17:28:59]\n&gt; \/workspace\/src\/learner.cc:334: Check failed: fi-&gt;Read(&amp;mparam_,\n&gt; sizeof(mparam_)) == sizeof(mparam_) (25 vs. 136) : BoostLearner: wrong\n&gt; model format Stack trace:   [bt] (0)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x24)\n&gt; [0x7f37ce1cacb4]   [bt] (1)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(xgboost::LearnerImpl::Load(dmlc::Stream*)+0x4b5)\n&gt; [0x7f37ce266985]   [bt] (2)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(XGBoosterLoadModel+0x37)\n&gt; [0x7f37ce1bf417]   [bt] (3)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c)\n&gt; [0x7f37ee993ec0]   [bt] (4)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d)\n&gt; [0x7f37ee99387d]   [bt] (5)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce)\n&gt; [0x7f37eeba91de]   [bt] (6)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12c14)\n&gt; [0x7f37eeba9c14]   [bt] (7)\n&gt; \/miniconda3\/bin\/python(_PyObject_FastCallKeywords+0x48b)\n&gt; [0x563d71b4218b]   [bt] (8)\n&gt; \/miniconda3\/bin\/python(_PyEval_EvalFrameDefault+0x52cf)\n&gt; [0x563d71b91e8f]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-27 17:39:45.817 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|xgboost|amazon-sagemaker",
        "Question_view_count":324,
        "Owner_creation_date":"2015-03-16 08:02:10.157 UTC",
        "Owner_last_access_date":"2022-09-14 09:01:01.523 UTC",
        "Owner_reputation":398,
        "Owner_up_votes":37,
        "Owner_down_votes":1,
        "Owner_views":84,
        "Answer_body":"<p>SageMaker Random Cut Forest is part of the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">built-in algorithm library<\/a> and cannot be deployed in multi-model endpoint (MME). Built-in algorithms currently cannot be deployed to MME. XGboost is an exception, since it has an open-source container <a href=\"https:\/\/github.com\/aws\/sagemaker-xgboost-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-xgboost-container<\/a>.<\/p>\n\n<p>If you really need to deploy a RCF to a multi-model endpoint, one option is to find a reasonably similar open-source implementation (for example <a href=\"https:\/\/github.com\/kLabUM\/rrcf\" rel=\"nofollow noreferrer\"><code>rrcf<\/code><\/a> looks reasonably serious: based <a href=\"http:\/\/proceedings.mlr.press\/v48\/guha16.pdf\" rel=\"nofollow noreferrer\">on the same paper from Guha et al<\/a> and with 170+ github stars) and create a custom MME docker container. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">documentation is here<\/a> and there is <a href=\"https:\/\/github.com\/giuseppeporcelli\/sagemaker-custom-serving-containers\/blob\/master\/multi-model-server-container\/notebook\/multi-model-server-container.ipynb\" rel=\"nofollow noreferrer\">an excellent tuto here<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-04-28 08:13:01.36 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61464960",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68186468,
        "Question_title":"Is there a REST API available for SageMaker, or is it possible to interact with SageMaker over the Amazon API Gateway?",
        "Question_body":"<p>SageMaker provides a full machine learning development environment on AWS. It works with the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">Amazon SageMaker Python SDK<\/a>, which allows Jupyter Notebooks to interact with the functionality. This also provides the path to using the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_featurestore.html\" rel=\"nofollow noreferrer\">Amazon SageMaker Feature Store<\/a>.<\/p>\n<p>Is there any REST API available for SageMaker? Say one wanted to create their own custom UI, but still use SageMaker features, is this possible?<\/p>\n<p>Can it be done using the <a href=\"https:\/\/aws.amazon.com\/api-gateway\/\" rel=\"nofollow noreferrer\">Amazon API Gateway<\/a>?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-29 22:44:19.24 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-api-gateway|amazon-sagemaker|feature-store",
        "Question_view_count":1164,
        "Owner_creation_date":"2012-08-31 20:08:40.09 UTC",
        "Owner_last_access_date":"2022-09-25 04:17:41.297 UTC",
        "Owner_reputation":11650,
        "Owner_up_votes":6318,
        "Owner_down_votes":21,
        "Owner_views":977,
        "Answer_body":"<p>Amazon API Gateway currently does not provide first-class integration for SageMaker. But you can use these services via AWS SDK. If you wish, you can embed the AWS SDK calls into a service, host on AWS (e.g. running on EC2 or as lambda functions) and use API gateway to expose your REST API.<\/p>\n<p>Actually, SageMaker is not fundamentally different from any other AWS service from this aspect.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-07-16 21:03:20.22 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68186468",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71308112,
        "Question_title":"How to return a float with 'text\/csv' as \"Content-Type\" from SageMaker endpoint that uses custom inference code?",
        "Question_body":"<p>I am trying to return output(or predictions) from a SageMaker endpoint with 'text\/csv' or 'text\/csv; charset=utf-8' as &quot;Content-Type&quot;. I have tried multiple ways, but the sagemaker always returns with 'text\/html; charset=utf-8' as the &quot;Content-Type&quot;, and I would like SageMaker to return 'text\/csv' or 'text\/csv; charset=utf-8'.<\/p>\n<p>Here's the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#process-output\" rel=\"nofollow noreferrer\"><code>output_fn<\/code><\/a> from my inference-code:<\/p>\n<pre><code>** my other code **\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return output_float\n<\/code><\/pre>\n<p>above function returns number with float data-type and I got error(in cloudwatch logs) that  this function should only be returning string, tuple, dict or Respoonse instance.<\/p>\n<p>So, here are all the different ways I have tried to have SageMaker return my number with 'text\/csv' but only receives 'text\/html; charset=utf-8'<\/p>\n<ol>\n<li><code>return json.dumps(output_float)<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float}&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float},&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float}\\n&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float},\\n&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li>by using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\" rel=\"nofollow noreferrer\"><code>sagemaker.serializers.CSVSerializer<\/code><\/a> like this:\n<pre><code>from sagemaker.serializers import CSVSerializer\ncsv_serialiser = CSVSerializer(content_type='text\/csv')\n\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return csv_serialiser.serialize(output_float)\n<\/code><\/pre>\nI got <code>'NoneType' object has no attribute 'startswith'<\/code> error with this.<\/li>\n<li>as a tuple: <code>return (output_float,)<\/code>. I haven't noted down what this did, but it sure didn't return the number with 'text\/csv' as &quot;Content-Type&quot;.<\/li>\n<li>made changes in my model object to return a float on calling <code>.predict_proba<\/code> on my model-object and deployed it from sagemaker studio without using any custom inference code and deployed this from SageMaker studio. but got this error after sending a request to the endpoint: <code>'NoneType' object has no attribute 'startswith'<\/code>, but at my side, when I pass proper inputs to the unpicked model and call .predict_proba, i get float as expected.<\/li>\n<li>by returning <a href=\"https:\/\/tedboy.github.io\/flask\/generated\/generated\/flask.Response.html#flask.Response\" rel=\"nofollow noreferrer\"><code>flask.Response<\/code><\/a> like this:\n<pre><code>from flask import Response\n\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return Response(response=output_float, status=200, headers={'Content-Type':'text\/csv; charset=utf-8'})\n<\/code><\/pre>\nbut, I got some IndexError with this(I didn't notedown the traceback.)<\/li>\n<\/ol>\n<p>Some other info:<\/p>\n<ol>\n<li>Model I am using is completely outside of SageMaker, not from a training job or anything like that.<\/li>\n<li>All of the aforementioned endpoints have been deployed entirely from aws-cli with relevant .json files, except the one in point-8 above.<\/li>\n<\/ol>\n<p>How do I return number with &quot;text\/csv&quot; as content-type from sagemaker? I need my output in &quot;text\/csv&quot; content-type specifically for Model-Quality-Monitor. How do I do this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-03-01 11:54:47.137 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-03-07 11:21:09.63 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":461,
        "Owner_creation_date":"2019-06-07 12:24:06.18 UTC",
        "Owner_last_access_date":"2022-09-24 17:19:11.323 UTC",
        "Owner_reputation":2046,
        "Owner_up_votes":2858,
        "Owner_down_votes":5,
        "Owner_views":369,
        "Answer_body":"<p>From the <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py#L88\" rel=\"nofollow noreferrer\">scikit_bring_your_own<\/a> example, I suggest testing by setting the Response as follows:<\/p>\n<pre><code>return flask.Response(response= output_float, status=200, mimetype=&quot;text\/csv&quot;)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-03-07 20:45:03.08 UTC",
        "Answer_score":1.0,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71308112",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68143997,
        "Question_title":"Automate Docker Run command on Sagemaker's Notebook Instance",
        "Question_body":"<p>I have a Docker image in AWS ECR and I open my Sagemaker Notebook instance---&gt;go to terminal--&gt;docker run....\nThis is how I start my Docker container.<\/p>\n<p>Now, I want to automate this process(running my docker image on Sagemaker Notebook Instance) instead of typing the docker run commands.<\/p>\n<p>Can I create a cron job on Sagemaker? or Is there any other approach?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-26 15:48:40.977 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|docker|containers|amazon-sagemaker",
        "Question_view_count":393,
        "Owner_creation_date":"2017-06-16 14:03:57.833 UTC",
        "Owner_last_access_date":"2022-09-20 06:42:37.06 UTC",
        "Owner_reputation":230,
        "Owner_up_votes":11,
        "Owner_down_votes":4,
        "Owner_views":27,
        "Answer_body":"<p>For this you can create an inline Bash shell in your SageMaker notebook as follows. This will take your Docker container, create the image, ECR repo if it does not exist and push the image.<\/p>\n<pre><code>%%sh\n\n# Name of algo -&gt; ECR\nalgorithm_name=your-algo-name\n\ncd container #your directory with dockerfile and other sm components\n\nchmod +x randomForest-Petrol\/train #train file for container\nchmod +x randomForest-Petrol\/serve #serve file for container\n\naccount=$(aws sts get-caller-identity --query Account --output text)\n\n# Region, defaults to us-west-2\nregion=$(aws configure get region)\nregion=${region:-us-west-2}\n\nfullname=&quot;${account}.dkr.ecr.${region}.amazonaws.com\/${algorithm_name}:latest&quot;\n\n# If the repository doesn't exist in ECR, create it.\naws ecr describe-repositories --repository-names &quot;${algorithm_name}&quot; &gt; \/dev\/null 2&gt;&amp;1\n\nif [ $? -ne 0 ]\nthen\n    aws ecr create-repository --repository-name &quot;${algorithm_name}&quot; &gt; \/dev\/null\nfi\n\n# Get the login command from ECR and execute it directly\naws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n\n# Build the docker image locally with the image name and then push it to ECR\n# with the full name.\n\ndocker build  -t ${algorithm_name} .\ndocker tag ${algorithm_name} ${fullname}\n\ndocker push ${fullname}\n<\/code><\/pre>\n<p>I am contributing this on behalf of my employer, AWS. My contribution is licensed under the MIT license. See here for a more detailed explanation\n<a href=\"https:\/\/aws-preview.aka.amazon.com\/tools\/stackoverflow-samples-license\/\" rel=\"nofollow noreferrer\">https:\/\/aws-preview.aka.amazon.com\/tools\/stackoverflow-samples-license\/<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-07-22 18:10:03.51 UTC",
        "Answer_score":0.0,
        "Owner_location":"Bangalore, Karnataka, India",
        "Answer_last_edit_date":"2021-07-22 18:41:04.577 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68143997",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52847777,
        "Question_title":"Customer Error: Additional hyperparameters are not allowed - Image classification training- Sagemaker",
        "Question_body":"<p>I'm learning image classification with Amazon SageMaker. I was trying to follow their  learning demo <strong>Image classification transfer learning demo<\/strong> (<code>Image-classification-transfer-learning-highlevel.ipynb<\/code>)<\/p>\n\n<p>I got up to Start the training. Executed below.<\/p>\n\n<pre><code>ic.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n\n<p>Set the hyper parameters as given in the demo<\/p>\n\n<pre><code>ic.set_hyperparameters(num_layers=18,\n                             use_pretrained_model=1,\n                             image_shape = \"3,224,224\",\n                             num_classes=257,\n                             num_training_samples=15420,\n                             mini_batch_size=128,\n                             epochs=1,\n                             learning_rate=0.01,\n                             precission_dtype='float32')\n<\/code><\/pre>\n\n<p>Got the client error<\/p>\n\n<pre><code>ERROR 140291262150464] Customer Error: Additional hyperparameters are not allowed (u'precission_dtype' was unexpected) (caused by ValidationError)\n\nCaused by: Additional properties are not allowed (u'precission_dtype' was unexpected)\n<\/code><\/pre>\n\n<p>Does anyone know how to overcome this? I'm also reporting this to aws support. Posting here for sharing and get a fix. Thanks !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-10-17 05:25:16.957 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|jupyter-notebook|classification|amazon-sagemaker",
        "Question_view_count":146,
        "Owner_creation_date":"2014-06-19 13:51:42.747 UTC",
        "Owner_last_access_date":"2019-01-10 08:10:48.667 UTC",
        "Owner_reputation":169,
        "Owner_up_votes":215,
        "Owner_down_votes":0,
        "Owner_views":81,
        "Answer_body":"<p>Assuming you are referring to <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb<\/a> - you have a typo, it's <code>precision_dtype<\/code>, not <code>precission_dtype<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-10-18 03:06:33.64 UTC",
        "Answer_score":0.0,
        "Owner_location":"Colombo, Sri Lanka",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52847777",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67205469,
        "Question_title":"Is it a good idea to store my dataset in my notebook instance in sagemaker?",
        "Question_body":"<p>I'm new to AWS and I am considering to use amazon sagemaker to train my deep learning model because I'm having memory issues due to the large dataset and neural network that I have to train. I'm confused whether to store my data in my notebook instance or in S3? If I store it in my s3 would I be able to access it to train on my notebook instance? I'm confused on the concepts. Can anyone explain the use of S3 in machine learning in AWS?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-22 01:17:18.143 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":322,
        "Owner_creation_date":"2020-11-21 06:04:32.327 UTC",
        "Owner_last_access_date":"2021-06-03 07:19:55.19 UTC",
        "Owner_reputation":97,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Answer_body":"<p>Yes you can use S3 as storage for your training datasets.<\/p>\n<p>Refer diagram in this link describing how everything works together: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html<\/a><\/p>\n<p>You may also want to checkout following blogs that details about File mode and Pipe mode, two mechanisms for transferring training data:<\/p>\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/<\/a><\/li>\n<\/ol>\n<blockquote>\n<p>In File mode, the training data is downloaded first to an encrypted EBS volume attached to the training instance prior to commencing the training. However, in Pipe mode the input data is streamed directly to the training algorithm while it is running.<\/p>\n<\/blockquote>\n<ol start=\"2\">\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/<\/a><\/li>\n<\/ol>\n<blockquote>\n<p>With Pipe input mode, your data is fed on-the-fly into the algorithm container without involving any disk I\/O. This approach shortens the lengthy download process and dramatically reduces startup time. It also offers generally better read throughput than File input mode. This is because your data is fetched from Amazon S3 by a highly optimized multi-threaded background process. It also allows you to train on datasets that are much larger than the 16 TB Amazon Elastic Block Store (EBS) volume size limit.<\/p>\n<\/blockquote>\n<p>The blog also contains python code snippets using Pipe input mode for reference.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-04-22 03:29:38.833 UTC",
        "Answer_score":2.0,
        "Owner_location":"Jakarta Selatan, South Jakarta City, Jakarta, Indonesia",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67205469",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60190365,
        "Question_title":"Sagemaker with tensorflow 2 not saving model",
        "Question_body":"<p>I am working with Keras and I am trying to train a model using Sagemaker. I have the following issue:\nWhen I train my model using TensorFlow 1.12 everything works fine:<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='entrypoint-2.py',\n                            base_job_name='mlearning-test',\n                         role=role,\n                         train_instance_count=1,\n                         input_mode='Pipe',\n                         train_instance_type='ml.p2.xlarge',\n                         framework_version='1.12.0')\n<\/code><\/pre>\n\n<p>My model is trained and the model is saved in S3. Not problems.<\/p>\n\n<p>However, if I changed the framework version to be 2.0.0<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='entrypoint-2.py',\n                                base_job_name='mlearning-test',\n                             role=role,\n                             train_instance_count=1,\n                             input_mode='Pipe',\n                             train_instance_type='ml.p2.xlarge',\n                             framework_version='2.0.0')\n<\/code><\/pre>\n\n<p>I get the following error: <\/p>\n\n<pre><code>2020-02-12 13:54:36,601 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path \/opt\/ml\/model. Your training job will not save any model files to S3.\nFor details of how to construct your training script see:\nhttps:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html#adapting-your-local-tensorflow-script\n<\/code><\/pre>\n\n<p>The training job is marked as successful but there is nothing in the S3 bucket and indeed there was not training.<\/p>\n\n<p>As an alternative, I tried putting the py_version='py3' but this keeps happening. is there a major difference that I am not aware of when using TF2 on sagemaker? <\/p>\n\n<p>I don't think the entry point is needed since it works fine with version 1.12 but in case you are curious or can spot something here it is:<\/p>\n\n<pre><code>import tensorflow as tf\nfrom sagemaker_tensorflow import PipeModeDataset\n#from tensorflow.contrib.data import map_and_batch\n\nINPUT_TENSOR_NAME = 'inputs_input'  \nBATCH_SIZE = 64\nNUM_CLASSES = 5\nBUFFER_SIZE = 50\nPREFETCH_SIZE = 1\nLENGHT = 512\nSEED = 26\nEPOCHS = 1\nWIDTH = 512\n\ndef keras_model_fn(hyperparameters):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(WIDTH, 'relu', input_shape=(None, WIDTH), name = 'inputs'),\n        #tf.keras.layers.InputLayer(input_shape=(None, WIDTH), name=INPUT_TENSOR_NAME),\n        tf.keras.layers.Dense(256, 'relu'),\n        tf.keras.layers.Dense(128, 'relu'),\n        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n    ])\n\n    opt = tf.keras.optimizers.RMSprop()\n\n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n    return model\n\ndef serving_input_fn(hyperparameters):\n    # Notice that the input placeholder has the same input shape as the Keras model input\n    tensor = tf.placeholder(tf.float32, shape=[None, WIDTH])\n\n    # The inputs key INPUT_TENSOR_NAME matches the Keras InputLayer name\n    inputs = {INPUT_TENSOR_NAME: tensor}\n    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n\ndef train_input_fn(training_dir, params):\n    \"\"\"Returns input function that would feed the model during training\"\"\"\n    return _input_fn('train')\n\ndef eval_input_fn(training_dir, params):\n    \"\"\"Returns input function that would feed the model during evaluation\"\"\"\n    return _input_fn('eval')\n\ndef _input_fn(channel):\n    \"\"\"Returns a Dataset for reading from a SageMaker PipeMode channel.\"\"\"\n    print(\"DATA \"+channel)\n    features={\n        'question': tf.FixedLenFeature([WIDTH], tf.float32),\n        'label': tf.FixedLenFeature([1], tf.int64)\n    }\n\n    def parse(record):\n        parsed = tf.parse_single_example(record, features)\n        #print(\"--------&gt;\"+str(tf.cast(parsed['question'], tf.float32))\n        return {\n            INPUT_TENSOR_NAME: tf.cast(parsed['question'], tf.float32)\n        }, parsed['label']\n\n    ds = PipeModeDataset(channel)\n    if EPOCHS &gt; 1:\n        ds = ds.repeat(EPOCHS)\n    ds = ds.prefetch(PREFETCH_SIZE)\n    #ds = ds.apply(map_and_batch(parse, batch_size=BATCH_SIZE,\n    #                            num_parallel_batches=BUFFER_SIZE))\n    ds = ds.map(parse, num_parallel_calls=NUM_PARALLEL_BATCHES)\n    ds = ds.batch(BATCH_SIZE)\n\n    return ds\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-12 14:15:27.893 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"tensorflow|keras|amazon-sagemaker",
        "Question_view_count":913,
        "Owner_creation_date":"2016-05-09 05:03:09.773 UTC",
        "Owner_last_access_date":"2021-07-16 12:54:52.163 UTC",
        "Owner_reputation":45,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":"<p>you're correct, <strong>there has been a major, beneficial change last year in the SageMaker TensorFlow experience named the <em>Script Mode<\/em> formalism<\/strong>. As you can see in the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">SDK Documentation<\/a>:<\/p>\n\n<p><em>\"Warning.\nWe have added a new format of your TensorFlow training script with TensorFlow version 1.11. This new way gives the user script more flexibility. This new format is called Script Mode, as opposed to Legacy Mode, which is what we support with TensorFlow 1.11 and older versions. In addition we are adding Python 3 support with Script Mode. The last supported version of Legacy Mode will be TensorFlow 1.12. Script Mode is available with TensorFlow version 1.11 and newer. Make sure you refer to the correct version of this README when you prepare your script. You can find the Legacy Mode README here.\"<\/em><\/p>\n\n<p>with TensorFlow 2, you need to follow that <em>Script Mode<\/em> formalism and save your model in the <code>opt\/ml\/model<\/code> path, otherwise nothing will be sent to S3. <em>Script Mode<\/em> is quite straightforward to implement and gives better flexibility and portability, and this spec is shared with SageMaker Sklearn container, SageMaker Pytorch container and SageMaker MXNet container so definitely worth adopting<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-02-14 23:54:16.863 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60190365",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60127972,
        "Question_title":"AWS educate account is giving client error when calling training job operation",
        "Question_body":"<pre><code>estimator.fit(inputs=data_channels)\n<\/code><\/pre>\n\n<blockquote>\n  <p>ClientError: An error occurred (AccessDeniedException) when calling\n  the CreateTrainingJob operation: User:\n  arn:aws:sts::078087344404:assumed-role\/AmazonSageMaker-ExecutionRole-20200206T231635\/SageMaker\n  is not authorized to perform: sagemaker:CreateTrainingJob on resource:\n  arn:aws:sagemaker:us-east-1:078087344404:training-job\/deepar-atm-no-categories-2020-02-08-14-45-32-321\n  with an explicit deny<\/p>\n<\/blockquote>",
        "Question_answer_count":2,
        "Question_comment_count":5,
        "Question_creation_date":"2020-02-08 14:57:13.477 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-02-08 19:34:43.693 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":510,
        "Owner_creation_date":"2020-02-08 14:49:13.197 UTC",
        "Owner_last_access_date":"2020-06-02 20:46:42.7 UTC",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>This is is because your user is not able to assume role to access sagemaker, ultimately you are getting permission denied error. <\/p>\n\n<p>FYI: SageMaker is not included in amazon educate account. You can ask your teacher\/account admin to allow you to access amazon sage maker.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2020-02-08 19:30:14.743 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60127972",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49689216,
        "Question_title":"How can I print the Canonical String which aws-requests-auth sends?",
        "Question_body":"<p>I want to have a lambda calling a Sagemaker instance in another region. If both are in the same region, everything works fine. If they are not, I get the following error:<\/p>\n\n<pre><code>The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\n\nThe Canonical String for this request should have been\n'POST\n\/endpoints\/foo-endpoint\/invocations\n\nhost:runtime.sagemaker.us-east-1.amazonaws.com\nx-amz-date:20180406T082536Z\n\nhost;x-amz-date\n1234567890foobarfoobarfoobarboofoobarfoobarfoobarfoobarfoobarfoo'\n\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20180406T082536Z\n20180406\/us-east-1\/sagemaker\/aws4_request\n987654321abcdeffoobarfoobarfoobarfoobarfoobarfoobarfoobarfoobarf'\n<\/code><\/pre>\n\n<p>I use <a href=\"https:\/\/github.com\/DavidMuller\/aws-requests-auth\" rel=\"nofollow noreferrer\"><code>aws-requests-auth<\/code><\/a> (0.4.1) with boto3 (1.5.15 - updating to 1.7.1 didn't change anything, <a href=\"https:\/\/github.com\/boto\/boto3\/blob\/develop\/CHANGELOG.rst\" rel=\"nofollow noreferrer\">changelog<\/a>) like this:<\/p>\n\n<pre><code>import requests\nfrom aws_requests_auth.aws_auth import AWSRequestsAuth\nauth = AWSRequestsAuth(aws_access_key=config['AWS']['ACCESS_KEY'],\n                       aws_secret_access_key=(\n                           config['AWS']['SECRET_ACCESS_KEY']),\n                       aws_host=config['AWS']['HOST'],\n                       aws_region=config['AWS']['REGION'],\n                       aws_service=config['AWS']['SERVICE'])\n\npayload = {'foo': 'bar'}\nresponse = requests.post(post_url,\n                         data=json.dumps(payload),\n                         headers={'content-type': 'application\/json'},\n                         auth=auth)\n<\/code><\/pre>\n\n<p>printing <code>auth<\/code> only gives <code>&lt;aws_requests_auth.aws_auth.AWSRequestsAuth object at 0x7f9d00c98390&gt;<\/code>.<\/p>\n\n<p>Is there a way to print the \"Canonical String\" mentioned in the error message?<\/p>\n\n<p>(Any other ideas how to fix this are appreciated as well)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-04-06 09:06:20.303 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-07-10 08:39:23.023 UTC",
        "Question_score":3,
        "Question_tags":"amazon-web-services|authentication|boto3|amazon-sagemaker",
        "Question_view_count":827,
        "Owner_creation_date":"2011-01-04 15:39:58.4 UTC",
        "Owner_last_access_date":"2022-09-24 15:06:20.15 UTC",
        "Owner_reputation":112558,
        "Owner_up_votes":2933,
        "Owner_down_votes":350,
        "Owner_views":21355,
        "Answer_body":"<p>A work-around for the asked question:<\/p>\n\n<pre><code>req = requests.request('POST', 'http:\/\/httpbin.org\/get')\nreq.body = b''\nreq.method = ''\nprint(auth.get_aws_request_headers(req,\n                                   aws_access_key=auth.aws_access_key,\n                                   aws_secret_access_key=auth.aws_secret_access_key,\n                                   aws_token=auth.aws_token))\n<\/code><\/pre>\n\n<p>The problem is not solved, though. And now I wonder what the first argument of <code>auth.get_aws_request_headers<\/code> is.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-04-06 10:55:41.48 UTC",
        "Answer_score":1.0,
        "Owner_location":"M\u00fcnchen, Deutschland",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49689216",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61550297,
        "Question_title":"Await returns too soon",
        "Question_body":"<p>I am retrieving a list of image filenames from DynamoDB and using those image filenames to replace the default <code>src=<\/code> image in a portion of a website.<\/p>\n\n<p>I'm a JS novice, so I'm certainly missing something, but my function is returning the list of filenames too late.<\/p>\n\n<p>My inline script is:<\/p>\n\n<pre><code>&lt;script&gt;\n        customElements.whenDefined( 'crowd-bounding-box' ).then( () =&gt; {  \n        var imgBox = document.getElementById('annotator');\n        newImg = imageslist();\n        console.log(\"Result of newImg is: \" + newImg);\n        imgBox.src = \"https:\/\/my-images-bucket.s3.amazonaws.com\/\" + newImg;\n    } )\n&lt;\/script&gt;\n<\/code><\/pre>\n\n<p>My JS function is:<\/p>\n\n<pre><code>async function imageslist() {\n    const username = \"sampleuser\";\n    const params = {\n        TableName: \"mytable\",\n        FilterExpression: \"attribute_not_exists(\" + username + \")\",\n        ReturnConsumedCapacity: \"NONE\"\n    };\n    try {\n        var data = await ddb.scan(params).promise()\n        var imglist = [];\n        for(let i = 0; i &lt; data.Items.length; i++) {\n            imglist.push(data.Items[i].img.S);\n        };\n        imglist.sort();\n        var firstimg = imglist[0];\n        console.log(firstimg);\n        return imglist\n    } catch(error) {\n        console.error(error);\n    }\n}\n<\/code><\/pre>\n\n<p>The console reports <code>Result of newImg is: [object Promise]<\/code> and shortly after that, it reports the expected filename.  After the page has been rendered, I can input <code>newImg<\/code> in the console and I receive the expected result.<\/p>\n\n<p>Am I using the <strong>await<\/strong> syntax improperly?<\/p>\n\n<p>Side note: This site uses the Crowd HTML Elements (for Ground Truth and Mechanical Turk), so I'm forced to have the <code>src=<\/code> attribute present in my <code>crowd-bounding-box<\/code> tag and it must be a non-zero value.  I'm loading a default image and replacing it with another image.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2020-05-01 20:28:20.607 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"javascript|promise|async-await|amazon-sagemaker|mechanicalturk",
        "Question_view_count":63,
        "Owner_creation_date":"2017-01-03 11:49:04.907 UTC",
        "Owner_last_access_date":"2022-02-04 13:53:41.673 UTC",
        "Owner_reputation":312,
        "Owner_up_votes":28,
        "Owner_down_votes":1,
        "Owner_views":65,
        "Answer_body":"<p>Anytime you use the <code>await<\/code> keyword, you must use the <code>async<\/code> keyword before the function definition (which you have done). The thing is, any time an async function is called, it will always return a <code>Promise<\/code> object since it expects that some asynchronous task will take place within the function.<\/p>\n\n<p>Therefore,you'll need to <code>await<\/code> the result of the imageslist function and make the surrounding function <code>async<\/code>.<\/p>\n\n<pre class=\"lang-js prettyprint-override\"><code>&lt;script&gt;\n    customElements.whenDefined( 'crowd-bounding-box' ).then( async () =&gt; {  \n        var imgBox = document.getElementById('annotator');\n        newImg = await imageslist();\n        console.log(\"Result of newImg is: \" + newImg);\n        imgBox.src = \"https:\/\/my-images-bucket.s3.amazonaws.com\/\" + newImg;\n    } )\n&lt;\/script&gt;\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-05-01 20:43:38.457 UTC",
        "Answer_score":1.0,
        "Owner_location":"Hoth",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61550297",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53809556,
        "Question_title":"Load csv into S3 from local",
        "Question_body":"<p>I am exploring AWS sagemaker for ML. I have created a bucket:<\/p>\n\n<pre><code>bucket_name = 'test-bucket' \ns3 = boto3.resource('s3')\ntry:\n   if  my_region == 'us-east-1':\n      s3.create_bucket(Bucket=bucket_name)\n   else: \n      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\nprint('S3 bucket created successfully')\nexcept Exception as e:\n    print('S3 error: ',e)\n<\/code><\/pre>\n\n<p>I have a csv in my local and I want to load that into the bucket I created.<\/p>\n\n<p>All the links I have referred  have directions to load it from a link and unzip it. Is there a way to load the data into the bucket from the local.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2018-12-17 05:46:04.273 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":1703,
        "Owner_creation_date":"2016-02-15 00:34:43.357 UTC",
        "Owner_last_access_date":"2022-01-18 23:02:48.19 UTC",
        "Owner_reputation":3912,
        "Owner_up_votes":135,
        "Owner_down_votes":47,
        "Owner_views":311,
        "Answer_body":"<p>If you are using Amazon SageMaker you can use the SageMaker python library that is implementing the most useful commands for data scientists, including the upload of files to S3. It is already installed on your SageMaker notebook instance by default. <\/p>\n\n<pre><code>import sagemaker\nsess = sagemaker.Session()\n\n# Uploading the local file to S3\nsess.upload_data(path='local-file.txt', bucket=bucket_name, key_prefix='input')    \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-12-21 21:43:22.017 UTC",
        "Answer_score":0.0,
        "Owner_location":"San Francisco, CA, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53809556",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51803032,
        "Question_title":"AWS SageMaker S3 os.listdir() Access denied",
        "Question_body":"<p>I'm Stumped.  <\/p>\n\n<p>I took my TensorFlow model and moved it up into SageMaker to try it out.  I put my own data up into an s3 bucket, set all the IAM roles\/access (or so I think).  I can read a file from s3.  I can push a new file to s3. I can read local directories from my SageMaker local directories.<\/p>\n\n<p><strong>I cannot traverse my s3 bucket directories.<\/strong>  I turned on logging and I get AccessDenied messages whenever I try access a URI of this format <strong>'s3:\/\/my_bucketName_here\/Directory_of_my_data\/'<\/strong>.<\/p>\n\n<p>Here is what I've done:\nI've confirmed that my notebook uses the AmazonSageMaker-ExecutionRole-***\nI've added AmazonSageMakerFullAccess Policy to that default role\nI've subsequently added AmazonS3FullAccess Policy as well<\/p>\n\n<p>I then created a bucket policy specifically granting s3:* access on the specific bucket to that specific role.<\/p>\n\n<p>Heck, I eventually made the bucket public with ListObjects = Yes.<\/p>\n\n<p>os.listdir() simply fails with file or directory not found and a lot message is created with AccessDenied. (TensorFlow libraries just didn't work, so I went with os.listdir() to simplify things.<\/p>\n\n<p>Finally, I test my access from the Policy Simulator - I selected the Role mentioned above, selected to test s3 and selected all 69 items and they all passed.<\/p>\n\n<p>But I continue to log AccessDenied and cannot actually list the contents of a directory from my SageMaker jupyter notebook.<\/p>\n\n<p>I'm at a loss.  Thoughts?<\/p>\n\n<p>EDIT:\nPer suggestion below, I have the following:\nbucket name contains sagemaker: '[redacted]-test-sagemaker'\nPublic access is off, and the only account is my root account.\n<code>\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::[redacted]-test-sagemaker\"\n            ]\n        },\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::[redacted]-test-sagemaker\/*\"\n            ]\n        }\n    ]\n}<\/code>\nand\narn:aws:iam::aws:policy\/AmazonSageMakerFullAccess<\/p>\n\n<p>Finally the bucket policy after the above failed:\n<code>{\n  \"Id\": \"Policy1534116031672\",\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Stmt1534116026409\",\n      \"Action\": \"s3:*\",\n      \"Effect\": \"Allow\",\n      \"Resource\": \"arn:aws:s3:::[redacted]-test-sagemaker\",\n      \"Principal\": {\n        \"AWS\": [\n          \"arn:aws:iam::[id]:role\/service-role\/AmazonSageMaker-ExecutionRole-***\"\n        ]\n      }\n    }\n  ]\n}<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-08-11 19:27:27.103 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-08-12 23:33:16.513 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":2961,
        "Owner_creation_date":"2015-12-09 17:36:29.303 UTC",
        "Owner_last_access_date":"2021-10-15 16:09:46.743 UTC",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>So you need to troubleshoot. Here are a few things to check:   <\/p>\n\n<p>0) Make sure the bucket is in the SageMaker region.<\/p>\n\n<p>1) Include the string \"sagemaker\" in your bucket name (e.g., <em>my_bucketName_here-sagemaker<\/em>, SageMaker has out of the box access to buckets named this way.<\/p>\n\n<p>2) Try using the SageMaker S3 <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/session.py\" rel=\"nofollow noreferrer\">default_bucket()<\/a>:<\/p>\n\n<pre><code>import sagemaker\ns = sagemaker.Session()\ns.upload_data(path='somefile.csv', bucket=s.default_bucket(), key_prefix='data\/train')\n<\/code><\/pre>\n\n<p>3) Open terminal on the Notebook instance, to try to list your bucket using AWS CLI in bash:<\/p>\n\n<pre><code>aws iam get-user\naws s3 ls my_bucketName_here\n<\/code><\/pre>\n\n<p>Finally, pasting the bucket's access and resource policy in your question could help others to answer you.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2018-08-12 06:59:37.853 UTC",
        "Answer_score":1.0,
        "Owner_location":"Phoenix, AZ, United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51803032",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60036916,
        "Question_title":"Sagemaker lifecycle configuration for installing pandas not working",
        "Question_body":"<p>I am trying to update pandas within a lifecycle configuration, and following the example of AWS I have the next code:<\/p>\n\n<pre><code>#!\/bin\/bash\n\nset -e\n\n# OVERVIEW\n# This script installs a single pip package in a single SageMaker conda environments.\n\nsudo -u ec2-user -i &lt;&lt;EOF\n# PARAMETERS\nPACKAGE=pandas\nENVIRONMENT=python3\nsource \/home\/ec2-user\/anaconda3\/bin\/activate \"$ENVIRONMENT\"\npip install --upgrade \"$PACKAGE\"==0.25.3\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\nEOF\n<\/code><\/pre>\n\n<p>Then I attach it to a notebook and when I enter the notebook and open a notebook file, I see that pandas have not been updated. Using <code>!pip show pandas<\/code> I get:<\/p>\n\n<pre><code>Name: pandas\nVersion: 0.24.2\nSummary: Powerful data structures for data analysis, time series, and statistics\nHome-page: http:\/\/pandas.pydata.org\nAuthor: None\nAuthor-email: None\nLicense: BSD\nLocation: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\nRequires: pytz, python-dateutil, numpy\nRequired-by: sparkmagic, seaborn, odo, hdijupyterutils, autovizwidget\n<\/code><\/pre>\n\n<p>So we can see that I am indeed in the python3 env although the version is 0.24. <\/p>\n\n<p>However, the log in cloudwatch shows that it has been installed:<\/p>\n\n<pre><code>Collecting pandas==0.25.3 Downloading https:\/\/files.pythonhosted.org\/packages\/52\/3f\/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d\/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in .\/anaconda3\/lib\/python3.6\/site-packages (from pandas==0.25.3) (2018.4)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: python-dateutil&gt;=2.6.1 in .\/anaconda3\/lib\/python3.6\/site-packages (from pandas==0.25.3) (2.7.3)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: numpy&gt;=1.13.3 in .\/anaconda3\/lib\/python3.6\/site-packages (from pandas==0.25.3) (1.16.4)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: six&gt;=1.5 in .\/anaconda3\/lib\/python3.6\/site-packages (from python-dateutil&gt;=2.6.1-&gt;pandas==0.25.3) (1.13.0)\n2020-02-03T12:33:09.065+01:00\nInstalling collected packages: pandas Found existing installation: pandas 0.24.2 Uninstalling pandas-0.24.2: Successfully uninstalled pandas-0.24.2\n2020-02-03T12:33:12.066+01:00\nSuccessfully installed pandas-0.25.3\n<\/code><\/pre>\n\n<p>What could be the problem? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-03 10:02:36.233 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-02-03 16:14:58.477 UTC",
        "Question_score":2,
        "Question_tags":"python|pandas|amazon-web-services|pip|amazon-sagemaker",
        "Question_view_count":1493,
        "Owner_creation_date":"2018-04-09 18:36:08.403 UTC",
        "Owner_last_access_date":"2022-09-23 12:00:52.963 UTC",
        "Owner_reputation":1754,
        "Owner_up_votes":396,
        "Owner_down_votes":76,
        "Owner_views":197,
        "Answer_body":"<p>if you want to install the  packages only in for the python3 environment, use the following script in your <strong>Create Sagemaker Lifecycle<\/strong> configurations. <\/p>\n\n<pre><code>#!\/bin\/bash\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\n# This will affect only the Jupyter kernel called \"conda_python3\".\nsource activate python3\n\n# Replace myPackage with the name of the package you want to install.\npip install pandas==0.25.3\n# You can also perform \"conda install\" here as well.\nsource deactivate\nEOF\n<\/code><\/pre>\n\n<p>Reference : \"<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">Lifecycle Configuration Best Practices<\/a>\" <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-02-10 16:30:36.82 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60036916",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72068059,
        "Question_title":"cannot load pickle files for xgboost images of version > 1.2-2 in sagemaker - UnpicklingError",
        "Question_body":"<p>I can train a XGBoost model using Sagemaker images like so:<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker.inputs import TrainingInput\nimport os\n\nfolder = r&quot;C:\\Somewhere&quot;\nos.chdir(folder)\n\ns3_prefix = 'some_model'\ns3_bucket_name = 'the_bucket'\ntrain_file_name = 'train.csv'\nval_file_name = 'val.csv'\nrole_arn = 'arn:aws:iam::482777693429:role\/bla_instance_role'\n\nregion_name = boto3.Session().region_name\n\ns3_input_train = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, train_file_name), content_type='csv')\ns3_input_val = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, val_file_name), content_type='csv')\n\nprint(type(s3_input_train))\n\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;13&quot;,\n        &quot;eta&quot;:&quot;0.15&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;objective&quot;:&quot;reg:squarederror&quot;,\n        &quot;num_round&quot;:&quot;50&quot;}\n\noutput_path = 's3:\/\/{}\/{}\/output'.format(s3_bucket_name, s3_prefix)\n\n# 1.5-1\n# 1.3-1\nestimator = sagemaker.estimator.Estimator(image_uri=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region_name, &quot;1.2-2&quot;), \n                                          hyperparameters=hyperparameters,\n                                          role=role_arn,\n                                          instance_count=1, \n                                          instance_type='ml.m5.2xlarge',\n                                          #instance_type='local', \n                                          volume_size=1, # 1 GB \n                                          output_path=output_path)\n\nestimator.fit({'train': s3_input_train, 'validation': s3_input_val})\n<\/code><\/pre>\n<p>This work for all versions 1.2-2, 1.3-1 and 1.5-1. Unfortunately the following code only works for version 1.2-2:<\/p>\n<pre><code>import boto3\nimport os\nimport pickle as pkl \nimport tarfile\nimport pandas as pd\nimport xgboost as xgb\n\nfolder = r&quot;C:\\Somewhere&quot;\nos.chdir(folder)\n\ns3_prefix = 'some_model'\ns3_bucket_name = 'the_bucket'\nmodel_path = 'output\/sagemaker-xgboost-2022-04-30-10-52-29-877\/output\/model.tar.gz'\nsession = boto3.Session(profile_name='default')\nsession.resource('s3').Bucket(s3_bucket_name).download_file('{}\/{}'.format(s3_prefix, model_path), 'model.tar.gz')\nt = tarfile.open('model.tar.gz', 'r:gz')\nt.extractall()\n\nmodel_file_name = 'xgboost-model'\nwith open(model_file_name, &quot;rb&quot;) as input_file:\ne = pkl.load(input_file) \n<\/code><\/pre>\n<p>Otherwise I get a:<\/p>\n<pre><code>_pickle.UnpicklingError: unpickling stack underflow\n<\/code><\/pre>\n<p>Am I missing something? Is my &quot;pickle loading code wrong&quot;?<\/p>\n<p>The version of xgboost is 1.6.0 where I run the pickle code.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-30 11:21:42.787 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|pickle|xgboost|amazon-sagemaker",
        "Question_view_count":140,
        "Owner_creation_date":"2010-03-01 10:53:04.443 UTC",
        "Owner_last_access_date":"2022-09-24 18:56:19.313 UTC",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Answer_body":"<p>I found the solution <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/2952\" rel=\"nofollow noreferrer\">here<\/a>. I will leave it in case someone come accross the same issue.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-04-30 11:32:19.853 UTC",
        "Answer_score":0.0,
        "Owner_location":"Somewhere",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72068059",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71665007,
        "Question_title":"Unable to see VIEWS in SageMaker UI",
        "Question_body":"<p>We have a connection from SageMaker to Snowflake, and need to see <strong>views<\/strong> (as opposed to tables) listed when using Data Wrangler. Is there a reason that views are not shown in the data listing of Data Wrangler?<\/p>\n<p>We have checked security settings and access. These are not materialized views.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-03-29 15:39:45.61 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":57,
        "Owner_creation_date":"2012-08-31 20:08:40.09 UTC",
        "Owner_last_access_date":"2022-09-25 04:17:41.297 UTC",
        "Owner_reputation":11650,
        "Owner_up_votes":6318,
        "Owner_down_votes":21,
        "Owner_views":977,
        "Answer_body":"<p>Currently Data Wrangler does not support browsing Views in the UI but you can still query them with SELECT.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-04-07 00:02:35.44 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71665007",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71777914,
        "Question_title":"Change AWS SageMaker LogGroup Prefix?",
        "Question_body":"<p>We have applications for multiple tenants on our AWS account and would like to distinguish between them in different IAM roles. In most places this is already possible by limiting resource access based on naming patterns.<\/p>\n<p>For CloudWatch log groups of SageMaker training jobs however I have not seen a working solution yet. The tenants can choose the job name arbitrarily, and hence the only part of the LogGroup name that is available for pattern matching would be the prefix before the job name. This prefix however seems to be fixed to <code>\/aws\/sagemaker\/TrainingJobs<\/code>.<\/p>\n<p>Is there a way to change or extend this prefix in order to make such limiting possible? Say, for example <code>\/aws\/sagemaker\/TrainingJobs\/&lt;product&gt;-&lt;stage&gt;-&lt;component&gt;\/&lt;training-job-name&gt;-...<\/code> so that a resource limitation like <code>\/aws\/sagemaker\/TrainingJobs\/&lt;product&gt;-*<\/code> becomes possible?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-07 07:22:33.02 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-cloudwatchlogs",
        "Question_view_count":48,
        "Owner_creation_date":"2009-11-06 19:20:37.563 UTC",
        "Owner_last_access_date":"2022-09-23 13:54:36.657 UTC",
        "Owner_reputation":1904,
        "Owner_up_votes":58,
        "Owner_down_votes":9,
        "Owner_views":321,
        "Answer_body":"<p>I think it is not possible to change the log streams names for any of the SageMaker services.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-04-30 01:17:31.297 UTC",
        "Answer_score":0.0,
        "Owner_location":"Germany",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71777914",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73133746,
        "Question_title":"Fb-Prophet, Apache Spark in Colab and AWS SageMaker\/ Lambda",
        "Question_body":"<p>I am using <code>Google-Colab<\/code> for creating a model by using FbProphet and i am try to use Apache Spark in the <code>Google-Colab<\/code> itself. Now can i upload this <code>Google-colab<\/code> notebook in <code>aws Sagemaker\/Lambda<\/code> for free <code>(without charge for Apache Spark and only charge for AWS SageMaker)<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-27 07:20:40.657 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-08-11 12:28:40.907 UTC",
        "Question_score":1,
        "Question_tags":"apache-spark|google-colaboratory|amazon-sagemaker|facebook-prophet",
        "Question_view_count":51,
        "Owner_creation_date":"2022-07-27 07:13:43.853 UTC",
        "Owner_last_access_date":"2022-09-23 08:24:17.75 UTC",
        "Owner_reputation":152,
        "Owner_up_votes":15,
        "Owner_down_votes":1,
        "Owner_views":11,
        "Answer_body":"<p>In short, You can upload the notebook without any issue into SageMaker. Few things to keep in mind<\/p>\n<ol>\n<li>If you are using the pyspark library in colab and running spark locally,  you should be able to do the same by installing necessary pyspark libs in Sagemaker studio kernels. Here you will only pay for the underlying compute for the notebook instance. If you are experimenting then I would recommend you to use <a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a> to create a free account and try things out.<\/li>\n<li>If you had a separate spark cluster setup then you may need a similar setup in AWS using EMR so that you can connect to the cluster to execute the job.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-27 23:32:23.5 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73133746",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56202697,
        "Question_title":"SageMaker TensorFlow Estimator source code S3 upload path",
        "Question_body":"<p>I'm using the SageMaker TensorFlow estimator for training, and specifying an output path for my model artifacts with the <code>output_path<\/code> argument, with a value of <code>s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\/<\/code>. <\/p>\n\n<p>After model training, a directory named <code>&lt;training_job_name&gt;\/output<\/code> is created in the specified <code>output_path<\/code>. <\/p>\n\n<p>The issue I'm having is, the source code that's used for training is also uploaded to S3 by default, but instead of being placed in <code>s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\/&lt;training_job_name&gt;\/source<\/code>, it's placed in <code>s3:\/\/&lt;bucket&gt;\/&lt;training_job_name&gt;\/source<\/code>. <\/p>\n\n<p>So how can I specify the S3 upload path for the training job's source code in order to make it use the bucket AND prefix name of <code>output_path<\/code>?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-18 20:42:30.767 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-s3|upload|amazon-sagemaker",
        "Question_view_count":645,
        "Owner_creation_date":"2013-02-20 05:47:52.693 UTC",
        "Owner_last_access_date":"2022-09-23 20:45:28.4 UTC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Answer_body":"<p>Have you tried using the \u201ccode_location\u201d argument: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html<\/a> to specify the location for the source code?<\/p>\n\n<p>Below is a snippet code example that use code_location<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ncode-path = \"s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\"\noutput-path = \"s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\"\n\nabalone_estimator = TensorFlow(entry_point='abalone.py',\n                           role=role,\n                           framework_version='1.12.0',\n                           training_steps= 100, \n                           image_name=image,\n                           evaluation_steps= 100,\n                           hyperparameters={'learning_rate': 0.001},\n                           train_instance_count=1,\n                           train_instance_type='ml.c4.xlarge',\n                           code_location= code-path,\n                           output_path = output-path,\n                           base_job_name='my-job-name'\n                           )\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-05-25 07:19:16.47 UTC",
        "Answer_score":4.0,
        "Owner_location":"NYC",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56202697",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57857195,
        "Question_title":"Is it possible to use RStudio IDE on an AWS SageMaker instance",
        "Question_body":"<p>I am trying to use the RStudio IDE to R on an Amazon SageMaker instance. What I have tried so far is to run the following docker command:<\/p>\n\n<pre><code>docker run --rm -p 8787:8787 rocker\/verse\n<\/code><\/pre>\n\n<p>which appears to work successfully. What I would then do when running that command from my local computer is go to <code>http:\/\/localhost:8787<\/code> where I would be able to login and find a fully functional RStudio IDE within my browser. <\/p>\n\n<p>However, this is obviously not possible from within SageMaker as there is no <code>localhost<\/code> to visit.<\/p>\n\n<p>Is there some way I can direct my browser to capture the output to port 8787 from the SageMaker instance? <\/p>\n\n<p>Thanks in advance. <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-09-09 15:40:33.847 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"r|rstudio|amazon-sagemaker",
        "Question_view_count":1935,
        "Owner_creation_date":"2015-07-21 19:31:47.363 UTC",
        "Owner_last_access_date":"2022-09-23 17:55:02.323 UTC",
        "Owner_reputation":553,
        "Owner_up_votes":34,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":"<p>RStudio is now available as a managed service in SageMaker. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/rstudio.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/rstudio.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-11-10 02:07:56.04 UTC",
        "Answer_score":3.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57857195",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71847442,
        "Question_title":"Train an already trained model in Sagemaker and Huggingface without re-initialising",
        "Question_body":"<p>Let's say I have successfully trained a model on some training data for 10 epochs. How can I then access the very same model and train for a further 10 epochs?<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html\" rel=\"nofollow noreferrer\">In the docs<\/a> it suggests &quot;you need to specify a checkpoint output path through hyperparameters&quot; --&gt; how?<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># define my estimator the standard way\nhuggingface_estimator = HuggingFace(\n    entry_point='train.py',\n    source_dir='.\/scripts',\n    instance_type='ml.p3.2xlarge',\n    instance_count=1,\n    role=role,\n    transformers_version='4.10',\n    pytorch_version='1.9',\n    py_version='py38',\n    hyperparameters = hyperparameters,\n    metric_definitions=metric_definitions\n)\n\n# train the model\nhuggingface_estimator.fit(\n    {'train': training_input_path, 'test': test_input_path}\n)\n<\/code><\/pre>\n<p>If I run <code>huggingface_estimator.fit<\/code> again it will just start the whole thing over again and overwrite my previous training.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-12 18:21:41.3 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|huggingface-transformers",
        "Question_view_count":110,
        "Owner_creation_date":"2015-07-16 20:30:51.387 UTC",
        "Owner_last_access_date":"2022-09-23 16:21:10.697 UTC",
        "Owner_reputation":901,
        "Owner_up_votes":150,
        "Owner_down_votes":10,
        "Owner_views":145,
        "Answer_body":"<p>You can find the relevant checkpoint save\/load code in <a href=\"https:\/\/github.com\/huggingface\/notebooks\/blob\/main\/sagemaker\/05_spot_instances\/sagemaker-notebook.ipynb\" rel=\"nofollow noreferrer\">Spot Instances - Amazon SageMaker x Hugging Face Transformers<\/a>.<br \/>\n(The example enables Spot instances, but you can use on-demand).<\/p>\n<ol>\n<li>In hyperparameters you set: <code>'output_dir':'\/opt\/ml\/checkpoints'<\/code>.<\/li>\n<li>You define a <code>checkpoint_s3_uri<\/code> in the Estimator (which is unique to the series of jobs you'll run).<\/li>\n<li>You add code for train.py to support checkpointing:<\/li>\n<\/ol>\n<blockquote>\n<pre><code>from transformers.trainer_utils import get_last_checkpoint\n\n# check if checkpoint existing if so continue training\nif get_last_checkpoint(args.output_dir) is not None:\n    logger.info(&quot;***** continue training *****&quot;)\n    last_checkpoint = get_last_checkpoint(args.output_dir)\n    trainer.train(resume_from_checkpoint=last_checkpoint)\nelse:\n    trainer.train()\n<\/code><\/pre>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-04-13 06:38:31.83 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71847442",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53660590,
        "Question_title":"PySpark Using collect_list to collect Arrays of Varying Length",
        "Question_body":"<p>I am attempting to use collect_list to collect arrays (and maintain order) from two different data frames.<\/p>\n\n<p>Test_Data and Train_Data have the same format.<\/p>\n\n<pre><code>from pyspark.sql import functions as F\nfrom pyspark.sql import Window\n\nw = Window.partitionBy('Group').orderBy('date')\n\n# Train_Data has 4 data points\n# Test_Data has 7 data points\n# desired target array:         [1, 1, 2, 3]\n# desired MarchMadInd array:    [0, 0, 0, 1, 0, 0, 1]\n\nsorted_list_diff_array_lens = train_data.withColumn('target', \nF.collect_list('target').over(w)\n                                  )\\\ntest_data.withColumn('MarchMadInd', F.collect_list('MarchMadInd').over(w))\\\n   .groupBy('Group')\\\n   .agg(F.max('target').alias('target'), \n    F.max('MarchMadInd').alias('MarchMadInd')\n)\n<\/code><\/pre>\n\n<p>I realize the syntax is incorrect with \"test_data.withColumn\", but I want to select the array for the <em>MarchMadInd<\/em> from the <strong>test_date<\/strong>, but the array for the <em>target<\/em> from the <strong>train_data<\/strong>. The desired output would look like the following:<\/p>\n\n<pre><code>{\"target\":[1, 1, 2, 3], \"MarchMadInd\":[0, 0, 0, 1, 0, 0, 1]}\n<\/code><\/pre>\n\n<p>Context: this is for a DeepAR time series model (using AWS) that requires dynamic features to include the prediction period, but the target should be historical data.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2018-12-06 22:22:48.25 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-12-11 13:26:45.89 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|pyspark|amazon-sagemaker",
        "Question_view_count":345,
        "Owner_creation_date":"2017-09-28 19:40:12.487 UTC",
        "Owner_last_access_date":"2022-09-23 21:00:56.663 UTC",
        "Owner_reputation":582,
        "Owner_up_votes":211,
        "Owner_down_votes":0,
        "Owner_views":75,
        "Answer_body":"<p>The solution involves using a join as recommended by pault. <\/p>\n\n<ol>\n<li>Create a dataframe with dynamic features of length equal to Training + Prediction period<\/li>\n<li>Create a dataframe with target values of length equal to just the Training period.<\/li>\n<li>Use a LEFT JOIN (with the dynamic feature data on LEFT) to bring these dataframes together<\/li>\n<\/ol>\n\n<p>Now, using collect_list will create the desired result.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-04-18 15:14:43.377 UTC",
        "Answer_score":0.0,
        "Owner_location":"Denver, CO, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53660590",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53280902,
        "Question_title":"AWS SageMaker pd.read_pickle() doesn't work but read_csv() does?",
        "Question_body":"<p>I've recently been trying to train some models on an AWS SageMaker jupyter notebook instance.<\/p>\n\n<p>Everything is worked very well until I tried to load in some custom dataset (REDD) through files.<\/p>\n\n<p>I have the dataframes stored in Pickle (.pkl) files on an S3 bucket. I couldn't manage to read them into sagemaker so I decided to convert them to csv's as this seemed to work but I ran into a problem. This data has an index of type datetime64 and when using <code>.to_csv()<\/code> this index gets converted to pure text and it loses it's data structure (and I need to keep this specific index for correct plotting.)<\/p>\n\n<p>So I decided to try the Pickle files again but I can't get it to work and have no idea why.<\/p>\n\n<p>The following code for csv's works but I can't use it due to the index problem:<\/p>\n\n<pre><code>bucket = 'sagemaker-peno'\nhouses_dfs = {}\ndata_key = 'compressed_data\/'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\nfor file in range(6):\n    houses_dfs[file+1] = pd.read_csv(data_location+'house_'+str(file+1)+'.csv', index_col='Unnamed: 0')\n<\/code><\/pre>\n\n<p>But this code does NOT work even though it uses almost the exact same syntax:<\/p>\n\n<pre><code>bucket = 'sagemaker-peno'\nhouses_dfs = {}\ndata_key = 'compressed_data\/'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\nfor file in range(6):\n    houses_dfs[file+1] =  pd.read_pickle(data_location+'house_'+str(file+1)+'.pkl')\n<\/code><\/pre>\n\n<p>Yes it's 100% the correct path, because the csv and pkl files are stored in the same directory (compressed_data).<\/p>\n\n<p>It throws me this error while using the Pickle method:<\/p>\n\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/sagemaker-peno\/compressed_data\/house_1.pkl'\n<\/code><\/pre>\n\n<p>I hope to find someone who has dealt with this before and can solve the <code>read_pickle()<\/code> issue or as an alternative fix my datetime64 type issue with csv's.<\/p>\n\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-11-13 12:20:48.037 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"pandas|csv|amazon-s3|pickle|amazon-sagemaker",
        "Question_view_count":788,
        "Owner_creation_date":"2015-08-09 12:51:41.797 UTC",
        "Owner_last_access_date":"2022-07-07 20:29:24.31 UTC",
        "Owner_reputation":87,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Answer_body":"<p>read_pickle() likes the full path more than a relative path from where it was run. This fixed my issue.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-11-26 20:32:53.167 UTC",
        "Answer_score":-1.0,
        "Owner_location":"Leuven, Belgi\u00eb",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53280902",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63304005,
        "Question_title":"sudo: not found on AWS Sagemaker Studio",
        "Question_body":"<p>I was following a guide on mounting EFS in SageMaker studio, but when using the following as a notebook cell:<\/p>\n<pre><code>%%sh \n\nsudo mount -t nfs \\\n    -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 \\\n    172.31.5.227:\/ \\\n    ..\/efs\n\nsudo chmod go+rw ..\/efs\n<\/code><\/pre>\n<p>I get<\/p>\n<pre><code>sh: 2: sudo: not found\nsh: 7: sudo: not found\n<\/code><\/pre>\n<p>Even in the terminal ('image terminal'), sudo is not found: <code># sudo \/bin\/sh: 1: sudo: not found<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-07 14:39:07.697 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-08-07 17:59:24.337 UTC",
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":980,
        "Owner_creation_date":"2017-01-02 15:26:06.803 UTC",
        "Owner_last_access_date":"2022-09-24 21:50:13.153 UTC",
        "Owner_reputation":15819,
        "Owner_up_votes":827,
        "Owner_down_votes":21,
        "Owner_views":1395,
        "Answer_body":"<p>I managed to get sudo working in the &quot;System Terminal&quot; instead. The image terminals don't seem to have access to sudo.<\/p>\n<p>Unrelated: But then when I tried to mount EFS onto the SageMaker studio app, it simply failed, saying mount target is not a directory. Looks like I'm not using Sagemaker Studio this year.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-08-07 14:45:24.96 UTC",
        "Answer_score":0.0,
        "Owner_location":"London, UK",
        "Answer_last_edit_date":"2020-08-07 15:48:01.143 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63304005",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59983062,
        "Question_title":"TSV as input to sagemaker",
        "Question_body":"<p>Is there any way to use a tsv instead of a csv as the input into sagemaker's autopilot ?<\/p>\n\n<p>Currently I'm inputting the data as such:<\/p>\n\n<pre><code>input_data_config = [{\n      'DataSource': {\n        'S3DataSource': {\n          'S3DataType': 'S3Prefix',\n          'S3Uri': 's3:\/\/{}\/{}\/train'.format(bucket,prefix)\n        }\n      },\n      'TargetAttributeName': 'sentiment'\n    }\n  ]\n<\/code><\/pre>\n\n<p>this seems to work file for .csv files but fails for my .tsv files.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-01-30 09:45:11.313 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":204,
        "Owner_creation_date":"2016-11-14 09:23:27.58 UTC",
        "Owner_last_access_date":"2022-09-03 21:38:33.3 UTC",
        "Owner_reputation":2944,
        "Owner_up_votes":177,
        "Owner_down_votes":79,
        "Owner_views":381,
        "Answer_body":"<p>I am a developer at AWS SageMaker. Autopilot currently only supports CSV data. While we are working on extending the support to more file formats: JSON, TSV, etc, this might be something that you can try to convert your .tsv file to .csv:<\/p>\n\n<pre><code>import csv\n\n# read tab-delimited file\nwith open('yourfile.tsv','rb') as fin:\n    cr = csv.reader(fin, delimiter='\\t')\n    filecontents = [line for line in cr]\n\n# write comma-delimited file (comma is the default delimiter)\nwith open('yourfile.csv','wb') as fou:\n    cw = csv.writer(fou, quotechar='', quoting=csv.QUOTE_NONE)\n    cw.writerows(filecontents)\n<\/code><\/pre>\n\n<p>Hope this helps.<\/p>\n\n<p>Ref: <a href=\"https:\/\/stackoverflow.com\/questions\/5590631\/how-to-convert-a-tab-separated-file-to-csv-format\">How to convert a tab separated file to CSV format?<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-02-04 01:08:51.437 UTC",
        "Answer_score":1.0,
        "Owner_location":"Earth",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59983062",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70513398,
        "Question_title":"I can not install \"git-lfs\" on aws sagemaker notebook instance",
        "Question_body":"<p>I Can not run <code>apt to install git-lfs<\/code> on sagemaker notebook instance. I want to run git commands in my notebook.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-28 22:56:44.667 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"git|amazon-sagemaker|apt",
        "Question_view_count":489,
        "Owner_creation_date":"2020-08-06 15:31:21.047 UTC",
        "Owner_last_access_date":"2022-08-22 18:00:08.567 UTC",
        "Owner_reputation":65,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>use the following commands to install git-lfs<\/p>\n<pre><code>!curl -s https:\/\/packagecloud.io\/install\/repositories\/github\/git-lfs\/script.rpm.sh | sudo bash\n\n!sudo yum install git-lfs -y\n\n!git lfs install\n<\/code><\/pre>\n<p>that should make it work<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-12-28 22:59:15.18 UTC",
        "Answer_score":6.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70513398",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59349805,
        "Question_title":"How to find XGBoost containers for different regions in AWS Sagemaker",
        "Question_body":"<p>I'm trying to follow AWS Sagemaker tutorial to train a machine learning model with a Jupyter notebook environment. <\/p>\n\n<p>According to the tutorial, I'm supposed to copy the following code and run it to import required libraries and set environment variables. <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># import libraries\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np                                \nimport pandas as pd                               \nimport matplotlib.pyplot as plt                   \nfrom IPython.display import Image                 \nfrom IPython.display import display               \nfrom time import gmtime, strftime                 \nfrom sagemaker.predictor import csv_serializer   \n\n# Define IAM role\nrole = get_execution_role()\nprefix = 'sagemaker\/DEMO-xgboost-dm'\ncontainers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com\/xgboost:latest',\n              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest',\n              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest',\n              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'} # each region has its XGBoost container\nmy_region = boto3.session.Session().region_name # set the region of the instance\nprint(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")\n<\/code><\/pre>\n\n<p>And the expected outcome is below.  <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/wLJ2j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wLJ2j.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>However, I am getting this error.<\/p>\n\n<blockquote>\n  <p>KeyError                                  Traceback (most recent call last)\n   in ()\n       18               'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'} # each region has its XGBoost container\n       19 my_region = boto3.session.Session().region_name # set the region of the instance\n  ---> 20 print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")<\/p>\n  \n  <p>KeyError: 'ap-northeast-2'<\/p>\n<\/blockquote>\n\n<p>I assume that this is happening because my region is <strong>\"ap-northeast-2\"<\/strong>. \nI have a feeling that I need to change the containers for my region.  <\/p>\n\n<p><strong>If my guess is correct, how can I find containers for my region?<\/strong><br>\n<strong>Also, am I overlooking anything else?<\/strong> <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2019-12-16 02:17:44.1 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":2197,
        "Owner_creation_date":"2018-10-14 22:28:32.483 UTC",
        "Owner_last_access_date":"2022-09-24 07:26:00.847 UTC",
        "Owner_reputation":2954,
        "Owner_up_votes":1127,
        "Owner_down_votes":88,
        "Owner_views":1143,
        "Answer_body":"<p>I expect your rational is correct. There isn't an entry for your region in the code. I don't know if there's a list of these containers per region. That being said, you find them in ECR (Elastic Container Registry). <\/p>\n\n<p>Keep in mind, that you can probably fix this quickly by switching to one of the supported regions. Otherwise:<\/p>\n\n<p>If AWS doesn't have a publicly listed container in your region, you can register the container yourself in AWS with ECR. You'll need to login to ECR using the AWS CLI and docker login.<\/p>\n\n<p>You can use the command <code>aws ecr get-login --region ap-northeast-2<\/code> in order to get the token you'll need for docker login.<\/p>\n\n<p>Then, clone this repo: <a href=\"https:\/\/github.com\/aws\/sagemaker-xgboost-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-xgboost-container<\/a><\/p>\n\n<p>You can build this image locally and push it up to ECR. After that, login to the AWS console (or use the AWS CLI) and find the ARN of the image. It should match the format of the others in your code. <\/p>\n\n<p>After that, just add another key\/value entry into the code for your <code>containers<\/code> variable and use <code>'ap-northeast-2': '&lt;ARN of the docker image&gt;'<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-12-16 02:46:24.063 UTC",
        "Answer_score":1.0,
        "Owner_location":"Seoul, South Korea",
        "Answer_last_edit_date":"2019-12-16 03:11:49.367 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59349805",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64173739,
        "Question_title":"AWS Sagemaker + AWS Lambda",
        "Question_body":"<p>I try to use AWS SageMaker following documentation. I successfully loaded data, trained and deployed the model.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4Mjew.png\" rel=\"nofollow noreferrer\">deployed-model<\/a><\/p>\n<p>My next step have to be using AWS Lambda, connect it to this SageMaker endpoint.\nI saw, that I need to give Lambda IAM execution role permission to invoke a model endpoint.\nI add some data to IAM policy JSON and now it has this view<\/p>\n<pre><code>{\n&quot;Version&quot;: &quot;2012-10-17&quot;,\n&quot;Statement&quot;: [\n    {\n        &quot;Effect&quot;: &quot;Allow&quot;,\n        &quot;Action&quot;: &quot;logs:CreateLogGroup&quot;,\n        &quot;Resource&quot;: &quot;arn:aws:logs:us-east-1:&lt;my-account&gt;:*&quot;\n    },\n    {\n        &quot;Effect&quot;: &quot;Allow&quot;,\n        &quot;Action&quot;: [\n            &quot;logs:CreateLogStream&quot;,\n            &quot;logs:PutLogEvents&quot;\n        ],\n        &quot;Resource&quot;: [\n            &quot;arn:aws:logs:us-east-1:&lt;my-account&gt;:log-group:\/aws\/lambda\/test-sagemaker:*&quot;\n        ]\n    },\n    {\n        &quot;Effect&quot;: &quot;Allow&quot;,\n        &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n        &quot;Resource&quot;: &quot;*&quot;\n    }\n]\n<\/code><\/pre>\n<p>}<\/p>\n<p>Problem that even with role that have permission for invoking SageMaker endpoint my Lambda function didn't see it<\/p>\n<pre><code>An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint xgboost-2020-10-02-12-15-36-097 of account &lt;my-account&gt; not found.: ValidationError\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-02 14:55:47.15 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-10-05 14:04:06.463 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":623,
        "Owner_creation_date":"2020-04-21 22:12:59.987 UTC",
        "Owner_last_access_date":"2022-06-15 18:57:09.433 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":"<p>I found an error by myself. Problem was in different regions. For training and deploying model I used us-east-2 and for lambda I used us-east-1. Just creating all in same region fixed this issue!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-10-05 14:06:22.043 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64173739",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69335737,
        "Question_title":"How to call a python file in aws sagemaker from angular application?",
        "Question_body":"<p>I have created an angular application that takes an image as input, the image is then passed to a python script that performs a neural style transfer and returns the stylized image. I have created the python file and the angular frontend seperately and I'm stuck on the integration. I am using aws sagemaker to run the python script (due to its computation speed) but I have no idea how to call the python script with the image passed to it from angular. Any suggestions would be really appreciated. Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-09-26 14:10:09.997 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|angular|amazon-web-services|amazon-sagemaker",
        "Question_view_count":85,
        "Owner_creation_date":"2020-09-30 06:51:14.66 UTC",
        "Owner_last_access_date":"2022-06-06 07:40:50.62 UTC",
        "Owner_reputation":45,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":"<p>You can create a lambda function and expose it using API gateway to be called by your angular app. this lambda in return will call the  Sagemaker function you have<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-09-26 14:15:52.713 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69335737",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63145277,
        "Question_title":"Sagemaker Training Job Not Uploading\/Saving Training Model to S3 Output Path",
        "Question_body":"<p>Ok I've been dealing with this issue in Sagemaker for almost a week and I'm ready to pull my hair out. I've got a custom training script paired with a data processing script in a BYO algorithm Docker deployment type scenario. It's a Pytorch model built with Python 3.x, and the BYO Docker file was originally built for Python 2, but I can't see an issue with the problem that I am having.....which is that after a successful training run Sagemaker doesn't save the model to the target S3 bucket.<\/p>\n<p>I've searched far and wide and can't seem to find an applicable answer anywhere. This is all done inside a Notebook instance. Note: I am using this as a contractor and don't have full permissions to the rest of AWS, including downloading the Docker image.<\/p>\n<p>Dockerfile:<\/p>\n<pre><code>FROM ubuntu:18.04\n\nMAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python-pip \\\n         python3-pip3\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\nRUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python3 get-pip.py &amp;&amp; \\\n    pip3 install future numpy torch scipy scikit-learn pandas flask gevent gunicorn &amp;&amp; \\\n        rm -rf \/root\/.cache\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=&quot;\/opt\/program:${PATH}&quot;\n\nCOPY decision_trees \/opt\/program\nWORKDIR \/opt\/program\n<\/code><\/pre>\n<p>Docker Image Build:<\/p>\n<pre><code>%%sh\n\nalgorithm_name=&quot;name-this-algo&quot;\n\ncd container\n\nchmod +x decision_trees\/train\nchmod +x decision_trees\/serve\n\naccount=$(aws sts get-caller-identity --query Account --output text)\n\nregion=$(aws configure get region)\nregion=${region:-us-east-2}\n\nfullname=&quot;${account}.dkr.ecr.${region}.amazonaws.com\/${algorithm_name}:latest&quot;\n\naws ecr describe-repositories --repository-names &quot;${algorithm_name}&quot; &gt; \/dev\/null 2&gt;&amp;1\n\nif [ $? -ne 0 ]\nthen\n    aws ecr create-repository --repository-name &quot;${algorithm_name}&quot; &gt; \/dev\/null\nfi\n\n# Get the login command from ECR and execute it directly\n$(aws ecr get-login --region ${region} --no-include-email)\n\n# Build the docker image locally with the image name and then push it to ECR\n# with the full name.\n\ndocker build  -t ${algorithm_name} .\ndocker tag ${algorithm_name} ${fullname}\n\ndocker push ${fullname}\n<\/code><\/pre>\n<p>Env setup and session start:<\/p>\n<pre><code>common_prefix = &quot;pytorch-lstm&quot;\ntraining_input_prefix = common_prefix + &quot;\/training-input-data&quot;\nbatch_inference_input_prefix = common_prefix + &quot;\/batch-inference-input-data&quot;\n\nimport os\nfrom sagemaker import get_execution_role\nimport sagemaker as sage\n\nsess = sage.Session()\n\nrole = get_execution_role()\nprint(role)\n<\/code><\/pre>\n<p>Training Directory, Image, and Estimator Setup, then a <code>fit<\/code> call:<\/p>\n<pre><code>TRAINING_WORKDIR = &quot;a\/local\/directory&quot;\n\ntraining_input = sess.upload_data(TRAINING_WORKDIR, key_prefix=training_input_prefix)\nprint (&quot;Training Data Location &quot; + training_input)\n\naccount = sess.boto_session.client('sts').get_caller_identity()['Account']\nregion = sess.boto_session.region_name\nimage = '{}.dkr.ecr.{}.amazonaws.com\/image-that-works:working'.format(account, region)\n\ntree = sage.estimator.Estimator(image,\n                       role, 1, 'ml.p2.xlarge',\n                       output_path=&quot;s3:\/\/sagemaker-directory-that-definitely\/exists&quot;,\n                       sagemaker_session=sess)\n\ntree.fit(training_input)\n<\/code><\/pre>\n<p>The above script is working, for sure. I have print statements in my script and they are printing the expected results to the console. This runs as it's supposed to, finishes up, and says that it's deploying model artifacts when IT DEFINITELY DOES NOT.<\/p>\n<p>Model Deployment:<\/p>\n<pre><code>model = tree.create_model()\npredictor = tree.deploy(1, 'ml.m4.xlarge')\n<\/code><\/pre>\n<p>This throws an error that the model can't be found. A call to <code>aws sagemaker describe-training-job<\/code> shows that the training was completed but I found that the time it took to upload the model was super fast, so obviously there's an error somewhere and it's not telling me. Thankfully it's not just uploading it to the aether.<\/p>\n<pre><code>{\n            &quot;Status&quot;: &quot;Uploading&quot;,\n            &quot;StartTime&quot;: 1595982984.068,\n            &quot;EndTime&quot;: 1595982989.994,\n            &quot;StatusMessage&quot;: &quot;Uploading generated training model&quot;\n        },\n<\/code><\/pre>\n<p>Here's what I've tried so far:<\/p>\n<ol>\n<li>I've tried uploading it to a different bucket. I figured my permissions were the problem so I pointed it to one that I new allowed me to upload as I had done it before to that bucket. No dice.<\/li>\n<li>I tried backporting the script to Python 2.x, but that caused more problems than it probably would have solved, and I don't really see how that would be the problem anyways.<\/li>\n<li>I made sure the Notebook's IAM role has sufficient permissions, and it does have a SagemakerFullAccess policy<\/li>\n<\/ol>\n<p>What bothers me is that there's no error log I can see. If I could be directed to that I would be happy too, but if there's some hidden Sagemaker kungfu that I don't know about I would be forever grateful.<\/p>\n<hr \/>\n<p>EDIT<\/p>\n<p>The training job runs and prints to both the Jupyter cell and CloudWatch as expected. I've since lost the cell output in the notebook but below is the last few lines in CloudWatch. The first number is the epoch and the rest are various custom model metrics.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/y3I9L.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/y3I9L.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-29 02:05:24.557 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-07-29 16:23:57.757 UTC",
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|pytorch|amazon-sagemaker",
        "Question_view_count":2438,
        "Owner_creation_date":"2013-02-10 22:40:48.187 UTC",
        "Owner_last_access_date":"2022-09-23 16:38:41.943 UTC",
        "Owner_reputation":45,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Answer_body":"<p>Can you verify from the training job logs that your training script is running? It doesn't look like your Docker image would respond to the command <code>train<\/code>, which is what SageMaker requires, and so I suspect that your model isn't actually getting trained\/saved to <code>\/opt\/ml\/model<\/code>.<\/p>\n<p>AWS documentation about how SageMaker runs the Docker container: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html<\/a><\/p>\n<p>edit: summarizing from the comments below - the training script must also save the model to <code>\/opt\/ml\/model<\/code> (the model isn't saved automatically).<\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_date":"2020-07-29 16:14:30.563 UTC",
        "Answer_score":1.0,
        "Owner_location":"Miami, FL, USA",
        "Answer_last_edit_date":"2021-04-29 21:01:48.173 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63145277",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59773503,
        "Question_title":"Using Sagemaker predictor in a Spark UDF function",
        "Question_body":"<p>I am trying to run inference on a Tensorflow model deployed on SageMaker from a Python Spark job.\nI am running a (Databricks) notebook which has the following cell:<\/p>\n\n<pre><code>def call_predict():\n        batch_size = 1\n        data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n        tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n        prediction = predictor.predict(tensor_proto)\n        print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n<\/code><\/pre>\n\n<p>If I just call call_predict() it works fine:<\/p>\n\n<pre><code>call_predict()\n<\/code><\/pre>\n\n<p>and I get the output:<\/p>\n\n<pre><code>Process time: 65.261396\nOut[61]: {'model_spec': {'name': u'generic_model',\n  'signature_name': u'serving_default',\n  'version': {'value': 1578909324L}},\n 'outputs': {u'ages': {'dtype': 1,\n   'float_val': [5.680944442749023],\n   'tensor_shape': {'dim': [{'size': 1L}]}}}}\n<\/code><\/pre>\n\n<p>but when I try to call from a Spark context (in a UDF) I get a serialization error.\nThe code I'm trying to run is:<\/p>\n\n<pre><code>dataRange = range(1, 10001)\nrangeRDD = sc.parallelize(dataRange, 8)\nnew_data = rangeRDD.map(lambda x : call_predict())\nnew_data.count()\n<\/code><\/pre>\n\n<p>and the error I get is:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nPicklingError                             Traceback (most recent call last)\n&lt;command-2282434&gt; in &lt;module&gt;()\n      2 rangeRDD = sc.parallelize(dataRange, 8)\n      3 new_data = rangeRDD.map(lambda x : call_predict())\n----&gt; 4 new_data.count()\n      5 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in count(self)\n   1094         3\n   1095         \"\"\"\n-&gt; 1096         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n   1097 \n   1098     def stats(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in sum(self)\n   1085         6.0\n   1086         \"\"\"\n-&gt; 1087         return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n   1088 \n   1089     def count(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in fold(self, zeroValue, op)\n    956         # zeroValue provided to each partition is unique from the one provided\n    957         # to the final reduce call\n--&gt; 958         vals = self.mapPartitions(func).collect()\n    959         return reduce(op, vals, zeroValue)\n    960 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in collect(self)\n    829         # Default path used in OSS Spark \/ for non-credential passthrough clusters:\n    830         with SCCallSiteSync(self.context) as css:\n--&gt; 831             sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n    832         return list(_load_from_socket(sock_info, self._jrdd_deserializer))\n    833 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _jrdd(self)\n   2573 \n   2574         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n-&gt; 2575                                       self._jrdd_deserializer, profiler)\n   2576         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n   2577                                              self.preservesPartitioning, self.is_barrier)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _wrap_function(sc, func, deserializer, serializer, profiler)\n   2475     assert serializer, \"serializer should not be empty\"\n   2476     command = (func, profiler, deserializer, serializer)\n-&gt; 2477     pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n   2478     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n   2479                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _prepare_for_python_RDD(sc, command)\n   2461     # the serialized command will be compressed by broadcast\n   2462     ser = CloudPickleSerializer()\n-&gt; 2463     pickled_command = ser.dumps(command)\n   2464     if len(pickled_command) &gt; sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  # Default 1M\n   2465         # The broadcast will have same life cycle as created PythonRDD\n\n\/databricks\/spark\/python\/pyspark\/serializers.pyc in dumps(self, obj)\n    709                 msg = \"Could not serialize object: %s: %s\" % (e.__class__.__name__, emsg)\n    710             cloudpickle.print_exec(sys.stderr)\n--&gt; 711             raise pickle.PicklingError(msg)\n    712 \n    713 \n\nPicklingError: Could not serialize object: TypeError: can't pickle _ssl._SSLSocket objects\n<\/code><\/pre>\n\n<p>Not sure what is this serialization error - does is complain about failing to deserialize the Predictor<\/p>\n\n<p>My notebook has a cell which was called prior to the above cells with the following imports:<\/p>\n\n<pre><code>import sagemaker\nimport boto3\nfrom sagemaker.tensorflow.model import TensorFlowPredictor\nimport tensorflow as tf\nimport numpy as np\nimport time\n<\/code><\/pre>\n\n<p>The Predictor was created with the following code:<\/p>\n\n<pre><code>sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY,\n                                aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\nsagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY,\n                                        aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\nboto_session = boto3.Session(region_name='us-east-1')\nsagemaker_session = sagemaker.Session(boto_session, sagemaker_client=sagemaker_client, sagemaker_runtime_client=sagemaker_runtime_client)\n\npredictor = TensorFlowPredictor('endpoint-poc', sagemaker_session)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-01-16 16:00:15.88 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"tensorflow|pyspark|amazon-sagemaker",
        "Question_view_count":322,
        "Owner_creation_date":"2016-03-21 08:49:39.92 UTC",
        "Owner_last_access_date":"2022-07-17 11:53:14.84 UTC",
        "Owner_reputation":383,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Answer_body":"<p>The udf function will be executed by multiple spark tasks in parallel. Those tasks run in completely isolated python processes and they are scheduled to physically different machines. Hence each data, those functions reference, must be on the same node. This is the case for everything created within the udf.<\/p>\n\n<p>Whenever you reference any object outside of the udf from the function, this data structure needs to be serialised (pickled) to each executor. Some object state, like open connections to a socket, cannot be pickled.<\/p>\n\n<p>You need to make sure, that connections are lazily opened each executor. It must happen only on the first function call on that executor. The <a href=\"https:\/\/spark.apache.org\/docs\/latest\/streaming-programming-guide.html#design-patterns-for-using-foreachrdd\" rel=\"nofollow noreferrer\">connection pooling topic<\/a> is covered in the docs, however only in the spark streaming guide (though it also applies for normal batch jobs).<\/p>\n\n<p>Normally one can use the Singleton Pattern for this. But in python people use the Borgh pattern.<\/p>\n\n<pre><code>class Env:\n    _shared_state = {\n        \"sagemaker_client\": None\n        \"sagemaker_runtime_client\": None\n        \"boto_session\": None\n        \"sagemaker_session\": None\n        \"predictor\": None\n    }\n    def __init__(self):\n        self.__dict__ = self._shared_state\n        if not self.predictor:\n            self.sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n            self.sagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\n            self.boto_session = boto3.Session(region_name='us-east-1')\n            self.sagemaker_session = sagemaker.Session(self.boto_session, sagemaker_client=self.sagemaker_client, sagemaker_runtime_client=self.sagemaker_runtime_client)\n\n            self.predictor = TensorFlowPredictor('endpoint-poc', self.sagemaker_session)\n\n\n#....\ndef call_predict():\n   env = Env()\n   batch_size = 1\n   data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n   tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n   prediction = env.predictor.predict(tensor_proto)\n\n   print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n\nnew_data = rangeRDD.map(lambda x : call_predict())\n<\/code><\/pre>\n\n<p>The Env class is defined on the master node. Its <code>_shared_state<\/code> has empty entries. When then Env object is instantiated first time, it shares the state with all further instances of Env on any subsequent call to the udf. On each separate parallel running process this will happen exactly one time. This way the sessions are shared and do not need to pickled. <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-01-16 20:30:04.95 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2020-01-20 07:54:05.7 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59773503",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70191668,
        "Question_title":"What are SageMaker pipelines actually?",
        "Question_body":"<p>Sagemaker pipelines are rather unclear to me, I'm not experienced in the field of ML but I'm working on figuring out the pipeline definitions.<\/p>\n<p>I have a few questions:<\/p>\n<ul>\n<li><p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/li>\n<li><p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/li>\n<li><p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/li>\n<\/ul>\n<p>I can't seem to find any examples besides the Python SDK usage, how come?<\/p>\n<p>The docs and workshops seem only to properly describe the Python SDK usage,it would be really helpful if someone could clear this up for me!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-01 21:50:43.06 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-sagemaker|mlops",
        "Question_view_count":716,
        "Owner_creation_date":"2020-01-05 18:52:39.257 UTC",
        "Owner_last_access_date":"2022-09-18 18:55:51.313 UTC",
        "Owner_reputation":197,
        "Owner_up_votes":53,
        "Owner_down_votes":7,
        "Owner_views":49,
        "Answer_body":"<p>SageMaker has two things called Pipelines: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pipelines.html\" rel=\"nofollow noreferrer\">Model Building Pipelines<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">Serial Inference Pipelines<\/a>. I believe you're referring to the former<\/p>\n<p>A model building pipeline defines steps in a machine learning workflow, such as pre-processing, hyperparameter tuning, batch transformations, and setting up endpoints<\/p>\n<p>A serial inference pipeline is two or more SageMaker models run one after the other<\/p>\n<p>A model building pipeline is defined in JSON, and is hosted\/run in some sort of proprietary, serverless fashion by SageMaker<\/p>\n<blockquote>\n<p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/blockquote>\n<p>You can create\/modify them using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreatePipeline.html\" rel=\"nofollow noreferrer\">API<\/a>, which can also be called via the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-pipeline.html\" rel=\"nofollow noreferrer\">CLI<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#sagemaker.workflow.pipeline.Pipeline.create\" rel=\"nofollow noreferrer\">Python SDK<\/a>, or <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html\" rel=\"nofollow noreferrer\">CloudFormation<\/a>. These all use the AWS API under the hood<\/p>\n<p>You can start\/stop\/view them in SageMaker Studio:<\/p>\n<pre><code>Left-side Navigation bar &gt; SageMaker resources &gt; Drop-down menu &gt; Pipelines\n<\/code><\/pre>\n<blockquote>\n<p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/blockquote>\n<p>Unlikely. CodePipeline is more for building and deploying code, not specific to SageMaker. There is no direct integration as far as I can tell, other than that you can start a SM pipeline with CP<\/p>\n<blockquote>\n<p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/blockquote>\n<p>The Python SDK is a stand-alone library to interact with SageMaker in a developer-friendly fashion. It's more dynamic than CloudFormation. Let's you build pipelines using code. Whereas CloudFormation takes a static JSON string<\/p>\n<p>A very simple example of Python SageMaker SDK usage:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>processor = SKLearnProcessor(\n    framework_version=&quot;0.23-1&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.large&quot;,\n    role=&quot;role-arn&quot;,\n)\n\nprocessing_step = ProcessingStep(\n    name=&quot;processing&quot;,\n    processor=processor,\n    code=&quot;preprocessor.py&quot;\n)\n\npipeline = Pipeline(name=&quot;foo&quot;, steps=[processing_step])\npipeline.upsert(role_arn = ...)\npipeline.start()\n<\/code><\/pre>\n<p><code>pipeline.definition()<\/code> produces rather verbose JSON like this:<\/p>\n\n<pre class=\"lang-json prettyprint-override\"><code>{\n&quot;Version&quot;: &quot;2020-12-01&quot;,\n&quot;Metadata&quot;: {},\n&quot;Parameters&quot;: [],\n&quot;PipelineExperimentConfig&quot;: {\n    &quot;ExperimentName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineName&quot;\n    },\n    &quot;TrialName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineExecutionId&quot;\n    }\n},\n&quot;Steps&quot;: [\n    {\n        &quot;Name&quot;: &quot;processing&quot;,\n        &quot;Type&quot;: &quot;Processing&quot;,\n        &quot;Arguments&quot;: {\n            &quot;ProcessingResources&quot;: {\n                &quot;ClusterConfig&quot;: {\n                    &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n                    &quot;InstanceCount&quot;: 1,\n                    &quot;VolumeSizeInGB&quot;: 30\n                }\n            },\n            &quot;AppSpecification&quot;: {\n                &quot;ImageUri&quot;: &quot;246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3&quot;,\n                &quot;ContainerEntrypoint&quot;: [\n                    &quot;python3&quot;,\n                    &quot;\/opt\/ml\/processing\/input\/code\/preprocessor.py&quot;\n                ]\n            },\n            &quot;RoleArn&quot;: &quot;arn:aws:iam::123456789012:role\/foo&quot;,\n            &quot;ProcessingInputs&quot;: [\n                {\n                    &quot;InputName&quot;: &quot;code&quot;,\n                    &quot;AppManaged&quot;: false,\n                    &quot;S3Input&quot;: {\n                        &quot;S3Uri&quot;: &quot;s3:\/\/bucket\/preprocessor.py&quot;,\n                        &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/input\/code&quot;,\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3InputMode&quot;: &quot;File&quot;,\n                        &quot;S3DataDistributionType&quot;: &quot;FullyReplicated&quot;,\n                        &quot;S3CompressionType&quot;: &quot;None&quot;\n                    }\n                }\n            ]\n        }\n    }\n  ]\n}\n<\/code><\/pre>\n<p>You could <em>use<\/em> the above JSON with CloudFormation\/CDK, but you <em>build<\/em> the JSON with the SageMaker SDK<\/p>\n<p>You can also define model building workflows using Step Function State Machines, using the <a href=\"https:\/\/aws-step-functions-data-science-sdk.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">Data Science SDK<\/a>, or <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/airflow\/index.html\" rel=\"nofollow noreferrer\">Airflow<\/a><\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_date":"2021-12-01 22:01:10.903 UTC",
        "Answer_score":2.0,
        "Owner_location":"Amsterdam",
        "Answer_last_edit_date":"2022-06-02 19:24:10.167 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70191668",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73200116,
        "Question_title":"AWS Sagemaker integration with mongodb and lambda",
        "Question_body":"<p>I'm looking for some advice from anyone who's tried aws Sagemaker. I'm very new to this and would appreciate anyone kind enough to help me out.<\/p>\n<p>I have created a basic time series project in a Sagemaker notebook. It trains the model on CSV file data and tests it, with good results.<\/p>\n<p>The data I am using is based on store profits. I am predicting the profit each week.<\/p>\n<p>However, my question is, how can I pass new store sales data into this model each week (only one day a week), and retrain it with the new weeks data (so it can notice any new patterns), then for it to predict the next week profit for each store?<\/p>\n<p>All my store data is synced into mongodb, so I'm presuming I would need a lambda function to get this data and pass it over to the Sagemaker model.<\/p>\n<p>Is it worth retraining the model every week? As I have years worth of store data? Or should I just pass over the old data with the new data added in for it to predict? How do I pass over this data? In a lambda function with a cloud event to make it run automatically every week?<\/p>\n<p>Can I write the predictions back into mongodb in a new table, or are they saved somewhere else first and this would have to be another lambda function?<\/p>\n<p>I have looked at so many tutorials, but none of them seem to explain how I can connect everything up and have the model make predictions automatically and then save them in a dB.<\/p>\n<p>Many thanks in advance to anyone who can explain this to me! Sorry for such a long question!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-01 22:17:07.39 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"mongodb|amazon-web-services|aws-lambda|time-series|amazon-sagemaker",
        "Question_view_count":58,
        "Owner_creation_date":"2022-05-31 07:56:46.887 UTC",
        "Owner_last_access_date":"2022-09-21 19:38:27.443 UTC",
        "Owner_reputation":67,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Answer_body":"<p>Recently I have completed similar use case and here's my answers -<\/p>\n<p><strong>Q1  : Is there need of retraining every week?<\/strong><\/p>\n<p>Ans : Yes, you need to do continuous training and continuous forecasting steps (tie using sagemaker pipeline) in prod to make it work perfectly automated for stable MAE, MAPE etc.<\/p>\n<p><strong>Q2  : How can I pass new data and forecast for next week? How to get input data from mongodb?<\/strong><\/p>\n<p>Ans : You could use Lambda, or Glue job (designed for ETL so better) to drop in S3 bucket. This will could become input raw data bucket for sagemaker pipeline.<\/p>\n<p><strong>Q3  : Can I write the predictions back into mongodb in a new table, or are they saved somewhere else first and this would have to be another lambda function?<\/strong><\/p>\n<p>Ans : Yes you can, both ways.<\/p>\n<p>I would suggest to start small i.e. First drop a csv file to s3 location in say YYMMDD folder. Use this as input and develop completely in one notebook (continuous train, continuous forecast).<\/p>\n<p>Later, learn about pipelines - how to write different steps, pass objects between steps etc and go modify your code to fit in pipeline.<\/p>\n<p>Create a sagemaker pipeline with steps : (Refer links below )<\/p>\n<ol>\n<li>Preprocess ( any transformations, cleansing )<\/li>\n<li>Training ( use prebuilt image or need to build one, depends)<\/li>\n<li>Forecast ( either do batch transform or deploy to endpoint and later delete )<\/li>\n<li>Post processing ( if required )<\/li>\n<\/ol>\n<p>Take the output the from sagemaker pipelines to mongodb. <strong>Sagemaker Pipelines help automate scheduled execution using AWS Event Bridge<\/strong><\/p>\n<p>Some references :<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-pipelines\/tabular\/abalone_build_train_deploy\/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.ipynb\" rel=\"nofollow noreferrer\">Example Pipelines<\/a> Look here in to know about pipelines<\/p>\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-forecasting-air-pollution-with-deepar\/blob\/5e29057a8c9d7f8db6cb1143f7838b6614f44ef1\/01_train_and_evaluate_air_quality_deepar_model.ipynb\" rel=\"nofollow noreferrer\">Example1 DeepAR<\/a><\/p>\n<p><a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/introduction_to_amazon_algorithms\/deepar_electricity\/DeepAR-Electricity.html?highlight=deepar\" rel=\"nofollow noreferrer\">Example2 DeepAR<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-03 11:36:34.223 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73200116",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70240640,
        "Question_title":"xgboost sagemaker train failure",
        "Question_body":"<p>I have type error when I run for training on sagemaker by using xgboost conatiner.\nPlease advise me to fix the issue.<\/p>\n<pre><code>container = 'southeast-2','783357654285.dkr.ecr.ap-southeast-2.amazonaws.com\/sagemaker- xgboost:latest'`\n\ntrain_input = TrainingInput(s3_data='s3:\/\/{}\/train'.format(bucket, prefix), content_type='csv')\nvalidation_input = TrainingInput(s3_data='s3:\/\/{}\/validation\/'.format(bucket, prefix), content_type='csv')\n\nsess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(\ncontainer,\nrole, \ninstance_count=1,\ninstance_type='ml.t2.medium',\noutput_path='s3:\/\/{}\/output'.format(bucket, prefix),\nsagemaker_session=sess\n)\n\nxgb.set_hyperparameters(\nmax_depth=5,\neta=0.1,\ngamma=4,\nmin_child_weight=6,\nsubsample=0.8,\nsilent=0,\nobjective=&quot;binary:logistic&quot;,\nnum_round=25,\n)\n\nxgb.fit({&quot;train&quot;: train_input, &quot;validation&quot;: validation_input})\n<\/code><\/pre>\n<hr \/>\n<p>TypeError                                 Traceback (most recent call last)\n in \n21 )\n22\n---&gt; 23 xgb.fit({&quot;train&quot;: train_input, &quot;validation&quot;: validation_input})<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n685                 * <code>TrialComponentDisplayName<\/code> is used for display in Studio.\n686         &quot;&quot;&quot;\n--&gt; 687         self._prepare_for_training(job_name=job_name)\n688\n689         self.latest_training_job = _TrainingJob.start_new(self, inputs, experiment_config)<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in _prepare_for_training(self, job_name)\n446                 constructor if applicable.\n447         &quot;&quot;&quot;\n--&gt; 448         self._current_job_name = self._get_or_create_name(job_name)\n449\n450         # if output_path was specified we use it otherwise initialize here.<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in _get_or_create_name(self, name)\n435             return name\n436\n--&gt; 437         self._ensure_base_job_name()\n438         return name_from_base(self.base_job_name)\n439<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in _ensure_base_job_name(self)\n420         # honor supplied base_job_name or generate it\n421         if self.base_job_name is None:\n--&gt; 422             self.base_job_name = base_name_from_image(self.training_image_uri())\n423\n424     def _get_or_create_name(self, name=None):<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/utils.py in base_name_from_image(image)\n95         str: Algorithm name, as extracted from the image name.\n96     &quot;&quot;&quot;\n---&gt; 97     m = re.match(&quot;^(.+\/)?([^:\/]+)(:[^:]+)?$&quot;, image)\n98     algo_name = m.group(2) if m else image\n99     return algo_name<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/re.py in match(pattern, string, flags)\n170     &quot;&quot;&quot;Try to apply the pattern at the start of the string, returning\n171     a match object, or None if no match was found.&quot;&quot;&quot;\n--&gt; 172     return _compile(pattern, flags).match(string)\n173\n174 def fullmatch(pattern, string, flags=0):<\/p>\n<p>TypeError: expected string or bytes-like object<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-06 03:44:13.813 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"xgboost|amazon-sagemaker",
        "Question_view_count":130,
        "Owner_creation_date":"2021-10-27 04:55:23.497 UTC",
        "Owner_last_access_date":"2022-04-02 03:02:17.893 UTC",
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<pre><code>import sagemaker\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.serializers import CSVSerializer\nfrom sagemaker.session import TrainingInput\nfrom sagemaker import image_uris\nfrom sagemaker.session import Session\n\n# initialize hyperparameters\nhyperparameters = {\n    &quot;max_depth&quot;:&quot;5&quot;,\n    &quot;eta&quot;:&quot;0.1&quot;,\n    &quot;gamma&quot;:&quot;4&quot;,\n    &quot;min_child_weight&quot;:&quot;6&quot;,\n    &quot;subsample&quot;:&quot;0.7&quot;,\n    &quot;objective&quot;:&quot;binary:logistic&quot;,\n    &quot;num_round&quot;:&quot;25&quot;}\n\n# set an output path where the trained model will be saved\nbucket = sagemaker.Session().default_bucket()\noutput_path = 's3:\/\/{}\/{}\/output'.format(bucket, 'rain-xgb-built-in-algo')\n\n\n# this line automatically looks for the XGBoost image URI and builds an \nXGBoost container.\n# specify the repo_version depending on your preference.\nxgboost_container = sagemaker.image_uris.retrieve(&quot;xgboost&quot;, 'ap-southeast- \n2', &quot;1.3-1&quot;)\n\n\n# construct a SageMaker estimator that calls the xgboost-container\nestimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n                                      hyperparameters=hyperparameters,\n                                      role=sagemaker.get_execution_role(),\n                                      instance_count=1, \n                                      instance_type='ml.m5.large', \n                                      volume_size=5, # 5 GB \n                                      output_path=output_path)\n\n\n\n # define the data type and paths to the training and validation datasets\n\n train_input = TrainingInput(&quot;s3:\/\/{}\/{}\/&quot;.format(bucket,'train'), \n content_type='csv')\n validation_input = TrainingInput(&quot;s3:\/\/{}\/{}&quot;.format(bucket,'validation'), \n content_type='csv')\n\n\n # execute the XGBoost training job\n estimator.fit({'train': train_input, 'validation': validation_input})\n<\/code><\/pre>\n<p>I have rewritten as above and could run training.\nthank you !<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-12-06 05:39:34.62 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70240640",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67020040,
        "Question_title":"Sagemaker Pytorch model - An error occurred (InternalFailure) when calling the InvokeEndpoint operation (reached max retries: 4):",
        "Question_body":"<p>I am facing an issue while invoking the Pytorch model Endpoint. Please check the below error for detail.<\/p>\n<p>Error Message:<\/p>\n<blockquote>\n<p>An error occurred (InternalFailure) when calling the InvokeEndpoint operation (reached max retries: 4): An exception occurred while sending request to model. Please contact customer support regarding request 9d4f143b-497f-47ce-9d45-88c697c4b0c4.<\/p>\n<\/blockquote>\n<p>Automatically restarted the Endpoint after this error. No specific log in cloud watch.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-09 11:11:46.043 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-09-15 12:28:49.473 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":407,
        "Owner_creation_date":"2020-02-20 05:53:43.44 UTC",
        "Owner_last_access_date":"2022-03-28 10:51:37.613 UTC",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>There may be a few issues here we can explore the paths and ways to resolve.<\/p>\n<ol>\n<li>Inference Code Error\nSometimes these errors occur when your payload or what you're feeding your endpoint is not in the appropriate format. When invoking the endpoint you want to make sure your data is in the correct format\/encoded properly. For this you can use the serializer SageMaker provides when creating the endpoint. The serializer takes care of encoding for you and sends data in the appropriate format. Look at the following code snippet.<\/li>\n<\/ol>\n<pre><code>from sagemaker.predictor import csv_serializer\nrf_pred = rf.deploy(1, &quot;ml.m4.xlarge&quot;, serializer=csv_serializer)\nprint(rf_pred.predict(payload).decode('utf-8'))\n<\/code><\/pre>\n<p>For more information about the different serializers based off the type of data you are feeding in check the following link.\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html<\/a><\/p>\n<ol start=\"2\">\n<li>Throttling Limits Reached\nSometimes the payload you are feeding in may be too large or the API request rate may have been exceeded for the endpoint so experiment with a more compute heavy instance or increase retries in your boto3 configuration. Here is a link for an example of what retries are and configuring them for your endpoint.<\/li>\n<\/ol>\n<p><a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/sagemaker-python-throttlingexception\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/sagemaker-python-throttlingexception\/<\/a><\/p>\n<p>I work for AWS &amp; my opinions are my own<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-07-22 17:58:53.377 UTC",
        "Answer_score":0.0,
        "Owner_location":"Palanpur, Gujarat, India",
        "Answer_last_edit_date":"2021-07-22 18:42:09.227 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67020040",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63115867,
        "Question_title":"Isolation Forest vs Robust Random Cut Forest in outlier detection",
        "Question_body":"<p>I am examining different methods in outlier detection. I came across sklearn's implementation of Isolation Forest and Amazon sagemaker's implementation of RRCF (Robust Random Cut Forest). Both are ensemble methods based on decision trees, aiming to isolate every single point. The more isolation steps there are, the more likely the point is to be an inlier, and the opposite is true.<\/p>\n<p>However, even after looking at the original papers of the algorithms, I am failing to understand exactly the difference between both algorithms. In what way do they work differently? Is one of them more efficient than the other?<\/p>\n<p>EDIT: I am adding the links to the research papers for more information, as well as some tutorials discussing the topics.<\/p>\n<p>Isolation Forest:<\/p>\n<p><a href=\"https:\/\/cs.nju.edu.cn\/zhouzh\/zhouzh.files\/publication\/icdm08b.pdf?q=isolation-forest\" rel=\"noreferrer\">Paper<\/a> <a href=\"https:\/\/towardsdatascience.com\/outlier-detection-with-isolation-forest-3d190448d45e\" rel=\"noreferrer\">Tutorial<\/a><\/p>\n<p>Robust Random Cut Forest:<\/p>\n<p><a href=\"http:\/\/proceedings.mlr.press\/v48\/guha16.pdf\" rel=\"noreferrer\">Paper<\/a> <a href=\"https:\/\/freecontent.manning.com\/the-randomcutforest-algorithm\/\" rel=\"noreferrer\">Tutorial<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-27 12:59:41.21 UTC",
        "Question_favorite_count":3.0,
        "Question_last_edit_date":"2020-07-28 11:54:56.953 UTC",
        "Question_score":5,
        "Question_tags":"python|scikit-learn|amazon-sagemaker|outliers|anomaly-detection",
        "Question_view_count":6062,
        "Owner_creation_date":"2020-03-30 17:07:50.093 UTC",
        "Owner_last_access_date":"2022-08-10 09:27:57.74 UTC",
        "Owner_reputation":123,
        "Owner_up_votes":21,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Answer_body":"<p>In part of my answers I'll assume you refer to Sklearn's Isolation Forest. I believe those are the 4 main differences:<\/p>\n<ol>\n<li><p><strong>Code availability:<\/strong> Isolation Forest has a popular open-source implementation in Scikit-Learn (<a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.IsolationForest.html\" rel=\"noreferrer\"><code>sklearn.ensemble.IsolationForest<\/code><\/a>), while both AWS implementation of Robust Random Cut Forest (RRCF) are closed-source, in <a href=\"https:\/\/docs.aws.amazon.com\/kinesisanalytics\/latest\/sqlref\/sqlrf-random-cut-forest.html\" rel=\"noreferrer\">Amazon Kinesis<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/randomcutforest.html\" rel=\"noreferrer\">Amazon SageMaker<\/a>. There is an interesting third party RRCF open-source implementation on GitHub though: <a href=\"https:\/\/github.com\/kLabUM\/rrcf\" rel=\"noreferrer\">https:\/\/github.com\/kLabUM\/rrcf<\/a> ; but unsure how popular it is yet<\/p>\n<\/li>\n<li><p><strong>Training design:<\/strong> RRCF can work on streams, as highlighted in the paper and as exposed in the streaming analytics service Kinesis Data Analytics. On the other hand, the absence of <code>partial_fit<\/code> method hints me that Sklearn's Isolation Forest is a batch-only algorithm that cannot readily work on data streams<\/p>\n<\/li>\n<li><p><strong>Scalability:<\/strong> SageMaker RRCF is more scalable. Sklearn's Isolation Forest is single-machine code, which can nonetheless be parallelized over CPUs with the <code>n_jobs<\/code> parameter. On the other hand, SageMaker RRCF can be used over <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html\" rel=\"noreferrer\">one machine or multiple machines<\/a>. Also, it supports SageMaker Pipe mode (streaming data via unix pipes) which makes it able to learn on much bigger data than what fits on disk<\/p>\n<\/li>\n<li><p><strong>the way features are sampled<\/strong> at each recursive isolation: RRCF gives more weight to dimension with higher variance (according to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/rcf_how-it-works.html\" rel=\"noreferrer\">SageMaker doc<\/a>), while I think isolation forest samples at random, which is one reason why RRCF is expected to perform better in high-dimensional space (picture from the RRCF paper)\n<a href=\"https:\/\/i.stack.imgur.com\/3FXmE.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3FXmE.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<\/ol>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2020-07-28 12:23:30.3 UTC",
        "Answer_score":11.0,
        "Owner_location":"Strasbourg, France",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63115867",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72096297,
        "Question_title":"Hyperparameter tuning job In Sagemaker with cross valdiation",
        "Question_body":"<p>I managed to get something <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-tuning-job.html\" rel=\"nofollow noreferrer\">along those lines<\/a> to work. This is great but to be more on the save side (i.e. not rely too much on the train validation split) one should really use cross validation. I am curious, if this can also be achieved via Sagemaker hyperparameter tuning jobs? I googled extensively ...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2022-05-03 07:52:01.897 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|cross-validation|amazon-sagemaker|hyperparameters",
        "Question_view_count":70,
        "Owner_creation_date":"2010-03-01 10:53:04.443 UTC",
        "Owner_last_access_date":"2022-09-24 18:56:19.313 UTC",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Answer_body":"<p>It is not possible through HPO.<\/p>\n<p>You need to add additional step in your workflow to achieve cross-validation.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-05-04 02:05:55.79 UTC",
        "Answer_score":1.0,
        "Owner_location":"Somewhere",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72096297",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63813624,
        "Question_title":"Load a Picked or Joblib Pre trained ML Model to Sagemaker and host as endpoint",
        "Question_body":"<p>If I have a trained model in Using pickle, or Joblib.\nLets say its Logistic regression or XGBoost.<\/p>\n<p>I would like to host that model in AWS Sagemaker as endpoint without running a training job.\nHow to achieve that.<\/p>\n<pre><code>#Lets Say myBucketName contains model.pkl\nmodel = joblib.load('filename.pkl')  \n# X_test = Numpy Array \nmodel.predict(X_test)  \n<\/code><\/pre>\n<p>I am not interested to <code>sklearn_estimator.fit('S3 Train, S3 Validate' )<\/code> , I have the trained model<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-09 14:19:08.083 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":1622,
        "Owner_creation_date":"2013-06-06 09:03:28.5 UTC",
        "Owner_last_access_date":"2022-09-24 23:52:14.747 UTC",
        "Owner_reputation":1573,
        "Owner_up_votes":97,
        "Owner_down_votes":12,
        "Owner_views":194,
        "Answer_body":"<p>For Scikit Learn for example, you can get inspiration from this public demo <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>\n<p>Step 1: Save your artifact (eg the joblib) compressed in S3 at <code>s3:\/\/&lt;your path&gt;\/model.tar.gz<\/code><\/p>\n<p>Step 2: Create an inference script with the deserialization function <code>model_fn<\/code>. (Note that you could also add custom inference functions <code>input_fn<\/code>, <code>predict_fn<\/code>, <code>output_fn<\/code> but for scikit the defaults function work fine)<\/p>\n<pre><code>%%writefile inference_script.py. # Jupiter command to create file in case you're in Jupiter\n\nimport joblib\nimport os\n\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return clf\n<\/code><\/pre>\n<p>Step 3: Create a model associating the artifact with the right container<\/p>\n<pre><code>from sagemaker.sklearn.model import SKLearnModel\n\nmodel = SKLearnModel(\n    model_data='s3:\/\/&lt;your path&gt;\/model.tar.gz',\n    role='&lt;your role&gt;',\n    entry_point='inference_script.py',\n    framework_version='0.23-1')\n<\/code><\/pre>\n<p>Step 4: Deploy!<\/p>\n<pre><code>model.deploy(\n    instance_type='ml.c5.large',  # choose the right instance type\n    initial_instance_count=1)\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-09-10 07:22:57.177 UTC",
        "Answer_score":5.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63813624",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48650152,
        "Question_title":"AWS uploading file into wrong bucket",
        "Question_body":"<p>I am using AWS Sagemaker and trying to upload a data folder into S3 from Sagemaker. I am trying to do is to upload my data into the s3_train_data directory (the directory exists in S3). However, it wouldn't upload it in that bucket, but in a default Bucket that has been created, and in turn creates a new folder directory with the S3_train_data variables.<\/p>\n\n<p>code to input in directory<\/p>\n\n<pre><code>import os\nimport sagemaker\nfrom sagemaker import get_execution_role\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\nbucket = &lt;bucket name&gt;\nprefix = &lt;folders1\/folders2&gt;\nkey = &lt;input&gt;\n\n\ns3_train_data = 's3:\/\/{}\/{}\/{}\/'.format(bucket, prefix, key)\n\n\n#path 'data' is the folder in the Jupyter Instance, contains all the training data\ninputs = sagemaker_session.upload_data(path= 'data', key_prefix= s3_train_data)\n<\/code><\/pre>\n\n<p>Is the problem in the code or more in how I created the notebook?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-02-06 19:03:37.34 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-05-06 15:50:37.033 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":722,
        "Owner_creation_date":"2017-10-30 19:41:59.217 UTC",
        "Owner_last_access_date":"2019-03-16 23:32:36.337 UTC",
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":"<p>You could look at the Sample notebooks, how to upload the data S3 bucket \nThere have many ways. I am just giving you hints to answer. \nAnd you forgot create a boto3 session to access the S3 bucket <\/p>\n\n<p><strong>It is one of the ways to do it.<\/strong> <\/p>\n\n<pre><code>import os \nimport urllib.request\nimport boto3\n\ndef download(url):\n    filename = url.split(\"\/\")[-1]\n    if not os.path.exists(filename):\n        urllib.request.urlretrieve(url, filename)\n\n\ndef upload_to_s3(channel, file):\n    s3 = boto3.resource('s3')\n    data = open(file, \"rb\")\n    key = channel + '\/' + file\n    s3.Bucket(bucket).put_object(Key=key, Body=data)\n\n\n# caltech-256\ndownload('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-train.rec')\nupload_to_s3('train', 'caltech-256-60-train.rec')\ndownload('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-val.rec')\nupload_to_s3('validation', 'caltech-256-60-val.rec')\n<\/code><\/pre>\n\n<p>link : <a href=\"https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb\" rel=\"nofollow noreferrer\">https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb<\/a><\/p>\n\n<p><strong>Another way to do it.<\/strong> <\/p>\n\n<pre><code>bucket = '&lt;your_s3_bucket_name_here&gt;'# enter your s3 bucket where you will copy data and model artifacts\nprefix = 'sagemaker\/breast_cancer_prediction' # place to upload training files within the bucket\n# do some processing then prepare to push the data. \n\nf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))\nf.seek(0)\n\nboto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', train_file)).upload_fileobj(f)\n<\/code><\/pre>\n\n<p>Link : <a href=\"https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_applying_machine_learning\/breast_cancer_prediction\/Breast%20Cancer%20Prediction.ipynb\" rel=\"nofollow noreferrer\">https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_applying_machine_learning\/breast_cancer_prediction\/Breast%20Cancer%20Prediction.ipynb<\/a><\/p>\n\n<p>Youtube link : <a href=\"https:\/\/www.youtube.com\/watch?v=-YiHPIGyFGo\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=-YiHPIGyFGo<\/a> - how to pull the data in S3 bucket.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-02-08 13:46:57.247 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48650152",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73677347,
        "Question_title":"Does SageMaker built-in LightGBM algorithm support distributed training?",
        "Question_body":"<p>Does  <strong>Amazon SageMaker built-in LightGBM<\/strong> algorithm support <strong>distributed training<\/strong>?<\/p>\n<p>I use Databricks for distributed training of LightGBM today. If SageMaker built-in LightGBM supports distributed training, I would consider migrating to SageMaker. It is not clear in the Amazon SageMaker's built-in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lightgbm.html\" rel=\"nofollow noreferrer\">LightGBM<\/a>'s documentation on whether it supports distributed training.<\/p>\n<p>Thanks very much for any suggestion or clarification on this.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-11 06:44:39.543 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|lightgbm|distributed-training|amazon-machine-learning",
        "Question_view_count":27,
        "Owner_creation_date":"2014-01-16 15:43:59.673 UTC",
        "Owner_last_access_date":"2022-09-25 03:22:08.463 UTC",
        "Owner_reputation":5854,
        "Owner_up_votes":155,
        "Owner_down_votes":70,
        "Owner_views":794,
        "Answer_body":"<p>I went through the LightGBM section of SageMaker documentation and there are no references that it supports distributed training. One of the example\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lightgbm.html\" rel=\"nofollow noreferrer\">here<\/a>\u00a0uses single instance type. Also looked at lightGBM documentation\u00a0<a href=\"https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parallel-Learning-Guide.html\" rel=\"nofollow noreferrer\">here<\/a>\u00a0. Here are the parameters that you need to specify<\/p>\n<p>tree_learner=your_parallel_algorithm,<\/p>\n<p>num_machines=your_num_machines,<\/p>\n<p>Given I couldnt find any reference of above in SageMaker documentation, I assume its not supported.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-20 22:00:20.203 UTC",
        "Answer_score":0.0,
        "Owner_location":"Singapore",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73677347",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67093041,
        "Question_title":"Aws Sagemaker - ModuleNotFoundError: No module named 'cv2'",
        "Question_body":"<p>I am trying to run a object detection code in Aws. Although opencv is listed in the requirement file, i have the error &quot;no module named cv2&quot;. I am not sure how to fix this error. could someone help me please.<\/p>\n<p>My requirement.txt file has<\/p>\n<ul>\n<li>opencv-python<\/li>\n<li>numpy&gt;=1.18.2<\/li>\n<li>scipy&gt;=1.4.1<\/li>\n<li>wget&gt;=3.2<\/li>\n<li>tensorflow==2.3.1<\/li>\n<li>tensorflow-gpu==2.3.1<\/li>\n<li>tqdm==4.43.0<\/li>\n<li>pandas<\/li>\n<li>boto3<\/li>\n<li>awscli<\/li>\n<li>urllib3<\/li>\n<li>mss<\/li>\n<\/ul>\n<p>I tried installing &quot;imgaug&quot; and &quot;opencv-python headless&quot; as well.. but still not able to get rid of this error.<\/p>\n<pre><code>sh-4.2$ python train_launch.py \n[INFO-ROLE] arn:aws:iam::021945294007:role\/service-role\/AmazonSageMaker-ExecutionRole-20200225T145269\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_count has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\n2021-04-14 13:29:58 Starting - Starting the training job...\n2021-04-14 13:30:03 Starting - Launching requested ML instances......\n2021-04-14 13:31:11 Starting - Preparing the instances for training......\n2021-04-14 13:32:17 Downloading - Downloading input data...\n2021-04-14 13:32:41 Training - Downloading the training image..WARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n2021-04-14 13:33:03,970 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\n2021-04-14 13:33:05,030 sagemaker-containers INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {\n        &quot;training&quot;: &quot;\/opt\/ml\/input\/data\/training&quot;\n    },\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;unfreezed_epochs&quot;: 2,\n        &quot;freezed_batch_size&quot;: 8,\n        &quot;freezed_epochs&quot;: 1,\n        &quot;unfreezed_batch_size&quot;: 8,\n        &quot;model_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {\n        &quot;training&quot;: {\n            &quot;TrainingInputMode&quot;: &quot;File&quot;,\n            &quot;S3DistributionType&quot;: &quot;FullyReplicated&quot;,\n            &quot;RecordWrapperType&quot;: &quot;None&quot;\n        }\n    },\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;yolov4-2021-04-14-15-29&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;train_indu&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 8,\n    &quot;num_gpus&quot;: 1,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;train_indu.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2}\nSM_USER_ENTRY_POINT=train_indu.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[&quot;training&quot;]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=train_indu\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=8\nSM_NUM_GPUS=1\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{&quot;training&quot;:&quot;\/opt\/ml\/input\/data\/training&quot;},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;yolov4-2021-04-14-15-29&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;train_indu&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:8,&quot;num_gpus&quot;:1,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;train_indu.py&quot;}\nSM_USER_ARGS=[&quot;--freezed_batch_size&quot;,&quot;8&quot;,&quot;--freezed_epochs&quot;,&quot;1&quot;,&quot;--model_dir&quot;,&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;--unfreezed_batch_size&quot;,&quot;8&quot;,&quot;--unfreezed_epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_CHANNEL_TRAINING=\/opt\/ml\/input\/data\/training\nSM_HP_UNFREEZED_EPOCHS=2\nSM_HP_FREEZED_BATCH_SIZE=8\nSM_HP_FREEZED_EPOCHS=1\nSM_HP_UNFREEZED_BATCH_SIZE=8\nSM_HP_MODEL_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages\n\nInvoking script with the following command:\n\n\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2\n\n\nWARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n[name: &quot;\/device:CPU:0&quot;\ndevice_type: &quot;CPU&quot;\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 4667030854237447206\n, name: &quot;\/device:XLA_CPU:0&quot;\ndevice_type: &quot;XLA_CPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 3059419181456814147\nphysical_device_desc: &quot;device: XLA_CPU device&quot;\n, name: &quot;\/device:XLA_GPU:0&quot;\ndevice_type: &quot;XLA_GPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 6024475084695919958\nphysical_device_desc: &quot;device: XLA_GPU device&quot;\n, name: &quot;\/device:GPU:0&quot;\ndevice_type: &quot;GPU&quot;\nmemory_limit: 14949928141\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 13034103301168381073\nphysical_device_desc: &quot;device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5&quot;\n]\nTraceback (most recent call last):\n  File &quot;train_indu.py&quot;, line 12, in &lt;module&gt;\n    from yolov3.dataset import Dataset\n  File &quot;\/opt\/ml\/code\/yolov3\/dataset.py&quot;, line 3, in &lt;module&gt;\n    import cv2\nModuleNotFoundError: No module named 'cv2'\n2021-04-14 13:33:08,453 sagemaker-containers ERROR    ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n\n2021-04-14 13:33:11 Uploading - Uploading generated training model\n2021-04-14 13:33:54 Failed - Training job failed\nTraceback (most recent call last):\n  File &quot;train_launch.py&quot;, line 41, in &lt;module&gt;\n    estimator.fit(s3_data_path, logs=True, job_name=job_name) #the argument logs is crucial if you want to see what happends\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 535, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 1210, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 3365, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 2957, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job yolov4-2021-04-14-15-29: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-04-14 13:46:26.697 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"opencv|image-processing|amazon-ec2|object-detection|amazon-sagemaker",
        "Question_view_count":1218,
        "Owner_creation_date":"2019-05-16 12:28:17.177 UTC",
        "Owner_last_access_date":"2021-04-18 14:20:03.61 UTC",
        "Owner_reputation":53,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<p>Make sure your estimator has<\/p>\n<ul>\n<li>framework_version = '2.3',<\/li>\n<li>py_version = 'py37',<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-04-14 14:21:31.9 UTC",
        "Answer_score":1.0,
        "Owner_location":"Cergy-Pontoise, Cergy, France",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67093041",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50985138,
        "Question_title":"Sagemaker Hyperparameter Optimization XGBoost",
        "Question_body":"<p>I am trying to build a hyperparameter optimization job in Amazon Sagemaker, in python, but something is not working. Here is what I have:<\/p>\n\n<pre><code>sess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n                                    role, \n                                    train_instance_count=1, \n                                    train_instance_type='ml.m4.4xlarge',\n                                    output_path=output_path_1,\n                                    base_job_name='HPO-xgb',\n                                    sagemaker_session=sess)\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter    \n\nhyperparameter_ranges = {'eta': ContinuousParameter(0.01, 0.2),\n                         'num_rounds': ContinuousParameter(100, 500),\n                         'num_class':  4,\n                         'max_depth': IntegerParameter(3, 9),\n                         'gamma': IntegerParameter(0, 5),\n                         'min_child_weight': IntegerParameter(2, 6),\n                         'subsample': ContinuousParameter(0.5, 0.9),\n                         'colsample_bytree': ContinuousParameter(0.5, 0.9)}\n\nobjective_metric_name = 'validation:mlogloss'\nobjective_type='minimize'\nmetric_definitions = [{'Name': 'validation-mlogloss',\n                       'Regex': 'validation-mlogloss=([0-9\\\\.]+)'}]\n\ntuner = HyperparameterTuner(xgb,\n                            objective_metric_name,\n                            objective_type,\n                            hyperparameter_ranges,\n                            metric_definitions,\n                            max_jobs=9,\n                            max_parallel_jobs=3)\n\ntuner.fit({'train': s3_input_train, 'validation': s3_input_validation}) \n<\/code><\/pre>\n\n<p>And the error I get is: <\/p>\n\n<pre><code>AttributeError: 'str' object has no attribute 'keys'\n<\/code><\/pre>\n\n<p>The error seems to come from the <code>tuner.py<\/code> file:<\/p>\n\n<pre><code>----&gt; 1 tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in fit(self, inputs, job_name, **kwargs)\n    144             self.estimator._prepare_for_training(job_name)\n    145 \n--&gt; 146         self._prepare_for_training(job_name=job_name)\n    147         self.latest_tuning_job = _TuningJob.start_new(self, inputs)\n    148 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in _prepare_for_training(self, job_name)\n    120 \n    121         self.static_hyperparameters = {to_str(k): to_str(v) for (k, v) in self.estimator.hyperparameters().items()}\n--&gt; 122         for hyperparameter_name in self._hyperparameter_ranges.keys():\n    123             self.static_hyperparameters.pop(hyperparameter_name, None)\n    124 \n\nAttributeError: 'list' object has no attribute 'keys'                           \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-06-22 09:42:02.477 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-06-28 08:11:28.99 UTC",
        "Question_score":3,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1523,
        "Owner_creation_date":"2015-05-26 22:53:10.12 UTC",
        "Owner_last_access_date":"2019-06-03 07:51:39.51 UTC",
        "Owner_reputation":455,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":58,
        "Answer_body":"<p>Your arguments when initializing the HyperparameterTuner object are in the wrong order. The constructor has the following signature:<\/p>\n\n<pre><code>HyperparameterTuner(estimator, \n                    objective_metric_name, \n                    hyperparameter_ranges, \n                    metric_definitions=None, \n                    strategy='Bayesian', \n                    objective_type='Maximize', \n                    max_jobs=1, \n                    max_parallel_jobs=1, \n                    tags=None, \n                    base_tuning_job_name=None)\n<\/code><\/pre>\n\n<p>so in this case, your <code>objective_type<\/code> is in the wrong position. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/tuner.html#sagemaker.tuner.HyperparameterTuner\" rel=\"nofollow noreferrer\">the docs<\/a> for more details.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-06-27 00:04:50.673 UTC",
        "Answer_score":5.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2018-07-10 18:56:01.193 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50985138",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60712225,
        "Question_title":"How to retrain a sagemaker model on new data, after a training job is created",
        "Question_body":"<p>I have a sagemaker model that is trained on a specific dataset, and training job is created. Now i have a new dataset that the model has to be trained on, how do I retrain the model on new data from the already existing model ? Can we have the model checkpoints saved ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2020-03-16 19:37:49.987 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-sagemaker|pre-trained-model",
        "Question_view_count":660,
        "Owner_creation_date":"2017-07-19 21:52:51.967 UTC",
        "Owner_last_access_date":"2020-06-02 23:45:58.22 UTC",
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<blockquote>\n  <p>Only three built-in algorithms currently support incremental training: Object Detection Algorithm, Image Classification Algorithm, and Semantic Segmentation Algorithm.<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/incremental-training.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/incremental-training.html<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-03-17 16:31:44.473 UTC",
        "Answer_score":0.0,
        "Owner_location":"New York, NY, United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60712225",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60157184,
        "Question_title":"Tensorflow Parameter Servers on SageMaker",
        "Question_body":"<p>I am trying to understand how parameters servers (PS's) work for distributed training in Tensorflow on Amazon SageMaker. <\/p>\n\n<p>To make things more concrete, I am able to run the example from AWS using PS's: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/tf-distributed-training.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/tf-distributed-training.ipynb<\/a><\/p>\n\n<p>Here is the code block that initializes the estimator for Tensorflow:<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ngit_config = {'repo': 'https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode', 'branch': 'master'}\n\nps_instance_type = 'ml.p3.2xlarge'\nps_instance_count = 2\n\nmodel_dir = \"\/opt\/ml\/model\"\n\ndistributions = {'parameter_server': {\n                    'enabled': True}\n                }\nhyperparameters = {'epochs': 60, 'batch-size' : 256}\n\nestimator_ps = TensorFlow(\n                       git_config=git_config,\n                       source_dir='tf-distribution-options\/code',\n                       entry_point='train_ps.py', \n                       base_job_name='ps-cifar10-tf',\n                       role=role,\n                       framework_version='1.13',\n                       py_version='py3',\n                       hyperparameters=hyperparameters,\n                       train_instance_count=ps_instance_count, \n                       train_instance_type=ps_instance_type,\n                       model_dir=model_dir,\n                       tags = [{'Key' : 'Project', 'Value' : 'cifar10'},{'Key' : 'TensorBoard', 'Value' : 'dist'}],\n                       distributions=distributions)\n<\/code><\/pre>\n\n<p>Going through the documentation for Tensorflow, it seems that a device scope can be used for assigning a variable to a particular worker. However, I never see this done when running training jobs on SageMaker. In the example from AWS, the model is defined by:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/code\/model_def.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/code\/model_def.py<\/a><\/p>\n\n<p>Here is a snippet:<\/p>\n\n<pre><code>def get_model(learning_rate, weight_decay, optimizer, momentum, size, mpi=False, hvd=False):\n\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), padding='same', input_shape=(HEIGHT, WIDTH, DEPTH)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Conv2D(32, (3, 3)))\n\n    ...\n\n    model.add(Flatten())\n    model.add(Dense(512))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(NUM_CLASSES))\n    model.add(Activation('softmax'))\n\n    if mpi:\n        size = hvd.size()\n\n    if optimizer.lower() == 'sgd':\n        ...\n\n    if mpi:\n        opt = hvd.DistributedOptimizer(opt)\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=opt,\n                  metrics=['accuracy'])\n\n    return model\n<\/code><\/pre>\n\n<p>Here, there are no references to distribution strategies (except with MPI, but that flag is set to False for PS's). Somehow, Tensorflow or the SageMaker container is able to decide where the variables for each layer should be stored. However, I'm not seeing anything in the container code that does anything with the distribution strategy.<\/p>\n\n<p>I am able to run this code and train the model using 1 and 2 instances. When i do so, I see a decrease of almost 50% in the runtime, suggesting that a distributed training is occurring.<\/p>\n\n<p>My questions are:<\/p>\n\n<ol>\n<li>How does Tensorflow decide the distribution of variables on the PS's? In the example code, there is no explicit reference to devices. Somehow the distribution is done automatically.<\/li>\n<li>Is it possible to see which parameters have been assigned to each PS? Or to see what the communication between PS's looks like? If so, how?<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-10 19:33:42.683 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker|distributed-tensorflow",
        "Question_view_count":307,
        "Owner_creation_date":"2014-11-18 19:04:38.873 UTC",
        "Owner_last_access_date":"2022-06-30 09:52:29.197 UTC",
        "Owner_reputation":101,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":"<blockquote>\n<p>My questions are:<\/p>\n<p>How does Tensorflow decide the distribution of variables on the PS's?\nIn the example code, there is no explicit reference to devices.\nSomehow the distribution is done automatically.<\/p>\n<\/blockquote>\n<p>The TensorFlow image provided by SageMaker has the code to setup TF_CONFIG and launching parameter server for multi work training. See the code [here][1] The setup is for each node in the cluster there is a PS and a worker thread configured.<\/p>\n<p>It's not using any DistributionStrategy so the default strategy is used. See [here][2].<\/p>\n<p>If you would like to use a different DistributionStrategy or different TF_CONFIG you will need to disable <code>parameter_server<\/code> option when launching the SageMaker training job and set everything up in your training script.<\/p>\n<blockquote>\n<p>Is it possible to see which parameters have been assigned to each PS?\nOr to see what the communication between PS's looks like? If so, how?<\/p>\n<\/blockquote>\n<p>You should be able to get some information from the output log which can be found in CloudWatch. The link is available on the Training Job console page.\n[1]: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-training-toolkit\/blob\/master\/src\/sagemaker_tensorflow_container\/training.py#L37\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-training-toolkit\/blob\/master\/src\/sagemaker_tensorflow_container\/training.py#L37<\/a>\n[2]: <a href=\"https:\/\/www.tensorflow.org\/guide\/distributed_training#default_strategy\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/guide\/distributed_training#default_strategy<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-09-15 21:09:11.31 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60157184",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71273364,
        "Question_title":"SparkJarProcessor in Sagemaker Pipeline",
        "Question_body":"<p>I would like run SparkJarProcessor within Sagemaker Pipeline.  After I create an instance of SparkJarProcessor, when I just <code>run<\/code> the processor, I can specify the jar and the class you I want to execute with the <code>submit_app<\/code> and <code>submit_class<\/code> parameters to the <code>run<\/code> method. e.g.,<\/p>\n<pre><code>processor.run(\n    submit_app=&quot;my.jar&quot;,\n    submit_class=&quot;program.to.run&quot;,\n    arguments=['--my_arg', &quot;my_arg&quot;],\n    configuration=my_config,\n    spark_event_logs_s3_uri=log_path\n)\n<\/code><\/pre>\n<p>If I want to run it as a step in the pipeline, what arguments can I give to ProcessingStep? According to <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.spark.processing.SparkJarProcessor.get_run_args\" rel=\"nofollow noreferrer\">this documentation<\/a>, you can call get_run_args on the processor to &quot;<em>get the normalized inputs, outputs and arguments needed when using a SparkJarProcessor in a ProcessingStep<\/em>&quot;, but when I run it like this,<\/p>\n<pre><code>processor.get_run_args(\n    submit_app=&quot;my.jar&quot;, \n    submit_class=&quot;program.to.run&quot;,\n    arguments=['--my_arg', &quot;my_arg&quot;],\n    configuration=my_config,\n    spark_event_logs_s3_uri=log_path\n)\n<\/code><\/pre>\n<p>My output looks like this:<\/p>\n<pre><code>RunArgs(code='my.jar', inputs=[&lt;sagemaker.processing.ProcessingInput object at 0x7fc53284a090&gt;], outputs=[&lt;sagemaker.processing.ProcessingOutput object at 0x7fc532845ed0&gt;], arguments=['--my_arg', 'my_arg'])\n<\/code><\/pre>\n<p>&quot;program.to.run&quot; is not part of the output.  So, assuming <code>code<\/code> is to specify the jar, what's the normalized version of <code>submit_class<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-26 01:26:56.427 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-03-02 17:36:47.94 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|apache-spark|amazon-sagemaker",
        "Question_view_count":197,
        "Owner_creation_date":"2012-05-06 04:24:18.637 UTC",
        "Owner_last_access_date":"2022-09-23 19:58:17.057 UTC",
        "Owner_reputation":543,
        "Owner_up_votes":272,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Answer_body":"<p>When <code>get_run_args<\/code> or <code>run<\/code> is called on a SparkJarProcessor, the <code>submit_class<\/code> <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/dev\/src\/sagemaker\/spark\/processing.py#L1143\" rel=\"nofollow noreferrer\">is used to set a property on the processor itself<\/a> which is why you don't see it in the <code>get_run_args<\/code> output.<\/p>\n<p>That processor property will be used during pipeline definition generation to set the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_AppSpecification.html#sagemaker-Type-AppSpecification-ContainerEntrypoint\" rel=\"nofollow noreferrer\">ContainerEntrypoint<\/a> argument to <code>CreateProcessingJob<\/code>.<\/p>\n<p>Example:<\/p>\n<pre><code>run_args = spark_processor.get_run_args(\n    submit_app=&quot;my.jar&quot;,\n    submit_class=&quot;program.to.run&quot;,\n    arguments=[]\n)\n\nstep_process = ProcessingStep(\n    name=&quot;SparkJarProcessStep&quot;,\n    processor=spark_processor,\n    inputs=run_args.inputs,\n    outputs=run_args.outputs,\n    code=run_args.code\n)\n\npipeline = Pipeline(\n    name=&quot;myPipeline&quot;,\n    parameters=[],\n    steps=[step_process],\n)\n\ndefinition = json.loads(pipeline.definition())\ndefinition\n<\/code><\/pre>\n<p>The output of <code>definition<\/code>:<\/p>\n<pre><code>...\n'Steps': [{'Name': 'SparkJarProcessStep',\n   'Type': 'Processing',\n   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n      'InstanceCount': 2,\n      'VolumeSizeInGB': 30}},\n    'AppSpecification': {'ImageUri': '153931337802.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-spark-processing:2.4-cpu',\n     'ContainerEntrypoint': ['smspark-submit',\n      '--class',\n      'program.to.run',\n      '--local-spark-event-logs-dir',\n      '\/opt\/ml\/processing\/spark-events\/',\n      '\/opt\/ml\/processing\/input\/code\/my.jar']},\n...\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-03-03 19:34:08.227 UTC",
        "Answer_score":1.0,
        "Owner_location":"Roseville, MN",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71273364",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63781356,
        "Question_title":"How to correctly write a sagemaker tensorflow input_handler() that returns a numpy array?",
        "Question_body":"<p>I am trying to implement a input_handler() in inference.py for a sagemaker inference container.<\/p>\n<p>The images\/arrays are very big (3D). So I want to pass in a S3 URI, then the input_handler() function should load the image\/array from s3 and return the actual numpy array for the model (which expects a tensor):<\/p>\n<pre><code>def input_handler(data, context):\n\n    d = data.read().decode('utf-8')\n\n    body = json.loads(d)\n    s3path = body['s3_path']\n\n    s3 = S3FileSystem()\n    df = np.load(s3.open(s3path))\n\n    return df\n<\/code><\/pre>\n<p>Returning a numpy array worked with the Sagemaker python api version &lt; 1.0 and input_fn(), but does not work with the new container used by sagemaker python api &gt; 2.0 that expects input_handler().<\/p>\n<p>The actual container image is &quot;763104351884.dkr.ecr.eu-central-1.amazonaws.com\/tensorflow-inference:1.15-gpu&quot;.<\/p>\n<p>During inference, I get the following error in CloudWatch thrown by the container:<\/p>\n<pre><code>ERROR:python_service:exception handling request: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(\n\nTraceback (most recent call last):\n  File &quot;\/sagemaker\/python_service.py&quot;, line 289, in _handle_invocation_post\n    res.body, res.content_type = self._handlers(data, context)\n  File &quot;\/sagemaker\/python_service.py&quot;, line 322, in handler\n    response = requests.post(context.rest_uri, data=processed_input)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/api.py&quot;, line 116, in post\n    return request('post', url, data=data, json=json, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/api.py&quot;, line 60, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/sessions.py&quot;, line 512, in request\n    data=data or \n{}\n,\n<\/code><\/pre>\n<p>What is the correct return type? All examples I found were for json &amp; text...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2020-09-07 16:39:14.993 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2020-09-08 07:02:35.527 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":636,
        "Owner_creation_date":"2009-10-06 11:50:17.773 UTC",
        "Owner_last_access_date":"2022-09-23 20:34:01.687 UTC",
        "Owner_reputation":2595,
        "Owner_up_votes":462,
        "Owner_down_votes":5,
        "Owner_views":357,
        "Answer_body":"<p>This seems to work:<\/p>\n<p><code>return json.dumps({&quot;inputs&quot;: df.tolist() }).<\/code><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-09-16 12:35:22.303 UTC",
        "Answer_score":0.0,
        "Owner_location":"Germany",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63781356",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63035151,
        "Question_title":"When calling a SageMaker deploy_endpoint function with an a1.small instance, I'm given an error that I can't open a m5.xlarge instance",
        "Question_body":"<p>So while executing through a notebook generated by Autopilot, I went to execute the final code cell:<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>pipeline_model.deploy(initial_instance_count=1,\n                      instance_type='a1.small',\n                      endpoint_name=pipeline_model.name,\n                      wait=True)\n<\/code><\/pre>\n<p>I get this error<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m5.2xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p>The most important part of that is the last line where it mentions resource limits.  I'm not trying to open the type of instance it's giving me an error about opening.<\/p>\n<p>Does the endpoint NEED to be on an ml.m5.2xlarge instance?  Or is the code acting up?<\/p>\n<p>Thanks in advance guys and gals.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-22 13:19:35.777 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|boto3|amazon-sagemaker",
        "Question_view_count":91,
        "Owner_creation_date":"2017-05-26 00:24:51.317 UTC",
        "Owner_last_access_date":"2022-04-19 19:57:14.917 UTC",
        "Owner_reputation":138,
        "Owner_up_votes":3,
        "Owner_down_votes":1,
        "Owner_views":4,
        "Answer_body":"<p>You should use one of on-demand ML hosting instances supported as detailed at <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/\" rel=\"nofollow noreferrer\">this link<\/a>. I think non-valid <code>instance_type='a1.small'<\/code> is replaced by a valid one (ml.m5.2xlarge), and that is not in your AWS service quota. The weird part is that seeing <code>instance_type='a1.small'<\/code> was generated by SageMaker Autopilot.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-23 21:28:32.143 UTC",
        "Answer_score":2.0,
        "Owner_location":"Temple University, West Berks Street, Philadelphia, PA, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63035151",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66915920,
        "Question_title":"Using pytorch cuda in AWS sagemaker notebook instance",
        "Question_body":"<p>In colab, whenever we need GPU, we simply click <code>change runtime type<\/code> and change hardware accelarator to <code>GPU<\/code><\/p>\n<p>and cuda becomes available, <code>torch.cuda.is_available()<\/code> is <code>True<\/code><\/p>\n<p>How to do this is AWS sagemaker, i.e. turning on cuda.\nI am new to AWS and trying to train model using pytorch in aws sagemaker, where Pytorch code is first tested in colab environment.<\/p>\n<p>my sagemaker notebook insatnce is <code>ml.t2.medium<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-02 07:33:56.21 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-04-02 07:48:18.827 UTC",
        "Question_score":0,
        "Question_tags":"amazon-ec2|pytorch|amazon-sagemaker",
        "Question_view_count":1608,
        "Owner_creation_date":"2019-09-07 18:22:12.003 UTC",
        "Owner_last_access_date":"2022-08-28 17:07:56.487 UTC",
        "Owner_reputation":137,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":100,
        "Answer_body":"<p>Using AWS Sagemaker you don't need to worry about the GPU, you simply select an instance type with GPU ans Sagemaker will use it. Specifically <code>ml.t2.medium<\/code> doesn't have a GPU but it's anyway not the right way to train a model.\nBasically you have 2 canonical ways to use Sagemaker (look at the documentation and examples please), the first is to use a notebook with a limited computing resource to spin up a training job using a prebuilt image, in that case when you call the estimator you simply specify what <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/\" rel=\"nofollow noreferrer\">instance type<\/a> you want (you'll choose one with GPU, looking at the costs). The second way is to use your own container, push it to ECR and launch a training job from the console, where you specify the instance type.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-04-03 15:36:02.903 UTC",
        "Answer_score":0.0,
        "Owner_location":"Lahore, Pakistan",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66915920",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60903256,
        "Question_title":"AWS Chalice, can't get image from POST request",
        "Question_body":"<p>I'm trying to invoke my sagemaker model using aws chalice, a lambda function, and an API Gateaway.<\/p>\n\n<p>I'm attempting to send the image over <code>POST<\/code> request but I'm having problem receiving it on the lambda function.<\/p>\n\n<p>My code looks like:<\/p>\n\n<pre><code>from chalice import Chalice\nfrom chalice import BadRequestError\nimport base64\nimport os\nimport boto3\nimport ast\nimport json\n\napp = Chalice(app_name='foo')\napp.debug = True\n\n\n@app.route('\/', methods=['POST'], content_types=['application\/json'])\ndef index():\n    body = ''\n\n    try:\n        body = app.current_request.json_body # &lt;- I suspect this is the problem\n        return {'response': body}\n    except Exception as e:\n        return  {'error':  str(e)}\n<\/code><\/pre>\n\n<p>It's just returning<\/p>\n\n<p><code>&lt;Response [200]&gt; {'error': 'BadRequestError: Error Parsing JSON'}<\/code><\/p>\n\n<p>As I mentioned before, my end goal is to receive my image and make a sagemaker request with it. But I just can't seem to read the image. <\/p>\n\n<p>My python test client looks like this:<\/p>\n\n<pre><code>import base64, requests, json\n\ndef test():\n\n    url = 'api_url_from_chalice'\n    body = ''\n\n    with open('b1.jpg', 'rb') as image:\n        f = image.read()\n        body = base64.b64encode(f)\n\n    payload = {'data': body}\n    headers = {'Content-Type': 'application\/json'}\n\n    r = requests.post(url, data=payload, headers=headers)\n    print(r)\n    r = r.json()\n    # r = r['response']\n\n    print(r)\n\ntest()\n<\/code><\/pre>\n\n<p>Please help me, I spent way to much time trying to figure this out<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-28 16:09:27.37 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-03-28 16:22:52.92 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|rest|amazon-sagemaker|chalice",
        "Question_view_count":1229,
        "Owner_creation_date":"2019-08-09 23:05:38.427 UTC",
        "Owner_last_access_date":"2022-05-24 07:32:00.683 UTC",
        "Owner_reputation":23,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>So I was able to figure it out with the help of an aws engineer (i got lucky I suppose). I'm including the complete lambda function. Nothing changed on the client.<\/p>\n\n<pre><code>from chalice import Chalice\nfrom chalice import BadRequestError\nimport base64\nimport os\nimport boto3\nimport ast\nimport json\nimport sys\n\n\nfrom chalice import Chalice\nif sys.version_info[0] == 3:\n    # Python 3 imports.\n    from urllib.parse import urlparse, parse_qs\nelse:\n    # Python 2 imports.\n    from urlparse import urlparse, parse_qs\n\napp = Chalice(app_name='app_name')\napp.debug = True\n\n\n@app.route('\/', methods=['POST'])\ndef index():\n    parsed = parse_qs(app.current_request.raw_body.decode())\n\n    body = parsed['data'][0]\n    print(type(body))\n\n    try:\n        body = base64.b64decode(body)\n        body = bytearray(body)\n    except e:\n        return {'error': str(e)}\n\n\n    endpoint = \"object-detection-endpoint_name\"\n    runtime = boto3.Session().client(service_name='sagemaker-runtime', region_name='us-east-2')\n\n    response = runtime.invoke_endpoint(EndpointName=endpoint, ContentType='image\/jpeg', Body=body)\n\n    print(response)\n    results = response['Body'].read().decode(\"utf-8\")\n    results = results['predictions']\n\n    results = json.loads(results)\n    results = results['predictions']\n\n    return {'result': results}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-04-02 23:26:08.03 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60903256",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64118186,
        "Question_title":"Getting an anomaly score for every datapoint in SageMaker?",
        "Question_body":"<p>I'm very new to SageMaker, and I've run into a bit of confusion as to how to achieve the output I am looking for. I am currently attempting to use the built-in RCF algorithm to perform anomaly detection on a list of stock volumes, like this:<\/p>\n<pre><code>apple_stock_volumes = [123412, 465125, 237564, 238172]\n<\/code><\/pre>\n<p>I have created a training job, model, and endpoint, and I'm trying now to invoke the endpoint using boto3. My current code looks like this:<\/p>\n<pre><code>apple_stock_volumes = [123412, 465125, 237564, 238172]\ndef inference():\n    client = boto3.client('sagemaker-runtime')\n    \n    body = &quot; &quot;.join(apple_stock_volumes)\n    response = client.invoke_endpoint(\n        EndpointName='apple-volume-endpoint',\n        Body=body,\n        ContentType='text\/csv'\n    )\n    inference = json.loads(response['Body'].read())\n    print(inference)\n\ninference()\n<\/code><\/pre>\n<p>What I wanted was to get an anomaly score for every datapoint, and then to alert if the anomaly score was a few standard deviations above the mean. However, what I'm actually receiving is just a single anomaly score. The following is my output:<\/p>\n<pre><code>{'scores': [{'score': 0.7164874384}]}\n<\/code><\/pre>\n<p>Can anyone explain to me what's going on here? Is this an average anomaly score? Why can't I seem to get SageMaker to output a list of anomaly scores corresponding to my data? Thanks in advance!<\/p>\n<p>Edit: I have already trained the model on a csv of historical volume data for the last year, and I have created an endpoint to hit.<\/p>\n<p>Edit 2: I've accepted @maafk's answer, although the actual answer to my question was provided in one of his comments. The piece I was missing was that each data point must be on a new line in your csv input to the endpoint. Once I substituted <code>body = &quot; &quot;.join(apple_stock_volumes)<\/code> for <code>body = &quot;\\n&quot;.join(apple_stock_volumes)<\/code>, everything worked as expected.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-29 10:56:43.873 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-09-29 15:44:02.04 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|random-forest|amazon-sagemaker",
        "Question_view_count":54,
        "Owner_creation_date":"2020-05-13 03:31:33.533 UTC",
        "Owner_last_access_date":"2022-07-26 19:57:01.63 UTC",
        "Owner_reputation":51,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>In your case, you'll want to get the standard deviation from getting the scores from historical stock volumes, and figuring out what your anomaly score is by calculating <code>3 * standard deviation<\/code><\/p>\n<p>Update your code to do inference on <em>multiple<\/em> records at once<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>apple_stock_volumes = [123412, 465125, 237564, 238172]\ndef inference():\n    client = boto3.client('sagemaker-runtime')\n    \n    body = &quot;\\n&quot;.join(apple_stock_volumes). # New line for each record\n    response = client.invoke_endpoint(\n        EndpointName='apple-volume-endpoint',\n        Body=body,\n        ContentType='text\/csv'\n    )\n    inference = json.loads(response['Body'].read())\n    print(inference)\n\ninference()\n<\/code><\/pre>\n<p>This will return a list of scores<\/p>\n<p>Assuming <code>apple_stock_volumes_df<\/code> has your volumes and the scores (after running inference on each record):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>score_mean = apple_stock_volumes_df['score'].mean()\nscore_std = apple_stock_volumes_df['score'].std()\nscore_cutoff = score_mean + 3*score_std\n<\/code><\/pre>\n<p>There is a great example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"nofollow noreferrer\">here<\/a> showing this<\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_date":"2020-09-29 11:10:32.67 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2020-09-29 16:58:49.94 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64118186",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64033258,
        "Question_title":"How to explicitly set sagemaker autopilot's validation set?",
        "Question_body":"<p>The example notebook: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/autopilot\/autopilot_customer_churn.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/autopilot\/autopilot_customer_churn.ipynb<\/a> states that in the Analyzing Data step:<\/p>\n<p><code>The dataset is analyzed and Autopilot comes up with a list of ML pipelines that should be tried out on the dataset. The dataset is also split into train and validation sets.<\/code><\/p>\n<p>Presumably, autopilot uses this validation set to select the best performing model candidates to return to the user. However, I have not found a way to manually set this validation set used by sagemaker autopilot.<\/p>\n<p>For example, google automl, allows users to add TRAIN, VALIDATE,TEST keywords to a data_split column to manually set which data points are in which set.<\/p>\n<p>Is something like this currently possible which sagemaker autopilot?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-23 17:26:01.153 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|automl",
        "Question_view_count":159,
        "Owner_creation_date":"2014-10-09 02:10:54.71 UTC",
        "Owner_last_access_date":"2022-09-10 14:48:43.73 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>I'm afraid you can't do this at the moment. The validation set is indeed built by Autopilot itself.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-09-24 07:04:57 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64033258",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61225077,
        "Question_title":"How to delete worker task templates in AWS Augmented AI?",
        "Question_body":"<p>I've created the worker task template for test in AWS Augmented AI.\nHowever, I don't know how to delete those template.<\/p>\n\n<p>Please tell me how to do it.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/CZkiz.png\" rel=\"nofollow noreferrer\">image<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-15 09:09:10.363 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":38,
        "Owner_creation_date":"2018-07-24 08:52:28.877 UTC",
        "Owner_last_access_date":"2020-11-27 01:51:12.017 UTC",
        "Owner_reputation":27,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>You cannot currently delete HumanTaskUis. That may be a capability added in the future.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-04-22 23:37:30.403 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61225077",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53154542,
        "Question_title":"AWS SageMaker - Realtime Data Processing",
        "Question_body":"<p>My company does online consumer behavior analysis and we do realtime predictions using the data we collected from various websites (with our java script embedded). <\/p>\n\n<p>We have been using AWS ML for real time prediction but now we are experimenting with AWS SageMaker it occurred to us that the realtime data processing is a problem compared to AWS ML. For example we have some string variables that AWS ML can convert to numerics and use them for real time prediction in AWS ML automatically. But it does not look like SageMaker can do it. <\/p>\n\n<p>Does anyone have any experience with real time data processing and prediction in AWS SageMaker?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_date":"2018-11-05 12:34:09.963 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"prediction|amazon-sagemaker",
        "Question_view_count":1508,
        "Owner_creation_date":"2016-12-13 17:34:19.98 UTC",
        "Owner_last_access_date":"2022-08-25 12:34:35.243 UTC",
        "Owner_reputation":1446,
        "Owner_up_votes":102,
        "Owner_down_votes":1,
        "Owner_views":107,
        "Answer_body":"<p>AWS SageMaker is a robust machine learning service in AWS that manages every major aspect of machine learning implementation, including data preparation, model construction, training and fine-tuning, and deployment.<\/p>\n<p><strong>Preparation<\/strong><\/p>\n<p>SageMaker uses a range of resources to make it simple to prepare data for machine learning models, even though it comes from many sources or is in a variety of formats.<\/p>\n<p>It's simple to mark data, including video, images, and text, that's automatically processed into usable data, with SageMaker Ground Truth. GroundWork will process and merge this data using auto-segmentation and a suite of tools to create a single data label that can be used in machine learning models. AWS, in conjunction with SageMaker Data Wrangler and SageMaker Processing, reduces a data preparation phase that may take weeks or months to a matter of days, if not hours.<\/p>\n<p><strong>Build<\/strong><\/p>\n<p>SageMaker Studio Notebooks centralize everything relevant to your machine learning models, allowing them to be conveniently shared along with their associated data. You can choose from a variety of built-in, open-source algorithms to start processing your data with SageMaker JumpStart, or you can build custom parameters for your machine learning model.<\/p>\n<p>Once you've chosen a model, SageMaker starts processing data automatically and offers a simple, easy-to-understand interface for tracking your model's progress and performance.<\/p>\n<p><strong>Training<\/strong><\/p>\n<p>SageMaker provides a range of tools for training your model from the data you've prepared, including a built-in debugger for detecting possible errors.<\/p>\n<p>Machine Learning\nThe training job's results are saved in an Amazon S3 bucket, where they can be viewed using other AWS services including AWS Quicksight.<\/p>\n<p><strong>Deployment<\/strong><\/p>\n<p>It's pointless to have strong machine learning models if they can't be easily deployed to your hosting infrastructure. Fortunately, SageMaker allows deploying machine learning models to your current services and applications as easy as a single click.<\/p>\n<p>SageMaker allows for real-time data processing and prediction after installation. This has far-reaching consequences in a variety of areas, including finance and health. Businesses operating in the stock market, for example, may make real-time financial decisions about stock and make more attractive acquisitions by pinpointing the best time to buy.<\/p>\n<p>Incorporation with Amazon Comprehend, allows for natural language processing, transforming human speech into usable data to train better models, or provide a chatbot to customers through Amazon Lex.<\/p>\n<p><strong>In conclusion\u2026<\/strong><\/p>\n<p>Machine Learning is no longer a niche technological curiosity; it now plays a critical role in the decision-making processes of thousands of companies around the world. There has never been a better time to start your Machine Learning journey than now, with virtually unlimited frameworks and simple integration into the AWS system.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-05-04 12:38:54.8 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53154542",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60716202,
        "Question_title":"SageMaker experiments store",
        "Question_body":"<p>I just started using aws sagemaker for running and maintaining models, experiments. just wanted to know is there any persistent layer for the sagemaker from where i can get data of my experiments\/models instead of looking into the sagemaker studio. Does sagemaker saves the experiments or its data like s3 location in any table  something like modelsdb? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-17 04:18:32.033 UTC",
        "Question_favorite_count":0.0,
        "Question_last_edit_date":"2020-03-17 04:34:04.403 UTC",
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":229,
        "Owner_creation_date":"2015-03-23 17:08:17.093 UTC",
        "Owner_last_access_date":"2022-04-22 14:16:10.363 UTC",
        "Owner_reputation":187,
        "Owner_up_votes":59,
        "Owner_down_votes":0,
        "Owner_views":151,
        "Answer_body":"<p>SageMaker Studio is using the SageMaker API to pull all of the data its displaying.  Essentially there's no secret API here getting invoked.<\/p>\n\n<p>Quite a bit of what's being displayed with respect to experiments is from the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_Search.html\" rel=\"nofollow noreferrer\">search results<\/a>, the rest coming from either List* or Describe* calls.  Studio is taking the results from the search request and displaying it in the table format that you're seeing.  Search results when searching over resource ExperimentTrialComponent that have a source (such as a training job) will be enhanced with the original sources data ([result]::SourceDetail::TrainingJob) if supported (work is ongoing to add additional source detail resource types).<\/p>\n\n<p>All of the metadata that is related to resources in SageMaker is available via the APIs; there is no other location (in the cloud) like s3 for that data.<\/p>\n\n<p>As of this time there is no effort to determine if it's possible to add support into <a href=\"https:\/\/github.com\/VertaAI\/modeldb\" rel=\"nofollow noreferrer\">modeldb<\/a> for SageMaker that I'm aware of.  Given that modeldb appears to make some assumptions about it's talking to a relational database it would appear unlikely to be something that would be doable. (I only read the overview very quickly so this might be inaccurate.)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-03-18 15:42:56.82 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60716202",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71126832,
        "Question_title":"Amazon Sagemaker: User Input data validation in Inference Endpoint",
        "Question_body":"<p>I have successfully built a Sagemaker endpoint using a Tensorflow model. The pre and post processing is done inside &quot;inference.py&quot; which calls a handler function based on this tutorial: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s<\/a><\/p>\n<p>My questions are:<\/p>\n<ul>\n<li>Which method is good for validating user input data within inference.py?<\/li>\n<li>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/li>\n<li>How is this compatible with the API gateway placed above the endpoint?<\/li>\n<\/ul>\n<p>Here is the structure of the inference.py with the desired validation check as a comment:<\/p>\n<pre><code>import json\nimport requests\n\n\ndef handler(data, context):\n    &quot;&quot;&quot;Handle request.\n    Args:\n        data (obj): the request data\n        context (Context): an object containing request and configuration details\n    Returns:\n        (bytes, string): data to return to client, (optional) response content type\n    &quot;&quot;&quot;\n    processed_input = _process_input(data, context)\n    response = requests.post(context.rest_uri, data=processed_input)\n    return _process_output(response, context)\n\n\ndef _process_input(data, context):\n    if context.request_content_type == 'application\/json':\n        # pass through json (assumes it's correctly formed)\n        d = data.read().decode('utf-8')\n        data_dict = json.loads(data)\n\n\n        # -----&gt;   if data_dict['input_1'] &gt; 25000:\n        # -----&gt;       return some error specific message with status code 123\n\n\n        return some_preprocessing_function(data_dict)\n\n    raise ValueError('{{&quot;error&quot;: &quot;unsupported content type {}&quot;}}'.format(\n        context.request_content_type or &quot;unknown&quot;))\n\n\ndef _process_output(data, context):\n    if data.status_code != 200:\n        raise ValueError(data.content.decode('utf-8'))\n\n    response_content_type = context.accept_header\n    prediction = data.content\n    return prediction, response_content_type\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-15 12:56:47.89 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2022-02-15 13:25:22.733 UTC",
        "Question_score":0,
        "Question_tags":"python|validation|amazon-sagemaker|endpoint|inference",
        "Question_view_count":245,
        "Owner_creation_date":"2021-02-18 15:25:28.947 UTC",
        "Owner_last_access_date":"2022-09-23 10:35:08.913 UTC",
        "Owner_reputation":160,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":"<p>I will answer your questions inline below:<\/p>\n<ol>\n<li><em>Which method is good for validating user input data within inference.py?<\/em><\/li>\n<\/ol>\n<p>Seeing that you have a <code>handler<\/code> function, <code>input_handler<\/code> and <code>output_handler<\/code> are ignored. Thus, inside your <code>handler<\/code> function (as you are correctly doing) you can have the validation logic.<\/p>\n<ol start=\"2\">\n<li><em>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/em><\/li>\n<\/ol>\n<p>I like to think of my SageMaker endpoint as a web server. Thus, you can return any valid HTTP response code with a response message. Please see this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_batch_transform\/tensorflow_cifar-10_with_inference_script\/code\/inference.py#L47\" rel=\"nofollow noreferrer\">inference.py<\/a> file that I found as a reference.<\/p>\n<pre><code>_return_error(\n            415, 'Unsupported content type &quot;{}&quot;'.format(context.request_content_type or &quot;Unknown&quot;)\n        )\n\ndef _return_error(code, message):\n    raise ValueError(&quot;Error: {}, {}&quot;.format(str(code), message))\n<\/code><\/pre>\n<ol start=\"3\">\n<li><em>How is this compatible with the API gateway placed above the endpoint?<\/em><\/li>\n<\/ol>\n<p>Please see this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">link<\/a> for details on Creating a machine learning-powered REST API with Amazon API Gateway mapping templates and Amazon SageMaker.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-22 21:59:21.52 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71126832",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70486162,
        "Question_title":"Conflicting Python versions in SageMaker Studio notebook with Python 3.8 kernel",
        "Question_body":"<p>I'm trying to run a SageMaker kernel with Python 3.8 in SageMaker Studio, and the notebook appears to use a separate distribution of Python 3.7. The <em>running app<\/em> is indicated as <em>tensorflow-2.6-cpu-py38-ubuntu20.04-v1<\/em>. When I run <code>!python3 -V<\/code> I get <em>Python 3.8.2<\/em>. However, the Python instance inside the notebook is different:<\/p>\n<pre><code>import sys\nsys.version\n<\/code><\/pre>\n<p>gives <code>'3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) \\n[GCC 9.4.0]'<\/code><\/p>\n<p>Similarly, running <code>%pip -V<\/code> and <code>%conda info<\/code> indicates Python 3.7.<\/p>\n<p>Also, <code>import tensorflow<\/code> fails, as it isn't preinstalled in the Python environment that the notebook invokes.<\/p>\n<p>I'm running in the <em>eu-west-2<\/em> region. Is there anything I can do to address this short of opening a support ticket?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-12-26 11:42:01.197 UTC",
        "Question_favorite_count":3.0,
        "Question_last_edit_date":null,
        "Question_score":5,
        "Question_tags":"python-3.x|amazon-web-services|jupyter-notebook|amazon-sagemaker|anaconda3",
        "Question_view_count":768,
        "Owner_creation_date":"2012-03-14 12:18:03.733 UTC",
        "Owner_last_access_date":"2022-09-23 12:09:26.313 UTC",
        "Owner_reputation":1060,
        "Owner_up_votes":102,
        "Owner_down_votes":0,
        "Owner_views":139,
        "Answer_body":"<p>are you still facing this issue?<\/p>\n<p>I am in eu-west-2 using a SageMaker Studio notebook and the TensorFlow 2.6 Python 3.8 CPU Optimized image (running app is tensorflow-2.6-cpu-py38-ubuntu20.04-v1).<\/p>\n<p>When I run the below commands, I get the right outputs.<\/p>\n<pre><code>!python3 -V\n<\/code><\/pre>\n<p>returns Python 3.8.2<\/p>\n<pre><code>import sys\nsys.version \n<\/code><\/pre>\n<p>returns\n3.8.2 (default, Dec  9 2021, 06:26:16) \\n[GCC 9.3.0]'<\/p>\n<pre><code>import tensorflow as tf\nprint(tf.__version__)\n<\/code><\/pre>\n<p>returns 2.6.2<\/p>\n<p>It seems this has now been fixed<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-25 13:00:46.503 UTC",
        "Answer_score":1.0,
        "Owner_location":"London, UK",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70486162",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62602435,
        "Question_title":"How to train tensorflow on sagemaker in script mode when the data resides in multiple files on s3?",
        "Question_body":"<p>I have a <code>.npy<\/code> file for each one of the training instances. All of these files are available on S3 in <code>train_data<\/code> folder. I want to train a tensorflow model on these training instances. To do that, I wish to spin up separate aws training instance for each training job which could access the files from s3 and train the model on it. What changes in the training script are required for doing this?<\/p>\n<p>I have following config in the training script:<\/p>\n<pre><code>parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])\nparser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\nparser.add_argument('--train_channel', type=str, default=os.environ['SM_CHANNELS'])\n<\/code><\/pre>\n<p>I have created the training estimator in jupyter instance as:<\/p>\n<pre><code>tf_estimator = TensorFlow(entry_point = 'my_model.py', \n                          role = role, \n                          train_instance_count = 1, \n                          train_instance_type = 'local_gpu', \n                          framework_version = '1.15.2', \n                          py_version = 'py3', \n                          hyperparameters = {'epochs': 1})\n<\/code><\/pre>\n<p>I am calling the fit function of the estimator as:<\/p>\n<pre><code>tf_estimator.fit({'train_channel':'s3:\/\/sagemaker-ml\/train_data\/'})\n<\/code><\/pre>\n<p>where <code>train_data<\/code> folder on S3 contains the <code>.npy<\/code> files of training instances.<\/p>\n<p>But when I call the fit function, I get an error:<\/p>\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '[&quot;train_channel&quot;]\/train_data_12.npy'\n<\/code><\/pre>\n<p>Not sure what am I missing here, as I can see the file mentioned above on S3.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-26 20:31:26.503 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-06-26 20:37:29.583 UTC",
        "Question_score":0,
        "Question_tags":"amazon-s3|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":340,
        "Owner_creation_date":"2013-09-04 04:27:22.847 UTC",
        "Owner_last_access_date":"2022-09-22 00:59:12.557 UTC",
        "Owner_reputation":4616,
        "Owner_up_votes":751,
        "Owner_down_votes":5,
        "Owner_views":592,
        "Answer_body":"<p><code>SM_CHANNELS<\/code> returns a list of channel names. What you're looking for is <code>SM_CHANNEL_TRAIN_CHANNEL<\/code> (&quot;SM_CHANNEL&quot; + your channel name), which provides the filesystem location for the channel:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>parser.add_argument('--train_channel', type=str, default=os.environ['SM_CHANNEL_TRAIN_CHANNEL'])\n<\/code><\/pre>\n<p>docs: <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_channel_channel_name\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_channel_channel_name<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-30 15:18:34.24 UTC",
        "Answer_score":1.0,
        "Owner_location":"Pune, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62602435",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51117133,
        "Question_title":"AWS Sagemaker - Install External Library and Make it Persist",
        "Question_body":"<p>I have a sagemaker instance up and running and I have a few libraries that I frequently use with it but each time I restart the instance they get wiped and I have to reinstall them. Is it possible to install my libraries to one of the anaconda environments and have the change remain?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-06-30 17:34:14.783 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":13,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":10578,
        "Owner_creation_date":"2017-03-23 00:32:36.263 UTC",
        "Owner_last_access_date":"2021-03-22 21:38:40.833 UTC",
        "Owner_reputation":401,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":"<p>The supported way to do this for Sagemaker notebook instances is with <strong>Lifecycle Configurations<\/strong>.<\/p>\n\n<p>You can create an <strong>onStart<\/strong> lifecycle hook that can install the required packages into the respective Conda environments each time your notebook instance starts.<\/p>\n\n<p>Please see the following blog post for more details<\/p>\n\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access\/<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-07-02 18:19:26.567 UTC",
        "Answer_score":12.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51117133",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56771758,
        "Question_title":"On-Premises Hosting of Trained DeepAR Model built on AWS SageMaker",
        "Question_body":"<p>I have started working with <strong>AWS SageMaker<\/strong> recently with the examples provided by AWS. I used this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/deepar_electricity\/DeepAR-Electricity.ipynb\" rel=\"nofollow noreferrer\">example<\/a> (<strong>DeepAR<\/strong> Model) in order to forecast a time series. After training, a model artifacts file has been created in my S3 bucket. <\/p>\n\n<p><strong>My question:<\/strong> Is there a way to host that trained model in a own hosting environment? (client premises)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-06-26 11:28:05.583 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":379,
        "Owner_creation_date":"2015-08-19 14:12:40.103 UTC",
        "Owner_last_access_date":"2022-09-23 15:54:06.173 UTC",
        "Owner_reputation":155,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":76,
        "Answer_body":"<p>Except SageMaker XGBoost, SageMaker built-in algorithms are not designed to be used out of Amazon. That does not mean that it's impossible, for example you can find here and there snippets peeking inside model artifacts (eg for <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations\/\" rel=\"nofollow noreferrer\">Factorization Machines<\/a> and <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/ntm_20newsgroups_topic_modeling\/ntm_20newsgroups_topic_model.ipynb\" rel=\"nofollow noreferrer\">Neural Topic Model<\/a>) but these things can be hacky and are usually not part of official service features. Regarding DeepAR specifically, the model was open-sourced couple weeks ago as part of <code>gluon-ts<\/code> python package (<a href=\"https:\/\/aws.amazon.com\/blogs\/opensource\/gluon-time-series-open-source-time-series-modeling-toolkit\/\" rel=\"nofollow noreferrer\">blog post<\/a>, <a href=\"https:\/\/gluon-ts.mxnet.io\/\" rel=\"nofollow noreferrer\">code<\/a>) so if you develop a model specifically for your own hosting environment I'd recommend to use that gluon-ts code in the MXNet container, so that you'll be able to open and read the artifact out of SageMaker.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-06-30 19:05:41.28 UTC",
        "Answer_score":1.0,
        "Owner_location":"Lebanon",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56771758",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68790568,
        "Question_title":"\"errorMessage\": \"Parameter validation failed in Lambda calling SageMaker endpoint",
        "Question_body":"<p>I am trying to invoke a SageMaker enpoint from AWS Lambda using a lambda function.<\/p>\n<p>This is a sample API call to the endpoint from SageMaker Studio, working as expected:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3iTPN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3iTPN.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>here's my Lambda function (<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">inspired from documentation<\/a>):<\/p>\n<pre><code>import os\nimport io\nimport boto3\nimport json\n\n\nENDPOINT_NAME = 'iris-autoscale-6'\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    # print(&quot;Received event: &quot; + json.dumps(event, indent=2))\n    payload = json.loads(json.dumps(event))\n    print(payload)\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\n    print(response)\n    result = json.loads(response['Body'].read().decode())\n    print(result)\n    \n    return result\n<\/code><\/pre>\n<p>My error message:<\/p>\n<pre><code>Test Event Name\nProperTest\n\nResponse\n{\n  &quot;errorMessage&quot;: &quot;Parameter validation failed:\\nInvalid type for parameter Body, value: {'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object&quot;,\n  &quot;errorType&quot;: &quot;ParamValidationError&quot;,\n  &quot;stackTrace&quot;: [\n    &quot;  File \\&quot;\/var\/task\/lambda_function.py\\&quot;, line 17, in lambda_handler\\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 386, in _api_call\\n    return self._make_api_call(operation_name, kwargs)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 678, in _make_api_call\\n    api_params, operation_model, context=request_context)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 726, in _convert_to_request_dict\\n    api_params, operation_model)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/validate.py\\&quot;, line 319, in serialize_to_request\\n    raise ParamValidationError(report=report.generate_report())\\n&quot;\n  ]\n}\n\nFunction Logs\nSTART RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512 Version: $LATEST\n{'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}\n[ERROR] ParamValidationError: Parameter validation failed:\nInvalid type for parameter Body, value: {'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\nTraceback (most recent call last):\n\u00a0\u00a0File &quot;\/var\/task\/lambda_function.py&quot;, line 17, in lambda_handler\n\u00a0\u00a0\u00a0\u00a0response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 386, in _api_call\n\u00a0\u00a0\u00a0\u00a0return self._make_api_call(operation_name, kwargs)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 678, in _make_api_call\n\u00a0\u00a0\u00a0\u00a0api_params, operation_model, context=request_context)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 726, in _convert_to_request_dict\n\u00a0\u00a0\u00a0\u00a0api_params, operation_model)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/validate.py&quot;, line 319, in serialize_to_request\n\u00a0\u00a0\u00a0\u00a0raise ParamValidationError(report=report.generate_report())\nEND RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512\nREPORT RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512  Duration: 26.70 ms  Billed Duration: 27 ms  Memory Size: 128 MB Max Memory Used: 76 MB  Init Duration: 343.10 ms\n<\/code><\/pre>\n<p>Here's the policy attached to the lambda function:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;VisualEditor0&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:sagemaker:ap-south-1:&lt;my-account-id&gt;:endpoint\/iris-autoscale-6&quot;\n        }\n    ]\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-15 10:10:35.437 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-08-15 10:28:29.567 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-lambda|boto3|amazon-sagemaker",
        "Question_view_count":499,
        "Owner_creation_date":"2019-06-07 12:24:06.18 UTC",
        "Owner_last_access_date":"2022-09-24 17:19:11.323 UTC",
        "Owner_reputation":2046,
        "Owner_up_votes":2858,
        "Owner_down_votes":5,
        "Owner_views":369,
        "Answer_body":"<p>The issue is that your <code>payload<\/code> has invalid format. It should be one of:<\/p>\n<pre><code>&lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\n<\/code><\/pre>\n<p>The following should address the error (note: you may have many other issues in your code):<\/p>\n<pre><code>    payload = json.dumps(event)\n    print(payload)\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload.encode())\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-08-15 10:47:14.667 UTC",
        "Answer_score":1.0,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68790568",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49064183,
        "Question_title":"How to set the percentage of inference calls when performing A\/B testing using AWS sagemaker?",
        "Question_body":"<p>I'm new to sagemaker. I'm trying to figure out how to perform A\/B testing using AWS sagemaker. I understand setting the train_instance_count will distribute the training across two instances. But how do I specify the set the percentage of inference calls each model will handle and perform A\/B testing? \nThis is all I could find from the docs <\/p>\n\n<blockquote>\n  <p>\"Amazon SageMaker can also manage model A\/B testing for you. You can\n  configure the endpoint to spread traffic across as many as five\n  different models and set the percentage of inference calls you want\n  each one to handle. You can change all of this on the fly, giving you\n  a lot of flexibility to run experiments and determine which model\n  produces the most accurate results in the real world.\"<\/p>\n<\/blockquote>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-03-02 07:00:37.75 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-ec2|amazon-sagemaker",
        "Question_view_count":1042,
        "Owner_creation_date":"2015-12-16 10:02:46.773 UTC",
        "Owner_last_access_date":"2022-09-23 17:46:43.357 UTC",
        "Owner_reputation":1587,
        "Owner_up_votes":123,
        "Owner_down_votes":8,
        "Owner_views":540,
        "Answer_body":"<p>You can have multiple Production Variants behind an Amazon SageMaker endpoint. Each production variant has an initial variant weight and based on the ratio of each variant weight to the total sum of weights, SageMaker can distribute the calls to each of the models. For example, if you have only one production variant with a weight of 1, all traffic will go to this variant. If you add another production variant with an initial weight of 2, the new variant will get 2\/3 of the traffic and the first variant will get 1\/3. <\/p>\n\n<p>You can see more details on ProductionVariant on Amazon SageMaker documentations here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_ProductionVariant.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_ProductionVariant.html<\/a> <\/p>\n\n<p>You can provide an array of ProductionVariants when you \"Create Endpoint Configuration\": <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpointConfig.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpointConfig.html<\/a> , and you can update the variants with \"Update Endpoint Weights and Capacities\" call: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_UpdateEndpointWeightsAndCapacities.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_UpdateEndpointWeightsAndCapacities.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-03-04 19:11:02.82 UTC",
        "Answer_score":3.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49064183",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71655510,
        "Question_title":"How to see all SageMaker service quota limits?",
        "Question_body":"<p>I believe there are different limits for SageMaker training, vs CreateTransformJob, spot vs not dedicated. Where can I see the current service limits for sagemaker services? Is there a place to check all SageMaker service quotas?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-29 01:39:42.11 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":215,
        "Owner_creation_date":"2015-01-15 17:43:03.7 UTC",
        "Owner_last_access_date":"2022-08-23 22:54:25.603 UTC",
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Answer_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html\" rel=\"nofollow noreferrer\">Here<\/a> in the documentation you can see the default sagemaker service quotas. Unfortunately, it's not yet possible to see the current quotas according to this <a href=\"https:\/\/repost.aws\/questions\/QUweO83CSlTu-3Zn2RxdESWg\/how-do-i-check-my-current-sage-maker-service-quotas\" rel=\"nofollow noreferrer\">post<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-03-29 07:21:07.293 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71655510",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73676483,
        "Question_title":"Can Horovod with TensorFlow work on non-GPU instances in Amazon SageMaker?",
        "Question_body":"<p>I want to perform <strong>distributed training<\/strong> on <strong>Amazon SageMaker<\/strong>. The code is written with <strong>TensorFlow<\/strong> and similar to the following code where I think CPU instance should be enough:\u00a0\n<a href=\"https:\/\/github.com\/horovod\/horovod\/blob\/master\/examples\/tensorflow_word2vec.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/horovod\/horovod\/blob\/master\/examples\/tensorflow_word2vec.py<\/a><\/p>\n<p>Can <strong>Horovod with TensorFlow<\/strong> work on <strong>non-GPU<\/strong> instances in Amazon SageMaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-11 02:50:03.78 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker|distributed-training|horovod",
        "Question_view_count":17,
        "Owner_creation_date":"2014-01-16 15:43:59.673 UTC",
        "Owner_last_access_date":"2022-09-25 03:22:08.463 UTC",
        "Owner_reputation":5854,
        "Owner_up_votes":155,
        "Owner_down_votes":70,
        "Owner_views":794,
        "Answer_body":"<p>Yeah you should be able to use both CPU's and GPU's with Horovod on Amazon SageMaker. Please follow the below example for the same<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/tensorflow_script_mode_horovod\/tensorflow_script_mode_horovod.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/tensorflow_script_mode_horovod\/tensorflow_script_mode_horovod.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-14 23:27:55.693 UTC",
        "Answer_score":0.0,
        "Owner_location":"Singapore",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73676483",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65941675,
        "Question_title":"Can gzip tar files be used for training data in Sagemaker?",
        "Question_body":"<p>I have 50TB of uncompressed data (images) that is in dozens of tar.gz files in S3. I'm training tensorflow models with a dozen of these tar.gz files at a time. I would like to use a Sagemaker training job to pull this data and unpack it before training. Is this possible? Do I have to change the way that the data is stored before running training?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-28 16:54:46.873 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":433,
        "Owner_creation_date":"2016-10-20 18:11:51.723 UTC",
        "Owner_last_access_date":"2022-09-23 19:32:59.27 UTC",
        "Owner_reputation":2021,
        "Owner_up_votes":1876,
        "Owner_down_votes":4,
        "Owner_views":134,
        "Answer_body":"<p><strong>Short Answer<\/strong> : No<\/p>\n<p><strong>Long Answer<\/strong>:\nThe recommended way to use Sagemaker with very large datasets is to use the Pipe API (as opposed to the File Api) which streams data to the training image rather than downloading the data. To take advantage of the Pipe API the data will need to be in one of the supported file types: <strong>text records, TFRecord or Protobuf<\/strong><\/p>\n<p>The benefits are<\/p>\n<ol>\n<li>reducing delay when the container is launched<\/li>\n<li>not needing to scale the instance storage to the size of the training data<\/li>\n<li>increasing throughput by moving most preprocessing before model training<\/li>\n<\/ol>\n<p>References:<\/p>\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/<\/a><\/li>\n<li><a href=\"https:\/\/julsimon.medium.com\/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233\" rel=\"nofollow noreferrer\">https:\/\/julsimon.medium.com\/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233<\/a> (This is a fantastic resource which answers a lot of questions regarding using Sagemaker on very large datasets)<\/li>\n<li><a href=\"https:\/\/julsimon.medium.com\/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c\" rel=\"nofollow noreferrer\">https:\/\/julsimon.medium.com\/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c<\/a><\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-01-28 18:22:51.047 UTC",
        "Answer_score":1.0,
        "Owner_location":"Canada",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65941675",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62734994,
        "Question_title":"Why package is not updated even the lifecycle script has been executed successfully in SageMaker?",
        "Question_body":"<p>I wanted to update pandas version in 'conda-python3' in SageMaker, I've followed the steps in this <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">page<\/a>, and linked the new configuration to my instance, CloudWatch log shows me the script has been executed successfully, but when I restart my instance and print out the panda version, it's still showing the old version 0.24.2, I don't understand why?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fR82t.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fR82t.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>This is the script in the lifecycle configuration:<\/p>\n<pre><code>#!\/bin\/bash\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\npip install pandas\n\nconda update pandas\n\nsource deactivate\n\nEOF\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-04 22:17:16.277 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|pandas|amazon-web-services|conda|amazon-sagemaker",
        "Question_view_count":106,
        "Owner_creation_date":"2018-10-30 17:35:56.27 UTC",
        "Owner_last_access_date":"2022-09-22 19:30:36.883 UTC",
        "Owner_reputation":2385,
        "Owner_up_votes":1007,
        "Owner_down_votes":16,
        "Owner_views":585,
        "Answer_body":"<p>You are not activating any conda environment such as <a href=\"https:\/\/stackoverflow.com\/questions\/60036916\/sagemaker-lifecycle-configuration-for-installing-pandas-not-working\">python3<\/a>.<\/p>\n<pre><code>#!\/bin\/bash\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\n# This will affect only the Jupyter kernel called &quot;conda_python3&quot;.\nsource activate python3\n\npip install pandas\n\nconda update pandas\n\nsource deactivate\n\nEOF\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-10 05:34:20.763 UTC",
        "Answer_score":1.0,
        "Owner_location":"United Kingdom",
        "Answer_last_edit_date":"2020-11-16 21:55:49.573 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62734994",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65168915,
        "Question_title":"AWS SageMaker - How to load trained sklearn model to serve for inference?",
        "Question_body":"<p>I am trying to deploy a model trained with sklearn to an endpoint and serve it as an API for predictions. All I want to use sagemaker for, is to deploy and server model I had serialised using <code>joblib<\/code>, nothing more. every blog I have read and sagemaker python documentation showed that sklearn model had to be trained on sagemaker in order to be deployed in sagemaker.<\/p>\n<p>When I was going through the SageMaker documentation I learned that sagemaker does let users <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#load-a-model\" rel=\"nofollow noreferrer\">load a serialised model<\/a> stored in S3 as shown below:<\/p>\n<pre><code>def model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return clf\n<\/code><\/pre>\n<p>And this is what documentation says about the argument <code>model_dir<\/code>:<\/p>\n<blockquote>\n<p>SageMaker will inject the directory where your model files and\nsub-directories, saved by save, have been mounted. Your model function\nshould return a model object that can be used for model serving.<\/p>\n<\/blockquote>\n<p>This again means that training has to be done on sagemaker.<\/p>\n<p>So, is there a way I can just specify the S3 location of my serialised model and have sagemaker de-serialise(or load) the model from S3 and use it for inference?<\/p>\n<h2>EDIT 1:<\/h2>\n<p>I used code in the answer to my application and I got below error when trying to deploy from notebook of SageMaker studio. I believe SageMaker is screaming that training wasn't done on SageMaker.<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-4-6662bbae6010&gt; in &lt;module&gt;\n      1 predictor = model.deploy(\n      2     initial_instance_count=1,\n----&gt; 3     instance_type='ml.m4.xlarge'\n      4 )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, **kwargs)\n    770         &quot;&quot;&quot;\n    771         removed_kwargs(&quot;update_endpoint&quot;, kwargs)\n--&gt; 772         self._ensure_latest_training_job()\n    773         self._ensure_base_job_name()\n    774         default_name = name_from_base(self.base_job_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in _ensure_latest_training_job(self, error_message)\n   1128         &quot;&quot;&quot;\n   1129         if self.latest_training_job is None:\n-&gt; 1130             raise ValueError(error_message)\n   1131 \n   1132     delete_endpoint = removed_function(&quot;delete_endpoint&quot;)\n\nValueError: Estimator is not associated with a training job\n<\/code><\/pre>\n<p>My code:<\/p>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\n# from sagemaker.pytorch import PyTorchModel\nfrom sagemaker.sklearn import SKLearn\nfrom sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n\nsm_role = sagemaker.get_execution_role()  # IAM role to run SageMaker, access S3 and ECR\n\nmodel_file = &quot;s3:\/\/sagemaker-manual-bucket\/sm_model_artifacts\/model.tar.gz&quot;   # Must be &quot;.tar.gz&quot; suffix\n\nclass AnalysisClass(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super().__init__(\n            endpoint_name,\n            sagemaker_session=sagemaker_session,\n            serializer=json_serializer,\n            deserializer=json_deserializer,   # To be able to use JSON serialization\n            content_type='application\/json'   # To be able to send JSON as HTTP body\n        )\n\nmodel = SKLearn(model_data=model_file,\n                entry_point='inference.py',\n                name='rf_try_1',\n                role=sm_role,\n                source_dir='code',\n                framework_version='0.20.0',\n                instance_count=1,\n                instance_type='ml.m4.xlarge',\n                predictor_cls=AnalysisClass)\npredictor = model.deploy(initial_instance_count=1,\n                         instance_type='ml.m4.xlarge')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-12-06 14:02:12.14 UTC",
        "Question_favorite_count":3.0,
        "Question_last_edit_date":"2020-12-08 03:54:05.213 UTC",
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-s3|scikit-learn|amazon-sagemaker",
        "Question_view_count":3221,
        "Owner_creation_date":"2019-06-07 12:24:06.18 UTC",
        "Owner_last_access_date":"2022-09-24 17:19:11.323 UTC",
        "Owner_reputation":2046,
        "Owner_up_votes":2858,
        "Owner_down_votes":5,
        "Owner_views":369,
        "Answer_body":"<p>Yes you can. AWS documentation focuses on end-to-end from training to deployment in SageMaker which makes the impression that training has to be done on sagemaker. AWS documentation and examples should have clear separation among Training in Estimator, Saving and loading model, and Deployment model to SageMaker Endpoint.<\/p>\n<h2>SageMaker Model<\/h2>\n<p>You need to create the <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-model.html\" rel=\"nofollow noreferrer\">AWS::SageMaker::Model<\/a> resource which refers to the &quot;model&quot; you have trained <strong>and more<\/strong>. AWS::SageMaker::Model is in CloudFormation document but it is only to explain what AWS resource you need.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\">CreateModel<\/a> API creates a SageMaker model resource. The parameters specifie the docker image to use, model location in S3, IAM role to use, etc. See <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-code-load-artifacts\" rel=\"nofollow noreferrer\">How SageMaker Loads Your Model Artifacts<\/a>.<\/p>\n<h3>Docker image<\/h3>\n<p>Obviously you need the framework e.g. ScikitLearn, TensorFlow, PyTorch, etc that you used to train your model to get inferences. You need a docker image that has the framework, and HTTP front end to respond to the prediction calls. See <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\" rel=\"nofollow noreferrer\">SageMaker Inference Toolkit<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-toolkits.html\" rel=\"nofollow noreferrer\">Using the SageMaker Training and Inference Toolkits<\/a>.<\/p>\n<p>To build the image is not easy. Hence AWS provides pre-built images called <a href=\"https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\" rel=\"nofollow noreferrer\">AWS Deep Learning Containers<\/a> and available images are in <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">Github<\/a>.<\/p>\n<p>If your framework and the version is listed there, you can use it as the image. Otherwise you need to build by yourself. See <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-mlops-workshop\/blob\/master\/lab\/01_CreateAlgorithmContainer\/01_Creating%20a%20Classifier%20Container.ipynb\" rel=\"nofollow noreferrer\">Building a docker container for training\/deploying our classifier<\/a>.<\/p>\n<h2>SageMaker Python SDK for Frameworks<\/h2>\n<p>Create SageMaker Model by yourself using API is hard. Hence AWS SageMaker Python SDK has provided utilities to create the SageMaker models for several frameworks. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/index.html\" rel=\"nofollow noreferrer\">Frameworks<\/a> for available frameworks. If it is not there, you may still be able to use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.FrameworkModel\" rel=\"nofollow noreferrer\">sagemaker.model.FrameworkModel<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"nofollow noreferrer\">Model<\/a> to load your trained model. For your case, see <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html\" rel=\"nofollow noreferrer\">Using Scikit-learn with the SageMaker Python SDK<\/a>.<\/p>\n<h3>model.tar.gz<\/h3>\n<p>For instance if you used PyTorch and save the model as model.pth. To load the model and the inference code to get the prediction from the model, you need to create a model.tar.gz file. The structure inside the model.tar.gz is explained in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#model-directory-structure\" rel=\"nofollow noreferrer\">Model Directory Structure<\/a>. If you use Windows, beware of the CRLF to LF. AWS SageMaker runs in *NIX environment. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-the-directory-structure-for-your-model-files\" rel=\"nofollow noreferrer\">Create the directory structure for your model files<\/a>.<\/p>\n<pre><code>|- model.pth        # model file is inside \/ directory.\n|- code\/            # Code artefacts must be inside \/code\n  |- inference.py   # Your inference code for the framework\n  |- requirements.txt  # only for versions 1.3.1 and higher. Name must be &quot;requirements.txt&quot;\n<\/code><\/pre>\n<p>Save the tar.gz file in S3. Make sure of the IAM role to access the S3 bucket and objects.<\/p>\n<h3>Loading model and get inference<\/h3>\n<p>See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-a-pytorchmodel-object\" rel=\"nofollow noreferrer\">Create a PyTorchModel object<\/a>. When instantiating the PyTorchModel class, SageMaker automatically selects the AWS Deep Learning Container image for PyTorch for the version specified in <strong>framework_version<\/strong>. If the image for the version does not exist, then it fails. This has not been documented in AWS but need to be aware of. SageMaker then internally calls the CreateModel API with the S3 model file location and the AWS Deep Learning Container image URL.<\/p>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.pytorch import PyTorchModel\nfrom sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n\nrole = sagemaker.get_execution_role()  # IAM role to run SageMaker, access S3 and ECR\nmodel_file = &quot;s3:\/\/YOUR_BUCKET\/YOUR_FOLDER\/model.tar.gz&quot;   # Must be &quot;.tar.gz&quot; suffix\n\n\nclass AnalysisClass(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super().__init__(\n            endpoint_name,\n            sagemaker_session=sagemaker_session,\n            serializer=json_serializer,\n            deserializer=json_deserializer,   # To be able to use JSON serialization\n            content_type='application\/json'   # To be able to send JSON as HTTP body\n        )\n\nmodel = PyTorchModel(\n    model_data=model_file,\n    name='YOUR_MODEL_NAME_WHATEVER',\n    role=role,\n    entry_point='inference.py',\n    source_dir='code',              # Location of the inference code\n    framework_version='1.5.0',      # Availble AWS Deep Learning PyTorch container version must be specified\n    predictor_cls=AnalysisClass     # To specify the HTTP request body format (application\/json)\n)\n\npredictor = model.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.xlarge'\n)\n\ntest_data = {&quot;body&quot;: &quot;YOUR PREDICTION REQUEST&quot;}\nprediction = predictor.predict(test_data)\n<\/code><\/pre>\n<p>By default, SageMaker uses NumPy as the serialization format. To be able to use JSON, need to specify the serializer and content_type. Instead of using RealTimePredictor class, you can specify them to predictor.<\/p>\n<pre><code>predictor.serializer=json_serializer\npredictor.predict(test_data)\n<\/code><\/pre>\n<p>Or<\/p>\n<pre><code>predictor.serializer=None # As the serializer is None, predictor won't serialize the data\nserialized_test_data=json.dumps(test_data) \npredictor.predict(serialized_test_data)\n<\/code><\/pre>\n<h3>Inference code sample<\/h3>\n<p>See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#process-model-input\" rel=\"nofollow noreferrer\">Process Model Input<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#get-predictions-from-a-pytorch-model\" rel=\"nofollow noreferrer\">Get Predictions from a PyTorch Model<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#process-model-output\" rel=\"nofollow noreferrer\">Process Model Output<\/a>. The prediction request is sent as JSON in HTTP request body in this example.<\/p>\n<pre><code>import os\nimport sys\nimport datetime\nimport json\nimport torch\nimport numpy as np\n\nCONTENT_TYPE_JSON = 'application\/json'\n\ndef model_fn(model_dir):\n    # SageMaker automatically load the model.tar.gz from the S3 and \n    # mount the folders inside the docker container. The  'model_dir'\n    # points to the root of the extracted tar.gz file.\n\n    model_path = f'{model_dir}\/'\n    \n    # Load the model\n    # You can load whatever from the Internet, S3, wherever &lt;--- Answer to your Question\n    # NO Need to use the model in tar.gz. You can place a dummy model file.\n    ...\n\n    return model\n\n\ndef predict_fn(input_data, model):\n    # Do your inference\n    ...\n\ndef input_fn(serialized_input_data, content_type=CONTENT_TYPE_JSON):\n    input_data = json.loads(serialized_input_data)\n    return input_data\n\n\ndef output_fn(prediction_output, accept=CONTENT_TYPE_JSON):\n    if accept == CONTENT_TYPE_JSON:\n        return json.dumps(prediction_output), accept\n    raise Exception('Unsupported content type') \n<\/code><\/pre>\n<h2>Related<\/h2>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker\" rel=\"nofollow noreferrer\">Using Models Trained Outside of Amazon SageMaker\n<\/a><\/li>\n<\/ul>\n<h2>Note<\/h2>\n<p>SageMaker team keeps changing the implementations and the documentations are frequently obsolete. When you are sure you did follow the documents and it does not work, obsolete documentation is quite likely. In such case, need to clarify with AWS support, or open an issue in the Github.<\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_date":"2020-12-06 22:17:54.05 UTC",
        "Answer_score":7.0,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Answer_last_edit_date":"2021-08-31 05:56:52.78 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65168915",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56773989,
        "Question_title":"In Amazon SageMaker, what (if any) is the difference between an inference and prediction?",
        "Question_body":"<p>Amazon SageMaker has <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">inference pipelines<\/a> that process requests for inferences on data. It sounds as though inferences are similar (or perhaps identical) to predictions. Are there any differences between inferences and predictions? If so, what? If not, why not just call it a prediction pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-06-26 13:27:38.777 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":42,
        "Owner_creation_date":"2012-05-14 05:36:47.643 UTC",
        "Owner_last_access_date":"2022-06-20 18:19:22.853 UTC",
        "Owner_reputation":1907,
        "Owner_up_votes":692,
        "Owner_down_votes":6,
        "Owner_views":174,
        "Answer_body":"<p>Inference usually refers to applying a learned transformation to input data. That learned transformation could be something else than a prediction (eg dim reduction, clustering, entity extraction etc). So calling that process a prediction would be a bit too restrictive in my opinion<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-06-26 17:24:45.853 UTC",
        "Answer_score":1.0,
        "Owner_location":"Minneapolis, MN, United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56773989",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72874937,
        "Question_title":"Is it possible set up an endpoint for a model I created in AWS SageMaker without using the SageMaker SDK",
        "Question_body":"<p>I've created my own model on a AWS SageMaker instance, with my own training and inference loops. I want to deploy it so that I can call the model for inference from AWS Lambda.<\/p>\n<p>I didn't use the SageMaker package to develop at all, but every tutorial (here is <a href=\"https:\/\/towardsdatascience.com\/using-aws-sagemaker-and-lambda-function-to-build-a-serverless-ml-platform-f14b3ec5854a%3E\" rel=\"nofollow noreferrer\">one<\/a>) I've looked at does so.<\/p>\n<p>How do I create an endpoint without using the SageMaker package.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-05 20:01:39.687 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|aws-lambda|amazon-sagemaker|endpoint",
        "Question_view_count":66,
        "Owner_creation_date":"2022-07-05 19:52:34.84 UTC",
        "Owner_last_access_date":"2022-09-21 21:49:37.727 UTC",
        "Owner_reputation":15,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>You can use the boto3 library to do this.<\/p>\n<p>Here is an example of pseudo code for this -<\/p>\n<pre><code>import boto3\nsm_client = boto3.client('sagemaker')\ncreate_model_respose = sm_client.create_model(ModelName=model_name, ExecutionRoleArn=role, Containers=[container] )\n\ncreate_endpoint_config_response = sm_client.create_endpoint_config(EndpointConfigName=endpoint_config_name)\n\ncreate_endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-05 20:42:25.44 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72874937",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55580232,
        "Question_title":"Update SageMaker Jupyterlab environment",
        "Question_body":"<p>How can I update my SageMaker notebook's jupyter environment to the latest alpha release and then restart the process?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-04-08 19:10:35.03 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"amazon-sagemaker|jupyter-lab",
        "Question_view_count":1720,
        "Owner_creation_date":"2013-02-20 05:47:52.693 UTC",
        "Owner_last_access_date":"2022-09-23 20:45:28.4 UTC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Answer_body":"<p>Hi and thank you for using SageMaker!<\/p>\n\n<p>To restart Jupyter from within a SageMaker Notebook Instance, you can issue the following command: <code>sudo initctl restart jupyter-server --no-wait<\/code>.<\/p>\n\n<p>Best,\nKevin<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-04-11 18:28:36.017 UTC",
        "Answer_score":8.0,
        "Owner_location":"NYC",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55580232",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62919671,
        "Question_title":"conda env build fails with \"[Errno 28] No space left on device\"",
        "Question_body":"<p>I'm trying to build a new conda environment in our Sagemaker ec2 environment in a terminal session.  Packages in the original copy of the environment were corrupted, and the environment became unusable. The issue couldn't be fixed by removing packages and re-installing or using <code>conda update<\/code>.<\/p>\n<p>I nuked the environment with <code>conda env remove -n python3-cn<\/code> and then attempted to recreate the environment with:<\/p>\n<pre><code>conda env create -p \/home\/ec2-user\/SageMaker\/anaconda3\/envs\/python3-cn --file=${HOME}\/SageMaker\/efs\/.sagemaker\/python3-cn_environment.yml --force\n<\/code><\/pre>\n<p>This environment has been created a number of times in several ec2 instances for individual Sagemaker users.<\/p>\n<p>Conda logs the following:<\/p>\n<pre><code>Collecting package metadata (repodata.json): done\nSolving environment: done\n\nDownloading and Extracting Packages\npytest-arraydiff-0.2 | 14 KB     | ##################################################################################################### | 100% \npartd-0.3.8          | 32 KB     | ##################################################################################################### | 100% \n\n... several progress bar lines later...\n\npsycopg2-2.7.5       | 507 KB    | ##################################################################################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\nERROR conda.core.link:_execute(700): An error occurred while installing package 'defaults::mkl-2018.0.3-1'.\nRolling back transaction: done\n\n[Errno 28] No space left on device\n()\n<\/code><\/pre>\n<p>The <code>No space left on device<\/code> error is consistent. I've tried<\/p>\n<ul>\n<li><code>conda clean --all<\/code>, removing the environment, re-building the environment<\/li>\n<li>removing the caches, removing the environment, re-building the environment<\/li>\n<li>removing the environment, shutting down and restarting JuypiterLab (our Sagemaker is configured to create <code>python3-cn<\/code> if the environment doesn't exist when JupyterLab starts)<\/li>\n<\/ul>\n<p>In the first two, I get <code>Errno 28<\/code>.<\/p>\n<p>In the last one, the instance is not created, <code>conda env list<\/code> does not show the <code>python3-cn<\/code>, but I see there is a <code>python3-cn<\/code> directory in the <code>anaconda\/envs\/<\/code> directory. If I do <code>conda activate python3-cn<\/code>, I see the prompt change, but the environment is unusuable. If I try <code>conda update --all<\/code>, I get a notification that one of the package files has been corrupted.<\/p>\n<p>Not really sure what to do here. I'm looking for space hogs, but not really finding anything significant.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-15 16:38:26.687 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"conda|amazon-sagemaker",
        "Question_view_count":4104,
        "Owner_creation_date":"2011-07-19 03:38:29.507 UTC",
        "Owner_last_access_date":"2022-09-06 01:14:00.39 UTC",
        "Owner_reputation":56,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":"<p>Try increasing the ebs volume amount of your notebook ... this blog explains it well: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-notebook-volume-size-up-to-16-tb-with-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-notebook-volume-size-up-to-16-tb-with-amazon-sagemaker\/<\/a><\/p>\n<p>Also, best practice is to use lifecycle configuration scripts to build\/add new dependencies ... official docs: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html<\/a><\/p>\n<p>This github page has some great template examples ... for example setting up specific configs like conda, etc: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-16 02:37:49.027 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62919671",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50732094,
        "Question_title":"Sagemaker PySpark: Kernel Dead",
        "Question_body":"<p>I followed the instructions <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/\" rel=\"nofollow noreferrer\">here<\/a> to set up an EMR cluster and a SageMaker notebook. I did not have any errors until the last step.<\/p>\n\n<p>When I open a new Notebook in Sagemaker, I get the message:<\/p>\n\n<pre><code>The kernel appears to have died. It will restart automatically.\n<\/code><\/pre>\n\n<p>And then:<\/p>\n\n<pre><code>        The kernel has died, and the automatic restart has failed.\n        It is possible the kernel cannot be restarted. \n        If you are not able to restart the kernel, you will still be able to save the \nnotebook, but running code will no longer work until the notebook is reopened.\n<\/code><\/pre>\n\n<p>This only happens when I use the pyspark\/Sparkmagic kernel. Notebooks opened with the Conda kernel or any other kernel work fine. <\/p>\n\n<p>My EMR cluster is set up exactly as in the instructions, with an added rule:<\/p>\n\n<pre><code>[\n  {\n    \"Classification\": \"spark\",\n    \"Properties\": {\n      \"maximizeResourceAllocation\": \"true\"\n    }\n  }\n]\n<\/code><\/pre>\n\n<p>I'd appreciate any pointers on why this is happening and how I can debug\/fix.<\/p>\n\n<p>P.S.: I've done this successfully in the past without any issues. When I tried re-doing this today, I ran into this issue. I tried re-creating the EMR clusters and Sagemaker notebooks, but that didn't help. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-06-07 02:17:12.39 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"pyspark|jupyter|amazon-sagemaker",
        "Question_view_count":1768,
        "Owner_creation_date":"2015-04-28 21:07:55.88 UTC",
        "Owner_last_access_date":"2019-05-11 03:01:07.777 UTC",
        "Owner_reputation":125,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":"<p>Thank you for using Amazon SageMaker.<\/p>\n\n<p>The issue here is Pandas 0.23.0 changed the location of a core class named DataError and SparkMagic has not been updated to require DataError from correct namespace.<\/p>\n\n<p>The workaround for this issue is to downgrade Pandas version in SageMaker Notebook Instance with <code>pip install pandas==0.22.0<\/code>.<\/p>\n\n<p>You can get more information in this open github issue <a href=\"https:\/\/github.com\/jupyter-incubator\/sparkmagic\/issues\/458\" rel=\"noreferrer\">https:\/\/github.com\/jupyter-incubator\/sparkmagic\/issues\/458<\/a>.<\/p>\n\n<p>Let us know if there is any other way we can be of assistance.<\/p>\n\n<p>Thanks,<br>\nNeelam<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-07-10 20:34:02.95 UTC",
        "Answer_score":5.0,
        "Owner_location":"Pittsburgh, PA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50732094",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54461730,
        "Question_title":"How to create a permanent login from jupyter notebook to github with ssh_rsa key pair",
        "Question_body":"<p>So my question is this, <\/p>\n\n<p>When creating a notebook in <code>Sagemaker<\/code> <code>AWS<\/code> I need to help the devEngineer keep his secret key in <code>.ssh\/id_rsa<\/code> as the file after every instance reboot becomes empty. \nHe requires a <code>github<\/code> repo to be downloaded and he has to work on the code and then push the updates as needed. \nPlease let me know what details I need to provide to help you help me. \nThanks. <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-31 13:30:58.327 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-01-31 13:38:52.587 UTC",
        "Question_score":2,
        "Question_tags":"github|jupyter-notebook|ssh-keys|amazon-sagemaker",
        "Question_view_count":1856,
        "Owner_creation_date":"2018-11-06 06:13:32.983 UTC",
        "Owner_last_access_date":"2021-12-08 23:53:21.34 UTC",
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>This is the filesystems for my notebook instance:<\/p>\n<pre><code>sh-4.2$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\ndevtmpfs         16G   76K   16G   1% \/dev\ntmpfs            16G     0   16G   0% \/dev\/shm\n\/dev\/nvme0n1p1   94G   76G   19G  81% \/\n\/dev\/nvme1n1     99G   40G   55G  43% \/home\/ec2-user\/SageMaker\n<\/code><\/pre>\n<p>Note that one pointing to <code>\/home\/ec2-user\/SageMaker<\/code> is the only one which is saved between reboots. Since ssh keys are stored in <code>\/home\/ec2-user\/.ssh<\/code>, they are lost after reboot.<\/p>\n<p>The way I make it work is:<\/p>\n<ol>\n<li>Create the folder <code>\/home\/ec2-user\/SageMaker\/.ssh<\/code><\/li>\n<li>Run <code>ssh-keygen<\/code> and set the location <code>\/home\/ec2-user\/SageMaker\/.ssh\/id_rsa<\/code><\/li>\n<li>Clone repo with <code>GIT_SSH_COMMAND=&quot;ssh -i ~\/SageMaker\/.ssh\/id_rsa -F \/dev\/null&quot; git clone git@domain:account\/repo.git<\/code><\/li>\n<li>cd repo<\/li>\n<li>Set your repo to use the custom location with <code>git config core.sshCommand &quot;ssh -i ~\/SageMaker\/.ssh\/id_rsa -F \/dev\/null&quot;<\/code><\/li>\n<\/ol>\n<p>Based on <a href=\"https:\/\/superuser.com\/a\/912281\">https:\/\/superuser.com\/a\/912281<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-02-02 22:32:47.163 UTC",
        "Answer_score":3.0,
        "Owner_location":"London, UK",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54461730",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64537150,
        "Question_title":"AWS Sagemaker Multiple Training Jobs",
        "Question_body":"<p>We currently have a system running on AWS Sagemaker whereby several units have their own trained machine learning model artifact (using an SKLearn training script with the Sagemaker SKLearn estimator).<\/p>\n<p>Through the use of Sagemaker's multi-model endpoints, we are able to host all of these units on a single instance.<\/p>\n<p>The problem we have is that we need to scale this system up such that we can train individual models for hundreds of thousand of units and then host the resulting model artifacts on a multi-model endpoint. But, Sagemaker has a limit to the number of models you can train in parallel (our limit is 30).<\/p>\n<p>Aside from training our models in batches, does anyone have any ideas how to go about implementing a system in AWS Sagemaker whereby for hundreds of thousands of units, we can have a separate trained model artifact for each unit?<\/p>\n<p>Is there a way to output multiple model artifacts for 1 sagemaker training job with the use of an SKLearn estimator?<\/p>\n<p>Furthermore, how does Sagemaker make use of multiple CPUs when a training script is submitted? Does this have to be specified in the training script\/estimator object or is this handled automatically?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-26 12:30:39.387 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-10-26 12:48:18.123 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|machine-learning|scikit-learn|amazon-sagemaker",
        "Question_view_count":1053,
        "Owner_creation_date":"2020-06-16 12:48:47.163 UTC",
        "Owner_last_access_date":"2022-03-19 12:18:47.233 UTC",
        "Owner_reputation":153,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":"<p>Here are some ideas:<\/p>\n<p><em><strong>1. does anyone have any ideas how to go about implementing a system in AWS Sagemaker whereby for hundreds of thousands of units, we can have a separate trained model artifact for each unit? Is there a way to output multiple model artifacts for 1 sagemaker training job with the use of an SKLearn estimator?<\/strong><\/em><\/p>\n<p>I don't know if the 30-training job concurrency is a hard limit, if it is a blocker you should try and open a support ticket to ask if it is and try and get it raised. Otherwise as you can point out, you can try and train multiple models in one job, and produce multiple artifacts that you can either (a) send to S3 manually, or (b) save to <code>opt\/ml\/model<\/code> so that they all get sent to the model.tar.gz artifact in S3. Note that if this artifact gets too big this could get impractical though<\/p>\n<p><em><strong>2. how does Sagemaker make use of multiple CPUs when a training script is submitted? Does this have to be specified in the training script\/estimator object or is this handled automatically?<\/strong><\/em><\/p>\n<p>This depends on the type of training container you are using. SageMaker built-in containers are developed by Amazon teams and designed to efficiently use available resources. If you use your own code such as custom python in the Sklearn container, you are responsible for making sure that your code is efficiently written and uses available hardware. Hence framework choice is quite important :) for example, some sklearn models support explicitly using multiple CPUs (eg the <code>n_jobs<\/code> parameter in the <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\" rel=\"nofollow noreferrer\">random forest<\/a>), but I don't think that Sklearn natively supports GPU, multi-GPU or multi-node training.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2020-10-27 09:16:19.687 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64537150",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69678393,
        "Question_title":"Creating docker file which installs python with sklearn and pandas that can be used on sagemaker",
        "Question_body":"<p>I am quite new to docker. My problem in short is to create a docker file that contains python with sklearn and pandas which can be used on aws sagemaker.<\/p>\n<p>My current docker file looks like the following:<\/p>\n<pre><code>FROM jupyter\/scipy-notebook\n\nRUN pip3 install sagemaker-training\n\nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>However when i try to create this image I get an error at line <code>pip3 install sagemaker-training<\/code>. The error is the following:<\/p>\n<pre><code>error: command 'gcc' failed: No such file or directory\n\nERROR: Command errored out with exit status 1: \/opt\/conda\/bin\/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-fj0cb373\/sagemaker-training_66ca9935ed134c95ac11a32e118e4568\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-fj0cb373\/sagemaker-training_66ca9935ed134c95ac11a32e118e4568\/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-o5rzjscd\/install-record.txt --single-version-externally-managed --compile --install-headers \/opt\/conda\/include\/python3.9\/sagemaker-training Check the logs for full command output.\n\nThe command '\/bin\/bash -o pipefail -c pip3 install sagemaker-training' returned a non-zero code: 1\n<\/code><\/pre>\n<p>If there is a more suitable base image can someone point that out to me? I am generally trying to follow this page <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit<\/a>.<\/p>\n<p>Note: I realise I can use some sagemaker pre-built containers without using my own docker file. However I am trying to do this for my own learning so I know what to do for projects that can't utilise them.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2021-10-22 14:13:28.467 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-10-22 14:39:02.57 UTC",
        "Question_score":0,
        "Question_tags":"docker|amazon-sagemaker",
        "Question_view_count":161,
        "Owner_creation_date":"2018-03-22 16:45:24.657 UTC",
        "Owner_last_access_date":"2022-09-23 14:29:34.747 UTC",
        "Owner_reputation":738,
        "Owner_up_votes":22,
        "Owner_down_votes":7,
        "Owner_views":69,
        "Answer_body":"<p>I adjusted your Dockerfile and it builds successfully for me.<\/p>\n<pre><code>FROM jupyter\/scipy-notebook\nARG defaultuser=jovyan\nUSER root\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update &amp;&amp; \\\n    apt-get -y install gcc mono-mcs &amp;&amp; \\\n    rm -rf \/var\/lib\/apt\/lists\/*\nUSER $defaultuser\n\nRUN pip3 install sagemaker-training\n\nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>(I had to adjust for the fact that the default user from the base container isn't root, when installing GCC)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-10-28 16:38:12.897 UTC",
        "Answer_score":0.0,
        "Owner_location":"Milton Keynes",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69678393",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60816944,
        "Question_title":"AWS Sagemaker Notebook with multiple users",
        "Question_body":"<p>I am still new in AWS sagemaker. Working on a architecture where we would have an AWS sagemaker notebook. There would be multiple users, I want that students don`t see each other work. would I need to do that in terminal? or we can do that in notebook itself?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-23 15:50:35.243 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":1907,
        "Owner_creation_date":"2017-12-21 19:07:16.66 UTC",
        "Owner_last_access_date":"2022-09-23 04:14:35.81 UTC",
        "Owner_reputation":465,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":46,
        "Answer_body":"<p>The simplest way is to create a small notebook instance for each student. This way you can have the needed isolation and also the responsibility of each student for their notebook to stop them when they are not in use.<\/p>\n\n<p>The smallest instance type <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">costs<\/a> $0.0464 per hour. If you have it running 24\/7 it costs about $30 per month. But if the students are responsible and stop their instances when they are not using them, it can be about $1 for 20 hours of work.<\/p>\n\n<p>If you want to enable isolation to the notebooks, you can use the ability to presign the URL that is used to open the Jupyter interface. See here on the way to use the CLI to create the URL: <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-presigned-notebook-instance-url.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-presigned-notebook-instance-url.html<\/a>. It is also supported in other SDK.<\/p>\n\n<pre><code>create-presigned-notebook-instance-url\n--notebook-instance-name &lt;student-instance-name&gt;\n--session-expiration-duration-in-seconds 3600\n<\/code><\/pre>\n\n<p>You can integrate it into the internal portal that you have in your institute. <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-03-24 09:11:16.743 UTC",
        "Answer_score":3.0,
        "Owner_location":"Noida, Uttar Pradesh, India",
        "Answer_last_edit_date":"2020-03-24 19:21:35.677 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60816944",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56500704,
        "Question_title":"SageMaker create PyTorchModel without deploying",
        "Question_body":"<p>If I instantiate a SageMaker <code>PyTorchModel<\/code> object like this:<\/p>\n\n<pre><code>from sagemaker.pytorch import PyTorchModel\n\nmodel = PyTorchModel(name=name_from_base('model-name'),\n                     model_data=model_data,\n                     role=role,\n                     framework_version='1.0.0',\n                     entry_point='serve.py',\n                     source_dir='src',\n                     sagemaker_session=sagemaker_session,\n                     predictor_cls=ImagePredictor)\n\n#model.create_without_deploying??\n<\/code><\/pre>\n\n<p>Is there a way that I can create this model using the sagemaker python SDK so that the model shows up in the SageMaker console, but <em>without<\/em> actually deploying it to an endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-06-07 20:12:23.56 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2019-06-07 20:21:50.38 UTC",
        "Question_score":2,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":434,
        "Owner_creation_date":"2013-02-20 05:47:52.693 UTC",
        "Owner_last_access_date":"2022-09-23 20:45:28.4 UTC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Answer_body":"<p>I don't think it is possible to do so using the high-level SageMaker Pyhton SDK. However, you should be able to do it by calling the CreateModel API using the low-level boto3 <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model<\/a>. For your reference, below is an example snippet code on how to do it.<\/p>\n\n<pre><code>%%time\nimport boto3\nimport time\n\nsage = boto3.Session().client(service_name='sagemaker')\n\nimage_uri = '520713654638.dkr.ecr.us-east-1.amazonaws.com\/sagemaker-pytorch:1.0.0-cpu-py3'\nmodel_data ='s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\/output\/model.tar.gz'\nsource = 's3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\/sourcedir.tar.gz'\nrole = 'arn:aws:iam::xxxxxxxx:role\/service-role\/AmazonSageMaker-ExecutionRole-xxxxxx'\n\ntimestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\nmodel_name = 'my-pytorch-model' + timestamp\n\nresponse = sage.create_model(\n    ModelName=model_name,\n    PrimaryContainer={\n        'Image': image_uri,\n        'ModelDataUrl': model_data,\n        'Environment': { 'SAGEMAKER_CONTAINER_LOG_LEVEL':'20', 'SAGEMAKER_ENABLE_CLOUDWATCH_METRICS': 'False', \n                   'SAGEMAKER_PROGRAM': 'generate.py','SAGEMAKER_REGION': 'us-east-1','SAGEMAKER_SUBMIT_DIRECTORY': source}\n         },\n         ExecutionRoleArn=role\n}\nprint(response)\n<\/code><\/pre>\n\n<p>If you get no error message, then the model will shows up in the SageMaker console<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-06-27 09:13:57.347 UTC",
        "Answer_score":1.0,
        "Owner_location":"NYC",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56500704",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50352412,
        "Question_title":"How to prevent a NoCredentialsError when calling the fit method in SageMaker?",
        "Question_body":"<p>I am a newbie when it comes to Python SageMaker (my background is C#). Currently, I have a problem because the last method call (I mean the fit method) results in a \"NoCredentialsError\". I do not understand that. The AWS credentials have been set and I do use them to communicate with AWS, for example to communicate with S3. How can I prevent this error? <\/p>\n\n<pre><code>import io\nimport os\nimport gzip\nimport pickle\nimport urllib.request\nimport boto3\nimport sagemaker\nimport sagemaker.amazon.common as smac\n\nDOWNLOADED_FILENAME = 'C:\/Users\/Daan\/PycharmProjects\/downloads\/mnist.pkl.gz'\nif not os.path.exists(DOWNLOADED_FILENAME):\n    urllib.request.urlretrieve(\"http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl.gz\", DOWNLOADED_FILENAME)\n\nwith gzip.open(DOWNLOADED_FILENAME, 'rb') as f:\n    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\nvectors = train_set[0].T\nbuf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, vectors)\nbuf.seek(0)\nkey = 'recordio-pb-data'\nbucket_name = 'SOMEKINDOFBUCKETNAME'\nprefix = 'sagemaker\/pca'\npath = os.path.join(prefix, 'train', key)\nprint(path)\n\nsession = boto3.session.Session(aws_access_key_id='SECRET',aws_secret_access_key='SECRET',region_name='eu-west-1')\nclient = boto3.client('sagemaker',region_name='eu-west-1',aws_access_key_id='SECRET',aws_secret_access_key='SECRET')\nregion='eu-west-1'\nsagemakerSession= sagemaker.Session(sagemaker_client=client,boto_session=session)\ns3_resource=session.resource('s3')\nbucket = s3_resource.Bucket(bucket_name)\ncurrent_bucket = bucket.Object(path)\n\ntrain_data = 's3:\/\/{}\/{}\/train\/{}'.format(bucket_name, prefix, key)\nprint('uploading training data location: {}'.format(train_data))\ncurrent_bucket.upload_fileobj(buf)\n\noutput_location = 's3:\/\/{}\/{}\/output'.format('SOMEBUCKETNAME', prefix)\nprint('training artifacts will be uploaded to: {}'.format(output_location))\n\nregion='eu-west-1'\n\ncontainers = {'us-west-2': 'SOMELOCATION',\n              'us-east-1': 'SOMELOCATION',\n              'us-east-2': 'SOMELOCATION',\n              'eu-west-1': 'SOMELOCATION'}\ncontainer = containers[region]\n\nrole='AmazonSageMaker-ExecutionRole-SOMEVALUE'\npca = sagemaker.estimator.Estimator(container,\n                                    role,\n                                    train_instance_count=1,\n                                    train_instance_type='ml.c4.xlarge',\n                                    output_path=output_location,\n                                    sagemaker_session=sagemakerSession)\n\n\npca.set_hyperparameters(feature_dim=50000,\n                        num_components=10,\n                        subtract_mean=True,\n                        algorithm_mode='randomized',\n                        mini_batch_size=200)\n\npca.fit(inputs=train_data)\n\nprint('END')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-05-15 14:10:14.957 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":447,
        "Owner_creation_date":"2013-01-17 13:27:30.663 UTC",
        "Owner_last_access_date":"2022-09-23 16:24:02.467 UTC",
        "Owner_reputation":2120,
        "Owner_up_votes":100,
        "Owner_down_votes":3,
        "Owner_views":279,
        "Answer_body":"<p>I am not sure if you have masked the actual access id and key or this is what you are running.<\/p>\n<pre><code>session = boto3.session.Session(aws_access_key_id='SECRET',aws_secret_access_key='SECRET',region_name='eu-west-1')\nclient = boto3.client('sagemaker',region_name='eu-west-1',aws_access_key_id='SECRET',aws_secret_access_key='SECRET')\n<\/code><\/pre>\n<p>I am hoping you are providing the actual aws_access_key_id and aws_secret_access_key in the above lines of code.<\/p>\n<p>Another way of specifying the same and not hardcoding in the code is to create a credentials file in your profile directory i.e.<\/p>\n<p>in Mac    ~\/.aws\/<\/p>\n<p>and in Windows <code>&quot;%UserProfile%\\.aws&quot;<\/code><\/p>\n<p>the file is a plain text file with a name &quot;credentials&quot; (without the quotes).\nfile contains<\/p>\n<pre><code>[default]\naws_access_key_id=XXXXXXXXXXXXXX\naws_secret_access_key=YYYYYYYYYYYYYYYYYYYYYYYYYYY\n<\/code><\/pre>\n<p>AWS CLI would pick it up from the above location and use it. You can also use non-default profiles and pass on the profile with<\/p>\n<pre><code>os.environ[&quot;AWS_PROFILE&quot;] = &quot;profile-name&quot;\n<\/code><\/pre>\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-05-15 17:31:40.043 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2021-11-15 10:05:07.96 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50352412",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54295445,
        "Question_title":"TensorFlow Serving send data as b64 instead of Numpy Array",
        "Question_body":"<p>I have a TensorFlow Serving container in a SageMaker endpoint. I'm able to take a batch of images as a Numpy array and get back predictions like this:<\/p>\n\n<pre><code>import numpy as np\nimport sagemaker\nfrom sagemaker.predictor import json_serializer, json_deserializer\n\nimage = np.random.uniform(low=-1.0, high=1.0, size=(1,128,128,3)).astype(np.float32)    \nimage = {'instances': image}\nimage = json_serializer(image)\n\nrequest_args = {}\nrequest_args['Body'] = image\nrequest_args['EndpointName'] = endpoint_name\nrequest_args['ContentType'] = 'application\/json'\nrequest_args['Accept'] = 'application\/json'\n\n# works successfully\nresponse = sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\nresponse_body = response['Body']\npredictions = json_deserializer(response_body, response['ContentType'])\n<\/code><\/pre>\n\n<p>The size of the <code>request_args<\/code> payload is large doing it this way. I'm wondering, is there a way to send this in a more compressed format? <\/p>\n\n<p>I've tried experimenting with <code>base64<\/code> and <code>json.dumps<\/code>, but can't get past <code>Invalid argument: JSON Value: ...<\/code> errors. Not sure if this isn't supported or if I'm just doing it incorrectly.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-21 17:56:05.813 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|image|base64|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":796,
        "Owner_creation_date":"2013-02-20 05:47:52.693 UTC",
        "Owner_last_access_date":"2022-09-23 20:45:28.4 UTC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Answer_body":"<p>I've talked to AWS support about this (see <a href=\"https:\/\/stackoverflow.com\/questions\/54090270\/more-efficient-way-to-send-a-request-than-json-to-deployed-tensorflow-model-in-s\">More efficient way to send a request than JSON to deployed tensorflow model in Sagemaker?<\/a>).<\/p>\n\n<p>They suggest that it is possible to pass in a custom input_fn that will be used by the serving container where one can unpack a compressed format (such as protobuf).<\/p>\n\n<p>I'll be testing this soon and hopefully this stuff works since it would add a lot of flexibility to the input processing.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2019-01-23 13:48:10.417 UTC",
        "Answer_score":2.0,
        "Owner_location":"NYC",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54295445",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63960011,
        "Question_title":"Using AWS Sagemaker for model performance without creating endpoint",
        "Question_body":"<p>I've been using Amazon Sagemaker Notebooks to build a pytorch model for an NLP task.\nI know you can use Sagemaker to train, deploy, hyper parameter tuning, and model monitoring.<\/p>\n<p>However, it looks like you have to create an inference endpoint in order to monitor the model's inference performance.<\/p>\n<p>I already have a EC2 instance setup to perform inference tasks on our model, which is currently on a development box and rather not use an endpoint to make<\/p>\n<p>Is it possible to use Sagemaker to train, run hyperparam tuning and model eval without creating an endpoint.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-18 17:06:23.673 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"pytorch|amazon-sagemaker",
        "Question_view_count":494,
        "Owner_creation_date":"2010-09-25 01:47:51.58 UTC",
        "Owner_last_access_date":"2022-04-04 12:09:39.223 UTC",
        "Owner_reputation":625,
        "Owner_up_votes":174,
        "Owner_down_votes":11,
        "Owner_views":110,
        "Answer_body":"<p>If you don't want to keep an inference endpoint up, one option is to use SageMaker Processing to run a job that takes your trained model and test dataset as input, performs inference and computes evaluation metrics, and saves them to S3 in a JSON file.<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">This Jupyter notebook example<\/a> steps through (1) preprocessing training and test data, (2) training a model, then (3) evaluating the model<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-10-29 23:01:49.39 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63960011",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54462105,
        "Question_title":"SageMaker Ground Truth with TensorFlow",
        "Question_body":"<p>I've seen examples of labeling data using SageMaker Ground Truth and then using that data to train off-the-shelf SageMaker models. However, am I able to use this same annotation format with TensorFlow Script Mode? <\/p>\n\n<p>More specifically, I have a tensorflow.keras model I'm training using TF Script Mode, and I'd like to take data labeled with Ground Truth and convert my script from File mode to Pipe mode.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-31 13:53:04.467 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|tensorflow|amazon-sagemaker|labeling",
        "Question_view_count":454,
        "Owner_creation_date":"2013-02-20 05:47:52.693 UTC",
        "Owner_last_access_date":"2022-09-23 20:45:28.4 UTC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Answer_body":"<p>I am from Amazon SageMaker Ground Truth team and happy to assist you in your experiment. Just to be clear our understanding, are you running TF model in SageMaker using TF estimator in your own container (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst<\/a>)? <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-02-11 20:12:56.767 UTC",
        "Answer_score":3.0,
        "Owner_location":"NYC",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54462105",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65587939,
        "Question_title":"Can I use pre-labeled data in AWS SageMaker Ground Truth NER?",
        "Question_body":"<p>Let's say I have some text data that has already been labeled in SageMaker. This data could have either been labeled by humans or an ner model. Then let's say I want to have a human go back over the dataset, either to label new entity class or correct existing labels. How would I set up a labeling job to allow this? I tried using an output manifest from another labeling job, but all of the documents that were already labeled cannot be accessed by workers to re-label.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-05 23:16:48.767 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|amazon-sagemaker|named-entity-recognition|labeling",
        "Question_view_count":367,
        "Owner_creation_date":"2020-05-08 15:45:28.737 UTC",
        "Owner_last_access_date":"2022-09-24 10:08:13.13 UTC",
        "Owner_reputation":35,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>Yes, this is possible you are looking for <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates.html\" rel=\"nofollow noreferrer\">Custom Labelling worklflows<\/a> you can also apply either Majority Voting (MV) or MDS to evaluate the accuracy of the job<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-01-05 23:51:20.277 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65587939",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51533650,
        "Question_title":"No space left on device in Sagemaker model training",
        "Question_body":"<p>I'm using custom algorithm running shipped with Docker image on p2 instance with AWS Sagemaker (a bit similar to <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a>)<\/p>\n\n<p>At the end of training process, I try to write down my model to output directory, that is mounted via Sagemaker (like in tutorial), like this:<\/p>\n\n<pre><code>model_path = \"\/opt\/ml\/model\"\nmodel.save(os.path.join(model_path, 'model.h5'))\n<\/code><\/pre>\n\n<p>Unluckily, apparently the model gets too big with time and I get the\nfollowing error:<\/p>\n\n<blockquote>\n  <p>RuntimeError: Problems closing file (file write failed: time = Thu Jul\n  26 00:24:48 2018<\/p>\n  \n  <p>00:24:49 , filename = 'model.h5', file descriptor = 22, errno = 28,\n  error message = 'No space left on device', buf = 0x1a41d7d0, total\n  write[...]<\/p>\n<\/blockquote>\n\n<p>So all my hours of GPU time are wasted. How can I prevent this from happening again? Does anyone know what is the size limit for model that I store on Sagemaker\/mounted directories?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2018-07-26 07:51:03.817 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|keras|amazon-sagemaker",
        "Question_view_count":2721,
        "Owner_creation_date":"2013-04-19 21:52:19.74 UTC",
        "Owner_last_access_date":"2022-09-21 08:47:03.06 UTC",
        "Owner_reputation":8057,
        "Owner_up_votes":1123,
        "Owner_down_votes":72,
        "Owner_views":491,
        "Answer_body":"<p>When you train a model with <code>Estimators<\/code>, it <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/estimators.html\" rel=\"nofollow noreferrer\">defaults to 30 GB of storage<\/a>, which may not be enough. You can use the <code>train_volume_size<\/code> param on the constructor to increase this value. Try with a large-ish number (like 100GB) and see how big your model is. In subsequent jobs, you can tune down the value to something closer to what you actually need.<\/p>\n\n<p>Storage costs <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">$0.14 per GB-month of provisioned storage<\/a>. Partial usage is prorated, so giving yourself some extra room is a cheap insurance policy against running out of storage.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-10-15 19:27:34.853 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51533650",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55868121,
        "Question_title":"Random cut forest anomaly detection on multi variant time series data",
        "Question_body":"<p>I have sensor data coming from equipment with times series along with many attributes,<\/p>\n\n<p>I have used RCF algorithm to detect anomalies.\nNow the challenge is,how to to convince the end user whether it is really anomaly or not.\nJust want to know which attribute is contributing to anomaly.<\/p>\n\n<p>Is there any best way to convince end user whether it is really anomaly or not.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-04-26 12:55:45.33 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-04-27 11:21:02.917 UTC",
        "Question_score":0,
        "Question_tags":"machine-learning|amazon-sagemaker|anomaly-detection",
        "Question_view_count":984,
        "Owner_creation_date":"2019-04-10 00:37:07.173 UTC",
        "Owner_last_access_date":"2021-02-05 10:45:12.313 UTC",
        "Owner_reputation":125,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Answer_body":"<p>The simplest way to run the RCF model and to get the explanation for the anomaly is to use the version of RCF in Kinesis Analytics (KA). Here is a link to the documentation of how to run from the KA documentations: <a href=\"https:\/\/docs.aws.amazon.com\/kinesisanalytics\/latest\/sqlref\/sqlrf-random-cut-forest-with-explanation.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/kinesisanalytics\/latest\/sqlref\/sqlrf-random-cut-forest-with-explanation.html<\/a><\/p>\n\n<p>Kinesis is taking care both for the training of the model, the inference after the initial training and for the attribution and explanation of the variables. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/p25FR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/p25FR.png\" alt=\"https:\/\/docs.aws.amazon.com\/kinesisanalytics\/latest\/sqlref\/images\/anomaly_results.png\"><\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2019-04-27 11:19:46.93 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55868121",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":47847736,
        "Question_title":"AWS Sagemaker Neural Topic Model",
        "Question_body":"<p>What is underlying algorithm for Sagemaker's <a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ntm.html\" rel=\"nofollow noreferrer\">Neural Topic Model<\/a>? I have hard time googling for details, and the documentation doesn't mention any paper.<\/p>\n\n<p>Googling for 'neural topic model' doesn't exactly answer my question, since a couple of methods seems to be called that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2017-12-16 16:29:08.563 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|neural-network|amazon-sagemaker",
        "Question_view_count":222,
        "Owner_creation_date":"2013-07-23 09:40:39.353 UTC",
        "Owner_last_access_date":"2022-09-21 17:49:28.25 UTC",
        "Owner_reputation":2177,
        "Owner_up_votes":523,
        "Owner_down_votes":113,
        "Owner_views":262,
        "Answer_body":"<p>Seems like AWS SageMaker team answered the question, \n<a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=270493&amp;tstart=0\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=270493&amp;tstart=0<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-01-06 17:35:39.877 UTC",
        "Answer_score":1.0,
        "Owner_location":"Poland",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47847736",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56469341,
        "Question_title":"Adding a {serve} metagraph to existing Tensorflow model",
        "Question_body":"<p>The situation:<\/p>\n\n<p>I've already created several models, trained over several days each, that we're ready to move from local testing to a serving environment.<\/p>\n\n<p>The models were saved using the function<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def save_graph_to_file(sess, graph, graph_file_name):\n    \"\"\"Saves an graph to file, creating a valid quantized one if necessary.\"\"\"\n    output_graph_def = graph_util.convert_variables_to_constants(sess, graph.as_graph_def(), [final_tensor_name])\n    with gfile.FastGFile(graph_file_name, 'wb') as f:\n        f.write(output_graph_def.SerializeToString())\n<\/code><\/pre>\n\n<p>Now when attempting to deploy to a serving environment (Sagemaker, using a correct directory structure and file naming convention), the system returns<\/p>\n\n<pre><code>2019-06-04 22:38:53.794056: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\n2019-06-04 22:38:53.798096: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:259] SavedModel load for tags { serve }; Status: fail. Took 83297 microseconds.\n2019-06-04 22:38:53.798132: E tensorflow_serving\/util\/retrier.cc:37] Loading servable: {name: model version: 1} failed: Not found: Could not find meta graph def matching supplied tags: { serve }. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`\n<\/code><\/pre>\n\n<p>All I have are the <code>*.pb<\/code> files and their label textfiles. These work lovely across multiple computers in local environments. <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def load_graph(model_file):\n    \"\"\"\n    Code from v1.6.0 of Tensorflow's label_image.py example\n    \"\"\"\n    graph = tf.Graph()\n    graph_def = tf.GraphDef()\n    with open(model_file, \"rb\") as f:\n        graph_def.ParseFromString(f.read())\n    with graph.as_default():\n        tf.import_graph_def(graph_def)\n    return graph\n\ninputLayer = \"Mul\"\noutputLayer = \"final_result\"\ninputName = \"import\/\" + inputLayer\noutputName = \"import\/\" + outputLayer\ngraph = load_graph(modelPath)\ninputOperation = graph.get_operation_by_name(inputName)\noutputOperation = graph.get_operation_by_name(outputName)\nwith tf.Session(graph= graph) as sess:\n    # ... make a tensor t\n    results = sess.run(outputOperation.outputs[0], {\n        inputOperation.outputs[0]: t\n    })\n    # lovely functional results here\n<\/code><\/pre>\n\n<p>All I want to do is to take these existing files, add the \"serve\" tag needed, and re-save them, but everything I see seems to be related to doing this from scratch.<\/p>\n\n<p>I tried to use the builder to append a graph to a model like so:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Load the graph\ngraph = load_graph(modelPath)\nimport shutil\nif os.path.exists(exportDir):\n    shutil.rmtree(exportDir)\n# Add the serving metagraph tag\nbuilder = tf.saved_model.builder.SavedModelBuilder(exportDir)\nfrom tensorflow.saved_model import tag_constants\nwith tf.Session(graph= graph) as sess:\n    builder.add_meta_graph_and_variables(sess, [tag_constants.SERVING, tag_constants.GPU], strip_default_attrs= True)\nbuilder.save()\nprint(\"Built a SavedModel\")\n<\/code><\/pre>\n\n<p>but got the same error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-06-05 23:40:36.413 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|tensorflow|amazon-sagemaker",
        "Question_view_count":1140,
        "Owner_creation_date":"2012-12-05 00:57:57.99 UTC",
        "Owner_last_access_date":"2022-09-23 20:19:36.4 UTC",
        "Owner_reputation":552,
        "Owner_up_votes":78,
        "Owner_down_votes":1,
        "Owner_views":68,
        "Answer_body":"<p>Finally solved it. This contains some S3 specific code and S3 instance calls (the <code>!<\/code> commands) but you should pretty much be able to slice that out to run this.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#!python3\n\"\"\"\nAssumes we've defined:\n\n- A directory for our working files to live in, CONTAINER_DIR\n- an arbitrary integer VERSION_INT\n- We have established local and S3 paths for our model and their labels as variables, particularly `modelLabel` and `modelPath`\n\"\"\"\n\n# Create a versioned path for the models to live in\n# See https:\/\/stackoverflow.com\/a\/54014480\/1877527\nexportDir = os.path.join(CONTAINER_DIR, VERSION_INT)\nif os.path.exists(exportDir):\n    shutil.rmtree(exportDir)\nos.mkdir(exportDir)\nimport tensorflow as tf\ndef load_graph(model_file, returnElements= None):\n    \"\"\"\n    Code from v1.6.0 of Tensorflow's label_image.py example\n    \"\"\"\n    graph = tf.Graph()\n    graph_def = tf.GraphDef()\n    with open(model_file, \"rb\") as f:\n        graph_def.ParseFromString(f.read())\n    returns = None\n    with graph.as_default():\n        returns = tf.import_graph_def(graph_def, return_elements= returnElements)\n    if returnElements is None:\n        return graph\n    return graph, returns\n\n# Add the serving metagraph tag\n# We need the inputLayerName; in Inception we're feeding the resized tensor\n# corresponding to resized_input_tensor_name\n# May be able to get away with auto-determining this if not using Inception,\n# but for Inception this is the 11th layer\ninputLayerName = \"Mul:0\"\n# Load the graph\nif inputLayerName is None:\n    graph = load_graph(modelPath)\n    inputTensor = None\nelse:\n    graph, returns = load_graph(modelPath, returnElements= [inputLayerName])\n    inputTensor = returns[0]\nwith tf.Session(graph= graph) as sess:\n    # Read the layers\n    try:\n        from tensorflow.compat.v1.saved_model import simple_save\n    except (ModuleNotFoundError, ImportError):\n        from tensorflow.saved_model import simple_save\n    with graph.as_default():\n        layers = [n.name for n in graph.as_graph_def().node]\n        outName = layers.pop() + \":0\"\n        if inputLayerName is None:\n            inputLayerName = layers.pop(0) + \":0\"\n    print(\"Checking outlayer\", outName)\n    outLayer = tf.get_default_graph().get_tensor_by_name(outName)\n    if inputTensor is None:\n        print(\"Checking inlayer\", inputLayerName)\n        inputTensor = tf.get_default_graph().get_tensor_by_name(inputLayerName)\n    inputs = {\n        inputLayerName: inputTensor\n    }\n    outputs = {\n        outName: outLayer\n    }\n    simple_save(sess, exportDir, inputs, outputs)\nprint(\"Built a SavedModel\")\n# Put the model label into the artifact dir\nmodelLabelDest = os.path.join(exportDir, \"saved_model.txt\")\n!cp {modelLabel} {modelLabelDest}\n# Prep for serving\nimport datetime as dt\nmodelArtifact = f\"livemodel_{dt.datetime.now().timestamp()}.tar.gz\"\n# Copy the version directory here to package\n!cp -R {exportDir} .\/\n# gziptar it\n!tar -czvf {modelArtifact} {VERSION_INT}\n# Shove it back to S3 for serving\n!aws s3 cp {modelArtifact} {bucketPath}\nshutil.rmtree(VERSION_INT) # Cleanup\nshutil.rmtree(exportDir) # Cleanup\n<\/code><\/pre>\n\n<p>This model is then deployable as a Sagemaker endpoint (and any other Tensorflow serving environment)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-06-14 17:47:36.823 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2019-10-23 21:59:21.113 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56469341",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53595157,
        "Question_title":"AWS Sagemaker Deploy fails",
        "Question_body":"<p>I am following the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-deploy-model.html\" rel=\"noreferrer\">sage maker documentation<\/a> to train and deploy an ML model. I am using the high-level Python library provided by Amazon SageMaker to achieve this. <\/p>\n\n<pre><code>kmeans_predictor = kmeans.deploy(initial_instance_count=1,\n                                 instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>The deployment fails with error<\/p>\n\n<p>ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.c4.8xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. <\/p>\n\n<p>Where am I going wrong?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-12-03 13:45:26.537 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":7,
        "Question_tags":"amazon-web-services|aws-sdk|amazon-sagemaker",
        "Question_view_count":5932,
        "Owner_creation_date":"2013-07-05 10:08:00.577 UTC",
        "Owner_last_access_date":"2021-02-21 06:05:20.94 UTC",
        "Owner_reputation":305,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":"<p>I resolved the issue by changing the instance type:<\/p>\n\n<pre><code>kmeans_predictor = kmeans.deploy(initial_instance_count=1,\n                                 instance_type='ml.t2.medium')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-12-04 06:51:45.89 UTC",
        "Answer_score":7.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53595157",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63441299,
        "Question_title":"Pytorch CUDA OutOfMemory Error while training",
        "Question_body":"<p>I'm trying to train a PyTorch FLAIR model in AWS Sagemaker.\nWhile doing so getting the following error:<\/p>\n<pre><code>RuntimeError: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 11.17 GiB total capacity; 9.29 GiB already allocated; 7.31 MiB free; 10.80 GiB reserved in total by PyTorch)\n<\/code><\/pre>\n<p>For training I used sagemaker.pytorch.estimator.PyTorch class.<\/p>\n<p>I tried with different variants of instance types from ml.m5, g4dn to p3(even with a 96GB memory one).\nIn the ml.m5 getting the error with CPUmemoryIssue, in g4dn with GPUMemoryIssue and in the P3 getting GPUMemoryIssue mostly because Pytorch is using only one of the GPU of 12GB out of 8*12GB.<\/p>\n<p>Not getting anywhere to complete this training, even in local tried with a CPU machine and got the following error:<\/p>\n<pre><code>RuntimeError: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 67108864 bytes. Buy new RAM!\n<\/code><\/pre>\n<p>The model training script:<\/p>\n<pre><code>    corpus = ClassificationCorpus(data_folder, test_file='..\/data\/exports\/val.csv', train_file='..\/data\/exports\/train.csv')\n                                          \n    print(&quot;finished loading corpus&quot;)\n\n    word_embeddings = [WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')]\n\n    document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n\n    classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n\n    trainer = ModelTrainer(classifier, corpus, optimizer=Adam)\n\n    trainer.train('..\/model_files', max_epochs=12,learning_rate=0.0001, train_with_dev=False, embeddings_storage_mode=&quot;none&quot;)\n<\/code><\/pre>\n<p>P.S.: I was able to train the same architecture with a smaller dataset in my local GPU machine with a 4GB GTX 1650 DDR5 memory and it was really quick.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_date":"2020-08-16 19:48:25.16 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-08-16 21:29:07.63 UTC",
        "Question_score":2,
        "Question_tags":"python|pytorch|torch|amazon-sagemaker|torchvision",
        "Question_view_count":2033,
        "Owner_creation_date":"2016-11-08 14:44:22.2 UTC",
        "Owner_last_access_date":"2022-04-29 00:09:02.08 UTC",
        "Owner_reputation":375,
        "Owner_up_votes":21,
        "Owner_down_votes":0,
        "Owner_views":69,
        "Answer_body":"<p>Okay, so after 2 days of continuous debugging was able to find out the root cause.\nWhat I understood is Flair does not have any limitation on the sentence length, in the sense the word count, it is taking the highest length sentence as the maximum.\nSo there it was causing issue, as in my case there were few content with 1.5 lakh rows which is too much to load the embedding of into the memory, even a 16GB GPU.\nSo there it was breaking.<\/p>\n<p><strong>To solve this<\/strong>: For content with this much lengthy words, you can take chunk of n words(10K in my case) from these kind of content from any portion(left\/right\/middle anywhere) and trunk the rest, or simply ignore those records for training if it is very minimal in comparative count.<\/p>\n<p>After this I hope you will be able to progress with your training, as it happened in my case.<\/p>\n<p>P.S.: If you are following this thread and face similar issue feel free to comment back so that I can explore and help on your case of the issue.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-08-18 17:28:38.657 UTC",
        "Answer_score":1.0,
        "Owner_location":"Kolkata, West Bengal, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63441299",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52588354,
        "Question_title":"How to deploy breast cancer prediction endpoint created by AWS Sagemaker using Lambda and API gateway?",
        "Question_body":"<p>I am trying to deploy the existing breast cancer prediction model on Amazon Sagemanker using AWS Lambda and API gateway. I have followed the official documentation from the below url.<\/p>\n\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/<\/a><\/p>\n\n<p>I am getting a type error at \"predicted_label\".<\/p>\n\n<pre><code> result = json.loads(response['Body'].read().decode())\n print(result)\n pred = int(result['predictions'][0]['predicted_label'])\n predicted_label = 'M' if pred == 1 else 'B'\n\n return predicted_label\n<\/code><\/pre>\n\n<p>please let me know if someone could resolve this issue. Thank you. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-10-01 09:43:17.197 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2019-07-04 13:13:09.407 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-lambda|aws-api-gateway|amazon-sagemaker",
        "Question_view_count":502,
        "Owner_creation_date":"2016-02-26 10:43:01.213 UTC",
        "Owner_last_access_date":"2022-09-24 17:11:04.75 UTC",
        "Owner_reputation":999,
        "Owner_up_votes":65,
        "Owner_down_votes":0,
        "Owner_views":92,
        "Answer_body":"<p>By printing the result type by <code>print(type(result))<\/code> you can see its a dictionary. now you can see the key name is \"score\" instead of \"predicted_label\" that you are giving to pred. Hence replace it with<\/p>\n\n<pre><code>pred = int(result['predictions'][0]['score'])\n<\/code><\/pre>\n\n<p>I think this solves your problem.<\/p>\n\n<p>here is my lambda function:<\/p>\n\n<pre><code>import os\nimport io\nimport boto3\nimport json\nimport csv\n\n# grab environment variables\nENDPOINT_NAME = os.environ['ENDPOINT_NAME']\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n   print(\"Received event: \" + json.dumps(event, indent=2))\n\n   data = json.loads(json.dumps(event))\n   payload = data['data']\n   print(payload)\n\n   response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                      ContentType='text\/csv',\n                                      Body=payload)\n   #print(response)\n   print(type(response))\n   for key,value in response.items():\n       print(key,value)\n   result = json.loads(response['Body'].read().decode())\n   print(type(result))\n   print(result['predictions'])\n   pred = int(result['predictions'][0]['score'])\n   print(pred)\n   predicted_label = 'M' if pred == 1 else 'B'\n\n   return predicted_label\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-10-01 09:53:33.807 UTC",
        "Answer_score":4.0,
        "Owner_location":"Pune, Maharashtra, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52588354",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56861525,
        "Question_title":"Interpretation of output of bounding box annotation job",
        "Question_body":"<p>The output folder of an annotation job contains the following file structure: <\/p>\n\n<ul>\n<li><p>active learning<\/p><\/li>\n<li><p>annotation-tools<\/p><\/li>\n<li><p>annotations<\/p><\/li>\n<li><p>intermediate<\/p><\/li>\n<li><p>manifests<\/p><\/li>\n<\/ul>\n\n<p>Each line of the manifests\/output\/output.manifest file is a dictionary, where the key 'jobname' contains information about the annotations, and the key 'jobname-metadata' contains confidence score and other information about each of the bounding box annotations. There is also another folder called annotations which contain json files which contain information about annotations and associated worker ids. How are the two annotation informations related to each other? Is there any blogs\/tutorials which discuss how to interpret the data received from amazon sagemaker ground-truth service? Thanks in advance. <\/p>\n\n<p>Links I referred to: \n1. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-output.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-output.html<\/a>\n2. <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb<\/a> <\/p>\n\n<p>I have displayed the annotations received using the code available in the link 2 <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, which treats consolidated annotations and worker response separately.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-03 00:37:14.297 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|bounding-box|amazon-sagemaker|labeling",
        "Question_view_count":199,
        "Owner_creation_date":"2019-07-02 23:47:49.163 UTC",
        "Owner_last_access_date":"2019-08-22 20:40:42.113 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>Thank you for your question. I\u2019m the product manager for Amazon SageMaker Ground Truth and am happy to answer your question here.<\/p>\n\n<p>We have a feature called annotation consolidation that takes the responses from multiple workers for a single image and then consolidates those responses into a single set of bounding boxes for the image. The bounding boxes referenced in the manifest file are the consolidated responses whereas what you see in the annotations folders are the raw annotations (which is why you have the respective worker IDs). <\/p>\n\n<p>You can find out more about the annotation consolidation feature here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-annotation-consolidation.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-annotation-consolidation.html<\/a><\/p>\n\n<p>Please let us know if you have any further questions.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-08-19 17:25:46.553 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56861525",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62557113,
        "Question_title":"'NoneType' object has no attribute 'boto_region_name'",
        "Question_body":"<p>I've been wrapped around this problem for a while and can't seem to understand where this issue is coming from. I'm deploying a model on Sagemaker and I get the error on this line of code:<\/p>\n<p><code>sm_model.deploy(initial_instance_count=1, instance_type='ml.m4.2xlarge', endpoint_name=endpoint_name)<\/code><\/p>\n<p>Jupyter Notebook outputs the error below. Note: Line 269 isn't code in my Notebook, it is just a reference I get as a result of my model.deploy command above.<\/p>\n<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n\n    267             return self.image\n    268 \n--&gt; 269         region_name = self.sagemaker_session.boto_region_name\n    270         return create_image_uri(\n    271             region_name,\n\nAttributeError: 'NoneType' object has no attribute 'boto_region_name'\n<\/code><\/pre>\n<p>Edit: This is just an example dataset that I'm using to create this pipeline. This is on a sagemaker notebook instance. I'm adding the entire code for clarification below.<\/p>\n<pre><code>from sagemaker.model import Model\nfrom sagemaker.pipeline import PipelineModel\nimport boto3\nfrom time import gmtime, strftime\n\nimport boto3\nimport s3fs\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\nsns.set()\n\n\n\nimport boto3\nimport sagemaker\nfrom sagemaker import get_execution_role\n\n# Using Amazon S3\ns3 = boto3.client('s3')\nsage = boto3.client('sagemaker')\n\nsession = boto3.session.Session()    \nsagemaker_session = sagemaker.Session()\n\n# Get a SageMaker-compatible role used by this Notebook Instance.\nrole = get_execution_role()\n\n\n#Upload file using AWS session\n# S3 prefix\n\nprefix = 'Scikit-keras-NLP-pipeline-Boston-Housing-example-June08-test1'\n\n\ntrain_input = sagemaker_session.upload_data(\n    path='housing.csv', \n    bucket=bucket,\n    key_prefix='{}\/{}'.format(prefix, 'train'))\n\n\n\nfrom sagemaker.sklearn.estimator import SKLearn\noutput_dir = 's3:\/\/sagemaker-us-east-1-819182027957\/Scikit-keras-NLP-pipeline-Boston-Housing-example-July08-test1\/train'\nmodel_dir = 's3:\/\/sagemaker-us-east-1-819182027957\/Scikit-keras-NLP-pipeline-Boston-Housing-example-June08-test1\/train'\n\nscript_path = 'Boston.py'\n\nsklearn_preprocessor = SKLearn(\n    entry_point=script_path,\n    role=role,\n    train_instance_type=&quot;ml.c4.xlarge&quot;,\n    sagemaker_session=sagemaker_session,\n    output_path=output_dir)\n\n\nsklearn_preprocessor.fit({'train': train_input,'model-dir':model_dir,'output-data-dir':output_dir})\n\n\n\n\n\n\nfrom sagemaker.tensorflow.serving import Model\nsagemaker_estimator = Model(model_data = 's3:\/\/' + sagemaker_session.default_bucket() + '\/Scikit-keras-NLP-pipeline-Boston-Housing-example-June08-test1\/train\/Bostonmodel.tar.gz',\n                                  role = role)\n\n\n#####\n\n\nscikit_learn_inference_model = sklearn_preprocessor.create_model()\n#sagemaker_model = sagemaker_estimator.create_model()                     # Does Not have create_model method\nsagemaker_model = sagemaker_estimator\n\n\n\n\nmodel_name = 'Boston-inf-pipeline-July08-model' \nendpoint_name = 'Boston-inf-pipeline-July08-endpoint'\n\n\n#Build Inference Pipeline\nsm_model = PipelineModel(\n    name=model_name, \n    role=role, \n    models=[\n        scikit_learn_inference_model, \n        sagemaker_model],\n    sagemaker_session=sagemaker_session)\n\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_date":"2020-06-24 14:12:39.69 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-07-08 18:42:20.347 UTC",
        "Question_score":2,
        "Question_tags":"python-3.x|boto3|boto|amazon-sagemaker",
        "Question_view_count":1169,
        "Owner_creation_date":"2017-03-15 21:45:51.973 UTC",
        "Owner_last_access_date":"2022-07-27 19:44:15.773 UTC",
        "Owner_reputation":379,
        "Owner_up_votes":21,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Answer_body":"<p>The error is caused by your <code>sagemaker.tensorflow.serving.Model<\/code> not having a <code>sagemaker.session.Session<\/code> associated with it.<\/p>\n<p>Add <code>sagemaker_session=sagemaker_session<\/code> to your Model instantiation:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tensorflow.serving import Model\nsagemaker_model = Model(model_data='s3:\/\/' + sagemaker_session.default_bucket() + '\/Scikit-keras-NLP-pipeline-Boston-Housing-example-June08-test1\/train\/Bostonmodel.tar.gz',\n                        role=role,\n                        sagemaker_session=sagemaker_session)\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-07-08 20:14:36.337 UTC",
        "Answer_score":3.0,
        "Owner_location":"Washington, DC, United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62557113",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54224934,
        "Question_title":"SageMaker TensorFlow serving stack comparisons",
        "Question_body":"<p>SageMaker seems to give examples of using two different serving stacks for serving custom docker images:<\/p>\n\n<ol>\n<li>NGINX + Gunicorn + Flask<\/li>\n<li>NGINX + TensorFlow Serving<\/li>\n<\/ol>\n\n<p>Could someone explain to me at a very high level (I have very little knowledge of network engineering) what responsibilities these different components have? And since the second stack has only two components instead of one, can I rightly assume that TensorFlow Serving does the job (whatever that may be) of both Gunicorn and Flask? <\/p>\n\n<p>Lastly, I've read that it's possible to use Flask and TensorFlow serving at the same time. Would this then be NGINX -> Gunicorn -> Flask -> TensorFlow Serving? And what are there advantages of this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-16 20:33:34.743 UTC",
        "Question_favorite_count":3.0,
        "Question_last_edit_date":"2019-01-18 09:03:43.467 UTC",
        "Question_score":2,
        "Question_tags":"nginx|networking|tensorflow-serving|amazon-sagemaker|serving",
        "Question_view_count":741,
        "Owner_creation_date":"2013-02-20 05:47:52.693 UTC",
        "Owner_last_access_date":"2022-09-23 20:45:28.4 UTC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Answer_body":"<p>I'll try to answer your question on a high level. Disclaimer: I'm not at an expert across the full stack of what you describe, and I would welcome corrections or additions from people who are. <\/p>\n\n<p>I'll go over the different components from bottom to top:<\/p>\n\n<p><strong>TensorFlow Serving<\/strong> is a library for deploying and hosting TensorFlow models as model servers that accept requests with input data and return model predictions. The idea is to train models with TensorFlow, export them to the SavedModel format and serve them with TF Serving. You can set up a TF Server to accept requests via HTTP and\/or RPC. One advantage of RPC is that the request message is compressed, which can be useful when sending large payloads, for instance with image data.<\/p>\n\n<p><strong>Flask<\/strong> is a python framework for writing web applications. It's much more general-purpose than TF Serving and is widely used to build web services, for instance in microservice architectures. <\/p>\n\n<p>Now, the combination of Flask and TensorFlow serving should make sense. You could write a Flask web application that exposes an API to the user and calls a TF model hosted with TF Serving under the hood. The user uses the API to transmit some data (<strong>1<\/strong>), the Flask app perhaps transform the data (for example, wrap it in numpy arrays), calls the TF Server to get a model prediction (<strong>2<\/strong>)(<strong>3<\/strong>), perhaps transforms the prediction (for example convert a predicted probability that is larger than 0.5 to a class label of 1), and returns the prediction to the user (<strong>4<\/strong>). You could visualize this as follows:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/67EXW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/67EXW.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Gunicorn<\/strong> is a Web Server Gateway Interface (WSGI) that is commonly used to host Flask applications in production systems. As the name says, it's the interface between a web server and a web application. When you are developing a Flask app, you can run it locally to test it. In production, gunicorn will run the app for you.<\/p>\n\n<p>TF Serving will host your model as a functional application. Therefore, you do not need gunicorn to run the TF Server application for you. <\/p>\n\n<p><strong>Nginx<\/strong> is the actual web server, which will host your application, handle requests from the outside and pass them to the application server (gunicorn). Nginx cannot talk directly to Flask applications, which is why gunicorn is there. <\/p>\n\n<p><a href=\"https:\/\/serverfault.com\/questions\/331256\/why-do-i-need-nginx-and-something-like-gunicorn\">This answer<\/a> might be helpful as well. <\/p>\n\n<p>Finally, if you are working on a cloud platform, the web server part will probably be handled for you, so you will either need to write the Flask app and host it with gunicorn, or setup the TF Serving server. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-01-18 08:53:59.5 UTC",
        "Answer_score":2.0,
        "Owner_location":"NYC",
        "Answer_last_edit_date":"2019-01-18 12:57:59.183 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54224934",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64286191,
        "Question_title":"How to add keyboard shortcuts to AWS Ground Truth labeler UI?",
        "Question_body":"<p>I'm using AWS Sagemaker Ground Truth for a Custom Labeling Task that involves editing bounding boxes and their labels.  Ground Truth's UI has built-in keyboard shortcuts for doing things like choosing the label for a box, but it seems to lack shortcuts for other built-in UI elements like &quot;No adjustments needed&quot; or the &quot;Submit&quot; button.<\/p>\n<p>Is there a way to add such shortcuts?  I've looked at the crowd-html-elements for customizing the appearance of the page, but can't find anything in there about keyboard shortcuts.  It doesn't even look like crowd-button or crowd-icon-button support specifying a shortcut as an attribute.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-09 19:28:19.933 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker|labeling",
        "Question_view_count":412,
        "Owner_creation_date":"2010-11-14 22:01:50.723 UTC",
        "Owner_last_access_date":"2022-09-13 00:36:55.383 UTC",
        "Owner_reputation":309,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":40,
        "Answer_body":"<p>Could try something like:<\/p>\n<pre><code>document.addEventListener('keydown', function(event) {\n  if (event.shiftKey &amp;&amp; event.keyCode === 13) {\n    document.getElementsByTagName('crowd-bounding-box')[0].shadowRoot.getElementById('nothing-to-adjust').querySelector('label').click();\n  }\n});\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-11-20 00:10:50.09 UTC",
        "Answer_score":3.0,
        "Owner_location":"Mt Kisco, NY",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64286191",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73471486,
        "Question_title":"How to prevent storing data in Jupyter project tree when writing data from Sagemaker to S3",
        "Question_body":"<p>I am new to AWS Sagemaker and I wrote data to my S3 bucket.\nBut these datasets also appear in the working tree of my jupyter instance.<\/p>\n<p>How can I move data directly to S3 without saving it &quot;locally&quot;?<\/p>\n<p>My code:<\/p>\n<pre><code>import os\nimport pandas as pd\n\nimport sagemaker, boto3\nfrom sagemaker import get_execution_role\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.serializers import CSVSerializer\n\n# please provide your own bucket and folder path of your bucket here\nbucket = &quot;test-bucket2342343&quot;\nsm_sess = sagemaker.Session(default_bucket=bucket)\nfile_path = &quot;Use Cases\/Sagemaker Demo\/xgboost&quot;\n\n# data \ndf_train = pd.DataFrame({'X':[0,100,200,400,450,  550,600,800,1600],\n                         'y':[0,0,  0,  0,  0,    1,  1,  1,  1]})\n\ndf_test = pd.DataFrame({'X':[10,90,240,459,120,  650,700,1800,1300],\n                        'y':[0,0,  0,  0,  0,    1,  1,  1,  1]})\n\n# move to S3 \ndf_train[['y','X']].to_csv('train.csv', header=False, index=False)\n\ndf_val = df_test.copy()\ndf_val[['y','X']].to_csv('val.csv', header=False, index=False)\n\nboto3.Session().resource(&quot;s3&quot;).Bucket(bucket) \\\n.Object(os.path.join(file_path, &quot;train.csv&quot;)).upload_file(&quot;train.csv&quot;)\n\nboto3.Session().resource(&quot;s3&quot;).Bucket(bucket) \\\n.Object(os.path.join(file_path, &quot;val.csv&quot;)).upload_file(&quot;val.csv&quot;)\n\n<\/code><\/pre>\n<p>It successfully appears in my S3 bucket.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/d1yCy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/d1yCy.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But it also appears here:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/RjGZr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RjGZr.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-24 10:24:43.943 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-08-24 10:33:21.25 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":17,
        "Owner_creation_date":"2019-08-16 07:41:01.083 UTC",
        "Owner_last_access_date":"2022-09-24 16:45:09.263 UTC",
        "Owner_reputation":188,
        "Owner_up_votes":134,
        "Owner_down_votes":4,
        "Owner_views":43,
        "Answer_body":"<p>with Pandas you can save to S3 directly (<a href=\"https:\/\/stackoverflow.com\/a\/56275519\/121956\">relevant answer<\/a>). For example:<\/p>\n<pre><code>import pandas as pd\ndf = pd.DataFrame( [ [1, 1, 1], [2, 2, 2] ], columns=['a', 'b', 'c'])\ndf.to_csv('s3:\/\/test-bucket2342343\/\/tmp.csv', index=False)\n<\/code><\/pre>\n<p>Or, use what you currently do and delete the local files:<\/p>\n<pre><code>import os\nos.remove('train.csv')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-25 17:19:20.213 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73471486",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51853249,
        "Question_title":"Error Tracking in Amazon SageMaker",
        "Question_body":"<p>I am trying to create a custom Image Classifier in Amazon SageMaker. It is giving me the following error:<\/p>\n\n<p><code>\"ClientError: Data download failed:NoSuchKey (404): The specified key does not exist.\"<\/code><\/p>\n\n<p>I'm assuming this means one of the pictures in my <code>.lst<\/code> file is missing from the directory. Is there some way to find out which <code>.lst<\/code> listing it is specifically having trouble with?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2018-08-15 05:08:01.237 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-08-15 08:48:50.32 UTC",
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":305,
        "Owner_creation_date":"2013-03-15 12:54:59.923 UTC",
        "Owner_last_access_date":"2022-09-23 16:05:44.973 UTC",
        "Owner_reputation":828,
        "Owner_up_votes":516,
        "Owner_down_votes":1172,
        "Owner_views":530,
        "Answer_body":"<p>Upon further examination (of the log files), it appears the issue does not lie with the .lst file itself, but with the image files it was referencing (which now leaves me wondering why AWS doesn't just say that instead of saying the .lst file is corrupt). I'm going through the image files one-by-one to verify they are correct, hopefully that will solve the problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-12-17 16:20:24.71 UTC",
        "Answer_score":0.0,
        "Owner_location":"Washington, DC, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51853249",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64684503,
        "Question_title":"Does AWS Sagemaker charges you per API request?",
        "Question_body":"<p>AWS pricing page describes how much it costs per hour to run AWS Sagemaker for online realtime inference.\n<a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/<\/a><\/p>\n<p>But AWS usually also charges for API requests.\nDo they charge extra per every API inference request to the Sagemaker model?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-11-04 17:01:34.833 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":187,
        "Owner_creation_date":"2017-08-02 21:51:50.163 UTC",
        "Owner_last_access_date":"2022-09-22 16:56:11.783 UTC",
        "Owner_reputation":404,
        "Owner_up_votes":55,
        "Owner_down_votes":2,
        "Owner_views":25,
        "Answer_body":"<p>I am on the AWS SageMaker team.  For &quot;Real-Time Inference&quot; you are only charged for:<\/p>\n<ol>\n<li>usage of the instance types you choose (instance hours)<\/li>\n<li>storage attached to those instance (GB storage hours)<\/li>\n<li>data in and out of your Endpoint (Bytes in\/out)<\/li>\n<\/ol>\n<p>See &quot;Pricing Example #6: Real-Time Inference&quot; as well.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-12-16 22:55:11.613 UTC",
        "Answer_score":1.0,
        "Owner_location":"London, \u0412\u0435\u043b\u0438\u043a\u043e\u0431\u0440\u0438\u0442\u0430\u043d\u0438\u044f",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64684503",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59088199,
        "Question_title":"incremental training on custom code in amazon sagemaker",
        "Question_body":"<p>I'm moving my first steps in <code>amazon sagemaker<\/code>. I'm using script mode to train a classification algorithm. Training is fine, however I'm not able to do incremental training. I want to train again the same model with new data. Here what I did. This is my script:<\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker.tensorflow import TensorFlow\nfrom sagemaker import get_execution_role\n\nbucket = 'sagemaker-blablabla'\ntrain_data = 's3:\/\/{}\/{}'.format(bucket,'train')\nvalidation_data = 's3:\/\/{}\/{}'.format(bucket,'test')\n\ns3_output_location = 's3:\/\/{}'.format(bucket)\n\ntf_estimator = TensorFlow(entry_point='main.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          framework_version='1.12', \n                          py_version='py3',\n                          output_path=s3_output_location)\n\ninputs = {'train': train_data, 'test': validation_data}\ntf_estimator.fit(inputs)\n<\/code><\/pre>\n\n<p>The entry point is my custom keras code, which I adapted to receive arguments from the script.\nNow the training is successfully completed and I have in my s3 bucket the model.tar.gz. I want to train again, but it's not clear to me how to do it.. I tried this<\/p>\n\n<pre><code>trained_model = 's3:\/\/sagemaker-blablabla\/sagemaker-tensorflow-scriptmode-2019-11-27-12-01-42-300\/output\/model.tar.gz'\n\ntf_estimator = sagemaker.estimator.Estimator(image_name='blablabla-west-1.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.12-gpu-py3', \n                                              role=get_execution_role(),\n                                              train_instance_count=1, \n                                              train_instance_type='ml.p2.xlarge',\n                                              output_path=s3_output_location,\n                                              model_uri = trained_model)\n\ninputs = {'train': train_data, 'test': validation_data}\n\ntf_estimator.fit(inputs)\n<\/code><\/pre>\n\n<p>Doesn't work. Firstly, I don't know how to retrieve the training image name (for this I looked for it in the <code>aws<\/code> console, but I guess there should be a smarter solution), second this code throws an exception about the entry point but it is my understanding that I shouldn't need it when I do incremental learning with a ready image.\nI'm surely missing something important, any help? Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-11-28 11:47:23.827 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2019-11-28 12:32:50.14 UTC",
        "Question_score":1,
        "Question_tags":"python|tensorflow|amazon-sagemaker",
        "Question_view_count":621,
        "Owner_creation_date":"2014-11-18 21:32:30.293 UTC",
        "Owner_last_access_date":"2022-09-24 17:06:59.437 UTC",
        "Owner_reputation":2302,
        "Owner_up_votes":51,
        "Owner_down_votes":4,
        "Owner_views":227,
        "Answer_body":"<p>Incremental training is a native feature for the built-in <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/now-easily-perform-incremental-learning-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">Image Classifier and Object Detector<\/a>. For custom code, it is the developer responsibility to write the incremental training logic and to verify its validity. Here is a possible path:<\/p>\n\n<ol>\n<li>use one of the data channels passed in the <code>fit<\/code> to load a model state (artifact to fine-tune)<\/li>\n<li>in your code, check if the model state channel is filled\nwith artifacts. If it is, instantiate a model from that state\nand continue training. This is framework specific and you may to take\nnecessary precautions to avoid forgetting previous learnings.<\/li>\n<\/ol>\n\n<p>Some frameworks provide better support for incremental learning that others. For example some sklearn models provide an <a href=\"https:\/\/scikit-learn.org\/0.15\/modules\/scaling_strategies.html#incremental-learning\" rel=\"nofollow noreferrer\">incremental_fit<\/a> method. For DL frameworks it is technically very easy to continue training from a checkpoint, but if new data is very different from previously-seen data this may lead your model to forget previous learnings.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-11-28 22:49:48.937 UTC",
        "Answer_score":0.0,
        "Owner_location":"Jesi, Italy",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59088199",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65794703,
        "Question_title":"Amazon Forecast - extra attributes in the TARGET_TIME_SERIES?",
        "Question_body":"<p>On Amazon Forecast, let\u2019s say I have a variable (Z) which is supposed to help me predict a set of target variables (Y1, Y2, Y3).<\/p>\n<p>First question is, what is the difference between:<\/p>\n<ol>\n<li>put Z as an extra attribute in the TARGET_TIME_SERIES, that is, as an extra column<\/li>\n<li>put Z as an attribute in the RELATED_TIME_SERIES<\/li>\n<\/ol>\n<p>Second question is, given that Z has just one value per day (let\u2019s say this is a stock price), how should I deal with the fact that I have 3x-repeated timestamps? Should I just repeat Z for each repeated date?<\/p>\n<p>I understand that, if I'm not training my model to predict Z, I need to provide future values for it. But this makes option 1) even it more confusing to me. In which cases should one add an extra attribute in TARGET_TIME_SERIES?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-19 15:39:21.497 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-forecast",
        "Question_view_count":36,
        "Owner_creation_date":"2019-05-11 00:49:37.467 UTC",
        "Owner_last_access_date":"2022-09-23 16:33:50.7 UTC",
        "Owner_reputation":373,
        "Owner_up_votes":349,
        "Owner_down_votes":1,
        "Owner_views":41,
        "Answer_body":"<p>I got a very nice explanation of it in here, so I'll leave it as the answer:\n<a href=\"https:\/\/github.com\/aws-samples\/amazon-forecast-samples\/issues\/104#issuecomment-764896502\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-forecast-samples\/issues\/104#issuecomment-764896502<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-01-21 21:04:52.84 UTC",
        "Answer_score":0.0,
        "Owner_location":"Stockholm, Sweden",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65794703",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73731665,
        "Question_title":"Can SageMaker Training have training data in NVMe volumes on compatible instances?",
        "Question_body":"<p>Can SageMaker Training have training data in NVMe volumes on compatible instances? (eg G4dn and P3dn). If so, if there an appropriate way to programmatically access that data?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-15 13:03:30.387 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|nvme",
        "Question_view_count":16,
        "Owner_creation_date":"2017-10-10 18:48:14.19 UTC",
        "Owner_last_access_date":"2022-09-22 19:06:20.42 UTC",
        "Owner_reputation":98,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":"<p>Yes on all nitro-backed instances EBS volumes that are exposed as NVMe block devices.<\/p>\n<p>In the Sagemaker Python SDK, you can specify the\u00a0<code>volume_size<\/code>\u00a0of the\u00a0<code>SM_TRAINING_CHANNEL<\/code>\u00a0path - the EBS (NVMe backed) will be in that path and when you go to actually run you pass the\u00a0<code>--train_dir<\/code>\u00a0path to your code.<\/p>\n<p>Code example below:<\/p>\n<pre><code>def main(aws_region,s3_location,instance_cout):\n    estimator = TensorFlow(\n        train_instance_type='ml.p3.16xlarge',\n            **train_volume_size=200,**\n        train_instance_count=int(instance_count),\n        framework_version='2.2',\n            py_version='py3',\n        image_name=&quot;231748552833.dkr.ecr.%s.amazonaws.com\/sage-py3-tf-hvd:latest&quot;%aws_region,\n<\/code><\/pre>\n<p>And then in your entry script<\/p>\n<pre><code>train_dir = os.environ.get('SM_CHANNEL_TRAIN')\nsubprocess.call(['python','-W ignore', 'deep-learning-models\/legacy\/models\/resnet\/tensorflow2\/train_tf2_resnet.py', \\\n            &quot;--data_dir=%s&quot;%train_dir, \\\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-17 23:24:54.04 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73731665",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50068941,
        "Question_title":"SageMaker Tensorflow - how to write my serving_input_fn()",
        "Question_body":"<p>I'm pretty new to Tensorflow and SageMaker and I'm trying to figure out how to write my <code>serving_input_fn()<\/code>. I've tried a number of ways to do it, but to no avail. <\/p>\n\n<p>my input function has 3 feature columns: <code>amount_normalized, x_month and y_month<\/code>:<\/p>\n\n<pre><code>def construct_feature_columns():\n    amount_normalized = tf.feature_column.numeric_column(key='amount_normalized')\n    x_month = tf.feature_column.numeric_column(key='x_month')\n    y_month = tf.feature_column.numeric_column(key='y_month')\n    return set([amount_normalized, x_month, y_month])\n<\/code><\/pre>\n\n<p>I want to be able to call my deployed model using something like <code>deployed_model.predict([1.23,0.3,0.8])<\/code> <\/p>\n\n<p>Where the first element is <code>amount_normalized<\/code>, second is <code>x_month<\/code> third is <code>y_month<\/code><\/p>\n\n<p>I've tried this:<\/p>\n\n<pre><code>FEATURES = ['amount_normalized', 'x_month', 'y_month']\ndef serving_input_fn(params):\n    feature_placeholders = {\n      key : tf.placeholder(tf.float32, [None]) \\\n        for key in FEATURES\n    }\nreturn tf.estimator.export.build_raw_serving_input_receiver_fn(feature_placeholders)()\n<\/code><\/pre>\n\n<p>But all I get is:\n<code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\".<\/code><\/p>\n\n<p>Any help would be <strong>really<\/strong> appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-04-27 19:14:45.467 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":1337,
        "Owner_creation_date":"2015-04-07 18:40:18.42 UTC",
        "Owner_last_access_date":"2019-03-12 12:37:26.967 UTC",
        "Owner_reputation":301,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":"<p>Posting this here in case anyone else has this issue.<\/p>\n\n<p>After a bunch of trial and error I managed to solve my issue by writing my serving input function like this:<\/p>\n\n<pre><code>FEATURES = ['amount_normalized', 'x_month', 'y_month']\ndef serving_input_fn(hyperparameters):\n    feature_spec = {\n        key : tf.FixedLenFeature(shape=[], dtype = tf.float32) \\\n          for key in FEATURES\n    }\n    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)()\n<\/code><\/pre>\n\n<p>I can then call my deployed model by passing in a hash:<\/p>\n\n<pre><code>deployed_model.predict({\"amount_normalized\": 2.3, \"x_month\": 0.2, \"y_month\": -0.3})\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-04-29 16:33:54.973 UTC",
        "Answer_score":3.0,
        "Owner_location":"New York, United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50068941",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52876202,
        "Question_title":"How to bulk test the Sagemaker Object detection model with a .mat dataset or S3 folder of images?",
        "Question_body":"<p>I have trained the following Sagemaker model: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco<\/a><\/p>\n\n<p>I've tried both the JSON and RecordIO version. In both, the algorithm is tested on ONE sample image. However, I have a dataset of 2000 pictures, which I would like to test. I have saved the 2000 jpg pictures in a folder within an S3 bucket and I also have two .mat files (pics + ground truth). How can I apply this model to all 2000 pictures at once and then save the results, rather than doing it one picture at a time?<\/p>\n\n<p>I am using the code below to load a single picture from my S3 bucket:<\/p>\n\n<pre><code>object = bucket.Object('pictures\/pic1.jpg')\nobject.download_file('pic1.jpg')\nimg=mpimg.imread('pic1.jpg')\nimg_name = 'pic1.jpg'\nimgplot = plt.imshow(img)\nplt.show(imgplot)\n\nwith open(img_name, 'rb') as image:\n    f = image.read()\n    b = bytearray(f)\n    ne = open('n.txt','wb')\n    ne.write(b)\n\nimport json\nobject_detector.content_type = 'image\/jpeg'\nresults = object_detector.predict(b)\ndetections = json.loads(results)\nprint (detections['prediction'])\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-10-18 14:22:53.95 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python-3.x|amazon-web-services|amazon-s3|mat|amazon-sagemaker",
        "Question_view_count":112,
        "Owner_creation_date":"2017-03-18 21:45:08.19 UTC",
        "Owner_last_access_date":"2022-04-01 20:48:19.743 UTC",
        "Owner_reputation":509,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":84,
        "Answer_body":"<p>I'm not sure if I understood your question correctly. However, if you want to feed multiple images to the model at once, you can create a multi-dimensional array of images (byte arrays) to feed the model.<\/p>\n\n<p>The code would look something like this.<\/p>\n\n<pre><code>import numpy as np\n...\n\n#  predict_images_list is a Python list of byte arrays\npredict_images = np.stack(predict_images_list)\n\nwith graph.as_default():\n    #  results is an list of typical results you'd get.\n    results = object_detector.predict(predict_images)\n<\/code><\/pre>\n\n<p>But, I'm not sure if it's a good idea to feed 2000 images at once. Better to batch them in 20-30 images at a time and predict. <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-10-19 16:10:56.44 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52876202",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72448994,
        "Question_title":"Train Amazon SageMaker object detection model on local PC",
        "Question_body":"<p>I wonder if it's possible to run training Amazon SageMaker object detection model on a local PC?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-31 13:46:55.903 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|training-data|custom-training",
        "Question_view_count":70,
        "Owner_creation_date":"2015-09-20 22:02:33.537 UTC",
        "Owner_last_access_date":"2022-09-24 17:11:02.827 UTC",
        "Owner_reputation":71,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Answer_body":"<p>You're probably referring to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection.html\" rel=\"nofollow noreferrer\">this<\/a> object detection algorithm which is part of of <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">Amazon SageMaker built-in algorithms<\/a>. <strong>Built-in algorithms must be trained on the cloud<\/strong>.<br \/>\nIf you're bringing your own Tensorflow or PyTorch model, you could use SageMaker training jobs to train either on the cloud or locally as @kirit noted.<\/p>\n<p>I would also look at <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-jumpstart.html\" rel=\"nofollow noreferrer\">SageMaker JumpStart<\/a> for a wide variety of object detection algorithm which are TF\/PT based.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-01 08:44:03.173 UTC",
        "Answer_score":1.0,
        "Owner_location":"Kyiv",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72448994",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52684987,
        "Question_title":"AWS NoCredentials in training",
        "Question_body":"<p>I am attempting to run the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">example code<\/a> for Amazon Sagemaker on a local GPU.  I have copied the code from the Jupyter notebook to the following Python script:<\/p>\n\n<pre><code>import boto3\nimport subprocess\nimport sagemaker\nfrom sagemaker.mxnet import MXNet\nfrom mxnet import gluon\nfrom sagemaker import get_execution_role\nimport os\n\nsagemaker_session = sagemaker.Session()\ninstance_type = 'local'\nif subprocess.call('nvidia-smi') == 0:\n    # Set type to GPU if one is present\n    instance_type = 'local_gpu'\n# role = get_execution_role()\n\ngluon.data.vision.MNIST('.\/data\/train', train=True)\ngluon.data.vision.MNIST('.\/data\/test', train=False)\n\n# successfully connects and uploads data\ninputs = sagemaker_session.upload_data(path='data', key_prefix='data\/mnist')\n\nhyperparameters = {\n    'batch_size': 100,\n    'epochs': 20,\n    'learning_rate': 0.1,\n    'momentum': 0.9,\n    'log_interval': 100\n}\n\nm = MXNet(\"mnist.py\",\n          role=role,\n          train_instance_count=1,\n          train_instance_type=instance_type,\n          framework_version=\"1.1.0\",\n          hyperparameters=hyperparameters)\n\n# fails in Docker container\nm.fit(inputs)\npredictor = m.deploy(initial_instance_count=1, instance_type=instance_type)\nm.delete_endpoint()\n<\/code><\/pre>\n\n<p>where the referenced <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mnist.py\" rel=\"nofollow noreferrer\">mnist.py<\/a> file is exactly as specified on Github. The script fails on <code>m.fit<\/code> in Docker container with the following error: <\/p>\n\n<pre><code>algo-1-1DUU4_1  | Downloading s3:\/\/&lt;S3-BUCKET&gt;\/sagemaker-mxnet-2018-10-07-00-47-10-435\/source\/sourcedir.tar.gz to \/tmp\/script.tar.gz\nalgo-1-1DUU4_1  | 2018-10-07 00:47:29,219 ERROR - container_support.training - uncaught exception during training: Unable to locate credentials\nalgo-1-1DUU4_1  | Traceback (most recent call last):\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/training.py\", line 36, in start\nalgo-1-1DUU4_1  |     fw.train()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/mxnet_container\/train.py\", line 169, in train\nalgo-1-1DUU4_1  |     mxnet_env.download_user_module()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/environment.py\", line 89, in download_user_module\nalgo-1-1DUU4_1  |     cs.download_s3_resource(self.user_script_archive, tmp)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/utils.py\", line 37, in download_s3_resource\nalgo-1-1DUU4_1  |     script_bucket.download_file(script_key_name, target)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 246, in bucket_download_file\nalgo-1-1DUU4_1  |     ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 172, in download_file\nalgo-1-1DUU4_1  |     extra_args=ExtraArgs, callback=Callback)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/transfer.py\", line 307, in download_file\nalgo-1-1DUU4_1  |     future.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 73, in result\nalgo-1-1DUU4_1  |     return self._coordinator.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 233, in result\nalgo-1-1DUU4_1  |     raise self._exception\nalgo-1-1DUU4_1  | NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n\n<p>I am confused that I can authenticate to S3 outside of the container (to pload the training\/test data) but I cannot within the Docker container.  So I am guessing the issues has to do with passing the AWS credentials to the Docker container.  Here is the generated Docker-compose file:<\/p>\n\n<pre><code>networks:\n  sagemaker-local:\n    name: sagemaker-local\nservices:\n  algo-1-1DUU4:\n    command: train\n    environment:\n    - AWS_REGION=us-west-2\n    - TRAINING_JOB_NAME=sagemaker-mxnet-2018-10-07-00-47-10-435\n    image: 123456789012.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.1.0-gpu-py2\n    networks:\n      sagemaker-local:\n        aliases:\n        - algo-1-1DUU4\n    stdin_open: true\n    tty: true\n    volumes:\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/input:\/opt\/ml\/input\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output:\/opt\/ml\/output\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output\/data:\/opt\/ml\/output\/data\n    - \/tmp\/tmpSkaR3x\/model:\/opt\/ml\/model\nversion: '2.1'\n<\/code><\/pre>\n\n<p>Should the AWS credentials be passed in as enviromental variables?<\/p>\n\n<p>I upgraded my <code>sagemaker<\/code> install to after reading <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/403\" rel=\"nofollow noreferrer\">Using boto3 in install local mode?<\/a>, but that had no effect.  I checked the credentials that are being fetched in the Sagemaker session (outside the container) and they appear to be blank, even though I have an <code>~\/.aws\/config<\/code> and <code>~\/.aws\/credentials<\/code> file:<\/p>\n\n<pre><code>{'_token': None, '_time_fetcher': &lt;function _local_now at 0x7f4dbbe75230&gt;, '_access_key': None, '_frozen_credentials': None, '_refresh_using': &lt;bound method AssumeRoleCredentialFetcher.fetch_credentials of &lt;botocore.credentials.AssumeRoleCredentialFetcher object at 0x7f4d2de48bd0&gt;&gt;, '_secret_key': None, '_expiry_time': None, 'method': 'assume-role', '_refresh_lock': &lt;thread.lock object at 0x7f4d9f2aafd0&gt;}\n<\/code><\/pre>\n\n<p>I am new to AWS so I do not know how to diagnose the issue regarding AWS credentials.  My <code>.aws\/config<\/code> file has the following information (with placeholder values):<\/p>\n\n<pre><code>[default]\noutput = json\nregion = us-west-2\nrole_arn = arn:aws:iam::123456789012:role\/SageMakers\nsource_profile = sagemaker-test\n\n[profile sagemaker-test]\noutput = json\nregion = us-west-2\n<\/code><\/pre>\n\n<p>Where the <code>sagemaker-test<\/code> profile has <code>AmazonSageMakerFullAccess<\/code> in the IAM Management Console.<\/p>\n\n<p>The <code>.aws\/credentials<\/code> file has the following information (represented by placeholder values):<\/p>\n\n<pre><code>[default]\naws_access_key_id = 1234567890\naws_secret_access_key = zyxwvutsrqponmlkjihgfedcba\n[sagemaker-test]\naws_access_key_id = 0987654321\naws_secret_access_key = abcdefghijklmopqrstuvwxyz\n<\/code><\/pre>\n\n<p>Lastly, these are versions of the applicable libraries from a <code>pip freeze<\/code>:<\/p>\n\n<pre><code>awscli==1.16.19\nboto==2.48.0\nboto3==1.9.18\nbotocore==1.12.18\ndocker==3.5.0\ndocker-compose==1.22.0\nmxnet-cu91==1.1.0.post0\nsagemaker==1.11.1\n<\/code><\/pre>\n\n<p>Please let me know if I left out any relevant information and thanks for any help\/feedback that you can provide.<\/p>\n\n<p><strong>UPDATE<\/strong>: Thanks for your help, everyone! While attempting some of your suggested fixes, I noticed that <code>boto3<\/code> was out of date, and update it (to <code>boto3-1.9.26<\/code> and <code>botocore-1.12.26<\/code>) which appeared to resolve the issue.  I was not able to find any documentation on that being an issue with <code>boto3==1.9.18<\/code>.  If someone could help me understand what the issue was with <code>boto3<\/code>, I would happy to make mark their answer as correct.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":3,
        "Question_creation_date":"2018-10-07 02:32:13.133 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-10-18 20:09:43.687 UTC",
        "Question_score":1,
        "Question_tags":"python|python-2.7|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":1374,
        "Owner_creation_date":"2013-09-29 23:23:04.177 UTC",
        "Owner_last_access_date":"2022-09-19 18:01:07.73 UTC",
        "Owner_reputation":544,
        "Owner_up_votes":174,
        "Owner_down_votes":1,
        "Owner_views":85,
        "Answer_body":"<p>SageMaker local mode is designed to pick up whatever credentials are available in your boto3 session, and pass them into the docker container as environment variables. <\/p>\n\n<p>However, the version of the sagemaker sdk that you are using (1.11.1 and earlier) will ignore the credentials if they include a token, because that usually indicates short-lived credentials that won't remain valid long enough for a training job to complete or endpoint to be useful.<\/p>\n\n<p>If you are using temporary credentials, try replacing them with permanent ones, or running from an ec2 instance (or SageMaker notebook!) that has an appropriate instance role assigned.<\/p>\n\n<p>Also, the sagemaker sdk's handling of credentials changed in v1.11.2 and later -- temporary credentials will be passed to local mode containers, but with a warning message. So you could just upgrade to a newer version and try again (<code>pip install -U sagemaker<\/code>). <\/p>\n\n<p>Also, try upgrading <code>boto3<\/code> can change, so try using the latest version.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-10-17 23:26:48.057 UTC",
        "Answer_score":1.0,
        "Owner_location":"Gloucester, VA, USA",
        "Answer_last_edit_date":"2018-11-18 08:20:34.207 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52684987",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55892554,
        "Question_title":"Invoke endpoint after model deployment : [Err 104] Connection reset by peer",
        "Question_body":"<p>I am new to Sagemaker. I have deployed my well trained model in tensorflow  by using Json and Weight file. But it is strange that in my note book, I didn't see it says \"Endpoint successfully built\". Only the below is shown:<\/p>\n\n<pre><code>--------------------------------------------------------------------------------!\n<\/code><\/pre>\n\n<p>Instead, I found the endpoint number from my console. <\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker.tensorflow.model import TensorFlowModel\n        predictor=sagemaker.tensorflow.model.TensorFlowPredictor(endpoint_name, sagemaker_session)\ndata= test_out2\npredictor.predict(data)\n<\/code><\/pre>\n\n<p>Then I try to invoke the endpoint by using 2D array:\n(1) If my 2D array is in size of (5000, 170), I am getting the error:<\/p>\n\n<pre><code>ConnectionResetError: [Errno 104] Connection reset by peer\n<\/code><\/pre>\n\n<p>(2) If reducing the array to size of (10,170), error is :<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\". See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-tensorflow-2019-04-28-XXXXXXXXX in account 15XXXXXXXX for more information.\n<\/code><\/pre>\n\n<p>Any suggestion please? Found similar case in github, <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/589\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/589<\/a>.<\/p>\n\n<p>Is it the similar case please?<\/p>\n\n<p>Thank you very much in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-04-28 16:54:16.677 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":465,
        "Owner_creation_date":"2014-05-28 14:38:02.53 UTC",
        "Owner_last_access_date":"2021-01-12 13:58:29.283 UTC",
        "Owner_reputation":61,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":"<p>The first error with data size (5000, 170) might be due to a capacity issue. SageMaker endpoint prediction has a size limit of 5mb. So if your data is larger than 5mb, you need to chop it into pieces and call predict multiple times. <\/p>\n\n<p>For the second error with data size (10, 170), the error message asks you to look into logs. Did you find anything interesting in the cloudwatch log? Anything can be shared in this question?<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-05-01 18:24:23.68 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55892554",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71880336,
        "Question_title":"can one run sagemaker notebook code locally in visual studio code",
        "Question_body":"<p>The code below works fine in a sagemaker notebook in the cloud. Locally I also have aws credentials created via the aws cli. Personally, I do not like notebooks (unless I do some EDA or the like). So I wonder if one can also fire this code up from a local machine (e.g. in visual studio code) as it only tells sagemaker what to do anyway? I guess it is only a question of authenticating and getting the session object? Thanks!<\/p>\n<pre><code>import boto3\nimport os\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.serializers import CSVSerializer\nfrom sagemaker import image_uris\n\nregion_name = boto3.session.Session().region_name\ns3_bucket_name = 'bucket_name'\n# this image cannot be used below !!! there must be an issue with sagemaker ?\ntraining_image_name = image_uris.retrieve(framework='xgboost', region=region_name, version='latest')\nrole = get_execution_role()\ns3_prefix = 'my_model'\ntrain_file_name = 'sagemaker_train.csv'\nval_file_name = 'sagemaker_val.csv'\nsagemaker_session = sagemaker.Session()\n\ns3_input_train = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, train_file_name), content_type='csv')\ns3_input_val = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, val_file_name), content_type='csv')\n\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;objective&quot;:&quot;reg:squarederror&quot;,\n        &quot;num_round&quot;:&quot;10&quot;}\n\noutput_path = 's3:\/\/{}\/{}\/output'.format(s3_bucket_name, s3_prefix)\n\nestimator = sagemaker.estimator.Estimator(image_uri=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region_name, &quot;1.2-2&quot;), \n                                          hyperparameters=hyperparameters,\n                                          role=role,\n                                          instance_count=1, \n                                          instance_type='ml.m5.2xlarge', \n                                          volume_size=1, # 1 GB \n                                          output_path=output_path)\n\nestimator.fit({'train': s3_input_train, 'validation': s3_input_val})\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-15 05:11:36.223 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-04-28 09:27:12.863 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":425,
        "Owner_creation_date":"2010-03-01 10:53:04.443 UTC",
        "Owner_last_access_date":"2022-09-24 18:56:19.313 UTC",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Answer_body":"<p>On a local machine,<\/p>\n<ul>\n<li>Make sure to install AWS CLI: <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/userguide\/getting-started-install.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cli\/latest\/userguide\/getting-started-install.html<\/a><\/li>\n<li>Create an access key id and secret access key to access Sagemaker services locally: <a href=\"https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/id_credentials_access-keys.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/id_credentials_access-keys.html<\/a><\/li>\n<li>Set these credentials using <code>aws configure<\/code> command locally.<\/li>\n<li>The code should work fine except getting the execution role. You can either hard code the Sagemaker role in the code (not best practice) or store it in the Parameter store and access it from there.<\/li>\n<\/ul>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-04-15 05:37:28.76 UTC",
        "Answer_score":1.0,
        "Owner_location":"Somewhere",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71880336",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53697587,
        "Question_title":"AWS Sagemaker ClientError: Unable to initialize the algorithm",
        "Question_body":"<p>Cannot run hyper-parameter auto tuning jobs using the image classification algorithm. <\/p>\n\n<p>Getting this from Sagemaker job info:<\/p>\n\n<blockquote>\n  <p>Failure reason\n  ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError) Caused by: Additional properties are not allowed (u'val' was unexpected) Failed validating u'additionalProperties' in schema: {u'$schema': u'<a href=\"http:\/\/json-schema.org\/draft-04\/schema#\" rel=\"noreferrer\">http:\/\/json-schema.org\/draft-04\/schema#<\/a>', u'additionalProperties': False, u'anyOf': [{u'required': [u'train']}, {u'required': [u'validation']}, {u'optional': [u'train_lst']}, {u'optional': [u'validation_lst']}, {u'optional': [u'model']}], u'definitions': {u'data_channel': {u'properties': {u'ContentType': {u'type': u'string'}}, u'type': u'object'}}, u'properties': {u'model': {u'$ref': u'#\/definitions\/data_channel'}, u'train': {u'$ref': u'#\/definitions\/data_channel'}, u'train_lst': {u'$ref': u'#\/definitions\/data_channel'}, u'validation': {u'$ref': u'#\/definitio<\/p>\n<\/blockquote>\n\n<p>CloudWatch is giving me this reason:<\/p>\n\n<blockquote>\n  <p>00:42:35\n  2018-12-09 22:42:35 Customer Error: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)<\/p>\n  \n  <p>Caused by: Additional properties are not allowed (u'val' was\n  unexpected)<\/p>\n<\/blockquote>\n\n<p>Any help please thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2018-12-09 23:04:49.43 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-01-04 21:20:01.95 UTC",
        "Question_score":6,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":1697,
        "Owner_creation_date":"2015-01-14 12:25:26.28 UTC",
        "Owner_last_access_date":"2022-09-24 21:33:27.523 UTC",
        "Owner_reputation":1951,
        "Owner_up_votes":95,
        "Owner_down_votes":32,
        "Owner_views":217,
        "Answer_body":"<p>as showed in your log, one of input channels was named as <code>val<\/code>. The correct channel name for validation data should be <code>validation<\/code>. More details on input configuration can be found here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-01-17 19:20:19.393 UTC",
        "Answer_score":3.0,
        "Owner_location":"Melrose, Johannesburg, Gauteng, South Africa",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53697587",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56835306,
        "Question_title":"Download file using boto3 within Docker container deployed on Sagemaker Endpoint",
        "Question_body":"<p>I have built my own Docker container that provides inference code to be deployed as endpoint on Amazon Sagemaker. However, this container needs to have access to some files from s3. The used IAM role has access to all s3 buckets that I am trying to reach.<\/p>\n\n<p>Code to download files using a boto3 client:<\/p>\n\n<pre><code>import boto3\n\nmodel_bucket = 'my-bucket'\n\ndef download_file_from_s3(s3_path, local_path):\n    client = boto3.client('s3')\n    client.download_file(model_bucket, s3_path, local_path)\n<\/code><\/pre>\n\n<p>The IAM role's policies:<\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::my-bucket\/*\"\n            ]\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>Starting the docker container locally allows me to download files from s3 just like expected. <\/p>\n\n<p>Deploying as an endpoint on Sagemaker, however, the request times out:<\/p>\n\n<pre><code>botocore.vendored.requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='my-bucket.s3.eu-central-1.amazonaws.com', port=443): Max retries exceeded with url: \/path\/to\/my-file (Caused by ConnectTimeoutError(&lt;botocore.awsrequest.AWSHTTPSConnection object at 0x7f66244e69b0&gt;, 'Connection to my-bucket.s3.eu-central-1.amazonaws.com timed out. (connect timeout=60)'))\n<\/code><\/pre>\n\n<p>Any help is appreciated!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-01 11:59:25.937 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"python-3.x|amazon-web-services|boto3|amazon-sagemaker",
        "Question_view_count":1171,
        "Owner_creation_date":"2016-02-25 14:44:25.89 UTC",
        "Owner_last_access_date":"2022-09-23 11:38:56.733 UTC",
        "Owner_reputation":105,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":"<p>For anyone coming across this question, when creating a model, the 'Enable Network Isolation' property defaults to True.\nFrom AWS docs:<\/p>\n<blockquote>\n<p>If you enable network isolation, the containers are not able to make any outbound network calls, even to other AWS services such as Amazon S3. Additionally, no AWS credentials are made available to the container runtime environment.<\/p>\n<\/blockquote>\n<p>So this property needs to be set to False in order to connect to any other AWS service.<\/p>\n<p><img src=\"https:\/\/i.stack.imgur.com\/5qERm.jpg\" alt=\"AWS Sagemaker UI Network Isolation set to False\" \/><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-07-03 08:22:00.397 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2022-01-04 12:28:54.4 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56835306",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65770913,
        "Question_title":"Sagemaker Studio Pyspark example fails",
        "Question_body":"<p>When I try to run the Sagemaker provided examples with PySpark in Sagemaker Studio<\/p>\n<pre><code>import os\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nimport sagemaker\nfrom sagemaker import get_execution_role\nimport sagemaker_pyspark\n\nrole = get_execution_role()\n\n# Configure Spark to use the SageMaker Spark dependency jars\njars = sagemaker_pyspark.classpath_jars()\n\nclasspath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars())\n\n# See the SageMaker Spark Github repo under sagemaker-pyspark-sdk\n# to learn how to connect to a remote EMR cluster running Spark from a Notebook Instance.\nspark = SparkSession.builder.config(&quot;spark.driver.extraClassPath&quot;, classpath)\\\n    .master(&quot;local[*]&quot;).getOrCreate()\n<\/code><\/pre>\n<p>I get the following exception:<\/p>\n<pre><code>    ---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n&lt;ipython-input-6-c8f6fff0daaf&gt; in &lt;module&gt;\n     19 # to learn how to connect to a remote EMR cluster running Spark from a Notebook Instance.\n     20 spark = SparkSession.builder.config(&quot;spark.driver.extraClassPath&quot;, classpath)\\\n---&gt; 21     .master(&quot;local[*]&quot;).getOrCreate()\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/sql\/session.py in getOrCreate(self)\n    171                     for key, value in self._options.items():\n    172                         sparkConf.set(key, value)\n--&gt; 173                     sc = SparkContext.getOrCreate(sparkConf)\n    174                     # This SparkContext may be an existing one.\n    175                     for key, value in self._options.items():\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/context.py in getOrCreate(cls, conf)\n    361         with SparkContext._lock:\n    362             if SparkContext._active_spark_context is None:\n--&gt; 363                 SparkContext(conf=conf or SparkConf())\n    364             return SparkContext._active_spark_context\n    365 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/context.py in __init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\n    127                     &quot; note this option will be removed in Spark 3.0&quot;)\n    128 \n--&gt; 129         SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    130         try:\n    131             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/context.py in _ensure_initialized(cls, instance, gateway, conf)\n    310         with SparkContext._lock:\n    311             if not SparkContext._gateway:\n--&gt; 312                 SparkContext._gateway = gateway or launch_gateway(conf)\n    313                 SparkContext._jvm = SparkContext._gateway.jvm\n    314 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/java_gateway.py in launch_gateway(conf)\n     44     :return: a JVM gateway\n     45     &quot;&quot;&quot;\n---&gt; 46     return _launch_gateway(conf)\n     47 \n     48 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/java_gateway.py in _launch_gateway(conf, insecure)\n    106 \n    107             if not os.path.isfile(conn_info_file):\n--&gt; 108                 raise Exception(&quot;Java gateway process exited before sending its port number&quot;)\n    109 \n    110             with open(conn_info_file, &quot;rb&quot;) as info:\n\nException: Java gateway process exited before sending its port number\n<\/code><\/pre>\n<p>Before running the example I installed pyspark and sagemaker_pyspark with pip from the notebook. I am also using SparkMagic kernel from the kernels library of SageMaker.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2021-01-18 08:19:49.217 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|pyspark|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":1827,
        "Owner_creation_date":"2016-01-07 18:07:35.593 UTC",
        "Owner_last_access_date":"2022-08-09 14:35:01.297 UTC",
        "Owner_reputation":247,
        "Owner_up_votes":40,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Answer_body":"<p>Maybe, you are having this issue because this notebook was designed to run when you have an EMR cluster. I suggest you start a notebook with conda_python3 kernel on Sagemaker instead of the SparkMagic kernel. You will need to install <code>pyspark<\/code> and <code>sagemaker_pyspark<\/code> using pip, but it should work with the code you posted.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-01-18 21:32:53.523 UTC",
        "Answer_score":2.0,
        "Owner_location":"Germany",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65770913",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56260720,
        "Question_title":"Import custom modules in Amazon Sagemaker Jupyter notebook",
        "Question_body":"<p>I want to import a custom module in my jupyter notebook in Sagemaker. Trying the import from Untitled1.ipynb I have tried two different structures. The first one is:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/tJb5l.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tJb5l.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Inside \"package folder\" there were the files \"cross_validation.py\" and \"<strong>init<\/strong>.py\". The followings commands have been tried:<\/p>\n\n<pre><code>from package import cross_validation\nimport package.cross_validation\n<\/code><\/pre>\n\n<p>The second one is<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/eplmc.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eplmc.png\" alt=\"emak\"><\/a><\/p>\n\n<p>and I have coded  <code>import cross_validation<\/code><\/p>\n\n<p>In both cases I get no error at all when importing, but I can't use the class inside the module because I receive the error name <code>Class_X is not defined<\/code><\/p>\n\n<p>I also have restarted the notebook, just in case and it still not working. How could I make it?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-22 15:52:03.46 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":5,
        "Question_tags":"python|import|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":6033,
        "Owner_creation_date":"2018-04-09 18:36:08.403 UTC",
        "Owner_last_access_date":"2022-09-23 12:00:52.963 UTC",
        "Owner_reputation":1754,
        "Owner_up_votes":396,
        "Owner_down_votes":76,
        "Owner_views":197,
        "Answer_body":"<p>You can add a <code>__init__.py<\/code> file to your <code>package<\/code> directory to make it a Python package. Then you will be import the modules from the package inside your Jupyter notebook<\/p>\n\n<pre><code>\/home\/ec2-user\/SageMaker\n    -- Notebook.ipynb \n    -- mypackage\n        -- __init__.py\n        -- mymodule.py\n<\/code><\/pre>\n\n<p>Contents of Notebook.ipynb<\/p>\n\n<pre><code>from mypackage.mymodule import SomeClass, SomeOtherClass\n<\/code><\/pre>\n\n<p>For more details, see <a href=\"https:\/\/docs.python.org\/3\/tutorial\/modules.html#packages\" rel=\"nofollow noreferrer\">https:\/\/docs.python.org\/3\/tutorial\/modules.html#packages<\/a><\/p>\n\n<p>Thanks for using Amazon SageMaker!<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-05-23 00:27:00.173 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56260720",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64170759,
        "Question_title":"Pyathena is super slow compared to querying from Athena",
        "Question_body":"<p>I run a query from AWS <strong>Athena console<\/strong> and takes 10s.\nThe same query run from <strong>Sagemaker<\/strong> using <strong>PyAthena<\/strong> takes 155s.\nIs PyAthena slowing it down or is the data transfer from Athena to sagemaker so time consuming?<\/p>\n<p>What could I do to speed this up?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-02 11:28:26.83 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-10-02 12:20:59.4 UTC",
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|amazon-athena|amazon-sagemaker|pyathena",
        "Question_view_count":3188,
        "Owner_creation_date":"2017-02-05 16:38:14.643 UTC",
        "Owner_last_access_date":"2022-09-21 10:19:38.63 UTC",
        "Owner_reputation":668,
        "Owner_up_votes":38,
        "Owner_down_votes":3,
        "Owner_views":85,
        "Answer_body":"<p>Just figure out a way of boosting the queries:<\/p>\n<p>Before I was trying:<\/p>\n<pre><code>import pandas as pd\nfrom pyathena import connect\n\nconn = connect(s3_staging_dir=STAGIN_DIR,\n             region_name=REGION)\npd.read_sql(QUERY, conn)\n# takes 160s\n<\/code><\/pre>\n<p>Figured out that using a <em>PandasCursor<\/em> instead of a <em>connection<\/em> is way faster<\/p>\n<pre><code>import pandas as pd\npyathena import connect\nfrom pyathena.pandas.cursor import PandasCursor\n\ncursor = connect(s3_staging_dir=STAGIN_DIR,\n                 region_name=REGION,\n                 cursor_class=PandasCursor).cursor()\ndf = cursor.execute(QUERY).as_pandas()\n# takes 12s\n<\/code><\/pre>\n<p>Ref: <a href=\"https:\/\/github.com\/laughingman7743\/PyAthena\/issues\/46\" rel=\"noreferrer\">https:\/\/github.com\/laughingman7743\/PyAthena\/issues\/46<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2020-10-02 11:48:43.903 UTC",
        "Answer_score":13.0,
        "Owner_location":"M\u00fcnchen, Germany",
        "Answer_last_edit_date":"2021-08-19 13:38:44.77 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64170759",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58806807,
        "Question_title":"Create a predictor from an endpoint in a different region",
        "Question_body":"<p>I have created an endpoint on us-east-1. try to create a predictor:<\/p>\n\n<pre><code>In [106]: sagemaker.predictor.RealTimePredictor(&lt;endpoint name&gt;)\n<\/code><\/pre>\n\n<p>and get<\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the DescribeEndpoint operation: \nCould not find endpoint \"arn:aws:sagemaker:us-east-2:&lt;account number&gt;:endpoint\/&lt;endpoint name&gt;\".\n<\/code><\/pre>\n\n<p>which is perfectly correct, since the endpoint is on us-east-1.  Probably I could change some defaults, but I'd rather not - I work on us-east-2 99% of the time.<\/p>\n\n<p>So, how can I set a different region when initializing the predictor?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2019-11-11 18:31:02.72 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":478,
        "Owner_creation_date":"2019-03-28 21:25:22.94 UTC",
        "Owner_last_access_date":"2022-07-14 13:44:34.627 UTC",
        "Owner_reputation":67,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>The (python) <code>Predictors<\/code> <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\" rel=\"nofollow noreferrer\">documentation<\/a> shows that you can pass a <code>Session<\/code> object. In turn, the <code>Session<\/code> can be <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.Session\" rel=\"nofollow noreferrer\">initialized<\/a> with a <em>client<\/em> and a <em>runtime client<\/em> - the former does everything except endpoint invocations, the latter does... endpoint invocations.<\/p>\n\n<p>Those clients are tied to specific regions. It seems like you should be able to set the runtime client region to match your endpoint, by manually instantiating it, while leaving the regular client alone (disclaimer here: I haven't tried this - if you do, let me\/us know how it goes :)).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-11-11 18:44:41.147 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58806807",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54465049,
        "Question_title":"Getting Out of Memory error when using Image Classification in Sage Maker",
        "Question_body":"<p>When using a p2.xlarge or p3.2xlarge with up to 1TB of memory trying to use the predefined SageMaker Image Classification algorithm in a training job I\u2019m getting the following error:<\/p>\n\n<p><code>ClientError: Out of Memory. Please use a larger instance and\/or reduce the values of other parameters (e.g. batch size, number of layers etc.) if applicable<\/code><\/p>\n\n<p>I\u2019m using 450+ images, I\u2019ve tried resizing them from their original 2000x3000px size to a 244x244px size down to a 24x24px size and keep getting the same error.<\/p>\n\n<p>I\u2019ve tried adjusting my hyper parameters: num_classes, num_layers, num_training_samples, optimizer, image_shape, checkpoint frequency, batch_size and epochs. Also tried using pretrained model. But the same error keeps occurring.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-31 16:27:28.437 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-02-07 04:20:17.233 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|artificial-intelligence|amazon-sagemaker",
        "Question_view_count":2360,
        "Owner_creation_date":"2013-02-25 19:50:19.38 UTC",
        "Owner_last_access_date":"2022-09-23 22:29:29.277 UTC",
        "Owner_reputation":147,
        "Owner_up_votes":148,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Answer_body":"<p>Would've added this as a comment but I don't have enough rep yet.<\/p>\n\n<p>A few clarifying questions so that I can have some more context:<\/p>\n\n<p><em>How exactly are you achieving 1TB of RAM?<\/em><\/p>\n\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p2\/\" rel=\"nofollow noreferrer\"><code>p2.xlarge<\/code><\/a> servers have 61GB of RAM, and <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p3\/\" rel=\"nofollow noreferrer\"><code>p3.2xlarge<\/code><\/a> servers have 61GB memory + 16GB onboard the Tesla V100 GPU. <\/li>\n<\/ol>\n\n<p><em>How are you storing, resizing, and ingesting the images into the SageMaker algorithm?<\/em><\/p>\n\n<ol start=\"2\">\n<li>The memory error seems suspect considering it still occurs when downsizing images to 24x24. If you are resizing your original images (450 images at 2000x3000 resolution) as in-memory objects and aren't performing the transformations in-place (ie: not creating new images), you may have a substantial bit of memory pre-allocated, causing the SageMaker training algorithm to throw an OOM error.<\/li>\n<\/ol>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2019-01-31 18:44:45.577 UTC",
        "Answer_score":2.0,
        "Owner_location":"Puebla",
        "Answer_last_edit_date":"2019-01-31 19:17:47.733 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54465049",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59091944,
        "Question_title":"Create a Multi Model Endpoint using AWS Sagemaker Boto",
        "Question_body":"<p>I have created 2 models which are not too complex and renamed them and placed them into a same location in S3 bucket.<\/p>\n\n<p>I need to create a multi model endpoint such that the 2 models have a same end point. \nThe model i am using is AWS in built Linear-learner model type regressor. <\/p>\n\n<p>I am stuck as to how they should be deployed. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-11-28 15:20:56.48 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|linear-regression|endpoint|amazon-sagemaker|multi-model-forms",
        "Question_view_count":364,
        "Owner_creation_date":"2019-11-28 15:14:17.27 UTC",
        "Owner_last_access_date":"2022-02-21 15:48:49.123 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>SageMaker's Linear Learner algorithm container does not currently implement the requirements for <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">multi-model endpoints<\/a>. You could request support in the <a href=\"https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285&amp;start=0\" rel=\"nofollow noreferrer\">AWS Forums<\/a>.<\/p>\n\n<p>You could also build your own version of the Linear Learner algorithm. To deploy the models to a multi-model endpoint you would need to build your own container that meets the requirements for multi-model endpoints and implement your own version of the Linear Learner algorithm. This sample notebook gives an example of how you would create your multi-model compatible container that serves MxNet models, but you could adapt it to implement a Linear Learner algorithm:<\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-02-25 00:24:13.857 UTC",
        "Answer_score":0.0,
        "Owner_location":"Carlow, Ireland",
        "Answer_last_edit_date":"2020-02-25 00:42:18.97 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59091944",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65322286,
        "Question_title":"AWS Sagemaker inference endpoint doesn't scale in with autoscaling",
        "Question_body":"<p>I have an AWS Sagemaker inference endpoint with autoscaling enabled with SageMakerVariantInvocationsPerInstance target metric. When I send a lot of requests to the endpoint the number of instances correctly scales out to the maximum instance count. But after I stop sending the requests the number of instances doesn't scale in to 1, minimum instance count. I waited for many hours. Is there a reason for this behaviour?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-12-16 11:23:07.513 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"autoscaling|amazon-sagemaker|aws-auto-scaling",
        "Question_view_count":1028,
        "Owner_creation_date":"2013-10-28 16:49:44.19 UTC",
        "Owner_last_access_date":"2022-09-24 09:44:05.48 UTC",
        "Owner_reputation":1311,
        "Owner_up_votes":149,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Answer_body":"<p>AutoScaling requires a cloudwatch alarm to trigger to scale in.  Sagemaker doesn't push 0 value metrics when there's no activity (it just doesn't push anything).  This leads to the alarm being put into insufficient data and not triggering the autoscaling scale in action when your workload suddenly ends.<\/p>\n<p>Workarounds are either:<\/p>\n<ol>\n<li>Have a step scaling policy using the cloudwatch metric math FILL() function for your scale in.  This way you can tell CloudWatch &quot;if there's no data, pretend this was the metric value when evaluating the alarm.  This is only possible with step scaling since target tracking creates the alarms for you (and AutoScaling will periodically recreate them, so if you make manual changes they'll get deleted)<\/li>\n<li>Have scheduled scaling set the size back down to 1 every evening<\/li>\n<li>Make sure traffic continues at a low level for some times<\/li>\n<\/ol>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-12-18 14:09:00.057 UTC",
        "Answer_score":5.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65322286",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52317237,
        "Question_title":"Machine Learning (NLP) on AWS. Cloud9? SageMaker? EC2-AMI?",
        "Question_body":"<p>I have finally arrived in the cloud to put my NLP work to the next level, but I am a bit overwhelmed with all the possibilities I have. So I am coming to you for advice.<\/p>\n\n<p>Currently I see three possibilities:<\/p>\n\n<ul>\n<li><strong>SageMaker<\/strong>\n\n<ul>\n<li>Jupyter Notebooks are great<\/li>\n<li>It's quick and simple<\/li>\n<li>saves a lot of time spent on managing everything, you can very easily get the model into production<\/li>\n<li>costs more<\/li>\n<li>no version control<\/li>\n<\/ul><\/li>\n<li><strong>Cloud9<\/strong><\/li>\n<li><strong>EC2(-AMI)<\/strong><\/li>\n<\/ul>\n\n<p>Well, that's where I am for now. I really like SageMaker, although I don't like the lack of version control (at least I haven't found anything for now).<\/p>\n\n<p>Cloud9 seems just to be an IDE to an EC2 instance.. I haven't found any comparisons of Cloud9 vs SageMaker for Machine Learning. Maybe because Cloud9 is not advertised as an ML solution. But it seems to be an option.<\/p>\n\n<p>What is your take on that question? What have I missed? What would you advise me to go for? What is your workflow and why? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2018-09-13 15:39:54.713 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-10-22 18:40:03.017 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-ec2|nlp|amazon-sagemaker|aws-cloud9",
        "Question_view_count":1442,
        "Owner_creation_date":"2016-09-07 15:36:35.243 UTC",
        "Owner_last_access_date":"2019-01-07 14:42:06.263 UTC",
        "Owner_reputation":23,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<blockquote>\n  <p>I am looking for an easy work environment where I can quickly test my models, exactly. And it won't be only me working on it, it's a team effort. <\/p>\n<\/blockquote>\n\n<p>Since you are working as a team I would recommend to use sagemaker with custom docker images. That way you have complete freedom over your algorithm. The docker images are stored in ecr. Here you can upload many versions of the same image and tag them to keep control of the different versions(which you build from a git repo).<\/p>\n\n<p>Sagemaker also gives the execution role to inside the docker image. So you still have full access to other aws resources (if the execution role has the right permissions)<\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a>\nIn my opinion this is a good example to start because it shows how sagemaker is interacting with your image.<\/p>\n\n<p><strong>Some notes on other solutions:<\/strong><\/p>\n\n<p>The problem of every other solution you posted is you want to build and execute on the same machine. Sure you can do this but keep in mind, that gpu instances are expensive and therefore you might only switch to the cloud when the code is ready to run.<\/p>\n\n<p><strong>Some other notes<\/strong><\/p>\n\n<ul>\n<li><p>Jupyter Notebooks in general are not made for collaborative programming. I think they want to change this with jupyter lab but this is still in development and sagemaker only use the notebook at the moment.<\/p><\/li>\n<li><p>EC2 is cheaper as sagemaker but you have  to do more work. Especially if you want to run your model as docker images. Also with sagemaker you can easily  build an endpoint for model inference which would be even more complex to realize with ec2.<\/p><\/li>\n<li><p>Cloud 9 I never used this service and but on first glance it seems good to develop on, but the question remains if you want to do this on a gpu machine. Because you're using ec2 as instance you have the same advantage\/disadvantage.<\/p><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-09-14 13:56:57.933 UTC",
        "Answer_score":2.0,
        "Owner_location":"Amsterdam, Netherlands",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52317237",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73431378,
        "Question_title":"Save images from sagemaker training",
        "Question_body":"<p>I am trying to save images that I configure during training to the output bucket in sagemaker.  I've read that all the information that needs to be saved during training goes into the model.tar.gz file.  I've tried saving plots using the model_dir and the output_data_dir to no avail.  The model itself is saved properly, but the additional information is not being stored with it.  I want to reload this additional information (the saved images) during inference but have heard that storing all the information in the model.tar.gz can cause slow inference.  I would love some help.<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>from sagemaker.pytorch import PyTorch\nestimator = PyTorch(entry_point='XXXXXXXX\/AWS\/mnist.py',\n                    role=role,\n                    py_version='py3',\n                    framework_version='1.8.0',\n                    instance_count=1,\n                    instance_type='ml.c5.xlarge',\n                    output_path='s3:\/\/XXXXX-bucket\/',\n                    )<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code># mnist.py\nimport os\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nimport argparse\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import ToTensor\nfrom torchvision.io import read_image\nfrom torch import nn\nimport matplotlib.pyplot as plt\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\n\ndef train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X.to(device))\n        loss = loss_fn(pred, y.to(device))\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}\/{size:&gt;5d}]\")\n\n\ndef test_loop(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X.to(device))\n            test_loss += loss_fn(pred, y.to(device)).item()\n            correct += (pred.argmax(1) == y.to(device)).type(torch.float).sum().item()\n\n    test_loss \/= num_batches\n    correct \/= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")\n\n# Initialize the loss function\nif __name__=='__main__':\n    # default to the value in environment variable `SM_MODEL_DIR`. Using args makes the script more portable.\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n\n    args, _ = parser.parse_known_args()\n\n    training_data = datasets.FashionMNIST(\n        root=\"data\",\n        train=True,\n        download=True,\n        transform=ToTensor()\n    )\n\n    test_data = datasets.FashionMNIST(\n        root=\"data\",\n        train=False,\n        download=True,\n        transform=ToTensor()\n    )\n\n    labels_map = {\n        0: \"T-Shirt\",\n        1: \"Trouser\",\n        2: \"Pullover\",\n        3: \"Dress\",\n        4: \"Coat\",\n        5: \"Sandal\",\n        6: \"Shirt\",\n        7: \"Sneaker\",\n        8: \"Bag\",\n        9: \"Ankle Boot\",\n    }\n\n    figure = plt.figure(figsize=(8, 8))\n    cols, rows = 3, 3\n    for i in range(1, cols * rows + 1):\n        sample_idx = torch.randint(len(training_data), size=(1,)).item()\n        img, label = training_data[sample_idx]\n        figure.add_subplot(rows, cols, i)\n        plt.title(labels_map[label])\n        plt.axis(\"off\")\n        plt.imsave(args.output_data_dir+'plot'+str(i)+'.jpg', img.squeeze(), cmap=\"gray\")\n\n    train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n    test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n\n    # Display image and label.\n    train_features, train_labels = next(iter(train_dataloader))\n    print(f\"Feature batch shape: {train_features.size()}\")\n    print(f\"Labels batch shape: {train_labels.size()}\")\n    img = train_features[0].squeeze()\n    label = train_labels[0]\n    plt.imsave(args.output_data_dir+'sample.jpg', img, cmap=\"gray\")\n    print(\"Saved img.\")\n    print(f\"Label: {label}\")\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using {device} device\")\n\n    model = NeuralNetwork().to(device)\n    print(model)\n\n    learning_rate = 1e-3\n    batch_size = 64\n    epochs = 5\n    # ... train `model`, then save it to `model_dir`\n    \n\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n    epochs = 1\n    for t in range(epochs):\n        print(f\"Epoch {t+1}\\n-------------------------------\")\n        train_loop(train_dataloader, model, loss_fn, optimizer)\n        test_loop(test_dataloader, model, loss_fn)\n    print(\"Done!\")\n\n    \n\n\n\n    with open(os.path.join(args.model_dir, 'model.pth'), 'wb') as f:\n        torch.save(model.state_dict(), f)\n        plt.plot([1,2,3,4])\n        plt.ylabel('some numbers')\n        plt.show()\n        plt.savefig('test.jpeg')<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-21 03:06:56.633 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-08-21 03:11:26.73 UTC",
        "Question_score":0,
        "Question_tags":"amazon-s3|neural-network|amazon-sagemaker",
        "Question_view_count":43,
        "Owner_creation_date":"2017-11-13 18:41:05.377 UTC",
        "Owner_last_access_date":"2022-09-22 01:02:04.68 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>I suspect there is an issue with string concatenation in <code>plt.imsave<\/code> because the environment variable <code>SM_OUTPUT_DATA_DIR<\/code> by default points to <code>\/opt\/ml\/output\/data<\/code> (that's the actual value of <code>args.output_data_dir<\/code>, since you don't pass this parameter) so the outcome is something like <code>\/opt\/ml\/output\/dataplot1.jpg<\/code>. The same happen if you use the <code>model_dir<\/code> in the same way. I'd rather use something like <code>os.path.join<\/code> like you're already doing for the model. <a href=\"https:\/\/nono.ma\/sagemaker-model-dir-output-dir-and-output-data-dir-parameters\" rel=\"nofollow noreferrer\">here<\/a> a nice exaplaination about these folders and environment variables in sagemaker.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-08-22 10:44:12.743 UTC",
        "Answer_score":0.0,
        "Owner_location":"San Francisco, CA, United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73431378",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63500377,
        "Question_title":"memory issues for sparse one hot encoded features",
        "Question_body":"<p>I want to create sparse matrix for one hot encoded features from data frame <code>df<\/code>. But I am getting memory issue for code given below. Shape of <code>sparse_onehot<\/code> is  (450138, 1508)<\/p>\n<pre><code>sp_features = ['id', 'video_id', 'genre']\nsparse_onehot = pd.get_dummies(df[sp_features], columns = sp_features)\nimport scipy\nX = scipy.sparse.csr_matrix(sparse_onehot.values)\n<\/code><\/pre>\n<p>I get memory error as shown below.<\/p>\n<pre><code>MemoryError: Unable to allocate 647. MiB for an array with shape (1508, 450138) and data type uint8\n<\/code><\/pre>\n<p>I have tried <code>scipy.sparse.lil_matrix<\/code> and get same error as above.<\/p>\n<p>Is there any efficient way of handling this?\nThanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2020-08-20 07:35:11.857 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-08-20 08:15:20.857 UTC",
        "Question_score":2,
        "Question_tags":"python-3.x|pandas|scipy|sparse-matrix|amazon-sagemaker",
        "Question_view_count":97,
        "Owner_creation_date":"2017-02-15 05:16:01.367 UTC",
        "Owner_last_access_date":"2022-09-23 14:58:48.51 UTC",
        "Owner_reputation":911,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":91,
        "Answer_body":"<p>Try setting to <code>True<\/code> the <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html\" rel=\"nofollow noreferrer\"><code>sparse<\/code> parameter<\/a>:<\/p>\n<blockquote>\n<p>sparsebool, default False\nWhether the dummy-encoded columns should be backed by a SparseArray (True) or a regular NumPy array (False).<\/p>\n<\/blockquote>\n<pre><code>sparse_onehot = pd.get_dummies(df[sp_features], columns = sp_features, sparse = True)\n<\/code><\/pre>\n<p>This will use a much more memory efficient (but somewhat slower) representation than the default one.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-08-20 08:18:59.563 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63500377",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54521080,
        "Question_title":"aws sagemaker for detecting text in an image",
        "Question_body":"<p>I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.<\/p>\n\n<p>I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-02-04 17:11:02.23 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":2255,
        "Owner_creation_date":"2014-10-03 01:47:05.55 UTC",
        "Owner_last_access_date":"2022-09-23 18:38:38.85 UTC",
        "Owner_reputation":463,
        "Owner_up_votes":50,
        "Owner_down_votes":0,
        "Owner_views":111,
        "Answer_body":"<p>The different services will all provide different levels of abstraction for Optical Character Recognition (OCR) depending on what parts of the pipeline you are most comfortable with working with, and what you prefer to have abstracted.<\/p>\n\n<p>Here are a few options:<\/p>\n\n<ul>\n<li><p><strong>Rekognition<\/strong> will provide out of the box OCR with the <a href=\"https:\/\/docs.aws.amazon.com\/rekognition\/latest\/dg\/text-detecting-text-procedure.html\" rel=\"nofollow noreferrer\">DetectText<\/a> feature. However, it seems you will need to perform some sort of pre-processing on your images in your current case in order to get better results. This can be done through any method of your choice (Lambda, EC2, etc).<\/p><\/li>\n<li><p><strong>SageMaker<\/strong> is a tool that will enable you to easily train and deploy your own models (of any type). You have two primary options with SageMaker:<\/p>\n\n<ol>\n<li><p><em>Do-it-yourself option<\/em>: If you're looking to go the route of labeling your own data, gathering a sizable training set, and training your own OCR model, this is possible by training and deploying your own model via SageMaker.<\/p><\/li>\n<li><p><em>Existing OCR algorithm<\/em>: There are many algorithms out there that all have different potential tradeoffs for OCR. One example would be <a href=\"https:\/\/github.com\/tesseract-ocr\/tesseract\" rel=\"nofollow noreferrer\">Tesseract<\/a>. Using this, you can more closely couple your pre-processing step to the text detection.<\/p><\/li>\n<\/ol><\/li>\n<li><p><a href=\"https:\/\/aws.amazon.com\/textract\/\" rel=\"nofollow noreferrer\"><strong>Amazon Textract<\/strong><\/a> (In preview) is a purpose-built dedicated OCR service that may offer better performance depending on what your images look like and the settings you choose. <\/p><\/li>\n<\/ul>\n\n<p>I would personally recommend looking into <a href=\"https:\/\/docparser.com\/blog\/improve-ocr-accuracy\/\" rel=\"nofollow noreferrer\">pre-processing for OCR<\/a> to see if it improves Rekognition accuracy before moving onto the other options. Even if it doesn't improve Rekognition's accuracy, it will still be valuable for most of the other options!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-02-04 20:46:42.943 UTC",
        "Answer_score":4.0,
        "Owner_location":"West Lafayette, IN, United States",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54521080",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71579883,
        "Question_title":"Missing -symbol.json error when trying to compile a SageMaker semantic segmentation model (built-in algorithm) with SageMaker Neo",
        "Question_body":"<p>I have trained a SageMaker semantic segmentation model, using the built-in sagemaker semantic segmentation algorithm. This deploys ok to a SageMaker endpoint and I can run inference in the cloud  successfully from it.\nI would like to use the model on a edge device (AWS Panorama Appliance) which should just mean compiling the model with SageMaker Neo to the specifications of the target device.<\/p>\n<p>However, regardless of what my target device is (the Neo settings), I cant seem to compile the model with Neo as I get the following error:<\/p>\n<pre><code>ClientError: InputConfiguration: No valid Mxnet model file -symbol.json found\n<\/code><\/pre>\n<p>The model.tar.gz for semantic segmentation models contains hyperparams.json, model_algo-1, model_best.params. According to the docs, model_algo-1 is the serialized mxnet model. Aren't gluon models supported by Neo?<\/p>\n<p>Incidentally I encountered the exact same problem with another SageMaker built-in algorithm, the k-Nearest Neighbour (k-NN). It too seems to be compiled without a -symbol.json.<\/p>\n<p>Is there some scripts I can run to recreated a -symbol.json file or convert the compiled sagemaker model?<\/p>\n<p>After building my model with an Estimator, I got to compile it in SageMaker Neo with code:<\/p>\n<pre><code>optimized_ic = my_estimator.compile_model(\n target_instance_family=&quot;ml_c5&quot;,\n target_platform_os=&quot;LINUX&quot;,\n target_platform_arch=&quot;ARM64&quot;,\n input_shape={&quot;data&quot;: [1,3,512,512]},  \n output_path=s3_optimized_output_location,\n framework=&quot;mxnet&quot;,\n framework_version=&quot;1.8&quot;, \n)\n<\/code><\/pre>\n<p>I would expect this to compile ok, but that is where I get the error saying the model is missing the *-symbol.json file.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-22 22:52:55.803 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-03-23 07:35:44.223 UTC",
        "Question_score":0,
        "Question_tags":"machine-learning|amazon-sagemaker|semantic-segmentation",
        "Question_view_count":52,
        "Owner_creation_date":"2020-04-15 05:33:39.953 UTC",
        "Owner_last_access_date":"2022-09-15 04:03:44.717 UTC",
        "Owner_reputation":25,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>For some reason, AWS has decided to not make its built-in algorithms directly compatible with Neo... However, you can re-engineer the network parameters using the model.tar.gz output file and then compile.<\/p>\n<p>Step 1: Extract model from tar file<\/p>\n<pre><code>import tarfile\n#path to local tar file\nmodel = 'ss_model.tar.gz'\n\n#extract tar file \nt = tarfile.open(model, 'r:gz')\nt.extractall()\n<\/code><\/pre>\n<p>This should output two files:\nmodel_algo-1, model_best.params<\/p>\n<ol start=\"2\">\n<li>Load weights into network from gluon model zoo for the architecture that you chose<\/li>\n<\/ol>\n<p>In this case I used DeepLabv3 with resnet50<\/p>\n<pre><code>import gluoncv\nimport mxnet as mx\nfrom gluoncv import model_zoo\nfrom gluoncv.data.transforms.presets.segmentation import test_transform\n\nmodel = model_zoo.DeepLabV3(nclass=2, backbone='resnet50', pretrained_base=False, height=800, width=1280, crop_size=240)\nmodel.load_parameters(&quot;model_algo-1&quot;)\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Check the parameters have loaded correctly by making a prediction with new model<\/li>\n<\/ol>\n<p>Use an image that was used for training.<\/p>\n<pre><code>#use cpu\nctx = mx.cpu(0)\n#decode image bytes of loaded file\nimg = image.imdecode(imbytes)\n\n#transform image\nimg = test_transform(img, ctx)\nimg = img.astype('float32')\nprint('tranformed image shape: ', img.shape)\n\n#get prediction\noutput = model.predict(img)\n<\/code><\/pre>\n<ol start=\"4\">\n<li>Hybridise model into output required by Sagemaker Neo<\/li>\n<\/ol>\n<p>Additional check for image shape compatibility<\/p>\n<pre><code>model.hybridize()\nmodel(mx.nd.ones((1,3,800,1280)))\nexport_block('deeplabv3-res50', model, data_shape=(3,800,1280), preprocess=None, layout='CHW')\n<\/code><\/pre>\n<ol start=\"5\">\n<li>Recompile model into tar.gz format<\/li>\n<\/ol>\n<p>This contains the params and json file which Neo looks for.<\/p>\n<pre><code>tar = tarfile.open(&quot;comp_model.tar.gz&quot;, &quot;w:gz&quot;)\nfor name in [&quot;deeplabv3-res50-0000.params&quot;, &quot;deeplabv3-res50-symbol.json&quot;]:\n    tar.add(name)\ntar.close()\n<\/code><\/pre>\n<ol start=\"6\">\n<li>Save tar.gz file to s3 and then compile using Neo GUI<\/li>\n<\/ol>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-03-23 12:23:37.5 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71579883",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61994821,
        "Question_title":"Cannot execute AWS Sagemaker Notebook",
        "Question_body":"<p>I cannot execute sagemaker notebook anymore.<br>\nThe following error occurs.<\/p>\n\n<pre><code>Failed to start kernel\nAn error occurred (ThrottlingException) when calling the CreateApp operation (reached max retries: 4): \nRate exceeded\n<\/code><\/pre>\n\n<p>I checked my app list and there are only two.\nOne app is trying to delete but never stops, this could be one of the problem.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/M0iqo.png\" rel=\"nofollow noreferrer\">image<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-05-25 02:20:05.72 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":965,
        "Owner_creation_date":"2018-07-24 08:52:28.877 UTC",
        "Owner_last_access_date":"2020-11-27 01:51:12.017 UTC",
        "Owner_reputation":27,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>Happened to me too. Contact support and ask them to delete the kernel behind the scenes.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-05-27 07:18:11.21 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61994821",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59762829,
        "Question_title":"AWS Sagemaker scikit_bring_your_own example",
        "Question_body":"<p>I am following the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own%20example\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own example<\/a> for product recommendations.<\/p>\n\n<p>I want to use the SVD from <a href=\"https:\/\/pypi.org\/project\/scikit-surprise\/\" rel=\"nofollow noreferrer\">scikit-surprise<\/a> library on Sagemaker.<\/p>\n\n<pre><code>from surprise import SVD\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\n<\/code><\/pre>\n\n<p>I added the scikit-surprise package in the Dockerfile, but i am getting the following errors:<\/p>\n\n<h1>Dockerfile:<\/h1>\n\n<pre><code># Build an image that can do training and inference in SageMaker\n# This is a Python 2 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:16.04\n\nMAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Here we get all python packages.\n# There's substantial overlap between scipy and numpy that we eliminate by\n# linking them together. Likewise, pip leaves the install caches populated which uses\n# a significant amount of space. These optimizations save a fair amount of space in the\n# image, which reduces start up time.\nRUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp; \\\n    pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp; \\\n        (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp; \\\n        rm -rf \/root\/.cache\n\nRUN pip install scikit-surprise\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=\"\/opt\/program:${PATH}\"\n\n# Set up the program in the image\nCOPY products_recommender \/opt\/program\nWORKDIR \/opt\/program\n<\/code><\/pre>\n\n<h1>Docker build and deploy :<\/h1>\n\n<pre><code>fullname:XXXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender:latest\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\nSending build context to Docker daemon  67.58kB\nStep 1\/10 : FROM ubuntu:16.04\n ---&gt; 13c9f1285025\nStep 2\/10 : MAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n ---&gt; Using cache\n ---&gt; 44baf3286201\nStep 3\/10 : RUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends          wget          python          nginx          ca-certificates     &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n ---&gt; Using cache\n ---&gt; 8983fa906515\nStep 4\/10 : RUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp;     pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp;         (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp;         rm -rf \/root\/.cache\n ---&gt; Using cache\n ---&gt; 9dbfedf02b57\nStep 5\/10 : RUN pip install scikit-surprise\n ---&gt; Running in 82295cb0affe\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\nCollecting scikit-surprise\n  Downloading https:\/\/files.pythonhosted.org\/packages\/f5\/da\/b5700d96495fb4f092be497f02492768a3d96a3f4fa2ae7dea46d4081cfa\/scikit-surprise-1.1.0.tar.gz (6.4MB)\nCollecting joblib&gt;=0.11 (from scikit-surprise)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/28\/5c\/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf\/joblib-0.14.1-py2.py3-none-any.whl (294kB)\nRequirement already satisfied: numpy&gt;=1.11.2 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.16.2)\nRequirement already satisfied: scipy&gt;=1.0.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.2.1)\nRequirement already satisfied: six&gt;=1.10.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.12.0)\nBuilding wheels for collected packages: scikit-surprise\n  Building wheel for scikit-surprise (setup.py): started\n  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n  ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-Bb1_iT --python-tag cp27:\n  ERROR: running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build\/lib.linux-x86_64-2.7\n  creating build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running egg_info\n  writing requirements to scikit_surprise.egg-info\/requires.txt\n  writing scikit_surprise.egg-info\/PKG-INFO\n  writing top-level names to scikit_surprise.egg-info\/top_level.txt\n  writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n  writing entry points to scikit_surprise.egg-info\/entry_points.txt\n  reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  reading manifest template 'MANIFEST.in'\n  writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running build_ext\n  building 'surprise.similarities' extension\n  creating build\/temp.linux-x86_64-2.7\n  creating build\/temp.linux-x86_64-2.7\/surprise\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n  ----------------------------------------\n  ERROR: Failed building wheel for scikit-surprise\n  Running setup.py clean for scikit-surprise\nFailed to build scikit-surprise\nInstalling collected packages: joblib, scikit-surprise\n  Running setup.py install for scikit-surprise: started\n    Running setup.py install for scikit-surprise: finished with status 'error'\n    ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile:\n    ERROR: running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib.linux-x86_64-2.7\n    creating build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running egg_info\n    writing requirements to scikit_surprise.egg-info\/requires.txt\n    writing scikit_surprise.egg-info\/PKG-INFO\n    writing top-level names to scikit_surprise.egg-info\/top_level.txt\n    writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n    writing entry points to scikit_surprise.egg-info\/entry_points.txt\n    reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    reading manifest template 'MANIFEST.in'\n    writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running build_ext\n    building 'surprise.similarities' extension\n    creating build\/temp.linux-x86_64-2.7\n    creating build\/temp.linux-x86_64-2.7\/surprise\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n    unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n    ----------------------------------------\nERROR: Command \"\/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in \/tmp\/pip-install-VsuzGr\/scikit-surprise\/\nWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nThe command '\/bin\/sh -c pip install scikit-surprise' returned a non-zero code: 1\nThe push refers to repository [XXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender]\n89c1adca7d35: Layer already exists \nddcb6879486f: Layer already exists \n4a02efecad74: Layer already exists \n92d3f22d44f3: Layer already exists \n10e46f329a25: Layer already exists \n24ab7de5faec: Layer already exists \n1ea5a27b0484: Layer already exists \nlatest: digest: sha256:5ed35f1964d10f13bc8a05d379913c24195ea31ec848157016381fbd1bb12f28 size: 1782\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-01-16 03:59:10.087 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"scikit-learn|amazon-sagemaker|svd",
        "Question_view_count":341,
        "Owner_creation_date":"2009-04-29 11:42:36.853 UTC",
        "Owner_last_access_date":"2022-09-18 14:23:33.13 UTC",
        "Owner_reputation":15794,
        "Owner_up_votes":1343,
        "Owner_down_votes":26,
        "Owner_views":1032,
        "Answer_body":"<p>The 'x86_64-linux-gnu-gcc' binary can't be found in environment where you're building the container. Make sure that gcc is installed, and that you use the right name (gcc?).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-01-17 15:03:32.89 UTC",
        "Answer_score":1.0,
        "Owner_location":"Kuala Lumpur, Malaysia",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59762829",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66142193,
        "Question_title":"AWS Sagemaker training job stuck in progress state",
        "Question_body":"<p>I have created a training job yesterday, same as usual, just adding few more training data. I didn't have any problem with this in the last 2 years (the same exact procedure and code). This time after 14 hours more or less simply stalled.\nTraining job is still &quot;in processing&quot;, but cloudwatch is not logging anything since then. Right now 8 more hours passed and no new entry is in the logs, no errors no crash.\nCan someone explain this ? Unfortunately I don't have any AWS support plan.\nAs you can see from the picture below after 11am there is nothing..<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hswD7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hswD7.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The training job is supposed to complete in the next couple of hours, but now I'm not sure if is actually running (in this case would be a cloudwatch problem) or not..<\/p>\n<p><strong>UPDATE<\/strong><\/p>\n<p>Suddenly the training job failed, without any further log. The reason is<\/p>\n<blockquote>\n<p>ClientError: Artifact upload failed:Error 7: The credentials received\nhave been expired<\/p>\n<\/blockquote>\n<p>But there is still nothing in the logs after 11am. Very weird.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2021-02-10 17:44:33.253 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-02-11 08:10:17.147 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":755,
        "Owner_creation_date":"2014-11-18 21:32:30.293 UTC",
        "Owner_last_access_date":"2022-09-24 17:06:59.437 UTC",
        "Owner_reputation":2302,
        "Owner_up_votes":51,
        "Owner_down_votes":4,
        "Owner_views":227,
        "Answer_body":"<p>For future readers I can confirm that is something that can happen very rarely (I' haven't experienced it anymore since then), but it's AWS fault. Same data, same algorithm.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-03-25 15:04:37.673 UTC",
        "Answer_score":1.0,
        "Owner_location":"Jesi, Italy",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66142193",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68396088,
        "Question_title":"Why AWS Lambda Internel Server Error 500 but successfully \/invocations POST 200 in Endpoint SageMaker?",
        "Question_body":"<pre><code>import os\nimport io\nimport boto3\nimport json\nimport csv\n\n\n# grab environment variables\nENDPOINT_NAME = os.environ['ENDPOINT_NAME']\n# grab runtime client\nruntime = boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    # Load data from POST request\n    data = json.loads(json.dumps(event))\n    \n    # Grab the payload\n    payload = data['body']\n    \n    # Invoke the model. In this case the data type is a JSON but can be other things such as a CSV\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                   ContentType='application\/json',\n                                   Body=payload)\n    \n    # Get the body of the response from the model\n    result = response['Body'].read().decode()\n\n    # Return it along with the status code of 200 meaning this was succesful \n    return {\n        'statusCode': 200,\n        'body': result\n    }\n<\/code><\/pre>\n<p><strong>response from AWS Lambda<\/strong><\/p>\n<pre><code>{\n  &quot;errorMessage&quot;: &quot;'body'&quot;,\n  &quot;errorType&quot;: &quot;KeyError&quot;,\n  &quot;stackTrace&quot;: [\n    [\n      &quot;\/var\/task\/lambda_function.py&quot;,\n      18,\n      &quot;lambda_handler&quot;,\n      &quot;payload = data['body']&quot;\n    ]\n  ]\n}\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/h8wvA.png\" rel=\"nofollow noreferrer\">response from Postman 500 Internal Server Error<\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/cuknX.png\" rel=\"nofollow noreferrer\">but successfully invoke POST 200 in SageMaker Endpoint<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-07-15 14:52:02.473 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-07-15 14:53:47.357 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":277,
        "Owner_creation_date":"2017-12-10 19:22:19.527 UTC",
        "Owner_last_access_date":"2022-01-24 09:53:07.33 UTC",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>The issue is when you are trying to parse your payload with data['body']. The data is not being passed in the format that the endpoint is expecting. Use the following code snippet to properly format\/serialize your data for the endpoint. Also to make all this clearer make sure to check for your payload type to make sure you have not serialized again by accident.<\/p>\n<pre><code>    data = json.loads(json.dumps(event))\n    payload = json.dumps(data)\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                       ContentType='application\/json',\n                                       Body=payload)\n    result = json.loads(response['Body'].read().decode())\n<\/code><\/pre>\n<p>I work for AWS &amp; my opinions are my own<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-07-22 17:45:23.23 UTC",
        "Answer_score":0.0,
        "Owner_location":"Petaling Jaya, Selangor, Malaysia",
        "Answer_last_edit_date":"2021-07-22 18:43:16.583 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68396088",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70565147,
        "Question_title":"AWS sagemaker datawrangler continues to be used after closing everything",
        "Question_body":"<p>I used data wrangler for maybe 3h a week ago, and I open my account today to see that Ive been charged for 6 days worth of data wrangler usage. Basically it was running in the background the whole time. The first 25h were part of free tier then I got charged for the rest of the time. I dont have any endpoints to close so whats the issue? I dont care about the costs, I know I can talk to support to get the charges reversed but they dont seem to know whats going on because they havent helped me at all.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-03 11:32:34.07 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":36,
        "Owner_creation_date":"2018-08-27 15:07:00.717 UTC",
        "Owner_last_access_date":"2022-03-20 12:33:05.177 UTC",
        "Owner_reputation":41,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":"<p>After going over <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-wrangler-shut-down.html\" rel=\"nofollow noreferrer\">the docs<\/a>, I found that I needed to shut down the wrangler instance under Running Instances and Kernels button.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-03 13:27:57.46 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2022-01-03 13:29:47.633 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70565147",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71738894,
        "Question_title":"Unable to Create AWS Segamaker, Error: The account-level service limit 'Number of elastic inference accelerators across all notebook instances.'",
        "Question_body":"<p>I'm facing an Error Message <br> &quot;The account-level service limit 'Number of elastic inference accelerators across all notebook instances.' is 0 Accelerators, with current utilization of 0 Accelerators and a request delta of 1 Accelerators. Please contact AWS support to request an increase for this limit.&quot;\n<a href=\"https:\/\/i.stack.imgur.com\/KWvQb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KWvQb.png\" alt=\"enter image description here\" \/><\/a>\n<br>\nI never used this account, I have 500$ in my account. But I'm unable to create the Juypter NoteBook on Aws SageMaker Instances.<\/p>\n<p>I already visited: <a href=\"https:\/\/stackoverflow.com\/questions\/53595157\/aws-sagemaker-deploy-fails\">AWS Sagemaker Deploy fails<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-04-04 14:22:15.41 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|instance|amazon-sagemaker",
        "Question_view_count":109,
        "Owner_creation_date":"2019-10-18 13:36:31.94 UTC",
        "Owner_last_access_date":"2022-09-24 18:46:24.74 UTC",
        "Owner_reputation":57,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Answer_body":"<p>By default AWS limits the number of instances you can use, here you have the default <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html\" rel=\"nofollow noreferrer\">limits<\/a>. As the error message says, you have to request for a limit increase, you can do it from <a href=\"https:\/\/us-east-1.console.aws.amazon.com\/support\/home?region=us-east-1&amp;skipRegion=true#\/case\/create\" rel=\"nofollow noreferrer\">here<\/a>, it will take couple of days from my experience.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-04-05 07:47:03.327 UTC",
        "Answer_score":1.0,
        "Owner_location":"UK",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71738894",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55621967,
        "Question_title":"Feature Importance for XGBoost in Sagemaker",
        "Question_body":"<p>I have built an XGBoost model using Amazon Sagemaker, but I was unable to find anything which will help me interpret the model and validate if it has learned the right dependencies.<\/p>\n\n<p>Generally, we can see Feature Importance for XGBoost by get_fscore() function in the python API (<a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html<\/a>) I see nothing of that sort in the sagemaker api(<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html<\/a>).<\/p>\n\n<p>I know I can build my own model and then deploy that using sagemaker but I am curious if anyone has faced this problem and how they overcame it.<\/p>\n\n<p>Thanks.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2019-04-10 22:06:42.317 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"xgboost|amazon-sagemaker",
        "Question_view_count":3637,
        "Owner_creation_date":"2014-12-23 09:45:25.267 UTC",
        "Owner_last_access_date":"2022-09-22 22:39:33.93 UTC",
        "Owner_reputation":123,
        "Owner_up_votes":35,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":"<p>SageMaker XGBoost currently does not provide interface to retrieve feature importance from the model. You can write some code to get the feature importance from the XGBoost model. You have to get the booster object artifacts from the model in S3 and then use the following snippet <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pickle as pkl\nimport xgboost\nbooster = pkl.load(open(model_file, 'rb'))\nbooster.get_score()\nbooster.get_fscore()\n<\/code><\/pre>\n\n<p>Refer <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\" rel=\"nofollow noreferrer\">XGBoost doc<\/a> for methods to get feature importance from the Booster object such as <code>get_score()<\/code> or <code>get_fscore()<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-04-11 16:54:55.857 UTC",
        "Answer_score":3.0,
        "Owner_location":"Mountain View, CA, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55621967",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65902366,
        "Question_title":"AWS Sagemaker - ClientError: Data download failed:Could not download",
        "Question_body":"<p>I encountered and error when I deploy my training job in my notebook instance.\nThis what it says:\n<code>&quot;UnexpectedStatusException: Error for Training job tensorflow-training-2021-01-26-09-55-05-768: Failed. Reason: ClientError: Data download failed:Could not download s3:\/\/forex-model-data\/data\/train2001_2020.npz: insufficient disk space&quot;<\/code><\/p>\n<p>I deploy training job to try running it to different instances in 3 epoch. I use ml.c5.4xlarge, ml.c5.18xlarge, ml.m5.24xlarge, also I have two sets of training data, train2001_2020.npz and train2016_2020.npz.<\/p>\n<p>First, I run train2001_2020 to ml.c5.18xlarge and ml.c5.18xlarge and the training job completed, then I switch to train2016_2020 and run it to ml.c5.4xlarge and ml.c5.18xlarge and it goes well. Then when I tried to run it using ml.m5.24xlarge I got an error (quoted above), but my dataset is train2016_2020 not train2001_2020 then when I rerun it again with all other instances it has the same error. What happen?<\/p>\n<p>I stopped the instances and refresh everything, but I encountered same issue.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-26 13:34:39.743 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":774,
        "Owner_creation_date":"2019-10-02 15:22:33.88 UTC",
        "Owner_last_access_date":"2021-06-07 03:53:56.07 UTC",
        "Owner_reputation":98,
        "Owner_up_votes":41,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>It's not really clear to all the test are you doing, but that error usually means that there is not enough disk space on the instance you are using for the training job. You can try to increase the additional storage for the instance (you can do in the estimator parameters if you are using the sagemaker SDK in a notebook).<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-01-28 10:54:28.823 UTC",
        "Answer_score":1.0,
        "Owner_location":"Philippines",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65902366",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64703268,
        "Question_title":"How to use AWS SageMaker and S3 for Object Detection?",
        "Question_body":"<p>I am looking to run a pre-trained object detection model onto a folder of ~400k images which is about 1.5GB. When I've tried running locally, it was estimated to take ~8 days to complete (with keras yolov3). Thus, I am looking to use AWS SageMaker and S3.<\/p>\n<p>When I have uploaded the zip folder of my images in the SageMaker jupyter notebook and tried to unzip by using bash command, an error pops ups saying that I have insufficient space. The volume assigned to my notebook is 5GB EBS, I do have other heavy datasets in my jupyter notebook space which could be causing this issue.<\/p>\n<p>To tackle that, I am looking for a way where I can upload my data to S3 and run SageMaker to read the images hosted and run an object detection model over. However, it does not look like there's a method to unzip folders on S3 without using an additional service (read that AWS Lambda may help) as these services are paid by my school.<\/p>\n<p>I could possibly re-run my code to extract my images from URL. In this case, how can I save these images to S3 directly in this case? Also, does anyone know if I am able to run yolov3 on SageMaker or if there is a better model I can look to use. Appreciate any advice that may help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2020-11-05 18:28:16.053 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|object-detection|amazon-sagemaker",
        "Question_view_count":256,
        "Owner_creation_date":"2017-10-13 07:37:10.153 UTC",
        "Owner_last_access_date":"2022-09-20 08:00:20.06 UTC",
        "Owner_reputation":180,
        "Owner_up_votes":268,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Answer_body":"<p>yes u are right u can upload thousands of images using aws cli using $aws s3 cp  ; or $aws s3 sync<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-11-06 02:23:45.8 UTC",
        "Answer_score":0.0,
        "Owner_location":"Singapore",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64703268",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73829280,
        "Question_title":"NVIDIA Triton vs TorchServe for SageMaker Inference",
        "Question_body":"<p><a href=\"https:\/\/developer.nvidia.com\/nvidia-triton-inference-server\" rel=\"nofollow noreferrer\">NVIDIA Triton<\/a>\u00a0vs\u00a0<a href=\"https:\/\/pytorch.org\/serve\/\" rel=\"nofollow noreferrer\">TorchServe<\/a>\u00a0for SageMaker inference? When to recommend each?<\/p>\n<p>Both are modern, production grade inference servers. TorchServe is the DLC default inference server for PyTorch models. Triton is also supported for PyTorch inference on SageMaker.<\/p>\n<p>Anyone has a good comparison matrix for both?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-23 14:28:58.403 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"pytorch|amazon-sagemaker|inference|torchserve|tritonserver",
        "Question_view_count":12,
        "Owner_creation_date":"2014-01-16 15:43:59.673 UTC",
        "Owner_last_access_date":"2022-09-25 03:22:08.463 UTC",
        "Owner_reputation":5854,
        "Owner_up_votes":155,
        "Owner_down_votes":70,
        "Owner_views":794,
        "Answer_body":"<p>Important notes to add here where both serving stacks differ:<\/p>\n<p>TorchServe does not provide the Instance Groups feature that Triton does (that is, stacking many copies of the same model or even different models onto the same GPU). This is a major advantage for both realtime and batch use-cases, as the performance increase is almost proportional to the model replication count (i.e. 2 copies of the model get you almost twice the throughput and half the latency; check out a BERT benchmark of this here). Hard to match a feature that is almost like having 2+ GPU's for the price of one.\nif you are deploying PyTorch DL models, odds are you often want to accelerate them with GPU's. TensorRT (TRT) is a compiler developed by NVIDIA that automatically quantizes and optimizes your model graph, which represents another huge speed up, depending on GPU architecture and model. It is understandably so probably the best way of automatically optimizing your model to run efficiently on GPU's and make good use of TensorCores. Triton has native integration to run TensorRT engines as they're called (even automatically converting your model to a TRT engine via config file), while TorchServe does not (even though you can use TRT engines with it).\nThere is more parity between both when it comes to other important serving features: both have dynamic batching support, you can define inference DAG's with both (not sure if the latter works with TorchServe on SageMaker without a big hassle), and both support custom code\/handlers instead of just being able to serve a model's forward function.<\/p>\n<p>Finally, MME on GPU (coming shortly) will be based on Triton, which is a valid argument for customers to get familiar with it so that they can quickly leverage this new feature for cost-optimization.<\/p>\n<p>Bottom line I think that Triton is just as easy (if not easier) ot use, a lot more optimized\/integrated for taking full advantage of the underlying hardware (and will be updated to keep being that way as newer GPU architectures are released, enabling an easy move to them), and in general blows TorchServe out of the water performance-wise when its optimization features are used in combination.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-23 22:14:00.67 UTC",
        "Answer_score":0.0,
        "Owner_location":"Singapore",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73829280",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71221741,
        "Question_title":"ValidationException in Sagemaker pipeline creation",
        "Question_body":"<p>I am new to Sagmaker. I am creating a pipeline in sagemaker where I initialize the number of epochs as a pipeline parameter. But when I upsert, it shows this error.\nCheck the following code for reference, please.<\/p>\n<pre><code>epoch_count = ParameterInteger(name=&quot;EpochCount&quot;, default_value=1)\npipeline = Pipeline(\nname=f&quot;a_name&quot;,\nparameters=[\n    training_instance_type,\n    training_instance_count,\n    epoch_count,\n    hugging_face_model_name,\n    endpoint_instance_type,\n    endpoint_instance_type_alternate,\n],\nsteps=[step_train, step_register, step_deploy_lambda],\nsagemaker_session=sagemaker_session,\n<\/code><\/pre>\n<p>)<\/p>\n<p>Error - ---<\/p>\n<pre><code>---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\n&lt;ipython-input-54-138a517611f0&gt; in &lt;module&gt;\n----&gt; 1 pipeline.upsert(role_arn=role)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in upsert(self, role_arn, description, tags, parallelism_config)\n    217         &quot;&quot;&quot;\n    218         try:\n--&gt; 219             response = self.create(role_arn, description, tags, parallelism_config)\n    220         except ClientError as e:\n    221             error = e.response[&quot;Error&quot;]\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in create(self, role_arn, description, tags, parallelism_config)\n    119             Tags=tags,\n    120         )\n--&gt; 121         return self.sagemaker_session.sagemaker_client.create_pipeline(**kwargs)\n    122 \n    123     def _create_args(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    389                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    390             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 391             return self._make_api_call(operation_name, kwargs)\n    392 \n    393         _api_call.__name__ = str(py_operation_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    717             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)\n    718             error_class = self.exceptions.from_code(error_code)\n--&gt; 719             raise error_class(parsed_response, operation_name)\n    720         else:\n    721             return parsed_response\n\nClientError: An error occurred (ValidationException) when calling the CreatePipeline operation: Cannot assign property reference [Parameters.EpochCount] to argument of type [String]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-22 12:57:42.633 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":608,
        "Owner_creation_date":"2013-10-29 05:52:07.423 UTC",
        "Owner_last_access_date":"2022-09-20 11:54:50.763 UTC",
        "Owner_reputation":647,
        "Owner_up_votes":62,
        "Owner_down_votes":3,
        "Owner_views":105,
        "Answer_body":"<p>I replace<\/p>\n<pre><code>epoch_count = ParameterInteger(name=&quot;EpochCount&quot;, default_value=1)\n<\/code><\/pre>\n<p>with<\/p>\n<pre><code>epoch_count = ParameterString(name=&quot;EpochCount&quot;, default_value=&quot;1&quot;)\n<\/code><\/pre>\n<p>And it works. Maybe we can only use an integer in pipeline parameters from the sagemaker notebook. But epoch_count is being used in the docker container, which is not directly something of Sagemaker, and that's my understanding.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-23 18:40:27.683 UTC",
        "Answer_score":0.0,
        "Owner_location":"Dhaka, Bangladesh",
        "Answer_last_edit_date":"2022-04-05 11:24:14.31 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71221741",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71246559,
        "Question_title":"How to verify all the dependencies are installed in sagemaker?",
        "Question_body":"<p>I am creating an sagemaker endpoint and loading a pretrained model from an s3 bucket. the model -&gt; model.tar.gz file has directory structure as documented here, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#model-directory-structure\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#model-directory-structure<\/a><\/p>\n<pre><code>model.tar.gz\/\n|- model.pth\n|- code\/\n  |- inference.py\n  |- requirements.txt  # only for versions 1.3.1 and higher\n<\/code><\/pre>\n<p>I have put few dependencies in requirements.txt, is there a way to verify that all the dependencies were installed correctly?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-24 03:01:36.107 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"serverless|amazon-sagemaker",
        "Question_view_count":107,
        "Owner_creation_date":"2020-05-30 00:10:41.983 UTC",
        "Owner_last_access_date":"2022-09-24 19:51:20.543 UTC",
        "Owner_reputation":525,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":98,
        "Answer_body":"<p>It is not possible to get access or SSH into the machine that's running your deployment. So, one way is to assert the versions of your dependencies in the <code>model_fn<\/code> inside &quot;inference.py&quot; something like below.<\/p>\n<p>if your requirements.txt looks like this:<\/p>\n<pre><code>numpy==1.20.3\npandas==1.3.4\n<\/code><\/pre>\n<p>get the versions and assert them in `model_fn like below:<\/p>\n<pre><code>import os\n\n### your other code ###\n\ndef model_fn(model_dir):\n    # assuming you have numpy and pandas\n    assert os.popen(&quot;python3 -m pip freeze | grep -E 'numpy|pandas'&quot;).read() == 'numpy==1.20.3\\npandas==1.3.4\\n'\n    ### your other code ###\n    return xxxx\n\n### your other code ###\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-24 09:25:18.683 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2022-02-24 09:37:54.837 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71246559",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61380051,
        "Question_title":"Sagemaker usage of EC2 instances",
        "Question_body":"<p>Is there a way to view\/monitor AWS Sagemaker's usage of EC2 instances?\nI am running a Sagemaker endpoint and tried to find its instances (ml.p3.2xlarge in this case) in the EC2 UI, but couldn't find them. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2020-04-23 05:37:43.5 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":5,
        "Question_tags":"amazon-ec2|amazon-sagemaker",
        "Question_view_count":615,
        "Owner_creation_date":"2016-03-21 08:49:39.92 UTC",
        "Owner_last_access_date":"2022-07-17 11:53:14.84 UTC",
        "Owner_reputation":383,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Answer_body":"<p>ml EC2 instances do not appear in the EC2 console. You can find their metrics in Cloudwatch though, and create dashboards to monitor what you need:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/BgUkm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BgUkm.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/wD4w0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wD4w0.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-05-04 07:02:18.65 UTC",
        "Answer_score":3.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61380051",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63775893,
        "Question_title":"How to get an Amazon ECR container URI for a specific model image in Sagemaker?",
        "Question_body":"<p>I want to know if it's possible to get an Amazon ECR container URI for a specific image programmatically (using AWS CLI or Python). For example, if I need the URL for the latest <code>linear-learner<\/code> (built-in model) image for the <code>eu-central-1<\/code> region.<\/p>\n<p>Expected result:<\/p>\n<pre><code>664544806723.dkr.ecr.eu-central-1.amazonaws.com\/linear-learner:latest\n<\/code><\/pre>\n<p>EDIT: I have found the solution with <code>get_image_uri<\/code>. It looks like this function will be depreceated and I don't know how to use <code>ImageURIProvider<\/code> instead.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_date":"2020-09-07 10:36:34.217 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-09-07 12:32:47.09 UTC",
        "Question_score":4,
        "Question_tags":"python|amazon-web-services|aws-cli|amazon-sagemaker|amazon-ecr",
        "Question_view_count":3017,
        "Owner_creation_date":"2017-11-20 20:38:25.77 UTC",
        "Owner_last_access_date":"2022-09-24 21:31:41.553 UTC",
        "Owner_reputation":12908,
        "Owner_up_votes":2204,
        "Owner_down_votes":404,
        "Owner_views":1267,
        "Answer_body":"<p>The newer versions of SageMaker SDK have a more centralized API for getting the URIs:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sagemaker \nsagemaker.image_uris.retrieve(&quot;linear-learner&quot;, &quot;eu-central-1&quot;)\n<\/code><\/pre>\n<p>which gives the expected result:<\/p>\n<pre><code>664544806723.dkr.ecr.eu-central-1.amazonaws.com\/linear-learner:1\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-04-07 16:42:26.593 UTC",
        "Answer_score":4.0,
        "Owner_location":"Bad Orb, Germany",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63775893",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64561968,
        "Question_title":"Building XGBoost on SageMaker",
        "Question_body":"<p>I am trying to run XGBoost on AWS Sagemaker and trying to call the container for XGBoost.<\/p>\n<pre><code>\ncontainers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com\/xgboost:latest',\n              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest',\n              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest',\n              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'}\n\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;objective&quot;:&quot;binary:logistic&quot;,\n        &quot;num_round&quot;:50\n        }\n\nestimator = sagemaker.estimator.Estimator(image_name=containers['us-east-1'], \n                                          hyperparameters=hyperparameters,\n                                          role=sagemaker.get_execution_role(),\n                                          train_instance_count=1, \n                                          train_instance_type='ml.m5.2xlarge', \n                                          train_volume_size=5, # 5 GB \n                                          output_path=output_path,\n                                          train_use_spot_instances=True,\n                                          train_max_run=300,\n                                          train_max_wait=600)\n\n\n<\/code><\/pre>\n<p>However, running the following throws an error:<\/p>\n<pre><code>estimator.fit({'train': s3_input_train,'validation': s3_input_test})\n<\/code><\/pre>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: Invalid DNS suffix 'amazonaws.com' for region 'us-east-1' in training image. Please provide the valid &lt;region&gt;.&lt;dns-suffix&gt;: 'ap-south-1.amazonaws.com'\n<\/code><\/pre>\n<p>Can someone help on how to fix this error? Thank you.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2020-10-27 19:56:07.09 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|boto3|amazon-sagemaker",
        "Question_view_count":357,
        "Owner_creation_date":"2011-10-19 10:12:30.6 UTC",
        "Owner_last_access_date":"2022-09-09 09:12:03.143 UTC",
        "Owner_reputation":3073,
        "Owner_up_votes":238,
        "Owner_down_votes":5,
        "Owner_views":341,
        "Answer_body":"<p>The notebook instance was created in ap-south-1 and the S3 bucket was in us-east-1. Creating another notebook instance from the same region as the S3 bucket resolved the issue.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-10-28 09:07:36.1 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64561968",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66196815,
        "Question_title":"Using Logistic Regression For Timeseries Data in Amazon SageMaker",
        "Question_body":"<p>For a project I am working on, which uses annual financial reports data (of multiple categories) from companies which have been successful or gone bust\/into liquidation, I previously created a (fairly well performing) model on AWS Sagemaker using a multiple linear regression algorithm (specifically, the AWS stock algorithm for logistic regression\/classification problems - the 'Linear Learner' algorithm)<\/p>\n<p>This model just produces a simple &quot;company is in good health&quot; or &quot;company looks like it will go bust&quot; binary prediction, based on one set of annual data fed in; e.g.<\/p>\n<pre><code>query input: {data:[{\n&quot;Gross Revenue&quot;: -4000,\n&quot;Balance Sheet&quot;: 10000,\n&quot;Creditors&quot;: 4000,\n&quot;Debts&quot;: 1000000 \n}]}\n\ninference output: &quot;in good health&quot; \/ &quot;in bad health&quot;\n<\/code><\/pre>\n<p>I trained this model by just ignoring what year for each company the values were from and pilling in all of the annual financial reports data (i.e. one years financial data for one company = one input line) for the training, along with the label of &quot;good&quot; or &quot;bad&quot; - a good company was one which has existed for a while, but hasn't gone bust, a bad company is one which was found to have eventually gone bust; e.g.:<\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>label<\/th>\n<th>Gross Revenue<\/th>\n<th>Balance Sheet<\/th>\n<th>Creditors<\/th>\n<th>Debts<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>good<\/td>\n<td>10000<\/td>\n<td>20000<\/td>\n<td>0<\/td>\n<td>0<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>0<\/td>\n<td>5<\/td>\n<td>100<\/td>\n<td>10000<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>20000<\/td>\n<td>0<\/td>\n<td>4<\/td>\n<td>100000000<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>I hence used these multiple features (gross revenue, balance sheet...) along with the label (good\/bad) in my training input, to create my first model.<\/p>\n<p>I would like to use the same features as before as input (gross revenue, balance sheet..) but over multiple years; e.g take the values from 2020 &amp; 2019 and use these (along with the eventual company status of &quot;good&quot; or &quot;bad&quot;) as the singular input for my new model. However I'm unsure of the following:<\/p>\n<ul>\n<li>is this an inappropriate use of logistic regression Machine learning? i.e. is there a more suitable algorithm I should consider?<\/li>\n<li>is it fine, or terribly wrong to try and just use the same technique as before, but combine the data for both years into one input line like:<\/li>\n<\/ul>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>label<\/th>\n<th>Gross Revenue(2019)<\/th>\n<th>Balance Sheet(2019)<\/th>\n<th>Creditors(2019)<\/th>\n<th>Debts(2019)<\/th>\n<th>Gross Revenue(2020)<\/th>\n<th>Balance Sheet(2020)<\/th>\n<th>Creditors(2020)<\/th>\n<th>Debts(2020)<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>good<\/td>\n<td>10000<\/td>\n<td>20000<\/td>\n<td>0<\/td>\n<td>0<\/td>\n<td>30000<\/td>\n<td>10000<\/td>\n<td>40<\/td>\n<td>500<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>100<\/td>\n<td>50<\/td>\n<td>200<\/td>\n<td>50000<\/td>\n<td>100<\/td>\n<td>5<\/td>\n<td>100<\/td>\n<td>10000<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>5000<\/td>\n<td>0<\/td>\n<td>2000<\/td>\n<td>800000<\/td>\n<td>2000<\/td>\n<td>0<\/td>\n<td>4<\/td>\n<td>100000000<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>I would personally expect that a company which has gotten worse over time (i.e. companies finances are worse in 2020 than in 2019) should be more likely to be found to be a &quot;bad&quot;\/likely to go bust, so I would hope that, if I feed in data like in the above example (i.e. earlier years data comes before later years data, on an input line) my training job ends up creating a model which gives greater weighting to the earlier years data, when making predictions<\/p>\n<p>Any advice or tips would be greatly appreciated - I'm pretty new to machine learning and would like to learn more<\/p>\n<p>UPDATE:<\/p>\n<p>Using Long-Short-Term-Memory Recurrent Neural Networks (LSTM RNN) is one potential route I think I could try taking, but this seems to commonly just be used with multivariate data over many dates; my data only has 2 or 3 dates worth of multivariate data, per company. I would want to try using the data I have for all the companies, over the few dates worth of data there are, in training<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-02-14 15:15:31.557 UTC",
        "Question_favorite_count":0.0,
        "Question_last_edit_date":"2021-02-24 12:09:13.343 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|machine-learning|logistic-regression|recurrent-neural-network|amazon-sagemaker",
        "Question_view_count":183,
        "Owner_creation_date":"2019-08-26 17:17:56.033 UTC",
        "Owner_last_access_date":"2022-01-09 18:01:35.493 UTC",
        "Owner_reputation":65,
        "Owner_up_votes":23,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":"<p>I once developed a so called Genetic Time Series in R. I used a Genetic Algorithm which sorted out the best solutions from multivariate data, which were fitted on a VAR in differences or a VECM. Your data seems more macro economic or financial than user-centric and VAR or VECM seems appropriate. (Surely it is possible to treat time-series data in the same way so that we can use LSTM or other approaches, but these are very common) However, I do not know if VAR in differences or VECM works with binary classified labels. Perhaps if you would calculate a metric outcome, which you later label encode to a categorical feature (or label it first to a categorical) than VAR or VECM may also be appropriate.<\/p>\n<p>However you may add all yearly data points to one data points per firm to forecast its survival, but you would loose a lot of insight. If you are interested in time series ML which works a little bit different than for neural networks or elastic net (which could also be used with time series) let me know. And we can work something out. Or I'll paste you some sources.<\/p>\n<p>Summary:\n1.)\nIt is possible to use LSTM, elastic NEt (time points may be dummies or treated as cross sectional panel) or you use VAR in differences and VECM with a slightly different out come variable<\/p>\n<p>2.)\nIt is possible but you will loose information over time.<\/p>\n<p>All the best,\nPatrick<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-02-27 18:21:16.54 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2021-02-27 18:46:22.833 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66196815",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73172644,
        "Question_title":"Running ipynb file, but can't import a certain module",
        "Question_body":"<p>l'am running a ipynb file on sagemaker, however the error of occurs.\nl have used 'pip install tqdm' in terminals to install the tqdm so l've no idea what's happening. Is it running in a different environment?\nThanks for any answer.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bO6zS.png\" rel=\"nofollow noreferrer\">error report from my ipynb file <\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/5tP1J.png\" rel=\"nofollow noreferrer\">what l've done in terminal<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-30 03:25:16.027 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-07-30 03:25:56.683 UTC",
        "Question_score":0,
        "Question_tags":"python|jupyter-notebook|development-environment|amazon-sagemaker",
        "Question_view_count":34,
        "Owner_creation_date":"2022-07-20 06:54:59.523 UTC",
        "Owner_last_access_date":"2022-08-14 12:22:20.077 UTC",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":"<p>There is a possibility you may be executing &quot;pip&quot; in a different environment.<\/p>\n<p>Try executing &quot;!pip install tqdm&quot; or &quot;!pip3 install tqdm&quot; as a code cell in the Sagemaker document itself.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-07-30 04:36:22.777 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73172644",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69254978,
        "Question_title":"how to use ARIMA model in AWS sagemaker?",
        "Question_body":"<p>I have created an ARIMA model for time-series forecasting and want to deploy it so as to use it at the API endpoint. But I am unable to find a way to deploy it on AWS SageMaker, how can I deploy it. I don't want to use DeepAR. Or is there any way to deploy the pickle file on SageMaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-20 13:07:20.477 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-09-20 14:52:47.147 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|arima",
        "Question_view_count":368,
        "Owner_creation_date":"2019-10-30 07:35:23.653 UTC",
        "Owner_last_access_date":"2022-09-16 10:45:36.747 UTC",
        "Owner_reputation":35,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":"<p>You can use Amazon Forecast, which has ARIMA <a href=\"https:\/\/docs.aws.amazon.com\/forecast\/latest\/dg\/aws-forecast-recipe-arima.html\" rel=\"nofollow noreferrer\">built in<\/a><\/p>\n<p>Or, if you prefer SageMaker, you need to build your own Docker container, publish it to ECR, and then use that<\/p>\n<p><a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.html#The-example\" rel=\"nofollow noreferrer\">https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.html#The-example<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-09-20 14:52:13.97 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2021-09-22 14:45:21.76 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69254978",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50418501,
        "Question_title":"Sagemaker model evaluation",
        "Question_body":"<p>The Amazon documentation lists several approaches to evaluate a model (e.g. cross validation, etc.) however these methods does not seem to be available in the Sagemaker Java SDK. \nCurrently if we want to do 5-fold cross validation it seems the only option is to create 5 models (and also deploy 5 endpoints) one model for each subset of data and manually compute the performance metric (recall, precision, etc.). <\/p>\n\n<p>This approach is not very efficient and can also be expensive need to deploy k-endpoints, based on the number of folds in the k-fold validation.<\/p>\n\n<p>Is there another way to test the performance of a model?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-05-18 20:01:39.553 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"aws-sdk|aws-java-sdk|amazon-sagemaker",
        "Question_view_count":1442,
        "Owner_creation_date":"2015-04-08 00:54:56.053 UTC",
        "Owner_last_access_date":"2019-10-03 00:46:15.877 UTC",
        "Owner_reputation":35,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":"<p>Amazon SageMaker is a set of multiple components that you can choose which ones to use. <\/p>\n\n<p>The built-in algorithms are designed for (infinite) scale, which means that you can have huge datasets and be able to build a model with them quickly and with low cost. Once you have large datasets you usually don't need to use techniques such as cross-validation, and the recommendation is to have a clear split between training data and validation data. Each of these parts will be defined with an input channel when you are submitting a training job.  <\/p>\n\n<p>If you have a small amount of data and you want to train on all of it and use cross-validation to allow it, you can use a different part of the service (interactive notebook instance). You can bring your own algorithm or even container image to be used in the development, training or hosting. You can have any python code based on any machine learning library or framework, including scikit-learn, R, TensorFlow, MXNet etc. In your code, you can define cross-validation based on the training data that you copy from S3 to the worker instances. <\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2018-05-19 19:29:38.327 UTC",
        "Answer_score":3.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2018-05-28 17:13:42.773 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50418501",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72319750,
        "Question_title":"Start Sagemaker Notebook Instances - Question",
        "Question_body":"<p>Quick question regarding the start\/boot up on a existing sagemaker notebook instance. I previously created a notebook instance, and shut it down when I stop using it. I was giving access to this shared account by a co-worker who manages our AWS infrastructure, and this account is shared to multiple members from other departments, but then he gave me a separate sandbox account, where I'm the only one using.<\/p>\n<p>After one month, I went back to that same account (shared one) and found out my notebook instance, along side others, were running. Not sure how, and why. I'm pretty sure I turned them off (at least mine), and no other person uses them, or uses Sagemaker.<\/p>\n<p>Is there any possible way that these notebook instances started by chance, or other possible explanation to it?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-20 13:28:38.85 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-05-20 13:37:29.907 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":34,
        "Owner_creation_date":"2019-10-27 20:29:51.12 UTC",
        "Owner_last_access_date":"2022-09-22 11:53:24.677 UTC",
        "Owner_reputation":139,
        "Owner_up_votes":19,
        "Owner_down_votes":4,
        "Owner_views":45,
        "Answer_body":"<p>A quick way to diagnose this would be using <a href=\"https:\/\/aws.amazon.com\/cloudtrail\/\" rel=\"nofollow noreferrer\">CloudTrail<\/a>. Go to the CloudTrail console in the shared account, choose &quot;Event History&quot; section on the left. You can then filter by &quot;Event name&quot;: &quot;StartNotebookInstance&quot; and you'll be able to see which identity made the call.<\/p>\n<p>Afaik, no AWS service would automatically start a notebook instance.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-05-20 16:15:17.113 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72319750",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72975189,
        "Question_title":"How to build a simple spark-test-app.jar to test AWS SageMaker SparkJarProcessing",
        "Question_body":"<p>Does anyone know a repo that shows what a simple HelloWorld java or scala code would look like to build the jar that could be executed using the AWS SageMaker <strong>SparkJarProcessing<\/strong> class?<\/p>\n<p>Readthedocs (<a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_processing\/spark_distributed_data_processing\/sagemaker-spark-processing.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_processing\/spark_distributed_data_processing\/sagemaker-spark-processing.html<\/a>) mentions:<\/p>\n<p>&quot;In the next example, you\u2019ll take a Spark application jar (located in .\/code\/spark-test-app.jar)...&quot;<\/p>\n<p>My question is how does the source code look like for this jar (spark-test-app.jar)?<\/p>\n<p>I tried building a simple Java project jar<\/p>\n<p>src&gt;com.test&gt;HW.java:<\/p>\n<pre><code>\npublic class HW {\n    public static void main(String[] args) {\n        System.out.printf(&quot;hello world!&quot;);\n    }\n}\n<\/code><\/pre>\n<p>and running it inside SageMaker Notebook conda_python3 kernel using<\/p>\n<pre><code>from sagemaker.spark.processing import SparkJarProcessor\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\nprint(role)\n\nspark_processor = SparkJarProcessor(\n    base_job_name=&quot;sm-spark-java&quot;,\n    framework_version=&quot;3.1&quot;,\n    role=role,\n    instance_count=2,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    max_runtime_in_seconds=1200,\n)\n\nspark_processor.run(\n    submit_app=&quot;.\/SparkJarProcessing-1.0-SNAPSHOT.jar&quot;,\n    submit_class=&quot;com.test.HW&quot;,\n    arguments=[&quot;--input&quot;, &quot;abc&quot;],\n    logs=True,\n)\n<\/code><\/pre>\n<p>But end up getting an error:\nCould not execute HW class.<\/p>\n<p>Any sample source code for <strong>spark-test-app.jar<\/strong> would be highly appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-14 04:25:45.6 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"java|amazon-web-services|scala|jar|amazon-sagemaker",
        "Question_view_count":49,
        "Owner_creation_date":"2020-09-24 17:50:50.14 UTC",
        "Owner_last_access_date":"2022-09-11 22:52:57.793 UTC",
        "Owner_reputation":51,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>To answer your question, the source code of that class looks like:<\/p>\n<pre><code>package com.amazonaws.sagemaker.spark.test;\n\nimport java.lang.invoke.SerializedLambda;\nimport org.apache.commons.cli.CommandLineParser;\nimport org.apache.commons.cli.ParseException;\nimport org.apache.commons.cli.HelpFormatter;\nimport org.apache.commons.cli.Option;\nimport org.apache.commons.cli.BasicParser;\nimport org.apache.commons.cli.Options;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.commons.cli.CommandLine;\nimport org.apache.spark.sql.types.DataTypes;\nimport org.apache.commons.lang3.StringUtils;\nimport java.util.List;\nimport org.apache.spark.sql.SparkSession;\n\npublic class HelloJavaSparkApp\n{\n    public static void main(final String[] args) {\n        System.out.println(&quot;Hello World, this is Java-Spark!&quot;);\n        final CommandLine parsedArgs = parseArgs(args);\n        final String inputPath = parsedArgs.getOptionValue(&quot;input&quot;);\n        final String outputPath = parsedArgs.getOptionValue(&quot;output&quot;);\n        final SparkSession spark = SparkSession.builder().appName(&quot;Hello Spark App&quot;).getOrCreate();\n        System.out.println(&quot;Got a Spark session with version: &quot; + spark.version());\n        System.out.println(&quot;Reading input from: &quot; + inputPath);\n        final Dataset salesDF = spark.read().json(inputPath);\n        salesDF.printSchema();\n        salesDF.createOrReplaceTempView(&quot;sales&quot;);\n        final Dataset topDF = spark.sql(&quot;SELECT date, sale FROM sales WHERE sale &gt; 750 SORT BY sale DESC&quot;);\n        topDF.show();\n        final Dataset avgDF = salesDF.groupBy(&quot;date&quot;, new String[0]).avg(new String[0]).orderBy(&quot;date&quot;, new String[0]);\n        System.out.println(&quot;Collected average sales: &quot; + StringUtils.join((Object[])new List[] { avgDF.collectAsList() }));\n        spark.sqlContext().udf().register(&quot;double&quot;, n -&gt; n + n, DataTypes.LongType);\n        final Dataset saleDoubleDF = salesDF.selectExpr(new String[] { &quot;date&quot;, &quot;sale&quot;, &quot;double(sale) as sale_double&quot; }).orderBy(&quot;date&quot;, new String[] { &quot;sale&quot; });\n        saleDoubleDF.show();\n        System.out.println(&quot;Writing output to: &quot; + outputPath);\n        saleDoubleDF.coalesce(1).write().json(outputPath);\n        spark.stop();\n    }\n    \n    private static CommandLine parseArgs(final String[] args) {\n        final Options options = new Options();\n        final CommandLineParser parser = (CommandLineParser)new BasicParser();\n        final Option input = new Option(&quot;i&quot;, &quot;input&quot;, true, &quot;input path&quot;);\n        input.setRequired(true);\n        options.addOption(input);\n        final Option output = new Option(&quot;o&quot;, &quot;output&quot;, true, &quot;output path&quot;);\n        output.setRequired(true);\n        options.addOption(output);\n        try {\n            return parser.parse(options, args);\n        }\n        catch (ParseException e) {\n            new HelpFormatter().printHelp(&quot;HelloScalaSparkApp --input \/opt\/ml\/input\/foo --output \/opt\/ml\/output\/bar&quot;, options);\n            throw new RuntimeException((Throwable)e);\n        }\n    }\n}\n<\/code><\/pre>\n<p>At the same time, I have created a simple example that shows how to run an hello world app <a href=\"https:\/\/github.com\/giuseppeporcelli\/sagemaker-misc-examples\/blob\/main\/spark-jar-example.ipynb\" rel=\"nofollow noreferrer\">here<\/a>. Please note that I have run that example on Amazon SageMaker Studio Notebooks, using the Data Science 1.0 kernel.<\/p>\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-19 13:44:43.58 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72975189",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64584295,
        "Question_title":"Sagemaker XGBoost Hyperparameter Tuning Error",
        "Question_body":"<p>I am new to Sagemaker and trying to set up a hyperparameter tuning job for xgboost algorithm in Sagemaker. I have very imbalanced data (98% majority class, 2% minority\u00a0class) and would like to use the\u00a0&quot;scale_pos_weight&quot; parameter but the below error happens.<\/p>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: The hyperparameter tuning job that you requested has the following untunable hyperparameters: [scale_pos_weight]. For the algorithm, ---------------.us-east-1.amazonaws.com\/xgboost:1, you can tune only [colsample_bytree, lambda, eta, max_depth, alpha, num_round, colsample_bylevel, subsample, min_child_weight, max_delta_step, gamma]. Delete untunable hyperparameters.\u00a0\u00a0\n<\/code><\/pre>\n<p>I have upgraded the sagemaker package, restarted my kernel (I am using juptyer notebook), and instance but the problem still exists.<\/p>\n<p>Does anyone have any ideas why this error happens and how I can fix it? I appreciate the help.<\/p>\n<p>\u00a0\nHere is my code that I followed from an example in AWS.\u00a0<\/p>\n<pre><code>sess = sagemaker.Session()\ncontainer = get_image_uri(region, 'xgboost', '1.0-1')\n\nxgb = sagemaker.estimator.Estimator(container,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 role, \n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 train_instance_count=1, \n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 train_instance_type='ml.m4.4xlarge',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output_path='s3:\/\/{}\/{}\/output'.format(bucket, prefix),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sagemaker_session=sess)\n\nxgb.set_hyperparameters(eval_metric='auc',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 objective='binary:logistic',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 num_round=100,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rate_drop=0.3,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 tweedie_variance_power=1.4)\n\nhyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'min_child_weight': ContinuousParameter(1, 10),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'scale_pos_weight' : ContinuousParameter(700, 800),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'alpha': ContinuousParameter(0, 2),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'max_depth': IntegerParameter(1, 10),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'colsample_bytree' : ContinuousParameter(0.1, 0.9)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\nobjective_metric_name = 'validation:auc'\n\ntuner = HyperparameterTuner(xgb,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 objective_metric_name,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 hyperparameter_ranges,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 max_jobs=10,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 max_parallel_jobs=2)\n\ns3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train'.format(bucket, prefix), content_type='csv')\ns3_input_validation = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/validation\/'.format(bucket, prefix), content_type='csv')\n\ntuner.fit({'train': s3_input_train, 'validation': s3_input_validation}, include_cls_metadata=False)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-29 02:52:26.643 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|xgboost|amazon-sagemaker|hyperparameters",
        "Question_view_count":827,
        "Owner_creation_date":"2018-04-24 20:58:14.497 UTC",
        "Owner_last_access_date":"2022-09-13 17:32:03.52 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>Based on the Sagemaker developer documentation, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html<\/a>, the hyperparameter <code>scale_pos_weight<\/code> is NOT tunable. The only parameters that you can tune are given in the link.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-10-29 03:15:58.963 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64584295",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59863842,
        "Question_title":"Limit on the rate of inferences one can make for a SageMaker endpoint",
        "Question_body":"<p>Is there a limit on the rate of inferences one can make for a SageMaker endpoint?<\/p>\n\n<p>Is it determined somehow by the instance type behind the endpoint or the number of instances?<\/p>\n\n<p>I tried looking for this info as <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws_service_limits.html#limits_sagemaker\" rel=\"nofollow noreferrer\">AWS Service Quotas for SageMaker<\/a> but couldn't find it.<\/p>\n\n<p>I am invoking the endpoint from a Spark job abd wondered if the number of concurrent tasks is a factor I should be taking care of when running inference (assuming each task runs one inference at a time) <\/p>\n\n<p>Here's the throttling error I got:<\/p>\n\n<pre><code>com.amazonaws.services.sagemakerruntime.model.AmazonSageMakerRuntimeException: null (Service: AmazonSageMakerRuntime; Status Code: 400; Error Code: ThrottlingException; Request ID: b515121b-f3d5-4057-a8a4-6716f0708980)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.doInvoke(AmazonSageMakerRuntimeClient.java:236)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.invoke(AmazonSageMakerRuntimeClient.java:212)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.executeInvokeEndpoint(AmazonSageMakerRuntimeClient.java:176)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.invokeEndpoint(AmazonSageMakerRuntimeClient.java:151)\n    at lineefd06a2d143b4016906a6138a6ffec15194.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$a5cddfc4633c5dd8aa603ddc4f9aad5$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Predictor.predict(command-2334973:41)\n    at lineefd06a2d143b4016906a6138a6ffec15200.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$50a9225beeac265557e61f69d69d7d$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(command-2307906:11)\n    at lineefd06a2d143b4016906a6138a6ffec15200.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$50a9225beeac265557e61f69d69d7d$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(command-2307906:11)\n    at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n    at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:2000)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1220)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1220)\n    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n    at org.apache.spark.scheduler.Task.run(Task.scala:113)\n    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2020-01-22 16:07:19.42 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-01-22 16:32:05.73 UTC",
        "Question_score":3,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1716,
        "Owner_creation_date":"2016-03-21 08:49:39.92 UTC",
        "Owner_last_access_date":"2022-07-17 11:53:14.84 UTC",
        "Owner_reputation":383,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Answer_body":"<p>Amazon SageMaker is offering model hosting service (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-hosting.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-hosting.html<\/a>), which gives you a lot of flexibility based on your inference requirements. <\/p>\n\n<p>As you noted, first you can choose the instance type to use for your model hosting. The large set of options is important to tune to your models. You can host the model on a GPU based machines (P2\/P3\/P4) or CPU ones. You can have instances with faster CPU (C4, for example), or more RAM (R4, for example). You can also choose instances with more cores (16xl, for example) or less (medium, for example). Here is a list of the full range of instances that you can choose: <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/<\/a> . It is important to balance your performance and costs. The selection of the instance type and the type and size of your model will determine the invocations-per-second that you can expect from your model in this single-node configuration. It is important to measure this number to avoid hitting the throttle errors that you saw. <\/p>\n\n<p>The second important feature of the SageMaker hosting that you use is the ability to auto-scale your model to multiple instances. You can configure the endpoint of your model hosting to automatically add and remove instances based on the load on the endpoint. AWS is adding a load balancer in front of the multiple instances that are hosting your models and distributing the requests among them. Using the autoscaling functionality allows you to keep a smaller instance for low traffic hours, and to be able to scale up during peak traffic hours, and still keep your costs low and your throttle errors to the minimum. See here for documentation on the SageMaker autoscaling options: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling.html<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-01-25 11:25:18.923 UTC",
        "Answer_score":4.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59863842",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61059996,
        "Question_title":"Sagemaker Processing doesn't upload",
        "Question_body":"<p>I'm trying to use the sagemaker processor to replace some processes we run on Amazon batch.<\/p>\n<pre><code>from sagemaker.processor import ScriptProcessor \nproc = ScriptProcessor(\n    image_uri='your-image-uri', \n    command=['python3'], \n    role=role, \n    instance_count=1, \n    instance_type='m4.4x.large',  \n    volume_size_in_gb=500,\n    base_job_name='preprocessing-test',\n)\nproc.run(\n    code='test.py',\n)\n<\/code><\/pre>\n<p>First of all, is it true that the <code>ScriptProcessing<\/code> syntax is more complicated than the <code>TrainingJob<\/code> version where you can specify the <code>source_dir<\/code> and <code>entrypoint<\/code> to upload your code to a default container?<\/p>\n<p>Secondly, this code above gives me this error<\/p>\n<pre><code>ParamValidationError: Parameter validation failed:\nInvalid bucket name &quot;sagemaker-eu-west-1-&lt;account-id&gt;\\preprocessing-test-&lt;timestamp&gt;\\input\\code&quot;: Bucket name must match the regex &quot;^[a-zA-Z0-9.\\-_]{1,255}$&quot; or be an ARN matching the regex &quot;^arn:(aws).*:s3:[a-z\\-0-9]+:[0-9]{12}:accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$&quot;\n<\/code><\/pre>\n<p>I guess this key is created internally when trying to upload my <code>test.py<\/code>, but why does it not work? :) The documentation says you can use both local and s3 paths.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2020-04-06 12:35:11.533 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-03-25 07:36:39.11 UTC",
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":314,
        "Owner_creation_date":"2017-01-19 15:07:44.573 UTC",
        "Owner_last_access_date":"2022-09-22 14:55:11.743 UTC",
        "Owner_reputation":3937,
        "Owner_up_votes":672,
        "Owner_down_votes":27,
        "Owner_views":387,
        "Answer_body":"<p>The bucket name `sagemaker-eu-west-1-\\preprocessing-test-\\input\\code looks like a hardcoded string. In SageMaker Python SDK, the code upload function is <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/3bf569ece9e46a097d1ab69286ee89f762931e6c\/src\/sagemaker\/processing.py#L463\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<p>Are you using a Windows environment? As Lauren noted in the comments, there have been some bug fixes there, so make sure to use the last version<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-04-17 06:06:31.427 UTC",
        "Answer_score":1.0,
        "Owner_location":"Amsterdam, Nederland",
        "Answer_last_edit_date":"2021-03-25 07:35:08.82 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61059996",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66086605,
        "Question_title":"Use AWS ML model Random Cut Forest locally",
        "Question_body":"<p>I wonder if it is possible to deploy Random Cut Forest (RCF) built-in algorithm of SageMaker to the local mode. I haven't come across any sample implementation about it. If not, can we simply say that models trained using RCF are limited to be consumed inside the platform via Inference Endpoints?<\/p>\n<p>I got this error when I tried to do so.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/YFlRf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YFlRf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ywlQU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ywlQU.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-07 10:02:35.937 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|rcf",
        "Question_view_count":195,
        "Owner_creation_date":"2020-07-12 10:41:34.35 UTC",
        "Owner_last_access_date":"2022-04-24 18:15:04.303 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":"<p>indeed you're right, <strong>SageMaker Random Cut Forest cannot be trained and deployed locally. The 18 Amazon SageMaker Built-in algorithms are designed to be trained and deployed on Amazon SageMaker.<\/strong> There are 2 exceptions: SageMaker BlazingText and SageMaker XGBoost, which can be read with their open-source counterparts (fastText and XGBoost) and used for  inference out of SageMaker (eg EC2, Lambda, on-prem or on your laptop - as long as you can install those libraries)<\/p>\n<p>There is an open-source attempt to implement the Random Cut Forest here <a href=\"https:\/\/github.com\/kLabUM\/rrcf\" rel=\"nofollow noreferrer\">https:\/\/github.com\/kLabUM\/rrcf<\/a> ; I don't think it has any connection to SageMaker RCF codebase so results, speed and scalability may differ.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-02-07 21:41:32.367 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2021-02-07 22:30:23.06 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66086605",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66899120,
        "Question_title":"Validation error on Role name when running AWS SageMaker linear-learner locally",
        "Question_body":"<p>I'm trying to build a machine learning model locally using AWS SageMaker, but I got a validation error on IAM Role name. Although it's the exact role name that I created on the console.<\/p>\n<p>This is my code<\/p>\n<pre><code>    import boto3\n    import sagemaker\n    from sagemaker import get_execution_role\n    from sagemaker.amazon.amazon_estimator import image_uris\n    from sagemaker.amazon.amazon_estimator import RecordSet\n\n    sess = sagemaker.Session()\n\n\n    bucket = sagemaker.Session().default_bucket()\n    prefix = 'sagemaker\/ccard19'\n\n    role ='arn:aws:iam::94911111111542:role\/SageMaker-Full-Access '\n\n    container = image_uris.retrieve('linear-learner',boto3.Session().region_name)\n    \n    # Some other code\n\n   linear = sagemaker.LinearLearner(role=role,\n                                               instance_count=1,\n                                               instance_type='ml.m4.xlarge',\n                                               predictor_type='binary_classifier')\n  \n  # Some other code\n\n  ### Fit the classifier\n  linear.fit([train_records,val_records,test_records], wait=True, logs='All')\n\n<\/code><\/pre>\n<p>And this is the error message<\/p>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: 1 validation error detected: Value 'arn:aws:iam::949010940542:role\/SageMaker-Full-Access ' at 'roleArn' failed to satisfy constraint: Member must satisfy regular expression pattern: ^arn:aws[a-z\\-]*:iam::\\d{12}:role\/?[a-zA-Z_0-9+=,.@\\-_\/]+$\n<\/code><\/pre>\n<p>Any Help please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-01 05:58:48.18 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":560,
        "Owner_creation_date":"2013-09-01 12:45:39.503 UTC",
        "Owner_last_access_date":"2022-09-23 06:02:47.613 UTC",
        "Owner_reputation":340,
        "Owner_up_votes":78,
        "Owner_down_votes":1,
        "Owner_views":17,
        "Answer_body":"<p>You have <strong>space<\/strong> in the name. It should be:<\/p>\n<pre><code>role ='arn:aws:iam::94911111111542:role\/SageMaker-Full-Access'\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-04-01 06:00:09.81 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66899120",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56255154,
        "Question_title":"How to use a pretrained model from s3 to predict some data?",
        "Question_body":"<p>I have trained a semantic segmentation model using the sagemaker and the out has been saved to a s3 bucket. I want to load this model from the s3 to predict some images in sagemaker. <\/p>\n\n<p>I know how to predict if I leave the notebook instance running after the training as its just an easy deploy but doesn't really help if I want to use an older model.<\/p>\n\n<p>I have looked at these sources and been able to come up with something myself but it doesn't work hence me being here:<\/p>\n\n<p><a href=\"https:\/\/course.fast.ai\/deployment_amzn_sagemaker.html#deploy-to-sagemaker\" rel=\"noreferrer\">https:\/\/course.fast.ai\/deployment_amzn_sagemaker.html#deploy-to-sagemaker<\/a>\n<a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a><\/p>\n\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/pipeline.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/pipeline.html<\/a><\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb<\/a><\/p>\n\n<p>My code is this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.pipeline import PipelineModel\nfrom sagemaker.model import Model\n\ns3_model_bucket = 'bucket'\ns3_model_key_prefix = 'prefix'\ndata = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\nmodels = ss_model.create_model() # ss_model is my sagemaker.estimator\n\nmodel = PipelineModel(name=data, role=role, models= [models])\nss_predictor = model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-22 10:50:48.223 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2019-05-22 11:21:34.857 UTC",
        "Question_score":6,
        "Question_tags":"python|amazon-web-services|amazon-s3|boto3|amazon-sagemaker",
        "Question_view_count":7404,
        "Owner_creation_date":"2019-05-22 09:45:28.153 UTC",
        "Owner_last_access_date":"2022-09-02 09:43:13.34 UTC",
        "Owner_reputation":79,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":"<p>You can actually instantiate a Python SDK <code>model<\/code> object from existing artifacts, and deploy it to an endpoint. This allows you to deploy a model from trained artifacts, without having to retrain in the notebook. For example, for the semantic segmentation model:<\/p>\n\n<pre><code>trainedmodel = sagemaker.model.Model(\n    model_data='s3:\/\/...model path here..\/model.tar.gz',\n    image='685385470294.dkr.ecr.eu-west-1.amazonaws.com\/semantic-segmentation:latest',  # example path for the semantic segmentation in eu-west-1\n    role=role)  # your role here; could be different name\n\ntrainedmodel.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')\n<\/code><\/pre>\n\n<p>And similarly, you can instantiate a predictor object on a deployed endpoint from any authenticated client supporting the SDK, with the following command:<\/p>\n\n<pre><code>predictor = sagemaker.predictor.RealTimePredictor(\n    endpoint='endpoint name here',\n    content_type='image\/jpeg',\n    accept='image\/png')\n<\/code><\/pre>\n\n<p>More on those abstractions:<\/p>\n\n<ul>\n<li><code>Model<\/code>: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html<\/a><\/li>\n<li><code>Predictor<\/code>:\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-05-23 14:25:59.713 UTC",
        "Answer_score":13.0,
        "Owner_location":"London, UK",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56255154",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58815367,
        "Question_title":"How to solve the error with deploying a model in aws sagemaker?",
        "Question_body":"<p>I have to deploy a custom keras model in AWS Sagemaker. I have a created a notebook instance and I have the following files:<\/p>\n\n<pre><code>AmazonSagemaker-Codeset16\n   -ann\n      -nginx.conf\n      -predictor.py\n      -serve\n      -train.py\n      -wsgi.py\n   -Dockerfile\n<\/code><\/pre>\n\n<p>I now open the AWS terminal and build the docker image and push the image in the ECR repository. Then I open a new jupyter python notebook and try to fit the model and deploy the same. The training is done correctly but while deploying I get the following error:<\/p>\n\n<blockquote>\n  <p>\"Error hosting endpoint sagemaker-example-2019-10-25-06-11-22-366: Failed. >Reason: The primary container for production variant AllTraffic did not pass >the ping health check. Please check CloudWatch logs for this endpoint...\"<\/p>\n<\/blockquote>\n\n<p>When I check the logs, I find the following:<\/p>\n\n<blockquote>\n  <p>2019\/11\/11 11:53:32 [crit] 19#19: *3 connect() to unix:\/tmp\/gunicorn.sock >failed (2: No such file or directory) while connecting to upstream, client: >10.32.0.4, server: , request: \"GET \/ping HTTP\/1.1\", upstream: >\"<a href=\"http:\/\/unix:\/tmp\/gunicorn.sock:\/ping\" rel=\"nofollow noreferrer\">http:\/\/unix:\/tmp\/gunicorn.sock:\/ping<\/a>\", host: \"model.aws.local:8080\"<\/p>\n<\/blockquote>\n\n<p>and <\/p>\n\n<blockquote>\n  <p>Traceback (most recent call last):\n   File \"\/usr\/local\/bin\/serve\", line 8, in \n     sys.exit(main())\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/cli\/serve.py\", line 19, in main\n     server.start(env.ServingEnv().framework_module)\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/_server.py\", line 107, in start\n     module_app,\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 711, in <strong>init<\/strong>\n     errread, errwrite)\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 1343, in _execute_child\n     raise child_exception<\/p>\n<\/blockquote>\n\n<p>I tried to deploy the same model in AWS Sagemaker with these files in my local computer and the model was deployed successfully but inside AWS, I am facing this problem.<\/p>\n\n<p>Here is my serve file code:<\/p>\n\n<pre><code>from __future__ import print_function\nimport multiprocessing\nimport os\nimport signal\nimport subprocess\nimport sys\n\ncpu_count = multiprocessing.cpu_count()\n\nmodel_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)\nmodel_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', cpu_count))\n\n\ndef sigterm_handler(nginx_pid, gunicorn_pid):\n    try:\n        os.kill(nginx_pid, signal.SIGQUIT)\n    except OSError:\n        pass\n    try:\n        os.kill(gunicorn_pid, signal.SIGTERM)\n    except OSError:\n        pass\n\n    sys.exit(0)\n\n\ndef start_server():\n    print('Starting the inference server with {} workers.'.format(model_server_workers))\n\n\n    # link the log streams to stdout\/err so they will be logged to the container logs\n    subprocess.check_call(['ln', '-sf', '\/dev\/stdout', '\/var\/log\/nginx\/access.log'])\n    subprocess.check_call(['ln', '-sf', '\/dev\/stderr', '\/var\/log\/nginx\/error.log'])\n\n    nginx = subprocess.Popen(['nginx', '-c', '\/opt\/ml\/code\/nginx.conf'])\n    gunicorn = subprocess.Popen(['gunicorn',\n                                 '--timeout', str(model_server_timeout),\n                                 '-b', 'unix:\/tmp\/gunicorn.sock',\n                                 '-w', str(model_server_workers),\n                                 'wsgi:app'])\n\n    signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))\n\n    # If either subprocess exits, so do we.\n    pids = set([nginx.pid, gunicorn.pid])\n    while True:\n        pid, _ = os.wait()\n        if pid in pids:\n            break\n\n    sigterm_handler(nginx.pid, gunicorn.pid)\n    print('Inference server exiting')\n\n\n# The main routine just invokes the start function.\nif __name__ == '__main__':\n    start_server()\n<\/code><\/pre>\n\n<p>I deploy the model using the following:<\/p>\n\n<blockquote>\n  <p>predictor = classifier.deploy(1, 'ml.t2.medium', serializer=csv_serializer)<\/p>\n<\/blockquote>\n\n<p>Kindly let me know the mistake I am doing while deploying.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_date":"2019-11-12 09:11:44.24 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2019-11-12 09:16:44.733 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":4605,
        "Owner_creation_date":"2019-02-21 13:41:11.933 UTC",
        "Owner_last_access_date":"2022-09-23 13:21:28.837 UTC",
        "Owner_reputation":67,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<p>Using Sagemaker script mode can be much simpler than dealing with container and nginx low-level stuff like you're trying to do, have you considered that?<br>\nYou only need to provide the keras script:   <\/p>\n\n<blockquote>\n  <p>With Script Mode, you can use training scripts similar to those you would use outside SageMaker with SageMaker's prebuilt containers for various deep learning frameworks such TensorFlow, PyTorch, and Apache MXNet.<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-11-13 07:40:07.523 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58815367",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72491505,
        "Question_title":"SageMaker Inference for a video input",
        "Question_body":"<p>I wonder if it's possible to run SageMaker Inference or Batch Transform job directly for a video input (.mp4 or another format)?<\/p>\n<p>If no could you please advice the best practice that might be used for pre-processing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-03 14:59:05.127 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-06-03 15:01:09.487 UTC",
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":93,
        "Owner_creation_date":"2015-09-20 22:02:33.537 UTC",
        "Owner_last_access_date":"2022-09-24 17:11:02.827 UTC",
        "Owner_reputation":71,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Answer_body":"<p>Asynchronous inference could be a good option for this use case. There is a blog published by AWS that talks about how you can do this.<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-computer-vision-inference-on-large-videos-with-amazon-sagemaker-asynchronous-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-computer-vision-inference-on-large-videos-with-amazon-sagemaker-asynchronous-endpoints\/<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-03 15:38:44.137 UTC",
        "Answer_score":1.0,
        "Owner_location":"Kyiv",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72491505",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73651368,
        "Question_title":"SageMaker Distributed Training in Local Mode (inside Notebook Instances)",
        "Question_body":"<p>I've been using SageMaker for a while and have performed several experiments already with distributed training. I am wondering if it is possible to test and run SageMaker distributed training in local mode (using SageMaker Notebook Instances)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-08 15:11:51.493 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":19,
        "Owner_creation_date":"2022-09-08 15:07:33.073 UTC",
        "Owner_last_access_date":"2022-09-12 19:57:11.407 UTC",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":"<p>No, not possible yet. <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">local mode<\/a> does not support the distributed training with <code>local_gpu<\/code>for Gzip compression, Pipe Mode, or manifest files for inputs<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-08 15:48:16.573 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73651368",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57819173,
        "Question_title":"SageMaker AWS Binary Text Classification",
        "Question_body":"<p>Can AWS SageMaker handle binary classification using TFidf vectorized text as prediction base?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-09-06 09:20:34.067 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|nlp|amazon-sagemaker",
        "Question_view_count":109,
        "Owner_creation_date":"2012-06-08 10:32:32.347 UTC",
        "Owner_last_access_date":"2022-09-22 01:55:42.393 UTC",
        "Owner_reputation":1125,
        "Owner_up_votes":490,
        "Owner_down_votes":77,
        "Owner_views":319,
        "Answer_body":"<p>You would have to use inference pipeline for your use case. What that means is that you will need to use a pre-processing step to featurize your text into tfidf and then feed into Sagemaker classification. Here's a <a href=\"https:\/\/stackoverflow.com\/questions\/57767899\/how-to-create-a-pipeline-in-sagemaker-with-pytorch\">SO answer<\/a> with more details around this.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-09-07 00:20:55.55 UTC",
        "Answer_score":1.0,
        "Owner_location":"Amsterdam, Netherlands",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57819173",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60619460,
        "Question_title":"Dynamically read changing filename key",
        "Question_body":"<p>I have <code>parquet<\/code> files that are generated via <code>spark<\/code> and the filename (key) in <code>s3<\/code> will always change post ETL job.  This is the code I use to read the <code>parquet<\/code> files via <code>boto3<\/code> in <code>sagemaker<\/code>.  Looking for a way to dynamically read the <code>S3<\/code> filename (key) since hard-coding the key will fail the read since it changes every time.  How can this be achieved?  Thanks.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>filename = \"datasets\/randomnumbergenerator.parquet\"\nbucketName = \"bucket-name\"\n\nbuffer = io.BytesIO()\nclient = boto3.resource(\"s3\")\nobj = client.Object(bucketName, filename)\nobj.download_fileobj(buffer)\ndf = pd.read_parquet(buffer)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-10 14:15:51.003 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-03-10 18:08:07.52 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|boto3|amazon-sagemaker",
        "Question_view_count":280,
        "Owner_creation_date":"2019-03-31 19:27:07.013 UTC",
        "Owner_last_access_date":"2022-09-11 23:11:56.96 UTC",
        "Owner_reputation":2433,
        "Owner_up_votes":55,
        "Owner_down_votes":28,
        "Owner_views":228,
        "Answer_body":"<p>This solution is working for me.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\nimport pandas as pd\nimport io\nimport pyarrow\nimport fastparquet\n\ndef dynamically_read_filename_key(bucket, prefix='', suffix=''):\n    s3 = boto3\\\n    .client(\"s3\",\\\n            region_name=os.environ['AWS_DEFAULT_REGION'],\\\n            aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\\\n            aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'])\n    kwargs = {'Bucket': bucket}\n    if isinstance(prefix, str):\n        kwargs['Prefix'] = prefix\n    resp = s3\\\n    .list_objects_v2(**kwargs)\n    for obj in resp['Contents']:\n        key = obj['Key']\n    if key.startswith(prefix) and key.endswith(suffix):\n        return key\n\nfilename = \"\".join(i for i in dynamically_read_filename_key\\\n                   (bucket=\"my-bucket\",\\\n                    prefix=\"datasets\/\",\\\n                    suffix=\".parquet\"))\n\nbucket = \"my-bucket\"\n\ndef parquet_read_filename_key(bucket, filename):\n    client = boto3\\\n    .resource(\"s3\",\\\n            region_name=os.environ['AWS_DEFAULT_REGION'],\\\n            aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\\\n            aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'])\n    buffer = io.BytesIO()\n    obj = client.Object(bucket, filename)\n    obj.download_fileobj(buffer)\n    df = pd.read_parquet(buffer)\n    return df\n\ndf = parquet_read_filename_key(bucket, filename)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-03-10 18:06:36.403 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60619460",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55158307,
        "Question_title":"Can I make Amazon SageMaker deliver a recommendation based on historic data instead of a probability score?",
        "Question_body":"<p>We have a huge set of data in CSV format, containing a few numeric elements, like this:<\/p>\n\n<pre><code>Year,BinaryDigit,NumberToPredict,JustANumber, ...other stuff\n1954,1,762,16, ...other stuff\n1965,0,142,16, ...other stuff\n1977,1,172,16, ...other stuff\n<\/code><\/pre>\n\n<p>The thing here is that there is a strong correlation between the third column and the columns before that. So I have pre-processed the data and it's now available in a format I think is perfect:<\/p>\n\n<pre><code>1954,1,762\n1965,0,142\n1977,1,172\n<\/code><\/pre>\n\n<p>What I want is a predicition on the value in the third column, using the first two as input. So in the case above, I want the input 1965,0 to return 142. In real life this file is thousands of rows, but since there's a pattern, I'd like to retrieve the most possible value.<\/p>\n\n<p>So far I've setup a train job on the CSV file using the L<em>inear Learner<\/em> algorithm, with the following settings:<\/p>\n\n<pre><code>label_size = 1\nfeature_dim = 2\npredictor_type = regression\n<\/code><\/pre>\n\n<p>I've also created a model from it, and setup an endpoint. When I invoke it, I get a score in return.<\/p>\n\n<pre><code>    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                   ContentType='text\/csv',\n                                   Body=payload)\n<\/code><\/pre>\n\n<p>My goal here is to get the third column prediction instead. How can I achieve that? I have read a lot of the documentation regarding this, but since I'm not very familiar with AWS, I might as well have used the wrong algorithms for what I am trying to do.<\/p>\n\n<p>(Please feel free to edit this question to better suit AWS terminology)<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-03-14 08:50:55.71 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-03-18 07:54:13.107 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":271,
        "Owner_creation_date":"2014-09-23 09:30:41.6 UTC",
        "Owner_last_access_date":"2022-09-20 09:14:32.227 UTC",
        "Owner_reputation":205,
        "Owner_up_votes":49,
        "Owner_down_votes":0,
        "Owner_views":38,
        "Answer_body":"<p>For csv input, the label should be in the first column, as mentioned <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/cdf-training.html\" rel=\"nofollow noreferrer\">here<\/a>:  So you should preprocess your data to put the label (the column you want to predict) on the left.<\/p>\n\n<p>Next, you need to decide whether this is a regression problem or a classification problem. <\/p>\n\n<p>If you want to predict a number that's as close as possible to the true number, that's regression. For example, the truth might be 4, and the model might predict 4.15. If you need an integer prediction, you could round the model's output.<\/p>\n\n<p>If you want the prediction to be one of a few categories, then you have a classification problem. For example, we might encode 'North America' = 0, 'Europe' = 1, 'Africa' = 2, and so on. In this case, a fractional prediction wouldn't make sense. <\/p>\n\n<p>For regression, use <code>'predictor_type' = 'regressor'<\/code> and for classification with more than 2 classes, use <code>'predictor_type' = 'multiclass_classifier'<\/code> as documented <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>The output of regression will contain only a <code>'score'<\/code> field, which is the model's prediction. The output of multiclass classification will contain a <code>'predicted_label'<\/code> field, which is the model's prediction, as well as a <code>'score'<\/code> field, which is a vector of probabilities representing the model's confidence. The index with the highest probability will be the one that's predicted as the <code>'predicted_label'<\/code>. The output formats are documented <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/LL-in-formats.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-03-29 17:29:58.533 UTC",
        "Answer_score":2.0,
        "Owner_location":"\u00d6rebro, Sverige",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55158307",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65465114,
        "Question_title":"AWS Sagemaker AttributeError: can't set attribute error",
        "Question_body":"<p>I am new to python programming. Following the AWS learning path:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/?trk=el_a134p000003yWILAA2&amp;trkCampaign=DS_SageMaker_Tutorial&amp;sc_channel=el&amp;sc_campaign=Data_Scientist_Hands-on_Tutorial&amp;sc_outcome=Product_Marketing&amp;sc_geo=mult\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/?trk=el_a134p000003yWILAA2&amp;trkCampaign=DS_SageMaker_Tutorial&amp;sc_channel=el&amp;sc_campaign=Data_Scientist_Hands-on_Tutorial&amp;sc_outcome=Product_Marketing&amp;sc_geo=mult<\/a><\/p>\n<p>I am getting an error when excuting the following block (in conda_python3):<\/p>\n<pre><code>test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\nxgb_predictor.content_type = 'text\/csv' # set the data type for an inference\nxgb_predictor.serializer = csv_serializer # set the serializer type\npredictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\npredictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an \narray\nprint(predictions_array.shape)\n<\/code><\/pre>\n<blockquote>\n<p>AttributeError                            Traceback (most recent call last)\n in \n1 test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\n----&gt; 2 xgb_predictor.content_type = 'text\/csv' # set the data type for an inference\n3 xgb_predictor.serializer = csv_serializer # set the serializer type\n4 predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n5 predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array<\/p>\n<\/blockquote>\n<blockquote>\n<p>AttributeError: can't set attribute<\/p>\n<\/blockquote>\n<p>I have looked at several prior questions but couldn't find much information related to this error when it comes to creating data types.<\/p>\n<p>Thanks in advance for any help.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-12-27 11:11:37.07 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":2479,
        "Owner_creation_date":"2013-07-20 01:45:54.81 UTC",
        "Owner_last_access_date":"2022-09-25 05:27:32.8 UTC",
        "Owner_reputation":279,
        "Owner_up_votes":49,
        "Owner_down_votes":0,
        "Owner_views":93,
        "Answer_body":"<p>If you just remove it then the prediction will work. Therefore, recommend removing this code line.\nxgb_predictor.content_type = 'text\/csv'<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-12-31 07:17:48.573 UTC",
        "Answer_score":8.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65465114",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57661142,
        "Question_title":"AWS S3 and Sagemaker: No such file or directory",
        "Question_body":"<p>I have created an S3 bucket 'testshivaproject' and uploaded an image in it. When I try to access it in sagemaker notebook, it throws an error 'No such file or directory'.<\/p>\n\n<pre><code># import libraries\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np                                   \n\n# Define IAM role\nrole = get_execution_role()\n\nmy_region = boto3.session.Session().region_name # set the region of the instance\n\nprint(\"success :\"+my_region)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> success :us-east-2<\/p>\n\n<pre><code>role\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> 'arn:aws:iam::847047967498:role\/service-role\/AmazonSageMaker-ExecutionRole-20190825T121483'<\/p>\n\n<pre><code>bucket = 'testprojectshiva2' \ndata_key = 'ext_image6.jpg' \ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key) \nprint(data_location)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> s3:\/\/testprojectshiva2\/ext_image6.jpg<\/p>\n\n<pre><code>test = load_img(data_location)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> No such file or directory<\/p>\n\n<p>There are similar questions raised (<a href=\"https:\/\/stackoverflow.com\/questions\/48264656\/load-s3-data-into-aws-sagemaker-notebook\">Load S3 Data into AWS SageMaker Notebook<\/a>) but did not find any solution?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-26 15:49:42.57 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-s3|amazon-sagemaker",
        "Question_view_count":4622,
        "Owner_creation_date":"2015-09-20 06:42:05.233 UTC",
        "Owner_last_access_date":"2022-04-07 05:34:25.537 UTC",
        "Owner_reputation":187,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":"<p>Thanks for using Amazon SageMaker!<\/p>\n\n<p>I sort of guessed from your description, but are you trying to use the Keras load_img function to load images directly from your S3 bucket?<\/p>\n\n<p>Unfortunately, <a href=\"https:\/\/github.com\/keras-team\/keras\/issues\/11684\" rel=\"nofollow noreferrer\">the load_img function is designed to only load files from disk<\/a>, so passing an s3:\/\/ URL to that function will always return a <code>FileNotFoundError<\/code>.<\/p>\n\n<p>It's common to first download images from S3 before using them, so you can use boto3 or the AWS CLI to download the file before calling load_img.<\/p>\n\n<p><strong>Alternatively<\/strong>, since the load_img function simply creates a <a href=\"https:\/\/en.wikipedia.org\/wiki\/Python_Imaging_Library\" rel=\"nofollow noreferrer\">PIL Image<\/a> object, you can create the PIL object directly from the data in S3 using boto3, and not use the load_img function at all.<\/p>\n\n<p>In other words, you could do something like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from PIL import Image\n\ns3 = boto3.client('s3')\ntest = Image.open(BytesIO(\n    s3.get_object(Bucket=bucket, Key=data_key)['Body'].read()\n    ))\n<\/code><\/pre>\n\n<p>Hope this helps you out in your project!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-09-07 04:07:48.83 UTC",
        "Answer_score":2.0,
        "Owner_location":"Hyderabad",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57661142",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69046990,
        "Question_title":"How to pass dependency files to sagemaker SKLearnProcessor and use it in Pipeline?",
        "Question_body":"<p>I need to import function from different python scripts, which will used inside <code>preprocessing.py<\/code> file. I was not able to find a way to pass the dependent files to <code>SKLearnProcessor<\/code> Object, due to which I am getting <code>ModuleNotFoundError<\/code>.<\/p>\n<p><strong>Code:<\/strong><\/p>\n<pre><code>from sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     instance_type='ml.m5.xlarge',\n                                     instance_count=1)\n\n\nsklearn_processor.run(code='preprocessing.py',\n                      inputs=[ProcessingInput(\n                        source=input_data,\n                        destination='\/opt\/ml\/processing\/input')],\n                      outputs=[ProcessingOutput(output_name='train_data',\n                                                source='\/opt\/ml\/processing\/train'),\n                               ProcessingOutput(output_name='test_data',\n                                                source='\/opt\/ml\/processing\/test')],\n                      arguments=['--train-test-split-ratio', '0.2']\n                     )\n<\/code><\/pre>\n<p>I would like to pass,\n<code>dependent_files = ['file1.py', 'file2.py', 'requirements.txt']<\/code>. So, that <code>preprocessing.py<\/code> have access to all the dependent modules.<\/p>\n<p>And also need to install libraries from <code>requirements.txt<\/code> file.<\/p>\n<p>Can you share any work around or a right way to do this?<\/p>\n<p><strong>Update-25-11-2021:<\/strong><\/p>\n<p><strong>Q1.<\/strong>(Answered but looking to solve using <code>FrameworkProcessor<\/code>)<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/99f023e76a5db060907a796d4d8fee550f005844\/src\/sagemaker\/processing.py#L1426\" rel=\"noreferrer\">Here<\/a>, the <code>get_run_args<\/code> function, is handling <code>dependencies<\/code>, <code>source_dir<\/code> and <code>code<\/code> parameters by using <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/99f023e76a5db060907a796d4d8fee550f005844\/src\/sagemaker\/processing.py#L1265\" rel=\"noreferrer\">FrameworkProcessor<\/a>. Is there any way that we can set this parameters from <code>ScriptProcessor<\/code> or <code>SKLearnProcessor<\/code> or any other <code>Processor<\/code> to set them?<\/p>\n<p><strong>Q2.<\/strong><\/p>\n<p>Can you also please show some reference to use our <code>Processor<\/code> as <code>sagemaker.workflow.steps.ProcessingStep<\/code> and then use in <code>sagemaker.workflow.pipeline.Pipeline<\/code>?<\/p>\n<p>For having <code>Pipeline<\/code>, do we need <code>sagemaker-project<\/code> as mandatory or can we create <code>Pipeline<\/code> directly without any <code>Sagemaker-Project<\/code>?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":5,
        "Question_creation_date":"2021-09-03 14:59:45.26 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2021-11-26 14:18:30.43 UTC",
        "Question_score":11,
        "Question_tags":"python|amazon-web-services|scikit-learn|amazon-sagemaker",
        "Question_view_count":2139,
        "Owner_creation_date":"2017-07-23 15:35:48.41 UTC",
        "Owner_last_access_date":"2022-09-24 13:11:02.21 UTC",
        "Owner_reputation":4419,
        "Owner_up_votes":434,
        "Owner_down_votes":324,
        "Owner_views":962,
        "Answer_body":"<p>There are a couple of options for you to accomplish that.<\/p>\n<p>One that is really simple is adding all additional files to a folder, example:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 my_package\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 file1.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 file2.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 preprocessing.py\n<\/code><\/pre>\n<p>Then send this entire folder as another input under the same <code>\/opt\/ml\/processing\/input\/code\/<\/code>, example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor = SKLearnProcessor(\n    framework_version=&quot;0.20.0&quot;,\n    role=role,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    instance_count=1,\n)\n\nsklearn_processor.run(\n    code=&quot;preprocessing.py&quot;,  # &lt;- this gets uploaded as \/opt\/ml\/processing\/input\/code\/preprocessing.py\n    inputs=[\n        ProcessingInput(source=input_data, destination='\/opt\/ml\/processing\/input'),\n        # Send my_package as \/opt\/ml\/processing\/input\/code\/my_package\/\n        ProcessingInput(source='my_package\/', destination=&quot;\/opt\/ml\/processing\/input\/code\/my_package\/&quot;)\n    ],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],\n)\n<\/code><\/pre>\n<p>What happens is that <code>sagemaker-python-sdk<\/code> is going to put your argument <code>code=&quot;preprocessing.py&quot;<\/code> under <code>\/opt\/ml\/processing\/input\/code\/<\/code> and you will have <code>my_package\/<\/code> under the same directory.<\/p>\n<p><strong>Edit:<\/strong><\/p>\n<p>For the <code>requirements.txt<\/code>, you can add to your <code>preprocessing.py<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport subprocess\n\nsubprocess.check_call([\n    sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, &quot;-r&quot;,\n    &quot;\/opt\/ml\/processing\/input\/code\/my_package\/requirements.txt&quot;,\n])\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2021-11-24 19:39:22.627 UTC",
        "Answer_score":17.0,
        "Owner_location":"India",
        "Answer_last_edit_date":"2021-11-24 19:47:08.437 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69046990",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69962965,
        "Question_title":"How to get an AWS Feature Store feature group into the ACTIVE state?",
        "Question_body":"<p>I am trying to ingest some rows into a Feature Store on AWS using:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>feature_group.ingest(data_frame=df, max_workers=8, wait=True)\n<\/code><\/pre>\n<p>but I am getting the following error:<\/p>\n<blockquote>\n<p>Failed to ingest row 1: An error occurred (ValidationError) when\ncalling the PutRecord operation: Validation Error: FeatureGroup\n[feature-group] is not in ACTIVE state.<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-14 12:25:51.657 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|aws-glue|amazon-sagemaker|data-ingestion|aws-feature-store",
        "Question_view_count":383,
        "Owner_creation_date":"2013-03-10 11:22:30.047 UTC",
        "Owner_last_access_date":"2022-08-30 07:21:38.897 UTC",
        "Owner_reputation":2791,
        "Owner_up_votes":163,
        "Owner_down_votes":13,
        "Owner_views":174,
        "Answer_body":"<p>It turns out the status of a feature group after its creation is <code>Created<\/code> but before you can ingest any rows you need to simply wait until it's <code>Active<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>while status != 'Created':\n        try:\n            status = feature_group.describe()['OfflineStoreStatus']['Status']\n        except:\n            pass\n        print('Offline store status: {}'.format(status))    \n        sleep(15)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-11-15 12:39:44.023 UTC",
        "Answer_score":0.0,
        "Owner_location":"Tel Aviv",
        "Answer_last_edit_date":"2022-03-03 07:01:53.523 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69962965",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64246437,
        "Question_title":"How to increase AWS Sagemaker invocation time out while waiting for a response",
        "Question_body":"<p>I deployed a large 3D model to aws sagemaker. Inference will take 2 minutes or more. I get the following error while calling the predictor from Python:<\/p>\n<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from model with message &quot;Your invocation timed out while waiting for a response from container model. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.&quot;'\n<\/code><\/pre>\n<p>In Cloud Watch I also see some PING time outs while the container is processing:<\/p>\n<pre><code>2020-10-07T16:02:39.718+02:00 2020\/10\/07 14:02:39 https:\/\/forums.aws.amazon.com\/ 106#106: *251 upstream timed out (110: Connection timed out) while reading response header from upstream, client: 10.32.0.2, server: , request: &quot;GET \/ping HTTP\/1.1&quot;, upstream: &quot;http:\/\/unix:\/tmp\/gunicorn.sock\/ping&quot;, host: &quot;model.aws.local:8080&quot;\n<\/code><\/pre>\n<p>How do I increase the invocation time out?<\/p>\n<p>Or is there a way to make async invocations to an sagemaker endpoint?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-07 14:38:44.867 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2020-10-10 12:09:25.72 UTC",
        "Question_score":10,
        "Question_tags":"python|amazon-web-services|timeout|amazon-sagemaker|inference",
        "Question_view_count":8464,
        "Owner_creation_date":"2009-10-06 11:50:17.773 UTC",
        "Owner_last_access_date":"2022-09-23 20:34:01.687 UTC",
        "Owner_reputation":2595,
        "Owner_up_votes":462,
        "Owner_down_votes":5,
        "Owner_views":357,
        "Answer_body":"<p>It\u2019s currently not possible to increase timeout\u2014this is an open issue in GitHub. Looking through the issue and similar questions on SO, it seems like you may be able to use batch transforms in conjunction with inference.<\/p>\n<h1>References<\/h1>\n<p><a href=\"https:\/\/stackoverflow.com\/a\/55642675\/806876\">https:\/\/stackoverflow.com\/a\/55642675\/806876<\/a><\/p>\n<p>Sagemaker Python SDK timeout issue: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1119\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1119<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-10-10 12:08:26.57 UTC",
        "Answer_score":6.0,
        "Owner_location":"Germany",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64246437",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60892850,
        "Question_title":"Deepracer log_analysis tool - sagemaker role errors",
        "Question_body":"<p>I'm trying to run the Deepracer log analysis tool from <a href=\"https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb<\/a> on my local laptop. However I get below error while trying to run step [5] \"Create an IAM role\". <\/p>\n\n<pre><code>try:\n    sagemaker_role = sagemaker.get_execution_role()\nexcept:\n    sagemaker_role = get_execution_role('sagemaker')\n\nprint(\"Using Sagemaker IAM role arn: \\n{}\".format(sagemaker_role))\n\nCouldn't call 'get_role' to get Role ARN from role name arn:aws:iam::26********:root to get Role path.\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-5-3bea8175b8c7&gt; in &lt;module&gt;\n      1 try:\n----&gt; 2     sagemaker_role = sagemaker.get_execution_role()\n      3 except:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in get_execution_role(sagemaker_session)\n   3302     )\n-&gt; 3303     raise ValueError(message.format(arn))\n   3304 \n\nValueError: The current AWS identity is not a role: arn:aws:iam::26********:root, therefore it cannot be used as a SageMaker execution role\n\nDuring handling of the above exception, another exception occurred:\n\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-5-3bea8175b8c7&gt; in &lt;module&gt;\n      2     sagemaker_role = sagemaker.get_execution_role()\n      3 except:\n----&gt; 4     sagemaker_role = get_execution_role('sagemaker')\n      5 \n      6 print(\"Using Sagemaker IAM role arn: \\n{}\".format(sagemaker_role))\n\nNameError: name 'get_execution_role' is not defined\n<\/code><\/pre>\n\n<p>Does anybody know what needs to be done to execute above code without errors? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-27 19:23:39.477 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|log-analysis",
        "Question_view_count":491,
        "Owner_creation_date":"2014-07-01 15:16:03.68 UTC",
        "Owner_last_access_date":"2022-08-03 13:15:26.86 UTC",
        "Owner_reputation":1807,
        "Owner_up_votes":303,
        "Owner_down_votes":2,
        "Owner_views":207,
        "Answer_body":"<p>AWS support recommended below solution:<\/p>\n\n<p>This seems to be a known issue when executing the code locally, as mentioned in the following Github issue [3]. A work-around to fix the issue is also defined in that issue [3] and can be referred to using the following link: aws\/sagemaker-python-sdk#300 (comment)<\/p>\n\n<p>The steps in the work-around given in the above link are:<\/p>\n\n<ol>\n<li><p>Login to the AWS console -> IAM -> Roles -> Create Role<\/p><\/li>\n<li><p>Create an IAM role and select the \"SageMaker\" service<\/p><\/li>\n<li><p>Give the role \"AmazonSageMakerFullAccess\" permission<\/p><\/li>\n<li><p>Review and create the role<\/p><\/li>\n<li><p>Next, also attach the \"AWSRoboMakerFullAccess\" permission policy to the above created role (as required in the Github notebook [1]).<\/p><\/li>\n<li><p>The original code would then need to be modified to fetch the IAM role directly when the code is executed on a local machine. The code snippet to be used is given below:<\/p><\/li>\n<\/ol>\n\n<pre><code>try:\n   sagemaker_role = sagemaker.get_execution_role()\n except ValueError:\n   iam = boto3.client('iam')\n   sagemaker_role = iam.get_role(RoleName='&lt;sagemaker-IAM-role-name&gt;')['Role']['Arn']\n<\/code><\/pre>\n\n<p>In the above snippet, replace the \"\" text with the IAM role name created in Step 4.<\/p>\n\n<p>References:<\/p>\n\n<p>[1] <a href=\"https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb<\/a><\/p>\n\n<p>[2] <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-role.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-role.html<\/a><\/p>\n\n<p>[3] aws\/sagemaker-python-sdk#300<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-04-01 17:00:37.23 UTC",
        "Answer_score":1.0,
        "Owner_location":"New York, NY, USA",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60892850",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52762367,
        "Question_title":"Re-hosting a trained model on AWS SageMaker",
        "Question_body":"<p>I have started exploring AWS SageMaker starting with these <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/blazingtext_word2vec_subwords_text8\" rel=\"nofollow noreferrer\">examples provided by AWS<\/a>. I then made some modifications to this particular setup so that it uses the data from my use case for training.<\/p>\n\n<p>Now, as I continue to work on this model and tuning, after I delete the inference endpoint once, I would like to be able to recreate the same endpoint -- even after stopping and restarting the notebook instance (so the notebook \/ kernel session is no longer valid) -- using the already trained model artifacts that gets uploaded to S3 under \/output folder.<\/p>\n\n<p>Now I cannot simply jump directly to this line of code:<\/p>\n\n<pre><code>bt_endpoint = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>I did some searching -- including <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/blazingtext_hosting_pretrained_fasttext\" rel=\"nofollow noreferrer\">amazon's own example of hosting pre-trained models<\/a>, but I am a little lost. I would appreciate any guidance, examples, or documentation that I could emulate and adapt to my case.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2018-10-11 14:15:43.403 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-10-15 21:05:45.79 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":1916,
        "Owner_creation_date":"2015-05-21 03:34:16.93 UTC",
        "Owner_last_access_date":"2022-08-28 06:46:14.863 UTC",
        "Owner_reputation":17,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":"<p>Your comment is correct - you can re-create an Endpoint given an existing EndpointConfiguration. This can be done via the console, the AWS CLI, or the SageMaker boto client.<\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-endpoint.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-endpoint.html<\/a><\/li>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_endpoint\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_endpoint<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-10-15 16:57:52.603 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52762367",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70873792,
        "Question_title":"How to handle Sagemaker Batch Transform discarding a file with a failed model request",
        "Question_body":"<p>I have a large number of JSON requests for a model split across multiple files in an S3 bucket. I would like to use Sagemaker's Batch Transform feature to process all of these requests (I have done a couple of test runs using small amounts of data and the transform job succeeds). My main issue is here (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors<\/a>), specifically:<\/p>\n<blockquote>\n<p>If a batch transform job fails to process an input file because of a problem with the dataset, SageMaker marks the job as failed. If an input file contains a bad record, the transform job doesn't create an output file for that input file because doing so prevents it from maintaining the same order in the transformed data as in the input file. When your dataset has multiple input files, a transform job continues to process input files even if it fails to process one. The processed files still generate useable results.<\/p>\n<\/blockquote>\n<p>This is not preferable mainly because if 1 request fails (whether its a transient error, a malformmated request, or something wrong with the model container) in a file with a large number of requests, all of those requests will get discarded (even if all of them succeeded and the last one failed). I would ideally prefer Sagemaker to just write the output of the failed response to the file and keep going, rather than discarding the entire file.<\/p>\n<p>My question is, are there any suggestions to mitigating this issue? I was thinking about storing 1 request per file in S3, but this seems somewhat ridiculous? Even if I did this, is there a good way of seeing which requests specifically failed after the transform job finishes?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-27 05:32:20.133 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":357,
        "Owner_creation_date":"2020-08-19 17:31:55.077 UTC",
        "Owner_last_access_date":"2022-09-23 23:40:01.1 UTC",
        "Owner_reputation":84,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":"<p>You've got the right idea: the fewer datapoints are in each file, the less likely a given file is to fail. The issue is that while you can pass a prefix with many files to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB\" rel=\"nofollow noreferrer\">CreateTransformJob<\/a>, partitioning one datapoint per file at least requires an S3 read per datapoint, plus a model invocation per datapoint, which is probably not great. Be aware also that <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=1000415&amp;tstart=0\" rel=\"nofollow noreferrer\">apparently there are hidden rate limits<\/a>.<\/p>\n<p>Here are a couple options:<\/p>\n<ol>\n<li><p>Partition into small-ish files, and plan on failures being rare. Hopefully, not many of your datapoints would actually fail. If you partition your dataset into e.g. 100 files, then a single failure only requires reprocessing 1% of your data. Note that Sagemaker has built-in retries, too, so most of the time failures should be caused by your data\/logic, not randomness on Sagemaker's side.<\/p>\n<\/li>\n<li><p>Deal with failures directly in your model. The same doc you quoted in your question also says:<\/p>\n<\/li>\n<\/ol>\n<blockquote>\n<p>If you are using your own algorithms, you can use placeholder text, such as ERROR, when the algorithm finds a bad record in an input file. For example, if the last record in a dataset is bad, the algorithm places the placeholder text for that record in the output file.<\/p>\n<\/blockquote>\n<p>Note that the reason Batch Transform does this whole-file failure is to maintain a 1-1 mapping between rows in the input and the output. If you can substitute the output for failed datapoints with an error message from inside your model, without actually causing the model itself to fail processing, Batch Transform will be happy.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-02-09 16:34:59.01 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2022-02-10 21:26:49.28 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70873792",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54825390,
        "Question_title":"create aws sagemker endpoint with lambda function",
        "Question_body":"<p>I created an endpoint in <code>aws sagemaker<\/code> and it works well, I created a <code>lambda<\/code> function(<code>python3.6<\/code>) that takes files from <code>S3<\/code>, invoke the endpoint and then put the output in a file in <code>S3<\/code>. <\/p>\n\n<p>I wonder if I can create the endpoint at every event(a file uploaded in an <code>s3 bucket)<\/code> and then delete the endpoint <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-02-22 10:47:04.633 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-02-22 13:00:59.833 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":180,
        "Owner_creation_date":"2017-08-06 09:14:59.81 UTC",
        "Owner_last_access_date":"2019-11-15 11:59:17.507 UTC",
        "Owner_reputation":109,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":"<p>Yes you can Using <code>S3<\/code> event notification for object-created and call a <code>lambda<\/code> for creating endpoint for <code>sagemaker<\/code>.<\/p>\n\n<p>This example shows how to make <code>object-created event trigger lambda<\/code><\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-s3.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-s3.html<\/a><\/p>\n\n<p>You can use <code>python sdk<\/code> to create endpoint for <code>sagemaker<\/code><\/p>\n\n<p><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_endpoint\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_endpoint<\/a><\/p>\n\n<p>But it might be slow for creating endpoint so you may be need to wait.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-02-22 12:00:36.773 UTC",
        "Answer_score":0.0,
        "Owner_location":"Tunis, Tunisia",
        "Answer_last_edit_date":"2019-02-22 12:04:43.23 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54825390",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73663585,
        "Question_title":"Add Security groups in Amazon SageMaker for distributed training jobs",
        "Question_body":"<p>We would like to enforce specific security groups to be set on the SageMaker training jobs (XGBoost in script mode).\nHowever, distributed training, in this case, won\u2019t work out of the box, since the containers need to communicate with each other. What are the minimum inbound\/outbound rules (ports) that we need to specify for training jobs so that they can communicate?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-09 14:20:16.593 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|xgboost|amazon-sagemaker|distributed-training",
        "Question_view_count":19,
        "Owner_creation_date":"2022-09-08 07:14:26.503 UTC",
        "Owner_last_access_date":"2022-09-23 21:03:19.637 UTC",
        "Owner_reputation":48,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>setting up training in VPC including specifying security groups is documented here:\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-vpc.html#train-vpc-groups\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-vpc.html#train-vpc-groups<\/a><\/p>\n<p>Normally you would allow all communication between the training nodes. To do this you specify the security group source and destination to the name of the security group itself, and allow all IPv4 traffic. If you want to figure out what ports are used, you could: 1\/ define the permissive security group. 2\/ Turn on VPC flow logs 3\/ run training. 4\/ examine VPC Flow logs 5\/ update the security group only to the required ports.<\/p>\n<p>I must say restricting communication between the training nodes might be an extreme, so I would challenge the customer why it's really needed, as all nodes carry the same job, have the same IAM role, and are transiate by nature.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-10 18:36:59.253 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73663585",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51391639,
        "Question_title":"Is it possible to predict in sagemaker without using s3",
        "Question_body":"<p>I have a .pkl which I would like to put into production. I would like to do a daily query of my SQL server and do a prediction on about 1000 rows. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html\" rel=\"nofollow noreferrer\">documentation<\/a> implies I have to load the daily data into s3. Is there a way around this? It should be able to fit in memory no problem. <\/p>\n\n<p>The answer to \" <a href=\"https:\/\/stackoverflow.com\/questions\/48319893\/is-there-some-kind-of-persistent-local-storage-in-aws-sagemaker-model-training\">is there some kind of persistent local storage in aws sagemaker model training?<\/a> \" says that \"<em>The notebook instance is coming with a local EBS (5GB) that you can use to copy some data into it and run the fast development iterations without copying the data every time from S3.<\/em>\" The 5GB could be enough but I am not sure you can actually run from a notebook in this manner. If I set up a VPN could I just query using pyodbc?<\/p>\n\n<p>Is there sagemaker integration with AWS Lambda? That in combination with a docker container would suit my needs.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2018-07-17 23:46:56.253 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-07-18 05:51:18.607 UTC",
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-s3|aws-lambda|amazon-sagemaker",
        "Question_view_count":1576,
        "Owner_creation_date":"2014-05-17 08:37:27.157 UTC",
        "Owner_last_access_date":"2022-09-21 19:31:38.353 UTC",
        "Owner_reputation":4342,
        "Owner_up_votes":318,
        "Owner_down_votes":25,
        "Owner_views":315,
        "Answer_body":"<p>While you need to to specify a s3 \"folder\" as input, this folder can contain only a dummy file. \nAlso if you bring your own docker container for training like in <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">this example<\/a>, you can do pretty much everthing in it. So you could do your daily query inside your docker container, because they have access to the internet. <\/p>\n\n<p>Also inside this container you have access to all the other aws services. Your access is defined by the role you're passing to your training job.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-07-18 13:07:32.07 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51391639",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72203674,
        "Question_title":"Deploy AWS SageMaker pipeline using the cloud development kit (CDK)",
        "Question_body":"<p>I'm looking to deploy the SageMaker pipeline using CDK (<a href=\"https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html<\/a>) but could not find any code examples. Any pointers?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-11 15:25:28.443 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"terraform-provider-aws|aws-cdk|amazon-sagemaker",
        "Question_view_count":237,
        "Owner_creation_date":"2017-03-16 17:36:25.577 UTC",
        "Owner_last_access_date":"2022-09-22 21:22:01.683 UTC",
        "Owner_reputation":65,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Answer_body":"<p>CDK L1 Constructs correspond 1:1 to a CloudFormation resource of the same name. The construct props match the resouce properties.  The go-to source is therefore the CloudFormation docs.<\/p>\n<p>The <code>AWS::SageMaker::Pipeline<\/code> <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html#aws-resource-sagemaker-pipeline--examples\" rel=\"nofollow noreferrer\">docs have a more complete example<\/a>.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2022-05-11 15:43:32.907 UTC",
        "Answer_score":0.0,
        "Owner_location":"Canada",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72203674",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57811873,
        "Question_title":"Can H2o AutoML benefit from a GPU instance on Sagemaker platform?",
        "Question_body":"<p>I'm running some projects with H2o AutoML using Sagemaker notebook instances, and I would like to know if H2o AutoML can benefit from a GPU Sagemaker instance, if so, how should I configure the notebook? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_date":"2019-09-05 19:25:33.23 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"gpu|h2o|amazon-sagemaker|automl",
        "Question_view_count":462,
        "Owner_creation_date":"2017-10-26 10:07:59.113 UTC",
        "Owner_last_access_date":"2021-09-21 19:53:06.417 UTC",
        "Owner_reputation":97,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":"<p><a href=\"http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html\" rel=\"nofollow noreferrer\">H2O AutoML<\/a> contains a handful of algorithms and one of them is <a href=\"http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-science\/xgboost.html\" rel=\"nofollow noreferrer\">XGBoost<\/a>, which has been part of H2O AutoML since H2O version 3.22.0.1.  XGBoost is the only GPU-capable algorithm inside of H2O AutoML, however, a lot of the models that are trained in AutoML are XGBoost models, so it still can be useful to utilize a GPU. Keep in mind that you must use H2O 3.22 or above to use this feature.<\/p>\n\n<p>My suggestion is to test it on a GPU-enabled instance and compare the results to a non-GPU instance and see if it's worth the extra cost.  <\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2019-09-05 23:49:07.09 UTC",
        "Answer_score":1.0,
        "Owner_location":"Belo Horizonte, MG, Brasil",
        "Answer_last_edit_date":"2019-09-06 05:30:43.127 UTC",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57811873",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68741326,
        "Question_title":"Sagemaker Instance not utilising GPU during training",
        "Question_body":"<p>I'm training a Seq2Seq model on Tensorflow on a ml.p3.2xlarge instance. When I tried running the code on google colab, the time per epoch was around 40 mins. However on the instance it's around 5 hours!<\/p>\n<p>This is my training code<\/p>\n<pre><code>def train_model(train_translator, dataset, path, num=8):\n\n  with tf.device(&quot;\/GPU:0&quot;):\n    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=path,\n                                                save_weights_only=True,\n                                                 verbose=1)\n    batch_loss = BatchLogs('batch_loss')\n    train_translator.fit(dataset, epochs=num,callbacks=[batch_loss,cp_callback])  \n\n  return train_translator\n<\/code><\/pre>\n<p>I have also tried without the <code>tf.device<\/code> command and I still get the same timing. Am I doing something wrong?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-11 11:41:55.84 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python-3.x|amazon-web-services|machine-learning|tensorflow2.0|amazon-sagemaker",
        "Question_view_count":995,
        "Owner_creation_date":"2020-01-31 12:10:38.26 UTC",
        "Owner_last_access_date":"2022-09-17 14:56:26.317 UTC",
        "Owner_reputation":127,
        "Owner_up_votes":282,
        "Owner_down_votes":1,
        "Owner_views":32,
        "Answer_body":"<p>I had to force GPU use with the help of<\/p>\n<pre><code>with tf.device('\/device:GPU:0')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-08-14 05:07:36.377 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68741326",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65577286,
        "Question_title":"Dlr model gives notorious result for object detection model",
        "Question_body":"<p>I am following this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_neo_compilation_jobs\/gluoncv_ssd_mobilenet\/gluoncv_ssd_mobilenet_neo.ipynb\" rel=\"nofollow noreferrer\">link<\/a> to train an object detection model. I am able to successfully deploy the model on EC2 instance. The accuracy was good. I complied the same model file for m edge Device Jetson Nano. My inference code looks like below,<\/p>\n<pre><code>from dlr import DLRModel\nimport json\nimport cv2\n\nmodel = DLRModel('model', 'gpu')\n\nimg = cv2.imread('test.jpg')\nimg = cv2.resize(img, (512, 512), interpolation=cv2.INTER_AREA)\nimg = np.expand_dims(img, 0)\n\noutputs = model.run(img)\nobjects=outputs[0][0]\nscores=outputs[1][0]\nbounding_boxes=outputs[2][0]\n<\/code><\/pre>\n<p>When I look the result, it's not at all matched with SageMaker Notebook instance result. Boudning boxes' values are sometime in ~70000. I couldn't understand the format of result produced by DLR.<\/p>\n<p>Sample result for an image.<\/p>\n<p>Classes:\n[[10.0], [14.0], [4.0], [10.0], [14.0], [-1.0], [-1.0], [-1.0], [7.0], [-1.0], [6.0], [11.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [2.0], [-1.0], [-1.0], [0.0], [-1.0], [17.0], [-1.0], [-1.0], [6.0], [18.0], [-1.0], [-1.0], [-1.0], [18.0], [-1.0], [12.0], [-1.0], [-1.0], [13.0], [-1.0], [-1.0], [-1.0], [1.0], [-1.0], [-1.0], [5.0], [-1.0], [0.0], [-1.0], [-1.0], [-1.0], [-1.0], [3.0], [8.0], [5.0], [-1.0], [-1.0], [15.0], [-1.0], [9.0], [3.0], [-1.0], [10.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [16.0], [8.0], [-1.0], [16.0], [19.0], [-1.0], [9.0], [-1.0], [4.0], [-1.0], [-1.0], [15.0], [10.0], [-1.0], [-1.0], [4.0], [-1.0], [8.0], [2.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0]]<\/p>\n<p>scores:\n[[0.9527158737182617], [0.910746157169342], [0.28013306856155396], [0.059000786393880844], [0.04898739233613014], [-1.0], [-1.0], [-1.0], [0.04864144325256348], [-1.0], [0.04847110062837601], [0.04843416064977646], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [0.04821664094924927], [-1.0], [-1.0], [0.04808368161320686], [-1.0], [0.0479729063808918], [-1.0], [-1.0], [0.04782549664378166], [0.04778601974248886], [-1.0], [-1.0], [-1.0], [0.0475776381790638], [-1.0], [0.047538403421640396], [-1.0], [-1.0], [0.047468967735767365], [-1.0], [-1.0], [-1.0], [0.04737424850463867], [-1.0], [-1.0], [0.047330085188150406], [-1.0], [0.04730956256389618], [-1.0], [-1.0], [-1.0], [-1.0], [0.04710235074162483], [0.04710135608911514], [0.047083333134651184], [-1.0], [-1.0], [0.047033149749040604], [-1.0], [0.0469636432826519], [0.046939317137002945], [-1.0], [0.04687687009572983], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [0.046708278357982635], [0.046680934727191925], [-1.0], [0.04660974070429802], [0.046597886830568314], [-1.0], [0.04656397923827171], [-1.0], [0.046513814479112625], [-1.0], [-1.0], [0.04647510126233101], [0.04644943028688431], [-1.0], [-1.0], [0.046245038509368896], [-1.0], [0.01647786796092987], [0.010514501482248306], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0]]<\/p>\n<p>Bouding boxes:\n[[523.2319946289062, -777751.875, 523.5722045898438, 778992.625], [425.0546875, -1141093.0, 429.6099853515625, 1142524.5], [680.2073364257812, 542.9566650390625, 680.2202758789062, 543.36376953125], [425.0546875, -1141093.0, 429.6099853515625, 1142524.5], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [591.7652587890625, -1121890.0, 592.9493408203125, 1123299.75], [523.2319946289062, -777751.875, 523.5722045898438, 778992.625], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0]]<\/p>\n<p>What causes this notorious result?\nIs there any issue while compiling model in Neo?\nAny issue in inference Code?<\/p>\n<p>Any hint would be appreciable.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-05 10:39:53.24 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-ec2|amazon-sagemaker|mxnet",
        "Question_view_count":98,
        "Owner_creation_date":"2015-03-18 10:49:38.223 UTC",
        "Owner_last_access_date":"2022-09-24 14:02:18.917 UTC",
        "Owner_reputation":10189,
        "Owner_up_votes":1483,
        "Owner_down_votes":261,
        "Owner_views":1471,
        "Answer_body":"<p>The reason behind the abnormal result is due to improper pre-processing method was applied. Here is the complete inference code for Mobilenet-ssd model.<\/p>\n<pre><code>def transform(img):\n    # Normalize\n    mean_vec = np.array([0.485, 0.456, 0.406])\n    stddev_vec = np.array([0.229, 0.224, 0.225])\n    image = (img \/ 255 - mean_vec) \/ stddev_vec\n\n    # Transpose\n    if len(image.shape) == 2:  # for greyscale image\n        image = np.expand_dims(image, axis=2)\n\n    image = np.rollaxis(image, axis=2, start=0)[np.newaxis, :]\n    return image\n\nmodel = DLRModel(model_dir, 'gpu')\n\n\nfor file_name in image_folder:\n    image = PIL.Image.open(file_name)\n    image = np.asarray(image.resize((512, 512)))\n    image = transform(image)\n    # flatten within a input array\n    input_data = {'data': image}\n    outputs = model.run(input_data)\n    objects = outputs[0]\n    scores = outputs[1]\n    bounding_boxes = outputs[2]\n    result = [objects.tolist(), scores.tolist(), bounding_boxes.tolist()]\n    print(json.dumps(result))\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-01-06 05:53:03.447 UTC",
        "Answer_score":0.0,
        "Owner_location":"Coimbatore, Tamil Nadu, India",
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65577286",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":47840209,
        "Question_title":"How to Curl an Amazon Sagemaker Endpoint",
        "Question_body":"<p>What is a is the curl command to make a POST request to sage-maker and receive a ML inference?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":1,
        "Question_creation_date":"2017-12-15 21:39:29.9 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":null,
        "Question_score":11,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":6889,
        "Owner_creation_date":"2015-09-25 17:16:18.36 UTC",
        "Owner_last_access_date":"2022-05-26 17:59:10.97 UTC",
        "Owner_reputation":749,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Answer_body":"<p>Rather than using curl, it's recommended that you use the SageMaker Runtime client to send data and get back inferences from a SageMaker Endpoint:<\/p>\n\n<p><a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html\" rel=\"nofollow noreferrer\">http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2017-12-26 22:15:23.573 UTC",
        "Answer_score":4.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47840209",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59648275,
        "Question_title":"What to define as entrypoint when initializing a pytorch estimator with a custom docker image for training on AWS Sagemaker?",
        "Question_body":"<p>So I created a docker image for training. In the dockerfile I have an entrypoint defined such that when <code>docker run<\/code> is executed, it will start running my python code.\nTo use this on aws sagemaker in my understanding I need to create a pytorch estimator in a jupyter notebook in sagemaker. I tried something like this:<\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker.pytorch import PyTorch\n\nsagemaker_session = sagemaker.Session()\n\nrole = sagemaker.get_execution_role()\n\nestimator = PyTorch(entry_point='train.py',\n                    role=role,\n                    framework_version='1.3.1',\n                    image_name='xxx.ecr.eu-west-1.amazonaws.com\/xxx:latest',\n                    train_instance_count=1,\n                    train_instance_type='ml.p3.xlarge',\n                    hyperparameters={})\n\nestimator.fit({})\n\n<\/code><\/pre>\n\n<p>In the documentation I found that as image name I can specify the link the my docker image on aws ecr. When I try to execute this it keeps complaining<\/p>\n\n<pre><code>[Errno 2] No such file or directory: 'train.py'\n<\/code><\/pre>\n\n<p>It complains immidiatly, so surely I am doing something completely wrong. I would expect that first my docker image should run, and than it could find out that the entry point does not exist.<\/p>\n\n<p>But besides this, why do I need to specify an entry point, as in, should it not be clear that the entry to my training is simply <code>docker run<\/code>?<\/p>\n\n<p>For maybe better understanding. The entrypoint python file in my docker image looks like this:<\/p>\n\n<pre><code>if __name__=='__main__':\n    parser = argparse.ArgumentParser()\n\n    # Hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=5)\n    parser.add_argument('--batch_size', type=int, default=16)\n    parser.add_argument('--learning_rate', type=float, default=0.0001)\n\n    # Data and output directories\n    parser.add_argument('--output_data_dir', type=str, default=os.environ['OUTPUT_DATA_DIR'])\n    parser.add_argument('--train_data_path', type=str, default=os.environ['CHANNEL_TRAIN'])\n    parser.add_argument('--valid_data_path', type=str, default=os.environ['CHANNEL_VALID'])\n\n    # Start training\n    ...\n<\/code><\/pre>\n\n<p>Later I would like to specify the hyperparameters and data channels. But for now I simply do not understand what to put as entry point. In the documentation it says that the entrypoint is required and it should be a local\/global path to the entrypoint...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2020-01-08 14:44:39.74 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-01-08 15:06:30.75 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|docker|pytorch|amazon-sagemaker|entry-point",
        "Question_view_count":768,
        "Owner_creation_date":"2015-11-10 10:27:50.223 UTC",
        "Owner_last_access_date":"2022-09-22 14:28:56.433 UTC",
        "Owner_reputation":141,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":"<p>If you really would like to use a complete separate by yourself build docker image, you should create an Amazon Sagemaker algorithm (which is one of the options in the Sagemaker menu). Here you have to specify a link to your docker image on amazon ECR as well as the input parameters and data channels etc. When choosing this options, you should <strong>not<\/strong> use the PyTorch estimater but the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/algorithm.html\" rel=\"nofollow noreferrer\">Algoritm estimater<\/a>. This way you indeed don't have to specify an entrypoint because it simple runs the docker when training and the default entrypoint can be defined in your docker file.<\/p>\n\n<p>The Pytorch estimator can be used when having you own model code, but you would like to run this code in an off-the-shelf Sagemaker PyTorch docker image. This is why you have to for example specify the PyTorch framework version. In this case the entrypoint file by default should be placed next to where your jupyter notebook is stored (just upload the file by clicking on the upload button). The PyTorch estimator inherits all options from the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html#sagemaker.estimator.Framework\" rel=\"nofollow noreferrer\">framework estimator<\/a> where options can be found where to place the entrypoint and model, for example <em>source_dir<\/em>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-01-17 14:08:07.087 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59648275",
        "Question_exclusive_tag":"Amazon SageMaker"
    }
]