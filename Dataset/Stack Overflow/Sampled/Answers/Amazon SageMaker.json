[
    {
        "Question_id":52847777,
        "Question_title":"Customer Error: Additional hyperparameters are not allowed - Image classification training- Sagemaker",
        "Question_body":"<p>I'm learning image classification with Amazon SageMaker. I was trying to follow their  learning demo <strong>Image classification transfer learning demo<\/strong> (<code>Image-classification-transfer-learning-highlevel.ipynb<\/code>)<\/p>\n\n<p>I got up to Start the training. Executed below.<\/p>\n\n<pre><code>ic.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n\n<p>Set the hyper parameters as given in the demo<\/p>\n\n<pre><code>ic.set_hyperparameters(num_layers=18,\n                             use_pretrained_model=1,\n                             image_shape = \"3,224,224\",\n                             num_classes=257,\n                             num_training_samples=15420,\n                             mini_batch_size=128,\n                             epochs=1,\n                             learning_rate=0.01,\n                             precission_dtype='float32')\n<\/code><\/pre>\n\n<p>Got the client error<\/p>\n\n<pre><code>ERROR 140291262150464] Customer Error: Additional hyperparameters are not allowed (u'precission_dtype' was unexpected) (caused by ValidationError)\n\nCaused by: Additional properties are not allowed (u'precission_dtype' was unexpected)\n<\/code><\/pre>\n\n<p>Does anyone know how to overcome this? I'm also reporting this to aws support. Posting here for sharing and get a fix. Thanks !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1539753916957,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|jupyter-notebook|classification|amazon-sagemaker",
        "Question_view_count":146,
        "Owner_creation_time":1403185902747,
        "Owner_last_access_time":1547107848667,
        "Owner_location":"Colombo, Sri Lanka",
        "Owner_reputation":169,
        "Owner_up_votes":215,
        "Owner_down_votes":0,
        "Owner_views":81,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Assuming you are referring to <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb<\/a> - you have a typo, it's <code>precision_dtype<\/code>, not <code>precission_dtype<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1539831993640,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52847777",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71827884,
        "Question_title":"create sagemaker notebook instance via Terraform",
        "Question_body":"<p>I am taking my first steps into the Terraform world so please be gentle with me. I have a user with AmazonSageMakerFullAccess, which I stored via AWS CLI in a profile called terraform. I can create an S3 bucket as follows no problem referring this user in Windows in VSC:<\/p>\n<pre><code>provider &quot;aws&quot; {\n    region = &quot;eu-west-2&quot;\n    shared_credentials_files = [&quot;C:\\\\Users\\\\amazinguser\\\\.aws\\\\credentials&quot;]\n    profile = &quot;terraform&quot;\n}\n\nresource &quot;aws_s3_bucket&quot; &quot;b&quot; {\n  bucket = &quot;blabla-test-bucket&quot;\n\n  tags = {\n    Name        = &quot;amazing_tag&quot;\n    Environment = &quot;dev&quot;\n  }\n}\n<\/code><\/pre>\n<p>I try to implement <a href=\"https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\/tree\/main\/Part%202a%20-%20Creating%20a%20SageMaker%20Notebook\" rel=\"nofollow noreferrer\">this<\/a> documented <a href=\"https:\/\/towardsdatascience.com\/terraform-sagemaker-part-2a-creating-a-custom-sagemaker-notebook-instance-1d68c90b192b\" rel=\"nofollow noreferrer\">here<\/a> and try to this:<\/p>\n<pre><code>resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot; {\n  name = &quot;titanic-sagemaker-byoc-notebook&quot;\n  role_arn = aws_iam_role.notebook_iam_role.arn\n  instance_type = &quot;ml.t2.medium&quot;\n  #lifecycle_config_name = aws_sagemaker_notebook_instance_lifecycle_configuration.notebook_config.name\n  #default_code_repository = aws_sagemaker_code_repository.git_repo.code_repository_name\n}\n<\/code><\/pre>\n<p>I am a bit confused about the role_arn which is defined here:<\/p>\n<p><a href=\"https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\/blob\/main\/Part%202a%20-%20Creating%20a%20SageMaker%20Notebook\/terraform\/iam.tf\" rel=\"nofollow noreferrer\">https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\/blob\/main\/Part%202a%20-%20Creating%20a%20SageMaker%20Notebook\/terraform\/iam.tf<\/a><\/p>\n<p>Can I not use the above user? Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_time":1649680247017,
        "Question_score":0,
        "Question_tags":"amazon-web-services|terraform|amazon-sagemaker",
        "Question_view_count":338,
        "Owner_creation_time":1267440784443,
        "Owner_last_access_time":1664045779313,
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Question_last_edit_time":null,
        "Answer_body":"<p>AWS services trying to call other AWS services and perform actions are not allowed to do so by default. For example, SageMaker Notebooks are basically EC2 instances. In order for SageMaker to create EC2 instances, it has to have a policy which allows e.g., injecting ENIs to a VPC. Since you probably do not want to do all that by yourself (it is a managed Notebook service after all), you have to give SageMaker permissions to perform actions on your behalf. Enter <strong>execution roles<\/strong>. For SageMaker, you can read more in [1]. Other services that you will commonly find using execution roles are Lambda, ECS and many others. An IAM role usually consists of two parts:<\/p>\n<ol>\n<li>Trust relationship (I like to call it trust policy)<\/li>\n<li>Permissions policy<\/li>\n<\/ol>\n<p>The first one decides which principal (AWS identifier, Service etc. [2]) will be able to assume the role. In your example, that is:<\/p>\n<pre><code>data &quot;aws_iam_policy_document&quot; &quot;sm_assume_role_policy&quot; {\n  statement {\n    actions = [&quot;sts:AssumeRole&quot;]\n    \n    principals {\n      type = &quot;Service&quot;\n      identifiers = [&quot;sagemaker.amazonaws.com&quot;]\n    }\n  }\n}\n<\/code><\/pre>\n<p>What this policy says is &quot;I am going to allow SageMaker (which is of type <code>Service<\/code>) to assume any role to which this policy is attached and perform actions that are defined in the permissions policy&quot;. The permissions policy is:<\/p>\n<pre><code># Attaching the AWS default policy, &quot;AmazonSageMakerFullAccess&quot;\nresource &quot;aws_iam_policy_attachment&quot; &quot;sm_full_access_attach&quot; {\n  name = &quot;sm-full-access-attachment&quot;\n  roles = [aws_iam_role.notebook_iam_role.name]\n  policy_arn = &quot;arn:aws:iam::aws:policy\/AmazonSageMakerFullAccess&quot;\n}\n<\/code><\/pre>\n<p>Without going into too much details about what the AWS managed policy for SageMaker does, it is enough to see the <code>FullAccess<\/code> part for it to be clear. What you could do if you want to be extra careful is to define a customer managed policy [3] for SageMaker notebooks. This permissions policy will be attached to the IAM role(s) defined in the <code>roles<\/code> argument. Note that it is a list, so multiple roles can have the same permissions policy attached.<\/p>\n<p>Last, but not the least, the glue between the trust and permissions policy is the role itself:<\/p>\n<pre><code>resource &quot;aws_iam_role&quot; &quot;notebook_iam_role&quot; {\n  name = &quot;sm_notebook_role&quot;\n  assume_role_policy = data.aws_iam_policy_document.sm_assume_role_policy.json\n}\n<\/code><\/pre>\n<p>As you can see, the <code>assume_role_policy<\/code> is the policy which will allow SageMaker to perform actions in the AWS account based on the permissions defined in the permissions policy.<\/p>\n<p>This topic is much more complex than in this answer, but it should give you a fair amount of information.<\/p>\n<p>NOTE: In theory, the same role accessing information in AWS and running the AWS API actions when using Terraform could be used for SageMaker, but I would strongly advise against it. Always keep in mind separation of concerns and principle of least privilege.<\/p>\n<hr \/>\n<p>[1] <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html#sagemaker-roles-create-execution-role\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html#sagemaker-roles-create-execution-role<\/a><\/p>\n<p>[2] <a href=\"https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/reference_policies_elements_principal.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/reference_policies_elements_principal.html<\/a><\/p>\n<p>[3] <a href=\"https:\/\/docs.aws.amazon.com\/acm\/latest\/userguide\/authen-custmanagedpolicies.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/acm\/latest\/userguide\/authen-custmanagedpolicies.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1649687933130,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71827884",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59091944,
        "Question_title":"Create a Multi Model Endpoint using AWS Sagemaker Boto",
        "Question_body":"<p>I have created 2 models which are not too complex and renamed them and placed them into a same location in S3 bucket.<\/p>\n\n<p>I need to create a multi model endpoint such that the 2 models have a same end point. \nThe model i am using is AWS in built Linear-learner model type regressor. <\/p>\n\n<p>I am stuck as to how they should be deployed. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1574954456480,
        "Question_score":1,
        "Question_tags":"amazon-web-services|linear-regression|endpoint|amazon-sagemaker|multi-model-forms",
        "Question_view_count":364,
        "Owner_creation_time":1574954057270,
        "Owner_last_access_time":1645458529123,
        "Owner_location":"Carlow, Ireland",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>SageMaker's Linear Learner algorithm container does not currently implement the requirements for <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">multi-model endpoints<\/a>. You could request support in the <a href=\"https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285&amp;start=0\" rel=\"nofollow noreferrer\">AWS Forums<\/a>.<\/p>\n\n<p>You could also build your own version of the Linear Learner algorithm. To deploy the models to a multi-model endpoint you would need to build your own container that meets the requirements for multi-model endpoints and implement your own version of the Linear Learner algorithm. This sample notebook gives an example of how you would create your multi-model compatible container that serves MxNet models, but you could adapt it to implement a Linear Learner algorithm:<\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1582590253857,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1582591338970,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59091944",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53636589,
        "Question_title":"AWS SageMaker: CapacityError: Unable to provision requested ML compute capacity.",
        "Question_body":"<p>We were running two TrainingJob instances of type (1) <em>ml.p3.8xlarge<\/em> and (2) <em>ml.p3.2xlarge<\/em> . <\/p>\n\n<p>Each training job is running a custom algorithm with Tensorflow plus a Keras backend.<\/p>\n\n<p>The instance (1) is running ok, while the instance (2) after a reported time of training of 1 hour, with any logging in CloudWatch (any text tow log), exits with this error:<\/p>\n\n<pre><code>Failure reason\nCapacityError: Unable to provision requested ML compute capacity. Please retry using a different ML instance type.\n<\/code><\/pre>\n\n<p>I'm not sure what this message mean.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1544027022437,
        "Question_score":3,
        "Question_tags":"tensorflow|keras|amazon-sagemaker",
        "Question_view_count":3875,
        "Owner_creation_time":1305708350447,
        "Owner_last_access_time":1663804694443,
        "Owner_location":"Bologna, Italy",
        "Owner_reputation":14823,
        "Owner_up_votes":4638,
        "Owner_down_votes":85,
        "Owner_views":1847,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This message mean SageMaker tried to launch the instance but EC2 was not having enough capacity of this instance hence after waiting for some time(in this case 1 hour) SageMaker gave up and failed the training job.<\/p>\n\n<p>For more information about capacity issue from ec2, please visit: \n<a href=\"https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/troubleshooting-launch.html#troubleshooting-launch-capacity\" rel=\"noreferrer\">troubleshooting-launch-capacity<\/a><\/p>\n\n<p>To solve this, you can either try running jobs with different instance type as suggested in failure reason or wait a few minutes and then submit your request again as suggested by EC2.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1544569877893,
        "Answer_score":5.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1544571385673,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53636589",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66817781,
        "Question_title":"What are the differences between AWS sagemaker and sagemaker_pyspark?",
        "Question_body":"<p>I'm currently running a quick Machine Learning proof of concept on AWS with SageMaker, and I've come across two libraries: <code>sagemaker<\/code> and <code>sagemaker_pyspark<\/code>. I would like to work with distributed data. My questions are:<\/p>\n<ol>\n<li><p>Is using <code>sagemaker<\/code> the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented <code>sagemaker_pyspark<\/code>? Based on this assumption, I do not understand what it would offer regarding using <code>scikit-learn<\/code> on a SageMaker notebook (in terms of computing capabilities).<\/p>\n<\/li>\n<li><p>Is it normal for something like <code>model = xgboost_estimator.fit(training_data)<\/code> to take 4 minutes to run with <code>sagemaker_pyspark<\/code> for a small set of test data? I see that what it does below is to train the model and also create an Endpoint to be able to offer its predictive services, and I assume that this endpoint is deployed on an EC2 instance that is created and started at the moment. Correct me if I'm wrong. I assume this from how the estimator is defined:<\/p>\n<\/li>\n<\/ol>\n<pre><code>from sagemaker import get_execution_role\nfrom sagemaker_pyspark.algorithms import XGBoostSageMakerEstimator\n\n\nxgboost_estimator = XGBoostSageMakerEstimator (\n    trainingInstanceType = &quot;ml.m4.xlarge&quot;,\n    trainingInstanceCount = 1,\n    endpointInstanceType = &quot;ml.m4.xlarge&quot;,\n    endpointInitialInstanceCount = 1,\n    sagemakerRole = IAMRole(get_execution_role())\n)\n\nxgboost_estimator.setNumRound(1)\n<\/code><\/pre>\n<p>If so, is there a way to reuse the same endpoint with different training jobs so that I don't have to wait for a new endpoint to be created each time?<\/p>\n<ol start=\"3\">\n<li><p>Does <code>sagemaker_pyspark<\/code> support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/p>\n<\/li>\n<li><p>Do you know if <code>sagemaker_pyspark<\/code> can perform hyperparameter optimization? From what I see, <code>sagemaker<\/code> offers the <code>HyperparameterTuner<\/code> class, but I can't find anything like it in <code>sagemaker_pyspark<\/code>. I suppose it is a more recent library and there is still a lot of functionality to implement.<\/p>\n<\/li>\n<li><p>I am a bit confused about the concept of <code>entry_point<\/code> and <code>container<\/code>\/<code>image_name<\/code> (both possible input arguments for the <code>Estimator<\/code> object from the <code>sagemaker<\/code> library): can you deploy models with and without containers? why would you use model containers? Do you always need to define the model externally with the <code>entry_point<\/code> script? It is also confusing that the class <code>AlgorithmEstimator<\/code> allows the input argument <code>algorithm_arn<\/code>; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/li>\n<li><p>I see the <code>sagemaker<\/code> library offers SageMaker Pipelines, which seem to be very handy for deploying properly structured ML workflows. However, I don't think this is available with <code>sagemaker_pyspark<\/code>, so in that case, I would rather create my workflows with a combination of Step Functions (to orchestrate the entire thing), Glue processes (for ETL, preprocessing and feature\/target engineering) and SageMaker processes using <code>sagemaker_pyspark<\/code>.<\/p>\n<\/li>\n<li><p>I also found out that <code>sagemaker<\/code> has the <code>sagemaker.sparkml.model.SparkMLModel<\/code> object. What is the difference between this and what <code>sagemaker_pyspark<\/code> offers?<\/p>\n<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616764992750,
        "Question_score":1,
        "Question_tags":"amazon-web-services|pyspark|aws-glue|amazon-sagemaker|aws-step-functions",
        "Question_view_count":242,
        "Owner_creation_time":1376052312530,
        "Owner_last_access_time":1619635368283,
        "Owner_location":"Australia",
        "Owner_reputation":135,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":75,
        "Question_last_edit_time":1618398204670,
        "Answer_body":"<p><code>sagemaker<\/code> is the SageMaker Python SDK. It calls SageMaker-related AWS service APIs on your behalf. You don't need to use it, but it can make life easier<\/p>\n<blockquote>\n<ol>\n<li>Is using sagemaker the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented sagemaker_pyspark?<\/li>\n<\/ol>\n<\/blockquote>\n<p>No. You can run distributed training jobs using <code>sagemaker<\/code> (see <code>instance_count<\/code> parameter)<\/p>\n<p><code>sagemaker_pyspark<\/code> facilitates calling SageMaker-related AWS service APIs from Spark. Use it if you want to use SageMaker services from Spark<\/p>\n<blockquote>\n<ol start=\"2\">\n<li>Is it normal for something like model = xgboost_estimator.fit(training_data) to take 4 minutes to run with sagemaker_pyspark for a small set of test data?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, it takes a few minutes for an EC2 instance to spin-up. Use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">Local Mode<\/a> if you want to iterate more quickly locally. Note: Local Mode won't work with SageMaker built-in algorithms, but you can prototype with (non AWS) XGBoost\/SciKit-Learn<\/p>\n<blockquote>\n<ol start=\"3\">\n<li>Does sagemaker_pyspark support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, but you'd probably want to extend <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/api.html#sagemakerestimator\" rel=\"nofollow noreferrer\">SageMakerEstimator<\/a>. Here you can provide the <code>trainingImage<\/code> URI<\/p>\n<blockquote>\n<ol start=\"4\">\n<li>Do you know if sagemaker_pyspark can perform hyperparameter optimization?<\/li>\n<\/ol>\n<\/blockquote>\n<p>It does not appear so. It'd probably be easier just to do this from SageMaker itself though<\/p>\n<blockquote>\n<p>can you deploy models with and without containers?<\/p>\n<\/blockquote>\n<p>You can certainly host your own models any way you want. But if you want to use SageMaker model inference hosting, then containers are required<\/p>\n<blockquote>\n<p>why would you use model containers?<\/p>\n<\/blockquote>\n<blockquote>\n<p>Do you always need to define the model externally with the entry_point script?<\/p>\n<\/blockquote>\n<p>The whole Docker thing makes bundling dependencies easier, and also makes things language\/runtime-neutral. SageMaker doesn't care if your algorithm is in Python or Java or Fortran. But it needs to know how to &quot;run&quot; it, so you tell it a working directory and a command to run. This is the entry point<\/p>\n<blockquote>\n<p>It is also confusing that the class AlgorithmEstimator allows the input argument algorithm_arn; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/blockquote>\n<p>Please clarify which &quot;three&quot; you are referring to<\/p>\n<p>6 is not a question, so no answer required :)<\/p>\n<blockquote>\n<ol start=\"7\">\n<li>What is the difference between this and what sagemaker_pyspark offers?<\/li>\n<\/ol>\n<\/blockquote>\n<p>sagemaker_pyspark lets you call SageMaker services from Spark, whereas SparkML Serving lets you use Spark ML services from SageMaker<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1617119678127,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1634230982413,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66817781",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56134165,
        "Question_title":"Render hyperlink in pandas df in jupyterlab",
        "Question_body":"<p>I am trying to render a url inside a pandas dataframe output.  I followed along with some of the other examples out there, here is my implementation:<\/p>\n\n<pre><code>def create_url(product_name):\n    search = 'http:\/\/www.example.com\/search'\n    url = 'http:\/\/www.example.com\/search\/'+product_name\n    return url\n\ndef make_clickable(url):\n    return '&lt;a target=\"_blank\" href=\"{}\"&gt;{}&lt;\/a&gt;'.format(url, url)\n\n...\n\ndf['url'] = df['product_name'].apply(format_url)\ndf.style.format({'url': make_clickable})\n<\/code><\/pre>\n\n<p>This produces a correctly formatted raw text hyperlink, however its not clickable within the output.<\/p>\n\n<p>I should add that I'm doing this in an AWS sagemaker jupyterlab notebook which potentially disables hyperlinking in the output.  Not sure how I would check that though.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1557848494950,
        "Question_score":1,
        "Question_tags":"pandas|formatting|amazon-sagemaker",
        "Question_view_count":241,
        "Owner_creation_time":1376684625670,
        "Owner_last_access_time":1663967616407,
        "Owner_location":null,
        "Owner_reputation":1105,
        "Owner_up_votes":658,
        "Owner_down_votes":2,
        "Owner_views":222,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If this doesn't work, I'm guessing it's an AWS thing<\/p>\n\n<ol>\n<li><code>IPython.display.HTML<\/code><\/li>\n<li><code>pandas.DataFrame.to_html<\/code> with <code>escape=False<\/code><\/li>\n<li><code>pandas.set_option('display.max_colwidth', 2000)<\/code> Must be a large number to accommodate length of link tag.  I'll say that I think this is broken.  It shouldn't be necessary to set <code>'display.max_colwidth'<\/code> in order to make sure <code>to_html<\/code> outputs properly.  But it is :-\/<\/li>\n<\/ol>\n\n<hr>\n\n<pre><code>from IPython import display\n\npd.set_option('display.max_colwidth', 2000)\n\ndisplay.HTML(df.assign(url=[*map(make_clickable, df.url)]).to_html(escape=False))\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YcVj6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YcVj6.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1557849422180,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1557931098117,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56134165",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66086605,
        "Question_title":"Use AWS ML model Random Cut Forest locally",
        "Question_body":"<p>I wonder if it is possible to deploy Random Cut Forest (RCF) built-in algorithm of SageMaker to the local mode. I haven't come across any sample implementation about it. If not, can we simply say that models trained using RCF are limited to be consumed inside the platform via Inference Endpoints?<\/p>\n<p>I got this error when I tried to do so.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/YFlRf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YFlRf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ywlQU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ywlQU.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1612692155937,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|rcf",
        "Question_view_count":195,
        "Owner_creation_time":1594550494350,
        "Owner_last_access_time":1650824104303,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":"<p>indeed you're right, <strong>SageMaker Random Cut Forest cannot be trained and deployed locally. The 18 Amazon SageMaker Built-in algorithms are designed to be trained and deployed on Amazon SageMaker.<\/strong> There are 2 exceptions: SageMaker BlazingText and SageMaker XGBoost, which can be read with their open-source counterparts (fastText and XGBoost) and used for  inference out of SageMaker (eg EC2, Lambda, on-prem or on your laptop - as long as you can install those libraries)<\/p>\n<p>There is an open-source attempt to implement the Random Cut Forest here <a href=\"https:\/\/github.com\/kLabUM\/rrcf\" rel=\"nofollow noreferrer\">https:\/\/github.com\/kLabUM\/rrcf<\/a> ; I don't think it has any connection to SageMaker RCF codebase so results, speed and scalability may differ.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1612734092367,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1612737023060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66086605",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72914046,
        "Question_title":"Sagemaker Project successfully creates but there are no linked pipelines or repositories",
        "Question_body":"<p>I created a sagemaker project with a terraform template which successfully created with a stack successfully created and associated with it. However, there is no repository associated or pipeline associated with the sagemaker project despite there being both in the cloudformation template I used. Can someone help with this?<\/p>\n<p>Is there a way to manually link a sagemaker project with a code commit repository? I see that succesfully linked repositories have the tag: <code>sagemaker:project-name<\/code> with the correct project name.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1657295938437,
        "Question_score":0,
        "Question_tags":"amazon-web-services|terraform|amazon-cloudformation|amazon-sagemaker",
        "Question_view_count":124,
        "Owner_creation_time":1653511725307,
        "Owner_last_access_time":1663251784530,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":1657311059267,
        "Answer_body":"<p>Using a different cloudformation template fixed the issue. Not sure why.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1657906025030,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72914046",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57946451,
        "Question_title":"Incremental learning with a built-in sagemaker algorithm",
        "Question_body":"<p>I am training the DeepAR AWS SageMaker built-in algorithm. With the sagemaker SDK, I can train the model with particular specified hyper-parameters:<\/p>\n\n<pre><code>estimator = sagemaker.estimator.Estimator(\n    sagemaker_session=sagemaker_session,\n    image_name=image_name,\n    role=role,\n    train_instance_count=1,\n    train_instance_type='ml.c4.2xlarge',\n    base_job_name='wfp-deepar',\n    output_path=join(s3_path, 'output')\n)\n\nestimator.set_hyperparameters(**{\n    'time_freq': 'M',\n    'epochs': '50',\n    'mini_batch_size': '96',\n    'learning_rate': '1E-3',\n    'context_length': '12',\n    'dropout_rate': 0,\n    'prediction_length': '12'\n})\n\nestimator.fit(inputs=data_channels, wait=True, job_name='wfp-deepar-job-level-5')\n<\/code><\/pre>\n\n<p>I would like to train the resulting model again with a <strong>smaller learning rate<\/strong>. I followed the incremental training method described here: <a href=\"https:\/\/docs.aws.amazon.com\/en_pv\/sagemaker\/latest\/dg\/incremental-training.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/en_pv\/sagemaker\/latest\/dg\/incremental-training.html<\/a>, but it does not work, apparently (according to the link), only two built-in models support incremental learning. <\/p>\n\n<p>Has anyone found a workaround for this so that they can train a built-in algorithm with a scheduled learning rate?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1568567773507,
        "Question_score":1,
        "Question_tags":"amazon-web-services|machine-learning|deep-learning|aws-sdk|amazon-sagemaker",
        "Question_view_count":225,
        "Owner_creation_time":1445974115333,
        "Owner_last_access_time":1664071374770,
        "Owner_location":"Washington, United States",
        "Owner_reputation":5939,
        "Owner_up_votes":1063,
        "Owner_down_votes":11,
        "Owner_views":324,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Unfortunately, the SageMaker built-in DeepAR model doesn't support learning rate scheduling nor incremental learning.  If you want to implement learning rate plateau schedule on a DeepAR architecture I recommend to consider:<\/p>\n\n<ul>\n<li>using the open-source DeepAR implementation (<a href=\"https:\/\/gluon-ts.mxnet.io\/api\/gluonts\/gluonts.model.deepar.html\" rel=\"nofollow noreferrer\">code<\/a>, <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-neural-time-series-models-with-gluon-time-series\/\" rel=\"nofollow noreferrer\">demo<\/a>)<\/li>\n<li>or using the <a href=\"https:\/\/docs.aws.amazon.com\/forecast\/latest\/dg\/aws-forecast-recipe-deeparplus.html\" rel=\"nofollow noreferrer\">DeepAR+ algo of the Amazon Forecast service<\/a>, that features learning rate scheduling ability.<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1568630636313,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57946451",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55987935,
        "Question_title":"How do I pull the pre-built docker images for SageMaker?",
        "Question_body":"<p>I'm trying to pull the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pre-built-docker-containers-frameworks.html\" rel=\"noreferrer\">pre-built docker images<\/a> for SageMaker. I am able to successfully <code>docker login<\/code> to ECR (my AWS credentials). When I try to pull the image I get the standard <code>no basic auth credentials<\/code>.<\/p>\n\n<p>Maybe I'm misunderstanding... I assumed those ECR URLs were public.<\/p>\n\n<pre><code>$(aws ecr get-login --region us-west-2 --no-include-email)\n\ndocker pull 246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-scikit-learn\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1557016435310,
        "Question_score":5,
        "Question_tags":"amazon-web-services|docker|amazon-sagemaker|amazon-ecr",
        "Question_view_count":2738,
        "Owner_creation_time":1343237570417,
        "Owner_last_access_time":1661366012027,
        "Owner_location":"California",
        "Owner_reputation":1325,
        "Owner_up_votes":303,
        "Owner_down_votes":10,
        "Owner_views":131,
        "Question_last_edit_time":1557175542127,
        "Answer_body":"<p>Could you show your ECR login command and pull command in the question?<\/p>\n\n<p>For SageMaker pre-built image 520713654638.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.3.0-cpu-py3<\/p>\n\n<p>What I do is:<\/p>\n\n<ol>\n<li>Log in ECR<\/li>\n<\/ol>\n\n<p><code>$(aws ecr get-login --no-include-email --registry-ids 520713654638 --region us-west-2)<\/code><\/p>\n\n<ol start=\"2\">\n<li>Pull the image<\/li>\n<\/ol>\n\n<p><code>docker pull 520713654638.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.3.0-cpu-py3<\/code><\/p>\n\n<p>These images are public readable so you can pull them from any AWS account. I guess the reason you failed is that you did not specify --registry-ids in your login. But it's better if you can provide your scripts for others to identify what's wrong.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1557174090813,
        "Answer_score":3.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55987935",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72448994,
        "Question_title":"Train Amazon SageMaker object detection model on local PC",
        "Question_body":"<p>I wonder if it's possible to run training Amazon SageMaker object detection model on a local PC?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1654004815903,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|training-data|custom-training",
        "Question_view_count":70,
        "Owner_creation_time":1442786553537,
        "Owner_last_access_time":1664039462827,
        "Owner_location":"Kyiv",
        "Owner_reputation":71,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You're probably referring to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection.html\" rel=\"nofollow noreferrer\">this<\/a> object detection algorithm which is part of of <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">Amazon SageMaker built-in algorithms<\/a>. <strong>Built-in algorithms must be trained on the cloud<\/strong>.<br \/>\nIf you're bringing your own Tensorflow or PyTorch model, you could use SageMaker training jobs to train either on the cloud or locally as @kirit noted.<\/p>\n<p>I would also look at <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-jumpstart.html\" rel=\"nofollow noreferrer\">SageMaker JumpStart<\/a> for a wide variety of object detection algorithm which are TF\/PT based.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1654073043173,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72448994",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63035151,
        "Question_title":"When calling a SageMaker deploy_endpoint function with an a1.small instance, I'm given an error that I can't open a m5.xlarge instance",
        "Question_body":"<p>So while executing through a notebook generated by Autopilot, I went to execute the final code cell:<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>pipeline_model.deploy(initial_instance_count=1,\n                      instance_type='a1.small',\n                      endpoint_name=pipeline_model.name,\n                      wait=True)\n<\/code><\/pre>\n<p>I get this error<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m5.2xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p>The most important part of that is the last line where it mentions resource limits.  I'm not trying to open the type of instance it's giving me an error about opening.<\/p>\n<p>Does the endpoint NEED to be on an ml.m5.2xlarge instance?  Or is the code acting up?<\/p>\n<p>Thanks in advance guys and gals.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1595423975777,
        "Question_score":2,
        "Question_tags":"python|boto3|amazon-sagemaker",
        "Question_view_count":91,
        "Owner_creation_time":1495758291317,
        "Owner_last_access_time":1650398234917,
        "Owner_location":"Temple University, West Berks Street, Philadelphia, PA, USA",
        "Owner_reputation":138,
        "Owner_up_votes":3,
        "Owner_down_votes":1,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You should use one of on-demand ML hosting instances supported as detailed at <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/\" rel=\"nofollow noreferrer\">this link<\/a>. I think non-valid <code>instance_type='a1.small'<\/code> is replaced by a valid one (ml.m5.2xlarge), and that is not in your AWS service quota. The weird part is that seeing <code>instance_type='a1.small'<\/code> was generated by SageMaker Autopilot.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1595539712143,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63035151",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61832086,
        "Question_title":"Having issues reading S3 bucket when transitioning a tensorflow model from local machine to AWS SageMaker",
        "Question_body":"<p>When testing on a local machine in Python I would normally use the following to read a training set with sub-directories of all the classes and files\/class:<\/p>\n\n<pre><code>train_path = r\"C:\\temp\\coins\\PCGS - Gold\\train\"\n\ntrain_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(100,100), classes=['0','1',2','3' etc...], batch_size=32)\n<\/code><\/pre>\n\n<p><strong>Found 4100 images belonging to 22 classes.<\/strong><\/p>\n\n<p>but on AWS SageMaker's Jupyter notebook I am now pulling the files from an S3 bucket.  I tried the following: <\/p>\n\n<pre><code>bucket = \"coinpath\"\n\ntrain_path = 's3:\/\/{}\/{}\/train'.format(bucket, \"v1\")   #note that the directory structure is coinpath\/v1\/train where coinpath is the bucket\n\ntrain_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(100,100), classes=\n['0','1',2','3' etc...], batch_size=32)\n<\/code><\/pre>\n\n<p>but I get: ** Found 0 images belonging to 22 classes.**<\/p>\n\n<p>Looking for some guidance on the right way to pull training data from S3.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1589605066783,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":608,
        "Owner_creation_time":1583123749267,
        "Owner_last_access_time":1663877641340,
        "Owner_location":"Washington D.C., DC, USA",
        "Owner_reputation":317,
        "Owner_up_votes":60,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":"<p>From <a href=\"https:\/\/stackoverflow.com\/questions\/54736505\/ideal-way-to-read-data-in-bucket-stored-batches-of-data-for-keras-ml-training-in\">Ideal way to read data in bucket stored batches of data for Keras ML training in Google Cloud Platform?<\/a> \"ImageDataGenerator.flow_from_directory() currently does not allow you to stream data directly from a GCS bucket. \"<\/p>\n\n<p>I had to download the image from S3 first.  This is best for latency reasons as well. <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1589768612457,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61832086",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68083831,
        "Question_title":"AWS Sagemaker inference endpoint not utilizing all vCPUs",
        "Question_body":"<p>I have deployed a custom model on sagemaker inference endpoint (single instance) and while I was load testing, I have observed that CPU utilization metric is maxing out at 100% but according to <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/sagemaker-cpu-gpu-utilization-100\/\" rel=\"nofollow noreferrer\">this post<\/a> it should max out at #vCPU*100 %. I have confirmed that the inference endpoint is not using all cores in clowdwatch logs.<\/p>\n<p>So if one prediction call requires one second to be processed to give response, the deployed model is only able to handle one API call per second which could have been increased to 8 calls per second if all vCPUs would have been used.<\/p>\n<p>Are there any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency?<\/p>\n<p>Or could we use multiprocessing python package inside <code>inference.py<\/code> file while deploying such that each call comes to the default core and from there all calculations\/prediction is done in any other core whichever is empty at that instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1624366085347,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":661,
        "Owner_creation_time":1473938483227,
        "Owner_last_access_time":1664046020100,
        "Owner_location":"https:\/\/dzone.com\/articles\/machine-learning-provides-360-degree-view-of-the-c",
        "Owner_reputation":909,
        "Owner_up_votes":41,
        "Owner_down_votes":1,
        "Owner_views":81,
        "Question_last_edit_time":1628443120180,
        "Answer_body":"<p>UPDATE<\/p>\n<ul>\n<li><p>Set three environment variables<\/p>\n<ol>\n<li>ENABLE_MULTI_MODEL as &quot;true&quot; (make sure it is string and not bool) and set <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L74\" rel=\"nofollow noreferrer\">SAGEMAKER_HANDLER<\/a> as custom model handler python module path if custom service else dont define it. Also make sure model name <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L94\" rel=\"nofollow noreferrer\">model.mar<\/a>, before compressing it as tar ball and storing in s3<\/li>\n<li>TS_DEFAULT_WORKERS_PER_MODEL as number of vcpus<\/li>\n<li>First environment variable makes sure torch serve env_vars are enabled and second one uses first setting and loads requested number of workers<\/li>\n<li>Setting can be done by passing env dictionary argument to <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-an-estimator\" rel=\"nofollow noreferrer\">PyTorch function<\/a>. Below is explanation as to why it works<\/li>\n<\/ol>\n<\/li>\n<li><p>From the looks of it, sagemaker deployment for pytorch model as given in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-an-estimator\" rel=\"nofollow noreferrer\">Sagemaker SDK guide<\/a>, uses <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu\" rel=\"nofollow noreferrer\">this dockerfile<\/a>. In this docker, entrypoint is <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">torchserve-entrypoint.py<\/a> as in <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu#L124\" rel=\"nofollow noreferrer\">Dockerfile line#124<\/a>.<\/p>\n<\/li>\n<li><p>This <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">torchserve-entrypoint.py<\/a> calls <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">serving.main()<\/a> from <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/serving.py\" rel=\"nofollow noreferrer\">serving.py<\/a>. Which ends up calling <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/serving.py#L34\" rel=\"nofollow noreferrer\">torchserve.start_torchserve(handler_service=HANDLER_SERVICE)<\/a> from <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py\" rel=\"nofollow noreferrer\">torchserve.py<\/a>.<\/p>\n<\/li>\n<li><p><a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L34\" rel=\"nofollow noreferrer\">At line 34 in torchserve.py<\/a> it defines &quot;\/etc\/default-ts.properties&quot; as DEFAULT_TS_CONFIG_FILE. This file is located <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/etc\/default-ts.properties\" rel=\"nofollow noreferrer\">here<\/a>. In this file <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/etc\/default-ts.properties#L2\" rel=\"nofollow noreferrer\">enable_envvars_config=true<\/a> is set. It will use this file setting IFF Environment variable &quot;ENABLE_MULTI_MODEL&quot; is set to &quot;false&quot; as refered <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L167\" rel=\"nofollow noreferrer\">here<\/a>. If it is set to &quot;true&quot; then it will use \/etc\/mme-ts.properties<\/p>\n<\/li>\n<\/ul>\n<hr \/>\n<p>As for the question <code>Are there any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency?<\/code>\nThere are various settings you can use\nFor models you can set <code>default_workers_per_model<\/code> in config.properties <code>TS_DEFAULT_WORKERS_PER_MODEL=$(nproc --all)<\/code> in environment variables. Environment variables take top priority.<\/p>\n<p>Other than that for each model, you can set the number of workers by using management API, but sadly it is not possible to curl to management API in sagemaker. SO TS_DEFAULT_WORKERS_PER_MODEL is the best bet.\nSetting this should make sure all cores are used.<\/p>\n<p>But if you are using docker file then in entrypoint you can setup scripts which wait for model loading and curl to it to set number of workers<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code># load the model\ncurl -X POST localhost:8081\/models?url=model_1.mar&amp;batch_size=8&amp;max_batch_delay=50\n# after loading the model it is possible to set min_worker, etc\ncurl -v -X PUT http:\/\/localhost:8081\/models\/model_1?min_worker=1\n<\/code><\/pre>\n<p>About the other issue that logs confirm that not all cores are used, I face the same issue and believe that is a problem in the logging system. Please look at this issue <a href=\"https:\/\/github.com\/pytorch\/serve\/issues\/782\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pytorch\/serve\/issues\/782<\/a>. The community itself agrees that if threads are not set, then by default then it prints 0, even if by default it uses 2*num_cores.<\/p>\n<p><strong>For an exhaustive set of all configs possible<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Reference: https:\/\/github.com\/pytorch\/serve\/blob\/master\/docs\/configuration.md\n# Variables that can be configured through config.properties and Environment Variables\n# NOTE: Variables which can be configured through environment variables **SHOULD** have a\n# &quot;TS_&quot; prefix\n# debug\ninference_address=http:\/\/0.0.0.0:8080\nmanagement_address=http:\/\/0.0.0.0:8081\nmetrics_address=http:\/\/0.0.0.0:8082\nmodel_store=\/opt\/ml\/model\nload_models=model_1.mar\n# blacklist_env_vars\n# default_workers_per_model\n# default_response_timeout\n# unregister_model_timeout\n# number_of_netty_threads\n# netty_client_threads\n# job_queue_size\n# number_of_gpu\n# async_logging\n# cors_allowed_origin\n# cors_allowed_methods\n# cors_allowed_headers\n# decode_input_request\n# keystore\n# keystore_pass\n# keystore_type\n# certificate_file\n# private_key_file\n# max_request_size\n# max_response_size\n# default_service_handler\n# service_envelope\n# model_server_home\n# snapshot_store\n# prefer_direct_buffer\n# allowed_urls\n# install_py_dep_per_model\n# metrics_format\n# enable_metrics_api\n# initial_worker_port\n\n# Configuration which are not documented or enabled through environment variables\n\n# When below variable is set true, then the variables set in environment have higher precedence.\n# For example, the value of an environment variable overrides both command line arguments and a property in the configuration file. The value of a command line argument overrides a value in the configuration file.\n# When set to false, environment variables are not used at all\n# use_native_io=\n# io_ratio=\n# metric_time_interval=\nenable_envvars_config=true\n# model_snapshot=\n# version=\n<\/code><\/pre>",
        "Answer_comment_count":6.0,
        "Answer_creation_time":1624416005593,
        "Answer_score":5.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1624470584453,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68083831",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73140531,
        "Question_title":"Deploy Pre-Trained model to SageMaker Endpoint from CloudFormation",
        "Question_body":"<p>This seems to be a tricky thing to do, as I haven't found too much documentation for it. I'm trying to deploy a Huggingface pre-trained model for NLU to a SageMaker endpoint. Naturally, I don't want to do this manually, I'd like to automate it through CloudFormation. I found a somewhat <a href=\"https:\/\/faun.pub\/mastering-the-mystical-art-of-model-deployment-part-2-deploying-amazon-sagemaker-endpoints-with-cf9539dc2579\" rel=\"nofollow noreferrer\">useful article<\/a> on how to deploy, but the name of the training model is confusing and I don't know where I would find the right name for the model I want to deploy or where I would put that name (I want to deploy an <a href=\"https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L6-v2\" rel=\"nofollow noreferrer\">all-MiniLM-L6-v2<\/a> model).<\/p>\n<p>Is this possible to do? Do I need to deploy a container? If so, how do I set up the container to process requests and return the text embeddings from the model? I've looked into doing this with just a lambda (which would satisfy the automated deployment process), but the packages I need to use greatly exceed the 250MB limit for lambda+layers.<\/p>\n<p>How do I deploy an endpoint from CloudFormation? Does anyone have experience doing this? If so, please share your wisdom.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1658934967237,
        "Question_score":1,
        "Question_tags":"aws-lambda|amazon-cloudformation|amazon-sagemaker",
        "Question_view_count":103,
        "Owner_creation_time":1605283363407,
        "Owner_last_access_time":1663456472463,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>To anyone curious, this is how I ended up solving this issue:<\/p>\n<p>I ran a Jupyter notebook locally to create the model artifact. Once complete, I zipped the model artifact into a tar.gz file.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModel, AutoTokenizer\nfrom os import makedirs\n\nsaved_model_dir = 'saved_model_dir'\nmakedirs(saved_model_dir, exist_ok=True)\n\n# models were obtained from https:\/\/huggingface.co\/models\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers\/multi-qa-MiniLM-L6-cos-v1')\nmodel = AutoModel.from_pretrained('sentence-transformers\/multi-qa-MiniLM-L6-cos-v1')\n\ntokenizer.save_pretrained(saved_model_dir)\nmodel.save_pretrained(saved_model_dir)\n<\/code><\/pre>\n<pre class=\"lang-bash prettyprint-override\"><code>cd saved_model_dir &amp;&amp; tar czvf ..\/model.tar.gz *\n<\/code><\/pre>\n<p>I included a script in my pipeline to then upload that artifact to S3.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>aws s3 cp path\/to\/model.tar.gz s3:\/\/bucket-name\/prefix\n<\/code><\/pre>\n<p>I also created a CloudFormation template that would stand up my SageMaker resources. The tricky part of this was finding a container image to use, and a colleague was able to point me to <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">this repo<\/a> that contained a massive list of AWS-maintained container images for deep learning and inference. From there, I just needed to select the one that fit my needs.<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>Resources:\n  SageMakerModel:\n    Type: AWS::SageMaker::Model\n    Properties:\n      PrimaryContainer:\n        Image: 763104351884.dkr.ecr.us-east-1.amazonaws.com\/pytorch-inference:1.12.0-cpu-py38-ubuntu20.04-sagemaker # image resource found at https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\n        Mode: SingleModel\n        ModelDataUrl: s3:\/\/path\/to\/model.tar.gz\n      ExecutionRole: \n      ModelName: inference-model\n\n  SageMakerEndpointConfig:\n    Type: AWS::SageMaker::EndpointConfig\n    Properties:\n      EndpointConfigName: endpoint-config-name\n      ProductionVariants:\n        - ModelName: inference-model\n          InitialInstanceCount: 1\n          InstanceType: ml.t2.medium\n          VariantName: dev\n  \n  SageMakerEndpoint:\n    Type: AWS::SageMaker::Endpoint\n    Properties:\n      EndpointName: endpoint-name\n      EndpointConfigName: !GetAtt SageMakerEndpointConfig.EndpointConfigName\n<\/code><\/pre>\n<p>Once the PyTorch model is created locally, this solution essentially automates the process of provisioning and deploying a SageMaker endpoint for inference. If I need to switch the model, I just need to run my notebook code and it will overwrite my existing model artifact. Then I can redeploy and my pipeline will re-upload the artifact to S3, modify the existing SageMaker resources, and the solution will begin operating using the new model.<\/p>\n<p>This may not be the most elegant solution out there, so any suggestions or pointers would be much appreciated!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1659976220937,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1659976426277,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73140531",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67899421,
        "Question_title":"Training keras model in AWS Sagemaker",
        "Question_body":"<p>I have keras training script on my machine. I am experimenting to run my script on AWS sagemaker container. For that I have used below code.<\/p>\n<pre><code>from sagemaker.tensorflow import TensorFlow\nest = TensorFlow(\n    entry_point=&quot;caller.py&quot;,\n    source_dir=&quot;.\/&quot;,\n    role='role_arn',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_type='ml.m5.large',\n    instance_count=1,\n    hyperparameters={'batch': 8, 'epochs': 10},\n)\n\nest.fit()\n<\/code><\/pre>\n<p>here <code>caller.py<\/code> is my entry point. After executing the above code I am getting <code>keras is not installed<\/code>. Here is the stacktrace.<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;executor.py&quot;, line 14, in &lt;module&gt;\n    est.fit()\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 682, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 1625, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3681, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3240, in _check_job_status\n    raise exceptions.UnexpectedStatusException(\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job tensorflow-training-2021-06-09-07-14-01-778: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/local\/bin\/python3.7 caller.py --batch 4 --epochs 10\n\nModuleNotFoundError: No module named 'keras'\n\n<\/code><\/pre>\n<ol>\n<li>Which instance has pre-installed keras?<\/li>\n<li>Is there any way I can install the python package to the AWS container? or any workaround for the issue?<\/li>\n<\/ol>\n<p>Note: I have tried with my own container uploading to ECR and successfully run my code. I am looking for AWS's existing container capability.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1623223568407,
        "Question_score":0,
        "Question_tags":"amazon-web-services|keras|amazon-ec2|amazon-sagemaker",
        "Question_view_count":316,
        "Owner_creation_time":1426675778223,
        "Owner_last_access_time":1664028138917,
        "Owner_location":"Coimbatore, Tamil Nadu, India",
        "Owner_reputation":10189,
        "Owner_up_votes":1483,
        "Owner_down_votes":261,
        "Owner_views":1471,
        "Question_last_edit_time":1623224188497,
        "Answer_body":"<p>Keras is now part of tensorflow, so you can just reformat your code to use <code>tf.keras<\/code> instead of <code>keras<\/code>. Since version 2.3.0 of tensorflow they are in sync, so it should not be that difficult.\nYou container is <a href=\"https:\/\/aws.amazon.com\/releasenotes\/aws-deep-learning-containers-for-tensorflow-2-3-1-with-cuda-11-0\/\" rel=\"nofollow noreferrer\">this<\/a>, as you can see from the list of the packages, there is no <code>Keras<\/code>.\nIf you instead want to extend a pre-built container you can take a look <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/prebuilt-containers-extend.html\" rel=\"nofollow noreferrer\">here<\/a> but I don't recommend in this specific use-case, because also for future code maintainability you should go for <code>tf.keras<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1623320398007,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67899421",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53770876,
        "Question_title":"AWS Sagemaker, InvokeEndpoint operation, Model error: \"setting an array element with a sequence.\"",
        "Question_body":"<p>I am trying to Invoke Endpoint, previously deployed on Amazon SageMaker.\nHere is my code:<\/p>\n\n<pre><code>import numpy as np\nimport boto3\n\nclient = boto3.client('sagemaker-runtime')\n\ndef np2csv(arr):\n    csv = io.BytesIO()\n    np.savetxt(csv, arr, delimiter=',', fmt='%g')\n    return csv.getvalue().decode().rstrip()\n\nendpoint_name = 'DEMO-XGBoostEndpoint-2018-12-12-22-07-28'\ntest_vector = np.array([3.60606061e+00, \n                        3.91395664e+00, \n                        1.34200000e+03, \n                        4.56100000e+03,\n                        2.00000000e+02, \n                        2.00000000e+02]) \ncsv_test_vector = np2csv(test_vector)\n\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                               ContentType='text\/csv',\n                               Body=csv_test_vector)\n<\/code><\/pre>\n\n<p>And here is the error I get:<\/p>\n\n<blockquote>\n  <p>ModelErrorTraceback (most recent call last)\n   in ()\n        1 response = client.invoke_endpoint(EndpointName=endpoint_name,\n        2                                    ContentType='text\/csv',\n  ----> 3                                    Body=csv_test_vector)<\/p>\n  \n  <p>\/home\/ec2-user\/anaconda3\/envs\/python2\/lib\/python2.7\/site-packages\/botocore\/client.pyc\n  in _api_call(self, *args, **kwargs)\n      318                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n      319             # The \"self\" in this scope is referring to the BaseClient.\n  --> 320             return self._make_api_call(operation_name, kwargs)\n      321 \n      322         _api_call.<strong>name<\/strong> = str(py_operation_name)<\/p>\n  \n  <p>\/home\/ec2-user\/anaconda3\/envs\/python2\/lib\/python2.7\/site-packages\/botocore\/client.pyc\n  in _make_api_call(self, operation_name, api_params)\n      621             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n      622             error_class = self.exceptions.from_code(error_code)\n  --> 623             raise error_class(parsed_response, operation_name)\n      624         else:\n      625             return parsed_response<\/p>\n  \n  <p>ModelError: An error occurred (ModelError) when calling the\n  InvokeEndpoint operation: Received client error (415) from model with\n  message \"setting an array element with a sequence.\". See\n  <a href=\"https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/DEMO-XGBoostEndpoint-2018-12-12-22-07-28\" rel=\"nofollow noreferrer\">https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/DEMO-XGBoostEndpoint-2018-12-12-22-07-28<\/a>\n  in account 249707424405 for more information.<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1544739509967,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":2561,
        "Owner_creation_time":1324988509370,
        "Owner_last_access_time":1663847888160,
        "Owner_location":"Moscow, Russia",
        "Owner_reputation":1593,
        "Owner_up_votes":55,
        "Owner_down_votes":2,
        "Owner_views":93,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This works:<\/p>\n\n<pre><code>import numpy as np\nimport boto3\n\nclient = boto3.client('sagemaker-runtime')\nendpoint_name = 'DEMO-XGBoostEndpoint-2018-12-12-22-07-28'\ntest_vector = [3.60606061e+00, \n               3.91395664e+00, \n               1.34200000e+03, \n               4.56100000e+03,\n               2.00000000e+02, \n               2.00000000e+02]) \n\nbody = ',',join([str(item) for item in test_vector])\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                               ContentType='text\/csv',\n                               Body=body)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1547665192370,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53770876",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71260306,
        "Question_title":"How to access\/invoke a sagemaker endpoint without lambda?",
        "Question_body":"<p>based on the aws documentation, maximum timeout limit is less that 30 seconds in api gateway.so hooking up an sagemaker endpoint with api gateway wouldn't make sense, if the request\/response is going to take more than 30 seconds. is there any workaround ? adding a lambda in between api gateway and sagemaker endpoint is going to add more time to process request\/response, which i would like to avoid. also, there will be added time for lambda cold starts and sagemaker serverless endpoints are built on top of lambda so that will also add cold start time. is there a way to invoke the serverless sagemaker endpoints , without these overhead?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1645755688313,
        "Question_score":3,
        "Question_tags":"serverless|amazon-sagemaker",
        "Question_view_count":1122,
        "Owner_creation_time":1590797441983,
        "Owner_last_access_time":1664049080543,
        "Owner_location":null,
        "Owner_reputation":525,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":98,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It is indeed possible to invoke sagemaker endpoints from sagemaker without using any other AWS services and that is also manifested by the fact that they have invocation URLs.<\/p>\n<p>Here's how you set it up:<\/p>\n<ol>\n<li>create an user with only programmatic access and attach a policy json that should look something like below:<\/li>\n<\/ol>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;VisualEditor0&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:sagemaker:&lt;region&gt;:&lt;account-id&gt;:endpoint\/&lt;endpoint-name&gt;&quot;\n        }\n    ]\n} \n<\/code><\/pre>\n<p>you can replace <code>&lt;endpoint-name&gt;<\/code> with <code>*<\/code> to let this user invoke all endpoints.<\/p>\n<ol start=\"2\">\n<li><p>use the ACCESS-KEY and SECRET-ACCESS-KEY to configure authorisation in postman like shown in this screenshot. also add the parameters in advanced tab like shown in the screenshot.\n<a href=\"https:\/\/i.stack.imgur.com\/cYkTf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cYkTf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>then fill up your body with the relevant content type.<\/p>\n<\/li>\n<li><p>then add or remove additional headers like variant-name or model-name, if you have them set up and the headers should look like shown in this screenshot: <a href=\"https:\/\/i.stack.imgur.com\/NLqkV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NLqkV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>send the request to receive reponse like this\n<a href=\"https:\/\/i.stack.imgur.com\/uA4kF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uA4kF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<\/ol>\n<p><em><strong>URL and credentials in the above screenshots doesn't work anymore, duh!<\/strong><\/em><\/p>\n<p>and if you want code to invoke the endpoint directly using some back-end language, <a href=\"https:\/\/stackoverflow.com\/a\/70803026\/11814996\">here's code for python<\/a>.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1645795642143,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1649698499117,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71260306",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58487710,
        "Question_title":"User Docker Hub registry containers in AWS Sagemaker",
        "Question_body":"<p>Is there any way to load containers stored in docker hub registry in Amazon Sagemaker.\nAccording to some documentation, it should be possible, but I have not been able to find any relevan example or guide for it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1571665775480,
        "Question_score":0,
        "Question_tags":"amazon-web-services|docker|amazon-sagemaker",
        "Question_view_count":135,
        "Owner_creation_time":1382876091093,
        "Owner_last_access_time":1640773246957,
        "Owner_location":"Valencia, Espa\u00f1a",
        "Owner_reputation":149,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":null,
        "Answer_body":"<p>While you can use any registry when working with Docker on a SageMaker notebook, as of this writing other SageMaker components presently only support images from Amazon ECR repositories.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1582242007587,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58487710",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67093041,
        "Question_title":"Aws Sagemaker - ModuleNotFoundError: No module named 'cv2'",
        "Question_body":"<p>I am trying to run a object detection code in Aws. Although opencv is listed in the requirement file, i have the error &quot;no module named cv2&quot;. I am not sure how to fix this error. could someone help me please.<\/p>\n<p>My requirement.txt file has<\/p>\n<ul>\n<li>opencv-python<\/li>\n<li>numpy&gt;=1.18.2<\/li>\n<li>scipy&gt;=1.4.1<\/li>\n<li>wget&gt;=3.2<\/li>\n<li>tensorflow==2.3.1<\/li>\n<li>tensorflow-gpu==2.3.1<\/li>\n<li>tqdm==4.43.0<\/li>\n<li>pandas<\/li>\n<li>boto3<\/li>\n<li>awscli<\/li>\n<li>urllib3<\/li>\n<li>mss<\/li>\n<\/ul>\n<p>I tried installing &quot;imgaug&quot; and &quot;opencv-python headless&quot; as well.. but still not able to get rid of this error.<\/p>\n<pre><code>sh-4.2$ python train_launch.py \n[INFO-ROLE] arn:aws:iam::021945294007:role\/service-role\/AmazonSageMaker-ExecutionRole-20200225T145269\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_count has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\n2021-04-14 13:29:58 Starting - Starting the training job...\n2021-04-14 13:30:03 Starting - Launching requested ML instances......\n2021-04-14 13:31:11 Starting - Preparing the instances for training......\n2021-04-14 13:32:17 Downloading - Downloading input data...\n2021-04-14 13:32:41 Training - Downloading the training image..WARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n2021-04-14 13:33:03,970 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\n2021-04-14 13:33:05,030 sagemaker-containers INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {\n        &quot;training&quot;: &quot;\/opt\/ml\/input\/data\/training&quot;\n    },\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;unfreezed_epochs&quot;: 2,\n        &quot;freezed_batch_size&quot;: 8,\n        &quot;freezed_epochs&quot;: 1,\n        &quot;unfreezed_batch_size&quot;: 8,\n        &quot;model_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {\n        &quot;training&quot;: {\n            &quot;TrainingInputMode&quot;: &quot;File&quot;,\n            &quot;S3DistributionType&quot;: &quot;FullyReplicated&quot;,\n            &quot;RecordWrapperType&quot;: &quot;None&quot;\n        }\n    },\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;yolov4-2021-04-14-15-29&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;train_indu&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 8,\n    &quot;num_gpus&quot;: 1,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;train_indu.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2}\nSM_USER_ENTRY_POINT=train_indu.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[&quot;training&quot;]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=train_indu\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=8\nSM_NUM_GPUS=1\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{&quot;training&quot;:&quot;\/opt\/ml\/input\/data\/training&quot;},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;yolov4-2021-04-14-15-29&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;train_indu&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:8,&quot;num_gpus&quot;:1,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;train_indu.py&quot;}\nSM_USER_ARGS=[&quot;--freezed_batch_size&quot;,&quot;8&quot;,&quot;--freezed_epochs&quot;,&quot;1&quot;,&quot;--model_dir&quot;,&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;--unfreezed_batch_size&quot;,&quot;8&quot;,&quot;--unfreezed_epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_CHANNEL_TRAINING=\/opt\/ml\/input\/data\/training\nSM_HP_UNFREEZED_EPOCHS=2\nSM_HP_FREEZED_BATCH_SIZE=8\nSM_HP_FREEZED_EPOCHS=1\nSM_HP_UNFREEZED_BATCH_SIZE=8\nSM_HP_MODEL_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages\n\nInvoking script with the following command:\n\n\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2\n\n\nWARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n[name: &quot;\/device:CPU:0&quot;\ndevice_type: &quot;CPU&quot;\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 4667030854237447206\n, name: &quot;\/device:XLA_CPU:0&quot;\ndevice_type: &quot;XLA_CPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 3059419181456814147\nphysical_device_desc: &quot;device: XLA_CPU device&quot;\n, name: &quot;\/device:XLA_GPU:0&quot;\ndevice_type: &quot;XLA_GPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 6024475084695919958\nphysical_device_desc: &quot;device: XLA_GPU device&quot;\n, name: &quot;\/device:GPU:0&quot;\ndevice_type: &quot;GPU&quot;\nmemory_limit: 14949928141\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 13034103301168381073\nphysical_device_desc: &quot;device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5&quot;\n]\nTraceback (most recent call last):\n  File &quot;train_indu.py&quot;, line 12, in &lt;module&gt;\n    from yolov3.dataset import Dataset\n  File &quot;\/opt\/ml\/code\/yolov3\/dataset.py&quot;, line 3, in &lt;module&gt;\n    import cv2\nModuleNotFoundError: No module named 'cv2'\n2021-04-14 13:33:08,453 sagemaker-containers ERROR    ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n\n2021-04-14 13:33:11 Uploading - Uploading generated training model\n2021-04-14 13:33:54 Failed - Training job failed\nTraceback (most recent call last):\n  File &quot;train_launch.py&quot;, line 41, in &lt;module&gt;\n    estimator.fit(s3_data_path, logs=True, job_name=job_name) #the argument logs is crucial if you want to see what happends\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 535, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 1210, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 3365, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 2957, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job yolov4-2021-04-14-15-29: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1618407986697,
        "Question_score":0,
        "Question_tags":"opencv|image-processing|amazon-ec2|object-detection|amazon-sagemaker",
        "Question_view_count":1218,
        "Owner_creation_time":1558009697177,
        "Owner_last_access_time":1618755603610,
        "Owner_location":"Cergy-Pontoise, Cergy, France",
        "Owner_reputation":53,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Make sure your estimator has<\/p>\n<ul>\n<li>framework_version = '2.3',<\/li>\n<li>py_version = 'py37',<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1618410091900,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67093041",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68374280,
        "Question_title":"Assigning name to AWS SageMaker Training job",
        "Question_body":"<p>This may be a very basic question (I imagine it is)<\/p>\n<pre><code>estimator = PyTorch(entry_point='train.py',\n                   source_dir = 'code',\n                    role = role,\n                   framework_version = '1.5.0',\n                   py_version = 'py3',\n                   instance_count = 2,\n                   instance_type = 'ml.g4dn.2xlarge',\n                   hyperparameters={&quot;epochs&quot;: 2,\n                                     &quot;num_labels&quot;: 46,\n                                     &quot;backend&quot;: &quot;gloo&quot;,    \n                                    },\n                   profiler_config=profiler_config,\n                    debugger_hook_config=debugger_hook_config,\n                    rules=rules\n                   )\n<\/code><\/pre>\n<p>I declare my estimator as above, and put this into training using fit().<br \/>\nI have done several of these on my sagemaker, and there are several training jobs in the aws training job log.<br \/>\nBut they all appear in the form 'pytorch-training-2021 ....'. <br \/>\nIs there anyway I could declare the name of the training job like 'custom-model-xgboost-ver1' ?<br \/>\nI thought it would be possible as one of the parameter of estimator, but i couldn't find it.<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1626249953723,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":445,
        "Owner_creation_time":1600718448277,
        "Owner_last_access_time":1663311569497,
        "Owner_location":"Seoul, South Korea",
        "Owner_reputation":69,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When you call <code>fit()<\/code> you can pass this parameter <code>job_name=yourJobName<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1626250161860,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68374280",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72874937,
        "Question_title":"Is it possible set up an endpoint for a model I created in AWS SageMaker without using the SageMaker SDK",
        "Question_body":"<p>I've created my own model on a AWS SageMaker instance, with my own training and inference loops. I want to deploy it so that I can call the model for inference from AWS Lambda.<\/p>\n<p>I didn't use the SageMaker package to develop at all, but every tutorial (here is <a href=\"https:\/\/towardsdatascience.com\/using-aws-sagemaker-and-lambda-function-to-build-a-serverless-ml-platform-f14b3ec5854a%3E\" rel=\"nofollow noreferrer\">one<\/a>) I've looked at does so.<\/p>\n<p>How do I create an endpoint without using the SageMaker package.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657051299687,
        "Question_score":0,
        "Question_tags":"python|aws-lambda|amazon-sagemaker|endpoint",
        "Question_view_count":66,
        "Owner_creation_time":1657050754840,
        "Owner_last_access_time":1663796977727,
        "Owner_location":null,
        "Owner_reputation":15,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use the boto3 library to do this.<\/p>\n<p>Here is an example of pseudo code for this -<\/p>\n<pre><code>import boto3\nsm_client = boto3.client('sagemaker')\ncreate_model_respose = sm_client.create_model(ModelName=model_name, ExecutionRoleArn=role, Containers=[container] )\n\ncreate_endpoint_config_response = sm_client.create_endpoint_config(EndpointConfigName=endpoint_config_name)\n\ncreate_endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1657053745440,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72874937",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71152047,
        "Question_title":"how to create a serverless endpoint in sagemaker?",
        "Question_body":"<p>I followed the aws documentation ( <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config<\/a>) to create a model and to use that model, i coded for a serverless endpoint config (sample code below) ,I have all the required values  but this throws an error below and i'm not sure why<\/p>\n<p>parameter validation failed unknown parameter inProductVariants [ 0 ]: &quot;ServerlessConfig&quot;, must be one of : VairantName, ModelName, InitialInstanceCount , Instancetype...<\/p>\n<pre><code>response = client.create_endpoint_config(\n   EndpointConfigName=&quot;abc&quot;,\n   ProductionVariants=[\n        {\n            &quot;ModelName&quot;: &quot;foo&quot;,\n            &quot;VariantName&quot;: &quot;variant-1&quot;,\n            &quot;ServerlessConfig&quot;: {\n                &quot;MemorySizeInMB&quot;: 1024,\n                &quot;MaxConcurrency&quot;: 2\n            }\n        } \n    ]\n)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645067620433,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":161,
        "Owner_creation_time":1590797441983,
        "Owner_last_access_time":1664049080543,
        "Owner_location":null,
        "Owner_reputation":525,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":98,
        "Question_last_edit_time":1645073711770,
        "Answer_body":"<p>You are probably using <strong>old boto3<\/strong> version. <code>ServerlessConfig<\/code> is a very new configuration option. You need to upgrade to the latest version (1.21.1) if possible.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1645069539320,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71152047",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73292975,
        "Question_title":"Sagemaker Data Capture does not write files",
        "Question_body":"<p>I want to enable data capture for a specific endpoint (so far, only via the console). The endpoint works fine and also logs &amp; returns the desired results. However, no files are written to the specified S3 location.<\/p>\n<h3>Endpoint Configuration<\/h3>\n<p>The endpoint is based on a training job with a scikit learn classifier. It has only one variant which is a <code>ml.m4.xlarge<\/code> instance type. Data Capture is enabled with a sampling percentage of 100%. As data capture storage locations I tried <code>s3:\/\/&lt;bucket-name&gt;<\/code> as well as <code>s3:\/\/&lt;bucket-name&gt;\/&lt;some-other-path&gt;<\/code>. With the &quot;Capture content type&quot; I tried leaving everything blank, setting <code>text\/csv<\/code> in &quot;CSV\/Text&quot; and <code>application\/json<\/code> in &quot;JSON&quot;.<\/p>\n<h3>Endpoint Invokation<\/h3>\n<p>The endpoint is invoked in a Lambda function with a client. Here's the call:<\/p>\n<pre><code>sagemaker_body_source = {\n            &quot;segments&quot;: segments,\n            &quot;language&quot;: language\n        }\npayload = json.dumps(sagemaker_body_source).encode()\nresponse = self.client.invoke_endpoint(EndpointName=endpoint_name,\n                                       Body=payload,\n                                       ContentType='application\/json',\n                                       Accept='application\/json')\nresult = json.loads(response['Body'].read().decode())\nreturn result[&quot;predictions&quot;]\n<\/code><\/pre>\n<p>Internally, the endpoint uses a Flask API with an <code>\/invocation<\/code> path that returns the result.<\/p>\n<h3>Logs<\/h3>\n<p>The endpoint itself works fine and the Flask API is logging input and output:<\/p>\n<pre><code>INFO:api:body: {'segments': [&lt;strings...&gt;], 'language': 'de'}\n<\/code><\/pre>\n<pre><code>INFO:api:output: {'predictions': [{'text': 'some text', 'label': 'some_label'}, ....]}\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1660052882547,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":65,
        "Owner_creation_time":1394703217223,
        "Owner_last_access_time":1663244105390,
        "Owner_location":"Cologne, Germany",
        "Owner_reputation":486,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":null,
        "Answer_body":"<p>So the issue seemed to be related to the IAM role. The default role (<code>ModelEndpoint-Role<\/code>) does not have access to write S3 files. It worked via the SDK since it uses another role in the sagemaker studio. I did not receive any error message about this.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1660654017400,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73292975",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57379173,
        "Question_title":"Where do I store my model's training data, artifacts, etc?",
        "Question_body":"<p>I'm trying to build and push a custom ML model with docker to Amazon SageMaker. I know things are supposed to follow the general structure of being in opt\/ml. But there's no such bucket in Amazon S3??? Am I supposed to create this directory within my container before I build and push the image to AWS? I just have no idea where to put my training data, etc.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1565104546377,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1022,
        "Owner_creation_time":1399251405187,
        "Owner_last_access_time":1651704417890,
        "Owner_location":null,
        "Owner_reputation":145,
        "Owner_up_votes":35,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":null,
        "Answer_body":"<p>SageMaker is automating the deployment of the Docker image with your code using the convention of channel->local-folder. Everything that you define with a channel in your <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig\" rel=\"nofollow noreferrer\">input data configuration<\/a>, will be copied to the local Docker file system under <em>\/opt\/ml\/<\/em> folder, using the name of the channel as the name of the sub-folder.<\/p>\n\n<pre><code>{\n\"train\" : {\"ContentType\":  \"trainingContentType\", \n           \"TrainingInputMode\": \"File\", \n           \"S3DistributionType\": \"FullyReplicated\", \n           \"RecordWrapperType\": \"None\"},\n\"evaluation\" : {\"ContentType\":  \"evalContentType\", \n                \"TrainingInputMode\": \"File\", \n                \"S3DistributionType\": \"FullyReplicated\", \n                \"RecordWrapperType\": \"None\"},\n\"validation\" : {\"TrainingInputMode\": \"File\", \n                \"S3DistributionType\": \"FullyReplicated\", \n                \"RecordWrapperType\": \"None\"}\n} \n<\/code><\/pre>\n\n<p>to:<\/p>\n\n<pre><code>\/opt\/ml\/input\/data\/training\n\/opt\/ml\/input\/data\/validation\n\/opt\/ml\/input\/data\/testing\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1565167576867,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57379173",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71467176,
        "Question_title":"How can I verify that my training job is reading the augmented manifest file?",
        "Question_body":"<p>Apologies for the long post.<\/p>\n<p>Originally, I had data in one location on an S3 bucket and used to train deep learning image classification models on this data using the typical 'File' mode and passing the S3 uri where the data is stored as training input. To try and accelerate training, I wanted to switch to using:<\/p>\n<ol>\n<li>Pipe mode, to stream data and not download all the data at the beginning of the training, starting training faster and saving disk space.<\/li>\n<li>Augmented Manifest File coupled with 1., so that I don't have to place my data in a single location on S3, so I avoid moving data around when I train models.<\/li>\n<\/ol>\n<p>I was making my script similar to <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=934156#934156\" rel=\"nofollow noreferrer\">the one in this example<\/a>. I printed the steps done when parsing the data, however I noticed that the data might not have been read because when printing it shows the following:<\/p>\n<pre><code>step 1 Tensor(&quot;ParseSingleExample\/ParseExample\/ParseExampleV2:0&quot;, shape=(), dtype=string)\nstep 2 Tensor(&quot;DecodePng:0&quot;, shape=(None, None, 3), dtype=uint8)\nstep 3 Tensor(&quot;Cast:0&quot;, shape=(None, None, 3), dtype=float32)\n<\/code><\/pre>\n<p>I guess the image is not being read\/found since the shape is <code>[None, None, 3]<\/code> when it should be <code>[224, 224, 3]<\/code>, so maybe the problem is from the Augmented Manifest file?<\/p>\n<p>Here's an example of how my Augmented Manifest file is written:<\/p>\n<pre><code>{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image1.png&quot;, &quot;label&quot;: 1}\n{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image2.png&quot;, &quot;label&quot;: 2}\n{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image3.png&quot;, &quot;label&quot;: 3}\n<\/code><\/pre>\n<p>Some other details I should probably mention:<\/p>\n<ol>\n<li>When I create the Training Input I pass <code>'content_type': 'application\/x-recordio', 'record_wrapping': 'RecordIO'<\/code>, even though my data are in .png format, but I assumed that as the augmented manifest file is read the data get wrapped in the RecordIO format.<\/li>\n<li>Following my first point, I pass <code>PipeModeDataset(channel=channel, record_format='RecordIO')<\/code>, so also not sure about the RecordIO thing.<\/li>\n<\/ol>\n<p>There isn't an actual error that is raised, just when I start fitting the model nothing happens, it keeps on running but nothing actually runs so I'm trying to find the issue.<\/p>\n<hr \/>\n<p>EDIT: It now reads the shape correctly, but there's still the issue where it enters the .fit method and does nothing, just keeps running without doing anything. Find part of the script below.<\/p>\n<pre><code>def train_input_fn(train_channel):\n    &quot;&quot;&quot;Returns input function that feeds the model during training&quot;&quot;&quot;\n    return _input_fn(train_channel)\n\ndef _input_fn(channel):\n    &quot;&quot;&quot;\n        Returns a Dataset which reads from a SageMaker PipeMode channel.\n    &quot;&quot;&quot;\n    \n    features = {\n        'image-ref': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([3], tf.int64),\n    }\n \n    def combine(records):\n        return records[0], records[1]\n \n    def parse(record):\n        \n        parsed = tf.io.parse_single_example(record, features)\n        \n                 \n\n        image = tf.io.decode_png(parsed[&quot;image-ref&quot;], channels=3, dtype=tf.uint8)\n        image = tf.reshape(image, [224, 224, 3])\n        \n        lbl = parsed['label']\n        print(image, lbl)\n        return (image, lbl)\n \n    ds = PipeModeDataset(channel=channel, record_format='RecordIO')\n    ds = ds.map(parse, num_parallel_calls=AUTOTUNE)\n    ds = ds.prefetch(AUTOTUNE)\n \n    return ds\n\ndef model(dataset):\n    &quot;&quot;&quot;Generate a simple model&quot;&quot;&quot;\n    inputs = Input(shape=(224, 224, 3))\n    prediction_layer = Dense(2, activation = 'softmax')\n\n\n    x = inputs\n    x = tf.keras.applications.mobilenet.MobileNet(include_top=False, input_shape=(224,224,3), weights='imagenet')(x)\n    outputs = prediction_layer(x)\n    rec_model = tf.keras.Model(inputs, outputs)    \n    \n    rec_model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n        metrics=['accuracy']\n    )\n    \n    \n    rec_model.fit(\n        dataset\n    )\n\n    return rec_model\n\ndef main(params):\n    \n    epochs = params['epochs']\n    train_channel = params['train_channel']\n    record_format = params['record_format']\n    batch_size = params['batch_size']\n        \n    train_spec = train_input_fn(train_channel)\n    model_classifier = model(train_spec)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647258085310,
        "Question_score":0,
        "Question_tags":"python|tensorflow|amazon-s3|manifest|amazon-sagemaker",
        "Question_view_count":127,
        "Owner_creation_time":1576016596283,
        "Owner_last_access_time":1660210631827,
        "Owner_location":"Beirut, Lebanon",
        "Owner_reputation":15,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1647268897627,
        "Answer_body":"<p>From <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<blockquote>\n<p>A PipeModeDataset can read TFRecord, RecordIO, or text line records.<\/p>\n<\/blockquote>\n<p>While your'e trying to read binary (PNG) files. I don't see a relevant <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions\/tree\/tf-2\/src\/pipemode_op\/RecordReader\" rel=\"nofollow noreferrer\">record reader here<\/a> to help you do that.<br \/>\nYou could build your own format pipe implementation like shown <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">here<\/a>, but it's considerably more effort.<\/p>\n<p>Alternatively, you mentioned your files are scattered in different folders, but if your files common path contains less than 2M files, you could use <a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/10\/amazon-sagemaker-fast-file-mode\/\" rel=\"nofollow noreferrer\">FastFile mode<\/a> to <strong>stream<\/strong> data. Currently, FastFile only supports an S3 Prefix, so you won't be able to use a manifest.<\/p>\n<p>Also see this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/choose-the-best-data-source-for-your-amazon-sagemaker-training-job\/\" rel=\"nofollow noreferrer\">general pros\/cons discussion of the different available storage and input types available in SageMaker<\/a>.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1647607100133,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71467176",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49168673,
        "Question_title":"How to load a training set in AWS SageMaker to build a model?",
        "Question_body":"<p>I am very new to SageMaker. Upon my first interaction, it looks like the AWS SageMaker requires you to start from its Notebook. I have a training set which is ready. Is there a way to bypass setting the Notebook and just to start by upload the training set? Or it should be done through the Notebook. If anyone knows some example fitting my need above, that will be great. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1520498020543,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":700,
        "Owner_creation_time":1305269513437,
        "Owner_last_access_time":1664081804010,
        "Owner_location":null,
        "Owner_reputation":10317,
        "Owner_up_votes":268,
        "Owner_down_votes":1,
        "Owner_views":595,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Amazon SageMaker is a combination of multiple services that each is independent of the others. You can use the notebook instances if you want to develop your models in the familiar Jupyter environment. But if just need to train a model, you can use the training jobs without opening a notebook instance. <\/p>\n\n<p>There a few ways to launch a training job:<\/p>\n\n<ul>\n<li>Use the high-level SDK for Python that is similar to the way that you start a training step in your python code<\/li>\n<\/ul>\n\n<p><code>kmeans.fit(kmeans.record_set(train_set[0]))<\/code><\/p>\n\n<p>Here is the link to the python library: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a><\/p>\n\n<ul>\n<li>Use the low-level API to Create-Training-Job, and you can do that using various SDK (Java, Python, JavaScript, C#...) or the CLI. <\/li>\n<\/ul>\n\n<p><code>sagemaker = boto3.client('sagemaker')\n sagemaker.create_training_job(**create_training_params)<\/code><\/p>\n\n<p>Here is a link to the documentation on these options: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a> <\/p>\n\n<ul>\n<li>Use Spark interface to launch it using a similar interface to creating an MLLib training job<\/li>\n<\/ul>\n\n<p><code>val estimator = new KMeansSageMakerEstimator(\n  sagemakerRole = IAMRole(roleArn),\n  trainingInstanceType = \"ml.p2.xlarge\",\n  trainingInstanceCount = 1,\n  endpointInstanceType = \"ml.c4.xlarge\",\n  endpointInitialInstanceCount = 1)\n  .setK(10).setFeatureDim(784)<\/code><\/p>\n\n<p><code>val model = estimator.fit(trainingData)<\/code><\/p>\n\n<p>Here is a link to the spark-sagemaker library: <a href=\"https:\/\/github.com\/aws\/sagemaker-spark\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-spark<\/a><\/p>\n\n<ul>\n<li>Create a training job in the Amazon SageMaker console using the wizard there: <a href=\"https:\/\/console.aws.amazon.com\/sagemaker\/home?region=us-east-1#\/jobs\" rel=\"nofollow noreferrer\">https:\/\/console.aws.amazon.com\/sagemaker\/home?region=us-east-1#\/jobs<\/a><\/li>\n<\/ul>\n\n<p>Please note that there a few options also to train models, either using the built-in algorithms such as K-Means, Linear Learner or XGBoost (see here for the complete list: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html<\/a>). But you can also bring your own models for pre-baked Docker images such as TensorFlow (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf.html<\/a>) or MXNet (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet.html<\/a>), your own Docker image (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html<\/a>).  <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1520631374253,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49168673",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56927813,
        "Question_title":"Using of Estamator.evaluate() on trained sagemaker tensorflow model",
        "Question_body":"<p>After I've trained and deployed the model with AWS SageMaker, I want to evaluate it on several csv files:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>- category-1-eval.csv (~700000 records)\n- category-2-eval.csv (~500000 records)\n- category-3-eval.csv (~800000 records)\n...\n<\/code><\/pre>\n\n<p>The right way to do this is with using <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/estimator\/Estimator#evaluate\" rel=\"nofollow noreferrer\">Estimator.evaluate()<\/a> method, as it is fast.<\/p>\n\n<p>The problem is - I cannot find the way to restore SageMaker model into Tensorflow Estimator, is it possible?<\/p>\n\n<p>I've tried to restore a model like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>tf.estimator.DNNClassifier(\n    feature_columns=...,\n    hidden_units=[...],\n    model_dir=\"s3:\/\/&lt;bucket_name&gt;\/checkpoints\",\n)\n<\/code><\/pre>\n\n<p>In AWS SageMaker documentation a different approach is described - <a href=\"https:\/\/docs.aws.amazon.com\/en_us\/sagemaker\/latest\/dg\/how-it-works-model-validation.html\" rel=\"nofollow noreferrer\">to test the actual endpoint from the Notebook<\/a> - but it takes to much time and requires a lot of API calls to the endpoint.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1562553970617,
        "Question_score":0,
        "Question_tags":"tensorflow|neural-network|amazon-sagemaker",
        "Question_view_count":155,
        "Owner_creation_time":1530642335903,
        "Owner_last_access_time":1622266440183,
        "Owner_location":"Vancouver, BC, Canada",
        "Owner_reputation":53,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":"<p>if you used the built-in Tensorflow container, your model has been saved in Tensorflow Serving format, e.g.:<\/p>\n\n<pre><code>$ tar tfz model.tar.gz\nmodel\/\nmodel\/1\/\nmodel\/1\/saved_model.pb\nmodel\/1\/variables\/\nmodel\/1\/variables\/variables.index\nmodel\/1\/variables\/variables.data-00000-of-00001\n<\/code><\/pre>\n\n<p>You can easily load it with Tensorflow Serving on your local machine, and send it samples to predict. More info at <a href=\"https:\/\/www.tensorflow.org\/tfx\/guide\/serving\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/tfx\/guide\/serving<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1562591149540,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1562591826680,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56927813",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64169189,
        "Question_title":"How can I deploy a re-trained Sagemaker model to an endpoint?",
        "Question_body":"<p>With an <code>sagemaker.estimator.Estimator<\/code>, I want to re-<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#sagemaker.estimator.EstimatorBase.deploy\" rel=\"nofollow noreferrer\">deploy<\/a> a model after retraining (calling <code>fit<\/code> with new data).<\/p>\n<p>When I call this<\/p>\n<pre><code>estimator.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')\n<\/code><\/pre>\n<p>I get an error<\/p>\n<pre><code>botocore.exceptions.ClientError: An error occurred (ValidationException) \nwhen calling the CreateEndpoint operation: \nCannot create already existing endpoint &quot;arn:aws:sagemaker:eu-east- \n1:1776401913911:endpoint\/zyx&quot;.\n<\/code><\/pre>\n<p>Apparently I want to use functionality like <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateEndpoint.html\" rel=\"nofollow noreferrer\">UpdateEndpoint<\/a>. How do I access that functionality from this API?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1601630651783,
        "Question_score":3,
        "Question_tags":"python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":1958,
        "Owner_creation_time":1227171471293,
        "Owner_last_access_time":1664047108080,
        "Owner_location":"Israel",
        "Owner_reputation":17500,
        "Owner_up_votes":463,
        "Owner_down_votes":87,
        "Owner_views":1561,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes, under the hood the <code>model.deploy<\/code> creates a model, an endpoint configuration and an endpoint. When you call again the method from an already-deployed, trained estimator it will create an error because a similarly-configured endpoint is already deployed. What I encourage you to try:<\/p>\n<ul>\n<li><p>use the <code>update_endpoint=True<\/code> parameter. From the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"noreferrer\">SageMaker SDK doc<\/a>:\n<em>&quot;Additionally, it is possible to deploy a different endpoint configuration, which links to your model, to an already existing\nSageMaker endpoint. This can be done by specifying the existing\nendpoint name for the <code>endpoint_name<\/code> parameter along with the\n<code>update_endpoint<\/code> parameter as True within your <code>deploy()<\/code> call.&quot;<\/em><\/p>\n<\/li>\n<li><p>Alternatively, if you want to create a separate model you can specify a new <code>model_name<\/code> in your <code>deploy<\/code><\/p>\n<\/li>\n<\/ul>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1601891469683,
        "Answer_score":6.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1602140796110,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64169189",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59717227,
        "Question_title":"AWS SageMaker - submit button is not working with custom template",
        "Question_body":"<p>When I create a new job on AWS SageMaker, using my custom template with crowd form (see attached sample) the SUBMIT button is not working and is not even clickable. Is there anyway to make this work? Haven`t see a good response on AWS support.<\/p>\n\n<pre><code>$('#submitButton').onclick = function() {\n   $('crowd-form').submit(); \n};\n\n\n &lt;body&gt;\n    &lt;h2 id=\"hit\"&gt;test&lt;\/h2&gt;\n        &lt;canvas id=\"canvas\" width=1210 height=687&gt;&lt;\/canvas&gt;    \n        &lt;crowd-button id=\"submitButton3\"&gt;Test button&lt;\/crowd-button&gt;\n\n    &lt;crowd-form&gt;\n\n        &lt;input type=\"hidden\" name=\"path0\" id=\"input0123\" value=\"{{task.input.metadata.images.path0}}\" \/&gt;\n        &lt;crowd-input label=\"Please input the character you see in the image\" max-length=\"1\" name=\"workerInput0\"&gt;&lt;\/crowd-input&gt;\n\n        &lt;crowd-button id=\"submitButto3223n\"&gt;Submit123&lt;\/crowd-button&gt;\n\n    &lt;\/div&gt;&lt;\/div&gt;\n\n    &lt;crowd-button id=\"submitButton\"&gt;Submit123&lt;\/crowd-button&gt;\n\n    &lt;\/crowd-form&gt;\n    &lt;crowd-button id=\"submitButton1\"&gt;Submit1232&lt;\/crowd-button&gt;\n\n    &lt;script src=\"http:\/\/code.jquery.com\/jquery-1.11.0.min.js\"&gt;&lt;\/script&gt;\n &lt;\/body&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1578920666227,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker|mechanicalturk",
        "Question_view_count":917,
        "Owner_creation_time":1544346330127,
        "Owner_last_access_time":1588231544867,
        "Owner_location":"Lviv, Lviv Oblast, Ukraine",
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":1578989573557,
        "Answer_body":"<p>There are few issues with you code snippet.<\/p>\n<p>Here are the links to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-ui-template-reference.html\" rel=\"nofollow noreferrer\">SageMaker's HTML Reference<\/a> and <a href=\"https:\/\/awsfeed.com\/whats-new\/machine-learning\/build-a-custom-data-labeling-workflow-with-amazon-sagemaker-ground-truth\/\" rel=\"nofollow noreferrer\">Example for building custom Labeling template<\/a><\/p>\n<p>First remove all those submit buttons (<code>&lt;crowd-button&gt;<\/code> elements) and the <code>onClick<\/code> event handler. From here you have two options use default SageMaker submit button or create your own in the template.<\/p>\n<h2>Use SageMaker's Submit Button<\/h2>\n<p>Leave out submit buttons (<code>crowd-button<\/code>) and SageMaker will automatically append one inside <code>crowd-form<\/code>. According to documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-ui-template-crowd-form.html\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<h2>Use custom Submit Button<\/h2>\n<p>In this case you need to:<\/p>\n<ol>\n<li>Prevent SageMaker adding button by including <code>crowd-button<\/code> <strong>inside<\/strong> the <code>crowd-form<\/code> element and setting <code>style=&quot;display: none;<\/code><\/li>\n<li>Add your own Submit button elsewhere on the template and add <code>onclick<\/code> even handler that will execute <code>form.submit()<\/code><\/li>\n<\/ol>\n<p>Here is the working example of the template (taken from the Example mentioned above).<\/p>\n<pre><code>&lt;script src=&quot;https:\/\/assets.crowd.aws\/crowd-html-elements.js&quot;&gt;&lt;\/script&gt;\n\n&lt;link rel=&quot;stylesheet&quot; href=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/css\/1.3fc3007b.chunk.css&quot;&gt;\n&lt;link rel=&quot;stylesheet&quot; href=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/css\/main.9504782e.chunk.css&quot;&gt;\n\n&lt;div id='document-text' style=&quot;display: none;&quot;&gt;\n  {{ task.input.text }}\n&lt;\/div&gt;\n&lt;div id='document-image' style=&quot;display: none;&quot;&gt;\n        {{ task.input.taskObject | grant_read_access }}\n&lt;\/div&gt;\n&lt;div id=&quot;metadata&quot; style=&quot;display: none;&quot;&gt;\n  {{ task.input.metadata }}\n&lt;\/div&gt;\n\n&lt;crowd-form&gt;\n    &lt;input name=&quot;annotations&quot; id=&quot;annotations&quot; type=&quot;hidden&quot;&gt;\n\n     &lt;!-- Prevent crowd-form from creating its own button --&gt;\n    &lt;crowd-button form-action=&quot;submit&quot; style=&quot;display: none;&quot;&gt;&lt;\/crowd-button&gt;\n&lt;\/crowd-form&gt;\n\n&lt;!-- Custom annotation user interface is rendered here --&gt;\n&lt;div id=&quot;root&quot;&gt;&lt;\/div&gt;\n\n&lt;crowd-button id=&quot;submitButton&quot;&gt;Submit&lt;\/crowd-button&gt;\n\n&lt;script&gt;\n    document.querySelector('crowd-form').onsubmit = function() {\n        document.getElementById('annotations').value = JSON.stringify(JSON.parse(document.querySelector('pre').innerText));\n    };\n\n    document.getElementById('submitButton').onclick = function() {\n        document.querySelector('crowd-form').submit();\n    };\n&lt;\/script&gt;\n\n&lt;script src=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/js\/1.3e5a6849.chunk.js&quot;&gt;&lt;\/script&gt;\n&lt;script src=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/js\/main.96e12312.chunk.js&quot;&gt;&lt;\/script&gt;\n&lt;script src=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/js\/runtime~main.229c360f.js&quot;&gt;&lt;\/script&gt;\n<\/code><\/pre>\n<p>Code source<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1579189368977,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1592644375060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59717227",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70306493,
        "Question_title":"View train error metrics for Hugging Face Sagemaker model",
        "Question_body":"<p>I have trained a model using Hugging Face's integration with Amazon Sagemaker <a href=\"https:\/\/huggingface.co\/docs\/sagemaker\/train\" rel=\"nofollow noreferrer\">and their Hello World example<\/a>.<\/p>\n<p>I can easily calculate and view the metrics generated on the evaluation test set: accuracy, f-score, precision, recall etc. by calling <code>training_job_analytics<\/code> on the trained model: <code>huggingface_estimator.training_job_analytics.dataframe()<\/code><\/p>\n<p>How can I also see the same metrics on training sets (or even training error for each epoch)?<\/p>\n<p>Training code is basically the same as the link with extra parts of the docs added:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.huggingface import HuggingFace\n\n# optionally parse logs for key metrics\n# from the docs: https:\/\/huggingface.co\/docs\/sagemaker\/train#sagemaker-metrics\nmetric_definitions = [\n    {'Name': 'loss', 'Regex': &quot;'loss': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'learning_rate', 'Regex': &quot;'learning_rate': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_loss', 'Regex': &quot;'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_accuracy', 'Regex': &quot;'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_f1', 'Regex': &quot;'eval_f1': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_precision', 'Regex': &quot;'eval_precision': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_recall', 'Regex': &quot;'eval_recall': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_runtime', 'Regex': &quot;'eval_runtime': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_samples_per_second', 'Regex': &quot;'eval_samples_per_second': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'epoch', 'Regex': &quot;'epoch': ([0-9]+(.|e\\-)[0-9]+),?&quot;}\n]\n\n# hyperparameters, which are passed into the training job\nhyperparameters={\n    'epochs': 5,\n    'train_batch_size': batch_size,\n    'model_name': model_checkpoint,\n    'task': task,\n}\n\n# init the model (but not yet trained)\nhuggingface_estimator = HuggingFace(\n    entry_point='train.py',\n    source_dir='.\/scripts',\n    instance_type='ml.p3.2xlarge',\n    instance_count=1,\n    role=role,\n    transformers_version='4.6',\n    pytorch_version='1.7',\n    py_version='py36',\n    hyperparameters = hyperparameters,\n    metric_definitions=metric_definitions\n)\n# starting the train job with our uploaded datasets as input\nhuggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})\n\n# does not return metrics on training - only on eval!\nhuggingface_estimator.training_job_analytics.dataframe()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639148719370,
        "Question_score":0,
        "Question_tags":"python|nlp|amazon-sagemaker|huggingface-transformers",
        "Question_view_count":184,
        "Owner_creation_time":1437078651387,
        "Owner_last_access_time":1663950070697,
        "Owner_location":null,
        "Owner_reputation":901,
        "Owner_up_votes":150,
        "Owner_down_votes":10,
        "Owner_views":145,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This can be solved by increasing the number of epochs in training to a more realistic value.<\/p>\n<p>Currently, the model trains in fewer than 300 seconds (which is when the following timestamp would be recorded) and presumably the loss function.<\/p>\n<p>Changes to make:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>hyperparameters={\n    'epochs': 100, # increase the number of epochs to realistic value!\n    'train_batch_size': batch_size,\n    'model_name': model_checkpoint,\n    'task': task,\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1639502011653,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1645119168563,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70306493",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63096583,
        "Question_title":"SageMaker: An error occurred (ModelError) when calling the InvokeEndpoint operation: unable to evaluate payload provided",
        "Question_body":"<p>I have a endpoint in Amazon SageMaker (Image-classification algorithm) in Jupyter notebook that works fine. In Lambda function works fine too, when I call the Lambda function from API Gateway, from test of API Gateway, works fine too.<\/p>\n<p>The problem is when I call the API from Postman according this answer: <a href=\"https:\/\/stackoverflow.com\/questions\/39660074\/post-image-data-using-postman\">&quot;Post Image data using POSTMAN&quot;<\/a><\/p>\n<p>The code in Lambda is:<\/p>\n<pre><code>import boto3\nimport json\nimport base64\n\nENDPOINT_NAME = &quot;DEMO-XGBoostEndpoint-Multilabel&quot;\nruntime= boto3.client(&quot;runtime.sagemaker&quot;)\nimagen_ = &quot;\/tmp\/imageToProcess.jpg&quot;\n\ndef write_to_file(save_path, data):\n    with open(save_path, &quot;wb&quot;) as f:\n        f.write(base64.b64decode(data))\n\ndef lambda_handler(event, context):\n    img_json = json.loads(json.dumps(event))\n\n    write_to_file(imagen_, json.dumps(event, indent=2))\n\n    with open(imagen_, &quot;rb&quot;) as image:\n        f = image.read()\n        b = bytearray(f)\n\n    payload = b\n\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                       ContentType=&quot;application\/x-image&quot;,\n                                       Body=payload)\n\n    #print(response)\n    result = json.loads(response[&quot;Body&quot;].read().decode())\n    print(result)\n    predicted_label=[]\n    classes = [&quot;chair&quot;, &quot;handbag&quot;, &quot;person&quot;, &quot;traffic light&quot;, &quot;clock&quot;]\n    for idx, val in enumerate(classes):\n        print(&quot;%s:%f &quot;%(classes[idx], result[idx]), end=&quot;&quot;)\n        predicted_label += (classes[idx], result[idx])\n\n    return {\n      &quot;statusCode&quot;: 200,\n      &quot;headers&quot;: { &quot;content-type&quot;: &quot;application\/json&quot;},\n      &quot;body&quot;:  predicted_label\n}\n<\/code><\/pre>\n<p>The error is:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/var\/task\/lambda_function.py&quot;, line 26, in lambda_handler\n    Body=payload)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 316, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 626, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;unable to evaluate payload provided&quot;. See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/DEMO-XGBoostEndpoint-Multilabel in account 866341179300 for more information. ```\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1595742856227,
        "Question_score":2,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":3996,
        "Owner_creation_time":1565376125573,
        "Owner_last_access_time":1612402865987,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1595997749087,
        "Answer_body":"<p>I resolved with <a href=\"https:\/\/medium.com\/swlh\/upload-binary-files-to-s3-using-aws-api-gateway-with-aws-lambda-2b4ba8c70b8e\" rel=\"nofollow noreferrer\">this<\/a> post:<\/p>\n<p>Thank all<\/p>\n<p>Finally the code in lambda function is:<\/p>\n<pre><code>import os\nimport boto3\nimport json\nimport base64\n\nENDPOINT_NAME = os.environ['endPointName']\nCLASSES = &quot;[&quot;chair&quot;, &quot;handbag&quot;, &quot;person&quot;, &quot;traffic light&quot;, &quot;clock&quot;]&quot;\nruntime= boto3.client(&quot;runtime.sagemaker&quot;)\n\ndef lambda_handler(event, context):\n    file_content = base64.b64decode(event['content'])\n\n    payload = file_content\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType=&quot;application\/x-image&quot;, Body=payload)\n\n    result = json.loads(response[&quot;Body&quot;].read().decode())\n    print(result)\n    predicted_label=[]\n    classes = CLASSES\n    for idx, val in enumerate(classes):\n       print(&quot;%s:%f &quot;%(classes[idx], result[idx]), end=&quot;&quot;)\n       predicted_label += (classes[idx], result[idx])\n\n    return {\n      &quot;statusCode&quot;: 200,\n      &quot;headers&quot;: { &quot;content-type&quot;: &quot;application\/json&quot;},\n      &quot;body&quot;:  predicted_label\n    }\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1597003004373,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1634524535897,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63096583",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53408927,
        "Question_title":"How to pass a bigger .csv files to amazon sagemaker for predictions using batch transform jobs",
        "Question_body":"<p>I created a custom model and deployed it on sagemaker. I am invoking the endpoint using batch transform jobs. It works if the input file is small, i.e, number of rows in the csv file is less. If I upload a file with around 200000 rows, I am getting this error in the cloudwatch logs.<\/p>\n\n<pre><code>2018-11-21 09:11:52.666476: W external\/org_tensorflow\/tensorflow\/core\/framework\/allocator.cc:113]\nAllocation of 2878368000 exceeds 10% of system memory.\n2018-11-21 09:11:53.166493: W external\/org_tensorflow\/tensorflow\/core\/framework\/allocator.cc:113] \nAllocation of 2878368000 exceeds 10% of system memory.\n[2018-11-21 09:12:02,544] ERROR in serving: &lt;_Rendezvous of RPC that \nterminated with:\n#011status = StatusCode.DEADLINE_EXCEEDED\n#011details = \"Deadline Exceeded\"\n#011debug_error_string = \"\n{\n\"created\": \"@1542791522.543282048\",\n\"description\": \"Error received from peer\",\n\"file\": \"src\/core\/lib\/surface\/call.cc\",\n\"file_line\": 1017,\n\"grpc_message\": \"Deadline Exceeded\",\n\"grpc_status\": 4\n}\n\"\n<\/code><\/pre>\n\n<p>Any ideas what might be going wrong. This is the transform function which I am using to create the transform job.<\/p>\n\n<pre><code>transformer =sagemaker.transformer.Transformer(\nbase_transform_job_name='Batch-Transform',\nmodel_name='sagemaker-tensorflow-2018-11-21-07-58-15-887',\ninstance_count=1,\ninstance_type='ml.m4.xlarge',\noutput_path='s3:\/\/2-n2m-sagemaker-json-output\/out_files\/'\n\n)\ninput_location = 's3:\/\/1-n2m-n2g-csv-input\/smal_sagemaker_sample.csv'\ntransformer.transform(input_location, content_type='text\/csv', split_type='Line')\n<\/code><\/pre>\n\n<p>The .csv file contains 2 columns for first and last name of customer, which I am then preprocessing it in the sagemaker itself using input_fn().<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1542792620897,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":1941,
        "Owner_creation_time":1444454434270,
        "Owner_last_access_time":1652071645660,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":140,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Question_last_edit_time":1542799396317,
        "Answer_body":"<p>The error looks to be coming from a GRPC client closing the connection before the server is able to respond. (There looks to be an existing feature request for the sagemaker tensorflow container on <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/issues\/46\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/issues\/46<\/a> to make this timeout configurable)<\/p>\n\n<p>You could try out a few things with the sagemaker Transformer to limit the size of each individual request so that it fits within the timeout:<\/p>\n\n<ul>\n<li>Set a <code>max_payload<\/code> to a smaller value, say 2-3 MB (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB\" rel=\"nofollow noreferrer\">the default is 6 MB<\/a>)<\/li>\n<li>If your instance metrics indicate it has compute \/ memory resources to spare, try <code>max_concurrent_transforms<\/code> > 1 to make use of multiple workers<\/li>\n<li>Split up your csv file into multiple input files. With a bigger dataset, you could also increase the instance count to fan out processing<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1544503147007,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53408927",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56740609,
        "Question_title":"Randomforest in amazon aws sagemaker?",
        "Question_body":"<p>I am looking to recreate a randomforest model built locally, and deploy it through sagemaker. The model is very basic, but for comparison I would like to use the same in sagemaker. I don't see randomforest among sagemaker's built in algorithms (which seems weird) - is my only option to go the route of <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/train-and-host-scikit-learn-models-in-amazon-sagemaker-by-building-a-scikit-docker-container\/\" rel=\"nofollow noreferrer\">deploying my own custom model<\/a>? Still learning about containers, and it seems like a lot of work for something that is just a simple randomforestclassifier() call locally. I just want to baseline against the out of the box randomforest model, and show that it works the same when deployed through AWS sagemaker.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1561393633193,
        "Question_score":4,
        "Question_tags":"amazon-web-services|docker|containers|random-forest|amazon-sagemaker",
        "Question_view_count":4295,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":null,
        "Answer_body":"<p><em>edit 03\/30\/2020: adding a link to the the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"noreferrer\">SageMaker Sklearn random forest demo<\/a><\/em><\/p>\n\n<p><br\/><\/p>\n\n<p>in SageMaker you have 3 options to write scientific code:<\/p>\n\n<ul>\n<li><strong>Built-in algorithms<\/strong><\/li>\n<li><strong>Open-source pre-written containers<\/strong> (available\nfor sklearn, tensorflow, pytorch, mxnet, chainer. Keras can be\nwritten in the tensorflow and mxnet containers)<\/li>\n<li><strong>Bring your own container<\/strong> (for R for example)<\/li>\n<\/ul>\n\n<p><strong>At the time of writing this post there is no random forest classifier nor regressor in the built-in library<\/strong>. There is an algorithm called <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-built-in-amazon-sagemaker-random-cut-forest-algorithm-for-anomaly-detection\/\" rel=\"noreferrer\">Random Cut Forest<\/a> in the built-in library but it is an unsupervised algorithm for anomaly detection, a different use-case than the <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\" rel=\"noreferrer\">scikit-learn random forest<\/a> used in a supervised fashion (also <a href=\"https:\/\/stackoverflow.com\/questions\/56728230\/aws-sagemaker-randomcutforest-rcf-vs-scikit-lean-randomforest-rf?noredirect=1&amp;lq=1\">answered in StackOverflow here<\/a>). But it is easy to use the open-source pre-written scikit-learn container to implement your own. There is a <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"noreferrer\">demo showing how to use Sklearn's random forest in SageMaker<\/a>, with training orchestration bother from the high-level SDK and <code>boto3<\/code>. You can also use this other <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_iris\/Scikit-learn%20Estimator%20Example%20With%20Batch%20Transform.ipynb\" rel=\"noreferrer\">public sklearn-on-sagemaker demo<\/a> and change the model. A benefit of the pre-written containers over the \"Bring your own\" option is that the dockerfile is already written, and web serving stack too.<\/p>\n\n<p>Regarding your surprise that Random Forest is not featured in the built-in algos, the library and its 18 algos already cover a rich set of use-cases. For example for supervised learning over structured data (the usual use-case for the random forest), if you want to stick to the built-ins, depending on your priorities (accuracy, inference latency, training scale, costs...) you can use SageMaker XGBoost (XGBoost has been winning tons of datamining competitions - every winning team in the top10 of the KDDcup 2015 used XGBoost <a href=\"https:\/\/arxiv.org\/pdf\/1603.02754.pdf\" rel=\"noreferrer\">according to the XGBoost paper<\/a> - and scales well) and linear learner, which is extremely fast at inference and can be trained at scale, in mini-batch fashion over GPU(s). <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/fact-machines-howitworks.html\" rel=\"noreferrer\">Factorization Machines<\/a> (linear + 2nd degree interaction with weights being column embedding dot-products) and <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-supports-knn-classification-and-regression\/\" rel=\"noreferrer\">SageMaker kNN<\/a> are other options. Also, things are not frozen in stone, and the list of built-in algorithms is being improved fast.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1561491486350,
        "Answer_score":10.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":1585586806547,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56740609",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58892606,
        "Question_title":"Sagemaker can't find paths in container",
        "Question_body":"<p>This is a hard situation to describe.<\/p>\n\n<p>I have a python model train script at:<\/p>\n\n<p><code>myproject\/opt\/program\/train<\/code><\/p>\n\n<p>This gets a file at <code>.\/opt\/ml\/input\/data\/external\/train.csv<\/code><\/p>\n\n<p>When I do <code>python3 opt\/program\/train<\/code> the training runs fine locally.<\/p>\n\n<p>Then I containerize the project and copy <code>opt<\/code> to <code>\/opt<\/code> in my Dockerfile.<\/p>\n\n<p>Now when I run <code>docker run &lt;image name&gt; train<\/code> it also trains fine.<\/p>\n\n<p>Then I deploy the image to SageMaker, create an estimator, and call <code>model.fit(my_data)<\/code> I get:<\/p>\n\n<p><code>Exception during training: [Errno 2] File b'.\/opt\/ml\/input\/data\/external\/train.csv' does not exist<\/code><\/p>\n\n<p>It's definitely there, I was able to train by running the container myself.  Also running the container and exploring the file system I can find the file.<\/p>\n\n<p>So I think I have some filesystem misunderstanding.  From the root of the container, all of these seem to have equivalent outputs.<\/p>\n\n<pre><code>root@798ffe7364c6:\/# ls opt\nml  program\nroot@798ffe7364c6:\/# ls \/opt\nml  program\nroot@798ffe7364c6:\/# ls .\/opt\nml  program\n<\/code><\/pre>\n\n<p>I'm trying to come up with a way to have one path that will work locally, in the container, and on AWS.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1573920322537,
        "Question_score":0,
        "Question_tags":"python|docker|amazon-sagemaker",
        "Question_view_count":2448,
        "Owner_creation_time":1360164540017,
        "Owner_last_access_time":1663899252397,
        "Owner_location":"Columbus, OH",
        "Owner_reputation":11190,
        "Owner_up_votes":136,
        "Owner_down_votes":11,
        "Owner_views":365,
        "Question_last_edit_time":1573920635263,
        "Answer_body":"<p>I was missing the fact that SageMaker looks for your data channels in S3 and copies those to your container at <code>\/opt\/ml\/input\/data<\/code><\/p>\n\n<p>By default it seems to use <code>training<\/code> and <code>validation<\/code> as the channel names.  Therefore, in my example above, it would have never copied data from my <code>external<\/code> folder on S3 to the right <code>external<\/code> folder in my container.  In fact, I discovered it was copying it instead to <code>\/opt\/ml\/input\/data\/training\/external\/train.csv<\/code>.<\/p>\n\n<p>To resolve this, I would have either had to change my folder names, or use <code>InputDataConfig<\/code> to define other channels.  I chose the later and was able to get it working.<\/p>\n\n<p>More info on <code>InputDataConfig<\/code> here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1574100008957,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58892606",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49064183,
        "Question_title":"How to set the percentage of inference calls when performing A\/B testing using AWS sagemaker?",
        "Question_body":"<p>I'm new to sagemaker. I'm trying to figure out how to perform A\/B testing using AWS sagemaker. I understand setting the train_instance_count will distribute the training across two instances. But how do I specify the set the percentage of inference calls each model will handle and perform A\/B testing? \nThis is all I could find from the docs <\/p>\n\n<blockquote>\n  <p>\"Amazon SageMaker can also manage model A\/B testing for you. You can\n  configure the endpoint to spread traffic across as many as five\n  different models and set the percentage of inference calls you want\n  each one to handle. You can change all of this on the fly, giving you\n  a lot of flexibility to run experiments and determine which model\n  produces the most accurate results in the real world.\"<\/p>\n<\/blockquote>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1519974037750,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-ec2|amazon-sagemaker",
        "Question_view_count":1042,
        "Owner_creation_time":1450260166773,
        "Owner_last_access_time":1663955203357,
        "Owner_location":null,
        "Owner_reputation":1587,
        "Owner_up_votes":123,
        "Owner_down_votes":8,
        "Owner_views":540,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can have multiple Production Variants behind an Amazon SageMaker endpoint. Each production variant has an initial variant weight and based on the ratio of each variant weight to the total sum of weights, SageMaker can distribute the calls to each of the models. For example, if you have only one production variant with a weight of 1, all traffic will go to this variant. If you add another production variant with an initial weight of 2, the new variant will get 2\/3 of the traffic and the first variant will get 1\/3. <\/p>\n\n<p>You can see more details on ProductionVariant on Amazon SageMaker documentations here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_ProductionVariant.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_ProductionVariant.html<\/a> <\/p>\n\n<p>You can provide an array of ProductionVariants when you \"Create Endpoint Configuration\": <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpointConfig.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpointConfig.html<\/a> , and you can update the variants with \"Update Endpoint Weights and Capacities\" call: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_UpdateEndpointWeightsAndCapacities.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_UpdateEndpointWeightsAndCapacities.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1520190662820,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49064183",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73728499,
        "Question_title":"How to update an existing model in AWS sagemaker >= 2.0",
        "Question_body":"<p>I have an XGBoost model currently in production using AWS sagemaker and making real time inferences. After a while, I would like to update the model with a newer one trained on more data and keep everything as is (e.g. same endpoint, same inference procedure, so really no changes aside from the model itself)<\/p>\n<p>The current deployment procedure is the following :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.xgboost.model import XGBoostModel\nfrom sagemaker.xgboost.model import XGBoostPredictor\n\nxgboost_model = XGBoostModel(\n    model_data = &lt;S3 url&gt;,\n    role = &lt;sagemaker role&gt;,\n    entry_point = 'inference.py',\n    source_dir = 'src',\n    code_location = &lt;S3 url of other dependencies&gt;\n    framework_version='1.5-1',\n    name = model_name)\n\nxgboost_model.deploy(\n    instance_type='ml.c5.large',\n    initial_instance_count=1,\n    endpoint_name = model_name)\n<\/code><\/pre>\n<p>Now that I updated the model a few weeks later, I would like to re-deploy it. I am aware that the <code>.deploy()<\/code> method creates an endpoint and an endpoint configuration so it does it all. I cannot simply re-run my script again since I would encounter an error.<\/p>\n<p>In previous versions of sagemaker I could have updated the model with an extra argument passed to the <code>.deploy()<\/code> method called <code>update_endpoint = True<\/code>. In sagemaker &gt;=2.0 this is a no-op. Now, in sagemaker &gt;= 2.0, I need to use the predictor object as stated in the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html\" rel=\"nofollow noreferrer\">documentation<\/a>. So I try the following :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>predictor = XGBoostPredictor(model_name)\npredictor.update_endpoint(model_name= model_name)\n<\/code><\/pre>\n<p>Which actually updates the endpoint according to a new endpoint configuration. However, I do not know what it is updating... I do not specify in the above 2 lines of code that we need to considering the new <code>xgboost_model<\/code> trained on more data...  so where do I tell the update to take a more recent model?<\/p>\n<p>Thank you!<\/p>\n<p><strong>Update<\/strong><\/p>\n<p>I believe that I need to be looking at production variants as stated in their documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-ab-testing.html\" rel=\"nofollow noreferrer\">here<\/a>. However, their whole tutorial is based on the amazon sdk for python (boto3) which has artifacts that are hard to manage when I have difference entry points for each model variant (e.g. different <code>inference.py<\/code> scripts).<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1663233254363,
        "Question_score":0,
        "Question_tags":"python-3.x|deployment|xgboost|amazon-sagemaker",
        "Question_view_count":41,
        "Owner_creation_time":1640956373383,
        "Owner_last_access_time":1663928794863,
        "Owner_location":null,
        "Owner_reputation":309,
        "Owner_up_votes":56,
        "Owner_down_votes":1,
        "Owner_views":15,
        "Question_last_edit_time":1663319367853,
        "Answer_body":"<p>Since I found an answer to my own question I will post it here for those who encounter the same problem.<\/p>\n<p>I ended up re-coding all my deployment script using the boto3 SDK rather than the sagemaker SDK (or a mix of both as some documentation suggest).<\/p>\n<p>Here's the whole script that shows how to create a sagemaker model object, an endpoint configuration and an endpoint to deploy the model on for the first time. In addition, it shows what to do how to update the endpoint with a newer model (which was my main question)<\/p>\n<p>Here's the code to do all 3 in case you want to bring your own model and update it safely in production using sagemaker :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\nimport time\nfrom datetime import datetime\nfrom sagemaker import image_uris\nfrom fileManager import *  # this is a local script for helper functions\n\n# name of zipped model and zipped inference code\nCODE_TAR = 'your_inference_code_and_other_artifacts.tar.gz'\nMODEL_TAR = 'your_saved_xgboost_model.tar.gz'\n\n# sagemaker params\nsmClient = boto3.client('sagemaker')\nsmRole = &lt;your_sagemaker_role&gt;\nbucket = sagemaker.Session().default_bucket()\n\n# deploy algorithm\nclass Deployer:\n\n    def __init__(self, modelName, deployRetrained=False):\n        self.modelName=modelName\n        self.deployRetrained = deployRetrained\n        self.prefix = &lt;S3_model_path_prefix&gt;\n    \n    def deploy(self):\n        '''\n        Main method to create a sagemaker model, create an endpoint configuration and deploy the model. If deployRetrained\n        param is set to True, this method will update an already existing endpoint.\n        '''\n        # define model name and endpoint name to be used for model deployment\/update\n        model_name = self.modelName + &lt;any_suffix&gt;\n        endpoint_config_name = self.modelName + '-%s' %datetime.now().strftime('%Y-%m-%d-%HH%M')\n        endpoint_name = self.modelName\n        \n        # deploy model for the first time\n        if not self.deployRetrained:\n            print('Deploying for the first time')\n\n            # here you should copy and zip the model dependencies that you may have (such as preprocessors, inference code, config code...)\n            # mine were zipped into the file called CODE_TAR\n\n            # upload model and model artifacts needed for inference to S3\n            uploadFile(list_files=[MODEL_TAR, CODE_TAR], prefix = self.prefix)\n\n            # create sagemaker model and endpoint configuration\n            self.createSagemakerModel(model_name)\n            self.createEndpointConfig(endpoint_config_name, model_name)\n\n            # deploy model and wait while endpoint is being created\n            self.createEndpoint(endpoint_name, endpoint_config_name)\n            self.waitWhileCreating(endpoint_name)\n        \n        # update model\n        else:\n            print('Updating existing model')\n\n            # upload model and model artifacts needed for inference (here the old ones are replaced)\n            # make sure to make a backup in S3 if you would like to keep the older models\n            # we replace the old ones and keep the same names to avoid having to recreate a sagemaker model with a different name for the update!\n            uploadFile(list_files=[MODEL_TAR, CODE_TAR], prefix = self.prefix)\n\n            # create a new endpoint config that takes the new model\n            self.createEndpointConfig(endpoint_config_name, model_name)\n\n            # update endpoint\n            self.updateEndpoint(endpoint_name, endpoint_config_name)\n\n            # wait while endpoint updates then delete outdated endpoint config once it is InService\n            self.waitWhileCreating(endpoint_name)\n            self.deleteOutdatedEndpointConfig(model_name, endpoint_config_name)\n\n    def createSagemakerModel(self, model_name):\n        ''' \n        Create a new sagemaker Model object with an xgboost container and an entry point for inference using boto3 API\n        '''\n        # Retrieve that inference image (container)\n        docker_container = image_uris.retrieve(region=region, framework='xgboost', version='1.5-1')\n\n        # Relative S3 path to pre-trained model to create S3 model URI\n        model_s3_key = f'{self.prefix}\/'+ MODEL_TAR\n\n        # Combine bucket name, model file name, and relate S3 path to create S3 model URI\n        model_url = f's3:\/\/{bucket}\/{model_s3_key}'\n\n        # S3 path to the necessary inference code\n        code_url = f's3:\/\/{bucket}\/{self.prefix}\/{CODE_TAR}'\n        \n        # Create a sagemaker Model object with all its artifacts\n        smClient.create_model(\n            ModelName = model_name,\n            ExecutionRoleArn = smRole,\n            PrimaryContainer = {\n                'Image': docker_container,\n                'ModelDataUrl': model_url,\n                'Environment': {\n                    'SAGEMAKER_PROGRAM': 'inference.py', #inference.py is at the root of my zipped CODE_TAR\n                    'SAGEMAKER_SUBMIT_DIRECTORY': code_url,\n                }\n            }\n        )\n    \n    def createEndpointConfig(self, endpoint_config_name, model_name):\n        ''' \n        Create an endpoint configuration (only for boto3 sdk procedure) and set production variants parameters.\n        Each retraining procedure will induce a new variant name based on the endpoint configuration name.\n        '''\n        smClient.create_endpoint_config(\n            EndpointConfigName=endpoint_config_name,\n            ProductionVariants=[\n                {\n                    'VariantName': endpoint_config_name,\n                    'ModelName': model_name,\n                    'InstanceType': INSTANCE_TYPE,\n                    'InitialInstanceCount': 1\n                }\n            ]\n        )\n\n    def createEndpoint(self, endpoint_name, endpoint_config_name):\n        '''\n        Deploy the model to an endpoint\n        '''\n        smClient.create_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=endpoint_config_name)\n    \n    def deleteOutdatedEndpointConfig(self, name_check, current_endpoint_config):\n        '''\n        Automatically detect and delete endpoint configurations that contain a string 'name_check'. This method can be used\n        after a retrain procedure to delete all previous endpoint configurations but keep the current one named 'current_endpoint_config'.\n        '''\n        # get a list of all available endpoint configurations\n        all_configs = smClient.list_endpoint_configs()['EndpointConfigs']\n\n        # loop over the names of endpoint configs\n        names_list = []\n        for config_dict in all_configs:\n            endpoint_config_name = config_dict['EndpointConfigName']\n\n            # get only endpoint configs that contain name_check in them and save names to a list\n            if name_check in endpoint_config_name:\n                names_list.append(endpoint_config_name)\n        \n        # remove the current endpoint configuration from the list (we do not want to detele this one since it is live)\n        names_list.remove(current_endpoint_config)\n\n        for name in names_list:\n            try:\n                smClient.delete_endpoint_config(EndpointConfigName=name)\n                print('Deleted endpoint configuration for %s' %name)\n            except:\n                print('INFO : No endpoint configuration was found for %s' %endpoint_config_name)\n\n    def updateEndpoint(self, endpoint_name, endpoint_config_name):\n        ''' \n        Update existing endpoint with a new retrained model\n        '''\n        smClient.update_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=endpoint_config_name,\n            RetainAllVariantProperties=True)\n    \n    def waitWhileCreating(self, endpoint_name):\n        ''' \n        While the endpoint is being created or updated sleep for 60 seconds.\n        '''\n        # wait while creating or updating endpoint\n        status = smClient.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n        print('Status: %s' %status)\n        while status != 'InService' and status !='Failed':\n            time.sleep(60)\n            status = smClient.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n            print('Status: %s' %status)\n        \n        # in case of a deployment failure raise an error\n        if status == 'Failed':\n            raise ValueError('Endpoint failed to deploy')\n\nif __name__==&quot;__main__&quot;:\n    deployer = Deployer('churnmodel', deployRetrained=True)\n    deployer.deploy()\n<\/code><\/pre>\n<p>Final comments :<\/p>\n<ul>\n<li><p>The sagemaker <a href=\"https:\/\/docs.amazonaws.cn\/en_us\/sagemaker\/latest\/dg\/realtime-endpoints-deployment.html\" rel=\"nofollow noreferrer\">documentation<\/a> mentions all this but fails to state that you can provide an 'entry_point' to the <code>create_model<\/code> method as well as a 'source_dir' for inference dependencies (e.g. normalization artifacts). It can be done as seen in <code>PrimaryContainer<\/code> argument.<\/p>\n<\/li>\n<li><p>my <code>fileManager.py<\/code> script just contains basic functions to make tar files, upload and download to and from my S3 paths. To simplify the class, I have not included them in.<\/p>\n<\/li>\n<li><p>The method deleteOutdatedEndpointConfig may seem like an overkill with an unnecessary loop and checks, I do so because I have multiple endpoint configurations to handle and wanted to remove the ones that weren't live AND contain the string <code>name_check<\/code> (I do not know the exact name of the configuration since there is a datetime suffix). Feel free to simplify it or remove it all together if you feel like it.<\/p>\n<\/li>\n<\/ul>\n<p>Hope it helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1663924957330,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1663925308150,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73728499",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66899120,
        "Question_title":"Validation error on Role name when running AWS SageMaker linear-learner locally",
        "Question_body":"<p>I'm trying to build a machine learning model locally using AWS SageMaker, but I got a validation error on IAM Role name. Although it's the exact role name that I created on the console.<\/p>\n<p>This is my code<\/p>\n<pre><code>    import boto3\n    import sagemaker\n    from sagemaker import get_execution_role\n    from sagemaker.amazon.amazon_estimator import image_uris\n    from sagemaker.amazon.amazon_estimator import RecordSet\n\n    sess = sagemaker.Session()\n\n\n    bucket = sagemaker.Session().default_bucket()\n    prefix = 'sagemaker\/ccard19'\n\n    role ='arn:aws:iam::94911111111542:role\/SageMaker-Full-Access '\n\n    container = image_uris.retrieve('linear-learner',boto3.Session().region_name)\n    \n    # Some other code\n\n   linear = sagemaker.LinearLearner(role=role,\n                                               instance_count=1,\n                                               instance_type='ml.m4.xlarge',\n                                               predictor_type='binary_classifier')\n  \n  # Some other code\n\n  ### Fit the classifier\n  linear.fit([train_records,val_records,test_records], wait=True, logs='All')\n\n<\/code><\/pre>\n<p>And this is the error message<\/p>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: 1 validation error detected: Value 'arn:aws:iam::949010940542:role\/SageMaker-Full-Access ' at 'roleArn' failed to satisfy constraint: Member must satisfy regular expression pattern: ^arn:aws[a-z\\-]*:iam::\\d{12}:role\/?[a-zA-Z_0-9+=,.@\\-_\/]+$\n<\/code><\/pre>\n<p>Any Help please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617256728180,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":560,
        "Owner_creation_time":1378039539503,
        "Owner_last_access_time":1663912967613,
        "Owner_location":null,
        "Owner_reputation":340,
        "Owner_up_votes":78,
        "Owner_down_votes":1,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You have <strong>space<\/strong> in the name. It should be:<\/p>\n<pre><code>role ='arn:aws:iam::94911111111542:role\/SageMaker-Full-Access'\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1617256809810,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66899120",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70156631,
        "Question_title":"How to pass additional parameters (as a dict) to sagemeker custom inference container?",
        "Question_body":"<p>Status:<\/p>\n<ul>\n<li>Custom container is built using the doc - <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own<\/a><\/li>\n<li>predict.py is coded to accommodate the custom inference script and its working well<\/li>\n<li>Using the classsagemaker.model.Model() class to pass the trained model.tar.gz and custom container image inorder to deploy the model<\/li>\n<\/ul>\n<p>Challenge:<\/p>\n<ul>\n<li>In the same Model class there is a ENV  parameter through which we can apparently send the environment variables to the custom image<\/li>\n<li>Tried passing a python dict to this , but facing difficulty to read this json dict inide the predict.py script<\/li>\n<\/ul>\n<p>Somebody faced the same difficulty ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1638197351547,
        "Question_score":0,
        "Question_tags":"python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":183,
        "Owner_creation_time":1604146329127,
        "Owner_last_access_time":1662133039343,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can pass your environment dict in your Model as:<\/p>\n<pre><code>Model(\n.\n.\nenv= {&quot;my_env&quot;: &quot;my_env_value&quot;}\n.\n.\n)\n<\/code><\/pre>\n<p>SageMaker will pass the enviroments dict to your container and you can access it in your predict.py script for example with:<\/p>\n<pre><code>my_env = os.environ.get('my_env',&quot;env key not set in Model&quot;)\nprint(my_env)\n<\/code><\/pre>\n<p>If your env dict was passed to your Model containing they <code>my_env<\/code> then you will receive the output : <code>my_env_value<\/code>. Else, then you will receive <code>env key not set in Model<\/code><\/p>\n<p>I work for AWS and my opinions are my own.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1645489319000,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70156631",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62187748,
        "Question_title":"Change datacapture encoding data to csv",
        "Question_body":"<p>I'm using sagemaker model monitor.<\/p>\n\n<p>When capturing data, it outputs the following json file.<\/p>\n\n<pre><code>{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text\/csv\",\"mode\":\"INPUT\",\"data\":\"MSwwLjUzLDAuNDIsMC4xMzUsMC42NzcsMC4yNTY1LDAuMTQxNSwwLjIx\",\"encoding\":\"BASE64\"},\"endpointOutput\":{\"observedContentType\":\"text\/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"MTEuNjQzNDU1NTA1MzcxMDk0\",\"encoding\":\"BASE64\"}},\"eventMetadata\":{\"eventId\":\"33404924-c0d4-4044-9dc2-1e1f5575cb0a\",\"inferenceTime\":\"2020-06-04T05:45:45Z\"},\"eventVersion\":\"0\"}\n<\/code><\/pre>\n\n<p>I want the encoding to be csv but somehow it outputs base64.<br>\nWhen or where do we change the setting of the encoding?<br>\nIs it during the invoking the endpoint? or set when making endpoint config.<br>\nI looked for some documents but I couldn't find it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591250114543,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":620,
        "Owner_creation_time":1532422348877,
        "Owner_last_access_time":1606441872017,
        "Owner_location":null,
        "Owner_reputation":27,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I just came across this same problem! Seems like you need to specify <code>CaptureContentTypeHeader<\/code> params to tell SageMaker which content type headers to treat as CSV (or JSON), versus the default which is to base64 encode the payload!<\/p>\n<p>So e.g. adding the following to your <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpointConfig.html\" rel=\"nofollow noreferrer\">CreateEndpointConfig<\/a> call or boto3\/sagemaker SDK equivalent should fix it:<\/p>\n<pre><code>{\n   &quot;DataCaptureConfig&quot;: { \n      &quot;CaptureContentTypeHeader&quot;: { \n         &quot;CsvContentTypes&quot;: [ &quot;text\/csv&quot; ]\n      },\n   }\n}\n<\/code><\/pre>\n<p>I guess this is to allow for non-standard Content-Type headers? Providing a layer of config to resolve e.g:<\/p>\n<ul>\n<li><code>application\/x-mycoolmodel<\/code> -&gt; <code>JSON<\/code>, versus<\/li>\n<li><code>application\/x-secretsauce<\/code> -&gt; <code>BASE64<\/code><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1593091560010,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62187748",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71197045,
        "Question_title":"How to pass the experiment configuration to a SagemakerTrainingOperator while training?",
        "Question_body":"<p>Idea:<\/p>\n<ul>\n<li>To use experiments and trials to log the training parameters and artifacts in sagemaker while using MWAA as the pipeline orchestrator<\/li>\n<\/ul>\n<p>I am using the training_config to create the dict to pass the training configuration to the Tensorflow estimator, but there is no parameter to pass the experiment configuration<\/p>\n<pre><code>tf_estimator = TensorFlow(entry_point='train_model.py',\n                                      source_dir= source\n                                      role=sagemaker.get_execution_role(),\n                                      instance_count=1,\n                                      framework_version='2.3.0',\n                                      instance_type=instance_type,\n                                      py_version='py37',\n                                      script_mode=True,\n                                      enable_sagemaker_metrics = True,\n                                      metric_definitions=metric_definitions,\n                                      output_path=output\n\nmodel_training_config = training_config(\n                    estimator=tf_estimator,\n                    inputs=input\n                    job_name=training_jobname,\n                )\n    \n\n\n\ntraining_task = SageMakerTrainingOperator(\n                    task_id=test_id,\n                    config=model_training_config,\n                    aws_conn_id=&quot;airflow-sagemaker&quot;,  \n                    print_log=True,\n                    wait_for_completion=True,\n                    check_interval=60  \n                )\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1645378809553,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|mwaa",
        "Question_view_count":128,
        "Owner_creation_time":1604146329127,
        "Owner_last_access_time":1662133039343,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The only way that i found right now is to use the CreateTrainigJob API (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html#sagemaker-CreateTrainingJob-request-RoleArn\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html#sagemaker-CreateTrainingJob-request-RoleArn<\/a>). The following steps are needed:<\/p>\n<ul>\n<li>I am not sure if this will work with Bring your own script method for E.g with a Tensorflow estimator<\/li>\n<li>it works with a build your own container approach<\/li>\n<li>Using the CreateTrainigJob API i created the configs which in turn includes all the needed configs like - training, experiment, algporthm etc and passed that to SagemakerTrainingOperator<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1646393704707,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71197045",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51517103,
        "Question_title":"Deploy my own tensorflow model on a virtual machine with AWS",
        "Question_body":"<p>I have a Tensorflow model which is working perfectly fine on my laptop (Tf 1.8 on OS HighSierra). However, I wanted to scale my operations up and use Amazon's Virtual Machine to run predictions faster. What is the best way to use my saved model and classify images in jpeg format which are stored locally? Thank you! <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1532515840557,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-ec2|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":196,
        "Owner_creation_time":1530739110150,
        "Owner_last_access_time":1649240672773,
        "Owner_location":null,
        "Owner_reputation":391,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":57,
        "Question_last_edit_time":null,
        "Answer_body":"<p>you have two options:<\/p>\n\n<p>1) Start a virtual machine on AWS (known as an Amazon EC2 instance). You can pick from many different instance types, including GPU instances. You'll have full administrative access on this machine, meaning that you can copy you TF model to it and predict just like you would on your own machine. <\/p>\n\n<p>More details on getting started with EC2 here: <a href=\"https:\/\/aws.amazon.com\/ec2\/getting-started\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/ec2\/getting-started\/<\/a> <\/p>\n\n<p>I would also recommend using the Deep Learning Amazon Machine Image, which bundles all the popular ML\/DL tools as well as the NVIDIA environment for GPU training\/prediction : <a href=\"https:\/\/aws.amazon.com\/machine-learning\/amis\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/machine-learning\/amis\/<\/a><\/p>\n\n<p>2) If you don't want to manage virtual machines, I'd recommend looking at Amazon SageMaker. You'll be able to import your TF model and to deploy it on fully-managed infrastructure for prediction. <\/p>\n\n<p>Here's a sample notebook showing you how to bring your own TF model to SageMaker: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/tensorflow_iris_byom\/tensorflow_BYOM_iris.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/tensorflow_iris_byom\/tensorflow_BYOM_iris.ipynb<\/a><\/p>\n\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1532521671907,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51517103",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72491505,
        "Question_title":"SageMaker Inference for a video input",
        "Question_body":"<p>I wonder if it's possible to run SageMaker Inference or Batch Transform job directly for a video input (.mp4 or another format)?<\/p>\n<p>If no could you please advice the best practice that might be used for pre-processing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654268345127,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":93,
        "Owner_creation_time":1442786553537,
        "Owner_last_access_time":1664039462827,
        "Owner_location":"Kyiv",
        "Owner_reputation":71,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":1654268469487,
        "Answer_body":"<p>Asynchronous inference could be a good option for this use case. There is a blog published by AWS that talks about how you can do this.<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-computer-vision-inference-on-large-videos-with-amazon-sagemaker-asynchronous-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-computer-vision-inference-on-large-videos-with-amazon-sagemaker-asynchronous-endpoints\/<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1654270724137,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72491505",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60816944,
        "Question_title":"AWS Sagemaker Notebook with multiple users",
        "Question_body":"<p>I am still new in AWS sagemaker. Working on a architecture where we would have an AWS sagemaker notebook. There would be multiple users, I want that students don`t see each other work. would I need to do that in terminal? or we can do that in notebook itself?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1584978635243,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":1907,
        "Owner_creation_time":1513883236660,
        "Owner_last_access_time":1663906475810,
        "Owner_location":"Noida, Uttar Pradesh, India",
        "Owner_reputation":465,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":46,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The simplest way is to create a small notebook instance for each student. This way you can have the needed isolation and also the responsibility of each student for their notebook to stop them when they are not in use.<\/p>\n\n<p>The smallest instance type <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">costs<\/a> $0.0464 per hour. If you have it running 24\/7 it costs about $30 per month. But if the students are responsible and stop their instances when they are not using them, it can be about $1 for 20 hours of work.<\/p>\n\n<p>If you want to enable isolation to the notebooks, you can use the ability to presign the URL that is used to open the Jupyter interface. See here on the way to use the CLI to create the URL: <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-presigned-notebook-instance-url.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-presigned-notebook-instance-url.html<\/a>. It is also supported in other SDK.<\/p>\n\n<pre><code>create-presigned-notebook-instance-url\n--notebook-instance-name &lt;student-instance-name&gt;\n--session-expiration-duration-in-seconds 3600\n<\/code><\/pre>\n\n<p>You can integrate it into the internal portal that you have in your institute. <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1585041076743,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1585077695677,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60816944",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48507471,
        "Question_title":"AWS Sagemaker custom user algorithms: how to take advantage of extra instances",
        "Question_body":"<p>This is a fundamental AWS Sagemaker question. When I run training with one of Sagemaker's built in algorithms I am able to take advantage of the massive speedup from distributing the job to many instances by increasing the instance_count argument of the training algorithm. However, when I package my own custom algorithm then increasing the instance count seems to just duplicate the training on every instance, leading to no speedup. <\/p>\n\n<p>I suspect that when I am packaging my own algorithm there is something special I need to do to control how it handles the training differently for a particular instance inside of the my custom train() function (otherwise, how would it know how the job should be distributed?), but I have not been able to find any discussion of how to do this online. <\/p>\n\n<p>Does anyone know how to handle this? Thank you very much in advance.<\/p>\n\n<p>Specific examples:\n=> It works well in a standard algorithm: I verified that increasing train_instance_count in the first documented sagemaker example speeds things up here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a><\/p>\n\n<p>=> It does not work in my custom algorithm. I tried taking the standard sklearn build-your-own-model example and adding a few extra sklearn variants inside of the training and then printing out results to compare. When I increase the train_instance_count that is passed to the Estimator object, it runs the same training on every instance, so the output gets duplicated across each instance (the printouts of the results are duplicated) and there is no speedup.\nThis is the sklearn example base: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a> . The third argument of the Estimator object partway down in this notebook is what lets you control the number of training instances.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1517249392277,
        "Question_score":3,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1369,
        "Owner_creation_time":1368093601223,
        "Owner_last_access_time":1599850446873,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Distributed training requires having a way to sync the results of the training between the training workers. Most of the traditional libraries, such as scikit-learn are designed to work with a single worker, and can't just be used in a distributed environment. Amazon SageMaker is distributing the data across the workers, but it is up to you to make sure that the algorithm can benefit from the multiple workers. Some algorithms, such as Random Forest, are easier to take advantage of the distribution, as each worker can build a different part of the forest, but other algorithms need more help. <\/p>\n\n<p>Spark MLLib has distributed implementations of popular algorithms such as k-means, logistic regression, or PCA, but these implementations are not good enough for some cases. Most of them were too slow and some even crushed when a lot of data was used for the training. The Amazon SageMaker team reimplemented many of these algorithms from scratch to benefit from the scale and economics of the cloud (20 hours of one instance costs the same as 1 hour of 20 instances, just 20 times faster). Many of these algorithms are now more stable and much faster beyond the linear scalability. See more details here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html<\/a><\/p>\n\n<p>For the deep learning frameworks (TensorFlow and MXNet) SageMaker is using the built-in parameters server that each one is using, but it is taking the heavy lifting of the building the cluster and configuring the instances to communicate with it. <\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1517706762273,
        "Answer_score":2.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48507471",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56799763,
        "Question_title":"Uploading a Dataframe to AWS S3 Bucket from SageMaker",
        "Question_body":"<p>I am new to AWS environment and trying to solve how the data flow works. After successfully uploading CSV files from S3 to SageMaker notebook instance, I am stuck on doing the reverse. <\/p>\n\n<p>I have a dataframe and want to upload that to S3 Bucket as CSV or JSON. The code that I have is below:<\/p>\n\n<pre><code>bucket='bucketname'\ndata_key = 'test.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\ndf.to_csv(data_location)\n<\/code><\/pre>\n\n<p>I assumed since I successfully used <code>pd.read_csv()<\/code> while loading, using <code>df.to_csv()<\/code> would also work but it didn't. Probably it is generating error because this way I cannot pick the privacy options while uploading a file manually to S3. Is there a way to upload the data to S3 from SageMaker?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1561682183347,
        "Question_score":9,
        "Question_tags":"python|pandas|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":16471,
        "Owner_creation_time":1541972092477,
        "Owner_last_access_time":1664035936107,
        "Owner_location":"Santa Clara, CA, USA",
        "Owner_reputation":731,
        "Owner_up_votes":65,
        "Owner_down_votes":0,
        "Owner_views":51,
        "Question_last_edit_time":1561688539710,
        "Answer_body":"<p>One way to solve this would be to save the CSV to the local storage on the SageMaker notebook instance, and then use the S3 API's via <code>boto3<\/code> to upload the file as an s3 object. \n<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file\" rel=\"noreferrer\">S3 docs for <code>upload_file()<\/code> available here.<\/a><\/p>\n\n<p>Note, you'll need to ensure that your SageMaker hosted notebook instance has proper <code>ReadWrite<\/code> permissions in its IAM role, otherwise you'll receive a permissions error.<\/p>\n\n<pre><code># code you already have, saving the file locally to whatever directory you wish\nfile_name = \"mydata.csv\" \ndf.to_csv(file_name)\n<\/code><\/pre>\n\n<pre><code># instantiate S3 client and upload to s3\nimport boto3\n\ns3 = boto3.resource('s3')\ns3.meta.client.upload_file(file_name, 'YOUR_S3_BUCKET_NAME', 'DESIRED_S3_OBJECT_NAME')\n<\/code><\/pre>\n\n<p>Alternatively, <code>upload_fileobj()<\/code> may help for parallelizing as a multi-part upload. <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1561684844023,
        "Answer_score":10.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":1561686321643,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56799763",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64302986,
        "Question_title":"How to highlight custom extractions using a2i's crowd-textract-analyze-document?",
        "Question_body":"<p>I would like to create a human review loop for images that undergone OCR using Amazon Textract and Entity Extraction using Amazon Comprehend.<\/p>\n<p>My process is:<\/p>\n<ol>\n<li>send image to Textract to extract the text<\/li>\n<li>send text to Comprehend to extract entities<\/li>\n<li>find the Block IDs in Textract's output of the entities extracted by Comprehend<\/li>\n<li>add new Blocks of type <code>KEY_VALUE_SET<\/code> to textract's JSON output <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/a2i-crowd-textract-detection.html\" rel=\"nofollow noreferrer\">per the docs<\/a><\/li>\n<li>create a Human Task with <code>crowd-textract-analyze-document<\/code> element in the template and feed it the modified textract output<\/li>\n<\/ol>\n<p>What fails to work in this process is step 5. My custom entities are not rendered properly. By &quot;fails to work&quot; I mean that the entities are not highlighted on the image when I click them on the sidebar. There is no error in the browser's console.<\/p>\n<p>Has anyone tried such a thing?<\/p>\n<p><em>Sorry for not including examples. I will remove secrets\/PII from my files and attach them to the question<\/em><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1602412144293,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-textract|amazon-comprehend",
        "Question_view_count":252,
        "Owner_creation_time":1354111242423,
        "Owner_last_access_time":1664060126030,
        "Owner_location":"Israel",
        "Owner_reputation":2162,
        "Owner_up_votes":390,
        "Owner_down_votes":16,
        "Owner_views":307,
        "Question_last_edit_time":1624091301367,
        "Answer_body":"<p>I used the AWS documentation of the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/a2i-crowd-textract-detection.html\" rel=\"nofollow noreferrer\">a2i-crowd-textract-detection human task element<\/a> to generate the value of the <code>initialValue<\/code> attribute. It appears the doc for that attribute is incorrect. While the the doc shows that the value should be in the same format as the output of Textract, namely:<\/p>\n<pre><code>[\n        {\n            &quot;BlockType&quot;: &quot;KEY_VALUE_SET&quot;,\n            &quot;Confidence&quot;: 38.43309020996094,\n            &quot;Geometry&quot;: { ... }\n            &quot;Id&quot;: &quot;8c97b240-0969-4678-834a-646c95da9cf4&quot;,\n            &quot;Relationships&quot;: [\n                { &quot;Type&quot;: &quot;CHILD&quot;, &quot;Ids&quot;: [...]},\n                { &quot;Type&quot;: &quot;VALUE&quot;, &quot;Ids&quot;: [...]}\n            ],\n            &quot;EntityTypes&quot;: [&quot;KEY&quot;],\n            &quot;Text&quot;: &quot;Foo bar&quot;\n        },\n]\n<\/code><\/pre>\n<p>the <code>a2i-crowd-textract-detection<\/code> expects the input to have lowerCamelCase attribute names (rather than UpperCamelCase). For example:<\/p>\n<pre><code>[\n        {\n            &quot;blockType&quot;: &quot;KEY_VALUE_SET&quot;,\n            &quot;confidence&quot;: 38.43309020996094,\n            &quot;geometry&quot;: { ... }\n            &quot;id&quot;: &quot;8c97b240-0969-4678-834a-646c95da9cf4&quot;,\n            &quot;relationships&quot;: [\n                { &quot;Type&quot;: &quot;CHILD&quot;, &quot;ids&quot;: [...]},\n                { &quot;Type&quot;: &quot;VALUE&quot;, &quot;ids&quot;: [...]}\n            ],\n            &quot;entityTypes&quot;: [&quot;KEY&quot;],\n            &quot;text&quot;: &quot;Foo bar&quot;\n        },\n]\n<\/code><\/pre>\n<p>I opened a support case about this documentation error to AWS.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1603012533233,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64302986",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65770913,
        "Question_title":"Sagemaker Studio Pyspark example fails",
        "Question_body":"<p>When I try to run the Sagemaker provided examples with PySpark in Sagemaker Studio<\/p>\n<pre><code>import os\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nimport sagemaker\nfrom sagemaker import get_execution_role\nimport sagemaker_pyspark\n\nrole = get_execution_role()\n\n# Configure Spark to use the SageMaker Spark dependency jars\njars = sagemaker_pyspark.classpath_jars()\n\nclasspath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars())\n\n# See the SageMaker Spark Github repo under sagemaker-pyspark-sdk\n# to learn how to connect to a remote EMR cluster running Spark from a Notebook Instance.\nspark = SparkSession.builder.config(&quot;spark.driver.extraClassPath&quot;, classpath)\\\n    .master(&quot;local[*]&quot;).getOrCreate()\n<\/code><\/pre>\n<p>I get the following exception:<\/p>\n<pre><code>    ---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n&lt;ipython-input-6-c8f6fff0daaf&gt; in &lt;module&gt;\n     19 # to learn how to connect to a remote EMR cluster running Spark from a Notebook Instance.\n     20 spark = SparkSession.builder.config(&quot;spark.driver.extraClassPath&quot;, classpath)\\\n---&gt; 21     .master(&quot;local[*]&quot;).getOrCreate()\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/sql\/session.py in getOrCreate(self)\n    171                     for key, value in self._options.items():\n    172                         sparkConf.set(key, value)\n--&gt; 173                     sc = SparkContext.getOrCreate(sparkConf)\n    174                     # This SparkContext may be an existing one.\n    175                     for key, value in self._options.items():\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/context.py in getOrCreate(cls, conf)\n    361         with SparkContext._lock:\n    362             if SparkContext._active_spark_context is None:\n--&gt; 363                 SparkContext(conf=conf or SparkConf())\n    364             return SparkContext._active_spark_context\n    365 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/context.py in __init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\n    127                     &quot; note this option will be removed in Spark 3.0&quot;)\n    128 \n--&gt; 129         SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    130         try:\n    131             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/context.py in _ensure_initialized(cls, instance, gateway, conf)\n    310         with SparkContext._lock:\n    311             if not SparkContext._gateway:\n--&gt; 312                 SparkContext._gateway = gateway or launch_gateway(conf)\n    313                 SparkContext._jvm = SparkContext._gateway.jvm\n    314 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/java_gateway.py in launch_gateway(conf)\n     44     :return: a JVM gateway\n     45     &quot;&quot;&quot;\n---&gt; 46     return _launch_gateway(conf)\n     47 \n     48 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/java_gateway.py in _launch_gateway(conf, insecure)\n    106 \n    107             if not os.path.isfile(conn_info_file):\n--&gt; 108                 raise Exception(&quot;Java gateway process exited before sending its port number&quot;)\n    109 \n    110             with open(conn_info_file, &quot;rb&quot;) as info:\n\nException: Java gateway process exited before sending its port number\n<\/code><\/pre>\n<p>Before running the example I installed pyspark and sagemaker_pyspark with pip from the notebook. I am also using SparkMagic kernel from the kernels library of SageMaker.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1610957989217,
        "Question_score":2,
        "Question_tags":"amazon-web-services|pyspark|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":1827,
        "Owner_creation_time":1452190055593,
        "Owner_last_access_time":1660055701297,
        "Owner_location":"Germany",
        "Owner_reputation":247,
        "Owner_up_votes":40,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Maybe, you are having this issue because this notebook was designed to run when you have an EMR cluster. I suggest you start a notebook with conda_python3 kernel on Sagemaker instead of the SparkMagic kernel. You will need to install <code>pyspark<\/code> and <code>sagemaker_pyspark<\/code> using pip, but it should work with the code you posted.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1611005573523,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65770913",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72663991,
        "Question_title":"Using external libraries for model training in aws sagemaker",
        "Question_body":"<p>I am just getting started with aws sagemaker and realized it doesn't have a random forest classifier. I found this github tutorial on creating your own and deploying it in sagemaker: <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a>.<\/p>\n<p>I am using the python sdk and was more or less curious to see if anyone actually uses this or any external libraries for training with sagemaker. It seems that if you aren't using the built in algorithms then it is very involved to create your own and the functionality of the model and ability to interpret it is very limited once you do get it trained.<\/p>\n<p>For example after deploying the model to an aws endpoint and pulling down the artifacts I could only call the <code>predict<\/code> method (no <code>predict_probab<\/code> as is possible in the actual <code>sklearn randomforestclassifier<\/code>). I also haven't been able to find anything like what you get in <code>sklearn.metrics<\/code> such as <code>accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix<\/code> etc so I'm assuming one would need to build equivalents to these from scratch to be able to interpret their model.<\/p>\n<p>I have spent a week or so researching this and trying to get this rigged up and it just seems that importing external libraries for model training in sagemaker is not very popular or well-documented online. Interested to know if I'm just unaware of more functionality or if there are alternatives that people prefer or if I should just stick with the built in xgboost classifier if I am looking for a tree-based option. Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655494198873,
        "Question_score":0,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":50,
        "Owner_creation_time":1589338575433,
        "Owner_last_access_time":1664013922157,
        "Owner_location":null,
        "Owner_reputation":373,
        "Owner_up_votes":73,
        "Owner_down_votes":0,
        "Owner_views":110,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It is possible to add extra packages for training when using any of the Framework containers such as SKLearn. Kindly see this <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#using-third-party-libraries\" rel=\"nofollow noreferrer\">link<\/a> for more information.<\/p>\n<p>From a hosting perspective, you do have the ability to provide a custom entry_point \/ inference.py script that you can use to control model loading, pre and post processing. Please see this <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#load-a-model\" rel=\"nofollow noreferrer\">link<\/a> for more information<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1657063869133,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72663991",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59430560,
        "Question_title":"Reading Data from AWS S3",
        "Question_body":"<p>I have some data with very particular format (e.g., tdms files generated by NI systems) and I stored them in a S3 bucket. Typically, for reading this data in python if the data was stored in my local computer, I would use npTDMS package. But, how should is read this tdms files when they are stored in a S3 bucket? One solution is to download the data for instance to the EC2 instance and then use npTDMS package for reading the data into python. But it does not seem to be a perfect solution. Is there any way that I can read the data similar to reading CSV files from S3? <\/p>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_time":1576870865157,
        "Question_score":1,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":4393,
        "Owner_creation_time":1534965197293,
        "Owner_last_access_time":1663788259063,
        "Owner_location":"Seattle, WA",
        "Owner_reputation":320,
        "Owner_up_votes":91,
        "Owner_down_votes":2,
        "Owner_views":48,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Some Python packages (such as Pandas) support reading data directly from S3, as it is the most popular location for data. See <a href=\"https:\/\/stackoverflow.com\/questions\/37703634\/how-to-import-a-text-file-on-aws-s3-into-pandas-without-writing-to-disk\">this question<\/a> for example on the way to do that with Pandas.<\/p>\n\n<p>If the package (npTDMS) doesn't support reading directly from S3, you should copy the data to the local disk of the notebook instance.<\/p>\n\n<p>The simplest way to copy is to run the AWS CLI in a cell in your notebook<\/p>\n\n<pre><code>!aws s3 cp s3:\/\/bucket_name\/path_to_your_data\/ data\/\n<\/code><\/pre>\n\n<p>This command will copy all the files under the \"folder\" in S3 to the local folder <code>data<\/code><\/p>\n\n<p>You can use more fine-grained copy using the filtering of the files and other specific requirements using the boto3 rich capabilities. For example:<\/p>\n\n<pre><code>s3 = boto3.resource('s3')\nbucket = s3.Bucket('my-bucket')\nobjs = bucket.objects.filter(Prefix='myprefix')\nfor obj in objs:\n   obj.download_file(obj.key)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1577181091057,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1577202690473,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59430560",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59679192,
        "Question_title":"Hosting multiple models for multiple datasets in aws sagemaker",
        "Question_body":"<p>I read that there is a way to train and host multiple models using a single endpoint for a single dataset in AWS Sagemaker. But I have 2 different datasets in S3 and have to train a model for each dataset. Can these 2 different models be hosted using a single endpoint? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1578649761440,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":411,
        "Owner_creation_time":1550756471933,
        "Owner_last_access_time":1663939288837,
        "Owner_location":null,
        "Owner_reputation":67,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes, this is called a multi-model endpoint. You can use a large number of models on the same endpoint. They get loaded and unloaded dynamically as needed, and you simply have to pass the model name in your prediction request.<\/p>\n\n<p>Here are some resources:<\/p>\n\n<ul>\n<li><p>Blog post + example : <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/<\/a><\/p><\/li>\n<li><p>Video explaining model deployment scenarios on SageMaker: <a href=\"https:\/\/youtu.be\/dT8jmdF-ZWw\" rel=\"nofollow noreferrer\">https:\/\/youtu.be\/dT8jmdF-ZWw<\/a><\/p><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1578755333997,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59679192",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61181955,
        "Question_title":"Is there any limits of saving result on S3 from sagemaker Processing?",
        "Question_body":"<p>\u203b I used google translation, if you have any question, let me know!<\/p>\n\n<p>I am trying to run python script with huge 4 data, using sagemaker processing. And my current situation are as follows:<\/p>\n\n<ul>\n<li>can run this script with 3 data<\/li>\n<li>can't run the script with only 1 data (the biggest, the same structure with others)<\/li>\n<li>as for all of 4 data, the script has finished (so, I suspected this error in S3, ie. when copying sagemaker result to S3)<\/li>\n<\/ul>\n\n<p>The error I got is this InternalServerError.<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"sagemaker_train_and_predict.py\", line 56, in &lt;module&gt;\n    outputs=outputs\n  File \"{xxx}\/sagemaker_constructor.py\", line 39, in run\n    outputs=outputs\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 408, in run\n    self.latest_job.wait(logs=logs)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 723, in wait\n    self.sagemaker_session.logs_for_processing_job(self.job_name, wait=True)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 3111, in logs_for_processing_job\n    self._check_job_status(job_name, description, \"ProcessingJobStatus\")\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 2615, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Processing job sagemaker-vm-train-and-predict-2020-04-12-04-15-40-655: Failed. Reason: InternalServerError: We encountered an internal error.  Please try again.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586755348657,
        "Question_score":1,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":95,
        "Owner_creation_time":1586754432800,
        "Owner_last_access_time":1593512686190,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There may be some issue transferring the output data to S3 if the output is generated at a high rate and size is too large. <\/p>\n\n<p>You can 1) try to slow down writing the output a bit or 2) call S3 from your algorithm container to upload the output directly using boto client (<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html<\/a>).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1586886613193,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61181955",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62453292,
        "Question_title":"How to find memory leak in Python MXNet?",
        "Question_body":"<p>I am afraid that my Neural Network in MXNet, written in Python, has a memory leak. I have tried the MXNet profiler and the tracemalloc module to get an understanding of memory profiling, but I want to get information on any potential memory leaks, just like I'd do with valgrind in C.<\/p>\n\n<p>I found <a href=\"https:\/\/cwiki.apache.org\/confluence\/display\/MXNET\/Detecting+Memory+Leaks+and+Buffer+Overflows+in+MXNet\" rel=\"nofollow noreferrer\">Detecting Memory Leaks and Buffer Overflows in MXNet<\/a>, and after managing to build like described in section \"Using ASAN builds with MXNet\", by replacing the \"ubuntu_cpu\" part in <code>docker\/Dockerfile.build.ubuntu_cpu -t mxnetci\/build.ubuntu_cpu<\/code> with \"ubuntu_cpu_python\", I tried executing in an AWS Sagemaker Notebook like this:<\/p>\n\n<pre><code>root@33e38e00f825:\/work\/mxnet# nosetests3 --verbose \/home\/ec2-user\/SageMaker\/run_predict.py\n<\/code><\/pre>\n\n<p>and I get this import error:<\/p>\n\n<blockquote>\n  <p>Failure: ImportError (No module named 'run_predict') ... ERROR<\/p>\n<\/blockquote>\n\n<p>My run_predict.py looks like this:<\/p>\n\n<pre><code>#!\/usr\/bin\/env python\ndef run_predict(n):\n  # calling MXNet inference method\n\nrun_predict(-1)  # tried it putting it under 'if __name__ == \"__main__\":'\n<\/code><\/pre>\n\n<p>What I am missing in my script, what should I change?<\/p>\n\n<p>The example script they use in the link is <a href=\"https:\/\/github.com\/apache\/incubator-mxnet\/blob\/faccd91071cc34ed0b3a192d3c7932441fe7e35e\/tests\/python\/unittest\/test_rnn.py\" rel=\"nofollow noreferrer\">rnn_test.py<\/a>, but even when I run this example, I still get an analogous Import Error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592493163510,
        "Question_score":0,
        "Question_tags":"python|machine-learning|memory-leaks|amazon-sagemaker|mxnet",
        "Question_view_count":263,
        "Owner_creation_time":1369257942213,
        "Owner_last_access_time":1663776093950,
        "Owner_location":"London, UK",
        "Owner_reputation":70285,
        "Owner_up_votes":7595,
        "Owner_down_votes":12100,
        "Owner_views":13121,
        "Question_last_edit_time":1592579151163,
        "Answer_body":"<p>In MXNet, we automatically test for this through examining the garbage collection records. You can find how it's implemented here: <a href=\"https:\/\/github.com\/apache\/incubator-mxnet\/blob\/c3aff732371d6177e5d522c052fb7258978d8ce4\/tests\/python\/conftest.py#L26-L79\" rel=\"nofollow noreferrer\">https:\/\/github.com\/apache\/incubator-mxnet\/blob\/c3aff732371d6177e5d522c052fb7258978d8ce4\/tests\/python\/conftest.py#L26-L79<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1598820674153,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62453292",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73232032,
        "Question_title":"Start execution of existing SageMaker pipeline using Python SDK",
        "Question_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/run-pipeline.html#run-pipeline-prereq\" rel=\"nofollow noreferrer\">SageMaker documentatin<\/a> explains how to run a pipeline, but it assumes I have just <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html\" rel=\"nofollow noreferrer\">defined it<\/a> and I have the object <code>pipeline<\/code> available.<\/p>\n<p>How can I run an <strong>existing<\/strong> pipeline with <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"nofollow noreferrer\">Python SDK<\/a>?<\/p>\n<p>I know how to read a pipeline with AWS CLI (i.e. <code>aws sagemaker describe-pipeline --pipeline-name foo<\/code>). Can the same be done with Python code? Then I would have <code>pipeline<\/code> object ready to use.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659598635513,
        "Question_score":0,
        "Question_tags":"aws-sdk|amazon-sagemaker",
        "Question_view_count":106,
        "Owner_creation_time":1217615304817,
        "Owner_last_access_time":1664033547990,
        "Owner_location":"Poland",
        "Owner_reputation":16694,
        "Owner_up_votes":3004,
        "Owner_down_votes":154,
        "Owner_views":3155,
        "Question_last_edit_time":1659598939403,
        "Answer_body":"<p>If the Pipeline has been created, you can use the Python Boto3 SDK to make the <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.start_pipeline_execution\" rel=\"nofollow noreferrer\"><code>StartPipelineExecution<\/code><\/a> API call.<\/p>\n<pre><code>response = client.start_pipeline_execution(\n    PipelineName='string',\n    PipelineExecutionDisplayName='string',\n    PipelineParameters=[\n        {\n            'Name': 'string',\n            'Value': 'string'\n        },\n    ],\n    PipelineExecutionDescription='string',\n    ClientRequestToken='string',\n    ParallelismConfiguration={\n        'MaxParallelExecutionSteps': 123\n    }\n)\n<\/code><\/pre>\n<p>If you prefer AWS CLI, the most basic call is:<\/p>\n<pre><code>aws sagemaker start-pipeline-execution --pipeline-name &lt;name-of-the-pipeline&gt;\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1659628256873,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1660212471007,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73232032",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51698373,
        "Question_title":"Amazon SageMaker: Invoke endpoint with file as multipart\/form-data",
        "Question_body":"<p>After setting up an endpoint for my model on Amazon SageMaker, I am trying to invoke it with a POST request which contains a file with a key <code>image<\/code> &amp; content type as <code>multipart\/form-data<\/code>.<\/p>\n\n<p>My AWS CLI command is like this:<\/p>\n\n<pre><code>aws sagemaker-runtime invoke-endpoint --endpoint-name &lt;endpoint-name&gt; --body image=@\/local\/file\/path\/dummy.jpg --content-type multipart\/form-data output.json --region us-east-1\n<\/code><\/pre>\n\n<p>which should be an equivalent of:<\/p>\n\n<pre><code>curl -X POST -F \"image=@\/local\/file\/path\/dummy.jpg\" http:\/\/&lt;endpoint&gt;\n<\/code><\/pre>\n\n<p>After running the <code>aws<\/code> command, the file is not transferred via the request, and my model is receiving the request without any file in it.<\/p>\n\n<p>Can someone please tell me what should be the correct format of the <code>aws<\/code> command in order to achieve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1533504341903,
        "Question_score":6,
        "Question_tags":"amazon-web-services|curl|aws-cli|amazon-sagemaker",
        "Question_view_count":2346,
        "Owner_creation_time":1472843515757,
        "Owner_last_access_time":1568228274983,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The first problem is that you're using 'http' for your CURL request. Virtually all AWS services strictly use 'https' as their protocol, SageMaker included. <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/rande.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/rande.html<\/a>. I'm going to assume this was a typo though.<\/p>\n\n<p>You can check the verbose output of the AWS CLI by passing the '--debug' argument to your call. I re-ran a similar experiment with my favorite duck.jpg image:<\/p>\n\n<pre><code>aws --debug sagemaker-runtime invoke-endpoint --endpoint-name MyEndpoint --body image=@\/duck.jpg --content-type multipart\/form-data  &gt;(cat)\n<\/code><\/pre>\n\n<p>Looking at the output, I see:<\/p>\n\n<pre><code>2018-08-10 08:42:20,870 - MainThread - botocore.endpoint - DEBUG - Making request for OperationModel(name=InvokeEndpoint) (verify_ssl=True) with params: {'body': 'image=@\/duck.jpg', 'url': u'https:\/\/sagemaker.us-west-2.amazonaws.com\/endpoints\/MyEndpoint\/invocations', 'headers': {u'Content-Type': 'multipart\/form-data', 'User-Agent': 'aws-cli\/1.15.14 Python\/2.7.10 Darwin\/16.7.0 botocore\/1.10.14'}, 'context': {'auth_type': None, 'client_region': 'us-west-2', 'has_streaming_input': True, 'client_config': &lt;botocore.config.Config object at 0x109a58ed0&gt;}, 'query_string': {}, 'url_path': u'\/endpoints\/MyEndpoint\/invocations', 'method': u'POST'}\n<\/code><\/pre>\n\n<p>It looks like the AWS CLI is using the string literal '@\/duck.jpg', not the file contents.<\/p>\n\n<p>Trying again with curl and the \"--verbose\" flag:<\/p>\n\n<pre><code>curl --verbose -X POST -F \"image=@\/duck.jpg\" https:\/\/sagemaker.us-west-2.amazonaws.com\/endpoints\/MyEndpoint\/invocations\n<\/code><\/pre>\n\n<p>I see the following:<\/p>\n\n<pre><code>Content-Length: 63097\n<\/code><\/pre>\n\n<p>Much better. The '@' operator is a CURL specific feature. The AWS CLI does have a way to pass files though: <\/p>\n\n<pre><code>--body fileb:\/\/\/duck.jpg\n<\/code><\/pre>\n\n<p>There is also a 'file' for non-binary files such as JSON. Unfortunately you cannot have the prefix. That is, you cannot say:<\/p>\n\n<pre><code> --body image=fileb:\/\/\/duck.jpg\n<\/code><\/pre>\n\n<p>You can prepend the string 'image=' to your file with a command such as the following. (You'll probably need to be more clever if your images are really big; this is really inefficient.)<\/p>\n\n<pre><code> echo -e \"image=$(cat \/duck.jpg)\" &gt; duck_with_prefix\n<\/code><\/pre>\n\n<p>Your final command would then be:<\/p>\n\n<pre><code> aws sagemaker-runtime invoke-endpoint --endpoint-name MyEndpoint --body fileb:\/\/\/duck_with_prefix --content-type multipart\/form-data  &gt;(cat)\n<\/code><\/pre>\n\n<p>Another note: Using raw curl with AWS services is extremely difficult due to the AWS Auth signing requirements - <a href=\"https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/API\/sig-v4-authenticating-requests.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/API\/sig-v4-authenticating-requests.html<\/a> <\/p>\n\n<p>It can be done, but you'll likely be more productive by using the AWS CLI or a pre-existing tool such as Postman - <a href=\"https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/how-to-use-postman-to-call-api.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/how-to-use-postman-to-call-api.html<\/a> <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1533918119980,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51698373",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55854377,
        "Question_title":"comprehend.start_topics_detection_job Fails with Silent Error?",
        "Question_body":"<p>I have <a href=\"https:\/\/docs.aws.amazon.com\/code-samples\/latest\/catalog\/python-comprehend-TopicModeling.py.html\" rel=\"nofollow noreferrer\">Amazon sample code<\/a> for running <code>comprehend.start_topics_detection_job<\/code>. Here is the code with the variables filled in for my job:<\/p>\n\n<pre><code>import re\nimport csv\nimport pytz\nimport boto3\nimport json\n\n# https:\/\/docs.aws.amazon.com\/code-samples\/latest\/catalog\/python-comprehend-TopicModeling.py.html\n# https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/API_InputDataConfig.html\n\n# Set these values before running the program\ninput_s3_url = \"s3:\/\/comprehend-topic-modelling-bucket\/input_800_cleaned_articles\/\"\ninput_doc_format = \"ONE_DOC_PER_LINE\"\noutput_s3_url = \"s3:\/\/comprehend-topic-modelling-bucket\/output\"\ndata_access_role_arn = \"arn:aws:iam::372656143103:role\/access-aws-services-from-sagemaker\"\nnumber_of_topics = 30\n\n# Set up job configuration\ninput_data_config = {\"S3Uri\": input_s3_url, \"InputFormat\": input_doc_format}\noutput_data_config = {\"S3Uri\": output_s3_url}\n\n# Begin a job to detect the topics in the document collection\ncomprehend = boto3.client('comprehend')\nstart_result = comprehend.start_topics_detection_job(\n    NumberOfTopics=number_of_topics,\n    InputDataConfig=input_data_config,\n    OutputDataConfig=output_data_config,\n    DataAccessRoleArn=data_access_role_arn)\n\n# Output the results\nprint('Start Topic Detection Job: ' + json.dumps(start_result))\njob_id = start_result['JobId']\nprint(f'job_id: {job_id}')\n\n# Retrieve and output information about the job\ndescribe_result = comprehend.describe_topics_detection_job(JobId=job_id)\nprint('Describe Job: ' + json.dumps(describe_result)) . #&lt;===LINE 36\n\n# List and output information about current jobs\nlist_result = comprehend.list_topics_detection_jobs()\nprint('list_topics_detection_jobs_result: ' + json.dumps(list_result))\n<\/code><\/pre>\n\n<p>It's failing with the error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-8-840a7ee043d4&gt; in &lt;module&gt;()\n     34 # Retrieve and output information about the job\n     35 describe_result = comprehend.describe_topics_detection_job(JobId=job_id)\n---&gt; 36 print('Describe Job: ' + json.dumps(describe_result))\n     37 \n     38 # List and output information about current jobs\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\n    229         cls is None and indent is None and separators is None and\n    230         default is None and not sort_keys and not kw):\n--&gt; 231         return _default_encoder.encode(obj)\n    232     if cls is None:\n    233         cls = JSONEncoder\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in encode(self, o)\n    197         # exceptions aren't as detailed.  The list call should be roughly\n    198         # equivalent to the PySequence_Fast that ''.join() would do.\n--&gt; 199         chunks = self.iterencode(o, _one_shot=True)\n    200         if not isinstance(chunks, (list, tuple)):\n    201             chunks = list(chunks)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in iterencode(self, o, _one_shot)\n    255                 self.key_separator, self.item_separator, self.sort_keys,\n    256                 self.skipkeys, _one_shot)\n--&gt; 257         return _iterencode(o, 0)\n    258 \n    259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in default(self, o)\n    178         \"\"\"\n    179         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n--&gt; 180                         o.__class__.__name__)\n    181 \n    182     def encode(self, o):\n\nTypeError: Object of type 'datetime' is not JSON serializable\n<\/code><\/pre>\n\n<p>It fails instantly, the second I pus \"run\". It seems to me that the call to <code>comprehend.start_topics_detection_job<\/code> may be failing, leading to an error line 36, <code>print('Describe Job: ' + json.dumps(describe_result))<\/code>.<\/p>\n\n<p>What am I missing?<\/p>\n\n<p><strong>UPDATE<\/strong><\/p>\n\n<p>The same IAM role is being used for the notebook, as well as in the above code. Here are the permissions currently assigned to that IAM role:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/6ihIr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6ihIr.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1556211825007,
        "Question_score":0,
        "Question_tags":"python|django|machine-learning|amazon-sagemaker|amazon-comprehend",
        "Question_view_count":232,
        "Owner_creation_time":1276294622427,
        "Owner_last_access_time":1664061462980,
        "Owner_location":null,
        "Owner_reputation":4334,
        "Owner_up_votes":409,
        "Owner_down_votes":1,
        "Owner_views":496,
        "Question_last_edit_time":1586235824610,
        "Answer_body":"<p>It turns out that there was nothing wrong with the call to <code>comprehend.describe_topics_detection_job<\/code> -- it was just returning, in <code>describe_result<\/code>, something that could not be json serialized, so <code>json.dumps(describe_result))<\/code> was throwing an error. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1556698045340,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55854377",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56497428,
        "Question_title":"Use images in s3 with SageMaker without .lst files",
        "Question_body":"<p>I am trying to create (what I thought was) a simple image classification pipeline between s3 and SageMaker.<\/p>\n\n<p>Images are stored in an s3 bucket with their class labels in their file names currently, e.g.<\/p>\n\n<p><strong>My-s3-bucket-dir<\/strong><\/p>\n\n<pre><code>cat-1.jpg\ndog-1.jpg\ncat-2.jpg\n..\n<\/code><\/pre>\n\n<p>I've been trying to leverage several related example .py scripts, but most seem to be download data sets already in .rec format or containing special manifest or annotation files I don't have.<\/p>\n\n<p>All I want is to pass the images from s3 to the SageMaker image classification algorithm that's located in the same region, IAM account, etc. I suppose this means I need a <code>.lst<\/code> file<\/p>\n\n<p>When I try to manually create the <code>.lst<\/code> it doesn't seem to like it and it also takes too long doing manual work to be a good practice.<\/p>\n\n<p>How can I automatically generate the <code>.lst<\/code> file (or otherwise send the images\/classes for training)? <\/p>\n\n<p>Things I read made it sound like <code>im2rec.py<\/code> was a solution, but I don't see how. The example I'm working with now is <\/p>\n\n<p><code>Image-classification-fulltraining-highlevel.ipynb<\/code><\/p>\n\n<p>but it seems to download the data as <code>.rec<\/code>, <\/p>\n\n<pre><code>download('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-train.rec')\ndownload('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-val.rec')\n<\/code><\/pre>\n\n<p>which just skips working with the .jpeg files. I found another that converts them to <code>.rec<\/code> but again it has essentially the <code>.lst<\/code> already as <code>.json<\/code> and just converts it.<\/p>\n\n<p>I have mostly been working in a Python Jupyter notebook within the AWS console (in my browser) but I have also tried using their GUI. <\/p>\n\n<p>How can I simply and automatically generate the <code>.lst<\/code> or otherwise get the data\/class info into SageMaker without manually creating a <code>.lst<\/code> file?<\/p>\n\n<p><strong><em>Update<\/em><\/strong><\/p>\n\n<p>It looks like im2py can't be run against s3. You'd have to completely download everything from all s3 buckets into the notebook's storage...<\/p>\n\n<blockquote>\n  <p>Please note that [...] im2rec.py is running locally,\n  therefore cannot take input from the S3 bucket. To generate the list\n  file, you need to download the data and then use the im2rec tool. - AWS SageMaker Team<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559921851153,
        "Question_score":2,
        "Question_tags":"python-3.x|amazon-s3|computer-vision|amazon-sagemaker",
        "Question_view_count":937,
        "Owner_creation_time":1399301338467,
        "Owner_last_access_time":1664044371257,
        "Owner_location":"Columbia, MD, USA",
        "Owner_reputation":21413,
        "Owner_up_votes":2163,
        "Owner_down_votes":516,
        "Owner_views":6465,
        "Question_last_edit_time":1560181849817,
        "Answer_body":"<p>There are 3 options to provide annotated data to the Image Classification algo: (1) packing labels in recordIO files, (2) storing labels in a JSON manifest file (\"augmented manifest\" option), (3) storing labels in a list file. All options are documented here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html<\/a>.<\/p>\n\n<p>Augmented Manifest and .lst files option are quick to do since they just require you to create an annotation file with a usually quick <code>for<\/code> loop for example. RecordIO requires you to use <code>im2rec.py<\/code> tool, which is a little more work.<\/p>\n\n<p>Using .lst files is <strong>another option<\/strong> that is reasonably easy: you just need to create annotation them with a quick for loop, like this:<\/p>\n\n<pre><code># assuming train_index, train_class, train_pics store the pic index, class and path\n\nwith open('train.lst', 'a') as file:\n    for index, cl, pic in zip(train_index, train_class, train_pics):\n        file.write(str(index) + '\\t' + str(cl) + '\\t' + pic + '\\n')\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1559948298333,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56497428",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68736614,
        "Question_title":"How to set \"Task title\" of a SageMaker GrountTruth labeling job",
        "Question_body":"<p>I'm creating a SageMaker GroundTruth Labeling Job using the console UI. I'm looking for a way to configure &quot;Task title&quot;, which is shown in the workers Job list.<\/p>\n<p>I think this is related to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-create-labeling-job-api.html\" rel=\"nofollow noreferrer\">TaskTitle<\/a> configuration of AWS CLI. However, I cannot configure it from the AWS console. Can we configure it from the console GUI?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1628661017973,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":55,
        "Owner_creation_time":1354705336800,
        "Owner_last_access_time":1663474776330,
        "Owner_location":"Pittsburgh",
        "Owner_reputation":2517,
        "Owner_up_votes":90,
        "Owner_down_votes":13,
        "Owner_views":78,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I was also doing the similar stuff and running a sagemaker GT labeling job.\nI am following the below git repo by amazon :<\/p>\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline<\/a><\/p>\n<p>If you look into the below file in the repo:<\/p>\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/main\/notebooks\/util\/smgt.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/main\/notebooks\/util\/smgt.py<\/a><\/p>\n<p>at line no:349 ,you will see how they do it and you can do the similar change in your code.<\/p>\n<p><strong>&quot;TaskTitle&quot;: &quot;Credit Card Agreement Entities&quot;<\/strong><\/p>\n<p>Hope this will help.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1656482770813,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68736614",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51968742,
        "Question_title":"AWS sagemaker invokeEndpoint model internal error",
        "Question_body":"<p>I am trying to send a request on a model on sagemaker using .NET. The code I am using is: <\/p>\n\n<pre><code>var data = File.ReadAllBytes(@\"C:\\path\\file.csv\");\nvar credentials = new Amazon.Runtime.BasicAWSCredentials(\"\",\"\");\nvar awsClient = new AmazonSageMakerRuntimeClient(credentials, RegionEndpoint.EUCentral1);\nvar request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest\n{\n    EndpointName = \"EndpointName\",\n    ContentType = \"text\/csv\",\n    Body = new MemoryStream(data),\n};\n\nvar response = awsClient.InvokeEndpoint(request);\nvar predictions = Encoding.UTF8.GetString(response.Body.ToArray());\n<\/code><\/pre>\n\n<p>the error that I am getting on <code>awsClient.InvokeEndpoint(request)<\/code><\/p>\n\n<p>is:<\/p>\n\n<blockquote>\n  <p>Amazon.SageMakerRuntime.Model.ModelErrorException: 'The service\n  returned an error with Error Code ModelError and HTTP Body:\n  {\"ErrorCode\":\"INTERNAL_FAILURE_FROM_MODEL\",\"LogStreamArn\":\"arn:aws:logs:eu-central-1:xxxxxxxx:log-group:\/aws\/sagemaker\/Endpoints\/myEndpoint\",\"Message\":\"Received\n  server error (500) from model with message \\\"\\\". See\n  \"https:\/\/ url_to_logs_on_amazon\"\n  in account xxxxxxxxxxx for more\n  information.\",\"OriginalMessage\":\"\",\"OriginalStatusCode\":500}'<\/p>\n<\/blockquote>\n\n<p>the url that the error message suggests for more information does not help at all.<\/p>\n\n<p>I believe that it is a data format issue but I was not able to find a solution.<\/p>\n\n<p>Does anyone has encountered this behavior before?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1534946906973,
        "Question_score":3,
        "Question_tags":"c#|.net|amazon-web-services|amazon-sagemaker",
        "Question_view_count":4385,
        "Owner_creation_time":1495715386880,
        "Owner_last_access_time":1592382740743,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The problem relied on the data format as suspected. In my case all I had to do is send the data as a json serialized string array and use <code>ContentType = application\/json<\/code> because the python function running on the endpoint which is responsible for sending the data to the predictor was only accepting json strings. <\/p>\n\n<p>Another way to solve this issues is to modify the python function which is responsible for the input handling to accept all content types and modify the data in a way that the predictor will understand.<\/p>\n\n<p>example of working code for my case:<\/p>\n\n<pre><code>        var data = new string[] { \"this movie was extremely good .\", \"the plot was very boring .\" };\n        var serializedData = JsonConvert.SerializeObject(data);\n\n        var credentials = new Amazon.Runtime.BasicAWSCredentials(\"\",\"\");\n        var awsClient = new AmazonSageMakerRuntimeClient(credentials, RegionEndpoint.EUCentral1);\n        var request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest\n        {\n            EndpointName = \"endpoint\",\n            ContentType = \"application\/json\",\n            Body = new MemoryStream(Encoding.ASCII.GetBytes(serializedData)),\n        };\n\n        var response = awsClient.InvokeEndpoint(request);\n        var predictions = Encoding.UTF8.GetString(response.Body.ToArray());\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1535104925600,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51968742",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55580232,
        "Question_title":"Update SageMaker Jupyterlab environment",
        "Question_body":"<p>How can I update my SageMaker notebook's jupyter environment to the latest alpha release and then restart the process?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1554750635030,
        "Question_score":4,
        "Question_tags":"amazon-sagemaker|jupyter-lab",
        "Question_view_count":1720,
        "Owner_creation_time":1361339272693,
        "Owner_last_access_time":1663965928400,
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Hi and thank you for using SageMaker!<\/p>\n\n<p>To restart Jupyter from within a SageMaker Notebook Instance, you can issue the following command: <code>sudo initctl restart jupyter-server --no-wait<\/code>.<\/p>\n\n<p>Best,\nKevin<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1555007316017,
        "Answer_score":8.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55580232",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63138835,
        "Question_title":"How to save models trained locally in Amazon SageMaker?",
        "Question_body":"<p>I'm trying to use a local training job in SageMaker.<\/p>\n<p>Following this AWS notebook (<a href=\"http:\/\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb<\/a>) I was able to train and predict locally.<\/p>\n<p>There is any way to train locally and save the trained model in the Amazon SageMaker Training Job section?\nOtherwise, how can I properly save trained models I trained using local mode?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1595954217667,
        "Question_score":7,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":3195,
        "Owner_creation_time":1464391892937,
        "Owner_last_access_time":1658153265653,
        "Owner_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Owner_reputation":2243,
        "Owner_up_votes":497,
        "Owner_down_votes":32,
        "Owner_views":148,
        "Question_last_edit_time":1595959142730,
        "Answer_body":"<p>There is no way to have your local mode training jobs appear in the AWS console. The intent of local mode is to allow for faster iteration\/debugging before using SageMaker for training your model.<\/p>\n<p>You can create SageMaker Models from local model artifacts. Compress your model artifacts into a <code>.tar.gz<\/code> file, upload that file to S3, and then create the Model (with the SDK or in the console).<\/p>\n<p>Documentation:<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1596039571220,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63138835",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63740792,
        "Question_title":"Provide additional input to docker container running inference model",
        "Question_body":"<p>We are using AWS Sagemaker feature, bring your own docker, where we have inference model written in R. As I understood, batch transform job runs container in a following way:<\/p>\n<pre><code>docker run image serve\n<\/code><\/pre>\n<p>Also, on docker we have a logic to determine which function to invoke:<\/p>\n<pre><code>args &lt;- commandArgs()\nif (any(grepl('train', args))) {\n    train()}\nif (any(grepl('serve', args))) {\n    serve()}\n<\/code><\/pre>\n<p>Is there a way, to override default container invocation so we can pass some additional parameters?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1599220820873,
        "Question_score":2,
        "Question_tags":"amazon-web-services|docker|amazon-sagemaker",
        "Question_view_count":702,
        "Owner_creation_time":1473837223713,
        "Owner_last_access_time":1658935159907,
        "Owner_location":"Belgrade",
        "Owner_reputation":353,
        "Owner_up_votes":23,
        "Owner_down_votes":1,
        "Owner_views":66,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As you said, and is indicated in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html\" rel=\"nofollow noreferrer\">AWS documentation<\/a>, Sagemaker will run your container with the following command:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>docker run image serve\n<\/code><\/pre>\n<p>By issuing this command Sagemaker will overwrite any <code>CMD<\/code> that you provide in your container Dockerfile, so you cannot use <code>CMD<\/code> to provide dynamic arguments to your program.<\/p>\n<p>We can think in use the Dockerfile <code>ENTRYPOINT<\/code> to consume some environment variables, but the documentation of AWS dictates that it is preferable use the <code>exec<\/code> form of the <code>ENTRYPOINT<\/code>. Somethink like:<\/p>\n<pre><code>ENTRYPOINT [&quot;\/usr\/bin\/Rscript&quot;, &quot;\/opt\/ml\/mars.R&quot;, &quot;--no-save&quot;]\n<\/code><\/pre>\n<p>I think that, for analogy with <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">model training<\/a>, they need this kind of container execution to enable the container to receive termination signals:<\/p>\n<blockquote>\n<p>The exec form of the <code>ENTRYPOINT<\/code> instruction starts the executable directly, not as a child of <code>\/bin\/sh<\/code>. This enables it to receive signals like <code>SIGTERM<\/code> and <code>SIGKILL<\/code> from SageMaker APIs.<\/p>\n<\/blockquote>\n<p>To allow variable expansion, we need to use the <code>ENTRYPOINT<\/code> <code>shell<\/code> form. Imagine:<\/p>\n<pre><code>ENTRYPOINT [&quot;sh&quot;, &quot;-c&quot;, &quot;\/usr\/bin\/Rscript&quot;, &quot;\/opt\/ml\/mars.R&quot;, &quot;--no-save&quot;, &quot;$ENV_VAR1&quot;]\n<\/code><\/pre>\n<p>If you try to do the same with the <code>exec<\/code> form the variables provided will be treated as a literal and will not be sustituited for their actual values.<\/p>\n<p>Please, see the approved answer of <a href=\"https:\/\/stackoverflow.com\/questions\/37904682\/how-do-i-use-docker-environment-variable-in-entrypoint-array\">this<\/a> stackoverflow question for a great explanation of this subject.<\/p>\n<p>But, one thing you can do is obtain the value of these variables in your R code, similar as when you process <code>commandArgs<\/code>:<\/p>\n<pre class=\"lang-r prettyprint-override\"><code>ENV_VAR1 &lt;- Sys.getenv(&quot;ENV_VAR1&quot;)\n<\/code><\/pre>\n<p>To pass environment variables to the container, as indicated in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html\" rel=\"nofollow noreferrer\">AWS documentation<\/a>, you can use the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\"><code>CreateModel<\/code><\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\"><code>CreateTransformJob<\/code><\/a> requests on your container.<\/p>\n<p>You probably will need to include in your Dockerfile <code>ENV<\/code> definitions for every required environment variable on your container, and provide for these definitions default values with <code>ARG<\/code>:<\/p>\n<pre><code>ARG ENV_VAR1_DEFAULT_VALUE=VAL1\nENV_VAR1=$ENV_VAR1_DEFAULT_VALUE\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1599691026140,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1599720910637,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63740792",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69853177,
        "Question_title":"Docker-compose wouldn't start on Sagemaker's Notebook instance",
        "Question_body":"<p>Docker-compose seems to have stopped working on Sagemaker Notebook instances. When running <code>docker-compose up<\/code> I encounter the following error:<\/p>\n<pre><code>During handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/bin\/docker-compose&quot;, line 8, in &lt;module&gt;\n    sys.exit(main())\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/main.py&quot;, line 81, in main\n    command_func()\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/main.py&quot;, line 200, in perform_command\n    project = project_from_options('.', options)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/command.py&quot;, line 70, in project_from_options\n    enabled_profiles=get_profiles_from_options(options, environment)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/command.py&quot;, line 153, in get_project\n    verbose=verbose, version=api_version, context=context, environment=environment\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/docker_client.py&quot;, line 43, in get_client\n    environment=environment, tls_version=get_tls_version(environment)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/docker_client.py&quot;, line 170, in docker_client\n    client = APIClient(use_ssh_client=not use_paramiko_ssh, **kwargs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/docker\/api\/client.py&quot;, line 197, in __init__\n    self._version = self._retrieve_server_version()\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/docker\/api\/client.py&quot;, line 222, in _retrieve_server_version\n    'Error while fetching server API version: {0}'.format(e)\ndocker.errors.DockerException: Error while fetching server API version: Timeout value connect was Timeout(connect=60, read=60, total=None), but it must be an int, float or None\n<\/code><\/pre>\n<p>I can start Docker containers as usual.<\/p>\n<pre><code>sh-4.2$ docker version\nClient:\n Version:           20.10.7\n API version:       1.41\n Go version:        go1.15.14\n Git commit:        f0df350\n Built:             Tue Sep 28 19:55:40 2021\n OS\/Arch:           linux\/amd64\n Context:           default\n Experimental:      true\n\nServer:\n Engine:\n  Version:          20.10.7\n  API version:      1.41 (minimum version 1.12)\n  Go version:       go1.15.14\n  Git commit:       b0f5bc3\n  Built:            Tue Sep 28 19:57:35 2021\n  OS\/Arch:          linux\/amd64\n  Experimental:     false\n containerd:\n  Version:          1.4.6\n  GitCommit:        d71fcd7d8303cbf684402823e425e9dd2e99285d\n runc:\n  Version:          1.0.0\n  GitCommit:        %runc_commit\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n<\/code><\/pre>\n<p>But <code>docker-compose<\/code> wouldn't work...<\/p>\n<pre><code>sh-4.2$ docker-compose version\ndocker-compose version 1.29.2, build unknown\ndocker-py version: 5.0.0\nCPython version: 3.6.13\nOpenSSL version: OpenSSL 1.1.1l  24 Aug 2021\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636114943820,
        "Question_score":0,
        "Question_tags":"amazon-web-services|docker|docker-compose|amazon-sagemaker",
        "Question_view_count":312,
        "Owner_creation_time":1384530039387,
        "Owner_last_access_time":1664020852630,
        "Owner_location":"Ljubljana, Slovenia",
        "Owner_reputation":2470,
        "Owner_up_votes":910,
        "Owner_down_votes":11,
        "Owner_views":285,
        "Question_last_edit_time":1636115811803,
        "Answer_body":"<p>For those of you who (might) have encountered the same issue, here's the fix:<\/p>\n<p>1). Install the newest version of docker-compose:<\/p>\n<pre><code>sh-4.2$ sudo curl -L &quot;https:\/\/github.com\/docker\/compose\/releases\/download\/1.29.2\/docker-compose-$(uname -s)-$(uname -m)&quot; -o \/usr\/local\/bin\/docker-compose\nsh-4.2$ sudo chmod +x \/usr\/local\/bin\/docker-compose\n<\/code><\/pre>\n<p>2). Change your <code>PATH<\/code> accordingly (since docker-compose is installed using <code>conda<\/code> and is picked up first) or use <code>\/usr\/local\/bin\/docker-compose<\/code> onwards:<\/p>\n<pre><code>sh-4.2$ PATH=\/usr\/local\/bin:$PATH\nsh-4.2$ docker-compose version\ndocker-compose version 1.29.2, build 5becea4c\ndocker-py version: 5.0.0\nCPython version: 3.7.10\nOpenSSL version: OpenSSL 1.1.0l  10 Sep 2019\n<\/code><\/pre>\n<p>Perhaps, the issue is related to this:<\/p>\n<blockquote>\n<p>On August 9, 2021 the Jupyter Notebook and Jupyter Lab open source software projects announced 2 security concerns that could impact Amazon Sagemaker Notebook Instance customers.<\/p>\n<p>Sagemaker has deployed updates to address these concerns, and we recommend customers with existing notebook sessions to stop and restart their notebook instance(s) to benefit from these updates. Notebook instances launched after August 10, 2021, when updates were deployed, are not impacted by this issue and do not need to be restarted.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1636114943820,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1636183275063,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69853177",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71623732,
        "Question_title":"ipydatagrid widget does not display in SageMaker Studio",
        "Question_body":"<h2>The problem<\/h2>\n<p>I have a jupyter notebook with an <code>ipydatagrid<\/code> widget that displays a dataframe. This notebook works correctly when run locally, but not when run in AWS SageMaker Studio. When run in SageMaker Studio, instead of showing the widget it simply shows the text <code>Loading widget...<\/code><\/p>\n<p>How does one use a <code>ipydatagrid<\/code> widget in the SageMaker Studio environment?<\/p>\n<h2>Details<\/h2>\n<p>Python version:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>$ python --version\nPython 3.7.10\n<\/code><\/pre>\n<p>Run at start:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>$ pip install -r requirements.txt\n$ jupyter nbextension enable --py --sys-prefix widgetsnbextension\n$ jupyter nbextension install --py --symlink --sys-prefix ipydatagrid\n$ jupyter nbextension enable --py --sys-prefix ipydatagrid\n<\/code><\/pre>\n<p>File <code>requirements.txt<\/code>:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>ipydatagrid==1.1.11\npandas==1.0.1\n<\/code><\/pre>\n<p>Notebook contents:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># %%\nimport pandas as pd\nfrom ipydatagrid import DataGrid\nfrom IPython.display import display\nimport ipywidgets\n\n# %%\ndata = [\n    (&quot;potato&quot;, 1.2, True),\n    (&quot;sweet potato&quot;, 0.8, False),\n    (&quot;french fries&quot;, 4.5, True),\n    (&quot;waffle fries&quot;, 4.9, True)\n]\ndf = pd.DataFrame(\n    data,\n    columns=[&quot;food&quot;, &quot;stars&quot;, &quot;is_available&quot;]\n)\n\n# %%\ngrid = DataGrid(df)\n\n# %%\ndisplay(grid)\n\n# %%\n# SANITY CHECK:\nbutton = ipywidgets.Button(\n    description=&quot;Button&quot;,\n    disabled=False\n)\ndef on_click(b):\n    print(&quot;CLICK&quot;)\nbutton.on_click(on_click)\ndisplay(button)\n<\/code><\/pre>\n<h3>Error messages<\/h3>\n<p>If I use the Google Chrome developer tools, I can see more logs in the browser that give some error messages, most of which are repeated:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>manager.js:305 Uncaught (in promise) Error: Module ipydatagrid, semver range ^1.1.11 is not registered as a widget module\n    at C.loadClass (manager.js:305:19)\n    at C.&lt;anonymous&gt; (manager-base.js:263:46)\n    at l (manager-base.js:44:23)\n    at Object.next (manager-base.js:25:53)\n    at manager-base.js:19:71\n    at new Promise (&lt;anonymous&gt;)\n    at Rtm6.k (manager-base.js:15:12)\n    at C.e._make_model (manager-base.js:257:16)\n    at C.&lt;anonymous&gt; (manager-base.js:246:45)\n    at l (manager-base.js:44:23)\n<\/code><\/pre>\n<pre class=\"lang-none prettyprint-override\"><code>utils.js:119 Error: Could not create a model.\n    at n (utils.js:119:27)\n    at async C._handleCommOpen (manager.js:61:51)\n    at async v._handleCommOpen (default.js:994:100)\n    at async v._handleMessage (default.js:1100:43)\n<\/code><\/pre>\n<pre class=\"lang-none prettyprint-override\"><code>manager-base.js:273 Could not instantiate widget\n<\/code><\/pre>\n<p>However, there is no overt error message that's immediately obvious to the user, including in the log where <code>print<\/code> statements send their output.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648246951157,
        "Question_score":0,
        "Question_tags":"python|pandas|amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":312,
        "Owner_creation_time":1560606498250,
        "Owner_last_access_time":1663010976300,
        "Owner_location":"Provo, UT, USA",
        "Owner_reputation":528,
        "Owner_up_votes":351,
        "Owner_down_votes":61,
        "Owner_views":39,
        "Question_last_edit_time":null,
        "Answer_body":"<p>SageMaker Studio currently runs JupyterLab v1.2 (as confirmed by <em>Help &gt; About JupyterLab<\/em>), and per the <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid#Installation\" rel=\"nofollow noreferrer\">ipydatagrid installation instructions<\/a>, current\/recent versions of this widget require v3+... So I think this is most likely your problem - as there were breaking changes in the interfaces for extensions between these major versions.<\/p>\n<p>I had a quick look at the past releases of <code>ipydatagrid<\/code> to see if using an older version would be possible, and it seems like the documented JLv3 requirement gets added between <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid\/tree\/0.2.16\" rel=\"nofollow noreferrer\">v0.2.16<\/a> and <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid\/tree\/1.0.1\" rel=\"nofollow noreferrer\">v1.0.1<\/a> (which are adjacent on GitHub).<\/p>\n<p>However, the old install instructions documented on 0.2 don't seem to work anymore: I get <code>ValueError: &quot;jupyter-datagrid&quot; is not a valid npm package<\/code> and also note that versions &lt;1.0 don't seem to be present <a href=\"https:\/\/libraries.io\/pypi\/ipydatagrid\/versions\" rel=\"nofollow noreferrer\">on PyPI<\/a>.<\/p>\n<p>So unfortunately I think (unless\/until SM Studio gets a JupyterLab version upgrade), this widget's not likely to work unless you dive in to building it from an old source code version.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1648549758097,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71623732",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57243583,
        "Question_title":"Sagemaker KMeans Built-In - List of files csv as input",
        "Question_body":"<p>I Want to use <strong>Sagemaker KMeans BuilIn Algorithm<\/strong> in one of my applications. I have a large CSV file in S3 (raw data) that I split into several parts to be easy to clean. Before I had cleaned, I tried to use it as the input of Kmeans to perform the training job but It doesn't work.<\/p>\n\n<p>My manifest file:<\/p>\n\n<pre><code>[\n    {\"prefix\": \"s3:\/\/&lt;BUCKET_NAME&gt;\/kmeans_data\/KMeans-2019-28-07-13-40-00-001\/\"}, \n    \"file1.csv\", \n    \"file2.csv\"\n]\n<\/code><\/pre>\n\n<p>The error I've got:<\/p>\n\n<pre><code>Failure reason: ClientError: Unable to read data channel 'train'. Requested content-type is 'application\/x-recordio-protobuf'. Please verify the data matches the requested content-type. (caused by MXNetError) Caused by: [16:47:31] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsCppLibs\/AIAlgorithmsCppLibs-2.0.1620.0\/AL2012\/generic-flavor\/src\/src\/aialgs\/io\/iterator_base.cpp:100: (Input Error) The header of the MXNet RecordIO record at position 0 in the dataset does not start with a valid magic number. Stack trace returned 10 entries: [bt] (0) \/opt\/amazon\/lib\/libaialgs.so(+0xb1f0) [0x7fb5674c31f0] [bt] (1) \/opt\/amazon\/lib\/libaialgs.so(+0xb54a) [0x7fb5674c354a] [bt] (2) \/opt\/amazon\/lib\/libaialgs.so(aialgs::iterator_base::Next()+0x4a6) [0x7fb5674cc436] [bt] (3) \/opt\/amazon\/lib\/libmxnet.so(MXDataIterNext+0x21) [0x7fb54ecbcdb1] [bt] (4) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call_unix64+0x4c) [0x7fb567a1e858] [bt] (5) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call+0x15f) [0x7fb567a1d95f\n<\/code><\/pre>\n\n<p>My question is: It's possible to use multiple CSV files as input in Sagemaker KMeans BuilIn Algorithm only in GUI? If it's possible, How should I format my manifest?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1564336514917,
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":522,
        "Owner_creation_time":1464391892937,
        "Owner_last_access_time":1658153265653,
        "Owner_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Owner_reputation":2243,
        "Owner_up_votes":497,
        "Owner_down_votes":32,
        "Owner_views":148,
        "Question_last_edit_time":1564342604717,
        "Answer_body":"<p>the manifest looks fine, but based on the error message, it looks like you haven't set the right data format for you S3 data. It's expecting protobuf, which is the default format :)<\/p>\n\n<p>You have to set the CSV data format explicitly. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.s3_input\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.s3_input<\/a>. <\/p>\n\n<p>It should look something like this:<\/p>\n\n<pre><code>s3_input_train = sagemaker.s3_input(\n  s3_data='s3:\/\/{}\/{}\/train\/manifest_file'.format(bucket, prefix),    \n  s3_data_type='ManifestFile',\n  content_type='csv')\n\n...\n\nkmeans_estimator = sagemaker.estimator.Estimator(kmeans_image, ...)\nkmeans_estimator.set_hyperparameters(...)\n\ns3_data = {'train': s3_input_train}\nkmeans_estimator.fit(s3_data)\n<\/code><\/pre>\n\n<p>Please note the KMeans estimator in the SDK only supports protobuf, see <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/kmeans.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/kmeans.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1564340123623,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57243583",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57273357,
        "Question_title":"Empty dictionary on AnnotationConsolidation lambda event for aws Sagemaker",
        "Question_body":"<p>I am starting to use aws sagemaker on the development of my machine learning model and I'm trying to build a lambda function to process the responses of a sagemaker labeling job. I already created my own lambda function but when I try to read the event contents I can see that the event dict is completely empty, so I'm not getting any data to read.<\/p>\n\n<p>I have already given enough permissions to the role of the lambda function. Including:\n- AmazonS3FullAccess.\n- AmazonSagemakerFullAccess.\n- AWSLambdaBasicExecutionRole<\/p>\n\n<p>I've tried using this code for the Post-annotation Lambda (adapted for python 3.6):<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step2-demo1.html#sms-custom-templates-step2-demo1-post-annotation\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step2-demo1.html#sms-custom-templates-step2-demo1-post-annotation<\/a><\/p>\n\n<p>As well as this one in this git repository:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py<\/a><\/p>\n\n<p>But none of them seemed to work.<\/p>\n\n<p>For creating the labeling job I'm using boto3's functions for sagemaker:\n<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_labeling_job\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_labeling_job<\/a><\/p>\n\n<p>This is the code i have for creating the labeling job:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def create_labeling_job(client,bucket_name ,labeling_job_name, manifest_uri, output_path):\n\n    print(\"Creating labeling job with name: %s\"%(labeling_job_name))\n\n    response = client.create_labeling_job(\n        LabelingJobName=labeling_job_name,\n        LabelAttributeName='annotations',\n        InputConfig={\n            'DataSource': {\n                'S3DataSource': {\n                    'ManifestS3Uri': manifest_uri\n                }\n            },\n            'DataAttributes': {\n                'ContentClassifiers': [\n                    'FreeOfAdultContent',\n                ]\n            }\n        },\n        OutputConfig={\n            'S3OutputPath': output_path\n        },\n        RoleArn='arn:aws:myrolearn',\n        LabelCategoryConfigS3Uri='s3:\/\/'+bucket_name+'\/config.json',\n        StoppingConditions={\n            'MaxPercentageOfInputDatasetLabeled': 100,\n        },\n        LabelingJobAlgorithmsConfig={\n            'LabelingJobAlgorithmSpecificationArn': 'arn:image-classification'\n        },\n        HumanTaskConfig={\n            'WorkteamArn': 'arn:my-private-workforce-arn',\n            'UiConfig': {\n                'UiTemplateS3Uri':'s3:\/\/'+bucket_name+'\/templatefile'\n            },\n            'PreHumanTaskLambdaArn': 'arn:aws:lambda:us-east-1:432418664414:function:PRE-BoundingBox',\n            'TaskTitle': 'Title',\n            'TaskDescription': 'Description',\n            'NumberOfHumanWorkersPerDataObject': 1,\n            'TaskTimeLimitInSeconds': 600,\n            'AnnotationConsolidationConfig': {\n                'AnnotationConsolidationLambdaArn': 'arn:aws:my-custom-post-annotation-lambda'\n            }\n        }\n    )\n\n    return response\n<\/code><\/pre>\n\n<p>And this is the one i have for the lambda function:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>    print(\"Received event: \" + json.dumps(event, indent=2))\n    print(\"event: %s\"%(event))\n    print(\"context: %s\"%(context))\n    print(\"event headers: %s\"%(event[\"headers\"]))\n\n    parsed_url = urlparse(event['payload']['s3Uri']);\n    print(\"parsed_url: \",parsed_url)\n\n    labeling_job_arn = event[\"labelingJobArn\"]\n    label_attribute_name = event[\"labelAttributeName\"]\n\n    label_categories = None\n    if \"label_categories\" in event:\n        label_categories = event[\"labelCategories\"]\n        print(\" Label Categories are : \" + label_categories)\n\n    payload = event[\"payload\"]\n    role_arn = event[\"roleArn\"]\n\n    output_config = None # Output s3 location. You can choose to write your annotation to this location\n    if \"outputConfig\" in event:\n        output_config = event[\"outputConfig\"]\n\n    # If you specified a KMS key in your labeling job, you can use the key to write\n    # consolidated_output to s3 location specified in outputConfig.\n    kms_key_id = None\n    if \"kmsKeyId\" in event:\n        kms_key_id = event[\"kmsKeyId\"]\n\n    # Create s3 client object\n    s3_client = S3Client(role_arn, kms_key_id)\n\n    # Perform consolidation\n    return do_consolidation(labeling_job_arn, payload, label_attribute_name, s3_client)\n<\/code><\/pre>\n\n<p>I've tried debugging the event object with:<\/p>\n\n<pre><code>    print(\"Received event: \" + json.dumps(event, indent=2))\n<\/code><\/pre>\n\n<p>But it just prints an empty dictionary: <code>Received event: {}<\/code><\/p>\n\n<p>I expect the output to be something like:<\/p>\n\n<pre><code>    #Content of an example event:\n    {\n        \"version\": \"2018-10-16\",\n        \"labelingJobArn\": &lt;labelingJobArn&gt;,\n        \"labelCategories\": [&lt;string&gt;],  # If you created labeling job using aws console, labelCategories will be null\n        \"labelAttributeName\": &lt;string&gt;,\n        \"roleArn\" : \"string\",\n        \"payload\": {\n            \"s3Uri\": &lt;string&gt;\n        }\n        \"outputConfig\":\"s3:\/\/&lt;consolidated_output configured for labeling job&gt;\"\n    }\n<\/code><\/pre>\n\n<p>Lastly, when I try yo get the labeling job ARN with:<\/p>\n\n<pre><code>    labeling_job_arn = event[\"labelingJobArn\"]\n<\/code><\/pre>\n\n<p>I just get a KeyError (which makes sense because the dictionary is empty).<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1564494701657,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|aws-lambda|python-3.6|amazon-sagemaker",
        "Question_view_count":846,
        "Owner_creation_time":1525449880547,
        "Owner_last_access_time":1662375685780,
        "Owner_location":"Madrid, Spain",
        "Owner_reputation":81,
        "Owner_up_votes":54,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1575462189053,
        "Answer_body":"<p>I found the problem, I needed to add the ARN of the role used by my Lamda function as a Trusted Entity on the Role used for the Sagemaker Labeling Job.<\/p>\n\n<p>I just went to <code>Roles &gt; MySagemakerExecutionRole &gt; Trust Relationships<\/code> and added:<\/p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": [\n          \"arn:aws:iam::xxxxxxxxx:role\/My-Lambda-Role\",\n           ...\n        ],\n        \"Service\": [\n          \"lambda.amazonaws.com\",\n          \"sagemaker.amazonaws.com\",\n           ...\n        ]\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n<\/code><\/pre>\n\n<p>This made it work for me.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1564741689013,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57273357",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69693666,
        "Question_title":"How to Deploy ML Recommender System on AWS",
        "Question_body":"<p>I'm dabbling with ML and was able to take a tutorial and get it to work for my needs.  It's a simple recommender system using TfidfVectorizer and linear_kernel.  I run into a problem with how I go about deploying it through Sagemaker with an end point.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel \nimport json\nimport csv\n\nwith open('data\/big_data.json') as json_file:\n    data = json.load(json_file)\n\nds = pd.DataFrame(data)\n\ntf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\ntfidf_matrix = tf.fit_transform(ds['content'])\ncosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n\nresults = {}\n\nfor idx, row in ds.iterrows():\n    similar_indices = cosine_similarities[idx].argsort()[:-100:-1]\n    similar_items = [(cosine_similarities[idx][i], ds['id'][i]) for i in similar_indices]\n\n    results[row['id']] = similar_items[1:]\n\ndef item(id):\n    return ds.loc[ds['id'] == id]['id'].tolist()[0]\n\ndef recommend(item_id, num):\n    print(&quot;Recommending &quot; + str(num) + &quot; products similar to &quot; + item(item_id) + &quot;...&quot;)\n    print(&quot;-------&quot;)\n    recs = results[item_id][:num]\n    for rec in recs:\n        print(&quot;Recommended: &quot; + item(rec[1]) + &quot; (score:&quot; + str(rec[0]) + &quot;)&quot;)\n\nrecommend(item_id='129035', num=5)\n<\/code><\/pre>\n<p>As a starting point I'm not sure if the output from <code>tf.fit_transform(ds['content'])<\/code> is considered the model or the output from <code>linear_kernel(tfidf_matrix, tfidf_matrix)<\/code>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635046349497,
        "Question_score":0,
        "Question_tags":"machine-learning|amazon-sagemaker|tfidfvectorizer",
        "Question_view_count":63,
        "Owner_creation_time":1635045129020,
        "Owner_last_access_time":1650386337657,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1635173315200,
        "Answer_body":"<p>I came to the conclusion that I didn't need to deploy this through SageMaker.  Since the final linear_kernel output was a Dictionary I could do quick ID lookups to find correlations.<\/p>\n<p>I have it working on AWS with API Gateway\/Lambda, DynamoDB and an EC2 server to collect, process and plug the data into DynamoDB for fast lookups.  No expensive SageMaker endpoint needed.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1636075476147,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69693666",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57212696,
        "Question_title":"Gluonnlp installation not found on Sagemaker jupyter notebook",
        "Question_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/I8c93.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/I8c93.png\" alt=\"enter image description here\"><\/a>I am attempting to install Gluonnlp to a sagemaker jupyter notebook. Im using the command <code>!sudo pip3 install gluonnlp<\/code> to install.  Which is successful.  However on import I get <code>ModuleNotFoundError: No module named 'gluonnlp'<\/code><\/p>\n\n<p>I got the same issue when attempting to install mxnet with pip in the same notebook.  It was resolved when I conda installed mxnet instead.  However conda install has not been working for gluonnlp as it cannot find the package.  I can't seem to find a way to conda install gluonnlp.  Any suggestions would be highly appreciated.<\/p>\n\n<p>Here are some of the commands I have tried<\/p>\n\n<p><code>!sudo pip3 install gluonnlp<\/code><\/p>\n\n<p><code>!conda install gluonnlp<\/code> --> Anaconda cant find the package in any channels<\/p>\n\n<pre><code>!conda install pip --y\n!sudo pip3 install gluonnlp\n\n!sudo pip3 install gluonnlp\n\n!conda install -c conda-forge gluonnlp --y\n<\/code><\/pre>\n\n<p>All these commands on my import \nimport warnings<\/p>\n\n<pre><code>warnings.filterwarnings('ignore')\n\nimport io\nimport random\nimport numpy as np\nimport mxnet as mx\nimport gluonnlp as nlp\nfrom bert import data, model\n<\/code><\/pre>\n\n<p>result in the error<\/p>\n\n<pre><code>ModuleNotFoundError: No module named 'gluonnlp'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1564111030610,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|jupyter-notebook|amazon-sagemaker|gluon",
        "Question_view_count":2015,
        "Owner_creation_time":1531840489147,
        "Owner_last_access_time":1636067734717,
        "Owner_location":"Berkeley, CA, USA",
        "Owner_reputation":425,
        "Owner_up_votes":12,
        "Owner_down_votes":1,
        "Owner_views":92,
        "Question_last_edit_time":1564413127093,
        "Answer_body":"<p>this is as simple as creating a Jupyter notebook using the 'conda_mxnet_p36' kernel, and adding a cell containing:<\/p>\n\n<pre><code>!pip install gluonnlp\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1564315813180,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57212696",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54485769,
        "Question_title":"SageMaker deploying to EIA from TF Script Mode Python3",
        "Question_body":"<p>I've fitted a Tensorflow Estimator in SageMaker using Script Mode with <code>framework_version='1.12.0'<\/code> and <code>python_version='py3'<\/code>, using a GPU instance. <\/p>\n\n<p>Calling deploy directly on this estimator works if I select deployment instance type as GPU as well. However, if I select a CPU instance type and\/or try to add an accelerator, it fails with an error that docker cannot find a corresponding image to pull. <\/p>\n\n<p>Anybody know how to train a py3 model on a GPU with Script Mode and then deploy to a CPU+EIA instance? <\/p>\n\n<hr>\n\n<p>I've found a partial workaround by taking the intermediate step of creating a TensorFlowModel from the estimator's training artifacts and then deploying from the model, but this does not seem to support python 3 (again, doesn't find a corresponding container). If I switch to python_version='py2', it will find the container, but fail to pass health checks because all my code is for python 3.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1549048113447,
        "Question_score":0,
        "Question_tags":"python-3.x|tensorflow|amazon-ec2|amazon-sagemaker|docker-pull",
        "Question_view_count":378,
        "Owner_creation_time":1361339272693,
        "Owner_last_access_time":1663965928400,
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Question_last_edit_time":1549048982467,
        "Answer_body":"<p>Unfortunately there are no TF + Python 3 + EI serving images at this time. If you would like to use TF + EI, you'll need to make sure your code is compatible with Python 2.<\/p>\n\n<p>Edit: after I originally wrote this, support for TF + Python 3 + EI has been released. At the time of this writing, I believe TF 1.12.0, 1.13.1, and 1.14.0 all have Python 3 + EI support. For the full list, see <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#tensorflow-sagemaker-estimators\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#tensorflow-sagemaker-estimators<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1549915825300,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1585784312460,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54485769",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62012264,
        "Question_title":"AWS SageMaker SparkML Schema Eroor: member.environment' failed to satisfy constraint",
        "Question_body":"<p>I am deploying a model onto AWS via Sagemaker:<\/p>\n\n<p>I set up my JSON schema as follow:<\/p>\n\n<pre><code>import json\nschema = {\n    \"input\": [\n        {\n            \"name\": \"V1\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V2\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V3\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V4\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V5\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V6\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V7\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V8\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V9\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V10\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V11\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V12\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V13\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V14\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V15\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V16\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V17\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V18\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V19\",\n            \"type\": \"double\"\n        }, \n                {\n            \"name\": \"V20\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V21\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V22\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V23\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V24\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V25\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V26\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V27\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V28\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"Amount\",\n            \"type\": \"double\"\n        },         \n    ],\n    \"output\": \n        {\n            \"name\": \"features\",\n            \"type\": \"double\",\n            \"struct\": \"vector\"\n        }\n}\nschema_json = json.dumps(schema)\nprint(schema_json)\n<\/code><\/pre>\n\n<p>And deployed as:<\/p>\n\n<pre><code>from sagemaker.model import Model\nfrom sagemaker.pipeline import PipelineModel\nfrom sagemaker.sparkml.model import SparkMLModel\n\nsparkml_data = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\n# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands\nsparkml_model = SparkMLModel(model_data=sparkml_data, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json})\nxgb_model = Model(model_data=xgb_model.model_data, image=training_image)\n\nmodel_name = 'inference-pipeline-' + timestamp_prefix\nsm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, xgb_model])\n\n    endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)\n<\/code><\/pre>\n\n<p>I got the error as below:<\/p>\n\n<p>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: 1 validation error detected: Value '{SAGEMAKER_SPARKML_SCHEMA={\"input\": [{\"type\": \"double\", \"name\": \"V1\"}, {\"type\": \"double\", \"name\": \"V2\"}, {\"type\": \"double\", \"name\": \"V3\"}, {\"type\": \"double\", \"name\": \"V4\"}, {\"type\": \"double\", \"name\": \"V5\"}, {\"type\": \"double\", \"name\": \"V6\"}, {\"type\": \"double\", \"name\": \"V7\"}, {\"type\": \"double\", \"name\": \"V8\"}, {\"type\": \"double\", \"name\": \"V9\"}, {\"type\": \"double\", \"name\": \"V10\"}, {\"type\": \"double\", \"name\": \"V11\"}, {\"type\": \"double\", \"name\": \"V12\"}, {\"type\": \"double\", \"name\": \"V13\"}, {\"type\": \"double\", \"name\": \"V14\"}, {\"type\": \"double\", \"name\": \"V15\"}, {\"type\": \"double\", \"name\": \"V16\"}, {\"type\": \"double\", \"name\": \"V17\"}, {\"type\": \"double\", \"name\": \"V18\"}, {\"type\": \"double\", \"name\": \"V19\"}, {\"type\": \"double\", \"name\": \"V20\"}, {\"type\": \"double\", \"name\": \"V21\"}, {\"type\": \"double\", \"name\": \"V22\"}, {\"type\": \"double\", \"name\": \"V23\"}, {\"type\": \"double\", \"name\": \"V24\"}, {\"type\": \"double\", \"name\": \"V25\"}, {\"type\": \"double\", \"name\": \"V26\"}, {\"type\": \"double\", \"name\": \"V27\"}, {\"type\": \"double\", \"name\": \"V28\"}, {\"type\": \"double\", \"name\": \"Amount\"}], \"output\": {\"type\": \"double\", \"name\": \"features\", \"struct\": \"vector\"}}}' at 'containers.1**.<strong>member.environment' failed to satisfy constraint: Map value must satisfy constraint: [Member must have length less than or equal to 1024<\/strong>,** Member must have length greater than or equal to 0, Member must satisfy regular expression pattern: [\\S\\s]*]<\/p>\n\n<p>I try to reduce my features to 20 and it able to deploy. Just wondering how can I Pass the schema with 29 attributes?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1590448983843,
        "Question_score":0,
        "Question_tags":"json|amazon-web-services|deployment|amazon-sagemaker",
        "Question_view_count":226,
        "Owner_creation_time":1359061977540,
        "Owner_last_access_time":1608951269537,
        "Owner_location":"New York, NY, USA",
        "Owner_reputation":427,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":62,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I do not think the environment length of 1024 limit will be increased in a short time. To work around this, you could try to rebuild the spark ml container with the <code>SAGEMAKER_SPARKML_SCHEMA<\/code> env var:<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1600448991413,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62012264",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69104302,
        "Question_title":"Terraform Error: error waiting for sagemaker notebook instance to create: unexpected state 'Failed', wanted target 'InService'. last error: %!s(<nil>)",
        "Question_body":"<p>The entire error message after executing <code>terraform apply<\/code> within the terraform-folder of <a href=\"https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance\" rel=\"nofollow noreferrer\">this source code in my GitHub-repo<\/a> (inspired by <a href=\"https:\/\/www.linkedin.com\/pulse\/terraform-sagemaker-part-2a-creating-custom-notebook-instance-david\" rel=\"nofollow noreferrer\">this tutorial<\/a> and <a href=\"https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\" rel=\"nofollow noreferrer\">its related GitHub-repo<\/a>):<\/p>\n<pre><code>aws_sagemaker_notebook_instance.notebook_instance: Creating...\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [10s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [20s elapsed]\n...\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [15m21s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [15m31s elapsed]\n\u2577\n\u2502 Error: error waiting for sagemaker notebook instance (aws-sm-notebook-instance) to create: unexpected state 'Failed', wanted target 'InService'. last error: %!s(&lt;nil&gt;)\n\u2502\n\u2502   with aws_sagemaker_notebook_instance.notebook_instance,\n\u2502   on notebook_instance.tf line 2, in resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot;:\n\u2502    2: resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot; {\n\u2502\n<\/code><\/pre>\n<p>Internet research seemed to provide the solution in <a href=\"https:\/\/yuyasugano.medium.com\/machine-learning-infrastructure-terraforming-sagemaker-part-2-f2460a9a4663\" rel=\"nofollow noreferrer\">this article<\/a>, which inspired be to increase the allowed <code>IDLE_TIME<\/code> in the <code>on-start.sh<\/code> - script to <code>IDLE_TIME=1800<\/code> (in seconds, which equals 30 minutes). This should've been sufficient for the deployment time of around 15 minutes; yet, it threw the same error again.<\/p>\n<p>Next, I found <a href=\"https:\/\/stackoverflow.com\/questions\/65884743\/resolving-broken-deleted-state-in-terraform\">this post on StackOverFlow<\/a> suggesting to<\/p>\n<blockquote>\n<p>run <code>terraform refresh<\/code>, which will cause Terraform to refresh its state\nfile against what actually exists with the cloud provider.<\/p>\n<\/blockquote>\n<p>Unfortunately, running <code>terraform apply<\/code> right after refreshing didn't resolve the issue either.\nI'm wondering why the aforementioned <code>IDLE_TIME=1800<\/code> - setting does not have any effect. This should be more than sufficient for a 15-minute apply-time.<\/p>\n<hr \/>\n<p><strong>EDIT: adding code specifics for enhanced understanding<\/strong><\/p>\n<p><strong>1. Creating the SageMaker notebook instance<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot; {\n  name                    = &quot;aws-sm-notebook-instance&quot;\n  role_arn                = aws_iam_role.notebook_iam_role.arn\n  instance_type           = &quot;ml.t2.medium&quot;\n  lifecycle_config_name   = aws_sagemaker_notebook_instance_lifecycle_configuration.notebook_config.name\n  default_code_repository = aws_sagemaker_code_repository.git_repo.code_repository_name\n}\n<\/code><\/pre>\n<p><strong>2. Defining the SageMaker notebook lifecycle configuration<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_notebook_instance_lifecycle_configuration&quot; &quot;notebook_config&quot; {\n  name      = &quot;dev-platform-al-sm-lifecycle-config&quot;\n  on_create = filebase64(&quot;..\/scripts\/on-create.sh&quot;)\n  on_start  = filebase64(&quot;..\/scripts\/on-start.sh&quot;)\n}\n<\/code><\/pre>\n<p><strong>3. Defining the Git repo to instantiate on the SageMaker notebook instance<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_code_repository&quot; &quot;git_repo&quot; {\n  code_repository_name = &quot;aws-sm-notebook-instance-repo&quot;\n\n  git_config {\n    repository_url = &quot;https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance.git&quot;\n  }\n}\n<\/code><\/pre>\n<p><strong>Contents of <code>on-start.sh<\/code> (including IDLE_TIME - parameter)<\/strong>\nNote that this script will be invoked by the <code>scripts\/autostop.py<\/code> - script, which you can find <a href=\"https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance\/blob\/main\/scripts\/autostop.py\" rel=\"nofollow noreferrer\">here<\/a> in the associated <a href=\"https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance\" rel=\"nofollow noreferrer\">public repo containing the source code<\/a>.<\/p>\n<pre><code>#!\/bin\/bash\n\nset -e\n\n## IDLE AUTOSTOP STEPS\n## ----------------------------------------------------------------\n\n## Setting the timeout (in seconds) for how long the SageMaker notebook can run idly before being auto-stopped\n# -&gt; e.g. 1800 s = 30 min since first deployment can take between 15 and 20 minutes which could then fail like so:\n# &quot;Error: error waiting for sagemaker notebook instance (aws-sm-notebook-instance) to create: unexpected state 'Failed', wanted target 'InService'. last error: %!s(&lt;nil&gt;)&quot;\n# Hint for solution under following link: https:\/\/yuyasugano.medium.com\/machine-learning-infrastructure-terraforming-sagemaker-part-2-f2460a9a4663\nIDLE_TIME=1800\n\n# Getting the autostop.py script from GitHub\necho &quot;Fetching the autostop script...&quot;\nwget https:\/\/raw.githubusercontent.com\/andreasluckert\/aws-sm-notebook-instance\/main\/scripts\/autostop.py\n\n# Using crontab to autostop the notebook when idle time is breached\necho &quot;Starting the SageMaker autostop script in cron.&quot;\n(crontab -l 2&gt;\/dev\/null; echo &quot;*\/5 * * * * \/usr\/bin\/python $PWD\/autostop.py --time $IDLE_TIME --ignore-connections&quot;) | crontab -\n\n\n\n## CUSTOM CONDA KERNEL USAGE STEPS\n## ----------------------------------------------------------------\n\n# Setting the proper user credentials\nsudo -u ec2-user -i &lt;&lt;'EOF'\nunset SUDO_UID\n\n# Setting the source for the custom conda kernel\nWORKING_DIR=\/home\/ec2-user\/SageMaker\/custom-miniconda\nsource &quot;$WORKING_DIR\/miniconda\/bin\/activate&quot;\n\n# Loading all the custom kernels\nfor env in $WORKING_DIR\/miniconda\/envs\/*; do\n    BASENAME=$(basename &quot;$env&quot;)\n    source activate &quot;$BASENAME&quot;\n    python -m ipykernel install --user --name &quot;$BASENAME&quot; --display-name &quot;Custom ($BASENAME)&quot;\ndone\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1631108848430,
        "Question_score":0,
        "Question_tags":"terraform|terraform-provider-aws|amazon-sagemaker",
        "Question_view_count":768,
        "Owner_creation_time":1572449042430,
        "Owner_last_access_time":1663946824940,
        "Owner_location":"Germany",
        "Owner_reputation":2082,
        "Owner_up_votes":342,
        "Owner_down_votes":3,
        "Owner_views":238,
        "Question_last_edit_time":1631177626360,
        "Answer_body":"<p>The solution to the problem was to check the CloudWatch Log events under <code>CloudWatch -&gt; Log groups -&gt; \/aws\/sagemaker\/NotebookInstances -&gt; aws-sm-notebook-instance\/LifecycleConfigOnCreate<\/code> to find the following error-message:<\/p>\n<pre><code>\/bin\/bash: \/tmp\/OnCreate_2021-09-08-12-24rw5al34g: \/bin\/bash^M: bad interpreter: No such file or directory\n<\/code><\/pre>\n<p>A bit of internet research brought me to <a href=\"https:\/\/askubuntu.com\/questions\/304999\/not-able-to-execute-a-sh-file-bin-bashm-bad-interpreter\/305001#305001\">this solution related to newline characters in shell-scripts<\/a>, which depend on whether you are on <code>Windows<\/code> or a <code>UNIX<\/code>-system.\nAs I'm working on Windows, the shell-scripts created in VS-Code comprised dos-specific <code>CRLF<\/code> newline-handling, which could be resolved via the button on the bottom-right in <code>VS-Code<\/code> to switch the <em>carriage return<\/em> (CRLF) character to the <em>line feed<\/em> (LF) character used by UNIX.<\/p>\n<p>As the compute instance employed by AWS Sagemaker is a Linux-system, it cannot handle the dos-style CRLF newline-characters in the shell-scripts and this &quot;adds&quot; a <code>^M<\/code> after <code>\/bin\/bash<\/code> which obviously leads to an error as such an interpreter does not exist.<\/p>\n<p>So, finally <code>terraform apply<\/code> worked out well:<\/p>\n<pre><code>$ terraform apply\n...\n...\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [7m30s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [7m40s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Creation complete after 7m43s [id=aws-sm-notebook-instance]\n\nApply complete! Resources: 1 added, 1 changed, 1 destroyed.\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1631187921613,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69104302",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63128111,
        "Question_title":"Sagemaker Object2Vec training samples per second",
        "Question_body":"<p>I am using Sagemaker Object2Vec to train on data of size 2GB.<\/p>\n<p>ml.p2.xlarge instance took 12 hours to train the data on 4 epochs going at the speed of 5000 samples\/sec.<\/p>\n<p>Now, I am using a higher level instance ml.p2.16xlarge and it only trains at 400 samples\/sec with this in the logs<\/p>\n<pre><code>[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:739: only 114 out of 240 GPU pairs are enabled direct access. It may affect the performance. You can set MXNET_ENABLE_GPU_P2P=0 to turn it off\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .vvvvvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: v.vvvvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vv.vvvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvv.vvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvv.vvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvv.vvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvvv.vv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvvvv.v.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvvvvv........\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: ..........vvvvvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........v.vvvvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vv.vvvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvv.vvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvvv.vv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvvvv.v\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvvvvv.\n<\/code><\/pre>\n<p>There are about 50 million samples.<\/p>\n<p>What can I do to correct this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1595917104483,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":59,
        "Owner_creation_time":1504123657673,
        "Owner_last_access_time":1622762763367,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>2 ideas:<\/p>\n<ol>\n<li>Before increasing the GPU count, grow batch size so that a single\nGPU is as busy as possible<\/li>\n<li>Use P3 instances instead of P2. P3 is more recent, has more memory, more CUDA cores, faster memory bandwidth and NVLink inter-GPU connections. Though it's more\nexpensive by hour, your total training cost may be much smaller if\nproperly tuned<\/li>\n<\/ol>\n<p>Also, if your problem involves sparse updates, meaning if just a small fraction of all tokens appear in a given mini-batch, you can try using <code>token_embedding_storage_type = 'row_sparse'<\/code>, which I think refers to using sparse gradient updates like described here <a href=\"https:\/\/medium.com\/apache-mxnet\/learning-embeddings-for-music-recommendation-with-mxnets-sparse-api-5698f4d7d8\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/apache-mxnet\/learning-embeddings-for-music-recommendation-with-mxnets-sparse-api-5698f4d7d8<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1595937519410,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63128111",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59773503,
        "Question_title":"Using Sagemaker predictor in a Spark UDF function",
        "Question_body":"<p>I am trying to run inference on a Tensorflow model deployed on SageMaker from a Python Spark job.\nI am running a (Databricks) notebook which has the following cell:<\/p>\n\n<pre><code>def call_predict():\n        batch_size = 1\n        data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n        tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n        prediction = predictor.predict(tensor_proto)\n        print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n<\/code><\/pre>\n\n<p>If I just call call_predict() it works fine:<\/p>\n\n<pre><code>call_predict()\n<\/code><\/pre>\n\n<p>and I get the output:<\/p>\n\n<pre><code>Process time: 65.261396\nOut[61]: {'model_spec': {'name': u'generic_model',\n  'signature_name': u'serving_default',\n  'version': {'value': 1578909324L}},\n 'outputs': {u'ages': {'dtype': 1,\n   'float_val': [5.680944442749023],\n   'tensor_shape': {'dim': [{'size': 1L}]}}}}\n<\/code><\/pre>\n\n<p>but when I try to call from a Spark context (in a UDF) I get a serialization error.\nThe code I'm trying to run is:<\/p>\n\n<pre><code>dataRange = range(1, 10001)\nrangeRDD = sc.parallelize(dataRange, 8)\nnew_data = rangeRDD.map(lambda x : call_predict())\nnew_data.count()\n<\/code><\/pre>\n\n<p>and the error I get is:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nPicklingError                             Traceback (most recent call last)\n&lt;command-2282434&gt; in &lt;module&gt;()\n      2 rangeRDD = sc.parallelize(dataRange, 8)\n      3 new_data = rangeRDD.map(lambda x : call_predict())\n----&gt; 4 new_data.count()\n      5 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in count(self)\n   1094         3\n   1095         \"\"\"\n-&gt; 1096         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n   1097 \n   1098     def stats(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in sum(self)\n   1085         6.0\n   1086         \"\"\"\n-&gt; 1087         return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n   1088 \n   1089     def count(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in fold(self, zeroValue, op)\n    956         # zeroValue provided to each partition is unique from the one provided\n    957         # to the final reduce call\n--&gt; 958         vals = self.mapPartitions(func).collect()\n    959         return reduce(op, vals, zeroValue)\n    960 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in collect(self)\n    829         # Default path used in OSS Spark \/ for non-credential passthrough clusters:\n    830         with SCCallSiteSync(self.context) as css:\n--&gt; 831             sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n    832         return list(_load_from_socket(sock_info, self._jrdd_deserializer))\n    833 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _jrdd(self)\n   2573 \n   2574         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n-&gt; 2575                                       self._jrdd_deserializer, profiler)\n   2576         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n   2577                                              self.preservesPartitioning, self.is_barrier)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _wrap_function(sc, func, deserializer, serializer, profiler)\n   2475     assert serializer, \"serializer should not be empty\"\n   2476     command = (func, profiler, deserializer, serializer)\n-&gt; 2477     pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n   2478     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n   2479                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _prepare_for_python_RDD(sc, command)\n   2461     # the serialized command will be compressed by broadcast\n   2462     ser = CloudPickleSerializer()\n-&gt; 2463     pickled_command = ser.dumps(command)\n   2464     if len(pickled_command) &gt; sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  # Default 1M\n   2465         # The broadcast will have same life cycle as created PythonRDD\n\n\/databricks\/spark\/python\/pyspark\/serializers.pyc in dumps(self, obj)\n    709                 msg = \"Could not serialize object: %s: %s\" % (e.__class__.__name__, emsg)\n    710             cloudpickle.print_exec(sys.stderr)\n--&gt; 711             raise pickle.PicklingError(msg)\n    712 \n    713 \n\nPicklingError: Could not serialize object: TypeError: can't pickle _ssl._SSLSocket objects\n<\/code><\/pre>\n\n<p>Not sure what is this serialization error - does is complain about failing to deserialize the Predictor<\/p>\n\n<p>My notebook has a cell which was called prior to the above cells with the following imports:<\/p>\n\n<pre><code>import sagemaker\nimport boto3\nfrom sagemaker.tensorflow.model import TensorFlowPredictor\nimport tensorflow as tf\nimport numpy as np\nimport time\n<\/code><\/pre>\n\n<p>The Predictor was created with the following code:<\/p>\n\n<pre><code>sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY,\n                                aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\nsagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY,\n                                        aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\nboto_session = boto3.Session(region_name='us-east-1')\nsagemaker_session = sagemaker.Session(boto_session, sagemaker_client=sagemaker_client, sagemaker_runtime_client=sagemaker_runtime_client)\n\npredictor = TensorFlowPredictor('endpoint-poc', sagemaker_session)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579190415880,
        "Question_score":1,
        "Question_tags":"tensorflow|pyspark|amazon-sagemaker",
        "Question_view_count":322,
        "Owner_creation_time":1458550179920,
        "Owner_last_access_time":1658058794840,
        "Owner_location":null,
        "Owner_reputation":383,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The udf function will be executed by multiple spark tasks in parallel. Those tasks run in completely isolated python processes and they are scheduled to physically different machines. Hence each data, those functions reference, must be on the same node. This is the case for everything created within the udf.<\/p>\n\n<p>Whenever you reference any object outside of the udf from the function, this data structure needs to be serialised (pickled) to each executor. Some object state, like open connections to a socket, cannot be pickled.<\/p>\n\n<p>You need to make sure, that connections are lazily opened each executor. It must happen only on the first function call on that executor. The <a href=\"https:\/\/spark.apache.org\/docs\/latest\/streaming-programming-guide.html#design-patterns-for-using-foreachrdd\" rel=\"nofollow noreferrer\">connection pooling topic<\/a> is covered in the docs, however only in the spark streaming guide (though it also applies for normal batch jobs).<\/p>\n\n<p>Normally one can use the Singleton Pattern for this. But in python people use the Borgh pattern.<\/p>\n\n<pre><code>class Env:\n    _shared_state = {\n        \"sagemaker_client\": None\n        \"sagemaker_runtime_client\": None\n        \"boto_session\": None\n        \"sagemaker_session\": None\n        \"predictor\": None\n    }\n    def __init__(self):\n        self.__dict__ = self._shared_state\n        if not self.predictor:\n            self.sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n            self.sagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\n            self.boto_session = boto3.Session(region_name='us-east-1')\n            self.sagemaker_session = sagemaker.Session(self.boto_session, sagemaker_client=self.sagemaker_client, sagemaker_runtime_client=self.sagemaker_runtime_client)\n\n            self.predictor = TensorFlowPredictor('endpoint-poc', self.sagemaker_session)\n\n\n#....\ndef call_predict():\n   env = Env()\n   batch_size = 1\n   data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n   tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n   prediction = env.predictor.predict(tensor_proto)\n\n   print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n\nnew_data = rangeRDD.map(lambda x : call_predict())\n<\/code><\/pre>\n\n<p>The Env class is defined on the master node. Its <code>_shared_state<\/code> has empty entries. When then Env object is instantiated first time, it shares the state with all further instances of Env on any subsequent call to the udf. On each separate parallel running process this will happen exactly one time. This way the sessions are shared and do not need to pickled. <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1579206604950,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1579506845700,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59773503",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62602435,
        "Question_title":"How to train tensorflow on sagemaker in script mode when the data resides in multiple files on s3?",
        "Question_body":"<p>I have a <code>.npy<\/code> file for each one of the training instances. All of these files are available on S3 in <code>train_data<\/code> folder. I want to train a tensorflow model on these training instances. To do that, I wish to spin up separate aws training instance for each training job which could access the files from s3 and train the model on it. What changes in the training script are required for doing this?<\/p>\n<p>I have following config in the training script:<\/p>\n<pre><code>parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])\nparser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\nparser.add_argument('--train_channel', type=str, default=os.environ['SM_CHANNELS'])\n<\/code><\/pre>\n<p>I have created the training estimator in jupyter instance as:<\/p>\n<pre><code>tf_estimator = TensorFlow(entry_point = 'my_model.py', \n                          role = role, \n                          train_instance_count = 1, \n                          train_instance_type = 'local_gpu', \n                          framework_version = '1.15.2', \n                          py_version = 'py3', \n                          hyperparameters = {'epochs': 1})\n<\/code><\/pre>\n<p>I am calling the fit function of the estimator as:<\/p>\n<pre><code>tf_estimator.fit({'train_channel':'s3:\/\/sagemaker-ml\/train_data\/'})\n<\/code><\/pre>\n<p>where <code>train_data<\/code> folder on S3 contains the <code>.npy<\/code> files of training instances.<\/p>\n<p>But when I call the fit function, I get an error:<\/p>\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '[&quot;train_channel&quot;]\/train_data_12.npy'\n<\/code><\/pre>\n<p>Not sure what am I missing here, as I can see the file mentioned above on S3.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593203486503,
        "Question_score":0,
        "Question_tags":"amazon-s3|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":340,
        "Owner_creation_time":1378268842847,
        "Owner_last_access_time":1663808352557,
        "Owner_location":"Pune, India",
        "Owner_reputation":4616,
        "Owner_up_votes":751,
        "Owner_down_votes":5,
        "Owner_views":592,
        "Question_last_edit_time":1593203849583,
        "Answer_body":"<p><code>SM_CHANNELS<\/code> returns a list of channel names. What you're looking for is <code>SM_CHANNEL_TRAIN_CHANNEL<\/code> (&quot;SM_CHANNEL&quot; + your channel name), which provides the filesystem location for the channel:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>parser.add_argument('--train_channel', type=str, default=os.environ['SM_CHANNEL_TRAIN_CHANNEL'])\n<\/code><\/pre>\n<p>docs: <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_channel_channel_name\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_channel_channel_name<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1593530314240,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62602435",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72490682,
        "Question_title":"How to create an aws sagemaker project using terraform?",
        "Question_body":"<p>This is the terraform shown in the docs:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.example.id\n  }\n}\n<\/code><\/pre>\n<p>I created a service catalog product with id: &quot;prod-xxxxxxxxxxxxx&quot;.\nWhen I substitute the service catalog product id into the above template,\nto get the following:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.prod-xxxxxxxxxxxxx\n  }\n}\n<\/code><\/pre>\n<p>I run terraform plan, but the following error occurs:<\/p>\n<pre><code>A managed resource &quot;aws_servicecatalog_product&quot; &quot;prod-xxxxxxxxxxxxx&quot; has not been declared in the root module.\n\n<\/code><\/pre>\n<p>What do I need to do to fix this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1654264489180,
        "Question_score":1,
        "Question_tags":"amazon-web-services|terraform|amazon-sagemaker",
        "Question_view_count":298,
        "Owner_creation_time":1653511725307,
        "Owner_last_access_time":1663251784530,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":1654266014543,
        "Answer_body":"<p>Since the documentation is lacking a bit of clarity, in order to have this work as in the example, you would first have to create the Service Catalog product in Terraform as well, e.g.:<\/p>\n<pre><code>resource &quot;aws_servicecatalog_product&quot; &quot;example&quot; {\n  name  = &quot;example&quot;\n  owner = [aws_security_group.example.id] # &lt;---- This would need to be created first\n  type  = aws_subnet.main.id # &lt;---- This would need to be created first\n\n  provisioning_artifact_parameters {\n    template_url = &quot;https:\/\/s3.amazonaws.com\/cf-templates-ozkq9d3hgiq2-us-east-1\/temp1.json&quot;\n  }\n\n  tags = {\n    foo = &quot;bar&quot;\n  }\n}\n<\/code><\/pre>\n<p>You can reference it then in the SageMaker project the same way as in the example:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.example.id\n  }\n}\n<\/code><\/pre>\n<p>Each of the resources that gets created has a set of attributes that can be accessed as needed by other resources, data sources or outputs. In order to understand how this works, I strongly suggest reading the documentation about referencing values [1]. Since you already created the Service Catalog product, the only thing you need to do is provide the string value for the product ID:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = &quot;prod-xxxxxxxxxxxxx&quot;\n  }\n}\n<\/code><\/pre>\n<p>When I can't understand what value is expected by an argument (e.g., <code>product_id<\/code> in this case), I usually read the docs and look for examples like in [2]. Note: That example is CloudFormation, but it can help you understand what type of a value is expected (e.g., string, number, bool).<\/p>\n<p>You could also import the created Service Catalog product into Terraform so you can manage it with IaC [3]. You should understand all the implications of <code>terraform import<\/code> though before trying it [4].<\/p>\n<hr \/>\n<p>[1] <a href=\"https:\/\/www.terraform.io\/language\/expressions\/references\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/expressions\/references<\/a><\/p>\n<p>[2] <a href=\"https:\/\/docs.amazonaws.cn\/en_us\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-project.html#aws-resource-sagemaker-project--examples--SageMaker_Project_Example\" rel=\"nofollow noreferrer\">https:\/\/docs.amazonaws.cn\/en_us\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-project.html#aws-resource-sagemaker-project--examples--SageMaker_Project_Example<\/a><\/p>\n<p>[3] <a href=\"https:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/resources\/servicecatalog_product#import\" rel=\"nofollow noreferrer\">https:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/resources\/servicecatalog_product#import<\/a><\/p>\n<p>[4] <a href=\"https:\/\/www.terraform.io\/cli\/commands\/import\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/cli\/commands\/import<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1654269294487,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72490682",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56721821,
        "Question_title":"Can I copy or upload a Google Colab notebook onto a AWS Sagemaker instance?",
        "Question_body":"<p>Want to have a copy of my Google Colab python notebook in my AWS Sagemaker Jupyter notebook(newbie to AWS Sagemaker)<\/p>\n\n<p>I tried selecting all cells in my Colab notebook and pasting in my sagemaker Jupyter notebook using copy paste icons and via cmd+C and cmd+V<\/p>\n\n<p>Cannot copy paste all selected cells at once between Colab and Sagemaker Jupyter notebooks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1561274387650,
        "Question_score":1,
        "Question_tags":"python|jupyter-notebook|google-colaboratory|amazon-sagemaker",
        "Question_view_count":1621,
        "Owner_creation_time":1540309037063,
        "Owner_last_access_time":1646061363073,
        "Owner_location":null,
        "Owner_reputation":404,
        "Owner_up_votes":44,
        "Owner_down_votes":1,
        "Owner_views":24,
        "Question_last_edit_time":1561274899233,
        "Answer_body":"<p>While doing the drudgery work of copy pasting each cell between the notebooks(my bad), I realized that we could just <strong>download the notebook as .ipynb file on Colab<\/strong> and <strong>upload on the Sagemaker notebook instance using the <code>Upload button<\/code><\/strong>.<a href=\"https:\/\/i.stack.imgur.com\/z5wHt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/z5wHt.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1561274854930,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1561536601703,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56721821",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58755708,
        "Question_title":"Sagemaker Notebook Instance Type Recommendation",
        "Question_body":"<p>I will be running ml models on a pretty large dataset. It is about 15 gb, with 200 columns and 4.3 million rows. I'm wondering what the best Notebook instance type is for this kind of dataset in AWS Sagemaker.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1573155038577,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":2326,
        "Owner_creation_time":1568738384723,
        "Owner_last_access_time":1630159299740,
        "Owner_location":null,
        "Owner_reputation":137,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p><strong>For choosing a SageMaker hosted notebook type:<\/strong><\/p>\n\n<p>Do you plan to do all of your preprocessing of your data in-memory on the notebook, or do you plan to orchestrate ETL with external services? <\/p>\n\n<p>If you're planning to load the dataset into memory on the notebook instance for exploration\/preprocessing, the primary bottleneck here would be ensuring the instance has enough memory for your dataset. This would require at least the 16gb types (<em>.xlarge<\/em>) (full list of ML instance types <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/\" rel=\"noreferrer\">available here<\/a>). Further, depending on how compute intensive your pre-processing is, and your desired pre-processing completion time, you can opt for a compute optimized instance (<em>c4, c5<\/em>) to speed this up.<\/p>\n\n<hr>\n\n<p><strong>For the training job, specifically:<\/strong><\/p>\n\n<p>Using the Amazon SageMaker SDK, your training data will be loaded and distributed to the training cluster, allowing your training job to be completely separate from the instance your hosted notebook is running on.<\/p>\n\n<p>Figuring out the ideal instance type for training will depend on whether your algorithm of choice\/training job is memory, CPU, or IO bound. Since your dataset will likely be loaded onto your training cluster from S3, the instance you choose for your hosted notebook will have no bearing on the speed of your training job.<\/p>\n\n<hr>\n\n<p><strong>Broadly:<\/strong>\nWhen it comes to SageMaker notebooks, the best practice is to use your notebook as a \"puppeteer\" or orchestrator, that calls out to external services (AWS Glue or Amazon EMR for preprocessing, SageMaker for training, S3 for storage, etc). It is best to treat them as ephemeral forms of compute\/storage for building and kicking off your experiment pipeline.<\/p>\n\n<p>This will allow you to more closely pair compute, storage, and hosting resources\/services with the demands for your workload, ultimately resulting in the best bang for your buck by not having you pay for latent or unused resources.<\/p>\n\n<hr>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1573157762370,
        "Answer_score":6.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58755708",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71221741,
        "Question_title":"ValidationException in Sagemaker pipeline creation",
        "Question_body":"<p>I am new to Sagmaker. I am creating a pipeline in sagemaker where I initialize the number of epochs as a pipeline parameter. But when I upsert, it shows this error.\nCheck the following code for reference, please.<\/p>\n<pre><code>epoch_count = ParameterInteger(name=&quot;EpochCount&quot;, default_value=1)\npipeline = Pipeline(\nname=f&quot;a_name&quot;,\nparameters=[\n    training_instance_type,\n    training_instance_count,\n    epoch_count,\n    hugging_face_model_name,\n    endpoint_instance_type,\n    endpoint_instance_type_alternate,\n],\nsteps=[step_train, step_register, step_deploy_lambda],\nsagemaker_session=sagemaker_session,\n<\/code><\/pre>\n<p>)<\/p>\n<p>Error - ---<\/p>\n<pre><code>---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\n&lt;ipython-input-54-138a517611f0&gt; in &lt;module&gt;\n----&gt; 1 pipeline.upsert(role_arn=role)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in upsert(self, role_arn, description, tags, parallelism_config)\n    217         &quot;&quot;&quot;\n    218         try:\n--&gt; 219             response = self.create(role_arn, description, tags, parallelism_config)\n    220         except ClientError as e:\n    221             error = e.response[&quot;Error&quot;]\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in create(self, role_arn, description, tags, parallelism_config)\n    119             Tags=tags,\n    120         )\n--&gt; 121         return self.sagemaker_session.sagemaker_client.create_pipeline(**kwargs)\n    122 \n    123     def _create_args(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    389                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    390             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 391             return self._make_api_call(operation_name, kwargs)\n    392 \n    393         _api_call.__name__ = str(py_operation_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    717             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)\n    718             error_class = self.exceptions.from_code(error_code)\n--&gt; 719             raise error_class(parsed_response, operation_name)\n    720         else:\n    721             return parsed_response\n\nClientError: An error occurred (ValidationException) when calling the CreatePipeline operation: Cannot assign property reference [Parameters.EpochCount] to argument of type [String]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645534662633,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":608,
        "Owner_creation_time":1383025927423,
        "Owner_last_access_time":1663674890763,
        "Owner_location":"Dhaka, Bangladesh",
        "Owner_reputation":647,
        "Owner_up_votes":62,
        "Owner_down_votes":3,
        "Owner_views":105,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I replace<\/p>\n<pre><code>epoch_count = ParameterInteger(name=&quot;EpochCount&quot;, default_value=1)\n<\/code><\/pre>\n<p>with<\/p>\n<pre><code>epoch_count = ParameterString(name=&quot;EpochCount&quot;, default_value=&quot;1&quot;)\n<\/code><\/pre>\n<p>And it works. Maybe we can only use an integer in pipeline parameters from the sagemaker notebook. But epoch_count is being used in the docker container, which is not directly something of Sagemaker, and that's my understanding.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1645641627683,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1649157854310,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71221741",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53280902,
        "Question_title":"AWS SageMaker pd.read_pickle() doesn't work but read_csv() does?",
        "Question_body":"<p>I've recently been trying to train some models on an AWS SageMaker jupyter notebook instance.<\/p>\n\n<p>Everything is worked very well until I tried to load in some custom dataset (REDD) through files.<\/p>\n\n<p>I have the dataframes stored in Pickle (.pkl) files on an S3 bucket. I couldn't manage to read them into sagemaker so I decided to convert them to csv's as this seemed to work but I ran into a problem. This data has an index of type datetime64 and when using <code>.to_csv()<\/code> this index gets converted to pure text and it loses it's data structure (and I need to keep this specific index for correct plotting.)<\/p>\n\n<p>So I decided to try the Pickle files again but I can't get it to work and have no idea why.<\/p>\n\n<p>The following code for csv's works but I can't use it due to the index problem:<\/p>\n\n<pre><code>bucket = 'sagemaker-peno'\nhouses_dfs = {}\ndata_key = 'compressed_data\/'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\nfor file in range(6):\n    houses_dfs[file+1] = pd.read_csv(data_location+'house_'+str(file+1)+'.csv', index_col='Unnamed: 0')\n<\/code><\/pre>\n\n<p>But this code does NOT work even though it uses almost the exact same syntax:<\/p>\n\n<pre><code>bucket = 'sagemaker-peno'\nhouses_dfs = {}\ndata_key = 'compressed_data\/'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\nfor file in range(6):\n    houses_dfs[file+1] =  pd.read_pickle(data_location+'house_'+str(file+1)+'.pkl')\n<\/code><\/pre>\n\n<p>Yes it's 100% the correct path, because the csv and pkl files are stored in the same directory (compressed_data).<\/p>\n\n<p>It throws me this error while using the Pickle method:<\/p>\n\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/sagemaker-peno\/compressed_data\/house_1.pkl'\n<\/code><\/pre>\n\n<p>I hope to find someone who has dealt with this before and can solve the <code>read_pickle()<\/code> issue or as an alternative fix my datetime64 type issue with csv's.<\/p>\n\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1542111648037,
        "Question_score":1,
        "Question_tags":"pandas|csv|amazon-s3|pickle|amazon-sagemaker",
        "Question_view_count":788,
        "Owner_creation_time":1439124701797,
        "Owner_last_access_time":1657225764310,
        "Owner_location":"Leuven, Belgi\u00eb",
        "Owner_reputation":87,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Question_last_edit_time":null,
        "Answer_body":"<p>read_pickle() likes the full path more than a relative path from where it was run. This fixed my issue.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1543264373167,
        "Answer_score":-1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53280902",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54334462,
        "Question_title":"How can I quickly debug a SageMaker training script?",
        "Question_body":"<p>When running an ML training job in Amazon SageMaker, the training script is \"deployed\" and given an ML training instance, which takes about 10 minutes to spin up and get the data it needs. <\/p>\n\n<p>I can only get one error message from the training job, then it dies and the instance is killed along with it. <\/p>\n\n<p>After I make a change to the training script to fix it, I need to deploy and run it which takes another 10 minutes or so.<\/p>\n\n<p>How can I accomplish this faster, or keep the training instance running?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1548272148563,
        "Question_score":4,
        "Question_tags":"amazon-web-services|tensorflow|machine-learning|amazon-sagemaker",
        "Question_view_count":1902,
        "Owner_creation_time":1416193017423,
        "Owner_last_access_time":1663696463113,
        "Owner_location":"Gensokyo",
        "Owner_reputation":880,
        "Owner_up_votes":211,
        "Owner_down_votes":2,
        "Owner_views":111,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It seems that you are running a training job using one of the SageMaker frameworks. Given that, you can use the \"local mode\" feature of SageMaker, which will run your training job (specifically the container) locally in your notebook instance. That way, you can iterate on your script until it works. Then you can move on to the remote training cluster to train the model against the whole dataset if needed. To use local mode, you just set the instance type to \"local\". More details about local mode can be found at <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#sagemaker-python-sdk-overview\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#sagemaker-python-sdk-overview<\/a> and the blog post: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1548282026927,
        "Answer_score":5.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54334462",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62002183,
        "Question_title":"Use tensorboard with object detection API in sagemaker",
        "Question_body":"<p>with <a href=\"https:\/\/github.com\/svpino\/tensorflow-object-detection-sagemaker\" rel=\"nofollow noreferrer\">this<\/a> I successfully created a training job on sagemaker using the Tensorflow Object Detection API in a docker container. Now I'd like to monitor the training job using sagemaker, but cannot find anything explaining how to do it. I don't use a sagemaker notebook.\nI think I can do it by saving the logs into a S3 bucket and point there a local tensorboard instance .. but don't know how to tell the tensorflow object detection API where to save the logs (is there any command line argument for this ?).\nSomething like <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/keras_script_mode_pipe_mode_horovod\/tensorflow_keras_CIFAR10.ipynb\" rel=\"nofollow noreferrer\">this<\/a>, but the script <code>generate_tensorboard_command.py<\/code> fails because my training job don't have the <code>sagemaker_submit_directory<\/code> parameter..<\/p>\n<p>The fact is when I start the training job nothing is created on my s3 until the job finish and upload everything. There should be a way tell tensorflow where to save the logs (s3) during the training, hopefully without modifying the API source code..<\/p>\n<p><strong>Edit<\/strong><\/p>\n<p>I can finally make it works with the accepted solution (tensorflow natively supports read\/write to s3), there are however additional steps to do:<\/p>\n<ol>\n<li>Disable network isolation in the training job configuration<\/li>\n<li>Provide credentials to the docker image to write to S3 bucket<\/li>\n<\/ol>\n<p>The only thing is that Tensorflow continuously polls filesystem (i.e. looking for an updated model in serving mode) and this cause useless requests to S3, that you will have to pay (together with a buch of errors in the console). I opened a new question <a href=\"https:\/\/stackoverflow.com\/q\/64969198\/4267439\">here<\/a> for this. At least it works.<\/p>\n<p><strong>Edit 2<\/strong><\/p>\n<p>I was wrong, TF just write logs, is not polling so it's an expected behavior and the extra costs are minimal.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1590408759343,
        "Question_score":2,
        "Question_tags":"object-detection|tensorboard|amazon-sagemaker|object-detection-api|tensorflow-model-garden",
        "Question_view_count":311,
        "Owner_creation_time":1416346350293,
        "Owner_last_access_time":1664039219437,
        "Owner_location":"Jesi, Italy",
        "Owner_reputation":2302,
        "Owner_up_votes":51,
        "Owner_down_votes":4,
        "Owner_views":227,
        "Question_last_edit_time":1606323198013,
        "Answer_body":"<p>Looking through the example you posted, it looks as though the <code>model_dir<\/code> passed to the TensorFlow Object Detection package is configured to <code>\/opt\/ml\/model<\/code>:<\/p>\n<pre><code># These are the paths to where SageMaker mounts interesting things in your container.\nprefix = '\/opt\/ml\/'\ninput_path = os.path.join(prefix, 'input\/data')\noutput_path = os.path.join(prefix, 'output')\nmodel_path = os.path.join(prefix, 'model')\nparam_path = os.path.join(prefix, 'input\/config\/hyperparameters.json')\n<\/code><\/pre>\n<p>During the training process, tensorboard logs will be written to <code>\/opt\/ml\/model<\/code>, and then uploaded to s3 as a final model artifact AFTER training: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html<\/a>.<\/p>\n<p>You <em>might<\/em> be able to side-step the SageMaker artifact upload step and point the <code>model_dir<\/code> of TensorFlow Object Detection API directly at an s3 location during training:<\/p>\n<pre><code>model_path = &quot;s3:\/\/your-bucket\/path\/here\n<\/code><\/pre>\n<p>This means that the TensorFlow library within the SageMaker job is directly writing to S3 instead of the filesystem inside of it's container. Assuming the underlying TensorFlow Object Detection code can write directly to S3 (something you'll have to verify), you should be able to see the tensorboard logs and checkpoints there in realtime.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1594137982057,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62002183",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62309772,
        "Question_title":"Difference between Processor and ScriptProcessor in AWS Sagemaker SDK",
        "Question_body":"<p>From <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Sagemaker python SDK<\/a> I have seen two API, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.ScriptProcessor\" rel=\"nofollow noreferrer\">ScriptProcessor<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.Processor\" rel=\"nofollow noreferrer\">Processor<\/a>. It seems like we can achieve the same goals using either of them, the only difference I noticed ScriptProcessor support docker <code>command<\/code> parameter on the other hand Processor support docker <code>entrypoint<\/code> parameter. Is there any other difference amongst them? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591810831397,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":400,
        "Owner_creation_time":1370231111260,
        "Owner_last_access_time":1663504379830,
        "Owner_location":"Toronto, ON, Canada",
        "Owner_reputation":5295,
        "Owner_up_votes":351,
        "Owner_down_votes":4,
        "Owner_views":455,
        "Question_last_edit_time":null,
        "Answer_body":"<p><code>sagemaker.processing.ScriptProcessor<\/code> subclasses <code>sagemaker.processing.Processor<\/code>. <code>ScriptProcessor<\/code> can be used to write a custom processing script. <code>Processor<\/code> can be subclassed to create a <code>CustomProcessor<\/code> class for a more complex use case.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1604879360050,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62309772",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65753455,
        "Question_title":"How to get the version of gremlin python client on AWS SageMaker",
        "Question_body":"<p>What is the command to check the version of Gremlin Python client running on a AWS Sagemaker jupyter notebook? I would like to run the command on the jupyter notebook cell.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610823303577,
        "Question_score":0,
        "Question_tags":"gremlin|amazon-sagemaker",
        "Question_view_count":198,
        "Owner_creation_time":1475704783950,
        "Owner_last_access_time":1639340625583,
        "Owner_location":null,
        "Owner_reputation":473,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Question_last_edit_time":null,
        "Answer_body":"<p>From a notebook cell you should be able to just ask Pip which version is being used<\/p>\n<pre><code>!pip list | grep gremlinpython\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1610839775480,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65753455",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59801874,
        "Question_title":"When do you specify the Target variable in a SageMaker Training job?",
        "Question_body":"<p>I'm trying to create a Machine Learning algorithm following this tutorial : <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-console.html\" rel=\"nofollow noreferrer\">Get Started with Amazon SageMaker<\/a><\/p>\n\n<p>Unless I missed something in the tutorial, I didn't find any steps where we specify the target variable. Can someone explain where \/ when we specify our target variable when creating an ML model using SageMaker built-in algorithms? <\/p>\n\n<p>Thanks a lot! <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1579360480360,
        "Question_score":2,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":715,
        "Owner_creation_time":1562055808543,
        "Owner_last_access_time":1663141286633,
        "Owner_location":null,
        "Owner_reputation":895,
        "Owner_up_votes":29,
        "Owner_down_votes":1,
        "Owner_views":53,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It depends on the scientific paradigm you're using in SageMaker :)<\/p>\n\n<ul>\n<li>SageMaker Built-in algorithms all have their input specification,\ndescribed in their respective documentation. For example, for\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner.html#ll-input_output\" rel=\"nofollow noreferrer\">SageMaker Linear Learner<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#InputOutput-XGBoost\" rel=\"nofollow noreferrer\">SageMaker XGBoost<\/a> the target is assumed\nto be the first column.<\/li>\n<li>With custom code, such as Bring-Your-Own-Docker or SageMaker Framework containers (for Sklearn, TF, PyTorch, MXNet) since you are the one writing the code you can write any sort of logic, and the target can be any column of your dataset.<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1579512805583,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59801874",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65285203,
        "Question_title":"Why does AWS SageMaker create an S3 Bucket",
        "Question_body":"<p>Upon deploying a custom pytorch model with the boto3 client in python. I noticed that a new S3 bucket had been created with no visible objects. Is there a reason for this?<\/p>\n<p>The bucket that contained my model was named with the keyword &quot;sagemaker&quot; included, so I don't any issue there.<\/p>\n<p>Here is the code that I used for deployment:<\/p>\n<pre><code>remote_model = PyTorchModel(\n                     name = model_name, \n                     model_data=model_url,\n                     role=role,\n                     sagemaker_session = sess,\n                     entry_point=&quot;inference.py&quot;,\n                     # image=image, \n                     framework_version=&quot;1.5.0&quot;,\n                     py_version='py3'\n                    )\n\nremote_predictor = remote_model.deploy(\n                         instance_type='ml.g4dn.xlarge', \n                         initial_instance_count=1,\n                         #update_endpoint = True, # comment or False if endpoint doesns't exist\n                         endpoint_name=endpoint_name, # define a unique endpoint name; if ommited, Sagemaker will generate it based on used container\n                         wait=True\n                         )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1607931328823,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|amazon-s3|boto3|amazon-sagemaker",
        "Question_view_count":1047,
        "Owner_creation_time":1577873077020,
        "Owner_last_access_time":1663647732437,
        "Owner_location":"Perth WA, Australia",
        "Owner_reputation":438,
        "Owner_up_votes":88,
        "Owner_down_votes":3,
        "Owner_views":67,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It was likely created as a default bucket by the SageMaker Python SDK. Note that the code you wrote about is not <code>boto3<\/code> (AWS python SDK), but <code>sagemaker<\/code> (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">link<\/a>), the SageMaker-specific Python SDK, that is higher-level than boto3.<\/p>\n<p>The SageMaker Python SDK uses S3 at multiple places, for example to stage training code when using a Framework Estimator, and to stage inference code when deployment with a Framework Model (your case). It gives you control of the S3 location to use, but if you don't specify it, it may use an automatically generated bucket, if it has the permissions to do so.<\/p>\n<p>To control code staging S3 location, you can use the parameter <code>code_location<\/code> in either your <code>PyTorchEstimator<\/code> (training) or your <code>PyTorchModel<\/code> (serving)<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1608132873677,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65285203",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61550297,
        "Question_title":"Await returns too soon",
        "Question_body":"<p>I am retrieving a list of image filenames from DynamoDB and using those image filenames to replace the default <code>src=<\/code> image in a portion of a website.<\/p>\n\n<p>I'm a JS novice, so I'm certainly missing something, but my function is returning the list of filenames too late.<\/p>\n\n<p>My inline script is:<\/p>\n\n<pre><code>&lt;script&gt;\n        customElements.whenDefined( 'crowd-bounding-box' ).then( () =&gt; {  \n        var imgBox = document.getElementById('annotator');\n        newImg = imageslist();\n        console.log(\"Result of newImg is: \" + newImg);\n        imgBox.src = \"https:\/\/my-images-bucket.s3.amazonaws.com\/\" + newImg;\n    } )\n&lt;\/script&gt;\n<\/code><\/pre>\n\n<p>My JS function is:<\/p>\n\n<pre><code>async function imageslist() {\n    const username = \"sampleuser\";\n    const params = {\n        TableName: \"mytable\",\n        FilterExpression: \"attribute_not_exists(\" + username + \")\",\n        ReturnConsumedCapacity: \"NONE\"\n    };\n    try {\n        var data = await ddb.scan(params).promise()\n        var imglist = [];\n        for(let i = 0; i &lt; data.Items.length; i++) {\n            imglist.push(data.Items[i].img.S);\n        };\n        imglist.sort();\n        var firstimg = imglist[0];\n        console.log(firstimg);\n        return imglist\n    } catch(error) {\n        console.error(error);\n    }\n}\n<\/code><\/pre>\n\n<p>The console reports <code>Result of newImg is: [object Promise]<\/code> and shortly after that, it reports the expected filename.  After the page has been rendered, I can input <code>newImg<\/code> in the console and I receive the expected result.<\/p>\n\n<p>Am I using the <strong>await<\/strong> syntax improperly?<\/p>\n\n<p>Side note: This site uses the Crowd HTML Elements (for Ground Truth and Mechanical Turk), so I'm forced to have the <code>src=<\/code> attribute present in my <code>crowd-bounding-box<\/code> tag and it must be a non-zero value.  I'm loading a default image and replacing it with another image.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1588364900607,
        "Question_score":0,
        "Question_tags":"javascript|promise|async-await|amazon-sagemaker|mechanicalturk",
        "Question_view_count":63,
        "Owner_creation_time":1483444144907,
        "Owner_last_access_time":1643982821673,
        "Owner_location":"Hoth",
        "Owner_reputation":312,
        "Owner_up_votes":28,
        "Owner_down_votes":1,
        "Owner_views":65,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Anytime you use the <code>await<\/code> keyword, you must use the <code>async<\/code> keyword before the function definition (which you have done). The thing is, any time an async function is called, it will always return a <code>Promise<\/code> object since it expects that some asynchronous task will take place within the function.<\/p>\n\n<p>Therefore,you'll need to <code>await<\/code> the result of the imageslist function and make the surrounding function <code>async<\/code>.<\/p>\n\n<pre class=\"lang-js prettyprint-override\"><code>&lt;script&gt;\n    customElements.whenDefined( 'crowd-bounding-box' ).then( async () =&gt; {  \n        var imgBox = document.getElementById('annotator');\n        newImg = await imageslist();\n        console.log(\"Result of newImg is: \" + newImg);\n        imgBox.src = \"https:\/\/my-images-bucket.s3.amazonaws.com\/\" + newImg;\n    } )\n&lt;\/script&gt;\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1588365818457,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61550297",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56308169,
        "Question_title":"Creating a model for use in a pipeline from a hyperparameter tuning job",
        "Question_body":"<p>I'm trying to implement the best estimator from a hyperparameter tuning job into a pipeline object to deploy an endpoint.<\/p>\n\n<p>I've read the docs in a best effort to include the results from the tuning job in the pipeline, but I'm having trouble creating the Model() class object.<\/p>\n\n<pre><code># This is the hyperparameter tuning job\ntuner.fit({'train': s3_train, 'validation': s3_val}, \ninclude_cls_metadata=False)\n\n\n#With a standard Model (Not from the tuner) the process was as follows:\nscikit_learn_inferencee_model_name = sklearn_preprocessor.create_model()\nxgb_model_name = Model(model_data=xgb_model.model_data, image=xgb_image)\n\n\nmodel_name = 'xgb-inference-pipeline-' + timestamp_prefix\nendpoint_name = 'xgb-inference-pipeline-ep-' + timestamp_prefix\nsm_model = PipelineModel(\n    name=model_name, \n    role=role, \n    models=[\n        scikit_learn_inferencee_model_name, \n        xgb_model_name])\n\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', \nendpoint_name=endpoint_name)\n<\/code><\/pre>\n\n<p>I would like to be able to cleanly instantiate a model object using my results from the tuning job and pass it into the PipelineModel object. Any guidance is appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1558813444520,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|pipeline|amazon-sagemaker",
        "Question_view_count":455,
        "Owner_creation_time":1558812981693,
        "Owner_last_access_time":1629605922633,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I think you are on the right track. Do you get any error? Refer this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/e9c295c8538d29cc9fea2f73a29649126628064a\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb\" rel=\"nofollow noreferrer\">notebook<\/a>  for instantiating the model from the tuner and use in inference pipeline.<\/p>\n\n<p>Editing previous response based on the comment. To create model from the best training job of the hyperparameter tuning job, you can use below snippet<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tuner import HyperparameterTuner\nfrom sagemaker.estimator import Estimator\nfrom sagemaker.model import Model\n\n# Attach to an existing hyperparameter tuning job.\nxgb_tuning_job_name = 'my_xgb_hpo_tuning_job_name'\nxgb_tuner = HyperparameterTuner.attach(xgb_tuning_job_name)\n\n# Get the best XGBoost training job name from the HPO job\nxgb_best_training_job = xgb_tuner.best_training_job()\nprint(xgb_best_training_job)\n\n# Attach estimator to the best training job name\nxgb_best_estimator = Estimator.attach(xgb_best_training_job)\n\n# Create model to be passed to the inference pipeline\nxgb_model = Model(model_data=xgb_best_estimator.model_data,\n                  role=sagemaker.get_execution_role(),\n                  image=xgb_best_estimator.image_name)\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1558879357103,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1559176084157,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56308169",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50281188,
        "Question_title":"Sagemaker Java client generate IOrecord",
        "Question_body":"<p>I am trying to build a training set for Sagemaker using the Linear Learner algorithm. This algorithm supports recordIO wrapped protobuf and csv as format for the training data. As the training data is generated using spark I am having issues to generate a csv file from a dataframe (this seem broken for now), so I am trying to use protobuf. <\/p>\n\n<p>I managed to create a binary file for the training dataset using Protostuff which is a library that allows to generate protobuf messages from POJO objects. The problem is when triggering the training job I receive that message from SageMaker:\nClientError: No training data processed. Either the training channel is empty or the mini-batch size is too high. Verify that training data contains non-empty files and the mini-batch size is less than the number of records per training host.<\/p>\n\n<p>The training file is certainly not null. I suspect the way I generate the training data to be incorrect as I am able to train models using the libsvm format. Is there a way to generate IOrecord using the Sagemaker java client ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1525984750483,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":222,
        "Owner_creation_time":1428454496053,
        "Owner_last_access_time":1570063575877,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Answering my own question. It was an issue in the algorithm configuration. I reduced mini batch size and it worked fine.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1526653073370,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50281188",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69024005,
        "Question_title":"How to use SageMaker Estimator for model training and saving",
        "Question_body":"<p>The documentations of how to use SageMaker estimators are scattered around, sometimes obsolete, incorrect. Is there a one stop location which gives the comprehensive views of how to use SageMaker SDK Estimator to train and save models?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630555307170,
        "Question_score":27,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":6655,
        "Owner_creation_time":1416648155470,
        "Owner_last_access_time":1664057583237,
        "Owner_location":null,
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Question_last_edit_time":null,
        "Answer_body":"<h1>Answer<\/h1>\n<p>There is no one such resource from AWS that provides the comprehensive view of how to use SageMaker SDK Estimator to train and save models.<\/p>\n<h2>Alternative Overview Diagram<\/h2>\n<p>I put a diagram and brief explanation to get the overview on how SageMaker Estimator runs a training.<\/p>\n<ol>\n<li><p>SageMaker sets up a docker container for a training job where:<\/p>\n<ul>\n<li>Environment variables are set as in <a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container. Environment Variables<\/a>.<\/li>\n<li>Training data is setup under <code>\/opt\/ml\/input\/data<\/code>.<\/li>\n<li>Training script codes are setup under <code>\/opt\/ml\/code<\/code>.<\/li>\n<li><code>\/opt\/ml\/model<\/code> and <code>\/opt\/ml\/output<\/code> directories are setup to store training outputs.<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json  &lt;--- From Estimator hyperparameter arg\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;        &lt;--- From Estimator fit method inputs arg\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 code\n\u2502   \u2514\u2500\u2500 &lt;code files&gt;              &lt;--- From Estimator src_dir arg\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;             &lt;--- Location to save the trained model artifacts\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure                   &lt;--- Training job failure logs\n<\/code><\/pre>\n<ol start=\"2\">\n<li><p>SageMaker Estimator <code>fit(inputs)<\/code> method executes the training script. Estimator <code>hyperparameters<\/code> and <code>fit<\/code> method <code>inputs<\/code> are provided as its command line arguments.<\/p>\n<\/li>\n<li><p>The training script saves the model artifacts in the <code>\/opt\/ml\/model<\/code> once the training is completed.<\/p>\n<\/li>\n<li><p>SageMaker archives the artifacts under <code>\/opt\/ml\/model<\/code> into <code>model.tar.gz<\/code> and save it to the S3 location specified to <code>output_path<\/code> Estimator parameter.<\/p>\n<\/li>\n<li><p>You can set Estimator <code>metric_definitions<\/code> parameter to extract model metrics from the training logs. Then you can monitor the training progress in the SageMaker console metrics.<\/p>\n<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I believe AWS needs to stop mass-producing verbose, redundant, wordy, scattered, and obsolete documents. AWS needs to understand <strong>A picture is worth thousand words<\/strong>.<\/p>\n<p>Have diagrams and piece document parts together in a <strong>context<\/strong> with a clear objective to achieve.<\/p>\n<hr \/>\n<h1>Problem<\/h1>\n<p>AWS documentations need serious re-design and re-structuring. Just to understand <strong>how to train and save a model<\/strong> forces us going through dozens of scattered,  fragmented, verbose, redundant documentations, which are often obsolete, incomplete, and sometime incorrect.<\/p>\n<p>It is well-summarized in <a href=\"https:\/\/nandovillalba.medium.com\/why-i-think-gcp-is-better-than-aws-ea78f9975bda\" rel=\"noreferrer\">Why I think GCP is better than AWS<\/a>:<\/p>\n<blockquote>\n<p>It\u2019s not that AWS is harder to use than GCP, it\u2019s that <strong>it is needlessly hard<\/strong>; a disjointed, sprawl of infrastructure primitives with poor cohesion between them.  <br><br>\nA challenge is nice, a confusing mess is not, and <strong>the problem with AWS is that a large part of your working hours will be spent untangling their documentation and weeding through features and products to find what you want<\/strong>, rather than focusing on cool interesting challenges.<\/p>\n<\/blockquote>\n<p>Especially the SageMaker team keeps changing implementations without updating documents. Its roll-out was also inconsistent, e.g. SDK version 2 was rolled out in the SageMaker Studio making the AWS examples in Github incompatible without announcing it. Whereas SageMaker instance still had SDK 1, hence code worked in Instance but not in Studio.<\/p>\n<p>It is mind-boggling that we have to go through these many documents below to understand how to use the SageMaker SDK Estimator for training.<\/p>\n<h2>Documents for Model Training<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\" rel=\"noreferrer\">Train a Model with Amazon SageMaker<\/a><\/li>\n<\/ul>\n<p>This document gives 20,000 feet overview of how SageMaker training but does not give any clue what to do.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<p>This document gives an overview of how SageMaker training looks like. However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<blockquote>\n<p>WARNING: This package has been deprecated. Please use the SageMaker Training Toolkit for model training and the SageMaker Inference Toolkit for model serving.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model.html\" rel=\"noreferrer\">Step 4: Train a Model<\/a><\/li>\n<\/ul>\n<p>This document layouts the steps for training.<\/p>\n<blockquote>\n<p>The Amazon SageMaker Python SDK provides framework estimators and generic estimators to train your model while orchestrating the machine learning (ML) lifecycle accessing the SageMaker features for training and the AWS infrastructures<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#train-a-model-with-the-sagemaker-python-sdk\" rel=\"noreferrer\">Train a Model with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To train a model by using the SageMaker Python SDK, you:<\/p>\n<ul>\n<li>Prepare a training script<\/li>\n<li>Create an estimator<\/li>\n<li>Call the fit method of the estimator<\/li>\n<\/ul>\n<\/blockquote>\n<p>Finally this document gives concrete steps and ideas. However still missing comprehensiv details about Environment Variables, Directory structure in the SageMaker docker container**, S3 for uploading code, placing data, S3 where the trained model is saved, etc.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html\" rel=\"noreferrer\">Use TensorFlow with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<p>This documents is focused on TensorFlow Estimator implementation steps. Use <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/get_started_mnist_train.ipynb\" rel=\"noreferrer\">Training a Tensorflow Model on MNIST<\/a> Github example to accompany with to follow the actual implementation.<\/p>\n<h2>Documents for passing parameters and data locations<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig\" rel=\"noreferrer\">How Amazon SageMaker Provides Training Information<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>This section explains how SageMaker makes training information, such as training data, hyperparameters, and other configuration information, available to your Docker container.<\/p>\n<\/blockquote>\n<p>This document finally gives the idea of how parameters and data are passed around but again, not comprehensive.<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container Environment Variables<\/a><\/li>\n<\/ul>\n<p>This documentation is marked as <strong>deprecated<\/strong> but the only document which explains the SageMaker Environment Variables.<\/p>\n<blockquote>\n<h3>IMPORTANT ENVIRONMENT VARIABLES<\/h3>\n<ul>\n<li>SM_MODEL_DIR<\/li>\n<li>SM_CHANNELS<\/li>\n<li>SM_CHANNEL_{channel_name}<\/li>\n<li>SM_HPS<\/li>\n<li>SM_HP_{hyperparameter_name}<\/li>\n<li>SM_CURRENT_HOST<\/li>\n<li>SM_HOSTS<\/li>\n<li>SM_NUM_GPUS<\/li>\n<\/ul>\n<h3>List of provided environment variables by SageMaker Containers<\/h3>\n<ul>\n<li>SM_NUM_CPUS<\/li>\n<li>SM_LOG_LEVEL<\/li>\n<li>SM_NETWORK_INTERFACE_NAME<\/li>\n<li>SM_USER_ARGS<\/li>\n<li>SM_INPUT_DIR<\/li>\n<li>SM_INPUT_CONFIG_DIR<\/li>\n<li>SM_OUTPUT_DATA_DIR<\/li>\n<li>SM_RESOURCE_CONFIG<\/li>\n<li>SM_INPUT_DATA_CONFIG<\/li>\n<li>SM_TRAINING_ENV<\/li>\n<\/ul>\n<\/blockquote>\n<h2>Documents for SageMaker Docker Container Directory Structure<\/h2>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure\n<\/code><\/pre>\n<p>This document explains the directory structure and purpose of each directory.<\/p>\n<blockquote>\n<h3>The input<\/h3>\n<ul>\n<li>\/opt\/ml\/input\/config contains information to control how your program runs. hyperparameters.json is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. resourceConfig.json is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn\u2019t support distributed training, we\u2019ll ignore it here.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;\/ (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it\u2019s generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;_&lt;epoch_number&gt; (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch.<\/li>\n<\/ul>\n<h3>The output<\/h3>\n<ul>\n<li>\/opt\/ml\/model\/ is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the DescribeTrainingJob result.<\/li>\n<li>\/opt\/ml\/output is a directory where the algorithm can write a file failure that describes why the job failed. The contents of this file will be returned in the FailureReason field of the DescribeTrainingJob result. For jobs that succeed, there is no reason to write this file as it will be ignored.<\/li>\n<\/ul>\n<\/blockquote>\n<p>However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<h2>Documents for Model Saving<\/h2>\n<p>The information on where the trained model is saved and in what format are fundamentally missing. The training script needs to save the model under <code>\/opt\/ml\/model<\/code> and the format and sub-directory structure depend on the frameworks e,g TensorFlow, Pytorch. This is because SageMaker deployment uses the Framework dependent model-serving, e,g. TensorFlow Serving for TensorFlow framework.<\/p>\n<p>This is not clearly documented and causing confusions. The developer needs to specify which format to use and under which sub-directory to save.<\/p>\n<p>To use TensorFlow Estimator training and deployment:<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/aws_sagemaker_studio\/frameworks\/keras_pipe_mode_horovod\/keras_pipe_mode_horovod_cifar10.html#Deploy-the-trained-model\" rel=\"noreferrer\">Deploy the trained model<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>Because <strong>we\u2019re using TensorFlow Serving for deployment<\/strong>, our training script <strong>saves the model in TensorFlow\u2019s SavedModel format<\/strong>.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/code\/train.py#L159-L166\" rel=\"noreferrer\">amazon-sagemaker-examples\/frameworks\/tensorflow\/code\/train.py <\/a><\/li>\n<\/ul>\n<pre><code>    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    ckpt_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n    model.save(ckpt_dir)\n<\/code><\/pre>\n<p>The code is saving the model in <code>\/opt\/ml\/model\/00000000<\/code> because this is for TensorFlow serving.<\/p>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/guide\/saved_model\" rel=\"noreferrer\">Using the SavedModel format<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>The save-path follows a convention used by TensorFlow Serving where the last path component (1\/ here) is a version number for your model - it allows tools like Tensorflow Serving to reason about the relative freshness.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/tfx\/tutorials\/serving\/rest_simple#save_your_model\" rel=\"noreferrer\">Train and serve a TensorFlow model with TensorFlow Serving<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To load our trained model into TensorFlow Serving we first need to save it in SavedModel format. This will create a protobuf file in a well-defined directory hierarchy, and will include a version number. TensorFlow Serving allows us to select which version of a model, or &quot;servable&quot; we want to use when we make inference requests. Each version will be exported to a different sub-directory under the given path.<\/p>\n<\/blockquote>\n<h2>Documents for API<\/h2>\n<p>Basically the SageMaker SDK Estimator implements the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html\" rel=\"noreferrer\">CreateTrainingJob<\/a> API for training part. Hence, better to understand how it is designed and what parameters need to be defined. Otherwise working on Estimators are like walking in the dark.<\/p>\n<hr \/>\n<h1>Example<\/h1>\n<h2>Jupyter Notebook<\/h2>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\nbucket = sagemaker_session.default_bucket()\n\nmetric_definitions = [\n    {&quot;Name&quot;: &quot;train:loss&quot;, &quot;Regex&quot;: &quot;.*loss: ([0-9\\\\.]+) - accuracy: [0-9\\\\.]+.*&quot;},\n    {&quot;Name&quot;: &quot;train:accuracy&quot;, &quot;Regex&quot;: &quot;.*loss: [0-9\\\\.]+ - accuracy: ([0-9\\\\.]+).*&quot;},\n    {\n        &quot;Name&quot;: &quot;validation:accuracy&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: ([0-9\\\\.]+).*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;validation:loss&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: ([0-9\\\\.]+) - val_accuracy: [0-9\\\\.]+.*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;sec\/sample&quot;,\n        &quot;Regex&quot;: &quot;.* - \\d+s (\\d+)[mu]s\/sample - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: [0-9\\\\.]+&quot;,\n    },\n]\n\nimport uuid\n\ncheckpoint_s3_prefix = &quot;checkpoints\/{}&quot;.format(str(uuid.uuid4()))\ncheckpoint_s3_uri = &quot;s3:\/\/{}\/{}\/&quot;.format(bucket, checkpoint_s3_prefix)\n\nfrom sagemaker.tensorflow import TensorFlow\n\n# --------------------------------------------------------------------------------\n# 'trainingJobName' msut satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}\n# --------------------------------------------------------------------------------\nbase_job_name = &quot;fashion-mnist&quot;\nhyperparameters = {\n    &quot;epochs&quot;: 2, \n    &quot;batch-size&quot;: 64\n}\nestimator = TensorFlow(\n    entry_point=&quot;fashion_mnist.py&quot;,\n    source_dir=&quot;src&quot;,\n    metric_definitions=metric_definitions,\n    hyperparameters=hyperparameters,\n    role=role,\n    input_mode='File',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    base_job_name=base_job_name,\n    checkpoint_s3_uri=checkpoint_s3_uri,\n    model_dir=False\n)\nestimator.fit()\n<\/code><\/pre>\n<h2>fashion_mnist.py<\/h2>\n<pre><code>import os\nimport argparse\nimport json\nimport multiprocessing\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras import backend as K\n\nprint(&quot;TensorFlow version: {}&quot;.format(tf.__version__))\nprint(&quot;Eager execution is: {}&quot;.format(tf.executing_eagerly()))\nprint(&quot;Keras version: {}&quot;.format(tf.keras.__version__))\n\n\nimage_width = 28\nimage_height = 28\n\n\ndef load_data():\n    fashion_mnist = tf.keras.datasets.fashion_mnist\n    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n\n    number_of_classes = len(set(y_train))\n    print(&quot;number_of_classes&quot;, number_of_classes)\n\n    x_train = x_train \/ 255.0\n    x_test = x_test \/ 255.0\n    x_full = np.concatenate((x_train, x_test), axis=0)\n    print(x_full.shape)\n\n    print(type(x_train))\n    print(x_train.shape)\n    print(x_train.dtype)\n    print(y_train.shape)\n    print(y_train.dtype)\n\n    # ## Train\n    # * C: Convolution layer\n    # * P: Pooling layer\n    # * B: Batch normalization layer\n    # * F: Fully connected layer\n    # * O: Output fully connected softmax layer\n\n    # Reshape data based on channels first \/ channels last strategy.\n    # This is dependent on whether you use TF, Theano or CNTK as backend.\n    # Source: https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/mnist_cnn.py\n    if K.image_data_format() == 'channels_first':\n        x = x_train.reshape(x_train.shape[0], 1, image_width, image_height)\n        x_test = x_test.reshape(x_test.shape[0], 1, image_width, image_height)\n        input_shape = (1, image_width, image_height)\n    else:\n        x_train = x_train.reshape(x_train.shape[0], image_width, image_height, 1)\n        x_test = x_test.reshape(x_test.shape[0], image_width, image_height, 1)\n        input_shape = (image_width, image_height, 1)\n\n    return x_train, y_train, x_test, y_test, input_shape, number_of_classes\n\n# tensorboard --logdir=\/full_path_to_your_logs\n\nvalidation_split = 0.2\nverbosity = 1\nuse_multiprocessing = True\nworkers = multiprocessing.cpu_count()\n\n\ndef train(model, x, y, args):\n    # SavedModel Output\n    tensorflow_saved_model_path = os.path.join(args.model_dir, &quot;tensorflow\/saved_model\/0&quot;)\n    os.makedirs(tensorflow_saved_model_path, exist_ok=True)\n\n    # Tensorboard Logs\n    tensorboard_logs_path = os.path.join(args.model_dir, &quot;tensorboard\/&quot;)\n    os.makedirs(tensorboard_logs_path, exist_ok=True)\n\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=tensorboard_logs_path,\n        write_graph=True,\n        write_images=True,\n        histogram_freq=1,  # How often to log histogram visualizations\n        embeddings_freq=1,  # How often to log embedding visualizations\n        update_freq=&quot;epoch&quot;,\n    )  # How often to write logs (default: once per epoch)\n\n    model.compile(\n        optimizer='adam',\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\n        metrics=['accuracy']\n    )\n    history = model.fit(\n        x,\n        y,\n        shuffle=True,\n        batch_size=args.batch_size,\n        epochs=args.epochs,\n        validation_split=validation_split,\n        use_multiprocessing=use_multiprocessing,\n        workers=workers,\n        verbose=verbosity,\n        callbacks=[\n            tensorboard_callback\n        ]\n    )\n    return history\n\n\ndef create_model(input_shape, number_of_classes):\n    model = Sequential([\n        Conv2D(\n            name=&quot;conv01&quot;,\n            filters=32,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=&quot;same&quot;,\n            activation='relu',\n            input_shape=input_shape\n        ),\n        MaxPooling2D(\n            name=&quot;pool01&quot;,\n            pool_size=(2, 2)\n        ),\n        Flatten(),  # 3D shape to 1D.\n        BatchNormalization(\n            name=&quot;batch_before_full01&quot;\n        ),\n        Dense(\n            name=&quot;full01&quot;,\n            units=300,\n            activation=&quot;relu&quot;\n        ),  # Fully connected layer\n        Dense(\n            name=&quot;output_softmax&quot;,\n            units=number_of_classes,\n            activation=&quot;softmax&quot;\n        )\n    ])\n    return model\n\n\ndef save_model(model, args):\n    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    model_save_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(model_save_dir):\n        os.makedirs(model_save_dir)\n    print(f&quot;saving model at {model_save_dir}&quot;)\n    model.save(model_save_dir)\n\n\ndef parse_args():\n    # --------------------------------------------------------------------------------\n    # https:\/\/docs.python.org\/dev\/library\/argparse.html#dest\n    # --------------------------------------------------------------------------------\n    parser = argparse.ArgumentParser()\n\n    # --------------------------------------------------------------------------------\n    # hyperparameters Estimator argument are passed as command-line arguments to the script.\n    # --------------------------------------------------------------------------------\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch-size', type=int, default=64)\n\n    # \/opt\/ml\/model\n    # sagemaker.tensorflow.estimator.TensorFlow override 'model_dir'.\n    # See https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/\\\n    # sagemaker.tensorflow.html#sagemaker.tensorflow.estimator.TensorFlow\n    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n\n    # \/opt\/ml\/output\n    parser.add_argument(&quot;--output_dir&quot;, type=str, default=os.environ[&quot;SM_OUTPUT_DIR&quot;])\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == &quot;__main__&quot;:\n    args = parse_args()\n    print(&quot;---------- key\/value args&quot;)\n    for key, value in vars(args).items():\n        print(f&quot;{key}:{value}&quot;)\n\n    x_train, y_train, x_test, y_test, input_shape, number_of_classes = load_data()\n    model = create_model(input_shape, number_of_classes)\n\n    history = train(model=model, x=x_train, y=y_train, args=args)\n    print(history)\n    \n    save_model(model, args)\n    results = model.evaluate(x_test, y_test, batch_size=100)\n    print(&quot;test loss, test accuracy:&quot;, results)\n<\/code><\/pre>\n<h2>SageMaker Console<\/h2>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Notebook output<\/h2>\n<pre><code>2021-09-03 03:02:04 Starting - Starting the training job...\n2021-09-03 03:02:16 Starting - Launching requested ML instancesProfilerReport-1630638122: InProgress\n......\n2021-09-03 03:03:17 Starting - Preparing the instances for training.........\n2021-09-03 03:04:59 Downloading - Downloading input data\n2021-09-03 03:04:59 Training - Downloading the training image...\n2021-09-03 03:05:23 Training - Training image download completed. Training in progress.2021-09-03 03:05:23.966037: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:23.969704: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n2021-09-03 03:05:24.118054: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:26,842 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n2021-09-03 03:05:26,852 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:27,734 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n\/usr\/local\/bin\/python3.7 -m pip install -r requirements.txt\nWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\nYou should consider upgrading via the '\/usr\/local\/bin\/python3.7 -m pip install --upgrade pip' command.\n\n2021-09-03 03:05:29,028 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,045 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,062 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,072 sagemaker-training-toolkit INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {},\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;batch-size&quot;: 64,\n        &quot;epochs&quot;: 2\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {},\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;fashion_mnist&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 4,\n    &quot;num_gpus&quot;: 0,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;fashion_mnist.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;batch-size&quot;:64,&quot;epochs&quot;:2}\nSM_USER_ENTRY_POINT=fashion_mnist.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=fashion_mnist\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=4\nSM_NUM_GPUS=0\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;batch-size&quot;:64,&quot;epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;fashion_mnist&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:4,&quot;num_gpus&quot;:0,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;fashion_mnist.py&quot;}\nSM_USER_ARGS=[&quot;--batch-size&quot;,&quot;64&quot;,&quot;--epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_HP_BATCH-SIZE=64\nSM_HP_EPOCHS=2\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/local\/lib\/python37.zip:\/usr\/local\/lib\/python3.7:\/usr\/local\/lib\/python3.7\/lib-dynload:\/usr\/local\/lib\/python3.7\/site-packages\n\nInvoking script with the following command:\n\n\/usr\/local\/bin\/python3.7 fashion_mnist.py --batch-size 64 --epochs 2\n\n\nTensorFlow version: 2.3.1\nEager execution is: True\nKeras version: 2.4.0\n---------- key\/value args\nepochs:2\nbatch_size:64\nmodel_dir:\/opt\/ml\/model\noutput_dir:\/opt\/ml\/output\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1630555307170,
        "Answer_score":65.0,
        "Question_favorite_count":14.0,
        "Answer_last_edit_time":1656913410063,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69024005",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73829280,
        "Question_title":"NVIDIA Triton vs TorchServe for SageMaker Inference",
        "Question_body":"<p><a href=\"https:\/\/developer.nvidia.com\/nvidia-triton-inference-server\" rel=\"nofollow noreferrer\">NVIDIA Triton<\/a>\u00a0vs\u00a0<a href=\"https:\/\/pytorch.org\/serve\/\" rel=\"nofollow noreferrer\">TorchServe<\/a>\u00a0for SageMaker inference? When to recommend each?<\/p>\n<p>Both are modern, production grade inference servers. TorchServe is the DLC default inference server for PyTorch models. Triton is also supported for PyTorch inference on SageMaker.<\/p>\n<p>Anyone has a good comparison matrix for both?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663943338403,
        "Question_score":0,
        "Question_tags":"pytorch|amazon-sagemaker|inference|torchserve|tritonserver",
        "Question_view_count":12,
        "Owner_creation_time":1389887039673,
        "Owner_last_access_time":1664076128463,
        "Owner_location":"Singapore",
        "Owner_reputation":5854,
        "Owner_up_votes":155,
        "Owner_down_votes":70,
        "Owner_views":794,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Important notes to add here where both serving stacks differ:<\/p>\n<p>TorchServe does not provide the Instance Groups feature that Triton does (that is, stacking many copies of the same model or even different models onto the same GPU). This is a major advantage for both realtime and batch use-cases, as the performance increase is almost proportional to the model replication count (i.e. 2 copies of the model get you almost twice the throughput and half the latency; check out a BERT benchmark of this here). Hard to match a feature that is almost like having 2+ GPU's for the price of one.\nif you are deploying PyTorch DL models, odds are you often want to accelerate them with GPU's. TensorRT (TRT) is a compiler developed by NVIDIA that automatically quantizes and optimizes your model graph, which represents another huge speed up, depending on GPU architecture and model. It is understandably so probably the best way of automatically optimizing your model to run efficiently on GPU's and make good use of TensorCores. Triton has native integration to run TensorRT engines as they're called (even automatically converting your model to a TRT engine via config file), while TorchServe does not (even though you can use TRT engines with it).\nThere is more parity between both when it comes to other important serving features: both have dynamic batching support, you can define inference DAG's with both (not sure if the latter works with TorchServe on SageMaker without a big hassle), and both support custom code\/handlers instead of just being able to serve a model's forward function.<\/p>\n<p>Finally, MME on GPU (coming shortly) will be based on Triton, which is a valid argument for customers to get familiar with it so that they can quickly leverage this new feature for cost-optimization.<\/p>\n<p>Bottom line I think that Triton is just as easy (if not easier) ot use, a lot more optimized\/integrated for taking full advantage of the underlying hardware (and will be updated to keep being that way as newer GPU architectures are released, enabling an easy move to them), and in general blows TorchServe out of the water performance-wise when its optimization features are used in combination.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1663971240670,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73829280",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73645084,
        "Question_title":"Create Hugging Face Transformers Tokenizer using Amazon SageMaker in a distributed way",
        "Question_body":"<p>I am using the SageMaker HuggingFace Processor to create a custom tokenizer on a large volume of text data.\nIs there a way to make this job data distributed - meaning read partitions of data across nodes and train the tokenizer leveraging multiple CPUs\/GPUs.<\/p>\n<p>At the moment, providing more nodes to the processing cluster merely replicates the tokenization process (basically duplicates the process of creation), which is redundant. You can primarily only scale vertically.<\/p>\n<p>Any insights into this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662621424647,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|huggingface-transformers|huggingface-tokenizers",
        "Question_view_count":27,
        "Owner_creation_time":1662621266503,
        "Owner_last_access_time":1663966999637,
        "Owner_location":null,
        "Owner_reputation":48,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1662627758727,
        "Answer_body":"<p>Considering the following example code for\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/processing-job-frameworks-hugging-face.html\" rel=\"nofollow noreferrer\">HuggingFaceProcessor<\/a>:<\/p>\n<p>If you have 100 large files in S3 and use a ProcessingInput with\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ProcessingS3Input.html#:%7E:text=S3DataDistributionType\" rel=\"nofollow noreferrer\">s3_data_distribution_type<\/a>=&quot;ShardedByS3Key&quot; (instead of FullyReplicated), the objects in your S3 prefix will be sharded and distributed to your instances.<\/p>\n<p>For example, if you have 100 large files and want to filter records from them using HuggingFace on 5 instances, the s3_data_distribution_type=&quot;ShardedByS3Key&quot; will put 20 objects on each instance, and each instance can read the files from its own path, filter out records, and write (uniquely named) files to the output paths, and SageMaker Processing will put the filtered files in S3.<\/p>\n<p>However, if your filtering criteria is stateful or depends on doing a full pass over the dataset first (such as: filtering outliers based on mean and standard deviation on a feature - in case of using SKLean Processor for example): you'll need to pass that information in to the job so each instance can know how to filter. To send information to the instances launched, you have to use the\u00a0<code>\/opt\/ml\/config\/resourceconfig.json<\/code>\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-your-own-processing-container.html#byoc-config\" rel=\"nofollow noreferrer\">file<\/a>:<\/p>\n<p><code>{ &quot;current_host&quot;: &quot;algo-1&quot;, &quot;hosts&quot;: [&quot;algo-1&quot;,&quot;algo-2&quot;,&quot;algo-3&quot;] }<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1662653309533,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73645084",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59446807,
        "Question_title":"sagemaker horizontally scaling tensorflow (keras) model",
        "Question_body":"<p>I am roughly following this script <a href=\"https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/blob\/master\/keras\/05-keras-blog-post\/Fashion%20MNIST-SageMaker.ipynb\" rel=\"nofollow noreferrer\">fashion-MNIST-sagemaker<\/a>.<\/p>\n\n<p>I see that in the notebook <\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='mnist_keras_tf.py', \n                          role=role,\n                          train_instance_count=1, \n                          train_instance_type='local',\n                          framework_version='1.12', \n                          py_version='py3',\n                          script_mode=True,\n                          hyperparameters={'epochs': 1}\n                         )\n<\/code><\/pre>\n\n<p>I am wondering to what extent I can and should use the <code>train_instance_count<\/code> parameter. Will it distribute training along some dimension automatically, if yes - what is the dimension?<\/p>\n\n<p>Further, does it generally make sense to distribute training horizontally in a keras (with tensorflow) based setting?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1577037857400,
        "Question_score":0,
        "Question_tags":"python|tensorflow|keras|amazon-sagemaker|horizontal-scaling",
        "Question_view_count":295,
        "Owner_creation_time":1456487654210,
        "Owner_last_access_time":1664019268877,
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":1464,
        "Owner_up_votes":81,
        "Owner_down_votes":9,
        "Owner_views":62,
        "Question_last_edit_time":null,
        "Answer_body":"<p>distributed training is model and framework specific. Not all models are easy to distribute, and from ML framework to ML framework things are not equally easy. <strong>It is rarely automatic, even less so with TensorFlow and Keras.<\/strong><\/p>\n\n<p>Neural nets are conceptually easy to distribute under the data-parallel paradigm, whereby the gradient computation of a given mini-batch is split among workers, which could be multiple devices in the same host (multi-device) or multiple hosts with each multiple devices (multi-device multi-host). The D2L.ai course provides an in-depth view of how neural nets are distributed <a href=\"http:\/\/d2l.ai\/chapter_computational-performance\/multiple-gpus.html\" rel=\"nofollow noreferrer\">here<\/a> and <a href=\"http:\/\/d2l.ai\/chapter_computational-performance\/parameterserver.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Keras used to be trivial to distribute in multi-device, single host fashion with the <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/utils\/multi_gpu_model\" rel=\"nofollow noreferrer\"><code>multi_gpu_model<\/code>, which will sadly get deprecated in 4 months<\/a>. In your case, you seem to refer to multi-host model (more than one machine), and that requires writing ad-hoc synchronization code such as the one seen in <a href=\"https:\/\/www.tensorflow.org\/tutorials\/distribute\/multi_worker_with_keras\" rel=\"nofollow noreferrer\">this official tutorial<\/a>. <\/p>\n\n<p>Now let's look at how does this relate to SageMaker.<\/p>\n\n<p>SageMaker comes with 3 options for algorithm development. Using distributed training may require a varying amount of custom work depending on the option you choose:<\/p>\n\n<ol>\n<li><p>The <strong>built-in algorithms<\/strong> is a library of 18 pre-written algorithms. Many of them are written to be distributed in single-host multi-GPU or multi-GPU multi-host. With that first option, you don't have anything to do apart from setting <code>train_instance_count<\/code> > 1 to distribute over multiple instances<\/p><\/li>\n<li><p>The <strong>Framework containers<\/strong> (the option you are using) are containers developed for popular frameworks (TensorFlow, PyTorch, Sklearn, MXNet) and provide pre-written docker environment in which you can write arbitrary code. In this options, some container will support one-click creation of ephemeral training clusters to do distributed training, <strong>however using <code>train_instance_count<\/code> greater than one is not enough to distribute the training of your model. It will just run your script on multiple machines. In order to distribute your training, you must write appropriate distribution and synchronization code in your <code>mnist_keras_tf.py<\/code> script.<\/strong> For some frameworks such code modification will be very simple, for example for TensorFlow and Keras, SageMaker comes with Horovod pre-installed. Horovod is a peer-to-peer ring-style communication mechanism that requires very little code modification and is highly scalable (<a href=\"https:\/\/eng.uber.com\/horovod\/\" rel=\"nofollow noreferrer\">initial annoucement from Uber<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html#training-with-horovod\" rel=\"nofollow noreferrer\">SageMaker doc<\/a>, <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_horovod\/tensorflow_script_mode_horovod.ipynb\" rel=\"nofollow noreferrer\">SageMaker example<\/a>, <a href=\"https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/launching-tensorflow-distributed-training-easily-with-horovod-or-parameter-servers-in-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">SageMaker blog post<\/a>). My recommendation would be to try using Horovod to distribute your code. Similarly, in Apache MXNet you can easily create Parameter Stores to host model parameters in a distributed fashion and sync with them from multiple nodes. MXNet scalability and ease of distribution is one of the reason Amazon loves it.<\/p><\/li>\n<li><p>The <strong>Bring-Your-Own Container<\/strong> requires you to write both docker container and algorithm code. In this situation, you can of course distribute your training over multiple machines but you also have to write machine-to-machine communication code<\/p><\/li>\n<\/ol>\n\n<p>For your specific situation my recommendation would be to scale horizontally first in a single node with multiple GPUs over bigger and bigger machine types, because latency and complexity increase drastically as you switch from single-host to multi-host context. If truly necessary, use multi-node context and things maybe easier if that's done with Horovod.\nIn any case, things are still much easier to do with SageMaker since it manages creation of ephemeral, billed-per-second clusters with built-in, logging and metadata and artifacts persistence and also handles fast training data loading from s3, sharded over training nodes. <\/p>\n\n<p><strong>Note on the relevancy of distributed training<\/strong>: Keep in mind that when you distribute over N devices a model that was running fine on one device, you usually grow the batch size by N so that the per-device batch size stays constant and each device keeps being busy. This will disturb your model convergence, because bigger batches means a less noisy SGD. A common heuristic is to grow the learning rate by N (more info in <a href=\"https:\/\/arxiv.org\/abs\/1706.02677\" rel=\"nofollow noreferrer\">this great paper from Priya Goyal et al<\/a>), but this on the other hand induces instability at the first couple epochs, so it is sometimes associated with a learning rate warmup. Scaling SGD to work well with very large batches is still an active research problem, with new ideas coming up frequently. Reaching good model performance with very large batches sometimes require ad-hoc research and a fair amount of parameter tuning, occasionally to the extent where the extra money spent on finding how to distribute well overcome the benefits of the faster training you eventually manage to run. A situation where distributed training makes sense is when an individual record represent too much compute to form a big enough physical batch on a device, a situation seen on big input sizes (eg vision over HD pictures) or big parameter counts (eg BERT). That being said, for those models requiring very big logical batch you don't necessarily have to distribute things physically: you can run sequentially N batches through your single GPU and wait N per-device batches before doing the gradient averaging and parameter update to simulate having an N times bigger GPU. (a clever hack sometimes called <em>gradient accumulation<\/em>)<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1577100115740,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1577101498307,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59446807",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64925353,
        "Question_title":"How to run and deploy AWS's XGBoost MNIST sample notebook on SageMaker?",
        "Question_body":"<p>I am trying to use <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/simplify-machine-learning-inference-on-kubernetes-with-amazon-sagemaker-operators\/\" rel=\"nofollow noreferrer\">Kubernetes SageMaker Operations<\/a> with the XGBoost MNIST AWS's <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_mnist\/xgboost_mnist.ipynb\" rel=\"nofollow noreferrer\">example<\/a>.<\/p>\n<p>Before I am enabling Kubernetes SageMaker Ops, I have deployed the XGBoost MNIST example via SageMaker WebUI itself and tried to access the endpoint via awscli:<\/p>\n<pre><code>$ aws sagemaker-runtime invoke-endpoint \\\n    --region eu-west-1 \\\n    --endpoint-name DEMO-XGBoostEndpoint-2020-11-20-06-26-30 \\\n    --body $(seq 784 | xargs echo | sed 's\/ \/,\/g') \\\n    &gt;(cat) \\\n    --content-type text\/csv &gt; \/dev\/null\n<\/code><\/pre>\n<p>However, I am encountering the following decoding error:<\/p>\n<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message &quot;Loading csv data failed with Exception, please ensure data is in csv format:\n &lt;class 'UnicodeDecodeError'&gt;\n 'utf-8' codec can't decode byte 0xd7 in position 0: invalid continuation byte&quot;. See https:\/\/xxxx.console.aws.amazon.com\/cloudwatch\/home?region=xxxxx#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/DEMO-XGBoostEndpoint-2020-11-20-06-26-30 in account XXX for more information.\n<\/code><\/pre>\n<p>And in the log I can see:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/algorithm_mode\/serve_utils.py&quot;, line 102, in parse_content_data\n    decoded_payload = payload.strip().decode(&quot;utf-8&quot;)\nTraceback (most recent call last): File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/algorithm_mode\/serve_utils.py&quot;, line 102, in parse_content_data decoded_payload = payload.strip().decode(&quot;utf-8&quot;)\n<\/code><\/pre>\n<p>When I go to the source code of <a href=\"https:\/\/github.com\/aws\/sagemaker-xgboost-container\/blob\/master\/src\/sagemaker_xgboost_container\/algorithm_mode\/serve_utils.py#L102\" rel=\"nofollow noreferrer\">sagemaker_xgboost_container<\/a> I can see that they expect UTF-8 format:<\/p>\n<pre><code>        decoded_payload = payload.strip().decode(&quot;utf-8&quot;)\n<\/code><\/pre>\n<p>My <code>locale<\/code> seems fine and I am really not sure what else could go wrong:<\/p>\n<pre><code>$ locale\nLANG=C.UTF-8\nLANGUAGE=\nLC_CTYPE=&quot;C.UTF-8&quot;\nLC_NUMERIC=&quot;C.UTF-8&quot;\nLC_TIME=&quot;C.UTF-8&quot;\nLC_COLLATE=&quot;C.UTF-8&quot;\nLC_MONETARY=&quot;C.UTF-8&quot;\nLC_MESSAGES=&quot;C.UTF-8&quot;\nLC_PAPER=&quot;C.UTF-8&quot;\nLC_NAME=&quot;C.UTF-8&quot;\nLC_ADDRESS=&quot;C.UTF-8&quot;\nLC_TELEPHONE=&quot;C.UTF-8&quot;\nLC_MEASUREMENT=&quot;C.UTF-8&quot;\nLC_IDENTIFICATION=&quot;C.UTF-8&quot;\nLC_ALL=\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1605856943127,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|encoding|amazon-sagemaker",
        "Question_view_count":183,
        "Owner_creation_time":1320517748837,
        "Owner_last_access_time":1664043736817,
        "Owner_location":null,
        "Owner_reputation":37777,
        "Owner_up_votes":1579,
        "Owner_down_votes":282,
        "Owner_views":5307,
        "Question_last_edit_time":1606193820233,
        "Answer_body":"<p>I have contacted AWS support and apparently this is a bug in <code>awscli<\/code>. Here is a revised excerpt from their long and detailed answer.<\/p>\n<blockquote>\n<p>This is an encoding issue with AWSCLI v2. For now, you can proceed\nwith AWSCLI v1.18 as a temporary solution.<\/p>\n<\/blockquote>\n<p>I also verified it works with aws-cli\/1.18.185:<\/p>\n<pre><code>$ aws --version\naws-cli\/1.18.185 Python\/3.8.3 Linux\/4.19.104-microsoft-standard botocore\/1.19.25\n\n$ aws sagemaker-runtime invoke-endpoint \\\n&gt;     --region eu-west-1 \\\n&gt;     --endpoint-name DEMO-XGBoostEndpoint-2020-11-20-06-26-30 \\\n&gt;     --body $(seq 784 | xargs echo | sed 's\/ \/,\/g') \\\n&gt;     &gt;(cat) \\\n&gt;     --content-type text\/csv &gt; \/dev\/null\n8.0%\n<\/code><\/pre>\n<p>In AWS cli v2.1.21 amazon added the <code>--cli-binary-format raw-in-base64-out<\/code> option and this should work as it worked with AWS cli v1.18:<\/p>\n<pre><code>aws sagemaker-runtime invoke-endpoint \\\n&gt;     --region &lt;aws-region&gt; \\\n&gt;     --endpoint-name &lt;you-endpoint-name&gt; \\\n&gt;     --cli-binary-format raw-in-base64-out \\\n&gt;     --body $(seq 784 | xargs echo | sed 's\/ \/,\/g') \\\n&gt;     &gt;(cat) \\\n&gt;     --content-type text\/csv &gt; \/dev\/null\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1606454756257,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1612457107053,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64925353",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53809556,
        "Question_title":"Load csv into S3 from local",
        "Question_body":"<p>I am exploring AWS sagemaker for ML. I have created a bucket:<\/p>\n\n<pre><code>bucket_name = 'test-bucket' \ns3 = boto3.resource('s3')\ntry:\n   if  my_region == 'us-east-1':\n      s3.create_bucket(Bucket=bucket_name)\n   else: \n      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\nprint('S3 bucket created successfully')\nexcept Exception as e:\n    print('S3 error: ',e)\n<\/code><\/pre>\n\n<p>I have a csv in my local and I want to load that into the bucket I created.<\/p>\n\n<p>All the links I have referred  have directions to load it from a link and unzip it. Is there a way to load the data into the bucket from the local.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1545025564273,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":1703,
        "Owner_creation_time":1455496483357,
        "Owner_last_access_time":1642546968190,
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":3912,
        "Owner_up_votes":135,
        "Owner_down_votes":47,
        "Owner_views":311,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If you are using Amazon SageMaker you can use the SageMaker python library that is implementing the most useful commands for data scientists, including the upload of files to S3. It is already installed on your SageMaker notebook instance by default. <\/p>\n\n<pre><code>import sagemaker\nsess = sagemaker.Session()\n\n# Uploading the local file to S3\nsess.upload_data(path='local-file.txt', bucket=bucket_name, key_prefix='input')    \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1545428602017,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53809556",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66915920,
        "Question_title":"Using pytorch cuda in AWS sagemaker notebook instance",
        "Question_body":"<p>In colab, whenever we need GPU, we simply click <code>change runtime type<\/code> and change hardware accelarator to <code>GPU<\/code><\/p>\n<p>and cuda becomes available, <code>torch.cuda.is_available()<\/code> is <code>True<\/code><\/p>\n<p>How to do this is AWS sagemaker, i.e. turning on cuda.\nI am new to AWS and trying to train model using pytorch in aws sagemaker, where Pytorch code is first tested in colab environment.<\/p>\n<p>my sagemaker notebook insatnce is <code>ml.t2.medium<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617348836210,
        "Question_score":0,
        "Question_tags":"amazon-ec2|pytorch|amazon-sagemaker",
        "Question_view_count":1608,
        "Owner_creation_time":1567880532003,
        "Owner_last_access_time":1661706476487,
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":137,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":100,
        "Question_last_edit_time":1617349698827,
        "Answer_body":"<p>Using AWS Sagemaker you don't need to worry about the GPU, you simply select an instance type with GPU ans Sagemaker will use it. Specifically <code>ml.t2.medium<\/code> doesn't have a GPU but it's anyway not the right way to train a model.\nBasically you have 2 canonical ways to use Sagemaker (look at the documentation and examples please), the first is to use a notebook with a limited computing resource to spin up a training job using a prebuilt image, in that case when you call the estimator you simply specify what <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/\" rel=\"nofollow noreferrer\">instance type<\/a> you want (you'll choose one with GPU, looking at the costs). The second way is to use your own container, push it to ECR and launch a training job from the console, where you specify the instance type.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1617464162903,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66915920",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70565147,
        "Question_title":"AWS sagemaker datawrangler continues to be used after closing everything",
        "Question_body":"<p>I used data wrangler for maybe 3h a week ago, and I open my account today to see that Ive been charged for 6 days worth of data wrangler usage. Basically it was running in the background the whole time. The first 25h were part of free tier then I got charged for the rest of the time. I dont have any endpoints to close so whats the issue? I dont care about the costs, I know I can talk to support to get the charges reversed but they dont seem to know whats going on because they havent helped me at all.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641209554070,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":36,
        "Owner_creation_time":1535382420717,
        "Owner_last_access_time":1647779585177,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":"<p>After going over <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-wrangler-shut-down.html\" rel=\"nofollow noreferrer\">the docs<\/a>, I found that I needed to shut down the wrangler instance under Running Instances and Kernels button.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1641216477460,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1641216587633,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70565147",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55964972,
        "Question_title":"Can I pass arguments to the entrypoint of a SageMaker estimator?",
        "Question_body":"<p>I'm using the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html#sagemaker.estimator.Framework\" rel=\"noreferrer\">SageMaker python sdk<\/a> and was hoping to pass in some arguments to be used by my entrypoint, I'm not seeing how to do this.<\/p>\n\n<pre><code>from sagemaker.sklearn.estimator import SKLearn  # sagemaker python sdk\n\nentrypoint = 'entrypoint_script.py'\n\nsklearn = SKLearn(entry_point=entrypoint,  # &lt;-- need to pass args to this\n                  train_instance_type=instance_class,\n                  role=role,\n                  sagemaker_session=sm)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1556867880610,
        "Question_score":5,
        "Question_tags":"python|scikit-learn|data-science|amazon-sagemaker",
        "Question_view_count":2780,
        "Owner_creation_time":1343237570417,
        "Owner_last_access_time":1661366012027,
        "Owner_location":"California",
        "Owner_reputation":1325,
        "Owner_up_votes":303,
        "Owner_down_votes":10,
        "Owner_views":131,
        "Question_last_edit_time":1556899526173,
        "Answer_body":"<p>The answer is no as there is no parameter on the Estimator base class, or the fit method, that accepts arguments to pass to the entrypoint.<\/p>\n\n<p>I resolved this by passing the parameter as part of the hyperparameter dictionary. This gets passed to the entrypoint as arguments.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1556949846450,
        "Answer_score":6.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1578346552883,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55964972",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65794703,
        "Question_title":"Amazon Forecast - extra attributes in the TARGET_TIME_SERIES?",
        "Question_body":"<p>On Amazon Forecast, let\u2019s say I have a variable (Z) which is supposed to help me predict a set of target variables (Y1, Y2, Y3).<\/p>\n<p>First question is, what is the difference between:<\/p>\n<ol>\n<li>put Z as an extra attribute in the TARGET_TIME_SERIES, that is, as an extra column<\/li>\n<li>put Z as an attribute in the RELATED_TIME_SERIES<\/li>\n<\/ol>\n<p>Second question is, given that Z has just one value per day (let\u2019s say this is a stock price), how should I deal with the fact that I have 3x-repeated timestamps? Should I just repeat Z for each repeated date?<\/p>\n<p>I understand that, if I'm not training my model to predict Z, I need to provide future values for it. But this makes option 1) even it more confusing to me. In which cases should one add an extra attribute in TARGET_TIME_SERIES?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611070761497,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-forecast",
        "Question_view_count":36,
        "Owner_creation_time":1557535777467,
        "Owner_last_access_time":1663950830700,
        "Owner_location":"Stockholm, Sweden",
        "Owner_reputation":373,
        "Owner_up_votes":349,
        "Owner_down_votes":1,
        "Owner_views":41,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I got a very nice explanation of it in here, so I'll leave it as the answer:\n<a href=\"https:\/\/github.com\/aws-samples\/amazon-forecast-samples\/issues\/104#issuecomment-764896502\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-forecast-samples\/issues\/104#issuecomment-764896502<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1611263092840,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65794703",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57908395,
        "Question_title":"How can I specify content_type in a training job of XGBoost from Sagemaker in Python?",
        "Question_body":"<p>I am trying to train a model using the sagemaker library. So far, my code is the following:<\/p>\n\n<pre><code>container = get_image_uri(boto3.Session().region_name,\n                      'xgboost', \n                      repo_version='0.90-1')\n\nestimator = sagemaker.estimator.Estimator(container, \n                                          role = 'AmazonSageMaker-ExecutionRole-20190305TXXX',\n                                          train_instance_count = 1,\n                                          train_instance_type = 'ml.m4.2xlarge',\n                                          output_path = 's3:\/\/antifraud\/production\/',\n                                          hyperparameters = {'num_rounds':'400',\n                                                             'objective':'binary:logistic',\n                                                             'eval_metric':'error@0.1'})\n\ntrain_config = training_config(estimator=estimator,\n                               inputs = {'train':'s3:\/\/antifraud\/production\/train',\n                                         'validation':'s3:\/\/-antifraud\/production\/validation'})\n<\/code><\/pre>\n\n<p>And I get an error parsing the hyperparameters. This commands gives me a configuration JSON output in the console. I have been able to run a training job using boto3 with the configuration as Json, so I have figured out that the thing I am missing in my json configuration generated by my code is the content_type parameter, which should be there as follow:<\/p>\n\n<pre><code>\"InputDataConfig\": [\n    {\n        \"ChannelName\": \"train\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"S3Prefix\",\n                \"S3Uri\": \"s3:\/\/antifraud\/production\/data\/train\",\n                \"S3DataDistributionType\": \"FullyReplicated\" \n            }\n        },\n        \"ContentType\": \"text\/csv\",\n        \"CompressionType\": \"None\"\n    },\n    {\n        \"ChannelName\": \"validation\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"S3Prefix\",\n                \"S3Uri\": \"s3:\/\/antifraud\/production\/validation\",\n                \"S3DataDistributionType\": \"FullyReplicated\"\n            }\n        },\n        \"ContentType\": \"text\/csv\",\n        \"CompressionType\": \"None\"\n    }\n]\n<\/code><\/pre>\n\n<p>I have tried coding content_type = 'text\/csv' in container, estimator and train_config as parameter and also inside inputs as another key of the dictionary, with no success. How could I make this work?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1568296785460,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":644,
        "Owner_creation_time":1523298968403,
        "Owner_last_access_time":1663934452963,
        "Owner_location":null,
        "Owner_reputation":1754,
        "Owner_up_votes":396,
        "Owner_down_votes":76,
        "Owner_views":197,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I have solved it using s3_input objects:<\/p>\n\n<pre><code>s3_input_train = sagemaker.s3_input(s3_data='s3:\/\/antifraud\/production\/data\/{domain}-{product}-{today}\/train_data.csv',\ncontent_type='text\/csv')\ns3_input_validation = sagemaker.s3_input(s3_data='s3:\/\/antifraud\/production\/data\/{domain}-{product}-{today}\/validation_data.csv',\ncontent_type='text\/csv')\n\ntrain_config = training_config(estimator=estimator,\ninputs = {'train':s3_input_train,\n          'validation':s3_input_validation})\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1568804066440,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57908395",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56773989,
        "Question_title":"In Amazon SageMaker, what (if any) is the difference between an inference and prediction?",
        "Question_body":"<p>Amazon SageMaker has <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">inference pipelines<\/a> that process requests for inferences on data. It sounds as though inferences are similar (or perhaps identical) to predictions. Are there any differences between inferences and predictions? If so, what? If not, why not just call it a prediction pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1561555658777,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":42,
        "Owner_creation_time":1336973807643,
        "Owner_last_access_time":1655749162853,
        "Owner_location":"Minneapolis, MN, United States",
        "Owner_reputation":1907,
        "Owner_up_votes":692,
        "Owner_down_votes":6,
        "Owner_views":174,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Inference usually refers to applying a learned transformation to input data. That learned transformation could be something else than a prediction (eg dim reduction, clustering, entity extraction etc). So calling that process a prediction would be a bit too restrictive in my opinion<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1561569885853,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56773989",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68193708,
        "Question_title":"Unable to Define Auto Scaling for SageMaker Endpoint",
        "Question_body":"<p>I have deployed an AWS endpoint using a Docker container (I followed <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/docker-containers.html\" rel=\"nofollow noreferrer\">this<\/a>).<\/p>\n<p>Everything is working perfectly but now I need to put it in production and define an auto scaling strategy.<\/p>\n<p>I tried 2 things:<\/p>\n<ol>\n<li><p>AWS console but the auto scaling button is greyed\nout.<\/p>\n<\/li>\n<li><p>The method described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-add-code-apply.html\" rel=\"nofollow noreferrer\">here<\/a>. My endpoint name\nis <code>EmbeddingEndpoint<\/code> and my variant name is <code>SimpleVariant<\/code>. So my\nfinal command is<\/p>\n<\/li>\n<\/ol>\n<pre><code>aws application-autoscaling put-scaling-policy \\\n--policy-name scalable_policy_for_embedding \\\n--policy-type TargetTrackingScaling \\\n--resource-id endpoint\/EmbeddingEndpoint\/variant\/SimpleVariant \\\n--service-namespace sagemaker \\\n--scalable-dimension sagemaker:variant:DesiredInstanceCount \\\n--target-tracking-scaling-policy-configuration file:\/\/policy_config.json\n<\/code><\/pre>\n<p>but I get this result :<\/p>\n<pre><code>An error occurred (ObjectNotFoundException) when calling the PutScalingPolicy operation: \nNo scalable target registered for service namespace: sagemaker, resource ID: \nendpoint\/EmbeddingEndpoint\/variant\/SimpleVariant, scalable dimension: \nsagemaker:variant:DesiredInstanceCount\n<\/code><\/pre>\n<p>Does someone have another solution, or is it that I didn't set the variable well ?\nThank you in advance !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625051653240,
        "Question_score":1,
        "Question_tags":"amazon-web-services|docker|scaling|endpoint|amazon-sagemaker",
        "Question_view_count":336,
        "Owner_creation_time":1570620624977,
        "Owner_last_access_time":1663790606097,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1630503140507,
        "Answer_body":"<p>Your <code>sagemaker<\/code> service-namespace does not have any registered scaling targets. You need to first run <code>register-scalable-target<\/code> before running <code>put-scaling-policy<\/code>.<\/p>\n<pre><code>aws application-autoscaling register-scalable-target \\\n    --service-namespace sagemaker \\\n    --scalable-dimension sagemaker:variant:DesiredInstanceCount \\\n    --resource-id endpoint\/EmbeddingEndpoint\/variant\/SimpleVariant\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1630466729773,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68193708",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67380942,
        "Question_title":"Pushing docker image in AWS ECR from SageMaker Studio using AWS CLI",
        "Question_body":"<p>We can now publish Docker images to AWS ECR directly from <strong>SageMaker Studio<\/strong> using this code <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli<\/a>\nI did follow the easy installation instructions:<\/p>\n<pre><code>!pip install sagemaker-studio-image-build\nsm-docker build .\n<\/code><\/pre>\n<p>Also Trust policy and permissions have been set as described in the instructions.\nBut I'm getting the error &quot;<strong>Command did not exit successfully docker push<\/strong>&quot; at the stage where it is pushing the Docker image to AWS ECR. Any idea why? Here are the details print as output:<\/p>\n<pre><code>[Container] 2021\/05\/04 06:57:20 Running command echo Pushing the Docker image...\nPushing the Docker image...\n\n[Container] 2021\/05\/04 06:57:20 Running command docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\/$IMAGE_REPO_NAME:$IMAGE_TAG\nThe push refers to repository [752731038471.dkr.ecr.eu-central-1.amazonaws.com\/sagemaker-studio-d-tfbogtriaiml]\nAn image does not exist locally with the tag: 752731038471.dkr.ecr.eu-central-1.amazonaws.com\/sagemaker-studio-d-tfbogtriaiml\n\n[Container] 2021\/05\/04 06:57:20 Command did not exit successfully docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\/$IMAGE_REPO_NAME:$IMAGE_TAG exit status 1\n[Container] 2021\/05\/04 06:57:20 Phase complete: POST_BUILD State: FAILED\n[Container] 2021\/05\/04 06:57:20 Phase context status code: COMMAND_EXECUTION_ERROR Message: Error while executing command: docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\/$IMAGE_REPO_NAME:$IMAGE_TAG. Reason: exit status 1\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620115524227,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|amazon-ecr",
        "Question_view_count":1046,
        "Owner_creation_time":1469978721363,
        "Owner_last_access_time":1664014644413,
        "Owner_location":null,
        "Owner_reputation":415,
        "Owner_up_votes":102,
        "Owner_down_votes":1,
        "Owner_views":48,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In the Dockerfile, there was a reference to another file that was not present in the directory from where the command <code>sm-docker build .<\/code> was launched.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620378926343,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67380942",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59882941,
        "Question_title":"ValueError: no SavedModel bundles found! when trying to deploy a TF2.0 model to SageMaker",
        "Question_body":"<p>I'm trying to deploy a TF2.0 model to SageMaker. So far, I managed to train the model and save it into an S3 bucket but when I'm calling the <code>.deploy()<\/code> method, I get the following error from cloud Watch <\/p>\n\n<p><code>ValueError: no SavedModel bundles found!<\/code><\/p>\n\n<p>Here is my training script: <\/p>\n\n<pre><code>### Code to add in a tensorflow_estimator.py file\n\nimport argparse\nimport os\nimport pathlib\nimport tensorflow as tf\n\n\nif __name__ == '__main__':\n\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--learning_rate', type=float, default=0.1)\n\n    # Data, model, and output directories\n    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n    args, _ = parser.parse_known_args()\n\n    print(\"##### ARGS ##### \\n{}\".format(args))\n\n    # Get files \n    path = pathlib.Path(args.train)\n\n    # Print out folder content \n    for item in path.iterdir():\n        print(\"##### DIRECTORIES ##### \\n{}\".format(item))\n\n    # Get all images \n    all_images = list(path.glob(\"*\/*\"))\n    all_image_paths = [str(path) for path in list(path.glob(\"*\/*\"))]\n\n    # Transform images into tensors\n    def preprocess_and_load_images(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, [192, 192])\n        return image\n\n    # Apply preprocessing function\n    ds_paths = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    ds_images = ds_paths.map(preprocess_and_load_images)\n\n    # Map Labels\n    labels = []\n    for data in path.iterdir():  \n        if data.is_dir():               \n            labels += [data.name]    \n\n    labels_index = {}\n    for i,label in enumerate(labels):\n        labels_index[label]=i\n\n    print(\"##### Label Index ##### \\n{}\".format(labels_index))\n\n    all_image_labels = [labels_index[path.parent.name] for path in list(path.glob(\"*\/*\"))]\n\n    # Create a tf Dataset\n    labels_ds = tf.data.Dataset.from_tensor_slices(all_image_labels)\n\n    # Zip train and labeled dataset\n    full_ds = tf.data.Dataset.zip((ds_images, labels_ds))\n\n    # Shuffle Dataset and batch it \n    full_ds = full_ds.shuffle(len(all_images)).batch(args.batch_size)\n\n    # Create a pre-trained model \n    base_model = tf.keras.applications.InceptionV3(input_shape=(192,192,3), \n                                               include_top=False,\n                                               weights = \"imagenet\"\n                                               )\n\n    base_model.trainable = False\n    model = tf.keras.Sequential([\n                base_model,\n                tf.keras.layers.GlobalAveragePooling2D(),\n                tf.keras.layers.Dense(len(labels), activation=\"softmax\")\n            ])\n\n    initial_learning_rate = args.learning_rate\n\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=1000,\n        decay_rate=0.96,\n        staircase=True) \n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n\n    # Train the model\n    model.fit(full_ds, epochs=args.epochs)\n\n    # Save the model \n    model.save(os.path.join(args.model_dir, \"tf_model\"), save_format=\"tf\")\n\ndef model_fn(model_dir):\n    classifier = tf.keras.models.load_model(os.path.join(model_dir, \"tf_model\"))\n    return classifier\n<\/code><\/pre>\n\n<p>And here is the code that I wrote into Colab <\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='tensorflow_estimator.py', \n                          role=role,\n                          train_instance_count=1, \n                          train_instance_type='ml.m5.large',\n                          framework_version='2.0.0', \n                          sagemaker_session=sagemaker_session,\n                          output_path=s3_output_location,\n                          hyperparameters={'epochs': 1,\n                                           'batch_size': 30,\n                                           'learning_rate': 0.001},\n                          py_version='py3')\n\n\ntf_estimator.fit({\"train\":train_data})\n\nfrom sagemaker.tensorflow.serving import Model\n\nmodel = Model(model_data='s3:\/\/path\/to\/model.tar.gz', \n              role=role,\n              framework_version=\"2.0.0\",\n              sagemaker_session=sagemaker_session)\n\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m5.large')\n<\/code><\/pre>\n\n<p>I already tried to look at <a href=\"https:\/\/stackoverflow.com\/questions\/57172147\/no-savedmodel-bundles-found-on-tensorflow-hub-model-deployment-to-aws-sagemak\">this thread<\/a> but I actually don't have the problem of versions in my tar.gz file as the structure is the following : <\/p>\n\n<pre><code>\u251c\u2500\u2500 assets\n\u251c\u2500\u2500 saved_model.pb\n\u2514\u2500\u2500 variables\n    \u251c\u2500\u2500 variables.data-00000-of-00001\n    \u2514\u2500\u2500 variables.index\n<\/code><\/pre>\n\n<p>I feel I might be wrong when defining <code>model_fn()<\/code> in my training script but definitely don't what to replace that with. Would you guys have an idea? <\/p>\n\n<p>Thanks a lot for your help!  <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1579796394240,
        "Question_score":4,
        "Question_tags":"python|deployment|tensorflow2.0|amazon-sagemaker",
        "Question_view_count":943,
        "Owner_creation_time":1562055808543,
        "Owner_last_access_time":1663141286633,
        "Owner_location":null,
        "Owner_reputation":895,
        "Owner_up_votes":29,
        "Owner_down_votes":1,
        "Owner_views":53,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I actually tried to modify my training script to the following : <\/p>\n\n<pre><code>### Code to add in a tensorflow_estimator.py file\n\nimport argparse\nimport os\nimport pathlib\nimport tensorflow as tf\n\n\nif __name__ == '__main__':\n\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--learning_rate', type=float, default=0.1)\n\n    # Data, model, and output directories\n    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n    args, _ = parser.parse_known_args()\n\n    print(\"##### ARGS ##### \\n{}\".format(args))\n\n    # Get files \n    path = pathlib.Path(args.train)\n\n    # Print out folder content \n    for item in path.iterdir():\n        print(\"##### DIRECTORIES ##### \\n{}\".format(item))\n\n    # Get all images \n    all_images = list(path.glob(\"*\/*\"))\n    all_image_paths = [str(path) for path in list(path.glob(\"*\/*\"))]\n\n    # Transform images into tensors\n    def preprocess_and_load_images(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, [192, 192])\n        return image\n\n    # Apply preprocessing function\n    ds_paths = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    ds_images = ds_paths.map(preprocess_and_load_images)\n\n    # Map Labels\n    labels = []\n    for data in path.iterdir():  \n        if data.is_dir():               \n            labels += [data.name]    \n\n    labels_index = {}\n    for i,label in enumerate(labels):\n        labels_index[label]=i\n\n    print(\"##### Label Index ##### \\n{}\".format(labels_index))\n\n    all_image_labels = [labels_index[path.parent.name] for path in list(path.glob(\"*\/*\"))]\n\n    # Create a tf Dataset\n    labels_ds = tf.data.Dataset.from_tensor_slices(all_image_labels)\n\n    # Zip train and labeled dataset\n    full_ds = tf.data.Dataset.zip((ds_images, labels_ds))\n\n    # Shuffle Dataset and batch it \n    full_ds = full_ds.shuffle(len(all_images)).batch(args.batch_size)\n\n    # Create a pre-trained model \n    base_model = tf.keras.applications.InceptionV3(input_shape=(192,192,3), \n                                               include_top=False,\n                                               weights = \"imagenet\"\n                                               )\n\n    base_model.trainable = False\n    model = tf.keras.Sequential([\n                base_model,\n                tf.keras.layers.GlobalAveragePooling2D(),\n                tf.keras.layers.Dense(len(labels), activation=\"softmax\")\n            ])\n\n    initial_learning_rate = args.learning_rate\n\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=1000,\n        decay_rate=0.96,\n        staircase=True) \n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n\n    # Train the model\n    model.fit(full_ds, epochs=args.epochs)\n\n    # Save the model \n    model.save(os.path.join(args.model_dir, \"tensorflow_model\/1\"), save_format=\"tf\")\n\n<\/code><\/pre>\n\n<p>It seems that it's important to have a numerical name for your folder:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Save the model\nmodel.save(os.path.join(args.model_dir, \"tensorflow_model\/1\"), save_format=\"tf\")\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1579799847967,
        "Answer_score":4.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1590696546173,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59882941",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54629890,
        "Question_title":"Invoke aws sagemaker endpoint",
        "Question_body":"<p>I have some data in S3 and I want to create a lambda function to predict the output with my deployed aws sagemaker endpoint then I put the outputs in S3 again. Is it necessary in this case to create an api gateway like decribed in this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"noreferrer\">link<\/a> ? and in the lambda function what I have to put. I expect to put (where to find the data, how to invoke the endpoint, where to put the data) <\/p>\n\n<pre><code>import boto3\nimport io\nimport json\nimport csv\nimport os\n\n\nclient = boto3.client('s3') #low-level functional API\n\nresource = boto3.resource('s3') #high-level object-oriented API\nmy_bucket = resource.Bucket('demo-scikit-byo-iris') #subsitute this for your s3 bucket name. \n\nobj = client.get_object(Bucket='demo-scikit-byo-iris', Key='foo.csv')\nlines= obj['Body'].read().decode('utf-8').splitlines()\nreader = csv.reader(lines)\n\nimport io\nfile = io.StringIO(lines)\n\n# grab environment variables\nruntime= boto3.client('runtime.sagemaker')\n\nresponse = runtime.invoke_endpoint(\n    EndpointName= 'nilm2',\n    Body = file.getvalue(),\n    ContentType='*\/*',\n    Accept = 'Accept')\n\noutput = response['Body'].read().decode('utf-8')\n<\/code><\/pre>\n\n<p>my data is a csv file of 2 columns of floats with no headers, the problem is that lines return a list of strings(each row is an element of this list:['11.55,65.23', '55.68,69.56'...]) the invoke work well but the response is also a string: output = '65.23\\n,65.23\\n,22.56\\n,...' <\/p>\n\n<p>So how to save this output to S3 as a csv file <\/p>\n\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1549885499183,
        "Question_score":5,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":3146,
        "Owner_creation_time":1502010899810,
        "Owner_last_access_time":1573819157507,
        "Owner_location":"Tunis, Tunisia",
        "Owner_reputation":109,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If your Lambda function is scheduled, then you won't need an API Gateway. But if the predict action will be triggered by a user, by an application, for example, you will need.<\/p>\n\n<p>When you call the invoke endpoint, actually you are calling a SageMaker endpoint, which is not the same as an API Gateway endpoint. <\/p>\n\n<p>A common architecture with SageMaker is:<\/p>\n\n<ol>\n<li>API Gateway with receives a request then calls an authorizer, then\ninvoke your Lambda; <\/li>\n<li>A Lambda with does some parsing in your input data, then calls your SageMaker prediction endpoint, then, handles the result and returns to your application.<\/li>\n<\/ol>\n\n<p>By the situation you describe, I can't say if your task is some academic stuff or a production one.<\/p>\n\n<p>So, how you can save the data as a CSV file from your Lambda? <\/p>\n\n<p>I believe you can just parse the output, then just upload the file to S3. Here you will parse manually or with a lib, with boto3 you can upload the file. The output of your model depends on your implementation on SageMaker image. So, if you need the response data in another format, maybe you will need to use a <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">custom image<\/a>. I normally use a custom image, which I can define how I want to handle my data on requests\/responses.<\/p>\n\n<p>In terms of a production task, I certainly recommend you check Batch transform jobs from SageMaker. You can provide an input file (the S3 path) and also a destination file (another S3 path). The SageMaker will run the batch predictions and will persist a file with the results. Also, you won't need to deploy your model to an endpoint, when this job run, will create an instance of your endpoint, download your data to predict, do the predictions, upload the output, and shut down the instance. You only need a trained model.<\/p>\n\n<p>Here some info about Batch transform jobs:<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html<\/a><\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-batch-transform.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-batch-transform.html<\/a><\/p>\n\n<p>I hope it helps, let me know if need more info.<\/p>\n\n<p>Regards.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1550022296023,
        "Answer_score":4.0,
        "Question_favorite_count":4.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54629890",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68193944,
        "Question_title":"Set custom environment variables in AWS",
        "Question_body":"<p>I am using AWS sagemaker, I have some secret keys and access keys to access some APIs that I don't want to expose directly in code.<\/p>\n<p>What are the ways like environment variables etc., that can be used to hide these keys and I can use them securely, and how to set them.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625052591003,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":277,
        "Owner_creation_time":1567880532003,
        "Owner_last_access_time":1661706476487,
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":137,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":100,
        "Question_last_edit_time":1625058756810,
        "Answer_body":"<p>AWS System Manager (SSM) is designed to store keys and tokens securely.<\/p>\n<p>Depending on how your notebook is defined, you could <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html\" rel=\"nofollow noreferrer\">use the 'env' property<\/a> directly or in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-environment-variables\" rel=\"nofollow noreferrer\">training data<\/a>, or you could access SSM directly from sagemaker. For example this Snowflake KB article explains how to fetch auth info from ssm: <a href=\"https:\/\/community.snowflake.com\/s\/article\/Connecting-a-Jupyter-Notebook-Part-3\" rel=\"nofollow noreferrer\">https:\/\/community.snowflake.com\/s\/article\/Connecting-a-Jupyter-Notebook-Part-3<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1625064328873,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68193944",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63457857,
        "Question_title":"What is called within a sagemaker custom (training) container?",
        "Question_body":"<p>Somewhere this spring the behaviour of the sagemaker docker image changed and I cannot find the way I need to construct it now.<\/p>\n<p><strong>Directory structure<\/strong><\/p>\n<pre><code>\/src\/some\/package\n\/project1\n    \/some_entrypoint.py\n    \/some_notebook.ipynb\n\/project2\n    \/another_entrypoint.py\n    \/another_notebook.ipynb\nsetup.py\n<\/code><\/pre>\n<p><strong>Docker file<\/strong><\/p>\n<p>Note that I want to shift tensorflow version, so I changed the <code>FROM<\/code> to the latest version. This was the\nbreaking change.<\/p>\n<pre><code># Core\nFROM 763104351884.dkr.ecr.eu-west-1.amazonaws.com\/tensorflow-training:2.3.0-cpu-py37-ubuntu18.04\n\nCOPY . \/opt\/ml\/code\/all\/\nRUN pip install \/opt\/ml\/code\/all\/\n\nWORKDIR &quot;\/opt\/ml\/code&quot;\n<\/code><\/pre>\n<p><strong>Python code<\/strong><\/p>\n<p>This code should start the entrypoints, for example here we have the code of some_notebook.ipynb. I tried all possible combinations of working directory + source_dir (None, '.', or '..'), entry_point (with or without \/), dependencies ('src')...<\/p>\n<ul>\n<li>if setup is present it tries to call my project as a module (python -m some_entrypoint)<\/li>\n<li>if not, it often is not able to find my entrypoint. Which I don't understand because the TensorFlow is supposed to add it to the container, isn't it?<\/li>\n<\/ul>\n<pre><code>estimator = TensorFlow(\n   entry_point='some_entrypoint.py', \n   image_name='ECR.dkr.ecr.eu-west-1.amazonaws.com\/overall-project\/sagemaker-training:latest',\n   source_dir='.',\n#    dependencies=['..\/src\/'],\n   script_mode=True,\n\n   train_instance_type='ml.m5.4xlarge',\n   train_instance_count=1,\n   train_max_run=60*60,  # seconds * minutes\n   train_max_wait=60*60,  # seconds * minutes. Must be &gt;= train_max_run\n   hyperparameters=hyperparameters,\n   metric_definitions=metrics,\n   role=role,\n   framework_version='2.0.0',\n   py_version='py3',\n  )\nestimator.fit({\n    'training': f&quot;s3:\/\/some-data\/&quot;}\n#   , wait=False\n)\n<\/code><\/pre>\n<p>Ideally I would want to understand the logic within: what is called given what settings?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597694559337,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":61,
        "Owner_creation_time":1484838464573,
        "Owner_last_access_time":1663858511743,
        "Owner_location":"Amsterdam, Nederland",
        "Owner_reputation":3937,
        "Owner_up_votes":672,
        "Owner_down_votes":27,
        "Owner_views":387,
        "Question_last_edit_time":1597697877660,
        "Answer_body":"<p>when the training container runs, your entry_point script will be executed.<\/p>\n<p>Since your notebook file and entry_point script are under the same directory, your <code>source_dir<\/code> should just be &quot;.&quot;<\/p>\n<p>Does your entry_point script import any modules that are not installed by the tensorflow training container by default? Also could you share your stacktrace of the error?<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1600272249597,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63457857",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72712449,
        "Question_title":"SageMaker Pipeline - Processing step for ImageClassification model",
        "Question_body":"<p>I'm trying to solve ImageClassification task. I have prepared a code to train, evaluate and deploy tensorflow model in SageMaker Notebook. I'm new with SageMaker and SageMaker Pipeline too. Currently, I'm trying to split my code and create SageMaker pipeline to solve Image Classification task.\nIn reference to AWS documentation there is <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-and-manage-steps.html#step-type-processing\" rel=\"nofollow noreferrer\">Processing steps<\/a>. I have a code which read data from S3 and use <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator\" rel=\"nofollow noreferrer\">ImageGenerator<\/a> to generate augmented images on the fly while tensorflow model is still in the training stage.<\/p>\n<p>I don't find anything of how I can use <code>ImageGenerator<\/code> inside of Processing step in SageMaker Pipeline.<\/p>\n<p>My Code of <code>ImageGenerator<\/code>:<\/p>\n<pre><code>def load_data(mode):\n    if mode == 'TRAIN':\n        datagen = ImageDataGenerator(\n            rescale=1. \/ 255,\n            rotation_range = 0.5,\n            shear_range=0.2,\n            zoom_range=0.2,\n            width_shift_range = 0.2,\n            height_shift_range = 0.2,\n            fill_mode = 'nearest',\n            horizontal_flip=True)\n    else:\n        datagen = ImageDataGenerator(rescale=1. \/ 255)\n    return datagen\n\n\ndef get_flow_from_directory(datagen,\n                            data_dir,\n                            batch_size,\n                            shuffle=True):\n    assert os.path.exists(data_dir), (&quot;Unable to find images resources for input&quot;)\n    generator = datagen.flow_from_directory(data_dir,\n                                            class_mode = &quot;categorical&quot;,\n                                            target_size=(HEIGHT, WIDTH),\n                                            batch_size=batch_size,\n                                            shuffle=shuffle\n                                            )\n    print('Labels are: ', generator.class_indices)\n    return generator\n<\/code><\/pre>\n<p>Question is - does it possible to use <code>ImageGenerator<\/code> inside of <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-and-manage-steps.html#step-type-processing\" rel=\"nofollow noreferrer\">Processing step<\/a> of SageMaker Pipeline?\nI'd appreciate for any ideas, Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655888089677,
        "Question_score":0,
        "Question_tags":"tensorflow|keras|amazon-sagemaker|image-preprocessing",
        "Question_view_count":68,
        "Owner_creation_time":1470228490790,
        "Owner_last_access_time":1663345526560,
        "Owner_location":null,
        "Owner_reputation":504,
        "Owner_up_votes":758,
        "Owner_down_votes":3,
        "Owner_views":82,
        "Question_last_edit_time":null,
        "Answer_body":"<p>So, <code>ImageGenerator<\/code> and <code>flow_from_directory<\/code> I continue use inside of Training step. Processing step I skip at all, just use Training, Evaluating and Register model.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1656099367803,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72712449",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53533434,
        "Question_title":"AWS Sagemaker - Blazingtext BatchTransform no output",
        "Question_body":"<p>I have trained a blazingText model and followed this guide.\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html<\/a><\/p>\n\n<p>\"Sample JSON request\" The Invoke end point is working perfectly. So I switched to,\nBatch Transform Job with \"content-type: application\/jsonlines\" and created a file in S3 with the following format data:<\/p>\n\n<pre><code>{\"source\": \"source_0\"}\n<\/code><\/pre>\n\n<p>The job ran success. But the output did not sent to S3. Also In the cloud logs,<\/p>\n\n<pre><code>\" [79] [INFO] Booting worker with pid: 79\"\n<\/code><\/pre>\n\n<p>This is is the last response. Did anyone know what went wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1543474625430,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":317,
        "Owner_creation_time":1433761344910,
        "Owner_last_access_time":1664023707623,
        "Owner_location":"Aruppukkottai, India",
        "Owner_reputation":802,
        "Owner_up_votes":556,
        "Owner_down_votes":6,
        "Owner_views":151,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I have found the issue. The batchtransform select the folder as input and the s3 source should be S3Prefix instead of manifest.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1543477871070,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53533434",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54671841,
        "Question_title":"Sagemaker endpoint(with VPC) url accessible from internet",
        "Question_body":"<p>I have created a model with VPC, Private subnet, and appropriate security group. The endpoint URL can, however, be reached through the internet though failing due to the lack of security token<\/p>\n\n<p>Things I need clarification on now are<\/p>\n\n<ol>\n<li>Is there a way to avoid the URL being accessible from the internet<\/li>\n<li>Are we not charged for requests failed on AUTH(like for API Gateway)<\/li>\n<li>Does that make our deployment vulnerable to any attacks<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1550066008687,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":679,
        "Owner_creation_time":1320419229500,
        "Owner_last_access_time":1655464612793,
        "Owner_location":null,
        "Owner_reputation":4924,
        "Owner_up_votes":236,
        "Owner_down_votes":14,
        "Owner_views":358,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You are not hitting your endpoint, but the endpoint of AWS SageMaker runtime. This endpoint is checking all the permissions to access your hosted model, and only if the credentials and requirements are met, the request is forwarded to your instances and models. <\/p>\n\n<p>Therefore, you can't prevent this URL from being accessible from the Internet, but at the same time, you don't need to protect it or pay for it. AWS has a high level of security on these endpoints, and I don't think that you have a more secure way to protect these endpoints. <\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1550250221603,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54671841",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60036916,
        "Question_title":"Sagemaker lifecycle configuration for installing pandas not working",
        "Question_body":"<p>I am trying to update pandas within a lifecycle configuration, and following the example of AWS I have the next code:<\/p>\n\n<pre><code>#!\/bin\/bash\n\nset -e\n\n# OVERVIEW\n# This script installs a single pip package in a single SageMaker conda environments.\n\nsudo -u ec2-user -i &lt;&lt;EOF\n# PARAMETERS\nPACKAGE=pandas\nENVIRONMENT=python3\nsource \/home\/ec2-user\/anaconda3\/bin\/activate \"$ENVIRONMENT\"\npip install --upgrade \"$PACKAGE\"==0.25.3\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\nEOF\n<\/code><\/pre>\n\n<p>Then I attach it to a notebook and when I enter the notebook and open a notebook file, I see that pandas have not been updated. Using <code>!pip show pandas<\/code> I get:<\/p>\n\n<pre><code>Name: pandas\nVersion: 0.24.2\nSummary: Powerful data structures for data analysis, time series, and statistics\nHome-page: http:\/\/pandas.pydata.org\nAuthor: None\nAuthor-email: None\nLicense: BSD\nLocation: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\nRequires: pytz, python-dateutil, numpy\nRequired-by: sparkmagic, seaborn, odo, hdijupyterutils, autovizwidget\n<\/code><\/pre>\n\n<p>So we can see that I am indeed in the python3 env although the version is 0.24. <\/p>\n\n<p>However, the log in cloudwatch shows that it has been installed:<\/p>\n\n<pre><code>Collecting pandas==0.25.3 Downloading https:\/\/files.pythonhosted.org\/packages\/52\/3f\/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d\/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in .\/anaconda3\/lib\/python3.6\/site-packages (from pandas==0.25.3) (2018.4)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: python-dateutil&gt;=2.6.1 in .\/anaconda3\/lib\/python3.6\/site-packages (from pandas==0.25.3) (2.7.3)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: numpy&gt;=1.13.3 in .\/anaconda3\/lib\/python3.6\/site-packages (from pandas==0.25.3) (1.16.4)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: six&gt;=1.5 in .\/anaconda3\/lib\/python3.6\/site-packages (from python-dateutil&gt;=2.6.1-&gt;pandas==0.25.3) (1.13.0)\n2020-02-03T12:33:09.065+01:00\nInstalling collected packages: pandas Found existing installation: pandas 0.24.2 Uninstalling pandas-0.24.2: Successfully uninstalled pandas-0.24.2\n2020-02-03T12:33:12.066+01:00\nSuccessfully installed pandas-0.25.3\n<\/code><\/pre>\n\n<p>What could be the problem? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1580724156233,
        "Question_score":2,
        "Question_tags":"python|pandas|amazon-web-services|pip|amazon-sagemaker",
        "Question_view_count":1493,
        "Owner_creation_time":1523298968403,
        "Owner_last_access_time":1663934452963,
        "Owner_location":null,
        "Owner_reputation":1754,
        "Owner_up_votes":396,
        "Owner_down_votes":76,
        "Owner_views":197,
        "Question_last_edit_time":1580746498477,
        "Answer_body":"<p>if you want to install the  packages only in for the python3 environment, use the following script in your <strong>Create Sagemaker Lifecycle<\/strong> configurations. <\/p>\n\n<pre><code>#!\/bin\/bash\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\n# This will affect only the Jupyter kernel called \"conda_python3\".\nsource activate python3\n\n# Replace myPackage with the name of the package you want to install.\npip install pandas==0.25.3\n# You can also perform \"conda install\" here as well.\nsource deactivate\nEOF\n<\/code><\/pre>\n\n<p>Reference : \"<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">Lifecycle Configuration Best Practices<\/a>\" <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1581352236820,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60036916",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73611956,
        "Question_title":"Remove JSON object via AWS Update* API to prevent Terraform from recreating the resource",
        "Question_body":"<p>I have an AWS SageMaker domain in my account created via Terraform. The resource was modified outside of Terraform. The modification was the equivalent of the following:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>aws sagemaker update-domain --domain-id d-domainid123 --default-user-settings '{&quot;KernelGatewayAppSettings&quot;: { &quot;CustomImages&quot;: [ { ... } ] } }'\n<\/code><\/pre>\n<p>Ever since, all <code>terraform plan<\/code> operations want to replace the AWS SageMaker domain:<\/p>\n<pre><code>  # module.main.aws_sagemaker_domain.default must be replaced\n-\/+ resource &quot;aws_sagemaker_domain&quot; &quot;default&quot; {\n      ~ arn                                            = &quot;arn:aws:sagemaker:eu-central-1:000111222333:domain\/d-domainid123&quot; -&gt; (known after apply)\n      ...\n        # (6 unchanged attributes hidden)\n      ~ default_user_settings {\n            # (2 unchanged attributes hidden)\n          - kernel_gateway_app_settings { # forces replacement\n               - custom_images = [ ... ]\n            }\n        }\n    }\n<\/code><\/pre>\n<p>My goal is to reconcile the situation without Terraform or me needing to create a new domain. I can't modify the Terraform sources to match the state of the SageMaker domain because that would force the recreation of domains in other accounts provisioned from the same Terraform source code.<\/p>\n<p><strong>I want to issue an <code>aws<\/code> CLI command that updates the domain and removes the <code>&quot;KernelGatewayAppSettings&quot;: { ... }<\/code> key completely from the <code>&quot;DefaultUserSettings&quot;<\/code> of the SageMaker domain. Is there a way to do this?<\/strong><\/p>\n<p>I tried the following, but the empty object is still there, so they did not work.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>aws sagemaker update-domain --domain-id d-domainid123 --default-user-settings '{&quot;KernelGatewayAppSettings&quot;: {} }'\naws sagemaker update-domain --domain-id d-domainid123 --default-user-settings '{&quot;KernelGatewayAppSettings&quot;: null }'\n\n# Still:\naws sagemaker describe-domain --domain-id d-domainid123\n{\n    &quot;DomainArn&quot;: ...,\n    &quot;DomainId&quot;: ...,\n    ...\n    &quot;DefaultUserSettings&quot;: {\n        &quot;ExecutionRole&quot;: &quot;arn:aws:iam::0001112233444:role\/SageMakerStudioExecutionRole&quot;,\n        &quot;SecurityGroups&quot;: [\n            &quot;...&quot;\n        ],\n        &quot;KernelGatewayAppSettings&quot;: {\n            &quot;CustomImages&quot;: []\n        }\n    },\n    ...\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662393203630,
        "Question_score":0,
        "Question_tags":"amazon-web-services|terraform|aws-cli|amazon-sagemaker",
        "Question_view_count":32,
        "Owner_creation_time":1219760368600,
        "Owner_last_access_time":1663933250613,
        "Owner_location":"Miskolc, Hungary",
        "Owner_reputation":3743,
        "Owner_up_votes":12,
        "Owner_down_votes":5,
        "Owner_views":178,
        "Question_last_edit_time":1662455073637,
        "Answer_body":"<p>One option you have is to use the <a href=\"https:\/\/www.terraform.io\/language\/meta-arguments\/lifecycle\" rel=\"nofollow noreferrer\">lifecycle meta argument<\/a> to ignore out-of-band changes to the resource.<\/p>\n<pre><code>  lifecycle {\n    ignore_changes = [\n      default_user_settings\n    ]\n  }\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1662394449850,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73611956",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73646830,
        "Question_title":"What inference can be made out of Baseline drift distance in data quality monitoring sagemaker?",
        "Question_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/GU4v0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/GU4v0.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>How is the above distance calculated and can I set my own threshold?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662629736510,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":55,
        "Owner_creation_time":1660119311500,
        "Owner_last_access_time":1664029159210,
        "Owner_location":null,
        "Owner_reputation":15,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1663220877290,
        "Answer_body":"<p>You can find information on how the distributions are compared here (see <code>distribution_constraints<\/code> in the table):<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-byoc-constraints.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-byoc-constraints.html<\/a><\/p>\n<p>You can change the threshold in the constraint file to what you would like.<\/p>\n<p>The baseline computes baseline schema constraints and statistics for each feature using Deequ. If youwould like more details you can take a look at the implementation here:<\/p>\n<p><a href=\"https:\/\/github.com\/awslabs\/deequ\/blob\/master\/src\/main\/scala\/com\/amazon\/deequ\/analyzers\/Distance.scala\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/deequ\/blob\/master\/src\/main\/scala\/com\/amazon\/deequ\/analyzers\/Distance.scala<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1662747458550,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73646830",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63500377,
        "Question_title":"memory issues for sparse one hot encoded features",
        "Question_body":"<p>I want to create sparse matrix for one hot encoded features from data frame <code>df<\/code>. But I am getting memory issue for code given below. Shape of <code>sparse_onehot<\/code> is  (450138, 1508)<\/p>\n<pre><code>sp_features = ['id', 'video_id', 'genre']\nsparse_onehot = pd.get_dummies(df[sp_features], columns = sp_features)\nimport scipy\nX = scipy.sparse.csr_matrix(sparse_onehot.values)\n<\/code><\/pre>\n<p>I get memory error as shown below.<\/p>\n<pre><code>MemoryError: Unable to allocate 647. MiB for an array with shape (1508, 450138) and data type uint8\n<\/code><\/pre>\n<p>I have tried <code>scipy.sparse.lil_matrix<\/code> and get same error as above.<\/p>\n<p>Is there any efficient way of handling this?\nThanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1597908911857,
        "Question_score":2,
        "Question_tags":"python-3.x|pandas|scipy|sparse-matrix|amazon-sagemaker",
        "Question_view_count":97,
        "Owner_creation_time":1487135761367,
        "Owner_last_access_time":1663945128510,
        "Owner_location":null,
        "Owner_reputation":911,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":91,
        "Question_last_edit_time":1597911320857,
        "Answer_body":"<p>Try setting to <code>True<\/code> the <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html\" rel=\"nofollow noreferrer\"><code>sparse<\/code> parameter<\/a>:<\/p>\n<blockquote>\n<p>sparsebool, default False\nWhether the dummy-encoded columns should be backed by a SparseArray (True) or a regular NumPy array (False).<\/p>\n<\/blockquote>\n<pre><code>sparse_onehot = pd.get_dummies(df[sp_features], columns = sp_features, sparse = True)\n<\/code><\/pre>\n<p>This will use a much more memory efficient (but somewhat slower) representation than the default one.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1597911539563,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63500377",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51110274,
        "Question_title":"Can I use AWS Sagemaker without S3",
        "Question_body":"<p>If I am not using the notebook on AWS but instead just the Sagemaker CLI and want to train a model, can I specify a local path to read from and write to?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1530312595250,
        "Question_score":1,
        "Question_tags":"amazon-s3|amazon-sagemaker",
        "Question_view_count":826,
        "Owner_creation_time":1528500562963,
        "Owner_last_access_time":1635459100793,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If you use local mode with the SageMaker Python SDK, you can train using local data:<\/p>\n\n<pre><code>from sagemaker.mxnet import MXNet\n\nmxnet_estimator = MXNet('train.py',\n                        train_instance_type='local',\n                        train_instance_count=1)\n\nmxnet_estimator.fit('file:\/\/\/tmp\/my_training_data')\n<\/code><\/pre>\n\n<p>However, this only works if you are training a model locally, not on SageMaker. If you want to train on SageMaker, then yes, you do need to use S3.<\/p>\n\n<p>For more about local mode: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#local-mode\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#local-mode<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1530570724340,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51110274",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67186515,
        "Question_title":"Installing modules inside python .py file",
        "Question_body":"<p>I am deploying a custom pytorch model on AWS sagemaker, Following <a href=\"https:\/\/github.com\/abiodunjames\/MachineLearning\/blob\/master\/DeployYourModelToSageMaker\/inference.py\" rel=\"nofollow noreferrer\">this<\/a> tutorial.\nIn my case I have few dependencies to install some modules.<\/p>\n<p>I need pycocotools in my inference.py script. I can easily install pycocotool inside a separate notebook using this bash command,<\/p>\n<p><code>%%bash<\/code><\/p>\n<p><code>pip -g install pycocotools<\/code><\/p>\n<p>But when I create my endpoint for deployment, I get this error that pycocotools in not defined.\nI need pycocotools inside my inference.py script. How I can install this inside a .py file<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1618953881933,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":167,
        "Owner_creation_time":1567880532003,
        "Owner_last_access_time":1661706476487,
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":137,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":100,
        "Question_last_edit_time":1618961452447,
        "Answer_body":"<p>At the beginning of inference.py add these lines:<\/p>\n<pre><code>from subprocess import check_call, run, CalledProcessError\nimport sys\nimport os\n\n# Since it is likely that you're going to run inference.py multiple times, this avoids reinstalling the same package:\nif not os.environ.get(&quot;INSTALL_SUCCESS&quot;):\n    \n    try:\n        check_call(\n        [ sys.executable, &quot;pip&quot;, &quot;install&quot;, &quot;pycocotools&quot;,]\n        )\n    except CalledProcessError:\n        run(\n        [&quot;pip&quot;, &quot;install&quot;, &quot;pycocotools&quot;,]\n        )\n    os.environ[&quot;INSTALL_SUCCESS&quot;] = &quot;True&quot;\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1618965088403,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1619882971870,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67186515",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54224934,
        "Question_title":"SageMaker TensorFlow serving stack comparisons",
        "Question_body":"<p>SageMaker seems to give examples of using two different serving stacks for serving custom docker images:<\/p>\n\n<ol>\n<li>NGINX + Gunicorn + Flask<\/li>\n<li>NGINX + TensorFlow Serving<\/li>\n<\/ol>\n\n<p>Could someone explain to me at a very high level (I have very little knowledge of network engineering) what responsibilities these different components have? And since the second stack has only two components instead of one, can I rightly assume that TensorFlow Serving does the job (whatever that may be) of both Gunicorn and Flask? <\/p>\n\n<p>Lastly, I've read that it's possible to use Flask and TensorFlow serving at the same time. Would this then be NGINX -> Gunicorn -> Flask -> TensorFlow Serving? And what are there advantages of this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1547670814743,
        "Question_score":2,
        "Question_tags":"nginx|networking|tensorflow-serving|amazon-sagemaker|serving",
        "Question_view_count":741,
        "Owner_creation_time":1361339272693,
        "Owner_last_access_time":1663965928400,
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Question_last_edit_time":1547802223467,
        "Answer_body":"<p>I'll try to answer your question on a high level. Disclaimer: I'm not at an expert across the full stack of what you describe, and I would welcome corrections or additions from people who are. <\/p>\n\n<p>I'll go over the different components from bottom to top:<\/p>\n\n<p><strong>TensorFlow Serving<\/strong> is a library for deploying and hosting TensorFlow models as model servers that accept requests with input data and return model predictions. The idea is to train models with TensorFlow, export them to the SavedModel format and serve them with TF Serving. You can set up a TF Server to accept requests via HTTP and\/or RPC. One advantage of RPC is that the request message is compressed, which can be useful when sending large payloads, for instance with image data.<\/p>\n\n<p><strong>Flask<\/strong> is a python framework for writing web applications. It's much more general-purpose than TF Serving and is widely used to build web services, for instance in microservice architectures. <\/p>\n\n<p>Now, the combination of Flask and TensorFlow serving should make sense. You could write a Flask web application that exposes an API to the user and calls a TF model hosted with TF Serving under the hood. The user uses the API to transmit some data (<strong>1<\/strong>), the Flask app perhaps transform the data (for example, wrap it in numpy arrays), calls the TF Server to get a model prediction (<strong>2<\/strong>)(<strong>3<\/strong>), perhaps transforms the prediction (for example convert a predicted probability that is larger than 0.5 to a class label of 1), and returns the prediction to the user (<strong>4<\/strong>). You could visualize this as follows:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/67EXW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/67EXW.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Gunicorn<\/strong> is a Web Server Gateway Interface (WSGI) that is commonly used to host Flask applications in production systems. As the name says, it's the interface between a web server and a web application. When you are developing a Flask app, you can run it locally to test it. In production, gunicorn will run the app for you.<\/p>\n\n<p>TF Serving will host your model as a functional application. Therefore, you do not need gunicorn to run the TF Server application for you. <\/p>\n\n<p><strong>Nginx<\/strong> is the actual web server, which will host your application, handle requests from the outside and pass them to the application server (gunicorn). Nginx cannot talk directly to Flask applications, which is why gunicorn is there. <\/p>\n\n<p><a href=\"https:\/\/serverfault.com\/questions\/331256\/why-do-i-need-nginx-and-something-like-gunicorn\">This answer<\/a> might be helpful as well. <\/p>\n\n<p>Finally, if you are working on a cloud platform, the web server part will probably be handled for you, so you will either need to write the Flask app and host it with gunicorn, or setup the TF Serving server. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1547801639500,
        "Answer_score":2.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":1547816279183,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54224934",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55645119,
        "Question_title":"How to remotely connect to GCP ML Engine\/AWS Sagemaker managed notebooks?",
        "Question_body":"<p>GCP has finally released managed Jupyter notebooks.  I would like to be able to interact with the notebook locally by connecting to it.  Ie. i use PyCharm to connect to the externaly configured jupyter notebbok server by passing its URL &amp; token param.<\/p>\n\n<p>Question also applies to AWS Sagemaker notebooks.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1555047546513,
        "Question_score":8,
        "Question_tags":"amazon-web-services|google-cloud-platform|google-cloud-ml|amazon-sagemaker|gcp-ai-platform-notebook",
        "Question_view_count":3701,
        "Owner_creation_time":1298837928100,
        "Owner_last_access_time":1663978463180,
        "Owner_location":"Los Angeles, CA",
        "Owner_reputation":1367,
        "Owner_up_votes":655,
        "Owner_down_votes":16,
        "Owner_views":243,
        "Question_last_edit_time":1561940059343,
        "Answer_body":"<p>On AWS, you can use AWS Glue to create a <a href=\"https:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/dev-endpoint.html\" rel=\"nofollow noreferrer\">developer endpoint<\/a>, and then you create the Sagemaker notebook from there. A developer endpoint gives you access to connect to your python or Scala spark REPL via ssh, and it also allows you to tunnel the connection and access from any other tool, including PyCharm.<\/p>\n\n<p>For PyCharm professional we have even <a href=\"https:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/dev-endpoint-tutorial-pycharm.html\" rel=\"nofollow noreferrer\">tighter integration<\/a>, allowing you to SFTP files and debug remotely.<\/p>\n\n<p>And if you need to install any dependencies on the notebook, apart from doing it directly on the notebook, you can always choose <code>new&gt;terminal<\/code> and you will have a connection to that machine directly from your jupyter environment where you can install <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\" rel=\"nofollow noreferrer\">anything you want<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1555057761750,
        "Answer_score":3.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55645119",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73133746,
        "Question_title":"Fb-Prophet, Apache Spark in Colab and AWS SageMaker\/ Lambda",
        "Question_body":"<p>I am using <code>Google-Colab<\/code> for creating a model by using FbProphet and i am try to use Apache Spark in the <code>Google-Colab<\/code> itself. Now can i upload this <code>Google-colab<\/code> notebook in <code>aws Sagemaker\/Lambda<\/code> for free <code>(without charge for Apache Spark and only charge for AWS SageMaker)<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658906440657,
        "Question_score":1,
        "Question_tags":"apache-spark|google-colaboratory|amazon-sagemaker|facebook-prophet",
        "Question_view_count":51,
        "Owner_creation_time":1658906023853,
        "Owner_last_access_time":1663921457750,
        "Owner_location":null,
        "Owner_reputation":152,
        "Owner_up_votes":15,
        "Owner_down_votes":1,
        "Owner_views":11,
        "Question_last_edit_time":1660220920907,
        "Answer_body":"<p>In short, You can upload the notebook without any issue into SageMaker. Few things to keep in mind<\/p>\n<ol>\n<li>If you are using the pyspark library in colab and running spark locally,  you should be able to do the same by installing necessary pyspark libs in Sagemaker studio kernels. Here you will only pay for the underlying compute for the notebook instance. If you are experimenting then I would recommend you to use <a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a> to create a free account and try things out.<\/li>\n<li>If you had a separate spark cluster setup then you may need a similar setup in AWS using EMR so that you can connect to the cluster to execute the job.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1658964743500,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73133746",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56046428,
        "Question_title":"What is \"[0]#011train-merror:0.17074#011validation-merror:0.1664\" error when running xgb_model.fit() in AWS Sagemaker?",
        "Question_body":"<p>I'm running through the official sagemaker tutorial <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>And although training completes, I'm getting errors like below periodically during training, <code>xgb_model.fit(inputs=data_channels,  logs=True)<\/code>.<\/p>\n\n<p>I have no experience with xgboost or sagemaker at this point.<\/p>\n\n<pre><code>[0]#011train-merror:0.17074#011validation-merror:0.1664\n<\/code><\/pre>\n\n<p><strong>Full logs:<\/strong><\/p>\n\n<pre><code>2019-05-08 17:04:32 Starting - Starting the training job...\n2019-05-08 17:04:33 Starting - Launching requested ML instances.........\n2019-05-08 17:06:10 Starting - Preparing the instances for training......\n2019-05-08 17:07:06 Downloading - Downloading input data...\n2019-05-08 17:07:50 Training - Training image download completed. Training in progress.\nArguments: train\n[2019-05-08:17:07:51:INFO] Running standalone xgboost training.\n[2019-05-08:17:07:51:INFO] File size need to be processed in the node: 1122.95mb. Available memory size in the node: 152390.7mb\n[2019-05-08:17:07:51:INFO] Determined delimiter of CSV input is ','\n[17:07:51] S3DistributionType set as FullyReplicated\n[17:07:55] 50000x784 matrix with 39200000 entries loaded from \/opt\/ml\/input\/data\/train?format=csv&amp;label_column=0&amp;delimiter=,\n[2019-05-08:17:07:55:INFO] Determined delimiter of CSV input is ','\n[17:07:55] S3DistributionType set as FullyReplicated\n[17:07:56] 10000x784 matrix with 7840000 entries loaded from \/opt\/ml\/input\/data\/validation?format=csv&amp;label_column=0&amp;delimiter=,\n[17:07:56] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 8 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 8 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n[0]#011train-merror:0.17074#011validation-merror:0.1664\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 14 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 8 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 12 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 12 pruned nodes, max_depth=5\n[1]#011train-merror:0.12624#011validation-merror:0.1273\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 10 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 20 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 2 pruned nodes, max_depth=5\n[2]#011train-merror:0.11272#011validation-merror:0.1143\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 8 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 20 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 8 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 12 pruned nodes, max_depth=5\n[3]#011train-merror:0.10072#011validation-merror:0.1052\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 12 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 22 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 4 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 8 pruned nodes, max_depth=5\n[4]#011train-merror:0.09216#011validation-merror:0.097\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 12 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 22 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 4 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 10 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 10 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 14 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 8 pruned nodes, max_depth=5\n[5]#011train-merror:0.08544#011validation-merror:0.0904\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 12 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 14 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 8 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 2 pruned nodes, max_depth=5\n\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 4 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 16 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 12 pruned nodes, max_depth=5\n[6]#011train-merror:0.08064#011validation-merror:0.0864\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 8 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 14 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 4 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 8 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 14 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 4 pruned nodes, max_depth=5\n[7]#011train-merror:0.0769#011validation-merror:0.0821\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 16 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 20 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 4 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 10 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 4 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 8 pruned nodes, max_depth=5\n[8]#011train-merror:0.0731#011validation-merror:0.0809\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 12 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 24 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 10 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 8 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 8 pruned nodes, max_depth=5\n[17:08:04] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:04] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 14 pruned nodes, max_depth=5\n[17:08:04] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:04] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 6 pruned nodes, max_depth=5\n[9]#011train-merror:0.06942#011validation-merror:0.0773\n\n2019-05-08 17:08:12 Uploading - Uploading generated training model\n2019-05-08 17:08:12 Completed - Training job completed\nBillable seconds: 66\n\n<\/code><\/pre>\n\n<p>Is this something I should be concerned about?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1557337671520,
        "Question_score":0,
        "Question_tags":"python|xgboost|amazon-sagemaker",
        "Question_view_count":302,
        "Owner_creation_time":1431872955547,
        "Owner_last_access_time":1663095099033,
        "Owner_location":"Unknown",
        "Owner_reputation":9923,
        "Owner_up_votes":288,
        "Owner_down_votes":0,
        "Owner_views":530,
        "Question_last_edit_time":null,
        "Answer_body":"<p>No need to worry. These are not errors in your code. These are info messages that are calculating the error of the model on the training data (train-error) and on the validation data (validation-error), and these values should get smaller as the training progress. <\/p>\n\n<p>In time, these values will be more meaningful for you. You will be able to compare different algorithms and hyper-parameters based on which is the smaller error, or you will be able to see that your model is overfitting, when the error values of the training is very different from the validation error. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1557581566207,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56046428",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56666667,
        "Question_title":"Clienterror: An error occured when calling the CreateModel operation",
        "Question_body":"<p>I want to deploy sklearn model in sagemaker. I created a training script.<\/p>\n\n<p>scripPath=' sklearn.py'<\/p>\n\n<p><code>sklearn=SKLearn(entry_point=scripPath,\n                                 train_instance_type='ml.m5.xlarge',\n                                   role=role,                  output_path='s3:\/\/{}\/{}\/output'.format(bucket,prefix), sagemaker_session=session)\nsklearn.fit({\"train-dir' : train_input})<\/code><\/p>\n\n<p>When I deploy it\n<code>predictor=sklearn.deploy(initial_count=1,instance_type='ml.m5.xlarge')<\/code><\/p>\n\n<p>It throws,\n<code>Clienterror: An error occured when calling the CreateModel operation:Could not find model data at s3:\/\/tree\/sklearn\/output\/model.tar.gz<\/code><\/p>\n\n<p>Can anyone say how to solve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1560943786943,
        "Question_score":0,
        "Question_tags":"aws-sdk|amazon-sagemaker",
        "Question_view_count":1033,
        "Owner_creation_time":1560085651597,
        "Owner_last_access_time":1561731351963,
        "Owner_location":null,
        "Owner_reputation":155,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When deploying models, SageMaker looks up S3 to find your trained model artifact. It seems that there is no trained model artifact at <code>s3:\/\/tree\/sklearn\/output\/model.tar.gz<\/code>. Make sure to persist your model artifact in your training script at the appropriate local location in docker which is <code>\/opt\/ml\/model<\/code>.\nfor example, in your training script this could look like:<\/p>\n\n<pre><code>joblib.dump(model, \/opt\/ml\/model\/mymodel.joblib)\n<\/code><\/pre>\n\n<p>After training, SageMaker will copy the content of <code>\/opt\/ml\/model<\/code> to s3 at the <code>output_path<\/code> location.<\/p>\n\n<p>If you deploy in the same session a <code>model.deploy()<\/code> will map automatically to the artifact path. If you want to deploy a model that you trained elsewhere, possibly during a different session or in a different hardware, you need to explicitly instantiate a model before deploying<\/p>\n\n<pre><code>from sagemaker.sklearn.model import SKLearnModel\n\nmodel = SKLearnModel(\n    model_data='s3:\/\/...model.tar.gz',  # your artifact\n    role=get_execution_role(),\n    entry_point='script.py')  # script containing inference functions\n\nmodel.deploy(\n    instance_type='ml.m5.xlarge',\n    initial_instance_count=1,\n    endpoint_name='your_endpoint_name')\n<\/code><\/pre>\n\n<p>See more about Sklearn in SageMaker here <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1560972233450,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56666667",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61191412,
        "Question_title":"Unable to install toc2 notebook extension for AWS Sagemaker Instance (Lifecycle Configurations)",
        "Question_body":"<p>There's probably something very obvious I'm missing or Sagemaker just doesn't support these kinds of extensions, but I've been trying to enable toc2 (Table of Contents) jupyter extension for my Sagemaker notebook via lifecycle configurations, but for whatever reason it still isn't showing up.<\/p>\n\n<p>I built my script out combining a sample AWS script and a quick article on the usual ways of enabling extensions:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-nb-extension\/on-start.sh\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-nb-extension\/on-start.sh<\/a><\/p>\n\n<p><a href=\"https:\/\/towardsdatascience.com\/jupyter-notebook-extensions-517fa69d2231\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/jupyter-notebook-extensions-517fa69d2231<\/a><\/p>\n\n<pre><code>#!\/bin\/bash\n\nset -e\nsudo -u ec2-user -i &lt;&lt;EOF\n\n--Activate notebook environment\nsource activate JupyterSystemEnv\n\n--Install extensions\npip install jupyter_contrib_nbextensions &amp;&amp; jupyter contrib\nnbextension install\njupyter nbextension enable toc2 --py --sys-prefix\n\nsource deactivate\n\n\nEOF\n<\/code><\/pre>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_time":1586792396013,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|jupyter-notebook|amazon-sagemaker|tableofcontents",
        "Question_view_count":1027,
        "Owner_creation_time":1456986606313,
        "Owner_last_access_time":1663952857883,
        "Owner_location":null,
        "Owner_reputation":757,
        "Owner_up_votes":83,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Answering my question, looks like I was just missing the line <code>jupyter contrib nbextension install --user<\/code> to copy the JS\/CSS files into Jupyter's search directory and some config updates (<a href=\"https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions\" rel=\"nofollow noreferrer\">https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions<\/a>).<\/p>\n<p>Corrected statement<\/p>\n<pre><code>#!\/bin\/bash\n\nset -e\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\nsource \/home\/ec2-user\/anaconda3\/bin\/activate JupyterSystemEnv\n\npip install jupyter_contrib_nbextensions\njupyter contrib nbextension install --user\njupyter nbextension enable toc2\/main\n\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\n\n\nEOF\n\n##Below may be unnecessary, but other user needed to run to see success\ninitctl restart jupyter-server --no-wait\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1610748471693,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1615352840850,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61191412",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60405600,
        "Question_title":"How to make inference on local PC with the model trained on AWS SageMaker by using the built-in algorithm Semantic Segmentation?",
        "Question_body":"<p>Similar to the issue of <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/200\" rel=\"nofollow noreferrer\">The trained model can be deployed on the other platform without dependency of sagemaker or aws service?<\/a>.<\/p>\n\n<p>I have trained a model on AWS SageMaker by using the built-in algorithm <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/semantic-segmentation.html\" rel=\"nofollow noreferrer\">Semantic Segmentation<\/a>. This trained model named as <code>model.tar.gz<\/code> is stored on S3. So I want to download this file from S3 and then use it to make inference on my local PC without using AWS SageMaker anymore. Since the built-in algorithm Semantic Segmentation is built using the <a href=\"https:\/\/github.com\/dmlc\/gluon-cv\" rel=\"nofollow noreferrer\">MXNet Gluon framework and the Gluon CV toolkit<\/a>, so I try to refer the documentation of <a href=\"https:\/\/mxnet.apache.org\/\" rel=\"nofollow noreferrer\">mxnet<\/a> and <a href=\"https:\/\/gluon-cv.mxnet.io\/\" rel=\"nofollow noreferrer\">gluon-cv<\/a> to make inference on local PC.<\/p>\n\n<p>It's easy to download this file from S3, and then I unzip this file to get three files:<\/p>\n\n<ol>\n<li><strong>hyperparams.json<\/strong>: includes the parameters for network architecture, data inputs, and training. Refer to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/segmentation-hyperparameters.html\" rel=\"nofollow noreferrer\">Semantic Segmentation Hyperparameters<\/a>.<\/li>\n<li><strong>model_algo-1<\/strong><\/li>\n<li><strong>model_best.params<\/strong><\/li>\n<\/ol>\n\n<p>Both <strong>model_algo-1<\/strong> and <strong>model_best.params<\/strong> are the trained models, and I think it's the output from <code>net.save_parameters<\/code> (Refer to <a href=\"https:\/\/beta.mxnet.io\/guide\/getting-started\/crash-course\/4-train.html\" rel=\"nofollow noreferrer\">Train the neural network<\/a>). I can also load them with the function <code>mxnet.ndarray.load<\/code>.<\/p>\n\n<p>Refer to <a href=\"https:\/\/beta.mxnet.io\/guide\/getting-started\/crash-course\/5-predict.html\" rel=\"nofollow noreferrer\">Predict with a pre-trained model<\/a>. I found there are two necessary things:<\/p>\n\n<ol>\n<li>Reconstruct the network for making inference.<\/li>\n<li>Load the trained parameters.<\/li>\n<\/ol>\n\n<p>As for reconstructing the network for making inference, since I have used PSPNet from training, so I can use the class <code>gluoncv.model_zoo.PSPNet<\/code> to reconstruct the network. And I know how to use some services of AWS SageMaker, for example batch transform jobs, to make inference. I want to reproduce it on my local PC. If I use the class <code>gluoncv.model_zoo.PSPNet<\/code> to reconstruct the network, I can't make sure whether the parameters for this network are same those used on AWS SageMaker while making inference. Because I can't see the image <code>501404015308.dkr.ecr.ap-northeast-1.amazonaws.com\/semantic-segmentation:latest<\/code> in detail. <\/p>\n\n<p>As for loading the trained parameters, I can use the <code>load_parameters<\/code>. But as for <strong>model_algo-1<\/strong> and <strong>model_best.params<\/strong>, I don't know which one I should use.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1582681249783,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|mxnet|gluon",
        "Question_view_count":351,
        "Owner_creation_time":1472967821507,
        "Owner_last_access_time":1663314033960,
        "Owner_location":"Tokyo, Japan",
        "Owner_reputation":61,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1582681944470,
        "Answer_body":"<p>The following code works well for me.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import mxnet as mx\nfrom mxnet import image\nfrom gluoncv.data.transforms.presets.segmentation import test_transform\nimport gluoncv\n\n# use cpu\nctx = mx.cpu(0)\n\n# load test image\nimg = image.imread('.\/img\/IMG_4015.jpg')\nimg = test_transform(img, ctx)\nimg = img.astype('float32')\n\n# reconstruct the PSP network model\nmodel = gluoncv.model_zoo.PSPNet(2)\n\n# load the trained model\nmodel.load_parameters('.\/model\/model_algo-1')\n\n# make inference\noutput = model.predict(img)\npredict = mx.nd.squeeze(mx.nd.argmax(output, 1)).asnumpy()\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1583126137777,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60405600",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65719292,
        "Question_title":"How to run tensorboard for tensorflow in AWS Sagemaker?",
        "Question_body":"<p>I need to visualize real-time losses and metrics for a tensorflow model on AWS Sagemaker instance.\nIn a Jupyter notebook, I tried running<\/p>\n<pre><code>%load_ext tensorboard\n%tensorboard --logdir &lt;path&gt;\n<\/code><\/pre>\n<p>But nothing really happened. How can I get this working?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1610628627223,
        "Question_score":1,
        "Question_tags":"amazon-web-services|tensorflow|tensorflow2.0|tensorboard|amazon-sagemaker",
        "Question_view_count":715,
        "Owner_creation_time":1512023194593,
        "Owner_last_access_time":1663919015323,
        "Owner_location":null,
        "Owner_reputation":547,
        "Owner_up_votes":71,
        "Owner_down_votes":0,
        "Owner_views":61,
        "Question_last_edit_time":1610628935100,
        "Answer_body":"<p>You need to use the conda_pytorch_36 kernel (this is the one I used) and tensorboard is not installed by default so you need to run<\/p>\n<pre><code>!pip install tensorboard\n<\/code><\/pre>\n<p>Then you will get a blank screen when you run.<\/p>\n<pre><code>%load_ext tensorboard\n%tensorboard --logdir &quot;.\/runs&quot;\n<\/code><\/pre>\n<p>You can connect to tensorboard using your URL with notebook or lab replaced with proxy\/6006<\/p>\n<pre><code>https:\/\/YOUR_NOTEBOOK_INSTANCE_NAME.notebook.ap-northeast-1.sagemaker.aws\/proxy\/6006\/\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1610631020420,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65719292",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72097417,
        "Question_title":"Segfault using htop on AWS Sagemaker pytorch-1.10-cpu-py38 app",
        "Question_body":"<p>I am trying to launch the htop command in the Pytorch 1.10 - Python 3.8 CPU optimized AWS Sagemaker container. This works fine in other images I have used till now, but in this one, the command fails with a segfault:<\/p>\n<pre><code>htop \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nSegmentation fault (core dumped)\n<\/code><\/pre>\n<p>More info :<\/p>\n<pre><code>htop --version\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop 2.2.0 - (C) 2004-2019 Hisham Muhammad\nReleased under the GNU GPL.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651570402183,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|conda|amazon-sagemaker|htop",
        "Question_view_count":147,
        "Owner_creation_time":1551431163127,
        "Owner_last_access_time":1660223920803,
        "Owner_location":"Paris, France",
        "Owner_reputation":494,
        "Owner_up_votes":133,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I fixed this with<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code># Note: add sudo if needed:\nln -fs \/lib\/x86_64-linux-gnu\/libncursesw.so.6 \/opt\/conda\/lib\/libncursesw.so.6\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655459795693,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72097417",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49582307,
        "Question_title":"How to schedule tasks on SageMaker",
        "Question_body":"<p>I have a notebook on SageMaker I would like to run every night. What's the best way to schedule this task. Is there a way to run a bash script and schedule Cron job from SageMaker?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1522449441927,
        "Question_score":14,
        "Question_tags":"amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":17339,
        "Owner_creation_time":1420001102893,
        "Owner_last_access_time":1638226427930,
        "Owner_location":null,
        "Owner_reputation":173,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Amazon SageMaker is a set of API that can help various machine learning and data science tasks. These API can be invoked from various sources, such as CLI, <a href=\"https:\/\/aws.amazon.com\/tools\/\" rel=\"noreferrer\">SDK<\/a> or specifically from schedule AWS Lambda functions (see here for documentation: <a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-scheduled-events.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-scheduled-events.html<\/a> )<\/p>\n\n<p>The main parts of Amazon SageMaker are notebook instances, training and tuning jobs, and model hosting for real-time predictions. Each one has different types of schedules that you might want to have. The most popular are:<\/p>\n\n<ul>\n<li><strong>Stopping and Starting Notebook Instances<\/strong> - Since the notebook instances are used for interactive ML models development, you don't really need them running during the nights or weekends. You can schedule a Lambda function to call the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StopNotebookInstance.html\" rel=\"noreferrer\">stop-notebook-instance<\/a> API at the end of the working day (8PM, for example), and the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StartNotebookInstance.html\" rel=\"noreferrer\">start-notebook-instance<\/a> API in the morning. Please note that you can also run crontab on the notebook instances (after opening the local terminal from the Jupyter interface).<\/li>\n<li><strong>Refreshing an ML Model<\/strong> - Automating the re-training of models, on new data that is flowing into the system all the time, is a common issue that with SageMaker is easier to solve. Calling <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html\" rel=\"noreferrer\">create-training-job<\/a> API from a scheduled Lambda function (or even from a <a href=\"https:\/\/docs.aws.amazon.com\/AmazonCloudWatch\/latest\/events\/WhatIsCloudWatchEvents.html\" rel=\"noreferrer\">CloudWatch Event<\/a> that is monitoring the performance of the existing models), pointing to the S3 bucket where the old and new data resides, can <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateModel.html\" rel=\"noreferrer\">create a refreshed model<\/a> that you can now deploy into an <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_UpdateEndpointWeightsAndCapacities.html\" rel=\"noreferrer\">A\/B testing environment<\/a> .<\/li>\n<\/ul>\n\n<p>----- UPDATE (thanks to @snat2100 comment) -----<\/p>\n\n<ul>\n<li><strong>Creating and Deleting Real-time Endpoints<\/strong> - If your realtime endpoints are not needed 24\/7 (for example, serving internal company users working during workdays and hours), you can also <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpoint.html\" rel=\"noreferrer\">create the endpoints<\/a> in the morning and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_DeleteEndpoint.html\" rel=\"noreferrer\">delete them<\/a> at night. <\/li>\n<\/ul>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1523213115337,
        "Answer_score":19.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":1570200701000,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49582307",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50983316,
        "Question_title":"Continuous Training in Sagemaker",
        "Question_body":"<p>I am trying out <strong>Amazon Sagemaker<\/strong>, I haven't figured out how we can have Continuous training.\n<br>\nFor example if i have a CSV file in s3 and I want to train each time the CSV file is updated.<\/p>\n\n<p>I know we can go again to the notebook and re-run the whole notebook to make this happen.\n<br>\nBut i am looking for an automated way, with some python scripts or using a lambda function with s3 events etc<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1529654311630,
        "Question_score":4,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1191,
        "Owner_creation_time":1440734188430,
        "Owner_last_access_time":1655569862827,
        "Owner_location":"India",
        "Owner_reputation":1491,
        "Owner_up_votes":198,
        "Owner_down_votes":32,
        "Owner_views":112,
        "Question_last_edit_time":1529654668140,
        "Answer_body":"<p>You can use boto3 sdk for python to start training on lambda then you need to trigger the lambda when csv is update.<\/p>\n\n<blockquote>\n  <p><a href=\"http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/sagemaker.html\" rel=\"nofollow noreferrer\">http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/sagemaker.html<\/a><\/p>\n<\/blockquote>\n\n<p>Example python code<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a><\/p>\n<\/blockquote>\n\n<p>Addition: You dont need to use lambda you just start\/cronjob the python script any kind of instance which has python and aws sdk in it.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1530199631933,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50983316",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54462105,
        "Question_title":"SageMaker Ground Truth with TensorFlow",
        "Question_body":"<p>I've seen examples of labeling data using SageMaker Ground Truth and then using that data to train off-the-shelf SageMaker models. However, am I able to use this same annotation format with TensorFlow Script Mode? <\/p>\n\n<p>More specifically, I have a tensorflow.keras model I'm training using TF Script Mode, and I'd like to take data labeled with Ground Truth and convert my script from File mode to Pipe mode.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548942784467,
        "Question_score":2,
        "Question_tags":"python|tensorflow|amazon-sagemaker|labeling",
        "Question_view_count":454,
        "Owner_creation_time":1361339272693,
        "Owner_last_access_time":1663965928400,
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I am from Amazon SageMaker Ground Truth team and happy to assist you in your experiment. Just to be clear our understanding, are you running TF model in SageMaker using TF estimator in your own container (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst<\/a>)? <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1549915976767,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54462105",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60953289,
        "Question_title":"SageMaker gives CannotStartContainerError although I specified an entrypoint",
        "Question_body":"<p>I want to train a custom ML model with SageMaker. The model is written in Python and should be shipped to SageMaker in a Docker image. Here is a simplified version of my Dockerfile (the model sits in the train.py file):<\/p>\n\n<pre><code>FROM amazonlinux:latest\n\n# Install Python 3\nRUN yum -y update &amp;&amp; yum install -y python3-pip python3-devel gcc &amp;&amp; yum clean all\n\n# Install sagemaker-containers (the official SageMaker utils package)\nRUN pip3 install --target=\/usr\/local\/lib\/python3.7\/site-packages sagemaker-containers &amp;&amp; rm -rf \/root\/.cache\n\n# Bring the script with the model to the image \nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n\n<p>Now, if I initialize this image as a SageMaker estimator and then run the <code>fit<\/code> method on this estimator I get the following error:<\/p>\n\n<p>\"AlgorithmError: CannotStartContainerError. Please make sure the container can be run with 'docker run  train'.\"<\/p>\n\n<p>In other words: SageMaker is not able to get into the container and run the train.py file. But why? The way I am specifying the entrypoint with <code>ENV SAGEMAKER_PROGRAM train.py<\/code> is recommended in the <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\/blob\/master\/README.rst\" rel=\"nofollow noreferrer\">docs of the sagemaker-containers package<\/a> (see 'How a script is executed inside the container').<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1585665709967,
        "Question_score":1,
        "Question_tags":"python|image|docker|amazon-sagemaker|amazon-linux-2",
        "Question_view_count":1133,
        "Owner_creation_time":1448655975827,
        "Owner_last_access_time":1663931632060,
        "Owner_location":null,
        "Owner_reputation":1478,
        "Owner_up_votes":653,
        "Owner_down_votes":1,
        "Owner_views":135,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I found a hint in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">the AWS docs<\/a> and came up with this solution:<\/p>\n\n<pre><code>ENTRYPOINT [\"python3.7\", \"\/opt\/ml\/code\/train.py\"]\n<\/code><\/pre>\n\n<p>With this the container <a href=\"https:\/\/docs.docker.com\/engine\/reference\/builder\/#entrypoint\" rel=\"nofollow noreferrer\">will run as an executable<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1585671760393,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60953289",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61059996,
        "Question_title":"Sagemaker Processing doesn't upload",
        "Question_body":"<p>I'm trying to use the sagemaker processor to replace some processes we run on Amazon batch.<\/p>\n<pre><code>from sagemaker.processor import ScriptProcessor \nproc = ScriptProcessor(\n    image_uri='your-image-uri', \n    command=['python3'], \n    role=role, \n    instance_count=1, \n    instance_type='m4.4x.large',  \n    volume_size_in_gb=500,\n    base_job_name='preprocessing-test',\n)\nproc.run(\n    code='test.py',\n)\n<\/code><\/pre>\n<p>First of all, is it true that the <code>ScriptProcessing<\/code> syntax is more complicated than the <code>TrainingJob<\/code> version where you can specify the <code>source_dir<\/code> and <code>entrypoint<\/code> to upload your code to a default container?<\/p>\n<p>Secondly, this code above gives me this error<\/p>\n<pre><code>ParamValidationError: Parameter validation failed:\nInvalid bucket name &quot;sagemaker-eu-west-1-&lt;account-id&gt;\\preprocessing-test-&lt;timestamp&gt;\\input\\code&quot;: Bucket name must match the regex &quot;^[a-zA-Z0-9.\\-_]{1,255}$&quot; or be an ARN matching the regex &quot;^arn:(aws).*:s3:[a-z\\-0-9]+:[0-9]{12}:accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$&quot;\n<\/code><\/pre>\n<p>I guess this key is created internally when trying to upload my <code>test.py<\/code>, but why does it not work? :) The documentation says you can use both local and s3 paths.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1586176511533,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":314,
        "Owner_creation_time":1484838464573,
        "Owner_last_access_time":1663858511743,
        "Owner_location":"Amsterdam, Nederland",
        "Owner_reputation":3937,
        "Owner_up_votes":672,
        "Owner_down_votes":27,
        "Owner_views":387,
        "Question_last_edit_time":1616657799110,
        "Answer_body":"<p>The bucket name `sagemaker-eu-west-1-\\preprocessing-test-\\input\\code looks like a hardcoded string. In SageMaker Python SDK, the code upload function is <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/3bf569ece9e46a097d1ab69286ee89f762931e6c\/src\/sagemaker\/processing.py#L463\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<p>Are you using a Windows environment? As Lauren noted in the comments, there have been some bug fixes there, so make sure to use the last version<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1587103591427,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1616657708820,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61059996",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57164290,
        "Question_title":"Training with TensorFlow on Sagemaker No module named 'tf_container'",
        "Question_body":"<p>I'm trying to training TensorFlow model on AWS Sagemaker.\nI created container with external lib for that (Use Your Own Algorithms or Models with Amazon SageMaker).<\/p>\n\n<p>we run a training job with TensorFlow API<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tensorflow import TensorFlow\nestimator = TensorFlow(\n  entry_point=\"entry.py\",             # entry script\n  role=role,\n  framework_version=\"1.13.0\",   \n  py_version='py3',\n  hyperparameters=hyperparameters,\n  train_instance_count=1,                   # \"The number of GPUs instances to use\"\n  train_instance_type=train_instance_type,\n  image_name=my_image\n\n)\nestimator.fit({'train': train_s3, 'eval': eval_s3})\n<\/code><\/pre>\n\n<p>and got an error:<\/p>\n\n<pre><code>09:06:46\n2019-07-23 09:06:45,463 INFO - root - running container entrypoint \uf141\n09:06:46\n2019-07-23 09:06:45,463 INFO - root - starting train task \uf141\n09:06:46\n2019-07-23 09:06:45,476 INFO - container_support.training - Training starting \uf141\n09:06:46\n2019-07-23 09:06:45,479 ERROR - container_support.training - uncaught exception during training: No module named 'tf_container'\n\uf141\n09:06:46\nTraceback (most recent call last): File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/environment.py\", line 136, in load_framework return importlib.import_module('mxnet_container') File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File \"&lt;frozen importlib._bootstrap&gt;\", line 994, in _gcd_i \uf141\n09:06:46\nModuleNotFoundError: No module named 'mxnet_container'\n\uf141\n09:06:46\nDuring handling of the above exception, another exception occurred:\n\uf141\n09:06:46\nTraceback (most recent call last): File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/training.py\", line 35, in start fw = TrainingEnvironment.load_framework() File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/environment.py\", line 138, in load_framework return importlib.import_module('tf_container') File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, \uf141\n09:06:46\nModuleNotFoundError: No module named 'tf_container'\n<\/code><\/pre>\n\n<p>What can I do to solve this issue? how can I debug this case?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1563885123080,
        "Question_score":1,
        "Question_tags":"python|tensorflow|amazon-sagemaker",
        "Question_view_count":1191,
        "Owner_creation_time":1471952551663,
        "Owner_last_access_time":1662451190670,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I'm guessing that you used your own TF container, not the SageMaker one at <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a><\/p>\n\n<p>If that's the case, your container is missing the support code needed to use the TensorFlow estimator ('tf_container' package).<\/p>\n\n<p>The solution is to start from the SageMaker container, customize it, push it back to ECR and pass the image name to the SageMaker estimator with the 'image_name' parameter.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1563901268600,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57164290",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51533650,
        "Question_title":"No space left on device in Sagemaker model training",
        "Question_body":"<p>I'm using custom algorithm running shipped with Docker image on p2 instance with AWS Sagemaker (a bit similar to <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a>)<\/p>\n\n<p>At the end of training process, I try to write down my model to output directory, that is mounted via Sagemaker (like in tutorial), like this:<\/p>\n\n<pre><code>model_path = \"\/opt\/ml\/model\"\nmodel.save(os.path.join(model_path, 'model.h5'))\n<\/code><\/pre>\n\n<p>Unluckily, apparently the model gets too big with time and I get the\nfollowing error:<\/p>\n\n<blockquote>\n  <p>RuntimeError: Problems closing file (file write failed: time = Thu Jul\n  26 00:24:48 2018<\/p>\n  \n  <p>00:24:49 , filename = 'model.h5', file descriptor = 22, errno = 28,\n  error message = 'No space left on device', buf = 0x1a41d7d0, total\n  write[...]<\/p>\n<\/blockquote>\n\n<p>So all my hours of GPU time are wasted. How can I prevent this from happening again? Does anyone know what is the size limit for model that I store on Sagemaker\/mounted directories?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1532591463817,
        "Question_score":2,
        "Question_tags":"amazon-web-services|keras|amazon-sagemaker",
        "Question_view_count":2721,
        "Owner_creation_time":1366408339740,
        "Owner_last_access_time":1663750023060,
        "Owner_location":null,
        "Owner_reputation":8057,
        "Owner_up_votes":1123,
        "Owner_down_votes":72,
        "Owner_views":491,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When you train a model with <code>Estimators<\/code>, it <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/estimators.html\" rel=\"nofollow noreferrer\">defaults to 30 GB of storage<\/a>, which may not be enough. You can use the <code>train_volume_size<\/code> param on the constructor to increase this value. Try with a large-ish number (like 100GB) and see how big your model is. In subsequent jobs, you can tune down the value to something closer to what you actually need.<\/p>\n\n<p>Storage costs <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">$0.14 per GB-month of provisioned storage<\/a>. Partial usage is prorated, so giving yourself some extra room is a cheap insurance policy against running out of storage.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1539631654853,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51533650",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58292566,
        "Question_title":"How can I use a list of files as the training set on Sagemaker with Tensorflow?",
        "Question_body":"<p>I have several million images in my training folder and want to specify a subset of them for training - the way to do this seems to be with a manifest file as described here.<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/augmented-manifest.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/augmented-manifest.html<\/a><\/p>\n\n<p>But this seems to be geared towards labelled data. How can I start a sagemaker training job using sagemaker's Tensorflow <code>estimator.fit<\/code> with a list of files instead of the entire directory as input?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1570561431850,
        "Question_score":1,
        "Question_tags":"python|tensorflow|computer-vision|amazon-sagemaker",
        "Question_view_count":809,
        "Owner_creation_time":1416193017423,
        "Owner_last_access_time":1663696463113,
        "Owner_location":"Gensokyo",
        "Owner_reputation":880,
        "Owner_up_votes":211,
        "Owner_down_votes":2,
        "Owner_views":111,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use an input type pipe parameter like so: <\/p>\n\n<pre><code>hyperparameters = {'save_checkpoints_secs':None,\n                   'save_checkpoints_steps':1000}\n\ntf_estimator = TensorFlow(entry_point='.\/my-training-file', role=role,\n                          training_steps=5100, evaluation_steps=100,\n                          train_instance_count=1, train_instance_type='ml.p3.2xlarge',\n                          input_mode = 'Pipe',\n                          train_volume_size=300, output_path = 's3:\/\/sagemaker-pocs\/test-carlsoa\/kepler\/model',\n                          framework_version = '1.12.0', hyperparameters=hyperparameters, checkpoint_path = None)\n<\/code><\/pre>\n\n<p>And create the manifest file pipe as an input:<\/p>\n\n<pre><code>train_data = sagemaker.session.s3_input('s3:\/\/sagemaker-pocs\/test-carlsoa\/manifest.json',\n                                        distribution='FullyReplicated',\n                                        content_type='image\/jpeg',\n                                        s3_data_type='ManifestFile',\n                                        attribute_names=['source-ref']) \n                                        #attribute_names=['source-ref', 'annotations']) \ndata_channels = {'train': train_data}\n<\/code><\/pre>\n\n<p>Note that you can use ManifestFile or AugmentedManifestFile depending on whether you have extra data or labels to provide. Now you can use data_channels as the input to the tf estimator:<\/p>\n\n<p><code>tf_estimator.fit(inputs=data_channels, logs=True)<\/code><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1570649242910,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58292566",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56467434,
        "Question_title":"Making a Prediction Sagemaker Pytorch",
        "Question_body":"<p>I have trained and deployed a model in Pytorch with Sagemaker. I am able to call the endpoint and get a prediction. I am using the default input_fn() function (i.e. not defined in my serve.py).<\/p>\n\n<pre><code>model = PyTorchModel(model_data=trained_model_location,\n                     role=role,\n                     framework_version='1.0.0',\n                     entry_point='serve.py',\n                     source_dir='source')\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>A prediction can be made as follows:<\/p>\n\n<pre><code>input =\"0.12787057,  1.0612601,  -1.1081504\"\npredictor.predict(np.genfromtxt(StringIO(input), delimiter=\",\").reshape(1,3) )\n<\/code><\/pre>\n\n<p>I want to be able to serve the model with REST API and am HTTP POST using lambda and API gateway. I was able to use invoke_endpoint() for this with an XGBOOST model in Sagemaker this way. I am not sure what to send into the body for Pytorch.<\/p>\n\n<pre><code>client = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(EndpointName=ENDPOINT  ,\nContentType='text\/csv',\nBody=???)\n<\/code><\/pre>\n\n<p>I believe I need to understand how to write the customer input_fn to accept and process the type of data I am able to send through invoke_client. Am I on the right track and if so, how could the input_fn be written to accept a csv from invoke_endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559765963530,
        "Question_score":1,
        "Question_tags":"python|pytorch|amazon-sagemaker",
        "Question_view_count":2346,
        "Owner_creation_time":1294628108597,
        "Owner_last_access_time":1663884676413,
        "Owner_location":null,
        "Owner_reputation":1748,
        "Owner_up_votes":80,
        "Owner_down_votes":5,
        "Owner_views":393,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes you are on the right track. You can send csv-serialized input to the endpoint without using the <code>predictor<\/code> from the SageMaker SDK, and using other SDKs such as <code>boto3<\/code> which is installed in lambda:<\/p>\n\n<pre><code>import boto3\nruntime = boto3.client('sagemaker-runtime')\n\npayload = '0.12787057,  1.0612601,  -1.1081504'\n\nresponse = runtime.invoke_endpoint(\n    EndpointName=ENDPOINT_NAME,\n    ContentType='text\/csv',\n    Body=payload.encode('utf-8'))\n\nresult = json.loads(response['Body'].read().decode()) \n<\/code><\/pre>\n\n<p>This will pass to the endpoint a csv-formatted input, that you may need to reshape back in the <code>input_fn<\/code> to put in the appropriate dimension expected by the model.<\/p>\n\n<p>for example:<\/p>\n\n<pre><code>def input_fn(request_body, request_content_type):\n    if request_content_type == 'text\/csv':\n        return torch.from_numpy(\n            np.genfromtxt(StringIO(request_body), delimiter=',').reshape(1,3))\n<\/code><\/pre>\n\n<p><strong>Note<\/strong>: I wasn't able to test the specific <code>input_fn<\/code> above with your input content and shape but I used the approach on Sklearn RandomForest couple times, and looking at the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html#model-serving\" rel=\"nofollow noreferrer\">Pytorch SageMaker serving doc<\/a> the above rationale should work.<\/p>\n\n<p>Don't hesitate to use endpoint logs in Cloudwatch to diagnose any inference error (available from the endpoint UI in the console), those logs are usually <strong>much more verbose<\/strong> that the high-level logs returned by the inference SDKs<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1559781766690,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56467434",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65091602,
        "Question_title":"Is it possible to use more than 50 Labels in AWS Ground Truth",
        "Question_body":"<p>Is it possible to use more than 50 labels with AWS Ground Truth?<\/p>\n<p>For example <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-bounding-box.html\" rel=\"nofollow noreferrer\">here<\/a> are 3 labels:<\/p>\n<ul>\n<li>bird<\/li>\n<li>plane<\/li>\n<li>kite<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/GII4X.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/GII4X.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It shows that only 50 labels can be created. Is it possible to create more than 50 labels via AWS-CLI or any other API?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1606830024587,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|labeling|amazon-ground-truth",
        "Question_view_count":149,
        "Owner_creation_time":1510064331503,
        "Owner_last_access_time":1664041948290,
        "Owner_location":"M\u00fcnchen, Deutschland",
        "Owner_reputation":5537,
        "Owner_up_votes":1253,
        "Owner_down_votes":7,
        "Owner_views":215,
        "Question_last_edit_time":null,
        "Answer_body":"<p>No, according to the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-text-classification-multilabel.html\" rel=\"nofollow noreferrer\">documentation<\/a> the maximum is 50.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1606832097010,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65091602",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54184145,
        "Question_title":"AWS Sagemaker does not update the package",
        "Question_body":"<p>AWS Sagemaker's notebook comes with Scikit-Learn version 0.19.1<\/p>\n\n<p>I would like to use version 0.20.2. To avoid updating it every time in the notebook code, I tried using the lifecycle configurations. I created one with the following code :<\/p>\n\n<pre><code>#!\/bin\/bash\nset -e\n\/home\/ec2-user\/anaconda3\/bin\/conda install scikit-learn -y\n<\/code><\/pre>\n\n<p>When I run the attached notebook instance and go to the terminal, the version of scikit-learn found with <code>conda list<\/code> is correct (0.20.2). But when I run a notebook and import sklearn, the version is still 0.19.2.<\/p>\n\n<pre><code>import sklearn\nprint(sklearn.__version__)\n<\/code><\/pre>\n\n<p>Is there any virtual environment on the SageMaker instances where I should install the package ? How can I fix my notebook lifecycle configuration ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1547478776530,
        "Question_score":5,
        "Question_tags":"python|conda|amazon-sagemaker",
        "Question_view_count":1546,
        "Owner_creation_time":1527781503483,
        "Owner_last_access_time":1625555943023,
        "Owner_location":"Metz, France",
        "Owner_reputation":352,
        "Owner_up_votes":81,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Your conda update does not refer to a specific virtualenv, while your notebook probably does. Therefore you dont see an update on the notebook virtualenv.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1547708377157,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54184145",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51549048,
        "Question_title":"How to convert jupyter notebook (ipython) to slideshow NOT using command line",
        "Question_body":"<p>I am new with <code>Jupyter<\/code>, and I use Amazon SageMaker so that everything is cloud based and not local.  I cannot use any resources locally, nor can I install <code>Jupyter<\/code> on this local computer that I want to do this on, so I cannot use the command line to put :<\/p>\n\n<pre><code>jupyter nbconvert Jupyter\\ Slides.ipynb --to slides --post serve\n<\/code><\/pre>\n\n<p>So, I am struggling to find a way to convert my notebook to a slideshow NOT using command line. Thanks in advance!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1532649972710,
        "Question_score":1,
        "Question_tags":"ipython|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":4610,
        "Owner_creation_time":1528567336707,
        "Owner_last_access_time":1652301904530,
        "Owner_location":"Kelowna, BC, Canada",
        "Owner_reputation":165,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":1532675039560,
        "Answer_body":"<p>You can follow below steps to convert your notebook to slides on AWS Sagemaker (tried on sagemaker notebook instance) without installing any extensions.<\/p>\n\n<p><strong>Step 1:<\/strong> Follow this <a href=\"https:\/\/medium.com\/@mjspeck\/presenting-code-using-jupyter-notebook-slides-a8a3c3b59d67\" rel=\"nofollow noreferrer\">article<\/a> to chose which cells in your notebook can be presented or skipped.\n  - Go to View \u2192 Cell Toolbar \u2192 Slideshow\n  - A light gray bar will appear above each cell with a scroll down window on the top right\n  - Select type of slide each cell should be - regular slide, sub-slide, skip, notes<\/p>\n\n<p><strong>Step 2:<\/strong> Go to Sagemaker notebook home page and open terminal<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/kDl3d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kDl3d.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 3:<\/strong> Change directory in the instance where your notebook exists<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/rA1lZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rA1lZ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 4:<\/strong> Clone <code>reveal.js<\/code> in the directory where notebook exists from <a href=\"https:\/\/github.com\/hakimel\/reveal.js\" rel=\"nofollow noreferrer\">github<\/a>. <code>reveal.js<\/code> is used for rendering HTML file as presentation.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/dillF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/dillF.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 5:<\/strong> Run the below command (same as in your question) to convert the notebook to slides without serving them (since there is no browser on the Sagemaker instance). This will just convert notebook to slides html.<\/p>\n\n<pre><code>jupyter nbconvert Image-classification-fulltraining.ipynb --to slides\n[NbConvertApp] Converting notebook Image-classification-fulltraining.ipynb to slides\n[NbConvertApp] Writing 346423 bytes to Image-classification-fulltraining.slides.html\n<\/code><\/pre>\n\n<p><strong>Step 6:<\/strong> Now open the html file from Sagemaker notebook file browser <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/fykyl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fykyl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Now you can see the notebook rendered as slides based on how setup each cell in your notebook in Step 1<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9PLUA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9PLUA.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Hope it helps.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1532732192903,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51549048",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71161777,
        "Question_title":"Sagemaker Batch Transform entry point",
        "Question_body":"<p>Before the AWS Sagemaker batch transform I need to do some transform. is it possible to have an custom script and associate as entry point to BatchTransformer?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1645114921967,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":230,
        "Owner_creation_time":1554425457573,
        "Owner_last_access_time":1659094784667,
        "Owner_location":null,
        "Owner_reputation":63,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The inference code and requirement.txt should be stored as part of model.gz while training.  They will be used in the batch transform!!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1646173336163,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71161777",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69076270,
        "Question_title":".txt altered after save leads to CSV reader seeing too many fields",
        "Question_body":"<p>I am running a <code>JupyterLab<\/code> on <code>AWS SageMaker<\/code>. Kernel: <code>conda_amazonei_mxnet_p27<\/code><\/p>\n<p>The number of fields found: <code>saw 9<\/code> increments by 1, each run.<\/p>\n<p><strong>Error:<\/strong> <code>ParserError: Error tokenizing data. C error: Expected 2 fields in line 50, saw 9<\/code><\/p>\n<hr \/>\n<h3>Code:<\/h3>\n<p>Invocation (Error doesn't appear when running all cells before this but does when this is ran):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>train = open('train_textcorrupted.csv', 'a')\nval = open('val.csv', 'a')\nclasses = open('classes.txt', 'a')\nuni_label = 'Organisation\\tUniversity'\nn_pad = 4\nfor i in range(len(unis)-n_pad):\n    record = ' '.join(unis[i:(i+n_pad)])\n    full_record = f'{uni_label}\\t{record}\\n'\n    if random.random() &gt; 0.9:\n        val.write(full_record)\n    else:\n        train.write(full_record) \n\nclasses.write(uni_label)\nclasses.close() \nval.close()\ntrain.close()                      \n<\/code><\/pre>\n<h3>Traceback:<\/h3>\n<pre class=\"lang-sh prettyprint-override\"><code>---------------------------------------------------------------------------\nParserError                               Traceback (most recent call last)\n&lt;ipython-input-8-89b1728bd5a6&gt; in &lt;module&gt;\n      7       --gpus 1\n      8     &quot;&quot;&quot;.split()\n----&gt; 9 run_training(args)\n&lt;ipython-input-5-091daf2638a1&gt; in run_training(input)\n     55     csv_logger = pl.loggers.CSVLogger(save_dir=f'{args.modeldir}\/csv_logs')\n     56     loggers = [logger, csv_logger]\n---&gt; 57     dm = OntologyTaggerDataModule.from_argparse_args(args)\n     58     if args.model_uri:\n     59         local_model_uri = os.environ.get('SM_CHANNEL_MODEL', '.')\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pytorch_lightning\/core\/datamodule.py in from_argparse_args(cls, args, **kwargs)\n    324         datamodule_kwargs.update(**kwargs)\n    325 \n--&gt; 326         return cls(**datamodule_kwargs)\n    327 \n    328     @classmethod\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pytorch_lightning\/core\/datamodule.py in __call__(cls, *args, **kwargs)\n     47 \n     48         # Get instance of LightningDataModule by mocking its __init__ via __call__\n---&gt; 49         obj = type.__call__(cls, *args, **kwargs)\n     50 \n     51         return obj\n&lt;ipython-input-3-66ee2be72e78&gt; in __init__(self, traindir, train_file, validate_file, model_name, labels, batch_size)\n     30         print('tokenizer', tokenizer)\n     31         print('labels_file', labels_file)\n---&gt; 32         label_mapper = LabelMapper(labels_file)\n     33         self.batch_size = batch_size\n     34         self.num_classes = label_mapper.num_classes\n&lt;ipython-input-3-66ee2be72e78&gt; in __init__(self, classes_file)\n    102 \n    103     def __init__(self, classes_file):\n--&gt; 104         self._raw_labels = pd.read_csv(classes_file, header=None, sep='\\t')\n    105 \n    106         self._map = []\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\n    686     )\n    687 \n--&gt; 688     return _read(filepath_or_buffer, kwds)\n    689 \n    690 \n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in _read(filepath_or_buffer, kwds)\n    458 \n    459     try:\n--&gt; 460         data = parser.read(nrows)\n    461     finally:\n    462         parser.close()\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in read(self, nrows)\n   1196     def read(self, nrows=None):\n   1197         nrows = _validate_integer(&quot;nrows&quot;, nrows)\n-&gt; 1198         ret = self._engine.read(nrows)\n   1199 \n   1200         # May alter columns \/ col_dict\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in read(self, nrows)\n   2155     def read(self, nrows=None):\n   2156         try:\n-&gt; 2157             data = self._reader.read(nrows)\n   2158         except StopIteration:\n   2159             if self._first_chunk:\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader.read()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._read_low_memory()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._read_rows()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._tokenize_rows()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.raise_parser_error()\nParserError: Error tokenizing data. C error: Expected 2 fields in line 50, saw 9\n<\/code><\/pre>\n<hr \/>\n<p><code>classes.txt<\/code> (tab-separated) Before runtime<\/p>\n<pre><code>Activity    Event\nActor   Person\nAgent   Person\nAlbum   Product\nAnimal  Object\nArchitecturalStructure  Location\nArtist  Person\nAthlete Person\nAutomobileEngine    Product\nAward   Object\nBiomolecule Object\nBird    Object\nBodyOfWater Location\nBuilding    Location\nChemicalSubstance   Object\nCompany Organisation\nCompetition Event\nDevice  Product\nDisease Object\nDistrict    Location\nEukaryote   Object\nEvent   Event\nFilm    Object\nFood    Object\nLanguage    Object\nLocation    Location\nMeanOfTransportation    Product\nMotorsportSeason    Event\nMunicipality    Location\nMusicalWork Product\nOrganisation    Organisation\nPainter Person\nPeriodicalLiterature    Product\nPerson  Person\nPersonFunction  Person\nPlant   Object\nPoet    Person\nPolitician  Person\nRiver   Location\nSchool  Organisation\nSettlement  Location\nSoftware    Product\nSong    Product\nSpecies Object\nSportsSeason    Event\nStation Location\nTown    Location\nVillage Location\nWriter  Person\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1630938895957,
        "Question_score":0,
        "Question_tags":"python|python-3.x|amazon-web-services|jupyter-lab|amazon-sagemaker",
        "Question_view_count":87,
        "Owner_creation_time":1622632545867,
        "Owner_last_access_time":1646839538183,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":160,
        "Owner_down_votes":15,
        "Owner_views":111,
        "Question_last_edit_time":1631000955160,
        "Answer_body":"<p>Problem Found:<\/p>\n<p>So no fault of my own, I keep ensuring these fields are on their own lines in <code>classes.txt<\/code> and <code>Ctrl+S<\/code>. Then when I reopen the file, <strong>after runtime<\/strong>, it'll have fields be on the same line again.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/UbRTm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/UbRTm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>To fix this, on line <code>classes.write(uni_label)<\/code>.<\/p>\n<p>I replaced it with <code>classes.write('\\n'+uni_label)<\/code>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1631000840377,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1631001042847,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69076270",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53125108,
        "Question_title":"How do I download files within a Sagemaker notebook instance programatically?",
        "Question_body":"<p>We have a notebook instance within Sagemaker which contains many Jupyter Python scripts. I'd like to write a program which downloads these various scripts each day (i.e. so that I could back them up). Unfortunately I don't see any reference to this in the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/index.html\" rel=\"nofollow noreferrer\">AWS CLI API<\/a>.<\/p>\n\n<p>Is this achievable? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1541188759393,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|aws-lambda|aws-cli|amazon-sagemaker",
        "Question_view_count":8552,
        "Owner_creation_time":1299752760990,
        "Owner_last_access_time":1664061914187,
        "Owner_location":null,
        "Owner_reputation":2563,
        "Owner_up_votes":121,
        "Owner_down_votes":0,
        "Owner_views":167,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It's not exactly that you want, but looks like <a href=\"https:\/\/en.wikipedia.org\/wiki\/Version_control\" rel=\"nofollow noreferrer\">VCS<\/a> can fit your needs. You can use Github(if you already use it) or CodeCommit(free privat repos) Details and additional ways like <code>sync<\/code> target <code>dir<\/code> with <code>S3<\/code> bucket - <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/how-to-use-common-workflows-on-amazon-sagemaker-notebook-instances\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/how-to-use-common-workflows-on-amazon-sagemaker-notebook-instances\/<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1542042145470,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1542125550813,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53125108",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64537150,
        "Question_title":"AWS Sagemaker Multiple Training Jobs",
        "Question_body":"<p>We currently have a system running on AWS Sagemaker whereby several units have their own trained machine learning model artifact (using an SKLearn training script with the Sagemaker SKLearn estimator).<\/p>\n<p>Through the use of Sagemaker's multi-model endpoints, we are able to host all of these units on a single instance.<\/p>\n<p>The problem we have is that we need to scale this system up such that we can train individual models for hundreds of thousand of units and then host the resulting model artifacts on a multi-model endpoint. But, Sagemaker has a limit to the number of models you can train in parallel (our limit is 30).<\/p>\n<p>Aside from training our models in batches, does anyone have any ideas how to go about implementing a system in AWS Sagemaker whereby for hundreds of thousands of units, we can have a separate trained model artifact for each unit?<\/p>\n<p>Is there a way to output multiple model artifacts for 1 sagemaker training job with the use of an SKLearn estimator?<\/p>\n<p>Furthermore, how does Sagemaker make use of multiple CPUs when a training script is submitted? Does this have to be specified in the training script\/estimator object or is this handled automatically?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603715439387,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|machine-learning|scikit-learn|amazon-sagemaker",
        "Question_view_count":1053,
        "Owner_creation_time":1592311727163,
        "Owner_last_access_time":1647692327233,
        "Owner_location":null,
        "Owner_reputation":153,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":1603716498123,
        "Answer_body":"<p>Here are some ideas:<\/p>\n<p><em><strong>1. does anyone have any ideas how to go about implementing a system in AWS Sagemaker whereby for hundreds of thousands of units, we can have a separate trained model artifact for each unit? Is there a way to output multiple model artifacts for 1 sagemaker training job with the use of an SKLearn estimator?<\/strong><\/em><\/p>\n<p>I don't know if the 30-training job concurrency is a hard limit, if it is a blocker you should try and open a support ticket to ask if it is and try and get it raised. Otherwise as you can point out, you can try and train multiple models in one job, and produce multiple artifacts that you can either (a) send to S3 manually, or (b) save to <code>opt\/ml\/model<\/code> so that they all get sent to the model.tar.gz artifact in S3. Note that if this artifact gets too big this could get impractical though<\/p>\n<p><em><strong>2. how does Sagemaker make use of multiple CPUs when a training script is submitted? Does this have to be specified in the training script\/estimator object or is this handled automatically?<\/strong><\/em><\/p>\n<p>This depends on the type of training container you are using. SageMaker built-in containers are developed by Amazon teams and designed to efficiently use available resources. If you use your own code such as custom python in the Sklearn container, you are responsible for making sure that your code is efficiently written and uses available hardware. Hence framework choice is quite important :) for example, some sklearn models support explicitly using multiple CPUs (eg the <code>n_jobs<\/code> parameter in the <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\" rel=\"nofollow noreferrer\">random forest<\/a>), but I don't think that Sklearn natively supports GPU, multi-GPU or multi-node training.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1603790179687,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64537150",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73663585,
        "Question_title":"Add Security groups in Amazon SageMaker for distributed training jobs",
        "Question_body":"<p>We would like to enforce specific security groups to be set on the SageMaker training jobs (XGBoost in script mode).\nHowever, distributed training, in this case, won\u2019t work out of the box, since the containers need to communicate with each other. What are the minimum inbound\/outbound rules (ports) that we need to specify for training jobs so that they can communicate?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662733216593,
        "Question_score":1,
        "Question_tags":"amazon-web-services|xgboost|amazon-sagemaker|distributed-training",
        "Question_view_count":19,
        "Owner_creation_time":1662621266503,
        "Owner_last_access_time":1663966999637,
        "Owner_location":null,
        "Owner_reputation":48,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>setting up training in VPC including specifying security groups is documented here:\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-vpc.html#train-vpc-groups\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-vpc.html#train-vpc-groups<\/a><\/p>\n<p>Normally you would allow all communication between the training nodes. To do this you specify the security group source and destination to the name of the security group itself, and allow all IPv4 traffic. If you want to figure out what ports are used, you could: 1\/ define the permissive security group. 2\/ Turn on VPC flow logs 3\/ run training. 4\/ examine VPC Flow logs 5\/ update the security group only to the required ports.<\/p>\n<p>I must say restricting communication between the training nodes might be an extreme, so I would challenge the customer why it's really needed, as all nodes carry the same job, have the same IAM role, and are transiate by nature.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1662835019253,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73663585",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60341782,
        "Question_title":"Reading a file from s3 to sagemaker on AWS gives 403 forbidden error, but other operations work on the file",
        "Question_body":"<p>This command:<\/p>\n\n<pre><code>BUCKET_TO_READ='my-bucket'\nFILE_TO_READ='myFile'\ndata_location = 's3:\/\/{}\/{}'.format(BUCKET_TO_READ, FILE_TO_READ)\ndf=pd.read_csv(data_location)\n<\/code><\/pre>\n\n<p>is failing with a <\/p>\n\n<pre><code>ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n\n<p>Error and I'm unable to figure out why.  That should work according to <a href=\"https:\/\/stackoverflow.com\/a\/50244897\/3763782\">https:\/\/stackoverflow.com\/a\/50244897\/3763782<\/a> <\/p>\n\n<p>Here are my permissions on the bucket:<\/p>\n\n<pre><code>            \"Action\": [\n                \"s3:ListMultipartUploadParts\",\n                \"s3:ListBucket\",\n                \"s3:GetObjectVersionTorrent\",\n                \"s3:GetObjectVersionTagging\",\n                \"s3:GetObjectVersionAcl\",\n                \"s3:GetObjectVersion\",\n                \"s3:GetObjectTorrent\",\n                \"s3:GetObjectTagging\",\n                \"s3:GetObjectAcl\",\n                \"s3:GetObject\"\n<\/code><\/pre>\n\n<p>And these commands work as expected: <\/p>\n\n<pre><code>role = get_execution_role()\nregion = boto3.Session().region_name\nprint(role)\nprint(region)\n\ns3 = boto3.resource('s3')\nbucket = s3.Bucket(BUCKET_TO_READ)\nprint(bucket.creation_date)\n\nfor my_bucket_object in bucket.objects.all():\n    print(my_bucket_object)\n    FILE_TO_READ = my_bucket_object.key\n    break\n\nobj = s3.Object(BUCKET_TO_READ, FILE_TO_READ)\nprint(obj)\n\n<\/code><\/pre>\n\n<p>All of those print statements worked just fine.  <\/p>\n\n<p>I'm not sure if it matters, but each file is within a folder, so my FILE_TO_READ looks like <code>folder\/file<\/code>.<\/p>\n\n<p>This command which should download the file to sagemaker also falied with a 403:<\/p>\n\n<pre><code>import boto3\ns3 = boto3.resource('s3')\ns3.Object(BUCKET_TO_READ, FILE_TO_READ).download_file(FILE_TO_READ)\n<\/code><\/pre>\n\n<p>This is also happening when I open a terminal and use <\/p>\n\n<pre><code>aws s3 cp AWSURI local_file_name\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1582298932137,
        "Question_score":4,
        "Question_tags":"python|pandas|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":2322,
        "Owner_creation_time":1403392071733,
        "Owner_last_access_time":1634681182993,
        "Owner_location":null,
        "Owner_reputation":91,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":1582306425573,
        "Answer_body":"<p>The reason was that we granted permission to the bucket not the objects.  That would be granting <code>\"Resource\": \"arn:aws:s3:::bucket-name\/\"<\/code> but not <code>\"Resource\": \"arn:aws:s3:::bucket-name\/*\"<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1582382844413,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60341782",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70873792,
        "Question_title":"How to handle Sagemaker Batch Transform discarding a file with a failed model request",
        "Question_body":"<p>I have a large number of JSON requests for a model split across multiple files in an S3 bucket. I would like to use Sagemaker's Batch Transform feature to process all of these requests (I have done a couple of test runs using small amounts of data and the transform job succeeds). My main issue is here (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors<\/a>), specifically:<\/p>\n<blockquote>\n<p>If a batch transform job fails to process an input file because of a problem with the dataset, SageMaker marks the job as failed. If an input file contains a bad record, the transform job doesn't create an output file for that input file because doing so prevents it from maintaining the same order in the transformed data as in the input file. When your dataset has multiple input files, a transform job continues to process input files even if it fails to process one. The processed files still generate useable results.<\/p>\n<\/blockquote>\n<p>This is not preferable mainly because if 1 request fails (whether its a transient error, a malformmated request, or something wrong with the model container) in a file with a large number of requests, all of those requests will get discarded (even if all of them succeeded and the last one failed). I would ideally prefer Sagemaker to just write the output of the failed response to the file and keep going, rather than discarding the entire file.<\/p>\n<p>My question is, are there any suggestions to mitigating this issue? I was thinking about storing 1 request per file in S3, but this seems somewhat ridiculous? Even if I did this, is there a good way of seeing which requests specifically failed after the transform job finishes?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643261540133,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":357,
        "Owner_creation_time":1597858315077,
        "Owner_last_access_time":1663976401100,
        "Owner_location":null,
        "Owner_reputation":84,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You've got the right idea: the fewer datapoints are in each file, the less likely a given file is to fail. The issue is that while you can pass a prefix with many files to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB\" rel=\"nofollow noreferrer\">CreateTransformJob<\/a>, partitioning one datapoint per file at least requires an S3 read per datapoint, plus a model invocation per datapoint, which is probably not great. Be aware also that <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=1000415&amp;tstart=0\" rel=\"nofollow noreferrer\">apparently there are hidden rate limits<\/a>.<\/p>\n<p>Here are a couple options:<\/p>\n<ol>\n<li><p>Partition into small-ish files, and plan on failures being rare. Hopefully, not many of your datapoints would actually fail. If you partition your dataset into e.g. 100 files, then a single failure only requires reprocessing 1% of your data. Note that Sagemaker has built-in retries, too, so most of the time failures should be caused by your data\/logic, not randomness on Sagemaker's side.<\/p>\n<\/li>\n<li><p>Deal with failures directly in your model. The same doc you quoted in your question also says:<\/p>\n<\/li>\n<\/ol>\n<blockquote>\n<p>If you are using your own algorithms, you can use placeholder text, such as ERROR, when the algorithm finds a bad record in an input file. For example, if the last record in a dataset is bad, the algorithm places the placeholder text for that record in the output file.<\/p>\n<\/blockquote>\n<p>Note that the reason Batch Transform does this whole-file failure is to maintain a 1-1 mapping between rows in the input and the output. If you can substitute the output for failed datapoints with an error message from inside your model, without actually causing the model itself to fail processing, Batch Transform will be happy.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1644424499010,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1644528409280,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70873792",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50271174,
        "Question_title":"How to make a Python Visualization as service | Integrate with website | specially sagemaker",
        "Question_body":"<p>I am from R background where we can use Plumber kind tool which provide visualization\/graph as Image via end points so we can integrate in our Java application.<\/p>\n\n<p>Now I want to integrate my Python\/Juypter visualization graph with my Java application but not sure how to host it and make it as endpoint. Right now I using AWS sagemaker to host Juypter notebook<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1525949293883,
        "Question_score":0,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":76,
        "Owner_creation_time":1501403168107,
        "Owner_last_access_time":1663685130870,
        "Owner_location":"Delhi, India",
        "Owner_reputation":1370,
        "Owner_up_votes":94,
        "Owner_down_votes":1,
        "Owner_views":125,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Amazon SageMaker is a set of different services for data scientists. You are using the notebook service that is used for developing ML models in an interactive way. The hosting service in SageMaker is creating an endpoint based on a trained model. You can call this endpoint with invoke-endpoint API call for real time inference. <\/p>\n\n<p>It seems that you are looking for a different type of hosting that is more suitable for serving HTML media rich pages, and doesn\u2019t fit into the hosting model of SageMaker. A combination of EC2 instances, with pre-built AMI or installation scripts, Congnito for authentication, S3 and EBS for object and block storage, and similar building blocks should give you a scalable and cost effective solution. <\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1526073355693,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50271174",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72760982,
        "Question_title":"How to use Tensorboard within a notebook running on Amazon SageMaker Studio Lab?",
        "Question_body":"<p>I have a Jupyter notebook running within an <code>Amazon SageMaker Studio Lab<\/code> (<a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a>) environment, and I want to use Tensordboard to monitor my model's performance inside the notebook.<\/p>\n<p>I have used the following commands to set up the Tensorboard:<\/p>\n<pre><code>%load_ext tensorboard\n# tb_log_dir variable holds the path to the log directory\n%tensorboard --logdir tb_log_dir\n<\/code><\/pre>\n<p>But nothing shows up in the output of the cell where I execute the commands. See:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TwbTk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TwbTk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The two buttons shown in the picture are not responding, BTW.<\/p>\n<p>How to solve this problem? Any suggestions would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656241190023,
        "Question_score":1,
        "Question_tags":"jupyter-notebook|amazon-sagemaker|tensorboard",
        "Question_view_count":121,
        "Owner_creation_time":1420286650807,
        "Owner_last_access_time":1663841459487,
        "Owner_location":"Trondheim, Norway",
        "Owner_reputation":720,
        "Owner_up_votes":129,
        "Owner_down_votes":0,
        "Owner_views":126,
        "Question_last_edit_time":1656243378470,
        "Answer_body":"<p>I would try the canonical way to use tensorboard in AWS Sagemaker, it should be supported also by Studio Lab, it is described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tensorboard.html\" rel=\"nofollow noreferrer\">here<\/a>. Basically install tensorboard and using the <code>EFS_PATH_LOG_DIR<\/code> launch tensorboard using the embedded console (you can do the following also from a cell):<\/p>\n<pre><code>pip install tensorboard\ntensorboard --logdir &lt;EFS_PATH_LOG_DIR&gt;\n<\/code><\/pre>\n<p>Be careful with the EFS_PATH_LOG_DIR, be sure this folder is valida path from the location you are, for example by default you are located in <code>studio-lab-user\/sagemaker-studiolab-notebooks\/<\/code> so the proper command would be <code>!tensorboard --logdir logs\/fit<\/code>.<\/p>\n<p>Then open a browser to:<\/p>\n<pre><code>https:\/\/&lt;YOUR URL&gt;\/studiolab\/default\/jupyter\/proxy\/6006\/\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1656410088380,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1657181121617,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72760982",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55158307,
        "Question_title":"Can I make Amazon SageMaker deliver a recommendation based on historic data instead of a probability score?",
        "Question_body":"<p>We have a huge set of data in CSV format, containing a few numeric elements, like this:<\/p>\n\n<pre><code>Year,BinaryDigit,NumberToPredict,JustANumber, ...other stuff\n1954,1,762,16, ...other stuff\n1965,0,142,16, ...other stuff\n1977,1,172,16, ...other stuff\n<\/code><\/pre>\n\n<p>The thing here is that there is a strong correlation between the third column and the columns before that. So I have pre-processed the data and it's now available in a format I think is perfect:<\/p>\n\n<pre><code>1954,1,762\n1965,0,142\n1977,1,172\n<\/code><\/pre>\n\n<p>What I want is a predicition on the value in the third column, using the first two as input. So in the case above, I want the input 1965,0 to return 142. In real life this file is thousands of rows, but since there's a pattern, I'd like to retrieve the most possible value.<\/p>\n\n<p>So far I've setup a train job on the CSV file using the L<em>inear Learner<\/em> algorithm, with the following settings:<\/p>\n\n<pre><code>label_size = 1\nfeature_dim = 2\npredictor_type = regression\n<\/code><\/pre>\n\n<p>I've also created a model from it, and setup an endpoint. When I invoke it, I get a score in return.<\/p>\n\n<pre><code>    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                   ContentType='text\/csv',\n                                   Body=payload)\n<\/code><\/pre>\n\n<p>My goal here is to get the third column prediction instead. How can I achieve that? I have read a lot of the documentation regarding this, but since I'm not very familiar with AWS, I might as well have used the wrong algorithms for what I am trying to do.<\/p>\n\n<p>(Please feel free to edit this question to better suit AWS terminology)<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1552553455710,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":271,
        "Owner_creation_time":1411464641600,
        "Owner_last_access_time":1663665272227,
        "Owner_location":"\u00d6rebro, Sverige",
        "Owner_reputation":205,
        "Owner_up_votes":49,
        "Owner_down_votes":0,
        "Owner_views":38,
        "Question_last_edit_time":1552895653107,
        "Answer_body":"<p>For csv input, the label should be in the first column, as mentioned <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/cdf-training.html\" rel=\"nofollow noreferrer\">here<\/a>:  So you should preprocess your data to put the label (the column you want to predict) on the left.<\/p>\n\n<p>Next, you need to decide whether this is a regression problem or a classification problem. <\/p>\n\n<p>If you want to predict a number that's as close as possible to the true number, that's regression. For example, the truth might be 4, and the model might predict 4.15. If you need an integer prediction, you could round the model's output.<\/p>\n\n<p>If you want the prediction to be one of a few categories, then you have a classification problem. For example, we might encode 'North America' = 0, 'Europe' = 1, 'Africa' = 2, and so on. In this case, a fractional prediction wouldn't make sense. <\/p>\n\n<p>For regression, use <code>'predictor_type' = 'regressor'<\/code> and for classification with more than 2 classes, use <code>'predictor_type' = 'multiclass_classifier'<\/code> as documented <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>The output of regression will contain only a <code>'score'<\/code> field, which is the model's prediction. The output of multiclass classification will contain a <code>'predicted_label'<\/code> field, which is the model's prediction, as well as a <code>'score'<\/code> field, which is a vector of probabilities representing the model's confidence. The index with the highest probability will be the one that's predicted as the <code>'predicted_label'<\/code>. The output formats are documented <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/LL-in-formats.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1553880598533,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55158307",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48438202,
        "Question_title":"Errors while using sagemaker api to invoke endpoints",
        "Question_body":"<p>I've deployed an endpoint in sagemaker and was trying to invoke it through my python program. I had tested it using postman and it worked perfectly ok. Then I wrote the invocation code as follows<\/p>\n\n<pre><code>import boto3\nimport pandas as pd\nimport io\nimport numpy as np\n\ndef np2csv(arr):\n    csv = io.BytesIO()\n    np.savetxt(csv, arr, delimiter=',', fmt='%g')\n    return csv.getvalue().decode().rstrip()\n\n\nruntime= boto3.client('runtime.sagemaker')\npayload = np2csv(test_X)\n\nruntime.invoke_endpoint(\n    EndpointName='&lt;my-endpoint-name&gt;',\n    Body=payload,\n    ContentType='text\/csv',\n    Accept='Accept'\n)\n<\/code><\/pre>\n\n<p>Now whe I run this I get a validation error<\/p>\n\n<pre><code>ValidationError: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint &lt;my-endpoint-name&gt; of account &lt;some-unknown-account-number&gt; not found.\n<\/code><\/pre>\n\n<p>While using postman i had given my access key and secret key but I'm not sure how to pass it when using sagemaker apis. I'm not able to find it in the documentation also. <\/p>\n\n<p>So my question is, how can I use sagemaker api from my local machine to invoke my endpoint?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1516867519790,
        "Question_score":5,
        "Question_tags":"python|python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":4467,
        "Owner_creation_time":1410972175307,
        "Owner_last_access_time":1663832955917,
        "Owner_location":null,
        "Owner_reputation":1124,
        "Owner_up_votes":156,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":1516875209620,
        "Answer_body":"<p>When you are using any of the AWS SDK (including the one for Amazon SageMaker), you need to configure the credentials of your AWS account on the machine that you are using to run your code. If you are using your local machine, you can use the AWS CLI flow. You can find detailed instructions on the Python SDK page: <a href=\"https:\/\/aws.amazon.com\/developers\/getting-started\/python\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/developers\/getting-started\/python\/<\/a> <\/p>\n\n<p>Please note that when you are deploying the code to a different machine, you will have to make sure that you are giving the EC2, ECS, Lambda or any other target a role that will allow the call to this specific endpoint. While in your local machine it can be OK to give you admin rights or other permissive permissions, when you are deploying to a remote instance, you should restrict the permissions as much as possible. <\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"sagemaker:InvokeEndpoint\",\n            \"Resource\": \"arn:aws:sagemaker:*:1234567890:endpoint\/&lt;my-endpoint-name&gt;\"\n        }\n    ]\n}\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1517113913470,
        "Answer_score":4.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48438202",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57032981,
        "Question_title":"how long does converting to amazon protobuf record using .record_set() take to complete",
        "Question_body":"<p>I am trying to convert a numpy array to an amazon protobuf record using <code>sagemaker.amazon.amazon_estimator.AmazonAlgorithmEstimatorBase.record_set()<\/code> However, this is taking a really long time. <\/p>\n\n<p>I'm wondering how the function actually performs and how long it should take<\/p>\n\n<pre><code>from sagemaker import LinearLearner\nimport numpy as np\n\nmodel=LinearLearner(role=get_execution_role(),\n                             train_instance_count=len(train_features),\n                             train_instance_type='ml.t2.medium',\n                             predictor_type='binary_classifier',\n                                )\n<\/code><\/pre>\n\n<pre><code>numpy_array = np.array([[7.4727994e-01 9.5506465e-01 7.6940370e-01 8.2015032e-01 1.8113719e-01\n  7.8720862e-01 2.9677063e-01 2.6711187e-01 7.9498607e-01 4.4924998e-01\n  4.9533784e-01 2.6846960e-01 7.0506859e-01 4.1573554e-01 6.5843487e-01\n  3.2448095e-01 4.3870610e-01 7.2739214e-01 6.0914969e-01 5.5108833e-01\n  5.8835250e-01 5.5872935e-01 4.4392920e-01 6.8353373e-01 4.7664520e-01\n  5.6887656e-01 4.7034043e-01 4.1631639e-01 3.1357434e-01 5.5933639e-04]\n [5.7815754e-01 9.5828843e-01 7.7824914e-01 8.3188844e-01 2.3287645e-01\n  7.7196079e-01 2.5512937e-01 2.7032304e-01 7.8349811e-01 5.0130588e-01\n  4.8345023e-01 3.8397798e-01 5.9922373e-01 4.7720599e-01 6.7832541e-01\n  2.7788603e-01 4.6435007e-01 7.6100332e-01 7.7771670e-01 5.1536995e-01\n  5.8536130e-01 5.6407303e-01 5.0898582e-01 6.7815554e-01 3.0614817e-01\n  5.7353836e-01 3.8981739e-01 4.1474316e-01 3.1389123e-01 3.5031504e-04]]) \n<\/code><\/pre>\n\n<pre><code>record=model.record_set(numpy_array)\n<\/code><\/pre>\n\n<h2>Expected output<\/h2>\n\n<p>I expect the variable record to container a record ready for training with linearlearning model<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1563161763097,
        "Question_score":0,
        "Question_tags":"python|linear-regression|amazon-sagemaker",
        "Question_view_count":119,
        "Owner_creation_time":1515320453680,
        "Owner_last_access_time":1664019125340,
        "Owner_location":"Bangkok",
        "Owner_reputation":1848,
        "Owner_up_votes":1616,
        "Owner_down_votes":1,
        "Owner_views":206,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I believe this is the problem:<\/p>\n\n<pre><code>train_instance_count=len(train_features)\n<\/code><\/pre>\n\n<p>This parameter is about infrastructure (how many SageMaker instances you want to train on), not about features. You should set it to 1.<\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker import LinearLearner\nimport numpy as np\n\nmodel=LinearLearner(role=sagemaker.get_execution_role(),\n                             train_instance_count=1,\n                             train_instance_type='ml.t2.medium',\n                             predictor_type='binary_classifier')\n\nnumpy_array = np.array(...)\n\nrecord=model.record_set(numpy_array)\n# This takes &lt;100 ms on my t3 notebook instance\n\nprint(record)\n\n(&lt;class 'sagemaker.amazon.amazon_estimator.RecordSet'&gt;, {'s3_data':\n's3:\/\/sagemaker-eu-west-1-123456789012\/sagemaker-record-sets\/LinearLearner-\n2019-07-18-09-48-21-639\/.amazon.manifest', 'feature_dim': 30, 'num_records': 2,\n's3_data_type': 'ManifestFile', 'channel': 'train'})\n<\/code><\/pre>\n\n<p>The manifest file lists the protobuf-encoded file(s):<\/p>\n\n<pre><code>[{\"prefix\": \"s3:\/\/sagemaker-eu-west-1-123456789012\/sagemaker-record-sets\/LinearLearner-2019-07-18-09-48-21-639\/\"}, \"matrix_0.pbr\"]\n<\/code><\/pre>\n\n<p>You can now use it for the training channel when you call fit(), re: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1563444116137,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57032981",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70163094,
        "Question_title":"SageMaker Model Registry Sharing",
        "Question_body":"<p>Is it possible to share a model registry completely between Dev and Prod environment? So my idea is to create 10000 models in dev and maybe select 2000 from there to work in prod. I am planning to use AWS model registry. So if I do the training and testing and hyperparameter tuning in my AWS dev environment, is it possible to then share the registry in prod? The obvious reason is that it does not make sense to use the prod to do the training and testing again.<\/p>\n<p>Please advise!<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638234190620,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":229,
        "Owner_creation_time":1557333597230,
        "Owner_last_access_time":1663953686617,
        "Owner_location":null,
        "Owner_reputation":99,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It depends how you define Dev and Prod.<\/p>\n<ul>\n<li><p>If by Dev and Prod you mean different AWS account (which is a good practice - see <a href=\"https:\/\/docs.aws.amazon.com\/whitepapers\/latest\/organizing-your-aws-environment\/benefits-of-using-multiple-aws-accounts.html\" rel=\"nofollow noreferrer\">doc<\/a> and <a href=\"https:\/\/aws.amazon.com\/blogs\/devops\/aws-building-a-secure-cross-account-continuous-delivery-pipeline\/\" rel=\"nofollow noreferrer\">blog<\/a>), you cannot share fractions of a model registry from a given account to another account, but you can create triggers to export models from one model registry to another, as documented in this blog post <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/patterns-for-multi-account-hub-and-spoke-amazon-sagemaker-model-registry\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/patterns-for-multi-account-hub-and-spoke-amazon-sagemaker-model-registry\/<\/a><\/p>\n<\/li>\n<li><p>If your Dev and Prod live in the same AWS account and you are just looking for ways to differentiate them, you can use:<\/p>\n<ul>\n<li>Model Registry Status information<\/li>\n<li>Tags<\/li>\n<\/ul>\n<\/li>\n<\/ul>",
        "Answer_comment_count":7.0,
        "Answer_creation_time":1638378803110,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70163094",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72419908,
        "Question_title":"How to match input\/output with sagemaker batch transform?",
        "Question_body":"<p>I'm using sagemaker batch transform, with json input files. see below for sample input\/output files. i have custom inference code below, and i'm using json.dumps to return prediction, but it's not returning json. I tried to use =&gt;    &quot;DataProcessing&quot;: {&quot;JoinSource&quot;: &quot;string&quot;,  }, to match input and output. but i'm getting error that &quot;unable to marshall ...&quot; . I think because , the output_fn is returning array of list or just list and not json , that is why it is unable to match input with output.any suggestions on how should i return the data?<\/p>\n<p>infernce code<\/p>\n<pre><code>def model_fn(model_dir):\n...\ndef input_fn(data, content_type):\n...\ndef predict_fn(data, model):\n...\ndef output_fn(prediction, accept):\n    if accept == &quot;application\/json&quot;:\n        return json.dumps(prediction), mimetype=accept)\n    raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))\n<\/code><\/pre>\n<p>input file<\/p>\n<pre><code>{&quot;data&quot; : &quot;input line  one&quot; }\n{&quot;data&quot; : &quot;input line  two&quot; }\n....\n<\/code><\/pre>\n<p>output file<\/p>\n<pre><code>[&quot;output line  one&quot; ]\n[&quot;output line  two&quot; ]\n<\/code><\/pre>\n<pre><code>{\n   &quot;BatchStrategy&quot;: SingleRecord,\n   &quot;DataProcessing&quot;: { \n      &quot;JoinSource&quot;: &quot;string&quot;,\n   },\n   &quot;MaxConcurrentTransforms&quot;: 3,\n   &quot;MaxPayloadInMB&quot;: 6,\n   &quot;ModelClientConfig&quot;: { \n      &quot;InvocationsMaxRetries&quot;: 1,\n      &quot;InvocationsTimeoutInSeconds&quot;: 3600\n   },\n   &quot;ModelName&quot;: &quot;some-model&quot;,\n   &quot;TransformInput&quot;: { \n      &quot;ContentType&quot;: &quot;string&quot;,\n      &quot;DataSource&quot;: { \n         &quot;S3DataSource&quot;: { \n            &quot;S3DataType&quot;: &quot;string&quot;,\n            &quot;S3Uri&quot;: &quot;s3:\/\/bucket-sample&quot;\n         }\n      },\n      &quot;SplitType&quot;: &quot;Line&quot;\n   },\n   &quot;TransformJobName&quot;: &quot;transform-job&quot;\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653780407103,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":358,
        "Owner_creation_time":1590797441983,
        "Owner_last_access_time":1664049080543,
        "Owner_location":null,
        "Owner_reputation":525,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":98,
        "Question_last_edit_time":null,
        "Answer_body":"<p><code>json.dumps<\/code> will not convert your array to a dict structure and serialize it to a JSON String.<\/p>\n<p>What data type is <code>prediction<\/code> ? Have you tested making sure <code>prediction<\/code> is a dict?<\/p>\n<p>You can confirm the data type by adding <code>print(type(prediction))<\/code> to see the data type in the CloudWatch Logs.<\/p>\n<p>If prediction is a <code>list<\/code> you can test the following:<\/p>\n<pre><code>def output_fn(prediction, accept):\n    if accept == &quot;application\/json&quot;:\n\n        my_dict = {'output': prediction}\n        return json.dumps(my_dict), mimetype=accept)\n\n    raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))\n<\/code><\/pre>\n<p><code>DataProcessing<\/code> and <code>JoinSource<\/code> are used to associate the data that is relevant to the prediction results in the output. It is not meant to be used to match the input and output format.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1654129068707,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72419908",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63813624,
        "Question_title":"Load a Picked or Joblib Pre trained ML Model to Sagemaker and host as endpoint",
        "Question_body":"<p>If I have a trained model in Using pickle, or Joblib.\nLets say its Logistic regression or XGBoost.<\/p>\n<p>I would like to host that model in AWS Sagemaker as endpoint without running a training job.\nHow to achieve that.<\/p>\n<pre><code>#Lets Say myBucketName contains model.pkl\nmodel = joblib.load('filename.pkl')  \n# X_test = Numpy Array \nmodel.predict(X_test)  \n<\/code><\/pre>\n<p>I am not interested to <code>sklearn_estimator.fit('S3 Train, S3 Validate' )<\/code> , I have the trained model<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1599661148083,
        "Question_score":3,
        "Question_tags":"python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":1622,
        "Owner_creation_time":1370509408500,
        "Owner_last_access_time":1664063534747,
        "Owner_location":null,
        "Owner_reputation":1573,
        "Owner_up_votes":97,
        "Owner_down_votes":12,
        "Owner_views":194,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For Scikit Learn for example, you can get inspiration from this public demo <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>\n<p>Step 1: Save your artifact (eg the joblib) compressed in S3 at <code>s3:\/\/&lt;your path&gt;\/model.tar.gz<\/code><\/p>\n<p>Step 2: Create an inference script with the deserialization function <code>model_fn<\/code>. (Note that you could also add custom inference functions <code>input_fn<\/code>, <code>predict_fn<\/code>, <code>output_fn<\/code> but for scikit the defaults function work fine)<\/p>\n<pre><code>%%writefile inference_script.py. # Jupiter command to create file in case you're in Jupiter\n\nimport joblib\nimport os\n\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return clf\n<\/code><\/pre>\n<p>Step 3: Create a model associating the artifact with the right container<\/p>\n<pre><code>from sagemaker.sklearn.model import SKLearnModel\n\nmodel = SKLearnModel(\n    model_data='s3:\/\/&lt;your path&gt;\/model.tar.gz',\n    role='&lt;your role&gt;',\n    entry_point='inference_script.py',\n    framework_version='0.23-1')\n<\/code><\/pre>\n<p>Step 4: Deploy!<\/p>\n<pre><code>model.deploy(\n    instance_type='ml.c5.large',  # choose the right instance type\n    initial_instance_count=1)\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1599722577177,
        "Answer_score":5.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63813624",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70455676,
        "Question_title":"AWS Sagemaker Pipelines throws a \"No finished training job found associated with this estimator\" after introducing a register step",
        "Question_body":"<p>I am currently working on creating a Sagemaker Pipeline to train a Tensorflow model. I'm new to this area and I have been following <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-pipelines\/tabular\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint.ipynb\" rel=\"nofollow noreferrer\">this guide<\/a> created by AWS as well as the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html\" rel=\"nofollow noreferrer\">standard pipeline workflow<\/a> listed in the Sagemaker developer guide.<\/p>\n<p>I have a pipeline that runs without error when I only include the preprocessing, training, evaluation, and condition steps. When I add the register step:<\/p>\n<pre><code># Package evaluation metrics into an evaluation report `PropertyFile`\nevaluation_report = PropertyFile(\n        name=&quot;EvaluationReport&quot;, output_name=&quot;evaluation&quot;, path=&quot;evaluation_report.json&quot;\n)\n\n# Create ModelMetrics object using the evaluation report from the evaluation step\n# A ModelMetrics object contains metrics captured from a model.\nmodel_metrics = ModelMetrics(model_statistics=evaluation_report)\n\n# Create a RegisterModel step, which registers the model with Sagemaker Model Registry.\nregister_step = RegisterModel(\n    name=&quot;Foo&quot;,\n    estimator=estimator,\n    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n    content_types=[&quot;text\/csv&quot;],\n    response_types=[&quot;text\/csv&quot;],\n    inference_instances=config[&quot;instance&quot;][&quot;inference&quot;],\n    transform_instances=config[&quot;instance&quot;][&quot;transform&quot;],\n    model_package_group_name=&quot;Bar&quot;,\n    model_metrics=model_metrics,\n    approval_status=&quot;approved&quot;,\n)\n<\/code><\/pre>\n<p>to the condition step's <code>if_steps<\/code>:<\/p>\n<pre><code># Create a Sagemaker Pipelines ConditionStep, using the condition above.\n# Enter the steps to perform if the condition returns True \/ False.\ncond_step = ConditionStep(\n    name=&quot;MSE-Lower-Than-Threshold-Condition&quot;,\n    conditions=[cond_lte],\n    if_steps=[register_step],\n    else_steps=[],\n)\n<\/code><\/pre>\n<p>I get the following trace:<\/p>\n<pre><code>PropertyFile(name='EvaluationReport', output_name='evaluation', path='evaluation_report.json')\nNo finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\nTraceback (most recent call last):\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/pipeline_definition.py&quot;, line 474, in &lt;module&gt;\n    main()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/pipeline_definition.py&quot;, line 466, in main\n    pipeline = define_pipeline()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/pipeline_definition.py&quot;, line 457, in define_pipeline\n    print(json.loads(pipeline.definition()))\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/pipeline.py&quot;, line 257, in definition\n    request_dict = self.to_request()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/pipeline.py&quot;, line 89, in to_request\n    &quot;Steps&quot;: list_to_request(self.steps),\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/utilities.py&quot;, line 37, in list_to_request\n    request_dicts.append(entity.to_request())\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/steps.py&quot;, line 99, in to_request\n    &quot;Arguments&quot;: self.arguments,\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/condition_step.py&quot;, line 87, in arguments\n    IfSteps=list_to_request(self.if_steps),\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/utilities.py&quot;, line 39, in list_to_request\n    request_dicts.extend(entity.request_dicts())\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/step_collections.py&quot;, line 50, in request_dicts\n    return [step.to_request() for step in self.steps]\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/step_collections.py&quot;, line 50, in &lt;listcomp&gt;\n    return [step.to_request() for step in self.steps]\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/steps.py&quot;, line 209, in to_request\n    step_dict = super().to_request()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/steps.py&quot;, line 99, in to_request\n    &quot;Arguments&quot;: self.arguments,\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/_utils.py&quot;, line 423, in arguments\n    model_package_args = get_model_package_args(\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/session.py&quot;, line 4217, in get_model_package_args\n    model_package_args[&quot;model_metrics&quot;] = model_metrics._to_request_dict()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/model_metrics.py&quot;, line 66, in _to_request_dict\n    model_quality[&quot;Statistics&quot;] = self.model_statistics._to_request_dict()\nAttributeError: 'PropertyFile' object has no attribute '_to_request_dict'\n<\/code><\/pre>\n<p>From this trace I see two, potentially related, issues. The immediate issue is the <code>AttributeError: 'PropertyFile' object has no attribute '_to_request_dict'<\/code>. I haven't been able to find any information on why we might be receiving it between forums and Sagemaker documentation.<\/p>\n<p>I also see a sneaky issue towards the top of the trace that has plagued me all day. The line <code>No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config<\/code> tells me that the register step is using our estimator when it should be waiting until after the training step has run. I can't seem to find any reference to this error, besides a somewhat-similar <a href=\"https:\/\/datascience.stackexchange.com\/questions\/100113\/how-to-fix-sagemakers-no-finished-training-job-found-associated-with-this-esti\">stack exchange post<\/a>.<\/p>\n<p>I've compared my code to the AWS-published examples many times and I'm confident that I'm not doing anything taboo. Would anyone be able to shine some light on what these errors are suggesting? Is there any more information or code that would be relevant?<\/p>\n<p>Thanks so much!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1640211128263,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":1006,
        "Owner_creation_time":1619402503747,
        "Owner_last_access_time":1655994422190,
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":36,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I was able to work through the issue! Here is the code for the register step that ended up working with my Tensorflow model:<\/p>\n<pre><code># Package the model\npipeline_model = PipelineModel(models=[model], role=params[&quot;role&quot;].default_value, sagemaker_session=session)\n\n# Create a RegisterModel step, which registers the model with Sagemaker Model Registry.\nregister_step = RegisterModel(\n    name=&quot;Bar&quot;,\n    model=pipeline_model,\n    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n    content_types=[&quot;application\/json&quot;],\n    response_types=[&quot;application\/json&quot;],\n    inference_instances=[config[&quot;instance&quot;][&quot;inference&quot;]],\n    transform_instances=[config[&quot;instance&quot;][&quot;transform&quot;]],\n    model_package_group_name=&quot;Foo&quot;,\n    approval_status=&quot;Approved&quot;,\n)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1642620406233,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70455676",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60892850,
        "Question_title":"Deepracer log_analysis tool - sagemaker role errors",
        "Question_body":"<p>I'm trying to run the Deepracer log analysis tool from <a href=\"https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb<\/a> on my local laptop. However I get below error while trying to run step [5] \"Create an IAM role\". <\/p>\n\n<pre><code>try:\n    sagemaker_role = sagemaker.get_execution_role()\nexcept:\n    sagemaker_role = get_execution_role('sagemaker')\n\nprint(\"Using Sagemaker IAM role arn: \\n{}\".format(sagemaker_role))\n\nCouldn't call 'get_role' to get Role ARN from role name arn:aws:iam::26********:root to get Role path.\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-5-3bea8175b8c7&gt; in &lt;module&gt;\n      1 try:\n----&gt; 2     sagemaker_role = sagemaker.get_execution_role()\n      3 except:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in get_execution_role(sagemaker_session)\n   3302     )\n-&gt; 3303     raise ValueError(message.format(arn))\n   3304 \n\nValueError: The current AWS identity is not a role: arn:aws:iam::26********:root, therefore it cannot be used as a SageMaker execution role\n\nDuring handling of the above exception, another exception occurred:\n\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-5-3bea8175b8c7&gt; in &lt;module&gt;\n      2     sagemaker_role = sagemaker.get_execution_role()\n      3 except:\n----&gt; 4     sagemaker_role = get_execution_role('sagemaker')\n      5 \n      6 print(\"Using Sagemaker IAM role arn: \\n{}\".format(sagemaker_role))\n\nNameError: name 'get_execution_role' is not defined\n<\/code><\/pre>\n\n<p>Does anybody know what needs to be done to execute above code without errors? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1585337019477,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|log-analysis",
        "Question_view_count":491,
        "Owner_creation_time":1404227763680,
        "Owner_last_access_time":1659532526860,
        "Owner_location":"New York, NY, USA",
        "Owner_reputation":1807,
        "Owner_up_votes":303,
        "Owner_down_votes":2,
        "Owner_views":207,
        "Question_last_edit_time":null,
        "Answer_body":"<p>AWS support recommended below solution:<\/p>\n\n<p>This seems to be a known issue when executing the code locally, as mentioned in the following Github issue [3]. A work-around to fix the issue is also defined in that issue [3] and can be referred to using the following link: aws\/sagemaker-python-sdk#300 (comment)<\/p>\n\n<p>The steps in the work-around given in the above link are:<\/p>\n\n<ol>\n<li><p>Login to the AWS console -> IAM -> Roles -> Create Role<\/p><\/li>\n<li><p>Create an IAM role and select the \"SageMaker\" service<\/p><\/li>\n<li><p>Give the role \"AmazonSageMakerFullAccess\" permission<\/p><\/li>\n<li><p>Review and create the role<\/p><\/li>\n<li><p>Next, also attach the \"AWSRoboMakerFullAccess\" permission policy to the above created role (as required in the Github notebook [1]).<\/p><\/li>\n<li><p>The original code would then need to be modified to fetch the IAM role directly when the code is executed on a local machine. The code snippet to be used is given below:<\/p><\/li>\n<\/ol>\n\n<pre><code>try:\n   sagemaker_role = sagemaker.get_execution_role()\n except ValueError:\n   iam = boto3.client('iam')\n   sagemaker_role = iam.get_role(RoleName='&lt;sagemaker-IAM-role-name&gt;')['Role']['Arn']\n<\/code><\/pre>\n\n<p>In the above snippet, replace the \"\" text with the IAM role name created in Step 4.<\/p>\n\n<p>References:<\/p>\n\n<p>[1] <a href=\"https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb<\/a><\/p>\n\n<p>[2] <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-role.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-role.html<\/a><\/p>\n\n<p>[3] aws\/sagemaker-python-sdk#300<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1585760437230,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60892850",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69338516,
        "Question_title":"how to train and deploy YOLOv5 on aws sagemaker",
        "Question_body":"<p>I want to train YOLOv5 on aws sagemaker also deploy the model on sagemaker itself,need to know about entrypoint python script as well. how can I build a pipeline for this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1632686467053,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|yolov5",
        "Question_view_count":2510,
        "Owner_creation_time":1556022524713,
        "Owner_last_access_time":1653309846467,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1632754422357,
        "Answer_body":"<p>This official AWS <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/speed-up-yolov4-inference-to-twice-as-fast-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">blog post<\/a> has information on how to deploy YOLOv4. I wonder if you can use it as a guide and change the model to v5.<\/p>\n<p>If not, there is a 3rd party implementation of YOLOv5 <a href=\"https:\/\/github.com\/HKT-SSA\/yolov5-on-sagemaker\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1632853607047,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69338516",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73751712,
        "Question_title":"Where can I find the Performance Metrics generated by SageMaker Debugger\/Profiler?",
        "Question_body":"<p>Where can I look for the <strong>performance metrics<\/strong> generated by <strong>Amazon SageMaker Debugger\/Profiler<\/strong>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663381119547,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|performancecounter|amazon-sagemaker-debugger",
        "Question_view_count":23,
        "Owner_creation_time":1389887039673,
        "Owner_last_access_time":1664076128463,
        "Owner_location":"Singapore",
        "Owner_reputation":5854,
        "Owner_up_votes":155,
        "Owner_down_votes":70,
        "Owner_views":794,
        "Question_last_edit_time":null,
        "Answer_body":"<p>1.<code>from sagemaker.debugger import ProfilerConfig<\/code><\/p>\n<p><code>profiler_config = ProfilerConfig( framework_profile_params=FrameworkProfile(start_step=1, num_steps=2) )<\/code><\/p>\n<p>2.\n<code>from sagemaker.debugger import TensorBoardOutputConfig<\/code><\/p>\n<p><code>tensorboard_output_config = TensorBoardOutputConfig(s3_output_path= &lt;&lt; add your bucket name an folder &gt;&gt; )<\/code><\/p>\n<ol start=\"3\">\n<li><p>In your estimator - specify :  <code>profiler_config= profiler_config<\/code> and <code>tensorboard_output_config=tensorboard_output_config<\/code><\/p>\n<\/li>\n<li><p>Train your model<\/p>\n<\/li>\n<li><p>Go to the s3 bucket specified  for your training job name that is assigned in Sagemaker . You should see a report under <strong>rule-output<\/strong> &gt; <strong>ProfilerReport<\/strong> *** &gt; <strong>profiler-output\/<\/strong> &gt; <strong>profiler-report.html<\/strong><\/p>\n<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1663457554493,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1663469070623,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73751712",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50508217,
        "Question_title":"Can sagemaker's linear learner be used for multiclass classification?",
        "Question_body":"<p>I am building a multiclass classifier on aws Sagemaker, and would love to use the predefined linearlearner algorithm for classification. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1527161288830,
        "Question_score":1,
        "Question_tags":"algorithm|amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":410,
        "Owner_creation_time":1527161034077,
        "Owner_last_access_time":1555067138630,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes, it is possible now.<\/p>\n\n<p>You can set the predictor_type hyper-parameter to <code>multiclass_classifier<\/code>.<\/p>\n\n<p>See the documentation here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1531423415907,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50508217",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73415182,
        "Question_title":"When do I use a glue job or a Sagemaker Processing job for an etl?",
        "Question_body":"<p>I am currently struggling to decide on what situations in which a glue job is preferable over a sagemaker processing job and vice versa? Some advice on this topic would be greatly appreciated.<\/p>\n<p>I can do the same on both, so why should I bother with the difference?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660903988283,
        "Question_score":2,
        "Question_tags":"amazon-web-services|aws-glue|amazon-sagemaker",
        "Question_view_count":34,
        "Owner_creation_time":1644981356940,
        "Owner_last_access_time":1663945577550,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<ul>\n<li>if you want to use a specific EC2 instance, use SageMaker<\/li>\n<li>Pricing: SageMaker is pro-rated per-second while Glue has minimum charge amount (1min or 10min depending on versions). You should measure how much would a workload cost you on each platform<\/li>\n<li>customization: in SageMaker Processing you can customize the execution environment, as you provide a Docker image (you could run more than Spark\/Python, such as C++ or R)<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1660951371720,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73415182",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68143997,
        "Question_title":"Automate Docker Run command on Sagemaker's Notebook Instance",
        "Question_body":"<p>I have a Docker image in AWS ECR and I open my Sagemaker Notebook instance---&gt;go to terminal--&gt;docker run....\nThis is how I start my Docker container.<\/p>\n<p>Now, I want to automate this process(running my docker image on Sagemaker Notebook Instance) instead of typing the docker run commands.<\/p>\n<p>Can I create a cron job on Sagemaker? or Is there any other approach?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1624722520977,
        "Question_score":0,
        "Question_tags":"amazon-web-services|docker|containers|amazon-sagemaker",
        "Question_view_count":393,
        "Owner_creation_time":1497621837833,
        "Owner_last_access_time":1663656157060,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":230,
        "Owner_up_votes":11,
        "Owner_down_votes":4,
        "Owner_views":27,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For this you can create an inline Bash shell in your SageMaker notebook as follows. This will take your Docker container, create the image, ECR repo if it does not exist and push the image.<\/p>\n<pre><code>%%sh\n\n# Name of algo -&gt; ECR\nalgorithm_name=your-algo-name\n\ncd container #your directory with dockerfile and other sm components\n\nchmod +x randomForest-Petrol\/train #train file for container\nchmod +x randomForest-Petrol\/serve #serve file for container\n\naccount=$(aws sts get-caller-identity --query Account --output text)\n\n# Region, defaults to us-west-2\nregion=$(aws configure get region)\nregion=${region:-us-west-2}\n\nfullname=&quot;${account}.dkr.ecr.${region}.amazonaws.com\/${algorithm_name}:latest&quot;\n\n# If the repository doesn't exist in ECR, create it.\naws ecr describe-repositories --repository-names &quot;${algorithm_name}&quot; &gt; \/dev\/null 2&gt;&amp;1\n\nif [ $? -ne 0 ]\nthen\n    aws ecr create-repository --repository-name &quot;${algorithm_name}&quot; &gt; \/dev\/null\nfi\n\n# Get the login command from ECR and execute it directly\naws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n\n# Build the docker image locally with the image name and then push it to ECR\n# with the full name.\n\ndocker build  -t ${algorithm_name} .\ndocker tag ${algorithm_name} ${fullname}\n\ndocker push ${fullname}\n<\/code><\/pre>\n<p>I am contributing this on behalf of my employer, AWS. My contribution is licensed under the MIT license. See here for a more detailed explanation\n<a href=\"https:\/\/aws-preview.aka.amazon.com\/tools\/stackoverflow-samples-license\/\" rel=\"nofollow noreferrer\">https:\/\/aws-preview.aka.amazon.com\/tools\/stackoverflow-samples-license\/<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1626977403510,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1626979264577,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68143997",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63145277,
        "Question_title":"Sagemaker Training Job Not Uploading\/Saving Training Model to S3 Output Path",
        "Question_body":"<p>Ok I've been dealing with this issue in Sagemaker for almost a week and I'm ready to pull my hair out. I've got a custom training script paired with a data processing script in a BYO algorithm Docker deployment type scenario. It's a Pytorch model built with Python 3.x, and the BYO Docker file was originally built for Python 2, but I can't see an issue with the problem that I am having.....which is that after a successful training run Sagemaker doesn't save the model to the target S3 bucket.<\/p>\n<p>I've searched far and wide and can't seem to find an applicable answer anywhere. This is all done inside a Notebook instance. Note: I am using this as a contractor and don't have full permissions to the rest of AWS, including downloading the Docker image.<\/p>\n<p>Dockerfile:<\/p>\n<pre><code>FROM ubuntu:18.04\n\nMAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python-pip \\\n         python3-pip3\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\nRUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python3 get-pip.py &amp;&amp; \\\n    pip3 install future numpy torch scipy scikit-learn pandas flask gevent gunicorn &amp;&amp; \\\n        rm -rf \/root\/.cache\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=&quot;\/opt\/program:${PATH}&quot;\n\nCOPY decision_trees \/opt\/program\nWORKDIR \/opt\/program\n<\/code><\/pre>\n<p>Docker Image Build:<\/p>\n<pre><code>%%sh\n\nalgorithm_name=&quot;name-this-algo&quot;\n\ncd container\n\nchmod +x decision_trees\/train\nchmod +x decision_trees\/serve\n\naccount=$(aws sts get-caller-identity --query Account --output text)\n\nregion=$(aws configure get region)\nregion=${region:-us-east-2}\n\nfullname=&quot;${account}.dkr.ecr.${region}.amazonaws.com\/${algorithm_name}:latest&quot;\n\naws ecr describe-repositories --repository-names &quot;${algorithm_name}&quot; &gt; \/dev\/null 2&gt;&amp;1\n\nif [ $? -ne 0 ]\nthen\n    aws ecr create-repository --repository-name &quot;${algorithm_name}&quot; &gt; \/dev\/null\nfi\n\n# Get the login command from ECR and execute it directly\n$(aws ecr get-login --region ${region} --no-include-email)\n\n# Build the docker image locally with the image name and then push it to ECR\n# with the full name.\n\ndocker build  -t ${algorithm_name} .\ndocker tag ${algorithm_name} ${fullname}\n\ndocker push ${fullname}\n<\/code><\/pre>\n<p>Env setup and session start:<\/p>\n<pre><code>common_prefix = &quot;pytorch-lstm&quot;\ntraining_input_prefix = common_prefix + &quot;\/training-input-data&quot;\nbatch_inference_input_prefix = common_prefix + &quot;\/batch-inference-input-data&quot;\n\nimport os\nfrom sagemaker import get_execution_role\nimport sagemaker as sage\n\nsess = sage.Session()\n\nrole = get_execution_role()\nprint(role)\n<\/code><\/pre>\n<p>Training Directory, Image, and Estimator Setup, then a <code>fit<\/code> call:<\/p>\n<pre><code>TRAINING_WORKDIR = &quot;a\/local\/directory&quot;\n\ntraining_input = sess.upload_data(TRAINING_WORKDIR, key_prefix=training_input_prefix)\nprint (&quot;Training Data Location &quot; + training_input)\n\naccount = sess.boto_session.client('sts').get_caller_identity()['Account']\nregion = sess.boto_session.region_name\nimage = '{}.dkr.ecr.{}.amazonaws.com\/image-that-works:working'.format(account, region)\n\ntree = sage.estimator.Estimator(image,\n                       role, 1, 'ml.p2.xlarge',\n                       output_path=&quot;s3:\/\/sagemaker-directory-that-definitely\/exists&quot;,\n                       sagemaker_session=sess)\n\ntree.fit(training_input)\n<\/code><\/pre>\n<p>The above script is working, for sure. I have print statements in my script and they are printing the expected results to the console. This runs as it's supposed to, finishes up, and says that it's deploying model artifacts when IT DEFINITELY DOES NOT.<\/p>\n<p>Model Deployment:<\/p>\n<pre><code>model = tree.create_model()\npredictor = tree.deploy(1, 'ml.m4.xlarge')\n<\/code><\/pre>\n<p>This throws an error that the model can't be found. A call to <code>aws sagemaker describe-training-job<\/code> shows that the training was completed but I found that the time it took to upload the model was super fast, so obviously there's an error somewhere and it's not telling me. Thankfully it's not just uploading it to the aether.<\/p>\n<pre><code>{\n            &quot;Status&quot;: &quot;Uploading&quot;,\n            &quot;StartTime&quot;: 1595982984.068,\n            &quot;EndTime&quot;: 1595982989.994,\n            &quot;StatusMessage&quot;: &quot;Uploading generated training model&quot;\n        },\n<\/code><\/pre>\n<p>Here's what I've tried so far:<\/p>\n<ol>\n<li>I've tried uploading it to a different bucket. I figured my permissions were the problem so I pointed it to one that I new allowed me to upload as I had done it before to that bucket. No dice.<\/li>\n<li>I tried backporting the script to Python 2.x, but that caused more problems than it probably would have solved, and I don't really see how that would be the problem anyways.<\/li>\n<li>I made sure the Notebook's IAM role has sufficient permissions, and it does have a SagemakerFullAccess policy<\/li>\n<\/ol>\n<p>What bothers me is that there's no error log I can see. If I could be directed to that I would be happy too, but if there's some hidden Sagemaker kungfu that I don't know about I would be forever grateful.<\/p>\n<hr \/>\n<p>EDIT<\/p>\n<p>The training job runs and prints to both the Jupyter cell and CloudWatch as expected. I've since lost the cell output in the notebook but below is the last few lines in CloudWatch. The first number is the epoch and the rest are various custom model metrics.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/y3I9L.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/y3I9L.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1595988324557,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|pytorch|amazon-sagemaker",
        "Question_view_count":2438,
        "Owner_creation_time":1360536048187,
        "Owner_last_access_time":1663951121943,
        "Owner_location":"Miami, FL, USA",
        "Owner_reputation":45,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Question_last_edit_time":1596039837757,
        "Answer_body":"<p>Can you verify from the training job logs that your training script is running? It doesn't look like your Docker image would respond to the command <code>train<\/code>, which is what SageMaker requires, and so I suspect that your model isn't actually getting trained\/saved to <code>\/opt\/ml\/model<\/code>.<\/p>\n<p>AWS documentation about how SageMaker runs the Docker container: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html<\/a><\/p>\n<p>edit: summarizing from the comments below - the training script must also save the model to <code>\/opt\/ml\/model<\/code> (the model isn't saved automatically).<\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_time":1596039270563,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1619730108173,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63145277",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63960011,
        "Question_title":"Using AWS Sagemaker for model performance without creating endpoint",
        "Question_body":"<p>I've been using Amazon Sagemaker Notebooks to build a pytorch model for an NLP task.\nI know you can use Sagemaker to train, deploy, hyper parameter tuning, and model monitoring.<\/p>\n<p>However, it looks like you have to create an inference endpoint in order to monitor the model's inference performance.<\/p>\n<p>I already have a EC2 instance setup to perform inference tasks on our model, which is currently on a development box and rather not use an endpoint to make<\/p>\n<p>Is it possible to use Sagemaker to train, run hyperparam tuning and model eval without creating an endpoint.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1600448783673,
        "Question_score":1,
        "Question_tags":"pytorch|amazon-sagemaker",
        "Question_view_count":494,
        "Owner_creation_time":1285379271580,
        "Owner_last_access_time":1649074179223,
        "Owner_location":null,
        "Owner_reputation":625,
        "Owner_up_votes":174,
        "Owner_down_votes":11,
        "Owner_views":110,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If you don't want to keep an inference endpoint up, one option is to use SageMaker Processing to run a job that takes your trained model and test dataset as input, performs inference and computes evaluation metrics, and saves them to S3 in a JSON file.<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">This Jupyter notebook example<\/a> steps through (1) preprocessing training and test data, (2) training a model, then (3) evaluating the model<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1604012509390,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63960011",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60163614,
        "Question_title":"What is correct input for mxnet's linear learner in AWS SageMaker?",
        "Question_body":"<p>I am trying to create a simple linear learner in AWS SageMaker with MXNet. I have never worked with SageMaker or MXNet previously. Fitting the model gives runtime error as follows and shuts the instance:<\/p>\n\n<blockquote>\n  <p>UnexpectedStatusException: Error for Training job\n  linear-learner-2020-02-11-06-13-22-712: Failed. Reason: ClientError:\n  Unable to read data channel 'train'. Requested content-type is\n  'application\/x-recordio-protobuf'. Please verify the data matches the\n  requested content-type. (caused by MXNetError)<\/p>\n<\/blockquote>\n\n<p>I think that the data should be converted to protobuf format before passing as training data. Could someone please explain to me what is the correct format for MXNet models? What is the best way to convert a simple data frame into protobuf?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1581404906080,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker|mxnet",
        "Question_view_count":617,
        "Owner_creation_time":1490866954400,
        "Owner_last_access_time":1664018123747,
        "Owner_location":"Ahmedabad, Gujarat, India",
        "Owner_reputation":834,
        "Owner_up_votes":118,
        "Owner_down_votes":11,
        "Owner_views":91,
        "Question_last_edit_time":null,
        "Answer_body":"<p><a href=\"https:\/\/github.com\/awslabs\/fraud-detection-using-machine-learning\/blob\/master\/source\/notebooks\/sagemaker_fraud_detection.ipynb\" rel=\"nofollow noreferrer\">This end-to-end demo<\/a> shows usage of Linear Learner from input data pre-processed in <code>pandas<\/code> dataframes and then converted to protobuf using the SDK. But note that:<\/p>\n\n<ul>\n<li>There is no need to use protobuf, you can also pass csv data with the target variable on the first column of the files, as <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner.html#ll-input_output\" rel=\"nofollow noreferrer\">indicated here<\/a>.<\/li>\n<li>There is no need to know MXNet in order to use the SageMaker Linear Learner, just use the SDK of your choice, bring data to S3, and orchestrate training and inference :)<\/li>\n<\/ul>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1581411605430,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60163614",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52359397,
        "Question_title":"AWS-ML: How to deploy\/setup my own ML algorithms on AWS platform as pay-to-use API?",
        "Question_body":"<p>The title sums it up. Essentially, I'd like to offer my own closed-source proprietary ML algorithms to Amazon AWS customers on a pay-to-use basis API - e.g., sales volumes prediction algorithm service licensed monthly or annually or per call. Most information found talks about how to build and give it away, or use it internally within one's company, but I'm looking to offer it to the public as a commercial offering on AWS.<\/p>\n\n<p>Thank you in advance for your help - links to articles, help pages, or direct steps on how to do this.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1537142950110,
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-machine-learning",
        "Question_view_count":167,
        "Owner_creation_time":1537030909473,
        "Owner_last_access_time":1554190731110,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":1537210523387,
        "Answer_body":"<p>Allow me please to answer my own question. Although not a 100% what I was hoping for, there's certainly support for this in the platform which is great to see: <a href=\"https:\/\/docs.aws.amazon.com\/marketplace\/latest\/userguide\/saas-products.html\" rel=\"nofollow noreferrer\">Software-as-a-Service-Based Products<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1537318983690,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52359397",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67361483,
        "Question_title":"AWS Sagemaker Studio, cannot load pickle files",
        "Question_body":"<p>I'm a newbie in Sagemaker and i'm trying to load a pickle dataset into sagemaker notebook.\nI'm using the Python 3 (Data Science) kernel and ml.t3.medium instance.\nEither i load the pickle from S3 or I upload it directly from the studio like this:<\/p>\n<pre><code>import pickle5\nwith open('filename', 'rb') as f:\n    x = pickle.load(f)\n<\/code><\/pre>\n<p><strong>I get this Error:<\/strong><\/p>\n<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n\/opt\/conda\/lib\/python3.7\/site-packages\/IPython\/core\/formatters.py in __call__(self, obj)\n    700                 type_pprinters=self.type_printers,\n    701                 deferred_pprinters=self.deferred_printers)\n--&gt; 702             printer.pretty(obj)\n    703             printer.flush()\n    704             return stream.getvalue()\n\n..................... more errors here\n\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pandas\/core\/generic.py in __getattr__(self, name)\n   5268             or name in self._accessors\n   5269         ):\n-&gt; 5270             return object.__getattribute__(self, name)\n   5271         else:\n   5272             if self._info_axis._can_hold_identifiers_and_holds_name(name):\n\npandas\/_libs\/properties.pyx in pandas._libs.properties.AxisProperty.__get__()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pandas\/core\/generic.py in __getattr__(self, name)\n   5268             or name in self._accessors\n   5269         ):\n-&gt; 5270             return object.__getattribute__(self, name)\n   5271         else:\n   5272             if self._info_axis._can_hold_identifiers_and_holds_name(name):\n\nAttributeError: 'DataFrame' object has no attribute '_data'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619992853797,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|pickle|amazon-sagemaker",
        "Question_view_count":237,
        "Owner_creation_time":1553704286213,
        "Owner_last_access_time":1663944110353,
        "Owner_location":null,
        "Owner_reputation":27,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Can you check your Pandas versions? This error typically occurs when the pickled file was written in an old Pandas version. Your Sagemaker notebook probably runs Pandas &gt; 1.1 where as the Pandas in which the dataframe was pickled is probably &lt; 1.1<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620025682633,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67361483",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52317237,
        "Question_title":"Machine Learning (NLP) on AWS. Cloud9? SageMaker? EC2-AMI?",
        "Question_body":"<p>I have finally arrived in the cloud to put my NLP work to the next level, but I am a bit overwhelmed with all the possibilities I have. So I am coming to you for advice.<\/p>\n\n<p>Currently I see three possibilities:<\/p>\n\n<ul>\n<li><strong>SageMaker<\/strong>\n\n<ul>\n<li>Jupyter Notebooks are great<\/li>\n<li>It's quick and simple<\/li>\n<li>saves a lot of time spent on managing everything, you can very easily get the model into production<\/li>\n<li>costs more<\/li>\n<li>no version control<\/li>\n<\/ul><\/li>\n<li><strong>Cloud9<\/strong><\/li>\n<li><strong>EC2(-AMI)<\/strong><\/li>\n<\/ul>\n\n<p>Well, that's where I am for now. I really like SageMaker, although I don't like the lack of version control (at least I haven't found anything for now).<\/p>\n\n<p>Cloud9 seems just to be an IDE to an EC2 instance.. I haven't found any comparisons of Cloud9 vs SageMaker for Machine Learning. Maybe because Cloud9 is not advertised as an ML solution. But it seems to be an option.<\/p>\n\n<p>What is your take on that question? What have I missed? What would you advise me to go for? What is your workflow and why? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1536853194713,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-ec2|nlp|amazon-sagemaker|aws-cloud9",
        "Question_view_count":1442,
        "Owner_creation_time":1473262595243,
        "Owner_last_access_time":1546872126263,
        "Owner_location":"Amsterdam, Netherlands",
        "Owner_reputation":23,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1540233603017,
        "Answer_body":"<blockquote>\n  <p>I am looking for an easy work environment where I can quickly test my models, exactly. And it won't be only me working on it, it's a team effort. <\/p>\n<\/blockquote>\n\n<p>Since you are working as a team I would recommend to use sagemaker with custom docker images. That way you have complete freedom over your algorithm. The docker images are stored in ecr. Here you can upload many versions of the same image and tag them to keep control of the different versions(which you build from a git repo).<\/p>\n\n<p>Sagemaker also gives the execution role to inside the docker image. So you still have full access to other aws resources (if the execution role has the right permissions)<\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a>\nIn my opinion this is a good example to start because it shows how sagemaker is interacting with your image.<\/p>\n\n<p><strong>Some notes on other solutions:<\/strong><\/p>\n\n<p>The problem of every other solution you posted is you want to build and execute on the same machine. Sure you can do this but keep in mind, that gpu instances are expensive and therefore you might only switch to the cloud when the code is ready to run.<\/p>\n\n<p><strong>Some other notes<\/strong><\/p>\n\n<ul>\n<li><p>Jupyter Notebooks in general are not made for collaborative programming. I think they want to change this with jupyter lab but this is still in development and sagemaker only use the notebook at the moment.<\/p><\/li>\n<li><p>EC2 is cheaper as sagemaker but you have  to do more work. Especially if you want to run your model as docker images. Also with sagemaker you can easily  build an endpoint for model inference which would be even more complex to realize with ec2.<\/p><\/li>\n<li><p>Cloud 9 I never used this service and but on first glance it seems good to develop on, but the question remains if you want to do this on a gpu machine. Because you're using ec2 as instance you have the same advantage\/disadvantage.<\/p><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1536933417933,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52317237",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65884046,
        "Question_title":"AWS Eventbridge Events (Sagemaker training job status change) fired multiple times with the same payload",
        "Question_body":"<p>I created an event rule for the Sagemaker training job state change in cloudwatch to monitor my training jobs. Then I use this events to trigger a lambda function that send messages in a telegram group as a bot. In this way I receive a message every time one of the training job change its status. It works but there is a problem with the events, they are fired multiple times with the same exact payload, so I receive tons of duplicate messages.\nSince all the payploads are identical (except the field <code>LastModifiedTime<\/code>) I cannot filter them in the lambda. Unfortunately I don't have the AWS Developer plan so I cannot receive support from Amazon. Any idea?<\/p>\n<p><strong>EDIT<\/strong><\/p>\n<p>There are no duplicate rules\/events. I also noticed that enabling the Sagemaker profiler (which is now by default) cause the number of identical rule invocations literally explode. All of them have the same payload except for the <code>LastModifiedTime<\/code> so I suspect that there is a bug in AWS for that. One solution could be to implement some sort of data retention on the lambda and check if an invocation has already been processed, but I don't want complicate a thing that should be very simple. Just tried to launch a new training job and got this sequence (I only report the fields I parse):<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Launching requested ML instances<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Starting the training job<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Starting the training job<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Starting the training job<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Preparing the instances for training<\/p>\n<p>Status: InProgress\nSecondary Status: Downloading\nStatus Message: Downloading input data<\/p>\n<p>Status: InProgress\nSecondary Status: Training\nStatus Message: Downloading the training image<\/p>\n<p>Status: InProgress\nSecondary Status: Training\nStatus Message: Training in-progres<\/p>\n<p>Status: InProgress\nSecondary Status: Training\nStatus Message: Training image download completed. Training in progress<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1611574816563,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|aws-event-bridge",
        "Question_view_count":780,
        "Owner_creation_time":1416346350293,
        "Owner_last_access_time":1664039219437,
        "Owner_location":"Jesi, Italy",
        "Owner_reputation":2302,
        "Owner_up_votes":51,
        "Owner_down_votes":4,
        "Owner_views":227,
        "Question_last_edit_time":1613131797383,
        "Answer_body":"<p>After a lot of experiments I can answer myself that Sagemaker generates multiple events with the same payload, except for the field <code>LastModifiedTime<\/code>. I don't know is this is a bug, but should not happen in my opinion. These are rules defined by AWS itself, so nothing I can customize. The situation is even worse if you enable the profiler.\nThere is nothing I can do, since I already posted on the official AWS forum multiple times without any luck.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1617006916700,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65884046",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63361229,
        "Question_title":"How do you write lifecycle configurations for SageMaker on windows?",
        "Question_body":"<p>I'm trying to set up a startup lifecycle configuration for a SageMaker sketchbook (which just ends up being a .sh file), and it seems like, regardless of what I do, my notebooks timeout on startup. I simplified everything as much as possible, to the point of commenting out all but <code>#!\/bin\/bash<\/code>, and I still get a timeout. Checking cloudwatch this shows up in the log:<\/p>\n<pre><code>\/bin\/bash: \/tmp\/OnStart_2020-08-11-07-01jgfhhkwa: \/bin\/bash^M: bad interpreter: No such file or directory\n<\/code><\/pre>\n<p>through testing, I also found that if I add a carriage return before <code>#!\/bin\/bash<\/code> I get this in the log:<\/p>\n<pre><code>\/tmp\/OnStart_2020-08-11-06-444y3fobzp: line 1: $'\\r': command not found\n<\/code><\/pre>\n<p>based on <a href=\"https:\/\/askubuntu.com\/questions\/966488\/how-do-i-fix-r-command-not-found-errors-running-bash-scripts-in-wsl\">this on the \\r error<\/a>, and <a href=\"https:\/\/stackoverflow.com\/questions\/14219092\/bash-script-and-bin-bashm-bad-interpreter-no-such-file-or-directory\">this on the ^M error<\/a>, this seems to be an incompatibility between windows and unix formatted text. However, I'm editing the lifecycle configuration through aws on my windows machine:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/A5oiU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/A5oiU.png\" alt=\"screenshot of editing the lifecycle config\" \/><\/a><\/p>\n<p>is there some way that I can edit this field on my windows machine through AWS, but it be properly written in unix on the other end?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597159003890,
        "Question_score":0,
        "Question_tags":"linux|windows|bash|amazon-web-services|amazon-sagemaker",
        "Question_view_count":608,
        "Owner_creation_time":1545360696800,
        "Owner_last_access_time":1664070875743,
        "Owner_location":"Earth",
        "Owner_reputation":1011,
        "Owner_up_votes":218,
        "Owner_down_votes":5,
        "Owner_views":93,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This is, indeed, to do with special character representation in different os' based on <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/issues\/8\" rel=\"nofollow noreferrer\">this<\/a> you can use notepad++ to easily convert the dos representation a unix representation, then just &quot;paste as plain text&quot;, and it works fine<\/p>\n<ul>\n<li>copy to notepad++ view<\/li>\n<li>show symbol<\/li>\n<li>show all symbols<\/li>\n<li>replace &quot;\/r&quot; with nothing CRLF should become LF which is valid in unix<\/li>\n<li>copy and paste as plain text<\/li>\n<\/ul>\n<p>Doing this fixed the problem<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1597164941183,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63361229",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57396212,
        "Question_title":"SageMaker and TensorFlow 2.0",
        "Question_body":"<p>What is the best way to run TensorFlow 2.0 with AWS Sagemeker?<\/p>\n\n<p>As of today (Aug 7th, 2019) AWS does not provide TensorFlow 2.0 <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"noreferrer\">SageMaker containers<\/a>, so my understanding is that I need to build my own.<\/p>\n\n<p>What is the best Base image to use? Example Dockerfile?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":1,
        "Question_creation_time":1565186451297,
        "Question_score":13,
        "Question_tags":"tensorflow|amazon-sagemaker|tensorflow2.0",
        "Question_view_count":4136,
        "Owner_creation_time":1446859510543,
        "Owner_last_access_time":1664052243983,
        "Owner_location":"Toronto, Canada",
        "Owner_reputation":3259,
        "Owner_up_votes":104,
        "Owner_down_votes":7,
        "Owner_views":233,
        "Question_last_edit_time":1565186790803,
        "Answer_body":"<p>EDIT: <strong>Amazon SageMaker does now support TF 2.0 and higher.<\/strong><\/p>\n<ul>\n<li>SageMaker + TensorFlow docs: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html<\/a><\/li>\n<li>Supported Tensorflow versions (and Docker URIs): <a href=\"https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images<\/a><\/li>\n<\/ul>\n<hr \/>\n<p><em>Original answer<\/em><\/p>\n<p>Here is an example Dockerfile that uses <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"nofollow noreferrer\">the underlying SageMaker Containers library<\/a> (this is what is used in the official pre-built Docker images):<\/p>\n<pre><code>FROM tensorflow\/tensorflow:2.0.0b1\n\nRUN pip install sagemaker-containers\n\n# Copies the training code inside the container\nCOPY train.py \/opt\/ml\/code\/train.py\n\n# Defines train.py as script entrypoint\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>For more information on this approach, see <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-container-to-train-script-get-started.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-container-to-train-script-get-started.html<\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1567635751827,
        "Answer_score":10.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1595008443227,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57396212",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60037376,
        "Question_title":"Can't use Keras CSVLogger callbacks in Sagemaker script mode. It fails to write the log file on S3 ( error - No such file or directory )",
        "Question_body":"<p>I have this script where I want to get the callbacks to a separate CSV file in sagemaker custom script docker container. But when I try to run in local mode, it fails giving the following error. I have a hyper-parameter tuning job(HPO) to run and this keeps giving me errors. I need to get this local mode run correctly before doing the HPO. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/de522.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/de522.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>In the notebook I use the following code.<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='lstm_model.py', \n                          role=role,\n                          code_location=custom_code_upload_location,\n                          output_path=model_artifact_location+'\/',\n                          train_instance_count=1, \n                          train_instance_type='local',\n                          framework_version='1.12', \n                          py_version='py3',\n                          script_mode=True,\n                          hyperparameters={'epochs': 1},\n                          base_job_name='hpo-lstm-local-test'\n                         )\n\ntf_estimator.fit({'training': training_input_path, 'validation': validation_input_path})\n<\/code><\/pre>\n\n<p>In my <strong>lstm_model.py<\/strong> script the following code is used.<\/p>\n\n<pre><code>lgdir = os.path.join(model_dir, 'callbacks_log.csv')\ncsv_logger = CSVLogger(lgdir, append=True)\n\nregressor.fit(x_train, y_train, batch_size=batch_size,\n              validation_data=(x_val, y_val), \n              epochs=epochs,\n              verbose=2,\n              callbacks=[csv_logger]\n              )\n<\/code><\/pre>\n\n<p>I tried creating a file before hand like shown below using tensorflow backend. But it doesn't create a file. ( K : tensorflow Backend, tf: tensorflow )<\/p>\n\n<pre><code>filename = tf.Variable(lgdir , tf.string)\ncontent = tf.Variable(\"\", tf.string)\nsess = K.get_session()\ntf.io.write_file(filename, content)\n<\/code><\/pre>\n\n<p>I can't use any other packages like pandas to create the file as the TensorFlow docker container in SageMaker for custom scripts doesn't provide them. They give only a limited amount of packages. <\/p>\n\n<p>Is there a way I can write the csv file to the S3 bucket location, before the fit method try to write the callback. Or is that the solution to the problem? I am not sure. <\/p>\n\n<p>If you can even suggest other suggestions to get callbacks, I would even accept that answer. But it should be worth the effort. <\/p>\n\n<p>This docker image is really narrowing the scope. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1580725861867,
        "Question_score":0,
        "Question_tags":"tensorflow|keras|amazon-sagemaker",
        "Question_view_count":412,
        "Owner_creation_time":1517147266417,
        "Owner_last_access_time":1663959762520,
        "Owner_location":null,
        "Owner_reputation":65,
        "Owner_up_votes":89,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Well for starters, you can always make your own docker image using the Tensorflow image as a base. I work in Tensorflow 2.0 so this will be slightly different for you but here is an example of my image pattern:<\/p>\n\n<pre><code># Downloads the TensorFlow library used to run the Python script\nFROM tensorflow\/tensorflow:2.0.0a0 # you would use the equivalent for your TF version\n\n# Contains the common functionality necessary to create a container compatible with Amazon SageMaker\nRUN pip install sagemaker-containers -q \n\n# Wandb allows us to customize and centralize logging while maintaining open-source agility\nRUN pip install wandb -q # here you would install pandas\n\n# Copies the training code inside the container to the design pattern created by the Tensorflow estimator\n# here you could copy over a callbacks csv\nCOPY mnist-2.py \/opt\/ml\/code\/mnist-2.py \nCOPY callbacks.py \/opt\/ml\/code\/callbacks.py \nCOPY wandb_setup.sh \/opt\/ml\/code\/wandb_setup.sh\n\n# Set the login script as the entry point\nENV SAGEMAKER_PROGRAM wandb_setup.sh # here you would instead launch lstm_model.py\n<\/code><\/pre>\n\n<p>I believe you are looking for a pattern similar to this, but I prefer to log all of my model data using <a href=\"https:\/\/www.wandb.com\/\" rel=\"nofollow noreferrer\">Weights and Biases<\/a>. They're a little out of data on their SageMaker integration but I'm actually in the midst of writing an updated tutorial for them. It should certainly be finished this month and include logging and comparing runs from hyperparameter tuning jobs<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1583860547413,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60037376",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62462790,
        "Question_title":"How can I get current job-name in SageMaker training job script?",
        "Question_body":"<p>I write some training job on AWS-SageMaker framework.<\/p>\n\n<p>For some it's requirements, it needs know the job-name of which current running on.<\/p>\n\n<p>I know this code works for it ...<\/p>\n\n<pre><code>import sagemaker_containers\nenv = sagemaker_containers.training_env()\njob_name = env['job_name']\n<\/code><\/pre>\n\n<p>But <code>sagemaker_containers<\/code> package has been deprecated. (I read that on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"nofollow noreferrer\">it's GitHub<\/a>)<\/p>\n\n<p>What should i do?<\/p>\n\n<p>I just started learning about this platform last month. I would appreciate any advice. Thank you.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592537326547,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":753,
        "Owner_creation_time":1401925028590,
        "Owner_last_access_time":1646876872843,
        "Owner_location":null,
        "Owner_reputation":113,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For older containers using the deprecated <code>sagemaker_containers<\/code>, the approach you described is correct.<\/p>\n<p>For newer containers that use <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\" rel=\"nofollow noreferrer\"><code>sagemaker-training-toolkit<\/code><\/a>, this is how you retrieve information about the environment: <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit#get-information-about-the-container-environment\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit#get-information-about-the-container-environment<\/a><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker_training import environment\n\nenv = environment.Environment()\n\njob_name = env[&quot;job_name&quot;]\n<\/code><\/pre>\n<p>You can check the <a href=\"https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/dlc-release-notes.html\" rel=\"nofollow noreferrer\">DLC Release Notes<\/a> to see what's installed in each version.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1593541151550,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62462790",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68251533,
        "Question_title":"Why is AWS Sagemaker notebook instance designed to only persist data under ~\/Sagemaker?",
        "Question_body":"<p>In my current job we use AWS managed notebooks on Sagemaker EC2. I am largely okay with the user experience but the lack of data persistency outside <code>~\/Sagemaker<\/code> has been quite inconvenient. Every time should the instance need restarting, I'd lose all the settings and python packages. Wonder why AWS would make this particular decision for Sagemaker. Have used Google Cloud's AI platform before and it does not have such settings and my configurations would always persist.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625466236780,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":266,
        "Owner_creation_time":1408744280997,
        "Owner_last_access_time":1664070721297,
        "Owner_location":"Vancouver, BC, Canada",
        "Owner_reputation":2758,
        "Owner_up_votes":603,
        "Owner_down_votes":2,
        "Owner_views":122,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I faced a similar issue on other AWS services. Usually for managed services AWS uses read-only containers approach and leave just one folder of the filesystem for read\/write that persist across the stop\/restart cycle.\nReguarding the packages installation the seems to be to install your custom environment on the notebook instance's Amazon EBS volume, as described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1625487892607,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68251533",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56422325,
        "Question_title":"AWS Sagemaker - Either the training channel is empty or the mini-batch size is too high",
        "Question_body":"<p>I am trying to train a linear learner model in Sagemaker. My training set is 422 rows split into 4 files on AWS S3. The mini-batch size that I set is 50. <\/p>\n\n<p>I keep on getting this error in Sagemaker.<\/p>\n\n<blockquote>\n  <p>Customer Error: No training data processed. Either the training\n  channel is empty or the mini-batch size is too high. Verify that\n  training data contains non-empty files and the mini-batch size is less\n  than the number of records per training host.<\/p>\n<\/blockquote>\n\n<p>I am using this InputDataConfig<\/p>\n\n<pre><code>InputDataConfig=[\n            {\n                'ChannelName': 'train',\n                'DataSource': {\n                    'S3DataSource': {\n                        'S3DataType': 'S3Prefix',\n                        'S3Uri': 's3:\/\/MY_S3_BUCKET\/REST_OF_PREFIX\/exported\/',\n                        'S3DataDistributionType': 'FullyReplicated'\n                    }\n                },\n                'ContentType': 'text\/csv',\n                'CompressionType': 'Gzip'\n            }\n        ],\n<\/code><\/pre>\n\n<p>I am not sure what I am doing wrong here. I tried increasing the number of records to 5547495 split across 6 files. The same error. That makes me think that somehow the config itself has something missing. Due to which it seems to think training channel is just not present. I tried changing 'train' to 'training' as that is what the erorr message is saying. But then I got <\/p>\n\n<blockquote>\n  <p>Customer Error: Unable to initialize the algorithm. Failed to validate\n  input data configuration. (caused by ValidationError)<\/p>\n  \n  <p>Caused by: {u'training': {u'TrainingInputMode': u'Pipe',\n  u'ContentType': u'text\/csv', u'RecordWrapperType': u'None',\n  u'S3DistributionType': u'FullyReplicated'}} is not valid under any of\n  the given schemas<\/p>\n<\/blockquote>\n\n<p>I went back to train as that seems to be what is needed. But what am I doing wrong with that? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559544607823,
        "Question_score":0,
        "Question_tags":"boto3|amazon-sagemaker",
        "Question_view_count":599,
        "Owner_creation_time":1364896706347,
        "Owner_last_access_time":1664082840050,
        "Owner_location":"Noida, India",
        "Owner_reputation":6584,
        "Owner_up_votes":477,
        "Owner_down_votes":15,
        "Owner_views":962,
        "Question_last_edit_time":1559545639143,
        "Answer_body":"<p>Found the problem. The CompressionType was mentioned as 'Gzip' but I had changed the actual file to be not compressed when doing the exports. As soon as I changed it to be 'None' the training went smoothly.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1559566976623,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56422325",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66692579,
        "Question_title":"AWS SageMaker: PermissionError: Access Denied - Reading data from S3 bucket",
        "Question_body":"<p>I am using AWS SageMaker. I already used it before and I had no problems reading data from an S3 bucket.\nSo, I set up a new notebook instance and id this:<\/p>\n<pre><code>from sagemaker import get_execution_role\nrole = get_execution_role()\n\nbucket='my-bucket'\n\ndata_key = 'myfile.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n\ndf = pd.read_csv(data_location)\n<\/code><\/pre>\n<p>What I got is this:<\/p>\n<pre><code>PermissionError: Access Denied\n<\/code><\/pre>\n<p>Note: I checked the IAM Roles and also the policies and it seems to me that I have all the necessary rights to access the S3 bucket (AmazonS3FullAccess etc. are granted). What is different from the situation before is that my data is encrypted. Is there something I have to set up besides the roles?<\/p>\n<p>Edit:<\/p>\n<p>The role I use consist of three policies. These are<\/p>\n<ul>\n<li>AmazonS3FullAccess<\/li>\n<li>AmazonSageMakerFullAccess<\/li>\n<\/ul>\n<p>and an Execution Role where I added kms:encrypt and kms:decrypt. It looks like this one:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;xyz&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:PutObject&quot;,\n                &quot;s3:GetObject&quot;,\n                &quot;s3:ListBucket&quot;,\n                &quot;s3:DeleteObject&quot;,\n                &quot;kms:Encrypt&quot;,\n                &quot;kms:Decrypt&quot;\n            ],\n            &quot;Resource&quot;: &quot;arn:aws:s3:::*&quot;\n        }\n    ]\n}\n<\/code><\/pre>\n<p>Is there something missing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1616075760093,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|error-handling|amazon-sagemaker",
        "Question_view_count":1327,
        "Owner_creation_time":1559131080073,
        "Owner_last_access_time":1663924217557,
        "Owner_location":null,
        "Owner_reputation":1166,
        "Owner_up_votes":555,
        "Owner_down_votes":1,
        "Owner_views":248,
        "Question_last_edit_time":1616077230863,
        "Answer_body":"<p>You need to add (or modify) an IAM policy to grant access to the key the bucket uses for its encryption:<\/p>\n<pre><code>{\n  &quot;Sid&quot;: &quot;KMSAccess&quot;,\n  &quot;Action&quot;: [\n    &quot;kms:Decrypt&quot;\n  ],\n  &quot;Effect&quot;: &quot;Allow&quot;,\n  &quot;Resource&quot;: &quot;arn:aws:kms:example-region-1:123456789098:key\/111aa2bb-333c-4d44-5555-a111bb2c33dd&quot;\n}\n<\/code><\/pre>\n<p>Alternatively you can change the key policy of the KMS key directly to grant the sagemaker role access directly. <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/s3-bucket-access-default-encryption\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/s3-bucket-access-default-encryption\/<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1616081046907,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66692579",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58325923,
        "Question_title":"Deepar Prediction Quantiles Explained",
        "Question_body":"<p>I am working with Deepar and trying to get a better understanding of the quantile values returned. From the documentation, the likelihood hyperparameter explains that: <code>...provide quantiles of the distribution and return samples<\/code>. <\/p>\n\n<p>If I look at a single data point the quantiles returned are linear. E.g. the 0.1 quantile has the lowest predicted value and 0.9 quantile has the highest predicted value. I am having trouble understanding this. If these are samples from the distribution, shouldn't they look similar to the distribution selected with the likelihood hyperparameter (negative-binomial in my case)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1570719704473,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":568,
        "Owner_creation_time":1457123770467,
        "Owner_last_access_time":1663868088727,
        "Owner_location":"Denver",
        "Owner_reputation":246,
        "Owner_up_votes":39,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":"<p>DeepAR returns probabilistic forecasts in terms of quantiles: by default, the 0.1, 0.2, 0.3, ..., 0.9 quantiles are returned. This means that, according to the model, in each future time step you have 10% chance of observing something lower than the 0.1 quantile, 20% chance of observing something lower than the 0.2 quantile, and so on. Quantiles are in fact in order, and they must be by definition of quantile. Hope this clarifies is a bit!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1573131591227,
        "Answer_score":4.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58325923",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70248817,
        "Question_title":"Shared input in Sagemaker inference pipeline models",
        "Question_body":"<p>I'm deploying a SageMaker inference pipeline composed of two PyTorch models (<code>model_1<\/code> and <code>model_2<\/code>), and I am wondering if it's possible to pass the same input to both the models composing the pipeline.<\/p>\n<p>What I have in mind would work more or less as follows<\/p>\n<ol>\n<li><p>Invoke the endpoint sending a binary encoded payload (namely <code>payload_ser<\/code>), for example:<\/p>\n<pre><code>client.invoke_endpoint(EndpointName=ENDPOINT,\n                       ContentType='application\/x-npy',\n                       Body=payload_ser)\n<\/code><\/pre>\n<\/li>\n<li><p>The first model parses the payload with <code>inut_fn<\/code> function, runs the predictor on it, and returns the output of the predictor. As a simplified example:<\/p>\n<pre><code>def input_fn(request_body, request_content_type):\n    if request_content_type == &quot;application\/x-npy&quot;:\n        input = some_function_to_parse_input(request_body)\n    return input\n\ndef predict_fn(input_object, predictor):\n    outputs = predictor(input_object)\n    return outputs\n\ndef output_fn(predictions, response_content_type):\n    return json.dumps(predictions)\n<\/code><\/pre>\n<\/li>\n<li><p>The second model gets as payload both the original payload (<code>payload_ser<\/code>) and the output of the previous model (predictions). Possibly, the <code>input_fn<\/code> function would be used to parse the output of model_1 (as in the &quot;standard case&quot;), but I'd need some way to also make the original payload available to model_2.  In this way, model_2 will use both the original payload and the output of model_1 to make the final prediction and return it to whoever invoked the endpoint.<\/p>\n<\/li>\n<\/ol>\n<p>Any idea if this is achievable?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638808672663,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":210,
        "Owner_creation_time":1508881117760,
        "Owner_last_access_time":1663930396547,
        "Owner_location":null,
        "Owner_reputation":157,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Sounds like you need an inference DAG. Amazon SageMaker Inference pipelines currently supports only a chain of handlers, where the output of handler N is the input for handler N+1.<\/p>\n<p>You could change model1's predict_fn() to return both (input_object, outputs), and output_fn(). output_fn() will receive these two objects as the predictions, and will handle serializing both as json. model2's input_fn() will need to know how to parse this pair input.<\/p>\n<p>Consider implementing this as a generic pipeline handling mechanism that adds the input to the model's output. This way you could reuse it for all models and pipelines.<\/p>\n<p>You could allow the model to be deployed as a standalone model, and as a part of a pipeline, and apply the relevant input\/output handling behavior that will be triggered by the presence of an environment variable (<code>Environment<\/code> dict), which you can specify when <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model\" rel=\"nofollow noreferrer\">creating<\/a> the inference pipelines model.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1638812806037,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70248817",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54408673,
        "Question_title":"Getting sagemaker container locally",
        "Question_body":"<p>When I try to run sagemaker locally for tensorflow in script mode. It seems like I cannot pull the docker container. I have ran the code below from a sagemaker notebook instance and everything ran fine. But when running it on my machine it doesn't work.<\/p>\n\n<p>How can I download the container, so I can debug things locally?<\/p>\n\n<pre><code>import os\n\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\n\n\nhyperparameters = {}\nrole = 'arn:aws:iam::xxxxxxxx:role\/yyyyyyy'\nestimator = TensorFlow(\n    entry_point='train.py',\n    source_dir='.',\n    train_instance_type='local',\n    train_instance_count=1,\n    hyperparameters=hyperparameters,\n    role=role,\n    py_version='py3',\n    framework_version='1.12.0',\n    script_mode=True)\n\nestimator.fit()\n<\/code><\/pre>\n\n<p>I get this output<\/p>\n\n<pre><code>INFO:sagemaker:Creating training-job with name: sagemaker-tensorflow-\nscriptmode-2019-01-28-18-51-57-787\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nError response from daemon: pull access denied for 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode, repository does not exist or may require 'docker login'\n\nsubprocess.CalledProcessError: Command 'docker pull 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.12.0-cpu-py3' returned non-zero exit status 1.\n<\/code><\/pre>\n\n<p>The warning looks like the output you get when using the docker login stuff <a href=\"https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/Registries.html\" rel=\"nofollow noreferrer\">here<\/a>. If I follow these steps to register to the directory with tensorflow container it says login success<\/p>\n\n<pre><code>Invoke-Expression -Command (aws ecr get-login --no-include-email --registry-ids 520713654638 --region eu-west-2)\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\n<\/code><\/pre>\n\n<p>But then I still cannot pull it<\/p>\n\n<pre><code>docker pull 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.11.0-cpu-py3\nError response from daemon: pull access denied for 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode, repository does not exist or may require 'docker login'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548702401737,
        "Question_score":2,
        "Question_tags":"amazon-web-services|docker|amazon-sagemaker",
        "Question_view_count":2235,
        "Owner_creation_time":1352206833663,
        "Owner_last_access_time":1664011820603,
        "Owner_location":null,
        "Owner_reputation":893,
        "Owner_up_votes":112,
        "Owner_down_votes":2,
        "Owner_views":185,
        "Question_last_edit_time":null,
        "Answer_body":"<p>the same sequence works for me locally : 'aws ecr get-login', 'docker login', 'docker pull'. <\/p>\n\n<p>Does your local IAM user have sufficient credentials to pull from ECR? The 'AmazonEC2ContainerRegistryReadOnly' policy should be enough: <a href=\"https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/ecr_managed_policies.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/ecr_managed_policies.html<\/a><\/p>\n\n<p>Alternatively, you can grab the container from Github and build it: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1548750603277,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54408673",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68479297,
        "Question_title":"AWS send image to Sagemaker from Lambda: how to set content handling?",
        "Question_body":"<p>similar question to\n<a href=\"https:\/\/stackoverflow.com\/a\/66683538\/6896705\">AWS Lambda send image file to Amazon Sagemaker<\/a><\/p>\n<p>I try to make simple-mnist work (the model was built by referring to <a href=\"https:\/\/sagemaker-immersionday.workshop.aws\/en\/lab3\/option1.html\" rel=\"nofollow noreferrer\">aws tutorial<\/a>)<\/p>\n<p>Then I am using API gateway (REST API w\/ proxy integration) to post image data to lambda, and would like to send it to sagemaker endpoint and make an inference.<\/p>\n<p>In lambda function, I wrote the code(.py) like this.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>runtime = boto3.Session().client('sagemaker-runtime')\n\nendpoint_name = 'tensorflow-training-YYYY-mm-dd-...'\nres = runtime.invoke_endpoint(EndpointName=endpoint_name,\n                              Body=Image,\n                              ContentType='image\/jpeg',\n                              Accept='image\/jpeg')\n<\/code><\/pre>\n<p>However, when I send image to lambda via API gateway, this error occurs.<\/p>\n<blockquote>\n<p>[ERROR] ModelError: An error occurred (ModelError) when calling the\nInvokeEndpoint operation: Received client error (415) from model with\nmessage &quot; {\n&quot;error&quot;: &quot;Unsupported Media Type: image\/jpeg&quot; }<\/p>\n<\/blockquote>\n<p>I think I need to do something referring to <a href=\"https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/api-gateway-payload-encodings.html\" rel=\"nofollow noreferrer\">Working with binary media types for REST APIs\n<\/a><\/p>\n<p>But since I am very new, I have no idea about the appropriate thing to do, on which page (maybe API Gateway page?) or how...<\/p>\n<p>I need some clues to solve this problem. Thank you in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626929018517,
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":390,
        "Owner_creation_time":1475109151950,
        "Owner_last_access_time":1652793680043,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":1626941375923,
        "Answer_body":"<p>Looking <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/adapt-inference-container.html\" rel=\"nofollow noreferrer\">here<\/a> you can see that only some specific content types are supported by default, and images are not in this list. I think you have to either implement your <code>input_fn<\/code> function or adapt your data to one of the supported content types.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1626967022753,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68479297",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54295445,
        "Question_title":"TensorFlow Serving send data as b64 instead of Numpy Array",
        "Question_body":"<p>I have a TensorFlow Serving container in a SageMaker endpoint. I'm able to take a batch of images as a Numpy array and get back predictions like this:<\/p>\n\n<pre><code>import numpy as np\nimport sagemaker\nfrom sagemaker.predictor import json_serializer, json_deserializer\n\nimage = np.random.uniform(low=-1.0, high=1.0, size=(1,128,128,3)).astype(np.float32)    \nimage = {'instances': image}\nimage = json_serializer(image)\n\nrequest_args = {}\nrequest_args['Body'] = image\nrequest_args['EndpointName'] = endpoint_name\nrequest_args['ContentType'] = 'application\/json'\nrequest_args['Accept'] = 'application\/json'\n\n# works successfully\nresponse = sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\nresponse_body = response['Body']\npredictions = json_deserializer(response_body, response['ContentType'])\n<\/code><\/pre>\n\n<p>The size of the <code>request_args<\/code> payload is large doing it this way. I'm wondering, is there a way to send this in a more compressed format? <\/p>\n\n<p>I've tried experimenting with <code>base64<\/code> and <code>json.dumps<\/code>, but can't get past <code>Invalid argument: JSON Value: ...<\/code> errors. Not sure if this isn't supported or if I'm just doing it incorrectly.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548093365813,
        "Question_score":1,
        "Question_tags":"python|image|base64|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":796,
        "Owner_creation_time":1361339272693,
        "Owner_last_access_time":1663965928400,
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I've talked to AWS support about this (see <a href=\"https:\/\/stackoverflow.com\/questions\/54090270\/more-efficient-way-to-send-a-request-than-json-to-deployed-tensorflow-model-in-s\">More efficient way to send a request than JSON to deployed tensorflow model in Sagemaker?<\/a>).<\/p>\n\n<p>They suggest that it is possible to pass in a custom input_fn that will be used by the serving container where one can unpack a compressed format (such as protobuf).<\/p>\n\n<p>I'll be testing this soon and hopefully this stuff works since it would add a lot of flexibility to the input processing.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1548251290417,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54295445",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53154542,
        "Question_title":"AWS SageMaker - Realtime Data Processing",
        "Question_body":"<p>My company does online consumer behavior analysis and we do realtime predictions using the data we collected from various websites (with our java script embedded). <\/p>\n\n<p>We have been using AWS ML for real time prediction but now we are experimenting with AWS SageMaker it occurred to us that the realtime data processing is a problem compared to AWS ML. For example we have some string variables that AWS ML can convert to numerics and use them for real time prediction in AWS ML automatically. But it does not look like SageMaker can do it. <\/p>\n\n<p>Does anyone have any experience with real time data processing and prediction in AWS SageMaker?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1541421249963,
        "Question_score":4,
        "Question_tags":"prediction|amazon-sagemaker",
        "Question_view_count":1508,
        "Owner_creation_time":1481650459980,
        "Owner_last_access_time":1661430875243,
        "Owner_location":null,
        "Owner_reputation":1446,
        "Owner_up_votes":102,
        "Owner_down_votes":1,
        "Owner_views":107,
        "Question_last_edit_time":null,
        "Answer_body":"<p>AWS SageMaker is a robust machine learning service in AWS that manages every major aspect of machine learning implementation, including data preparation, model construction, training and fine-tuning, and deployment.<\/p>\n<p><strong>Preparation<\/strong><\/p>\n<p>SageMaker uses a range of resources to make it simple to prepare data for machine learning models, even though it comes from many sources or is in a variety of formats.<\/p>\n<p>It's simple to mark data, including video, images, and text, that's automatically processed into usable data, with SageMaker Ground Truth. GroundWork will process and merge this data using auto-segmentation and a suite of tools to create a single data label that can be used in machine learning models. AWS, in conjunction with SageMaker Data Wrangler and SageMaker Processing, reduces a data preparation phase that may take weeks or months to a matter of days, if not hours.<\/p>\n<p><strong>Build<\/strong><\/p>\n<p>SageMaker Studio Notebooks centralize everything relevant to your machine learning models, allowing them to be conveniently shared along with their associated data. You can choose from a variety of built-in, open-source algorithms to start processing your data with SageMaker JumpStart, or you can build custom parameters for your machine learning model.<\/p>\n<p>Once you've chosen a model, SageMaker starts processing data automatically and offers a simple, easy-to-understand interface for tracking your model's progress and performance.<\/p>\n<p><strong>Training<\/strong><\/p>\n<p>SageMaker provides a range of tools for training your model from the data you've prepared, including a built-in debugger for detecting possible errors.<\/p>\n<p>Machine Learning\nThe training job's results are saved in an Amazon S3 bucket, where they can be viewed using other AWS services including AWS Quicksight.<\/p>\n<p><strong>Deployment<\/strong><\/p>\n<p>It's pointless to have strong machine learning models if they can't be easily deployed to your hosting infrastructure. Fortunately, SageMaker allows deploying machine learning models to your current services and applications as easy as a single click.<\/p>\n<p>SageMaker allows for real-time data processing and prediction after installation. This has far-reaching consequences in a variety of areas, including finance and health. Businesses operating in the stock market, for example, may make real-time financial decisions about stock and make more attractive acquisitions by pinpointing the best time to buy.<\/p>\n<p>Incorporation with Amazon Comprehend, allows for natural language processing, transforming human speech into usable data to train better models, or provide a chatbot to customers through Amazon Lex.<\/p>\n<p><strong>In conclusion\u2026<\/strong><\/p>\n<p>Machine Learning is no longer a niche technological curiosity; it now plays a critical role in the decision-making processes of thousands of companies around the world. There has never been a better time to start your Machine Learning journey than now, with virtually unlimited frameworks and simple integration into the AWS system.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620131934800,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53154542",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65609804,
        "Question_title":"How to append stepfunction execution id to SageMaker job names?",
        "Question_body":"<p>I have a step function statemachine which creates SageMaker batch transform job, the definition is written in Terraform, I wanted to add the stepfunction execution id to the batch transform job names:<\/p>\n<p>in stepfunction terraform file:<\/p>\n<pre><code>  definition = templatefile(&quot;stepfuntion.json&quot;,\n    {\n      xxxx\n)\n<\/code><\/pre>\n<p>in the &quot;stepfuntion.json&quot;:<\/p>\n<pre><code>{...\n          &quot;TransformJobName&quot;: &quot;jobname-$$.Execution.Id&quot;,\n  \n          }\n      },\n        &quot;End&quot;: true\n      }\n    }\n  }\n<\/code><\/pre>\n<p>But after terraform apply, it didn't generate the actual id, it gave me <code>jobname-$$.Execution.Id<\/code>, can anyone help with this please?<\/p>\n<p>Resources: <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/input-output-contextobject.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/input-output-contextobject.html<\/a>\n&quot;To access the context object, first specify the parameter name by appending .$ to the end, as you do when selecting state input with a path. Then, to access context object data instead of the input, prepend the path with $$.. This tells AWS Step Functions to use the path to select a node in the context object.&quot;<\/p>\n<p>Can someone tell me what I'm missing please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610012100150,
        "Question_score":0,
        "Question_tags":"amazon-web-services|terraform|terraform-provider-aws|amazon-sagemaker|aws-step-functions",
        "Question_view_count":488,
        "Owner_creation_time":1540920956270,
        "Owner_last_access_time":1663875036883,
        "Owner_location":"United Kingdom",
        "Owner_reputation":2385,
        "Owner_up_votes":1007,
        "Owner_down_votes":16,
        "Owner_views":585,
        "Question_last_edit_time":1610014278923,
        "Answer_body":"<p>The var you are trying to use terraform doesn't know about it<\/p>\n<blockquote>\n<p>jobname-$$.Execution.Id.<\/p>\n<\/blockquote>\n<p>That's something specific to the Step function and available within state machine not available for terraform.<\/p>",
        "Answer_comment_count":17.0,
        "Answer_creation_time":1610017101707,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65609804",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61464960,
        "Question_title":"SageMaker multimodel and RandomCutForest",
        "Question_body":"<p>I am trying to invoke a MultiModel Endpoint with a RandomCutForest Model. I receive error though, 'Error loading model'. I can invoke the endpoint with models given from the examples.\nAm I missing something e.g. limitations on what models I can use? <\/p>\n\n<p>For MultiModel inspiration I am using below:<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb<\/a><\/p>\n  \n  <p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/<\/a><\/p>\n<\/blockquote>\n\n<p>I am trying to deploy the outputted 'model.tar.gz' from below RCF example in the MultiModel endpoint:<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb<\/a><\/p>\n<\/blockquote>\n\n<pre><code>model_name = 'model'\nfull_model_name = '{}.tar.gz'.format(model_name)\nfeatures = data\n\nbody = ','.join(map(str, features)) + '\\n'\nresponse = runtime_sm_client.invoke_endpoint(\n                    EndpointName=endpoint_name,\n                    ContentType='text\/csv',\n                    TargetModel=full_model_name,\n                    Body=body)\nprint(response)\n<\/code><\/pre>\n\n<p><strong>Cloudwatch log Error:<\/strong><\/p>\n\n<pre><code>&gt; 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Error loading model: Unable\n&gt; to load model: invalid load key, '{'. [17:28:59]\n&gt; \/workspace\/src\/learner.cc:334: Check failed: fi-&gt;Read(&amp;mparam_,\n&gt; sizeof(mparam_)) == sizeof(mparam_) (25 vs. 136) : BoostLearner: wrong\n&gt; model format 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Stack trace: 2020-04-27\n&gt; 17:28:59,005 [INFO ] W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (0)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x24)\n&gt; [0x7f37ce1cacb4] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9 com.amazonaws.ml.mms.wlm.WorkerThread\n&gt; - Backend response time: 0 2020-04-27 17:28:59,005 [INFO ] W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (1)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(xgboost::LearnerImpl::Load(dmlc::Stream*)+0x4b5)\n&gt; [0x7f37ce266985] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (2)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(XGBoosterLoadModel+0x37)\n&gt; [0x7f37ce1bf417] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (3)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c)\n&gt; [0x7f37ee993ec0] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (4)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d)\n&gt; [0x7f37ee99387d] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (5)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce)\n&gt; [0x7f37eeba91de] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (6)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12c14)\n&gt; [0x7f37eeba9c14] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (7)\n&gt; \/miniconda3\/bin\/python(_PyObject_FastCallKeywords+0x48b)\n&gt; [0x563d71b4218b] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (8)\n&gt; \/miniconda3\/bin\/python(_PyEval_EvalFrameDefault+0x52cf)\n&gt; [0x563d71b91e8f] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  2020-04-27 17:28:59,005\n&gt; [WARN ] W-9003-b39b888fb4a3fa6cf83bb34a9\n&gt; com.amazonaws.ml.mms.wlm.WorkerThread - Backend worker thread\n&gt; exception. java.lang.IllegalArgumentException: reasonPhrase contains\n&gt; one of the following prohibited characters: \\r\\n: Unable to load\n&gt; model: Unable to load model: invalid load key, '{'. [17:28:59]\n&gt; \/workspace\/src\/learner.cc:334: Check failed: fi-&gt;Read(&amp;mparam_,\n&gt; sizeof(mparam_)) == sizeof(mparam_) (25 vs. 136) : BoostLearner: wrong\n&gt; model format Stack trace:   [bt] (0)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x24)\n&gt; [0x7f37ce1cacb4]   [bt] (1)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(xgboost::LearnerImpl::Load(dmlc::Stream*)+0x4b5)\n&gt; [0x7f37ce266985]   [bt] (2)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(XGBoosterLoadModel+0x37)\n&gt; [0x7f37ce1bf417]   [bt] (3)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c)\n&gt; [0x7f37ee993ec0]   [bt] (4)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d)\n&gt; [0x7f37ee99387d]   [bt] (5)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce)\n&gt; [0x7f37eeba91de]   [bt] (6)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12c14)\n&gt; [0x7f37eeba9c14]   [bt] (7)\n&gt; \/miniconda3\/bin\/python(_PyObject_FastCallKeywords+0x48b)\n&gt; [0x563d71b4218b]   [bt] (8)\n&gt; \/miniconda3\/bin\/python(_PyEval_EvalFrameDefault+0x52cf)\n&gt; [0x563d71b91e8f]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1588009185817,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|xgboost|amazon-sagemaker",
        "Question_view_count":324,
        "Owner_creation_time":1426492930157,
        "Owner_last_access_time":1663146061523,
        "Owner_location":null,
        "Owner_reputation":398,
        "Owner_up_votes":37,
        "Owner_down_votes":1,
        "Owner_views":84,
        "Question_last_edit_time":null,
        "Answer_body":"<p>SageMaker Random Cut Forest is part of the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">built-in algorithm library<\/a> and cannot be deployed in multi-model endpoint (MME). Built-in algorithms currently cannot be deployed to MME. XGboost is an exception, since it has an open-source container <a href=\"https:\/\/github.com\/aws\/sagemaker-xgboost-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-xgboost-container<\/a>.<\/p>\n\n<p>If you really need to deploy a RCF to a multi-model endpoint, one option is to find a reasonably similar open-source implementation (for example <a href=\"https:\/\/github.com\/kLabUM\/rrcf\" rel=\"nofollow noreferrer\"><code>rrcf<\/code><\/a> looks reasonably serious: based <a href=\"http:\/\/proceedings.mlr.press\/v48\/guha16.pdf\" rel=\"nofollow noreferrer\">on the same paper from Guha et al<\/a> and with 170+ github stars) and create a custom MME docker container. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">documentation is here<\/a> and there is <a href=\"https:\/\/github.com\/giuseppeporcelli\/sagemaker-custom-serving-containers\/blob\/master\/multi-model-server-container\/notebook\/multi-model-server-container.ipynb\" rel=\"nofollow noreferrer\">an excellent tuto here<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1588061581360,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61464960",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63568274,
        "Question_title":"How to use Serializer and Deserializer in Sagemaker 2",
        "Question_body":"<p>I spin up a Sagemaker notebook using the <code>conda_python3<\/code> kernel, and follow the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"noreferrer\">example<\/a> Notebook for <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/randomcutforest.html\" rel=\"noreferrer\">Random Cut Forest<\/a>.<\/p>\n<p>As of this writing, the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"noreferrer\">Sagemaker SDK<\/a> that comes with <code>conda_python3<\/code> is version 1.72.0, but I want to use new features, so I update my notebook to use the latest<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>%%bash\npip install -U sagemaker\n<\/code><\/pre>\n<p>And I see it updates.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>print(sagemaker.__version__)\n\n# 2.4.1\n<\/code><\/pre>\n<p>A change from version 1.x to 2.x was the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html#serializer-and-deserializer-classes\" rel=\"noreferrer\">serializer\/deserializer classes<\/a><\/p>\n<p>Previously (in version 1.72.0) I'd update my predictor to use the proper serializer\/deserializer, and could run inference on my model<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.predictor import csv_serializer, json_deserializer\n\n\nrcf_inference = rcf.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m4.xlarge',\n)\n\nrcf_inference.content_type = 'text\/csv'\nrcf_inference.serializer = csv_serializer\nrcf_inference.accept = 'application\/json'\nrcf_inference.deserializer = json_deserializer\n\nresults = rcf_inference.predict(some_numpy_array)\n<\/code><\/pre>\n<p>(Note this all comes from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"noreferrer\">example<\/a><\/p>\n<p>I try and replicate this using sagemaker 2.4.1 like so<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.deserializers import JSONDeserializer\nfrom sagemaker.serializers import CSVSerializer\n\nrcf_inference = rcf.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.xlarge',\n    serializer=CSVSerializer,\n    deserializer=JSONDeserializer\n)\n\nresults = rcf_inference.predict(some_numpy_array)\n<\/code><\/pre>\n<p>And I receive an error of<\/p>\n<pre><code>TypeError: serialize() missing 1 required positional argument: 'data'\n<\/code><\/pre>\n<p>I know I'm using the serliaizer\/deserializer incorrectly, but can't find good documentation on how this should be used<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1598300762780,
        "Question_score":8,
        "Question_tags":"python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":5910,
        "Owner_creation_time":1348841548500,
        "Owner_last_access_time":1663964727910,
        "Owner_location":null,
        "Owner_reputation":5182,
        "Owner_up_votes":1902,
        "Owner_down_votes":7,
        "Owner_views":315,
        "Question_last_edit_time":null,
        "Answer_body":"<p>in order to use the new serializers\/deserializers, you will need to init them, for example:<\/p>\n<pre><code>from sagemaker.deserializers import JSONDeserializer\nfrom sagemaker.serializers import CSVSerializer\n\nrcf_inference = rcf.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.xlarge',\n    serializer=CSVSerializer(),\n    deserializer=JSONDeserializer()\n)\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1598503719773,
        "Answer_score":16.0,
        "Question_favorite_count":4.0,
        "Answer_last_edit_time":1598615993680,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63568274",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59150100,
        "Question_title":"Sagemaker and Tensorflow model not saved",
        "Question_body":"<p>I am learning Sagemaker and I have this entry point:<\/p>\n\n<pre><code>import os\nimport tensorflow as tf\nfrom tensorflow.python.estimator.model_fn import ModeKeys as Modes\n\nINPUT_TENSOR_NAME = 'inputs'\nSIGNATURE_NAME = 'predictions'\n\nLEARNING_RATE = 0.001\n\n\ndef model_fn(features, labels, mode, params):\n    # Input Layer\n    input_layer = tf.reshape(features[INPUT_TENSOR_NAME], [-1, 28, 28, 1])\n\n    # Convolutional Layer #1\n    conv1 = tf.layers.conv2d(\n        inputs=input_layer,\n        filters=32,\n        kernel_size=[5, 5],\n        padding='same',\n        activation=tf.nn.relu)\n\n    # Pooling Layer #1\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n    # Convolutional Layer #2 and Pooling Layer #2\n    conv2 = tf.layers.conv2d(\n        inputs=pool1,\n        filters=64,\n        kernel_size=[5, 5],\n        padding='same',\n        activation=tf.nn.relu)\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n    # Dense Layer\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n    dropout = tf.layers.dropout(\n        inputs=dense, rate=0.4, training=(mode == Modes.TRAIN))\n\n    # Logits Layer\n    logits = tf.layers.dense(inputs=dropout, units=10)\n\n    # Define operations\n    if mode in (Modes.PREDICT, Modes.EVAL):\n        predicted_indices = tf.argmax(input=logits, axis=1)\n        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\n\n    if mode in (Modes.TRAIN, Modes.EVAL):\n        global_step = tf.train.get_or_create_global_step()\n        label_indices = tf.cast(labels, tf.int32)\n        loss = tf.losses.softmax_cross_entropy(\n            onehot_labels=tf.one_hot(label_indices, depth=10), logits=logits)\n        tf.summary.scalar('OptimizeLoss', loss)\n\n    if mode == Modes.PREDICT:\n        predictions = {\n            'classes': predicted_indices,\n            'probabilities': probabilities\n        }\n        export_outputs = {\n            SIGNATURE_NAME: tf.estimator.export.PredictOutput(predictions)\n        }\n        return tf.estimator.EstimatorSpec(\n            mode, predictions=predictions, export_outputs=export_outputs)\n\n    if mode == Modes.TRAIN:\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n        train_op = optimizer.minimize(loss, global_step=global_step)\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n\n    if mode == Modes.EVAL:\n        eval_metric_ops = {\n            'accuracy': tf.metrics.accuracy(label_indices, predicted_indices)\n        }\n        return tf.estimator.EstimatorSpec(\n            mode, loss=loss, eval_metric_ops=eval_metric_ops)\n\n\ndef serving_input_fn(params):\n    inputs = {INPUT_TENSOR_NAME: tf.placeholder(tf.float32, [None, 784])}\n    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n\n\ndef read_and_decode(filename_queue):\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\n            'image_raw': tf.FixedLenFeature([], tf.string),\n            'label': tf.FixedLenFeature([], tf.int64),\n        })\n\n    image = tf.decode_raw(features['image_raw'], tf.uint8)\n    image.set_shape([784])\n    image = tf.cast(image, tf.float32) * (1. \/ 255)\n    label = tf.cast(features['label'], tf.int32)\n\n    return image, label\n\n\ndef train_input_fn(training_dir, params):\n    return _input_fn(training_dir, 'train.tfrecords', batch_size=100)\n\n\ndef eval_input_fn(training_dir, params):\n    return _input_fn(training_dir, 'test.tfrecords', batch_size=100)\n\n\ndef _input_fn(training_dir, training_filename, batch_size=100):\n    test_file = os.path.join(training_dir, training_filename)\n    filename_queue = tf.train.string_input_producer([test_file])\n\n    image, label = read_and_decode(filename_queue)\n    images, labels = tf.train.batch(\n        [image, label], batch_size=batch_size,\n        capacity=1000 + 3 * batch_size)\n\n    return {INPUT_TENSOR_NAME: images}, labels\n\ndef neo_preprocess(payload, content_type):\n    import logging\n    import numpy as np\n    import io\n\n    logging.info('Invoking user-defined pre-processing function')\n\n    if content_type != 'application\/x-image' and content_type != 'application\/vnd+python.numpy+binary':\n        raise RuntimeError('Content type must be application\/x-image or application\/vnd+python.numpy+binary')\n\n    f = io.BytesIO(payload)\n    image = np.load(f)*255\n\n    return image\n\n### NOTE: this function cannot use MXNet\ndef neo_postprocess(result):\n    import logging\n    import numpy as np\n    import json\n\n    logging.info('Invoking user-defined post-processing function')\n\n    # Softmax (assumes batch size 1)\n    result = np.squeeze(result)\n    result_exp = np.exp(result - np.max(result))\n    result = result_exp \/ np.sum(result_exp)\n\n    response_body = json.dumps(result.tolist())\n    content_type = 'application\/json'\n\n    return response_body, content_type\n<\/code><\/pre>\n\n<p>And I am training it <\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='cnn_fashion_mnist.py',\n                       role=role,\n                       input_mode='Pipe',\n                       training_steps=1, \n                       evaluation_steps=1,\n                       train_instance_count=1,\n                       output_path=output_path,\n                       train_instance_type='ml.c5.2xlarge',\n                       base_job_name='mnist')\n<\/code><\/pre>\n\n<p>so far it is trying correctly and it tells me that everything when well, but when I check the output there is nothing there or if I try to deploy it I get the error saying it couldn't find the model because there is nothing in the bucker, any ideas or extra configurations? Thank you<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1575346450973,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":658,
        "Owner_creation_time":1462770189773,
        "Owner_last_access_time":1626440092163,
        "Owner_location":null,
        "Owner_reputation":45,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Looks like you are using one of the older Tensorflow versions.\nWe would recommend switching to a newer more straight-forward way of running Tensorflow in SageMaker (script mode) by switching to a more recent Tensorflow version.<\/p>\n\n<p>You can read more about it in our documentation:\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html<\/a><\/p>\n\n<p>Here is an example that might help:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/tensorflow_script_mode_training_and_serving.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/tensorflow_script_mode_training_and_serving.ipynb<\/a> <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1576626745670,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59150100",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61212875,
        "Question_title":"Access Crowd HTML output results",
        "Question_body":"<p>I'm creating a website using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-ui-template-reference.html\" rel=\"nofollow noreferrer\">Crowd HTML Elements<\/a> that let users\/workers annotate images with the bounding box format.  The form looks like this:<\/p>\n\n<pre><code>&lt;crowd-form&gt;\n  &lt;crowd-bounding-box\n    name=\"annotatedResult\"\n    labels=\"['Referee', 'Player']\"\n    src=\"https:\/\/s3.amazonaws.com\/cv-demo-images\/basketball-outdoor.jpg\"\n    header=\"Draw boxes around each basketball player and referee in this image\"\n  &gt;\n    &lt;full-instructions header=\"Bounding Box Instructions\" &gt;\n      &lt;p&gt;Use the bounding box tool to draw boxes around the requested target of interest:&lt;\/p&gt;\n      &lt;ol&gt;\n        &lt;li&gt;Draw a rectangle using your mouse over each instance of the target.&lt;\/li&gt;\n        &lt;li&gt;Make sure the box does not cut into the target, leave a 2 - 3 pixel margin&lt;\/li&gt;\n        &lt;li&gt;\n          When targets are overlapping, draw a box around each object,\n          include all contiguous parts of the target in the box.\n          Do not include parts that are completely overlapped by another object.\n        &lt;\/li&gt;\n        &lt;li&gt;\n          Do not include parts of the target that cannot be seen,\n          even though you think you can interpolate the whole shape of the target.\n        &lt;\/li&gt;\n        &lt;li&gt;Avoid shadows, they're not considered as a part of the target.&lt;\/li&gt;\n        &lt;li&gt;If the target goes off the screen, label up to the edge of the image.&lt;\/li&gt;\n      &lt;\/ol&gt;\n    &lt;\/full-instructions&gt;\n\n    &lt;short-instructions&gt;\n      Draw boxes around each basketball player and referee in this image.\n    &lt;\/short-instructions&gt;\n  &lt;\/crowd-bounding-box&gt;\n&lt;\/crowd-form&gt;\n<\/code><\/pre>\n\n<p>The results of a worker's submission looks like the following:<\/p>\n\n<pre><code>  {\n    \"annotatedResult\": {\n      \"boundingBoxes\": [\n        {\n          \"height\": 3300,\n          \"label\": \"Dog\",\n          \"left\": 536,\n          \"top\": 154,\n          \"width\": 4361\n        }\n      ],\n      \"inputImageProperties\": {\n        \"height\": 3456,\n        \"width\": 5184\n      }\n    }\n  }\n]\n<\/code><\/pre>\n\n<p>I'd like to take this output and write it to a database, pass it to AWS Lambda, store it as metadata, etc. but I don't know how to access the results.  Is the JSON output a property of some HTML DOM property I can grab?<\/p>\n\n<p>I can attach a javascript function to the submit action of the crowd-form portion...<\/p>\n\n<pre><code>&lt;script&gt;\n  document.querySelector('crowd-form').onsubmit = function() {\n      ???\n  };\n&lt;\/script&gt;\n<\/code><\/pre>\n\n<p>...but I'm not sure what object I need to grab to get the results.  Thanks for your help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586882719607,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|mechanicalturk",
        "Question_view_count":371,
        "Owner_creation_time":1483444144907,
        "Owner_last_access_time":1643982821673,
        "Owner_location":"Hoth",
        "Owner_reputation":312,
        "Owner_up_votes":28,
        "Owner_down_votes":1,
        "Owner_views":65,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can access the bounding boxes during the onsubmit event like this:<\/p>\n\n<pre><code>&lt;script&gt;\n    document.querySelector('crowd-form').onsubmit = function(e) {\n      const boundingBoxes = document.querySelector('crowd-bounding-box').value.boundingBoxes || document.querySelector('crowd-bounding-box')._submittableValue.boundingBoxes;\n    }\n&lt;\/script&gt;\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/jsfiddle.net\/ap56djgq\/\" rel=\"nofollow noreferrer\">Here's<\/a> a working jsfiddle.<\/p>\n\n<p>Your use case sounds interesting. If you don't mind sharing, please email me at samhenry@amazon.com and I may be able to help further.<\/p>\n\n<p>Thank you,<\/p>\n\n<p>Amazon Mechanical Turk <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1586884865260,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61212875",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64215998,
        "Question_title":"Why is Crowd HTML breaking this image?",
        "Question_body":"<p>I'm using Crowd HTML Elements to perform bounding box annotation, but when I attempt to load some of my images, I get this error in the dev tools console:<\/p>\n<pre><code>crowd-html-elements.js:1 window.onError received an event without an error:  {event: ErrorEvent}\n(anonymous) @ crowd-html-elements.js:1\nerror (async)\ne @ crowd-html-elements.js:1\ne @ crowd-html-elements.js:1\n.\/src\/crowd-html-elements-loader.ts @ crowd-html-elements.js:1\ns @ crowd-html-elements.js:1\n(anonymous) @ crowd-html-elements.js:1\n(anonymous) @ crowd-html-elements.js:1\ncrowd-html-elements-without-ce-polyfill.js:6282 window.onError received an event without an error:  {event: ErrorEvent}\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:6282\nerror (async)\ne @ crowd-html-elements-without-ce-polyfill.js:6282\ne @ crowd-html-elements-without-ce-polyfill.js:6282\n.\/src\/index.ts @ crowd-html-elements-without-ce-polyfill.js:6282\nr @ crowd-html-elements-without-ce-polyfill.js:1\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:1\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:1\ncrowd-html-elements-without-ce-polyfill.js:6282 Uncaught Error: Unexpected image dimensions during normalization\n    at Function.normalizeHeight (crowd-html-elements-without-ce-polyfill.js:6282)\n    at Function.normalizeDimensions (crowd-html-elements-without-ce-polyfill.js:6282)\n    at new a (crowd-html-elements-without-ce-polyfill.js:6282)\n    at ie.handleTargetImageLoaded (crowd-html-elements-without-ce-polyfill.js:6282)\n    at Image.r.onload (crowd-html-elements-without-ce-polyfill.js:6282)\nnormalizeHeight @ crowd-html-elements-without-ce-polyfill.js:6282\nnormalizeDimensions @ crowd-html-elements-without-ce-polyfill.js:6282\na @ crowd-html-elements-without-ce-polyfill.js:6282\nhandleTargetImageLoaded @ crowd-html-elements-without-ce-polyfill.js:6282\nr.onload @ crowd-html-elements-without-ce-polyfill.js:6282\nload (async)\nsetBackgroundImage @ crowd-html-elements-without-ce-polyfill.js:6282\nrenderImageSrcChange @ crowd-html-elements-without-ce-polyfill.js:6282\nshouldComponentUpdate @ crowd-html-elements-without-ce-polyfill.js:6282\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nG @ crowd-html-elements-without-ce-polyfill.js:6278\nw @ crowd-html-elements-without-ce-polyfill.js:6278\nS @ crowd-html-elements-without-ce-polyfill.js:6278\ne.reactMount @ crowd-html-elements-without-ce-polyfill.js:3\ne.updateRegion @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\ne.reactBatchUpdate @ crowd-html-elements-without-ce-polyfill.js:3\ni @ crowd-html-elements-without-ce-polyfill.js:3\nf.componentDidUpdate @ crowd-html-elements-without-ce-polyfill.js:3\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nG @ crowd-html-elements-without-ce-polyfill.js:6278\nw @ crowd-html-elements-without-ce-polyfill.js:6278\n_renderReactComponent @ crowd-html-elements-without-ce-polyfill.js:6282\n_updateReactComponent @ crowd-html-elements-without-ce-polyfill.js:6282\nY @ crowd-html-elements-without-ce-polyfill.js:5984\nC @ crowd-html-elements-without-ce-polyfill.js:5984\nk @ crowd-html-elements-without-ce-polyfill.js:5984\n_propertiesChanged @ crowd-html-elements-without-ce-polyfill.js:5984\n_flushProperties @ crowd-html-elements-without-ce-polyfill.js:5954\n_flushProperties @ crowd-html-elements-without-ce-polyfill.js:5984\n_invalidateProperties @ crowd-html-elements-without-ce-polyfill.js:5984\n_setProperty @ crowd-html-elements-without-ce-polyfill.js:5984\nObject.defineProperty.set @ crowd-html-elements-without-ce-polyfill.js:5954\n(anonymous) @ labeling.html:199\nasync function (async)\n(anonymous) @ labeling.html:198\nPromise.then (async)\n(anonymous) @ labeling.html:196\n<\/code><\/pre>\n<p>The <strong>Unexpected image dimensions during normalization<\/strong> portion seems like the issue, but I've found nothing with regard to troubleshooting.  Can someone explain what expected image dimensions are and why some are failing?<\/p>\n<p>Here's a snippet of the code that's throwing the error.<\/p>\n<pre><code>            static normalizeHeight(e) {\n                if (e.height === e.naturalHeight)\n                    return e.height;\n                if (e.height === e.naturalWidth)\n                    return e.height;\n                if (Math.abs(e.height - e.naturalHeight) &lt; 2)\n                    return e.naturalHeight;\n                if (Math.abs(e.height - e.naturalWidth) &lt; 2)\n                    return e.naturalWidth;\n                throw new Error(&quot;Unexpected image dimensions during normalization&quot;)\n            }\n            static normalizeWidth(e) {\n                if (e.width === e.naturalWidth)\n                    return e.width;\n                if (e.width === e.naturalHeight)\n                    return e.width;\n                if (Math.abs(e.width - e.naturalWidth) &lt; 2)\n                    return e.naturalWidth;\n                if (Math.abs(e.width - e.naturalHeight) &lt; 2)\n                    return e.naturalHeight;\n                throw new Error(&quot;Unexpected image dimensions during normalization&quot;)\n<\/code><\/pre>\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1601930941820,
        "Question_score":1,
        "Question_tags":"javascript|amazon-sagemaker|mechanicalturk",
        "Question_view_count":168,
        "Owner_creation_time":1483444144907,
        "Owner_last_access_time":1643982821673,
        "Owner_location":"Hoth",
        "Owner_reputation":312,
        "Owner_up_votes":28,
        "Owner_down_votes":1,
        "Owner_views":65,
        "Question_last_edit_time":1602868269100,
        "Answer_body":"<p>The issue turned out to be related to the css styling that was being applied to the canvas portion of my site that was loading the labeling tools.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1603367517400,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64215998",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62320331,
        "Question_title":"Sagemaker API to list Hyperparameters",
        "Question_body":"<p>I'm currently trying to implement MLFlow Tracking into my training pipeline and would like to log the hyperparameters of my hyperparameter Tuning of each training job.<\/p>\n\n<p>Does anyone know, how to pull the list of hyperparameters that can be seen on the sagemaker training job interface (on the AWS console)? Is there any other smarter way to list how models perform in comparison in Sagemaker (and displayed)?<\/p>\n\n<p>I would assume there must be an easy and Pythonic way to do this (either boto3 or the sagemaker api) to get this data. I wasn't able to find it in Cloudwatch.<\/p>\n\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1591864702897,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|mlflow",
        "Question_view_count":484,
        "Owner_creation_time":1469720117217,
        "Owner_last_access_time":1648482744487,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":"<p>there is indeed a rather pythonic way in the SageMaker python SDK:<\/p>\n\n<pre><code>tuner = sagemaker.tuner.HyperparameterTuner.attach('&lt; your tuning jobname&gt;')\n\nresults = tuner.analytics().dataframe()  # all your tuning metadata, in pandas!\n<\/code><\/pre>\n\n<p>See full example here <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1592173650467,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62320331",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61052173,
        "Question_title":"Is there a limit on input json string for aws sagemaker endpoint?",
        "Question_body":"<p>I have ~5MB json string that I want to send to my endpoint. I am using boto3.client to invoke the endpoint from my python client. It throws ConnectionResetError. <\/p>\n\n<pre><code>    File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\urllib3\\connectionpool.py\", line 600, in urlopen\n    chunked=chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\urllib3\\connectionpool.py\", line 354, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 1229, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\botocore\\awsrequest.py\", line 92, in _send_request\n    method, url, body, headers, *args, **kwargs)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 1275, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 1224, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\botocore\\awsrequest.py\", line 119, in _send_output\n    self.send(msg)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\botocore\\awsrequest.py\", line 203, in send\n    return super(AWSConnection, self).send(str)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 977, in send\n    self.sock.sendall(data)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\ssl.py\", line 1012, in sendall\n    v = self.send(byte_view[count:])\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\ssl.py\", line 981, in send\n    return self._sslobj.write(data)\nConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n<\/code><\/pre>\n\n<p>Looking at the trace, I am guessing it is due to json string size. Could someone please help me how to get around this? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586140541570,
        "Question_score":1,
        "Question_tags":"json|python-3.x|http-post|amazon-sagemaker|urllib3",
        "Question_view_count":1281,
        "Owner_creation_time":1584461369093,
        "Owner_last_access_time":1663592296570,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1586146490857,
        "Answer_body":"<p>Exceeding the payload size limit does result in a connection reset from the SageMaker Runtime service.<\/p>\n\n<p>From the SageMaker <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>Maximum payload size for endpoint invocation |    5 MB<\/p>\n<\/blockquote>\n\n<p>There are likely more space-efficient data formats than JSON that you could use to transmit the payload, but the available options will depend on the type of data and what model image you are using (i.e. whether Amazon-provided or a custom implementation).<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1586421549190,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61052173",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54026623,
        "Question_title":"AWS Sagemaker - ClientError: Data download failed",
        "Question_body":"<p><strong>Problem:<\/strong>\nI am trying to setup a model in Sagemaker, however it fails when it comes to downloading the data.\nDoes anyone know what I am doing wrong?<\/p>\n\n<p><strong>What I did so far<\/strong>:\nIn order to avoid any mistakes on my side I decided to use the AWS tutorial:\ntensorflow_iris_dnn_classifier_using_estimators<\/p>\n\n<p>And I made only two changes:<\/p>\n\n<ol>\n<li>I copied the dataset to my own S3 instance. --> I tested if I could access \/ show the data and it worked.<\/li>\n<li>I edited the path to point to the new folder.<\/li>\n<\/ol>\n\n<p>This is the AWS source code:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators<\/a><\/p>\n\n<pre><code>%%time\nimport boto3\n\n# use the region-specific sample data bucket\nregion = boto3.Session().region_name\n#train_data_location = 's3:\/\/sagemaker-sample-data-{}\/tensorflow\/iris'.format(region)\ntrain_data_location = 's3:\/\/my-s3-bucket'\n\niris_estimator.fit(train_data_location)\n<\/code><\/pre>\n\n<p>And this is the error I get:<\/p>\n\n<pre><code>\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/interactiveshell.pyc in run_cell_magic(self, magic_name, line, cell)\n   2115             magic_arg_s = self.var_expand(line, stack_depth)\n   2116             with self.builtin_trap:\n-&gt; 2117                 result = fn(magic_arg_s, cell)\n   2118             return result\n   2119 \n\n&lt;decorator-gen-60&gt; in time(self, line, cell, local_ns)\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/magic.pyc in &lt;lambda&gt;(f, *a, **k)\n    186     # but it's overkill for just that one bit of state.\n    187     def magic_deco(arg):\n--&gt; 188         call = lambda f, *a, **k: f(*a, **k)\n    189 \n    190         if callable(arg):\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/magics\/execution.pyc in time(self, line, cell, local_ns)\n   1191         else:\n   1192             st = clock2()\n-&gt; 1193             exec(code, glob, local_ns)\n   1194             end = clock2()\n   1195             out = None\n\n&lt;timed exec&gt; in &lt;module&gt;()\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/tensorflow\/estimator.pyc in fit(self, inputs, wait, logs, job_name, run_tensorboard_locally)\n    314                 tensorboard.join()\n    315         else:\n--&gt; 316             fit_super()\n    317 \n    318     @classmethod\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/tensorflow\/estimator.pyc in fit_super()\n    293 \n    294         def fit_super():\n--&gt; 295             super(TensorFlow, self).fit(inputs, wait, logs, job_name)\n    296 \n    297         if run_tensorboard_locally and wait is False:\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/estimator.pyc in fit(self, inputs, wait, logs, job_name)\n    232         self.latest_training_job = _TrainingJob.start_new(self, inputs)\n    233         if wait:\n--&gt; 234             self.latest_training_job.wait(logs=logs)\n    235 \n    236     def _compilation_job_name(self):\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/estimator.pyc in wait(self, logs)\n    571     def wait(self, logs=True):\n    572         if logs:\n--&gt; 573             self.sagemaker_session.logs_for_job(self.job_name, wait=True)\n    574         else:\n    575             self.sagemaker_session.wait_for_job(self.job_name)\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/session.pyc in logs_for_job(self, job_name, wait, poll)\n   1126 \n   1127         if wait:\n-&gt; 1128             self._check_job_status(job_name, description, 'TrainingJobStatus')\n   1129             if dot:\n   1130                 print()\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/session.pyc in _check_job_status(self, job, desc, status_key_name)\n    826             reason = desc.get('FailureReason', '(No reason provided)')\n    827             job_type = status_key_name.replace('JobStatus', ' job')\n--&gt; 828             raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))\n    829 \n    830     def wait_for_endpoint(self, endpoint, poll=5):\n\nValueError: Error for Training job sagemaker-tensorflow-2019-01-03-16-32-16-435: Failed Reason: ClientError: Data download failed:S3 key: s3:\/\/my-s3-bucket\/\/sagemaker-tensorflow-2019-01-03-14-02-39-959\/source\/sourcedir.tar.gz has an illegal char sub-sequence '\/\/' in it\n<\/code><\/pre>",
        "Question_answer_count":4,
        "Question_comment_count":2,
        "Question_creation_time":1546534753933,
        "Question_score":9,
        "Question_tags":"python|amazon-web-services|tensorflow|jupyter|amazon-sagemaker",
        "Question_view_count":5145,
        "Owner_creation_time":1546261116430,
        "Owner_last_access_time":1620827806300,
        "Owner_location":null,
        "Owner_reputation":115,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The script is expecting 'bucket' to be bucket = Session().default_bucket() or your own. Have you tried setting bucket equal to your personal bucket?<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1546594981603,
        "Answer_score":2.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54026623",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72096297,
        "Question_title":"Hyperparameter tuning job In Sagemaker with cross valdiation",
        "Question_body":"<p>I managed to get something <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-tuning-job.html\" rel=\"nofollow noreferrer\">along those lines<\/a> to work. This is great but to be more on the save side (i.e. not rely too much on the train validation split) one should really use cross validation. I am curious, if this can also be achieved via Sagemaker hyperparameter tuning jobs? I googled extensively ...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1651564321897,
        "Question_score":0,
        "Question_tags":"python|cross-validation|amazon-sagemaker|hyperparameters",
        "Question_view_count":70,
        "Owner_creation_time":1267440784443,
        "Owner_last_access_time":1664045779313,
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It is not possible through HPO.<\/p>\n<p>You need to add additional step in your workflow to achieve cross-validation.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1651629955790,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72096297",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60006106,
        "Question_title":"NotImplementedError: Text mode not supported, use mode='wb' and manage bytes in s3fs",
        "Question_body":"<p>I know that there are a similar question but it is more general and not specific of this package. I am saving a pandas dataframe within a Sagemaker Jupyter notebook into a csv in S3 as follow:<\/p>\n\n<pre><code>df.to_csv('s3:\/\/bucket\/key\/file.csv', index=False)\n<\/code><\/pre>\n\n<p>However I am getting the following error:<\/p>\n\n<pre><code>NotImplementedError: Text mode not supported, use mode='wb' and manage bytes\n<\/code><\/pre>\n\n<p>The code more or less is that I read a csv from S3, make some preprocessing on it and then saves it to S3. I can read csv from S3 successfully with:<\/p>\n\n<pre><code>df.read_csv('s3:\/\/bucket\/key\/file.csv')\n<\/code><\/pre>\n\n<p>The object that I am trying to save to S3 is indeed a <em>pandas.core.frame.DataFrame<\/em><\/p>\n\n<p>In the notebook I can see using <code>!pip show package<\/code> that I have pandas 0.24.2 and s3fs 0.1.5. <\/p>\n\n<p>What could be the problem? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":5,
        "Question_creation_time":1580482050793,
        "Question_score":2,
        "Question_tags":"python|pandas|jupyter-notebook|amazon-sagemaker|python-s3fs",
        "Question_view_count":1738,
        "Owner_creation_time":1523298968403,
        "Owner_last_access_time":1663934452963,
        "Owner_location":null,
        "Owner_reputation":1754,
        "Owner_up_votes":396,
        "Owner_down_votes":76,
        "Owner_views":197,
        "Question_last_edit_time":1581088182973,
        "Answer_body":"<p>Can you Please try<\/p>\n\n<pre><code>df.to_csv(\"s3:\/\/bucket\/key\/file.csv\", index=False, mode='wb')\n<\/code><\/pre>\n\n<p>It should fix your error. The default mode is <strong>w<\/strong> which writes in the file system as text and not bytes. Where as s3 expects the data to be bytes. hence you have to specify mode as <strong>wb<\/strong>(write bytes) while writing the dataframe as csv to the filesystem.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1580493486723,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60006106",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49374476,
        "Question_title":"How do I call a SageMaker Endpoint using the AWS CLI (",
        "Question_body":"<p>I'm trying to invoke the iris endpointfrom the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\/tensorflow_iris_dnn_classifier_using_estimators.ipynb\" rel=\"nofollow noreferrer\">SageMaker example notebooks<\/a> using the aws cli. I've tried using the following command:<\/p>\n\n<pre><code>!aws sagemaker-runtime invoke-endpoint \\\n--endpoint-name sagemaker-tensorflow-py2-cpu-2018-03-19-21-27-52-956 \\\n--body \"[6.4, 3.2, 4.5, 1.5]\" \\\n--content-type \"application\/json\" \\\noutput.json\n<\/code><\/pre>\n\n<p>I get the following response:<\/p>\n\n<pre><code>{\n    \"InvokedProductionVariant\": \"AllTraffic\", \n    \"ContentType\": \"*\/*\"\n}\n<\/code><\/pre>\n\n<p>What am I doing wrong?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1521505092823,
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":4027,
        "Owner_creation_time":1443201378360,
        "Owner_last_access_time":1653587950970,
        "Owner_location":null,
        "Owner_reputation":749,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If you've gotten that response, your request is successful. The output should be in the output file you specified - output.json :)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1521567781630,
        "Answer_score":5.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49374476",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61520346,
        "Question_title":"No such file or directory: 'docker': 'docker' when running sagemaker studio in local mode",
        "Question_body":"<p>I try to train a pytorch model on amazon sagemaker studio.<\/p>\n\n<p>It's working when I use an EC2 for training with:<\/p>\n\n<pre><code>estimator = PyTorch(entry_point='train_script.py',\n                role=role,\n                sagemaker_session = sess,\n                train_instance_count=1,\n                train_instance_type='ml.c5.xlarge',\n                framework_version='1.4.0', \n                source_dir='.',\n                git_config=git_config, \n               )\nestimator.fit({'stockdata': data_path})\n<\/code><\/pre>\n\n<p>and it's work on local mode in classic sagemaker notebook (non studio) with:<\/p>\n\n<pre><code> estimator = PyTorch(entry_point='train_script.py',\n                role=role,\n                train_instance_count=1,\n                train_instance_type='local',\n                framework_version='1.4.0', \n                source_dir='.',\n                git_config=git_config, \n               )\nestimator.fit({'stockdata': data_path})\n<\/code><\/pre>\n\n<p>But when I use it the same code (with train_instance_type='local') on sagemaker studio it doesn't work and I have the following error: No such file or directory: 'docker': 'docker'<\/p>\n\n<p>I tried to install docker with pip install but the docker command is not found if use it in terminal<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1588239469173,
        "Question_score":5,
        "Question_tags":"docker|pytorch|amazon-sagemaker",
        "Question_view_count":1884,
        "Owner_creation_time":1576136255053,
        "Owner_last_access_time":1663858972457,
        "Owner_location":null,
        "Owner_reputation":795,
        "Owner_up_votes":37,
        "Owner_down_votes":1,
        "Owner_views":37,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This indicates that there is a problem finding the Docker service.<\/p>\n<p>By default, the Docker is not installed in the SageMaker Studio  (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/656#issuecomment-632170943\" rel=\"nofollow noreferrer\">confirming github ticket response<\/a>).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1596561017227,
        "Answer_score":7.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1620410809823,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61520346",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67123040,
        "Question_title":"How to tell programmatically that an AWS Step Function execution has been completed?",
        "Question_body":"<p>I am triggering a Step Function execution via a Python cell in a SageMaker Notebook, like this:<\/p>\n<pre><code>state_machine_arn = 'arn:aws:states:us-west-1:1234567891:stateMachine:alexanderMyPackageStateMachineE3411O13-A1vQWERTP9q9'\nsfn = boto3.client('stepfunctions')\n..\nsfn.start_execution(**kwargs)  # Non Blocking Call\nrun_arn = response['executionArn']\nprint(f&quot;Started run {run_name}. ARN is {run_arn}.&quot;)\n<\/code><\/pre>\n<p>and then in order to check that the execution (which might take hours to complete depending on the input) has been completed, before I start doing some custom post-analysis on the results, I manually execute a cell with:<\/p>\n<pre><code>response = sfn.list_executions(\n    stateMachineArn=state_machine_arn,\n    maxResults=1\n)\nprint(response)\n<\/code><\/pre>\n<p>where I can see from the output the status of the execution, e.g. <code>'status': 'RUNNING'<\/code>.<\/p>\n<p>How can I automate this, i.e. trigger the Step Function and continue the execution on my post-analysis custom logic only after the execution has finished? Is there for example a blocking call to start the execution, or a callback method I could use?<\/p>\n<p>I can think of putting a sleep method, so that the Python Notebook cell would periodically call <code>list_executions()<\/code> and check the status, and only when the execution is completed, continue to rest of the code. I can statistically determine the sleep period, but I was wondering if there is a simpler\/more accurate way.<\/p>\n<hr \/>\n<p>PS: Related: <a href=\"https:\/\/stackoverflow.com\/questions\/46878423\/how-to-avoid-simultaneous-execution-in-aws-step-function\">How to avoid simultaneous execution in aws step function<\/a>, however I would like to avoid creating any new AWS resource, just for this, I would like to do everything from within the Notebook.<\/p>\n<p>PPS: I cannot make any change to <code>MyPackage<\/code> and the Step Function definition.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1618566783027,
        "Question_score":4,
        "Question_tags":"python|amazon-web-services|asynchronous|amazon-sagemaker|aws-step-functions",
        "Question_view_count":1216,
        "Owner_creation_time":1369257942213,
        "Owner_last_access_time":1663776093950,
        "Owner_location":"London, UK",
        "Owner_reputation":70285,
        "Owner_up_votes":7595,
        "Owner_down_votes":12100,
        "Owner_views":13121,
        "Question_last_edit_time":1618585069787,
        "Answer_body":"<p>Based on the comments.<\/p>\n<p>If no new resources are to be created (no CloudWatch Event rules, lambda functions) nor any changes to existing Step Function are allowed, then <strong>pooling iteratively<\/strong> <code>list_executions<\/code> would be the best solution.<\/p>\n<p>AWS CLI and boto3 have implemented similar solutions (not for Step Functions), but for some other services. They are called <code>waiters<\/code> (e.g. <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/ec2.html#waiters\" rel=\"nofollow noreferrer\">ec2 waiters<\/a>). So basically you would have to create your own <strong>waiter for Step Function<\/strong>, as AWS does not provide one for that. AWS uses <strong>15 seconds<\/strong> sleep time from what I recall for its waiters.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1618568305233,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67123040",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69046990,
        "Question_title":"How to pass dependency files to sagemaker SKLearnProcessor and use it in Pipeline?",
        "Question_body":"<p>I need to import function from different python scripts, which will used inside <code>preprocessing.py<\/code> file. I was not able to find a way to pass the dependent files to <code>SKLearnProcessor<\/code> Object, due to which I am getting <code>ModuleNotFoundError<\/code>.<\/p>\n<p><strong>Code:<\/strong><\/p>\n<pre><code>from sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     instance_type='ml.m5.xlarge',\n                                     instance_count=1)\n\n\nsklearn_processor.run(code='preprocessing.py',\n                      inputs=[ProcessingInput(\n                        source=input_data,\n                        destination='\/opt\/ml\/processing\/input')],\n                      outputs=[ProcessingOutput(output_name='train_data',\n                                                source='\/opt\/ml\/processing\/train'),\n                               ProcessingOutput(output_name='test_data',\n                                                source='\/opt\/ml\/processing\/test')],\n                      arguments=['--train-test-split-ratio', '0.2']\n                     )\n<\/code><\/pre>\n<p>I would like to pass,\n<code>dependent_files = ['file1.py', 'file2.py', 'requirements.txt']<\/code>. So, that <code>preprocessing.py<\/code> have access to all the dependent modules.<\/p>\n<p>And also need to install libraries from <code>requirements.txt<\/code> file.<\/p>\n<p>Can you share any work around or a right way to do this?<\/p>\n<p><strong>Update-25-11-2021:<\/strong><\/p>\n<p><strong>Q1.<\/strong>(Answered but looking to solve using <code>FrameworkProcessor<\/code>)<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/99f023e76a5db060907a796d4d8fee550f005844\/src\/sagemaker\/processing.py#L1426\" rel=\"noreferrer\">Here<\/a>, the <code>get_run_args<\/code> function, is handling <code>dependencies<\/code>, <code>source_dir<\/code> and <code>code<\/code> parameters by using <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/99f023e76a5db060907a796d4d8fee550f005844\/src\/sagemaker\/processing.py#L1265\" rel=\"noreferrer\">FrameworkProcessor<\/a>. Is there any way that we can set this parameters from <code>ScriptProcessor<\/code> or <code>SKLearnProcessor<\/code> or any other <code>Processor<\/code> to set them?<\/p>\n<p><strong>Q2.<\/strong><\/p>\n<p>Can you also please show some reference to use our <code>Processor<\/code> as <code>sagemaker.workflow.steps.ProcessingStep<\/code> and then use in <code>sagemaker.workflow.pipeline.Pipeline<\/code>?<\/p>\n<p>For having <code>Pipeline<\/code>, do we need <code>sagemaker-project<\/code> as mandatory or can we create <code>Pipeline<\/code> directly without any <code>Sagemaker-Project<\/code>?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":5,
        "Question_creation_time":1630681185260,
        "Question_score":11,
        "Question_tags":"python|amazon-web-services|scikit-learn|amazon-sagemaker",
        "Question_view_count":2139,
        "Owner_creation_time":1500824148410,
        "Owner_last_access_time":1664025062210,
        "Owner_location":"India",
        "Owner_reputation":4419,
        "Owner_up_votes":434,
        "Owner_down_votes":324,
        "Owner_views":962,
        "Question_last_edit_time":1637936310430,
        "Answer_body":"<p>There are a couple of options for you to accomplish that.<\/p>\n<p>One that is really simple is adding all additional files to a folder, example:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 my_package\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 file1.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 file2.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 preprocessing.py\n<\/code><\/pre>\n<p>Then send this entire folder as another input under the same <code>\/opt\/ml\/processing\/input\/code\/<\/code>, example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor = SKLearnProcessor(\n    framework_version=&quot;0.20.0&quot;,\n    role=role,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    instance_count=1,\n)\n\nsklearn_processor.run(\n    code=&quot;preprocessing.py&quot;,  # &lt;- this gets uploaded as \/opt\/ml\/processing\/input\/code\/preprocessing.py\n    inputs=[\n        ProcessingInput(source=input_data, destination='\/opt\/ml\/processing\/input'),\n        # Send my_package as \/opt\/ml\/processing\/input\/code\/my_package\/\n        ProcessingInput(source='my_package\/', destination=&quot;\/opt\/ml\/processing\/input\/code\/my_package\/&quot;)\n    ],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],\n)\n<\/code><\/pre>\n<p>What happens is that <code>sagemaker-python-sdk<\/code> is going to put your argument <code>code=&quot;preprocessing.py&quot;<\/code> under <code>\/opt\/ml\/processing\/input\/code\/<\/code> and you will have <code>my_package\/<\/code> under the same directory.<\/p>\n<p><strong>Edit:<\/strong><\/p>\n<p>For the <code>requirements.txt<\/code>, you can add to your <code>preprocessing.py<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport subprocess\n\nsubprocess.check_call([\n    sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, &quot;-r&quot;,\n    &quot;\/opt\/ml\/processing\/input\/code\/my_package\/requirements.txt&quot;,\n])\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1637782762627,
        "Answer_score":17.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":1637783228437,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69046990",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65464181,
        "Question_title":"An alternative to tf.distribute.cluster_resolver.TPUClusterResolver( tpu_name) to be used in Sagemaker?",
        "Question_body":"<ol>\n<li><p>task : object_detection<\/p>\n<\/li>\n<li><p>environment: AWS sagemaker<\/p>\n<\/li>\n<li><p>instance type: 'ml.p2.xlarge' | num_instances = 1<\/p>\n<\/li>\n<li><p>Main file to be run: <a href=\"https:\/\/github.com\/tensorflow\/models\/blob\/master\/research\/object_detection\/model_main_tf2.py\" rel=\"nofollow noreferrer\">original<\/a><\/p>\n<\/li>\n<li><p>Problematic code segment from the main file:<\/p>\n<pre><code>    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n    FLAGS.tpu_name)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n    elif FLAGS.num_workers &gt; 1:\n        strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n    else:\n        strategy = tf.compat.v2.distribute.MirroredStrategy()\n<\/code><\/pre>\n<\/li>\n<li><p>Problem : Can't find the proper value to be given as <code>tpu_name<\/code> argument.<\/p>\n<\/li>\n<li><p>My research on the problem:<\/p>\n<\/li>\n<\/ol>\n<p>According to the tensorflow documentation in <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/cluster_resolver\/TPUClusterResolver\" rel=\"nofollow noreferrer\">tf.distribute.cluster_resolver.TPUClusterResolver<\/a>, it says that this resolver works only on Google Cloud platform.<\/p>\n<blockquote>\n<p>This is an implementation of cluster resolvers for the Google Cloud\nTPU service.<\/p>\n<p>TPUClusterResolver supports the following distinct environments:\nGoogle Compute Engine Google Kubernetes Engine Google internal<\/p>\n<p>It can be passed into tf.distribute.TPUStrategy to support TF2\ntraining on Cloud TPUs.<\/p>\n<\/blockquote>\n<p>But from <a href=\"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/39721\" rel=\"nofollow noreferrer\">this issue in github<\/a>, I found out that a similar code also works in Azure.<\/p>\n<ol start=\"8\">\n<li>My question :<\/li>\n<\/ol>\n<p>Is there a way I can bypass this resolver and initialize my tpu in <strong>sagemaker<\/strong> ?<\/p>\n<p>Even better, if I can find a way to insert the name or url of sagemaker gpu to the resolver and initiate it from there ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1609059338347,
        "Question_score":0,
        "Question_tags":"python|tensorflow|gpu|amazon-sagemaker|tpu",
        "Question_view_count":355,
        "Owner_creation_time":1517147266417,
        "Owner_last_access_time":1663959762520,
        "Owner_location":null,
        "Owner_reputation":65,
        "Owner_up_votes":89,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":1609063955520,
        "Answer_body":"<p>Let me clarify some confusion here. TPUs are only offered on Google Cloud and the <code>TPUClusterResolver<\/code> implementation queries GCP APIs to get the cluster config for the TPU node. Thus, no you can't use <code>TPUClusterResolver<\/code> with AWS sagemaker, but you should try it out with TPUs on GCP instead or try find some other documentation on Sagemaker's end on how they enable cluster resolving on their end (if they do).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1614007459500,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65464181",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57622122,
        "Question_title":"Custom sagemaker container for training, write forecast to AWS RDS, on a daily basis",
        "Question_body":"<p>I have 3 main process to perform using Amazon SageMaker.<\/p>\n\n<ol>\n<li>Using own training python script, (not using sagemaker container, inbuilt algorithm) [Train.py]<\/li>\n<\/ol>\n\n<p>-> For this, I have referred to this link:<br>\n<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/train-and-host-scikit-learn-models-in-amazon-sagemaker-by-building-a-scikit-docker-container\/\" rel=\"nofollow noreferrer\">Bring own algorithm to AWS sagemaker<\/a>\nand it seems that we can bring our own training script to sagemaker managed training setup, and model artifacts can be uploaded to s3 etc.\nNote: I am using Light GBM model for training.<\/p>\n\n<ol start=\"2\">\n<li>Writing forecast to AWS RDS DB:<\/li>\n<\/ol>\n\n<p>-> There is no need to deploy model and create endpoint, because training will happen everyday, and will create forecast as soon as training completes. (Need to generate forecast in train.py itself)<\/p>\n\n<p>-> <strong>Challenge is how can I write forecast in AWS RDS DB from train.py script. (Given that script is running in Private VPC)<\/strong><\/p>\n\n<ol start=\"3\">\n<li>Scheduling this process as daily job:<\/li>\n<\/ol>\n\n<p>--> I have gone through AWS step functions and seems to be the way to trigger daily training and write forecast to RDS.<\/p>\n\n<p>--> <strong>Challenge is how to use step function for time based trigger and not event based.<\/strong><\/p>\n\n<p>Any suggestions on how to do this? Any best practices to follow? Thank you in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1566547602673,
        "Question_score":0,
        "Question_tags":"python|amazon-rds|amazon-ecs|amazon-sagemaker|aws-step-functions",
        "Question_view_count":466,
        "Owner_creation_time":1469183608840,
        "Owner_last_access_time":1657205920673,
        "Owner_location":"Gurugram, Haryana, India",
        "Owner_reputation":624,
        "Owner_up_votes":95,
        "Owner_down_votes":0,
        "Owner_views":67,
        "Question_last_edit_time":1566550066240,
        "Answer_body":"<p>The way to trigger Step Functions on schedule is by using CloudWatch Events (sort of cron). Check out this tutorial: <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/tutorial-cloudwatch-events-target.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/tutorial-cloudwatch-events-target.html<\/a><\/p>\n\n<p>Don't write to the RDS from your Python code! It is better to write the output to S3 and then \"copy\" the files from S3 into the RDS. Decoupling these batches will make a more reliable and scalable process. You can trigger the bulk copy into the RDS when the files are written to S3 or to a later time when your DB is not too busy. <\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1566713256897,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57622122",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73676684,
        "Question_title":"How to run SageMaker Distributed training from SageMaker Studio?",
        "Question_body":"<p>The sample notebooks for <strong>SageMaker Distributed training<\/strong>, like here:\u00a0https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/distributed_tensorflow_mask_rcnn\/mask-rcnn-scriptmode-s3.ipynb rely on the\u00a0<code>docker build .<\/code> and\u00a0<code>docker push .<\/code> commands, which are not available or installable in <a href=\"https:\/\/aws.amazon.com\/sagemaker\/studio\/\" rel=\"nofollow noreferrer\">Amazon SageMaker Studio<\/a>.<\/p>\n<p>Are there alternatives of these notebooks that are compatible with the SageMaker Studio?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662868156347,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|distributed-training|amazon-machine-learning|amazon-sagemaker-studio",
        "Question_view_count":16,
        "Owner_creation_time":1389887039673,
        "Owner_last_access_time":1664076128463,
        "Owner_location":"Singapore",
        "Owner_reputation":5854,
        "Owner_up_votes":155,
        "Owner_down_votes":70,
        "Owner_views":794,
        "Question_last_edit_time":null,
        "Answer_body":"<p>SageMaker Studio does not support Docker, since the Studio apps are containers themselves. You can use the SageMaker Docker Build tool to build docker images from Studio (uses CodeBuild in the backend). See the blog <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks\/\" rel=\"nofollow noreferrer\">Using the Amazon SageMaker Studio Image Build CLI to build container images from your Studio notebooks<\/a> and the <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli\" rel=\"nofollow noreferrer\">Github repo<\/a> for details.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1663095381000,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73676684",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67707288,
        "Question_title":"Do you get charged for production variants on sagemaker that have no traffic going through them?",
        "Question_body":"<p>I have a deployed model on sagemaker with two production variants. I was wondering if you get charged for both variants even if I set all the traffic to just go through one of them.<\/p>\n<p>The docs on pricing are found below but I couldn't seem to find the answer to this.<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622040262077,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":40,
        "Owner_creation_time":1521737124657,
        "Owner_last_access_time":1663943374747,
        "Owner_location":"Milton Keynes",
        "Owner_reputation":738,
        "Owner_up_votes":22,
        "Owner_down_votes":7,
        "Owner_views":69,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You will be charged as long as the model is running on an instance regardless of whether the traffic is going through it or not as still the model is running on an instance.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1622110871903,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67707288",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65881699,
        "Question_title":"SageMaker failed to extract model data archive tar.gz for container when deploying",
        "Question_body":"<p>I am trying in Amazon Sagemaker to deploy an existing Scikit-Learn model. So a model that wasn't trained on SageMaker, but locally on my machine.<\/p>\n<p>On my local (windows) machine I've saved my model as model.joblib and tarred the model to model.tar.gz.<\/p>\n<p>Next, I've uploaded this model to my S3 bucket ('my_bucket') in the following path s3:\/\/my_bucket\/models\/model.tar.gz. I can see the tar file in S3.<\/p>\n<p>But when I'm trying to deploy the model, it keeps giving the error message &quot;Failed to extract model data archive&quot;.<\/p>\n<p>The .tar.gz is generated on my local machine by running 'tar -czf model.tar.gz model.joblib' in a powershell command window.<\/p>\n<p>The code for uploading to S3<\/p>\n<pre><code>import boto3\ns3 = boto3.client(&quot;s3&quot;, \n              region_name='eu-central-1', \n              aws_access_key_id=AWS_KEY_ID, \n              aws_secret_access_key=AWS_SECRET)\ns3.upload_file(Filename='model.tar.gz', Bucket=my_bucket, Key='models\/model.tar.gz')\n<\/code><\/pre>\n<p>The code for creating the estimator and deploying:<\/p>\n<pre><code>import boto3\nfrom sagemaker.sklearn.estimator import SKLearnModel\n\n...\n\nmodel_data = 's3:\/\/my_bucket\/models\/model.tar.gz'\nsklearn_model = SKLearnModel(model_data=model_data,\n                             role=role,\n                             entry_point=&quot;my-script.py&quot;,\n                             framework_version=&quot;0.23-1&quot;)\npredictor = sklearn_model.deploy(instance_type=&quot;ml.t2.medium&quot;, initial_instance_count=1)                             \n<\/code><\/pre>\n<p>The error message:<\/p>\n<blockquote>\n<p>error message: UnexpectedStatusException: Error hosting endpoint\nsagemaker-scikit-learn-2021-01-24-17-24-42-204: Failed. Reason: Failed\nto extract model data archive for container &quot;container_1&quot; from URL\n&quot;s3:\/\/my_bucket\/models\/model.tar.gz&quot;. Please ensure that the object\nlocated at the URL is a valid tar.gz archive<\/p>\n<\/blockquote>\n<p>Is there a way to see why the archive is invalid?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1611565411923,
        "Question_score":4,
        "Question_tags":"machine-learning|scikit-learn|deployment|amazon-sagemaker",
        "Question_view_count":1859,
        "Owner_creation_time":1472970520890,
        "Owner_last_access_time":1660385188500,
        "Owner_location":"Amersfoort, Nederland",
        "Owner_reputation":424,
        "Owner_up_votes":15,
        "Owner_down_votes":3,
        "Owner_views":21,
        "Question_last_edit_time":1611823707607,
        "Answer_body":"<p>I had a similar issue as well, along with a similar fix to Bas (per comment above).<\/p>\n<p>I was finding I wasn't necessarily having issues with the .tar.gz step, this command does work fine:<\/p>\n<p><code>tar -czf &lt;filename&gt; .\/&lt;directory-with-files&gt;<\/code><\/p>\n<p>but rather with the uploading step.<\/p>\n<p>Manually uploading to S3 should take care of this, however, if you're doing this step programmatically, you might need to double check the steps taken. Bas appears to have had filename issues, mine were around using boto properly. Here's some code that works (Python only here, but watch for similar issues with other libraries):<\/p>\n<pre><code>bucket = 'bucket-name'\nkey = 'directory-inside-bucket'\nfile = 'the file name of the .tar.gz'\n\ns3_client = boto3.client('s3')\ns3_client.upload_file(file, bucket, key)\n<\/code><\/pre>\n<p>Docs: <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1611852069143,
        "Answer_score":1.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65881699",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57661142,
        "Question_title":"AWS S3 and Sagemaker: No such file or directory",
        "Question_body":"<p>I have created an S3 bucket 'testshivaproject' and uploaded an image in it. When I try to access it in sagemaker notebook, it throws an error 'No such file or directory'.<\/p>\n\n<pre><code># import libraries\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np                                   \n\n# Define IAM role\nrole = get_execution_role()\n\nmy_region = boto3.session.Session().region_name # set the region of the instance\n\nprint(\"success :\"+my_region)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> success :us-east-2<\/p>\n\n<pre><code>role\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> 'arn:aws:iam::847047967498:role\/service-role\/AmazonSageMaker-ExecutionRole-20190825T121483'<\/p>\n\n<pre><code>bucket = 'testprojectshiva2' \ndata_key = 'ext_image6.jpg' \ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key) \nprint(data_location)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> s3:\/\/testprojectshiva2\/ext_image6.jpg<\/p>\n\n<pre><code>test = load_img(data_location)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> No such file or directory<\/p>\n\n<p>There are similar questions raised (<a href=\"https:\/\/stackoverflow.com\/questions\/48264656\/load-s3-data-into-aws-sagemaker-notebook\">Load S3 Data into AWS SageMaker Notebook<\/a>) but did not find any solution?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1566834582570,
        "Question_score":1,
        "Question_tags":"amazon-s3|amazon-sagemaker",
        "Question_view_count":4622,
        "Owner_creation_time":1442731325233,
        "Owner_last_access_time":1649309665537,
        "Owner_location":"Hyderabad",
        "Owner_reputation":187,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Thanks for using Amazon SageMaker!<\/p>\n\n<p>I sort of guessed from your description, but are you trying to use the Keras load_img function to load images directly from your S3 bucket?<\/p>\n\n<p>Unfortunately, <a href=\"https:\/\/github.com\/keras-team\/keras\/issues\/11684\" rel=\"nofollow noreferrer\">the load_img function is designed to only load files from disk<\/a>, so passing an s3:\/\/ URL to that function will always return a <code>FileNotFoundError<\/code>.<\/p>\n\n<p>It's common to first download images from S3 before using them, so you can use boto3 or the AWS CLI to download the file before calling load_img.<\/p>\n\n<p><strong>Alternatively<\/strong>, since the load_img function simply creates a <a href=\"https:\/\/en.wikipedia.org\/wiki\/Python_Imaging_Library\" rel=\"nofollow noreferrer\">PIL Image<\/a> object, you can create the PIL object directly from the data in S3 using boto3, and not use the load_img function at all.<\/p>\n\n<p>In other words, you could do something like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from PIL import Image\n\ns3 = boto3.client('s3')\ntest = Image.open(BytesIO(\n    s3.get_object(Bucket=bucket, Key=data_key)['Body'].read()\n    ))\n<\/code><\/pre>\n\n<p>Hope this helps you out in your project!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1567829268830,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57661142",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51803032,
        "Question_title":"AWS SageMaker S3 os.listdir() Access denied",
        "Question_body":"<p>I'm Stumped.  <\/p>\n\n<p>I took my TensorFlow model and moved it up into SageMaker to try it out.  I put my own data up into an s3 bucket, set all the IAM roles\/access (or so I think).  I can read a file from s3.  I can push a new file to s3. I can read local directories from my SageMaker local directories.<\/p>\n\n<p><strong>I cannot traverse my s3 bucket directories.<\/strong>  I turned on logging and I get AccessDenied messages whenever I try access a URI of this format <strong>'s3:\/\/my_bucketName_here\/Directory_of_my_data\/'<\/strong>.<\/p>\n\n<p>Here is what I've done:\nI've confirmed that my notebook uses the AmazonSageMaker-ExecutionRole-***\nI've added AmazonSageMakerFullAccess Policy to that default role\nI've subsequently added AmazonS3FullAccess Policy as well<\/p>\n\n<p>I then created a bucket policy specifically granting s3:* access on the specific bucket to that specific role.<\/p>\n\n<p>Heck, I eventually made the bucket public with ListObjects = Yes.<\/p>\n\n<p>os.listdir() simply fails with file or directory not found and a lot message is created with AccessDenied. (TensorFlow libraries just didn't work, so I went with os.listdir() to simplify things.<\/p>\n\n<p>Finally, I test my access from the Policy Simulator - I selected the Role mentioned above, selected to test s3 and selected all 69 items and they all passed.<\/p>\n\n<p>But I continue to log AccessDenied and cannot actually list the contents of a directory from my SageMaker jupyter notebook.<\/p>\n\n<p>I'm at a loss.  Thoughts?<\/p>\n\n<p>EDIT:\nPer suggestion below, I have the following:\nbucket name contains sagemaker: '[redacted]-test-sagemaker'\nPublic access is off, and the only account is my root account.\n<code>\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::[redacted]-test-sagemaker\"\n            ]\n        },\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::[redacted]-test-sagemaker\/*\"\n            ]\n        }\n    ]\n}<\/code>\nand\narn:aws:iam::aws:policy\/AmazonSageMakerFullAccess<\/p>\n\n<p>Finally the bucket policy after the above failed:\n<code>{\n  \"Id\": \"Policy1534116031672\",\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Stmt1534116026409\",\n      \"Action\": \"s3:*\",\n      \"Effect\": \"Allow\",\n      \"Resource\": \"arn:aws:s3:::[redacted]-test-sagemaker\",\n      \"Principal\": {\n        \"AWS\": [\n          \"arn:aws:iam::[id]:role\/service-role\/AmazonSageMaker-ExecutionRole-***\"\n        ]\n      }\n    }\n  ]\n}<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1534015647103,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":2961,
        "Owner_creation_time":1449682589303,
        "Owner_last_access_time":1634314186743,
        "Owner_location":"Phoenix, AZ, United States",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1534116796513,
        "Answer_body":"<p>So you need to troubleshoot. Here are a few things to check:   <\/p>\n\n<p>0) Make sure the bucket is in the SageMaker region.<\/p>\n\n<p>1) Include the string \"sagemaker\" in your bucket name (e.g., <em>my_bucketName_here-sagemaker<\/em>, SageMaker has out of the box access to buckets named this way.<\/p>\n\n<p>2) Try using the SageMaker S3 <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/session.py\" rel=\"nofollow noreferrer\">default_bucket()<\/a>:<\/p>\n\n<pre><code>import sagemaker\ns = sagemaker.Session()\ns.upload_data(path='somefile.csv', bucket=s.default_bucket(), key_prefix='data\/train')\n<\/code><\/pre>\n\n<p>3) Open terminal on the Notebook instance, to try to list your bucket using AWS CLI in bash:<\/p>\n\n<pre><code>aws iam get-user\naws s3 ls my_bucketName_here\n<\/code><\/pre>\n\n<p>Finally, pasting the bucket's access and resource policy in your question could help others to answer you.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1534057177853,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51803032",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62313532,
        "Question_title":"xgboost on Sagemaker notebook import fails",
        "Question_body":"<p>I am trying to use XGBoost on Sagemaker notebook.<\/p>\n\n<p>I am using <code>conda_python3<\/code> kernel, and the following packages are installed:<\/p>\n\n<ul>\n<li>py-xgboost-mutex<\/li>\n<li>libxgboost<\/li>\n<li>py-xgboost<\/li>\n<li>py-xgboost-gpu<\/li>\n<\/ul>\n\n<p>But once I am trying to import xgboost it fails on import:<\/p>\n\n<pre><code>ModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-5-5943d1bfe3f1&gt; in &lt;module&gt;()\n----&gt; 1 import xgboost as xgb\n\nModuleNotFoundError: No module named 'xgboost'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1591825043587,
        "Question_score":0,
        "Question_tags":"python|jupyter-notebook|conda|xgboost|amazon-sagemaker",
        "Question_view_count":1532,
        "Owner_creation_time":1391632571720,
        "Owner_last_access_time":1663859036253,
        "Owner_location":"Tel Aviv-Yafo, Israel",
        "Owner_reputation":1248,
        "Owner_up_votes":97,
        "Owner_down_votes":1,
        "Owner_views":137,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In Sagemaker notebooks  use the below steps <\/p>\n\n<h3>a) If in Notebook<\/h3>\n\n<p>i)  <code>!type python3<\/code><\/p>\n\n<p>ii) Say the above is \/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python3 for you <\/p>\n\n<p>iii) <code>!\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python3 -m pip install  xgboost<\/code><\/p>\n\n<p>iv)  <code>import xgboost<\/code><\/p>\n\n<hr>\n\n<h3>b) If using Terminal<\/h3>\n\n<p>i) <code>conda activate conda_python3<\/code><br>\nii) <code>pip install xgboost<\/code><\/p>\n\n<p>Disclaimer :  sometimes the installation would fail with gcc version ,in that case  update pip version before running install<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1592571589163,
        "Answer_score":3.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":1592573769990,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62313532",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48365866,
        "Question_title":"How to call Sagemaker training model endpoint API in C#",
        "Question_body":"<p>I have implemented machine learning algorithms through sagemaker.<\/p>\n\n<p>I have installed SDK for .net, and tried by executing below code.<\/p>\n\n<pre><code>Uri sagemakerEndPointURI = new Uri(\"https:\/\/runtime.sagemaker.us-east-2.amazonaws.com\/endpoints\/MyEndpointName\/invocations\");\nAmazon.SageMakerRuntime.Model.InvokeEndpointRequest request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest();\nrequest.EndpointName = \"MyEndpointName\";\nAmazonSageMakerRuntimeClient aawsClient = new AmazonSageMakerRuntimeClient(myAwsAccessKey,myAwsSecreteKey);            \nAmazon.SageMakerRuntime.Model.InvokeEndpointResponse resposnse= aawsClient.InvokeEndpoint(request);\n<\/code><\/pre>\n\n<p>By executing this, I am getting validation error as \"<code>1 validation error detected: Value at 'body' failed to satisfy constraint: Member must not be null<\/code>\"<\/p>\n\n<p>Can anyone guide me on how and what more input data I need to pass to call the given API?<\/p>\n\n<p>EDIT<\/p>\n\n<p>Further I'd tried by provinding body parameter which contains a MemoryStream written by a '.gz' or '.pkl' file, and it giving me error as : \"Error unmarshalling response back from AWS,  HTTP content length exceeded 5246976 bytes.\"<\/p>\n\n<p>EDIT 1\/23\/2018<\/p>\n\n<p>Further I came up with the error message as <\/p>\n\n<blockquote>\n  <p>ERROR - model server - 'TypeError' object has no attribute 'message'<\/p>\n<\/blockquote>\n\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1516531050743,
        "Question_score":8,
        "Question_tags":"c#|amazon-web-services|amazon-s3|sparkr|amazon-sagemaker",
        "Question_view_count":2093,
        "Owner_creation_time":1337759214690,
        "Owner_last_access_time":1532954663377,
        "Owner_location":"Pune India",
        "Owner_reputation":1036,
        "Owner_up_votes":34,
        "Owner_down_votes":1,
        "Owner_views":124,
        "Question_last_edit_time":1516720777517,
        "Answer_body":"<p>Later solved it by <code>Encoding.ASCII.GetBytes<\/code>as in below code.<\/p>\n\n<pre><code> byte[] bytes = System.IO.File.ReadAllBytes(@\"EXCEL_FILE_PATH\");\n    string listA = \"\";\n    while (!reader.EndOfStream)\n        {\n            var line = reader.ReadLine();\n            listA = listA + line + \"\\n\";\n        }\n    byte[] bytes = Encoding.ASCII.GetBytes(listA);\n    request.Body = new MemoryStream(bytes);\n    InvokeEndpointResponse response = sagemakerRunTimeClient.InvokeEndpoint(request);\n    string predictions = Encoding.UTF8.GetString(response.Body.ToArray());\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1519637555373,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48365866",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64286191,
        "Question_title":"How to add keyboard shortcuts to AWS Ground Truth labeler UI?",
        "Question_body":"<p>I'm using AWS Sagemaker Ground Truth for a Custom Labeling Task that involves editing bounding boxes and their labels.  Ground Truth's UI has built-in keyboard shortcuts for doing things like choosing the label for a box, but it seems to lack shortcuts for other built-in UI elements like &quot;No adjustments needed&quot; or the &quot;Submit&quot; button.<\/p>\n<p>Is there a way to add such shortcuts?  I've looked at the crowd-html-elements for customizing the appearance of the page, but can't find anything in there about keyboard shortcuts.  It doesn't even look like crowd-button or crowd-icon-button support specifying a shortcut as an attribute.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1602271699933,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker|labeling",
        "Question_view_count":412,
        "Owner_creation_time":1289772110723,
        "Owner_last_access_time":1663029415383,
        "Owner_location":"Mt Kisco, NY",
        "Owner_reputation":309,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":40,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Could try something like:<\/p>\n<pre><code>document.addEventListener('keydown', function(event) {\n  if (event.shiftKey &amp;&amp; event.keyCode === 13) {\n    document.getElementsByTagName('crowd-bounding-box')[0].shadowRoot.getElementById('nothing-to-adjust').querySelector('label').click();\n  }\n});\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1605831050090,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64286191",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54638364,
        "Question_title":"SageMaker Script Mode + Pipe Mode",
        "Question_body":"<p>I'm training in SageMaker using TensorFlow + Script Mode and currently using 'File' input mode for my data.<\/p>\n\n<p>Has anyone figured out how to stream data using 'Pipe' data format in conjunction with Script Mode training?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1549915827960,
        "Question_score":3,
        "Question_tags":"python|tensorflow|streaming|amazon-sagemaker",
        "Question_view_count":1385,
        "Owner_creation_time":1361339272693,
        "Owner_last_access_time":1663965928400,
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Question_last_edit_time":1549923138163,
        "Answer_body":"<p>You can import <code>sagemaker_tensorflow<\/code> from the training script as follows:<\/p>\n\n<pre><code>from sagemaker_tensorflow import PipeModeDataset\nfrom tensorflow.contrib.data import map_and_batch\n\nchannel = 'my-pipe-channel-name'\n\nds = PipeModeDataset(channel)\nds = ds.repeat(EPOCHS)\nds = ds.prefetch(PREFETCH_SIZE)\nds = ds.apply(map_and_batch(parse, batch_size=BATCH_SIZE,\n                            num_parallel_batches=NUM_PARALLEL_BATCHES))\n<\/code><\/pre>\n\n<p>You can find the full example here: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_pipemode_example\/pipemode.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_pipemode_example\/pipemode.py<\/a><\/p>\n\n<p>You can find documentation about sagemaker_tensorflow here <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset<\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1551201776033,
        "Answer_score":4.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54638364",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55674959,
        "Question_title":"Importing a file into jupyterlabs from s3",
        "Question_body":"<p>I have a file I want to import into a Sagemaker Jupyter notebook python 3 instance for use.  The exact code would be 'import lstm.'  I can store the file in s3 (which would probably be ideal) or locally, whichever you prefer.  I have been searching the internet for a while and have been unable to find a solution to this.  I am actually just trying to run\/understand this code from Suraj Raval's youtube channel: <a href=\"https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot\" rel=\"nofollow noreferrer\">https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot<\/a>.  The 'import lstm' line is failing when I run, and I am trying to figure out how to make this work.  <\/p>\n\n<p>I have tried:\nfrom s3:\/\/... import lstm.  failed\nI have tried some boto3 methods and wasn't able to get it to work.  <\/p>\n\n<pre><code>import time\nimport threading\nimport lstm, etl, json. ##this line\nimport numpy as np\nimport pandas as pd\nimport h5py\nimport matplotlib.pyplot as plt\nconfigs = json.loads(open('configs.json').read())\ntstart = time.time()\n<\/code><\/pre>\n\n<p>I would just like to be able to import the lstm file and all the others into a Jupyter notebook instance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1555242786160,
        "Question_score":3,
        "Question_tags":"amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":1863,
        "Owner_creation_time":1555241852167,
        "Owner_last_access_time":1635370371113,
        "Owner_location":"Chicago, IL, USA",
        "Owner_reputation":393,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":81,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I think you should  be cloning the Github repo in SageMaker instance and not importing the files from S3. I was able to reproduce the Bitcoin Trading Bot notebook from SageMaker by cloning it. You can follow the below steps<\/p>\n\n<h3>Cloning Github Repo to SageMaker Notebook<\/h3>\n\n<ol>\n<li>Open JupyterLab from the AWS SageMaker console.<\/li>\n<li>From the JupyterLab  Launcher, open the Terminal.<\/li>\n<li>Change directory to SageMaker<\/li>\n<\/ol>\n\n<pre><code>cd ~\/SageMaker\n<\/code><\/pre>\n\n<ol start=\"4\">\n<li>Clone the BitCoin Trading Bot <a href=\"https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot\" rel=\"nofollow noreferrer\">git repo<\/a><\/li>\n<\/ol>\n\n<pre><code>git clone https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot.git\ncd Bitcoin_Trading_Bot\n<\/code><\/pre>\n\n<ol start=\"5\">\n<li>Now you can open the notebook <code>Bitcoin LSTM Prediction.ipynb<\/code> and select the Tensorflow Kernel to run the notebook.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YbKic.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YbKic.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<h3>Adding files from local machine to SageMaker Notebook<\/h3>\n\n<p>To add files from your local machine to SageMaker Notebook instance, you can use <a href=\"https:\/\/jupyterlab.readthedocs.io\/en\/stable\/user\/files.html\" rel=\"nofollow noreferrer\">file upload<\/a> functionality in JupyterLab<\/p>\n\n<h3>Adding files from S3 to SageMaker Notebook<\/h3>\n\n<p>To add files from S3 to SageMaker Notebook instance, use AWS CLI or Python SDK to upload\/download files. <\/p>\n\n<p>For example, to download <code>lstm.py<\/code> file from S3 to SageMaker using AWS CLI<\/p>\n\n<pre><code>aws s3 cp s3:\/\/mybucket\/bot\/src\/lstm.py .\n<\/code><\/pre>\n\n<p>Using <code>boto3<\/code> API<\/p>\n\n<pre><code>import boto3\ns3 = boto3.resource('s3')\ns3.meta.client.download_file('mybucket', 'bot\/src\/lstm.py', '.\/lstm.py')\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1555252725130,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1555346571920,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55674959",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52876202,
        "Question_title":"How to bulk test the Sagemaker Object detection model with a .mat dataset or S3 folder of images?",
        "Question_body":"<p>I have trained the following Sagemaker model: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco<\/a><\/p>\n\n<p>I've tried both the JSON and RecordIO version. In both, the algorithm is tested on ONE sample image. However, I have a dataset of 2000 pictures, which I would like to test. I have saved the 2000 jpg pictures in a folder within an S3 bucket and I also have two .mat files (pics + ground truth). How can I apply this model to all 2000 pictures at once and then save the results, rather than doing it one picture at a time?<\/p>\n\n<p>I am using the code below to load a single picture from my S3 bucket:<\/p>\n\n<pre><code>object = bucket.Object('pictures\/pic1.jpg')\nobject.download_file('pic1.jpg')\nimg=mpimg.imread('pic1.jpg')\nimg_name = 'pic1.jpg'\nimgplot = plt.imshow(img)\nplt.show(imgplot)\n\nwith open(img_name, 'rb') as image:\n    f = image.read()\n    b = bytearray(f)\n    ne = open('n.txt','wb')\n    ne.write(b)\n\nimport json\nobject_detector.content_type = 'image\/jpeg'\nresults = object_detector.predict(b)\ndetections = json.loads(results)\nprint (detections['prediction'])\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1539872573950,
        "Question_score":1,
        "Question_tags":"python-3.x|amazon-web-services|amazon-s3|mat|amazon-sagemaker",
        "Question_view_count":112,
        "Owner_creation_time":1489873508190,
        "Owner_last_access_time":1648846099743,
        "Owner_location":null,
        "Owner_reputation":509,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":84,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I'm not sure if I understood your question correctly. However, if you want to feed multiple images to the model at once, you can create a multi-dimensional array of images (byte arrays) to feed the model.<\/p>\n\n<p>The code would look something like this.<\/p>\n\n<pre><code>import numpy as np\n...\n\n#  predict_images_list is a Python list of byte arrays\npredict_images = np.stack(predict_images_list)\n\nwith graph.as_default():\n    #  results is an list of typical results you'd get.\n    results = object_detector.predict(predict_images)\n<\/code><\/pre>\n\n<p>But, I'm not sure if it's a good idea to feed 2000 images at once. Better to batch them in 20-30 images at a time and predict. <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1539965456440,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52876202",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51792005,
        "Question_title":"Sagemaker: DeepAR Hyperparameter Tuning Error",
        "Question_body":"<p>Running into a new issue with tuning DeepAR on SageMaker when trying to initialize a hyperparameter tuning job - this error also occurs when calling the test:mean_wQuantileLoss. I've upgraded the sagemaker package, restarted my instance, restarted the kernel (using a juptyer notebook), and yet the problem persists. <\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the \nCreateHyperParameterTuningJob operation: The objective metric type, [Maximize], that you specified for objective metric, [test:RMSE], isn\u2019t valid for the [156387875391.dkr.ecr.us-west-2.amazonaws.com\/forecasting-deepar:1] algorithm. Choose a valid objective metric type.\n<\/code><\/pre>\n\n<p>Code:<\/p>\n\n<pre><code>my_tuner = HyperparameterTuner(estimator=estimator,\n                               objective_metric_name=\"test:RMSE\",\n                               hyperparameter_ranges=hyperparams,\n                               max_jobs=20,\n                               max_parallel_jobs=2)\n\n# Start hyperparameter tuning job\nmy_tuner.fit(inputs=data_channels)\n\nStack Trace:\nClientError                               Traceback (most recent call last)\n&lt;ipython-input-66-9d6d8de89536&gt; in &lt;module&gt;()\n      7 \n      8 # Start hyperparameter tuning job\n----&gt; 9 my_tuner.fit(inputs=data_channels)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in fit(self, inputs, job_name, include_cls_metadata, **kwargs)\n    255 \n    256         self._prepare_for_training(job_name=job_name, include_cls_metadata=include_cls_metadata)\n--&gt; 257         self.latest_tuning_job = _TuningJob.start_new(self, inputs)\n    258 \n    259     @classmethod\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in start_new(cls, tuner, inputs)\n    525                                                output_config=(config['output_config']),\n    526                                                resource_config=(config['resource_config']),\n--&gt; 527                                                stop_condition=(config['stop_condition']), tags=tuner.tags)\n    528 \n    529         return cls(tuner.sagemaker_session, tuner._current_job_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in tune(self, job_name, strategy, objective_type, objective_metric_name, max_jobs, max_parallel_jobs, parameter_ranges, static_hyperparameters, image, input_mode, metric_definitions, role, input_config, output_config, resource_config, stop_condition, tags)\n    348         LOGGER.info('Creating hyperparameter tuning job with name: {}'.format(job_name))\n    349         LOGGER.debug('tune request: {}'.format(json.dumps(tune_request, indent=4)))\n--&gt; 350         self.sagemaker_client.create_hyper_parameter_tuning_job(**tune_request)\n    351 \n    352     def stop_tuning_job(self, name):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    312                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    313             # The \"self\" in this scope is referring to the BaseClient.\n--&gt; 314             return self._make_api_call(operation_name, kwargs)\n    315 \n    316         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    610             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n    611             error_class = self.exceptions.from_code(error_code)\n--&gt; 612             raise error_class(parsed_response, operation_name)\n    613         else:\n    614             return parsed_response\n\nClientError: An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: \nThe objective metric type, [Maximize], that you specified for objective metric, [test:RMSE], isn\u2019t valid for the [156387875391.dkr.ecr.us-west-2.amazonaws.com\/forecasting-deepar:1] algorithm. \nChoose a valid objective metric type.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1533924861517,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1355,
        "Owner_creation_time":1378935265347,
        "Owner_last_access_time":1541614115827,
        "Owner_location":null,
        "Owner_reputation":27,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It looks like you are trying to maximize this metric, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar-tuning.html\" rel=\"nofollow noreferrer\">test:RMSE can only be minimized<\/a> by SageMaker HyperParameter Tuning. <\/p>\n\n<p>To achieve this in the SageMaker Python SDK, create your HyperparameterTuner with objective_type='Minimize'. You can see the signature of the init method <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tuner.py#L158\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Here is the change you should make to your call to HyperparameterTuner:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>my_tuner = HyperparameterTuner(estimator=estimator,\n                               objective_metric_name=\"test:RMSE\",\n                               objective_type='Minimize',\n                               hyperparameter_ranges=hyperparams,\n                               max_jobs=20,\n                               max_parallel_jobs=2)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1533929944953,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51792005",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65619881,
        "Question_title":"SageMaker tuning job cannot use P2 or P3 instances",
        "Question_body":"<p>I am trying to use AWS SageMaker Hyperparameter tuning job. I can use C5 instance, however, when trying to use either p2 or p3 I get this error.<\/p>\n<pre><code>{{botocore.errorfactory.ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateHyperParameterTuningJob operation: The account-level service limit 'ml.p3.2xlarge for training job usage' is 2 Instances, with current utilization of 0 Instances and a request delta of 5 Instances. Please contact AWS support to request an increase for this limit.\n}}\n<\/code><\/pre>\n<p>Does anybody have idea about it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1610053277393,
        "Question_score":0,
        "Question_tags":"amazon-ec2|amazon-sagemaker|auto-tuning",
        "Question_view_count":209,
        "Owner_creation_time":1495636394673,
        "Owner_last_access_time":1663616691417,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There is a limitation in our account so we had to request for using the instances and increasing the available resource from AWS.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1613677698377,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65619881",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67131617,
        "Question_title":"Connect to redshift using sagemaker notebook instances",
        "Question_body":"<p>I would like to connect to redshift using sagemaker notebook instances. I want to run Unload commands to unload data from redshift to s3 using IAM role and schedule the sagemaker notebook.\nI want to know how I can import db credentials in sagemaker without hardcoding.<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618603786843,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-redshift|amazon-sagemaker",
        "Question_view_count":1504,
        "Owner_creation_time":1468599558957,
        "Owner_last_access_time":1658530343460,
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":107,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use <a href=\"https:\/\/docs.aws.amazon.com\/secretsmanager\/latest\/userguide\/intro.html\" rel=\"nofollow noreferrer\">AWS Secrets Manager<\/a> to store and access credentials. Your Sagemaker execution role should have permission to read from Secrets Manager (AFAIK AWS managed-role does have it). This is the same mechanism that's used by Sagemaker notebooks to get access to github repo, for example<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1618908321397,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67131617",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65102618,
        "Question_title":"struggling to install python package via amazon sagemaker",
        "Question_body":"<p>I am using Amazon Sagemaker and trying to install gaapi4py package via anaconda python3 notebook.<\/p>\n<p>So far I've tried the following commands:<\/p>\n<pre><code>%conda install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>conda install gaapi4py\n\nGot same error:\n\nCollecting package metadata (current_repodata.json): failed\n\nCondaHTTPError: HTTP 000 CONNECTION FAILED for url &lt;https:\/\/conda.anaconda.org\/conda-forge\/linux-64\/current_repodata.json&gt;\nElapsed: -\n\nAn HTTP error occurred when trying to retrieve this URL.\nHTTP errors are often intermittent, and a simple retry will get you on your way.\n'https:\/\/conda.anaconda.org\/conda-forge\/linux-64'\n\n\n\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>Alternatively I've tried the below but it failed as well:<\/p>\n<pre><code>pip install gaapi4py\n<\/code><\/pre>\n<p>Error text:<\/p>\n<pre><code>WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803c50&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c8035f8&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803550&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803400&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803358&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nERROR: Could not find a version that satisfies the requirement gaapi4py (from versions: none)\nERROR: No matching distribution found for gaapi4py\nWARNING: You are using pip version 20.0.2; however, version 20.3 is available.\nYou should consider upgrading via the '\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python -m pip install --upgrade pip' command.\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>What am I doing wrong? All previous packages worked well.<\/p>\n<p>UPD:<\/p>\n<p>Tried also as recommended in amazon book:<\/p>\n<pre><code>import sys\n!{sys.executable} -m pip install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>import sys\n!conda install -y --prefix {sys.prefix} gaapi4py\n<\/code><\/pre>\n<p>Both didn't work neither, getting same errors as above.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1606886205577,
        "Question_score":1,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":1148,
        "Owner_creation_time":1474967309677,
        "Owner_last_access_time":1661467549403,
        "Owner_location":null,
        "Owner_reputation":59,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":66,
        "Question_last_edit_time":1606887753310,
        "Answer_body":"<p>After talking back-in-forth with our IT department I figured out that custom libraries installation was blocked for security reasons.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1644193786213,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65102618",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55531608,
        "Question_title":"Accessing Google BigQuery from AWS SageMaker",
        "Question_body":"<p>When running locally, my Jupyter notebook is able to reference Google BigQuery like so:<\/p>\n\n<pre><code>%%bigquery some_bq_table\n\nSELECT *\nFROM\n  `some_bq_dataset.some_bq_table` \n<\/code><\/pre>\n\n<p>So that later in my notebook I can reference some_bq_table as a pandas dataframe, as exemplified here: <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/visualize-jupyter\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/bigquery\/docs\/visualize-jupyter<\/a><\/p>\n\n<p>I want to run my notebook on AWS SageMaker to test a few things. To authenticate with BigQuery it seems that the only two ways are using a service account on GCP (or locally) or pointing the the SDK to a credentials JSON using an env var (as explained here: <a href=\"https:\/\/cloud.google.com\/docs\/authentication\/getting-started\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/docs\/authentication\/getting-started<\/a>).<\/p>\n\n<p>For example<\/p>\n\n<pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"\/home\/user\/Downloads\/[FILE_NAME].json\"\n<\/code><\/pre>\n\n<p>Is there an easy way to connect to bigquery from SageMaker? My best idea right now is to download the JSON from somewhere to the SageMaker instnace and then set the env var from the python code.<\/p>\n\n<p>For example, I would do this:<\/p>\n\n<pre><code>os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"\/home\/user\/Downloads\/[FILE_NAME].json\"\n<\/code><\/pre>\n\n<p>However, this isn't very secure - I don't like the idea of downloading my credentials JSON to a SageMaker instance (this means I would have to upload the credentials to some private s3 bucket and then store them on the SageMaker instance). Not the end of the world but I rather avoid this. <\/p>\n\n<p>Any ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1554454321543,
        "Question_score":4,
        "Question_tags":"python|google-bigquery|amazon-sagemaker",
        "Question_view_count":1078,
        "Owner_creation_time":1372517243827,
        "Owner_last_access_time":1663839924047,
        "Owner_location":null,
        "Owner_reputation":5921,
        "Owner_up_votes":333,
        "Owner_down_votes":19,
        "Owner_views":388,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As you mentioned GCP currently authenticates using service account, credentials JSON and API tokens. Instead of storing credentials in S3 bucket you can consider using AWS Secrets Manager or AWS Systems Manager Parameter Store to store the GCP credentials and then fetch them in Jupyter notebook. This way credentials can be secured and the credentials file will be created from Secrets Manager only when needed. <\/p>\n\n<p>This is sample code I used previously to connect to BigQuery from SageMaker instance.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nimport boto3\nfrom google.cloud.bigquery import magics\nfrom google.oauth2 import service_account\n\ndef get_gcp_credentials_from_ssm(param_name):\n    # read credentials from SSM parameter store\n    ssm = boto3.client('ssm')\n    # Get the requested parameter\n    response = ssm.get_parameters(Names=[param_name], WithDecryption=True)\n    # Store the credentials in a variable\n    gcp_credentials = response['Parameters'][0]['Value']\n    # save credentials temporarily to a file\n    credentials_file = '\/tmp\/.gcp\/service_credentials.json'\n    with open(credentials_file, 'w') as outfile:  \n        json.dump(json.loads(gcp_credentials), outfile)\n    # create google.auth.credentials.Credentials to use for queries \n    credentials = service_account.Credentials.from_service_account_file(credentials_file)\n    # remove temporary file\n    if os.path.exists(credentials_file):\n        os.remove(credentials_file)\n    return credentials\n\n# this will set the context credentials to use for queries performed in jupyter \n# using bigquery cell magic\nmagics.context.credentials = get_gcp_credentials_from_ssm('my_gcp_credentials')\n<\/code><\/pre>\n\n<p>Please note that SageMaker execution role should have access to SSM and of course other necessary route to connect to GCP. I am not sure if this is the best way though. Hope someone has better way.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1554514774277,
        "Answer_score":7.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55531608",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72203674,
        "Question_title":"Deploy AWS SageMaker pipeline using the cloud development kit (CDK)",
        "Question_body":"<p>I'm looking to deploy the SageMaker pipeline using CDK (<a href=\"https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html<\/a>) but could not find any code examples. Any pointers?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1652282728443,
        "Question_score":0,
        "Question_tags":"terraform-provider-aws|aws-cdk|amazon-sagemaker",
        "Question_view_count":237,
        "Owner_creation_time":1489685785577,
        "Owner_last_access_time":1663881721683,
        "Owner_location":"Canada",
        "Owner_reputation":65,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Question_last_edit_time":null,
        "Answer_body":"<p>CDK L1 Constructs correspond 1:1 to a CloudFormation resource of the same name. The construct props match the resouce properties.  The go-to source is therefore the CloudFormation docs.<\/p>\n<p>The <code>AWS::SageMaker::Pipeline<\/code> <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html#aws-resource-sagemaker-pipeline--examples\" rel=\"nofollow noreferrer\">docs have a more complete example<\/a>.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1652283812907,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72203674",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63199239,
        "Question_title":"Why are shh keys lost on reboot of AWS ec2 instance (sage maker)?",
        "Question_body":"<p>I have an AWS SageMaker notebook running some ML stuff for work, and I have a private github repo with some of my commonly used functions which is formatted in such a way to be pip install-able, so I set up an SSH key by doing this:<\/p>\n<pre><code>ssh-keygen \n\n-t rsa -b 4096 -C &quot;danielwarfield1@gmail.com&quot;\n<\/code><\/pre>\n<p>enter, enter, enter (default save location no password)<\/p>\n<pre><code>eval $(ssh-agent -s)\nssh-add ~\/.ssh\/id_rs\n<\/code><\/pre>\n<p>then I copy the public key into github, then I run this to install my library<\/p>\n<pre><code>$PWD\/pip install git+ssh:\/\/git@github.com\/...\n<\/code><\/pre>\n<p>where <code>$PWD<\/code> is the directory containing pip for the conda env I'm using (tensorflow2_p36 specifically, the one that AWS provides)<\/p>\n<p>this works fine, until I restart the EC2, then it appears my shh key (along with all my other installs) are lost, and I have to repeat the process. I expect the modules to be lost, I know SageMaker manages the environments, but me loosing my ssh key seems peculiar, is there a place I can save my ssh key wher it wont get lost, but I can still find it when I pip install?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596230563253,
        "Question_score":0,
        "Question_tags":"linux|amazon-web-services|amazon-ec2|conda|amazon-sagemaker",
        "Question_view_count":278,
        "Owner_creation_time":1545360696800,
        "Owner_last_access_time":1664070875743,
        "Owner_location":"Earth",
        "Owner_reputation":1011,
        "Owner_up_votes":218,
        "Owner_down_votes":5,
        "Owner_views":93,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The <code>\/home\/ec2-user\/SageMaker<\/code> location is persisted even when you switch down the notebook instance, you can try saving things here to get them persisted. Things saved elsewhere will be lost when you switch off the instance<\/p>\n<p>Regarding private git integration, you can use the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-git-repo.html\" rel=\"nofollow noreferrer\">SageMaker git Notebook integration<\/a>, which uses Secrets Manager to safely handle your credentials<\/p>\n<p>You can perform steps automatically when the notebook starts with a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">lifecycle configuration<\/a>. This is useful for example to standardise and automatise copying of data and environment customization<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1596232881237,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63199239",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62860539,
        "Question_title":"How can I clean memory or use SageMaker instead to avoid MemoryError: Unable to allocate for an array with shape (25000, 2000) and data type float64",
        "Question_body":"<p>I'm using keras to train a model on SageMaker, here's the code I'm using but I hit the error:<\/p>\n<pre><code>MemoryError: Unable to allocate 381. MiB for an array with shape (25000, 2000) \n    and data type float64\n<\/code><\/pre>\n<p>Here's the code:<\/p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom keras.datasets import imdb\nfrom keras import models, layers, optimizers, losses, metrics\nimport matplotlib.pyplot as plt\n\n# load imbd preprocessed dataset\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n    num_words=2000)\n\n# one-hot encoding all the integer into a binary matrix\ndef vectorize_sequences(sequences, dimension=2000):\n    results = np.zeros((len(sequences), dimension))        \n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.                          \n    return results\n\nx_train = vectorize_sequences(train_data)                  \nx_test = vectorize_sequences(test_data)\n<\/code><\/pre>\n<p>Then I get the error.<\/p>\n<p>The first time when I run this code it works but it failed when I tried to re-run it, how I can fix it by cleaning the memory or is there a way that I can use the memory on SageMaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1594553641273,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|tensorflow|keras|amazon-sagemaker",
        "Question_view_count":1210,
        "Owner_creation_time":1540920956270,
        "Owner_last_access_time":1663875036883,
        "Owner_location":"United Kingdom",
        "Owner_reputation":2385,
        "Owner_up_votes":1007,
        "Owner_down_votes":16,
        "Owner_views":585,
        "Question_last_edit_time":1594581031423,
        "Answer_body":"<p>I wouldn't know about SageMaker or AWS specifically, but something you can do is cast your input to <code>float32<\/code>, which takes less memory space. You can cast it like this:<\/p>\n<pre><code>train_data = tf.cast(train_data, tf.float32)\n<\/code><\/pre>\n<p><code>float32<\/code> is the default value of Tensorflow weights so you don't need <code>float64<\/code> anyway. Proof:<\/p>\n<pre><code>import tensorflow as tf\nlayer = tf.keras.layers.Dense(8)\nprint(layer(tf.random.uniform((10, 100), 0, 1)).dtype)\n<\/code><\/pre>\n<pre><code>&lt;dtype: 'float32'&gt;\n<\/code><\/pre>\n<p>My other suggestions are to get less words from your dataset, or to not one-hot encode them. If you're planning on training a recurrent model with an embedding layer, you won't need to anyway.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1594554603093,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62860539",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55112494,
        "Question_title":"Install graphiz on AWS Sagemaker",
        "Question_body":"<p>I'm on a Jupyter notebook using Python3 and trying to plot a tree with code like this:<\/p>\n\n<pre><code>import xgboost as xgb\nfrom xgboost import plot_tree\n\nplot_tree(model, num_trees=4)\n<\/code><\/pre>\n\n<p>On the last line I get:<\/p>\n\n<pre><code>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/xgboost\/plotting.py in to_graphviz(booster, fmap, num_trees, rankdir, yes_color, no_color, **kwargs)\n196         from graphviz import Digraph\n197     except ImportError:\n--&gt; 198         raise ImportError('You must install graphviz to plot tree')\n199 \n200     if not isinstance(booster, (Booster, XGBModel)):\n\nImportError: You must install graphviz to plot tree\n<\/code><\/pre>\n\n<p>How do I install graphviz so I can see the plot_tree?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1552345636840,
        "Question_score":1,
        "Question_tags":"xgboost|amazon-sagemaker",
        "Question_view_count":572,
        "Owner_creation_time":1357263005087,
        "Owner_last_access_time":1663816028883,
        "Owner_location":"Granada Hills, Los Angeles, CA, United States",
        "Owner_reputation":6262,
        "Owner_up_votes":1258,
        "Owner_down_votes":9,
        "Owner_views":428,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I was finally able to learn that Conda has a package which can install it for you. I was able to get it installed by running the command:<\/p>\n\n<pre><code>!conda install python-graphviz --yes\n<\/code><\/pre>\n\n<p>Note the <code>--yes<\/code> is only needed if the installation needs to verify adding\/changing other packages since the Jupyter notebook is not interactive once it is running.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1552366926347,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55112494",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57347278,
        "Question_title":"Does AWS Sagemaker charge for S3 streamed data in PIPE mode (for model training)?",
        "Question_body":"<p>On the AWS developer docs for Sagemaker, they recommend us to use PIPE mode to directly stream large datasets from S3 to the model training containers (since it's faster, uses less disk storage, reduces training time, etc.).<\/p>\n\n<p>However, they don't include information on whether this data streaming transfer is charged for (they only include data transfer pricing for their model building &amp; deployment stages, not training).<\/p>\n\n<p>So, I wanted to ask if anyone knew whether this data transfer in PIPE mode is charged for, since if it is, I don't get how this would be recommended for large datasets, since streaming a few epochs for each model iteration can get prohibitively expensive for large datasets (my dataset, for example, is 6.3TB on S3).<\/p>\n\n<p>Thank you!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1564927931767,
        "Question_score":1,
        "Question_tags":"amazon-web-services|tensorflow|amazon-s3|amazon-sagemaker",
        "Question_view_count":828,
        "Owner_creation_time":1401475981033,
        "Owner_last_access_time":1664084530580,
        "Owner_location":null,
        "Owner_reputation":440,
        "Owner_up_votes":13,
        "Owner_down_votes":1,
        "Owner_views":63,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You are charged for the S3 GET calls that you do similarly to what you would be charged if you used the FILE option of the training. However, these charges are usually marginal compared to the alternatives. <\/p>\n\n<p>When you are using the FILE mode, you need to pay for the local EBS on the instances, and for the extra time that your instances are up and only copying the data from S3. If you are running multiple epochs, you will not benefit much from the PIPE mode, however, when you have so much data (6.3 TB), you don't really need to run multiple epochs. <\/p>\n\n<p>The best usage of PIPE mode is when you can use a <strong>single pass<\/strong> over the data. In the era of big data, this is a better model of operation, as you can't retrain your models often. In SageMaker, you can point to your \"old\" model in the \"model\" channel, and your \"new\" data in the \"train\" channel and benefit from the PIPE mode to the maximum. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1564985810717,
        "Answer_score":3.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":1565038250957,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57347278",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55132599,
        "Question_title":"Difference in usecases for AWS Sagemaker vs Databricks?",
        "Question_body":"<p>I was looking at Databricks because it integrates with AWS services like Kinesis, but it looks to me like SageMaker is a direct competitor to Databricks? We are heavily using AWS, is there any reason to add DataBricks into the stack or odes SageMaker fill the same role?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1552436606600,
        "Question_score":9,
        "Question_tags":"apache-spark|pyspark|databricks|amazon-sagemaker",
        "Question_view_count":11894,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":null,
        "Answer_body":"<p>SageMaker is a great tool for deployment, it simplifies a lot of processes configuring containers, you only need to write 2-3 lines to deploy the model as an endpoint and use it.  SageMaker also provides the dev platform (Jupyter Notebook) which supports Python and Scala (sparkmagic kernal) developing, and i managed installing external scala kernel in jupyter notebook. Overall, SageMaker provides end-to-end ML services. Databricks has unbeatable Notebook environment for Spark development. <\/p>\n\n<p>Conclusion<\/p>\n\n<ol>\n<li><p>Databricks is a better platform for Big data(scala, pyspark) Developing.(unbeatable notebook environment)<\/p><\/li>\n<li><p>SageMaker is better for Deployment. and if you are not working on big data, SageMaker is a perfect choice working with (Jupyter notebook + Sklearn + Mature containers + Super easy deployment). <\/p><\/li>\n<li><p>SageMaker provides \"real time inference\", very easy to build and deploy, very impressive. you can check the official SageMaker Github.\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline<\/a><\/p><\/li>\n<\/ol>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1553118034270,
        "Answer_score":14.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55132599",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68790568,
        "Question_title":"\"errorMessage\": \"Parameter validation failed in Lambda calling SageMaker endpoint",
        "Question_body":"<p>I am trying to invoke a SageMaker enpoint from AWS Lambda using a lambda function.<\/p>\n<p>This is a sample API call to the endpoint from SageMaker Studio, working as expected:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3iTPN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3iTPN.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>here's my Lambda function (<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">inspired from documentation<\/a>):<\/p>\n<pre><code>import os\nimport io\nimport boto3\nimport json\n\n\nENDPOINT_NAME = 'iris-autoscale-6'\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    # print(&quot;Received event: &quot; + json.dumps(event, indent=2))\n    payload = json.loads(json.dumps(event))\n    print(payload)\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\n    print(response)\n    result = json.loads(response['Body'].read().decode())\n    print(result)\n    \n    return result\n<\/code><\/pre>\n<p>My error message:<\/p>\n<pre><code>Test Event Name\nProperTest\n\nResponse\n{\n  &quot;errorMessage&quot;: &quot;Parameter validation failed:\\nInvalid type for parameter Body, value: {'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object&quot;,\n  &quot;errorType&quot;: &quot;ParamValidationError&quot;,\n  &quot;stackTrace&quot;: [\n    &quot;  File \\&quot;\/var\/task\/lambda_function.py\\&quot;, line 17, in lambda_handler\\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 386, in _api_call\\n    return self._make_api_call(operation_name, kwargs)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 678, in _make_api_call\\n    api_params, operation_model, context=request_context)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 726, in _convert_to_request_dict\\n    api_params, operation_model)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/validate.py\\&quot;, line 319, in serialize_to_request\\n    raise ParamValidationError(report=report.generate_report())\\n&quot;\n  ]\n}\n\nFunction Logs\nSTART RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512 Version: $LATEST\n{'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}\n[ERROR] ParamValidationError: Parameter validation failed:\nInvalid type for parameter Body, value: {'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\nTraceback (most recent call last):\n\u00a0\u00a0File &quot;\/var\/task\/lambda_function.py&quot;, line 17, in lambda_handler\n\u00a0\u00a0\u00a0\u00a0response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 386, in _api_call\n\u00a0\u00a0\u00a0\u00a0return self._make_api_call(operation_name, kwargs)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 678, in _make_api_call\n\u00a0\u00a0\u00a0\u00a0api_params, operation_model, context=request_context)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 726, in _convert_to_request_dict\n\u00a0\u00a0\u00a0\u00a0api_params, operation_model)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/validate.py&quot;, line 319, in serialize_to_request\n\u00a0\u00a0\u00a0\u00a0raise ParamValidationError(report=report.generate_report())\nEND RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512\nREPORT RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512  Duration: 26.70 ms  Billed Duration: 27 ms  Memory Size: 128 MB Max Memory Used: 76 MB  Init Duration: 343.10 ms\n<\/code><\/pre>\n<p>Here's the policy attached to the lambda function:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;VisualEditor0&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:sagemaker:ap-south-1:&lt;my-account-id&gt;:endpoint\/iris-autoscale-6&quot;\n        }\n    ]\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629022235437,
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-lambda|boto3|amazon-sagemaker",
        "Question_view_count":499,
        "Owner_creation_time":1559910246180,
        "Owner_last_access_time":1664039951323,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":2046,
        "Owner_up_votes":2858,
        "Owner_down_votes":5,
        "Owner_views":369,
        "Question_last_edit_time":1629023309567,
        "Answer_body":"<p>The issue is that your <code>payload<\/code> has invalid format. It should be one of:<\/p>\n<pre><code>&lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\n<\/code><\/pre>\n<p>The following should address the error (note: you may have many other issues in your code):<\/p>\n<pre><code>    payload = json.dumps(event)\n    print(payload)\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload.encode())\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1629024434667,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68790568",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65054187,
        "Question_title":"Can you use sagemaker Python libraries on my localhost?",
        "Question_body":"<p>I am interested if one can import sagemaker packages on your own local Python environment or whether they are restricted to AWS Sagemaker?<\/p>\n<pre><code>from sagemaker_automl import AutoMLInteractiveRunner, AutoMLLocalCandidate\n<\/code><\/pre>\n<p>For instance can I somehow download the sagemaker_automl?<\/p>\n<p>I know the there are no sagemaker packages available in the conda repository. Perhaps there is some other way to get them.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1606595637157,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":162,
        "Owner_creation_time":1310821356880,
        "Owner_last_access_time":1664050779590,
        "Owner_location":"Slovenia",
        "Owner_reputation":14913,
        "Owner_up_votes":307,
        "Owner_down_votes":1,
        "Owner_views":1093,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The Sagemaker Python SDK is <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">open source and on GitHub<\/a>, as well as published on <a href=\"https:\/\/pypi.org\/project\/sagemaker\/\" rel=\"nofollow noreferrer\">Pypi<\/a>.<\/p>\n<p>You can install it by running:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install sagemaker\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1606601111440,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65054187",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51790720,
        "Question_title":"Brewing up custom ML models on AWS SageMaker",
        "Question_body":"<p>Iam new with SageMaker and I try to use my own sickit-learn algorithm . For this I use Docker.\nI try to do the same task as described here in this github account : <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a><\/p>\n\n<p>My question is should I create manually the repository <strong><code>\/opt\/ml<\/code><\/strong>  (I work with windows OS) ?<\/p>\n\n<p>Can you explain me please?<\/p>\n\n<p>thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1533919042793,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":106,
        "Owner_creation_time":1518617852857,
        "Owner_last_access_time":1572431644507,
        "Owner_location":null,
        "Owner_reputation":495,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":81,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You don't need to create <code>\/opt\/ml<\/code>, SageMaker will do it for you when it launches your training job.<\/p>\n\n<p>The contents of the <code>\/opt\/ml<\/code> directory are determined by the parameters you pass to the CreateTrainingJob API call. The scikit example notebook you linked to describes this (look at the <strong>Running your container<\/strong> sections). You can find more info about this in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">Create a Training Job<\/a> section of the main SageMaker documentation.<\/p>\n\n<hr>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1533928824743,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51790720",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66656120,
        "Question_title":"SageMaker TF 2.3 distributed training",
        "Question_body":"<p>Using SageMaker v2.29.2 and Tensorflow v2.3.2 I'm trying to implement distributed training as explained in the following blogpost:<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23<\/a><\/p>\n<p>However I'm having difficulties importing the smdistributed script.<\/p>\n<p>Here is my code:<\/p>\n<pre><code>import tensorflow as tf\nimport smdistributed.modelparallel.tensorflow as smp\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;temp.py&quot;, line 2, in &lt;module&gt;\n    import smdistributed.modelparallel.tensorflow as smp\nModuleNotFoundError: No module named 'smdistributed'\n<\/code><\/pre>\n<p>What am I missing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1615901050403,
        "Question_score":2,
        "Question_tags":"python|tensorflow|amazon-sagemaker",
        "Question_view_count":383,
        "Owner_creation_time":1324808381143,
        "Owner_last_access_time":1663842092260,
        "Owner_location":null,
        "Owner_reputation":9050,
        "Owner_up_votes":1458,
        "Owner_down_votes":21,
        "Owner_views":1750,
        "Question_last_edit_time":1615921589293,
        "Answer_body":"<p>smdistributed is only available on the SageMaker containers. It is supported for specific TensorFlow versions and you must add:<\/p>\n<pre><code>distribution={'smdistributed': {\n            'dataparallel': {\n                'enabled': True\n            }\n        }}\n<\/code><\/pre>\n<p>On the estimator code in order to enable it<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1623834809930,
        "Answer_score":0.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66656120",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73058582,
        "Question_title":"Access denied for aws public sagemaker xgboost registry",
        "Question_body":"<p>I am trying to pull a prebuilt xgboost image from the public aws xgboost registry specified here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ecr-us-west-2.html#xgboost-us-west-2.title\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ecr-us-west-2.html#xgboost-us-west-2.title<\/a>, however whenever I run the sagemaker pipeline I get the error:<\/p>\n<pre><code>ClientError: Failed to invoke sagemaker:CreateModelPackage. \nError Details: Access denied for registry ID: 246618743249, repository name: sagemaker-xgboost. \nPlease check if your ECR image exists and has proper pull permissions for SageMaker.\n<\/code><\/pre>\n<p>Here is the attached role boundary I am using to run the pipeline:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Action&quot;: [\n                &quot;codebuild:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;codepipeline:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;events:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;logs:CreateLogGroup&quot;,\n                &quot;logs:CreateLogStream&quot;,\n                &quot;logs:DescribeLogStreams&quot;,\n                &quot;logs:DescribeLogGroups&quot;,\n                &quot;logs:PutLogEvents&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;iam:PassRole&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:iam::xxxxxxxxxxxx:role\/ml-*&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;ecr:*&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:ecr:us-west-2:246618743249:repository\/246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-xgboost:1.0-1&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;ecr:GetAuthorizationToken&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;*&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        }\n    ]\n}\n<\/code><\/pre>\n<p>and below is the attached policies for the role:<\/p>\n<pre><code>{\n    &quot;Statement&quot;: [\n        {\n            &quot;Action&quot;: &quot;ecr:*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Sid&quot;: &quot;&quot;\n        }\n    ],\n    &quot;Version&quot;: &quot;2012-10-17&quot;\n}\n<\/code><\/pre>\n<p>plus the AWSCodePipelineFullAccess, AWSCodeBuildAdminAccess, and AmazonSageMakerFullAccess managed policies.<\/p>\n<p>Why can't I access the image\/why am I getting this error? As you can see I gave my role full permissions for the ecr registry in the boundary, and full permissions for ecr in the attached policy.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1658354562373,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-iam|amazon-sagemaker|amazon-ecr",
        "Question_view_count":61,
        "Owner_creation_time":1653511725307,
        "Owner_last_access_time":1663251784530,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":1658354892733,
        "Answer_body":"<p>I had to change the boundary to be this: <code> arn:aws:ecr:us-west-2:246618743249:repository\/sagemaker-xgboost<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1658508649573,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73058582",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58237749,
        "Question_title":"AWS Glue Sagemaker Notebook \"No module named awsglue.transforms\"",
        "Question_body":"<p>I've created a Sagemaker notebook to dev AWS Glue jobs, but when running through the provided example (\"Joining, Filtering, and Loading Relational Data with AWS Glue\") I get the following error:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/3hF5s.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3hF5s.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Does anyone know what I've setup wrong\/haven't setup to cause the import to not work?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1570197038960,
        "Question_score":3,
        "Question_tags":"amazon-web-services|aws-glue|amazon-sagemaker",
        "Question_view_count":3551,
        "Owner_creation_time":1384795490587,
        "Owner_last_access_time":1663932905217,
        "Owner_location":null,
        "Owner_reputation":1012,
        "Owner_up_votes":1106,
        "Owner_down_votes":1,
        "Owner_views":102,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You'll need to download the library files from <a href=\"https:\/\/s3.amazonaws.com\/aws-glue-jes-prod-us-east-1-assets\/etl\/python\/PyGlue.zip\" rel=\"nofollow noreferrer\">here<\/a> for Glue 0.9 or <a href=\"https:\/\/s3.amazonaws.com\/aws-glue-jes-prod-us-east-1-assets\/etl-1.0\/python\/PyGlue.zip\" rel=\"nofollow noreferrer\">here<\/a> for Glue 1.0 (Check your Glue jobs for the version). <\/p>\n\n<p>Put the zip in S3 and reference it in the \"Python library path\" on your Dev Endpoint.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1570201013273,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58237749",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62142825,
        "Question_title":"How to Enable SageMaker Debugger in the SageMaker AutoPilot",
        "Question_body":"<p>I'd like to (a) plot SHAP values out of the SageMaker (b) AutoML pipeline. To achieve (a), debugger shall be used according to: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/<\/a>.<\/p>\n\n<p>But how to enable the debug model in the AutoPilot without hacking into the background?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591056499550,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|amazon-machine-learning",
        "Question_view_count":215,
        "Owner_creation_time":1392607100777,
        "Owner_last_access_time":1664069299613,
        "Owner_location":"Sydney NSW, Australia",
        "Owner_reputation":133,
        "Owner_up_votes":304,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Question_last_edit_time":null,
        "Answer_body":"<p>SageMaker Autopilot doesn't support SageMaker Debugger out of the box currently (as of Dec 2020). You can hack the Hyperparameter Tuning job to pass in a debug parameter.<\/p>\n<p>However, there is a way to use SHAP with Autopilot models. Take a look at this blog post explaining how to use SHAP with SageMaker Autopilot: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1607117580710,
        "Answer_score":0.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62142825",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62319753,
        "Question_title":"How to I pass secrets stored in AWS Secret Manager to a Docker container in Sagemaker?",
        "Question_body":"<p>My code is in R. And I need to excess external database. I am storing database credentials in AWS Secret Manager.<\/p>\n<p>So I first tried using paws library to get aws secrets in R but that would require storing access key, secret id and session token, and I want to avoid that.<\/p>\n<p>Is there a better way to do this? I have created IAM role for Sagemaker. Is it possible to pass secrets as environment variables?<\/p>\n<p>Edit: I wanted to trigger Sagemaker Processing<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1591862670733,
        "Question_score":0,
        "Question_tags":"r|amazon-web-services|docker|amazon-sagemaker",
        "Question_view_count":1063,
        "Owner_creation_time":1444035983570,
        "Owner_last_access_time":1656492622493,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":1593501623133,
        "Answer_body":"<p>I found a simple solution to it. Env variables can be passed via Sagemaker sdk. It minimizes the dependencies.<\/p>\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html<\/a><\/p>\n<p>As another answer suggested, paws can be used as well to get secrets from aws. This would be a better approach<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1593501738047,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1602059220017,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62319753",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52684987,
        "Question_title":"AWS NoCredentials in training",
        "Question_body":"<p>I am attempting to run the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">example code<\/a> for Amazon Sagemaker on a local GPU.  I have copied the code from the Jupyter notebook to the following Python script:<\/p>\n\n<pre><code>import boto3\nimport subprocess\nimport sagemaker\nfrom sagemaker.mxnet import MXNet\nfrom mxnet import gluon\nfrom sagemaker import get_execution_role\nimport os\n\nsagemaker_session = sagemaker.Session()\ninstance_type = 'local'\nif subprocess.call('nvidia-smi') == 0:\n    # Set type to GPU if one is present\n    instance_type = 'local_gpu'\n# role = get_execution_role()\n\ngluon.data.vision.MNIST('.\/data\/train', train=True)\ngluon.data.vision.MNIST('.\/data\/test', train=False)\n\n# successfully connects and uploads data\ninputs = sagemaker_session.upload_data(path='data', key_prefix='data\/mnist')\n\nhyperparameters = {\n    'batch_size': 100,\n    'epochs': 20,\n    'learning_rate': 0.1,\n    'momentum': 0.9,\n    'log_interval': 100\n}\n\nm = MXNet(\"mnist.py\",\n          role=role,\n          train_instance_count=1,\n          train_instance_type=instance_type,\n          framework_version=\"1.1.0\",\n          hyperparameters=hyperparameters)\n\n# fails in Docker container\nm.fit(inputs)\npredictor = m.deploy(initial_instance_count=1, instance_type=instance_type)\nm.delete_endpoint()\n<\/code><\/pre>\n\n<p>where the referenced <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mnist.py\" rel=\"nofollow noreferrer\">mnist.py<\/a> file is exactly as specified on Github. The script fails on <code>m.fit<\/code> in Docker container with the following error: <\/p>\n\n<pre><code>algo-1-1DUU4_1  | Downloading s3:\/\/&lt;S3-BUCKET&gt;\/sagemaker-mxnet-2018-10-07-00-47-10-435\/source\/sourcedir.tar.gz to \/tmp\/script.tar.gz\nalgo-1-1DUU4_1  | 2018-10-07 00:47:29,219 ERROR - container_support.training - uncaught exception during training: Unable to locate credentials\nalgo-1-1DUU4_1  | Traceback (most recent call last):\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/training.py\", line 36, in start\nalgo-1-1DUU4_1  |     fw.train()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/mxnet_container\/train.py\", line 169, in train\nalgo-1-1DUU4_1  |     mxnet_env.download_user_module()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/environment.py\", line 89, in download_user_module\nalgo-1-1DUU4_1  |     cs.download_s3_resource(self.user_script_archive, tmp)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/utils.py\", line 37, in download_s3_resource\nalgo-1-1DUU4_1  |     script_bucket.download_file(script_key_name, target)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 246, in bucket_download_file\nalgo-1-1DUU4_1  |     ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 172, in download_file\nalgo-1-1DUU4_1  |     extra_args=ExtraArgs, callback=Callback)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/transfer.py\", line 307, in download_file\nalgo-1-1DUU4_1  |     future.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 73, in result\nalgo-1-1DUU4_1  |     return self._coordinator.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 233, in result\nalgo-1-1DUU4_1  |     raise self._exception\nalgo-1-1DUU4_1  | NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n\n<p>I am confused that I can authenticate to S3 outside of the container (to pload the training\/test data) but I cannot within the Docker container.  So I am guessing the issues has to do with passing the AWS credentials to the Docker container.  Here is the generated Docker-compose file:<\/p>\n\n<pre><code>networks:\n  sagemaker-local:\n    name: sagemaker-local\nservices:\n  algo-1-1DUU4:\n    command: train\n    environment:\n    - AWS_REGION=us-west-2\n    - TRAINING_JOB_NAME=sagemaker-mxnet-2018-10-07-00-47-10-435\n    image: 123456789012.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.1.0-gpu-py2\n    networks:\n      sagemaker-local:\n        aliases:\n        - algo-1-1DUU4\n    stdin_open: true\n    tty: true\n    volumes:\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/input:\/opt\/ml\/input\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output:\/opt\/ml\/output\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output\/data:\/opt\/ml\/output\/data\n    - \/tmp\/tmpSkaR3x\/model:\/opt\/ml\/model\nversion: '2.1'\n<\/code><\/pre>\n\n<p>Should the AWS credentials be passed in as enviromental variables?<\/p>\n\n<p>I upgraded my <code>sagemaker<\/code> install to after reading <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/403\" rel=\"nofollow noreferrer\">Using boto3 in install local mode?<\/a>, but that had no effect.  I checked the credentials that are being fetched in the Sagemaker session (outside the container) and they appear to be blank, even though I have an <code>~\/.aws\/config<\/code> and <code>~\/.aws\/credentials<\/code> file:<\/p>\n\n<pre><code>{'_token': None, '_time_fetcher': &lt;function _local_now at 0x7f4dbbe75230&gt;, '_access_key': None, '_frozen_credentials': None, '_refresh_using': &lt;bound method AssumeRoleCredentialFetcher.fetch_credentials of &lt;botocore.credentials.AssumeRoleCredentialFetcher object at 0x7f4d2de48bd0&gt;&gt;, '_secret_key': None, '_expiry_time': None, 'method': 'assume-role', '_refresh_lock': &lt;thread.lock object at 0x7f4d9f2aafd0&gt;}\n<\/code><\/pre>\n\n<p>I am new to AWS so I do not know how to diagnose the issue regarding AWS credentials.  My <code>.aws\/config<\/code> file has the following information (with placeholder values):<\/p>\n\n<pre><code>[default]\noutput = json\nregion = us-west-2\nrole_arn = arn:aws:iam::123456789012:role\/SageMakers\nsource_profile = sagemaker-test\n\n[profile sagemaker-test]\noutput = json\nregion = us-west-2\n<\/code><\/pre>\n\n<p>Where the <code>sagemaker-test<\/code> profile has <code>AmazonSageMakerFullAccess<\/code> in the IAM Management Console.<\/p>\n\n<p>The <code>.aws\/credentials<\/code> file has the following information (represented by placeholder values):<\/p>\n\n<pre><code>[default]\naws_access_key_id = 1234567890\naws_secret_access_key = zyxwvutsrqponmlkjihgfedcba\n[sagemaker-test]\naws_access_key_id = 0987654321\naws_secret_access_key = abcdefghijklmopqrstuvwxyz\n<\/code><\/pre>\n\n<p>Lastly, these are versions of the applicable libraries from a <code>pip freeze<\/code>:<\/p>\n\n<pre><code>awscli==1.16.19\nboto==2.48.0\nboto3==1.9.18\nbotocore==1.12.18\ndocker==3.5.0\ndocker-compose==1.22.0\nmxnet-cu91==1.1.0.post0\nsagemaker==1.11.1\n<\/code><\/pre>\n\n<p>Please let me know if I left out any relevant information and thanks for any help\/feedback that you can provide.<\/p>\n\n<p><strong>UPDATE<\/strong>: Thanks for your help, everyone! While attempting some of your suggested fixes, I noticed that <code>boto3<\/code> was out of date, and update it (to <code>boto3-1.9.26<\/code> and <code>botocore-1.12.26<\/code>) which appeared to resolve the issue.  I was not able to find any documentation on that being an issue with <code>boto3==1.9.18<\/code>.  If someone could help me understand what the issue was with <code>boto3<\/code>, I would happy to make mark their answer as correct.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":3,
        "Question_creation_time":1538879533133,
        "Question_score":1,
        "Question_tags":"python|python-2.7|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":1374,
        "Owner_creation_time":1380496984177,
        "Owner_last_access_time":1663610467730,
        "Owner_location":"Gloucester, VA, USA",
        "Owner_reputation":544,
        "Owner_up_votes":174,
        "Owner_down_votes":1,
        "Owner_views":85,
        "Question_last_edit_time":1539893383687,
        "Answer_body":"<p>SageMaker local mode is designed to pick up whatever credentials are available in your boto3 session, and pass them into the docker container as environment variables. <\/p>\n\n<p>However, the version of the sagemaker sdk that you are using (1.11.1 and earlier) will ignore the credentials if they include a token, because that usually indicates short-lived credentials that won't remain valid long enough for a training job to complete or endpoint to be useful.<\/p>\n\n<p>If you are using temporary credentials, try replacing them with permanent ones, or running from an ec2 instance (or SageMaker notebook!) that has an appropriate instance role assigned.<\/p>\n\n<p>Also, the sagemaker sdk's handling of credentials changed in v1.11.2 and later -- temporary credentials will be passed to local mode containers, but with a warning message. So you could just upgrade to a newer version and try again (<code>pip install -U sagemaker<\/code>). <\/p>\n\n<p>Also, try upgrading <code>boto3<\/code> can change, so try using the latest version.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1539818808057,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1542529234207,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52684987",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51725489,
        "Question_title":"QuickSight using ML",
        "Question_body":"<p>I would like to use the ML model I created in AWS in my QuickSight reports.<\/p>\n\n<ul>\n<li>Is there a way to consume the ML endpoint in order to run batch predictions in QuickSight?<\/li>\n<li>Can I define a 'calculated field' in order to do that?<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1533640683263,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-quicksight",
        "Question_view_count":490,
        "Owner_creation_time":1501420854057,
        "Owner_last_access_time":1663846017710,
        "Owner_location":null,
        "Owner_reputation":507,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":78,
        "Question_last_edit_time":null,
        "Answer_body":"<p>At this time there is no direct integration with AWS SageMaker and QuickSight, however you can use utilize SageMaker's batch transform jobs to convert data outside of QuickSight and then import this information into QuickSight for visualization. The output format for SageMaker's batch transform jobs is S3, which is a supported input data source for QuickSight.<\/p>\n\n<ul>\n<li><a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2018\/07\/amazon-sagemaker-supports-high-throughput-batch-transform-jobs-for-non-real-time-inferencing\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/about-aws\/whats-new\/2018\/07\/amazon-sagemaker-supports-high-throughput-batch-transform-jobs-for-non-real-time-inferencing\/<\/a><\/li>\n<\/ul>\n\n<p>Depending on how fancy you want to be, you can also integrate calls to AWS services such as AWS Lambda or AWS SageMaker as a user-defined function (UDF) within your datastore. Here are a few resources that may help:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/redshift\/latest\/dg\/user-defined-functions.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/redshift\/latest\/dg\/user-defined-functions.html<\/a><\/li>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/big-data\/from-sql-to-microservices-integrating-aws-lambda-with-relational-databases\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/big-data\/from-sql-to-microservices-integrating-aws-lambda-with-relational-databases\/<\/a><\/li>\n<\/ul>\n\n<p>Calculated fields will probably not help you in this regard - calculated fields are restricted to a relatively small set of operations, and none of these operations support calls to external sources.<\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/quicksight\/latest\/user\/calculated-field-reference.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/quicksight\/latest\/user\/calculated-field-reference.html<\/a> <\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1534265395897,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51725489",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68396088,
        "Question_title":"Why AWS Lambda Internel Server Error 500 but successfully \/invocations POST 200 in Endpoint SageMaker?",
        "Question_body":"<pre><code>import os\nimport io\nimport boto3\nimport json\nimport csv\n\n\n# grab environment variables\nENDPOINT_NAME = os.environ['ENDPOINT_NAME']\n# grab runtime client\nruntime = boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    # Load data from POST request\n    data = json.loads(json.dumps(event))\n    \n    # Grab the payload\n    payload = data['body']\n    \n    # Invoke the model. In this case the data type is a JSON but can be other things such as a CSV\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                   ContentType='application\/json',\n                                   Body=payload)\n    \n    # Get the body of the response from the model\n    result = response['Body'].read().decode()\n\n    # Return it along with the status code of 200 meaning this was succesful \n    return {\n        'statusCode': 200,\n        'body': result\n    }\n<\/code><\/pre>\n<p><strong>response from AWS Lambda<\/strong><\/p>\n<pre><code>{\n  &quot;errorMessage&quot;: &quot;'body'&quot;,\n  &quot;errorType&quot;: &quot;KeyError&quot;,\n  &quot;stackTrace&quot;: [\n    [\n      &quot;\/var\/task\/lambda_function.py&quot;,\n      18,\n      &quot;lambda_handler&quot;,\n      &quot;payload = data['body']&quot;\n    ]\n  ]\n}\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/h8wvA.png\" rel=\"nofollow noreferrer\">response from Postman 500 Internal Server Error<\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/cuknX.png\" rel=\"nofollow noreferrer\">but successfully invoke POST 200 in SageMaker Endpoint<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1626360722473,
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":277,
        "Owner_creation_time":1512933739527,
        "Owner_last_access_time":1643017987330,
        "Owner_location":"Petaling Jaya, Selangor, Malaysia",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1626360827357,
        "Answer_body":"<p>The issue is when you are trying to parse your payload with data['body']. The data is not being passed in the format that the endpoint is expecting. Use the following code snippet to properly format\/serialize your data for the endpoint. Also to make all this clearer make sure to check for your payload type to make sure you have not serialized again by accident.<\/p>\n<pre><code>    data = json.loads(json.dumps(event))\n    payload = json.dumps(data)\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                       ContentType='application\/json',\n                                       Body=payload)\n    result = json.loads(response['Body'].read().decode())\n<\/code><\/pre>\n<p>I work for AWS &amp; my opinions are my own<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1626975923230,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1626979396583,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68396088",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69335737,
        "Question_title":"How to call a python file in aws sagemaker from angular application?",
        "Question_body":"<p>I have created an angular application that takes an image as input, the image is then passed to a python script that performs a neural style transfer and returns the stylized image. I have created the python file and the angular frontend seperately and I'm stuck on the integration. I am using aws sagemaker to run the python script (due to its computation speed) but I have no idea how to call the python script with the image passed to it from angular. Any suggestions would be really appreciated. Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1632665409997,
        "Question_score":0,
        "Question_tags":"python|angular|amazon-web-services|amazon-sagemaker",
        "Question_view_count":85,
        "Owner_creation_time":1601448674660,
        "Owner_last_access_time":1654501250620,
        "Owner_location":null,
        "Owner_reputation":45,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can create a lambda function and expose it using API gateway to be called by your angular app. this lambda in return will call the  Sagemaker function you have<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1632665752713,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69335737",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54154455,
        "Question_title":"How do you invoke a sagemaker xgboost endpoint from a chalice app?",
        "Question_body":"<p>I built a chalice web-app that is hosted in an s3 bucket and calls an xgboost endpoint. I keep getting an error when I invoke the model through the web-app. When I looked into the Lambda log files I discovered my input is not properly decoding. <code>input_text = app.current_request.raw_body.decode()<\/code> What would be the correct code to decode the input from binary so I can pass in a regular string to my endpoint?<\/p>\n\n<p>Here is the error:<\/p>\n\n<p>botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message \"could not convert string to float: user_input=1%\". <\/p>\n\n<p>Here is my index.html file:<\/p>\n\n<pre><code>&lt;html&gt;\n&lt;head&gt;&lt;\/head&gt;\n&lt;body&gt;\n&lt;form method=\"post\" action=\"&lt;chalice_deployed_http&gt;\"&gt;\n\n&lt;input type=\"text\" name=\"user_input\"&gt;&lt;br&gt;\n\n&lt;input type=\"submit\" value=\"Submit\"&gt;\n&lt;\/form&gt;\n&lt;\/body&gt;\n&lt;\/html&gt;\n<\/code><\/pre>\n\n<p>Here is my app.py file:<\/p>\n\n<pre><code>try:\n    from StringIO import StringIO\nexcept ImportError:\n    from io import StringIO\n\nfrom io import BytesIO\nimport csv\nimport sys, os, base64, datetime, hashlib, hmac\nfrom chalice import Chalice, NotFoundError, BadRequestError\nimport boto3\n\n\napp = Chalice(app_name='&lt;name_of_chalice_app&gt;')\napp.debug = True\n\nsagemaker = boto3.client('sagemaker-runtime')\n\n@app.route('\/', methods=['POST'], content_types=['application\/x-www-form-urlencoded'])\ndef handle_data():\n    input_text = app.current_request.raw_body.decode()\n\n    res = sagemaker.invoke_endpoint(\n                    EndpointName='&lt;endpoint_name&gt;',\n                    Body=input_text,\n                    ContentType='text\/csv',\n                    Accept='Accept'\n                )\n    return res['Body'].read().decode()[0]\n<\/code><\/pre>\n\n<p>I should be able to pass in a string like this:<\/p>\n\n<p>'1,4,26,0.076923077,2,3,1,0.611940299,0.7818181820000001,0.40376569,0.571611506,0.12,12,1,0.0,2,1.0,1,2,6,3,1,1,1,1,1,3,1,0.000666667,1,1,2,2,-1.0,0.490196078,-1.0,0.633928571,6.0,145,2,2,1,3,2,2,1,3,2,3,3,-1.0,1,3,1,1,2,1,2,3,1,3,3,1,3,2,3,-1.0,3,3,1,2,2,1,3,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,0.3497921158934803,0'<\/p>\n\n<p>and get output like this:<\/p>\n\n<p>'5'<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/sb5Nw.jpg\" rel=\"nofollow noreferrer\">When I run it in a jupyter notebook it works.<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1547243089770,
        "Question_score":0,
        "Question_tags":"python|web-applications|chalice|amazon-sagemaker",
        "Question_view_count":805,
        "Owner_creation_time":1546566576913,
        "Owner_last_access_time":1614803818790,
        "Owner_location":null,
        "Owner_reputation":25,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1547479657317,
        "Answer_body":"<p>This worked:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>    input_text = app.current_request.raw_body\n    d = parse_qs(input_text)\n    lst = d[b'user_input'][0].decode()\n    res = sagemaker.invoke_endpoint(\n                    EndpointName='&lt;name-of-SageMaker-Endpoint&gt;',\n                    Body=lst,\n                    ContentType='text\/csv',\n                    Accept='Accept'\n                )\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1547657753670,
        "Answer_score":1.0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54154455",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73520188,
        "Question_title":"How to get batch predictions with jsonl data in sagemaker?",
        "Question_body":"<p>I have a pytorch model that i have tested as a real-time endpoint in sagemaker, now i want to test it with batch inference. I am using jsonl data, and setting up a batch transform job as documented in aws documentation, in addition, i'm using my own inference.py (see sample below). I'm getting a json decode error inside the input_fn , function, when i try =&gt; json.loads(request_body).<\/p>\n<p>the error is =&gt; raise JSONDecodeError(&quot;Extra data&quot;, s, end)<\/p>\n<p>has anyone tried this? I sucessfully tested this model and json input with a real time endpoint in sagemaker, but now i'm trying to switch to batch and it is erroring it out.<\/p>\n<p>inference.py<\/p>\n<pre><code>def model_fn(model_dir):\n   ....\n\n\ndef input_fn(request_body, request_content_type):\n    data = json.loads(request_body)\n    return data\n\ndef predict_fn(data, model)\n  ...\n<\/code><\/pre>\n<p>set up for batch job via lambda<\/p>\n<pre><code>response = client.create_transform_job(\n    TransformJobName='some-job',\n    ModelName='mypytorchmodel',\n    ModelClientConfig={\n        'InvocationsTimeoutInSeconds': 3600,\n        'InvocationsMaxRetries': 1\n    },\n    BatchStrategy='MultiRecord',\n    TransformInput={\n        'DataSource': {\n            'S3DataSource': {\n                'S3DataType': 'S3Prefix',\n                'S3Uri': 's3:\/\/inputpath'\n            }\n        },\n        'ContentType': 'application\/json',\n        'SplitType': 'Line'\n    },\n    TransformOutput={\n        'S3OutputPath': 's3:\/\/outputpath',\n        'Accept': 'application\/json',\n        'AssembleWith': 'Line',\n    },\n    TransformResources={\n        'InstanceType': 'ml.g4dn.xlarge'\n        'InstanceCount': 1\n    }\n)\n<\/code><\/pre>\n<p>input file<\/p>\n<pre><code>{&quot;input&quot; : &quot;input line one&quot;}\n{&quot;input&quot; : &quot;input line two&quot;}\n{&quot;input&quot; : &quot;input line three&quot;}\n{&quot;input&quot; : &quot;input line four&quot;}\n{&quot;input&quot; : &quot;input line five&quot;}\n...\n\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661702506533,
        "Question_score":1,
        "Question_tags":"python|aws-lambda|amazon-sagemaker|inference",
        "Question_view_count":52,
        "Owner_creation_time":1584308275360,
        "Owner_last_access_time":1664049064830,
        "Owner_location":null,
        "Owner_reputation":365,
        "Owner_up_votes":53,
        "Owner_down_votes":2,
        "Owner_views":94,
        "Question_last_edit_time":null,
        "Answer_body":"<p>What is your client side code where you are invoking the endpoint? You should also be properly serializing the data on the client side and handling it in your inference script. Example:<\/p>\n<pre><code>import json\ndata = json.loads(json.dumps(request_body))\npayload = json.dumps(data)\nresponse = client.invoke_endpoint(\n    EndpointName=endpoint_name,\n    ContentType=content_type,\n    Body=payload)\nresult = json.loads(response['Body'].read().decode())['Output']\nresult\n<\/code><\/pre>\n<p>Make sure to also specify your content_type appropriately &quot;application\/jsonlines&quot;.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1661785882720,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73520188",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62919671,
        "Question_title":"conda env build fails with \"[Errno 28] No space left on device\"",
        "Question_body":"<p>I'm trying to build a new conda environment in our Sagemaker ec2 environment in a terminal session.  Packages in the original copy of the environment were corrupted, and the environment became unusable. The issue couldn't be fixed by removing packages and re-installing or using <code>conda update<\/code>.<\/p>\n<p>I nuked the environment with <code>conda env remove -n python3-cn<\/code> and then attempted to recreate the environment with:<\/p>\n<pre><code>conda env create -p \/home\/ec2-user\/SageMaker\/anaconda3\/envs\/python3-cn --file=${HOME}\/SageMaker\/efs\/.sagemaker\/python3-cn_environment.yml --force\n<\/code><\/pre>\n<p>This environment has been created a number of times in several ec2 instances for individual Sagemaker users.<\/p>\n<p>Conda logs the following:<\/p>\n<pre><code>Collecting package metadata (repodata.json): done\nSolving environment: done\n\nDownloading and Extracting Packages\npytest-arraydiff-0.2 | 14 KB     | ##################################################################################################### | 100% \npartd-0.3.8          | 32 KB     | ##################################################################################################### | 100% \n\n... several progress bar lines later...\n\npsycopg2-2.7.5       | 507 KB    | ##################################################################################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\nERROR conda.core.link:_execute(700): An error occurred while installing package 'defaults::mkl-2018.0.3-1'.\nRolling back transaction: done\n\n[Errno 28] No space left on device\n()\n<\/code><\/pre>\n<p>The <code>No space left on device<\/code> error is consistent. I've tried<\/p>\n<ul>\n<li><code>conda clean --all<\/code>, removing the environment, re-building the environment<\/li>\n<li>removing the caches, removing the environment, re-building the environment<\/li>\n<li>removing the environment, shutting down and restarting JuypiterLab (our Sagemaker is configured to create <code>python3-cn<\/code> if the environment doesn't exist when JupyterLab starts)<\/li>\n<\/ul>\n<p>In the first two, I get <code>Errno 28<\/code>.<\/p>\n<p>In the last one, the instance is not created, <code>conda env list<\/code> does not show the <code>python3-cn<\/code>, but I see there is a <code>python3-cn<\/code> directory in the <code>anaconda\/envs\/<\/code> directory. If I do <code>conda activate python3-cn<\/code>, I see the prompt change, but the environment is unusuable. If I try <code>conda update --all<\/code>, I get a notification that one of the package files has been corrupted.<\/p>\n<p>Not really sure what to do here. I'm looking for space hogs, but not really finding anything significant.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1594831106687,
        "Question_score":1,
        "Question_tags":"conda|amazon-sagemaker",
        "Question_view_count":4104,
        "Owner_creation_time":1311046709507,
        "Owner_last_access_time":1662426840390,
        "Owner_location":null,
        "Owner_reputation":56,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Try increasing the ebs volume amount of your notebook ... this blog explains it well: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-notebook-volume-size-up-to-16-tb-with-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-notebook-volume-size-up-to-16-tb-with-amazon-sagemaker\/<\/a><\/p>\n<p>Also, best practice is to use lifecycle configuration scripts to build\/add new dependencies ... official docs: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html<\/a><\/p>\n<p>This github page has some great template examples ... for example setting up specific configs like conda, etc: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1594867069027,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62919671",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58992447,
        "Question_title":"AWS SageMaker notebook list tables using boto3 and PySpark",
        "Question_body":"<p>Having some difficulty executing the following code in AWS SageMaker. It is supposed to just list all of the tables in DynamoDB.<\/p>\n\n<pre><code>import boto3\nresource = boto3.resource('dynamodb', region_name='xxxx')\nresponse  = resource.tables.all()\nfor r in response:\n    print(r.name)\n<\/code><\/pre>\n\n<p>If the SageMaker notebook kernel is set to \"conda_python3\" the code executes fine and the tables are listed out in the notebook as expected (this happens pretty much instantly).<\/p>\n\n<p>However, if I set the kernel to \"Sparkmagic (PySpark)\" the same code infinitely runs and doesn't output the table list at all.<\/p>\n\n<p>Does anyone know why this would happen for the PySpark kernel but not for the conda3 kernel? Ideally I need to run this code as part of a bigger script that relies on PySpark, so would like to get it working with PySpark.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1574419108777,
        "Question_score":1,
        "Question_tags":"pyspark|amazon-dynamodb|boto3|amazon-sagemaker",
        "Question_view_count":440,
        "Owner_creation_time":1384795490587,
        "Owner_last_access_time":1663932905217,
        "Owner_location":null,
        "Owner_reputation":1012,
        "Owner_up_votes":1106,
        "Owner_down_votes":1,
        "Owner_views":102,
        "Question_last_edit_time":1574419700060,
        "Answer_body":"<p>Figured out what the issue was, you need to end an endpoint to tour VPC for DyanmoDB.<\/p>\n\n<p>To do this navigate to:<\/p>\n\n<ol>\n<li>AWS VPC<\/li>\n<li>Endpoints<\/li>\n<li>Create Endpoint<\/li>\n<li>Select the dynamodb service (will be type Gateway)<\/li>\n<li>Select the VPC your Notebook is using<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1574430450807,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58992447",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":47840209,
        "Question_title":"How to Curl an Amazon Sagemaker Endpoint",
        "Question_body":"<p>What is a is the curl command to make a POST request to sage-maker and receive a ML inference?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":1,
        "Question_creation_time":1513373969900,
        "Question_score":11,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":6889,
        "Owner_creation_time":1443201378360,
        "Owner_last_access_time":1653587950970,
        "Owner_location":null,
        "Owner_reputation":749,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Rather than using curl, it's recommended that you use the SageMaker Runtime client to send data and get back inferences from a SageMaker Endpoint:<\/p>\n\n<p><a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html\" rel=\"nofollow noreferrer\">http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1514326523573,
        "Answer_score":4.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47840209",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68741326,
        "Question_title":"Sagemaker Instance not utilising GPU during training",
        "Question_body":"<p>I'm training a Seq2Seq model on Tensorflow on a ml.p3.2xlarge instance. When I tried running the code on google colab, the time per epoch was around 40 mins. However on the instance it's around 5 hours!<\/p>\n<p>This is my training code<\/p>\n<pre><code>def train_model(train_translator, dataset, path, num=8):\n\n  with tf.device(&quot;\/GPU:0&quot;):\n    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=path,\n                                                save_weights_only=True,\n                                                 verbose=1)\n    batch_loss = BatchLogs('batch_loss')\n    train_translator.fit(dataset, epochs=num,callbacks=[batch_loss,cp_callback])  \n\n  return train_translator\n<\/code><\/pre>\n<p>I have also tried without the <code>tf.device<\/code> command and I still get the same timing. Am I doing something wrong?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1628682115840,
        "Question_score":1,
        "Question_tags":"python-3.x|amazon-web-services|machine-learning|tensorflow2.0|amazon-sagemaker",
        "Question_view_count":995,
        "Owner_creation_time":1580472638260,
        "Owner_last_access_time":1663426586317,
        "Owner_location":null,
        "Owner_reputation":127,
        "Owner_up_votes":282,
        "Owner_down_votes":1,
        "Owner_views":32,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I had to force GPU use with the help of<\/p>\n<pre><code>with tf.device('\/device:GPU:0')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1628917656377,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68741326",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73674278,
        "Question_title":"How to use multiple instances with the SageMaker XGBoost built-in algorithm?",
        "Question_body":"<p>If we use multiple instances for training will the built-in algorithm automatically exploit it? For example, what if we used 2 instances for training using built-in XGBoost container and we used the same customer churn example? Will one instance be ignored?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662834395330,
        "Question_score":0,
        "Question_tags":"amazon-web-services|xgboost|amazon-sagemaker|distributed-training",
        "Question_view_count":19,
        "Owner_creation_time":1507661294190,
        "Owner_last_access_time":1663873580420,
        "Owner_location":null,
        "Owner_reputation":98,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":1662834724687,
        "Answer_body":"<p>Yes SageMaker XGBoost supports distributed training. If you set instance count &gt; 1, SageMaker XGBoost will distribute the files from S3 to individual instances and perform distributed training. This, however, requires number of files on S3 &gt;= number of instances. Otherwise, you will be charged for using two training instances without the benefit of using distributed training.<\/p>\n<p>You can find an example here<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone_dist_script_mode.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone_dist_script_mode.ipynb<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1663199408803,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73674278",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52404879,
        "Question_title":"Efficient management of large amounts of data with SageMaker for training a keras model",
        "Question_body":"<p>I'm working on a deep learning project with about 700GB of table-like time series data in thousands of .csv files (each about 15MB). <br>\nAll the data is on S3 and it needs some preprocessing before being fed into the model. The question is how to best go about automating the process of loading, preprocessing and training. <br><br>Is a custom keras generator with some built in preprocessing the best solution?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1537356638503,
        "Question_score":2,
        "Question_tags":"amazon-s3|keras|bigdata|amazon-sagemaker",
        "Question_view_count":423,
        "Owner_creation_time":1488416220370,
        "Owner_last_access_time":1615723626053,
        "Owner_location":"London, UK",
        "Owner_reputation":142,
        "Owner_up_votes":0,
        "Owner_down_votes":1,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Preprocessing implies that this is something you might want to decouple from the model execution and run separately, possibly on a schedule or in response to new data flowing in.<\/p>\n\n<p>If so, you'll probably want to do the preprocessing outside of SageMaker. You could orchestrate it using <a href=\"https:\/\/aws.amazon.com\/glue\/\" rel=\"nofollow noreferrer\">Glue<\/a>, or you could write a custom job and run it through <a href=\"https:\/\/aws.amazon.com\/batch\/\" rel=\"nofollow noreferrer\">AWS Batch<\/a> or alternatively on an EMR cluster.<\/p>\n\n<p>That way, your Keras notebook can load the already preprocessed data, train and test through SageMaker.<\/p>\n\n<p>With a little care, you should be able to perform at least some of the heavy lifting incrementally in the preprocessing step, saving both time and cost downstream in the Deep Learning pipeline.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1537380210983,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52404879",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69666500,
        "Question_title":"Training Job is Stopping in Sagemaker",
        "Question_body":"<p>Recently, I have changed account on AWS and faced with weird error in Sagemaker.<\/p>\n<p>Basically, I'm just checking <code>xgboost<\/code> algo with some toy dataset in this manner:<\/p>\n<pre><code>from sagemaker import image_uris\n\nxgb_image_uri = image_uris.retrieve(&quot;xgboost&quot;, boto3.Session().region_name, &quot;1&quot;)\n\nclf = sagemaker.estimator.Estimator(xgb_image_uri,\n                   role, 1, 'ml.c4.2xlarge',\n                   output_path=&quot;s3:\/\/{}\/output&quot;.format(session.default_bucket()),\n                   sagemaker_session=session)\n\nclf.fit(location_data)\n<\/code><\/pre>\n<p>Then the training job is starting to be executed but for some reason, on downloading data step it stops the training job and displays the following message:<\/p>\n<pre><code>2021-10-21 17:33:27 Downloading - Downloading input data\n2021-10-21 17:33:27 Stopping - Stopping the training job\n2021-10-21 17:33:27 Stopped - Training job stopped\nProfilerReport-1634837444: Stopping\n..\nJob ended with status 'Stopped' rather than 'Completed'. This could mean the job timed out or stopped early for some other reason: Consider checking whether it completed as you expect.\n<\/code><\/pre>\n<p>Also, when I'm trying to go back to training jobs section and check for logs in cloudwatch there is nothing to be displayed. Is it common issue and who had faced with that? Are there any workarounds?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634838048463,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|xgboost|amazon-sagemaker",
        "Question_view_count":361,
        "Owner_creation_time":1386491614717,
        "Owner_last_access_time":1663783708293,
        "Owner_location":null,
        "Owner_reputation":2778,
        "Owner_up_votes":138,
        "Owner_down_votes":1,
        "Owner_views":352,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The problem was most likely with templates for sagemaker that was runned before creating the instance.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1634844298240,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69666500",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63114905,
        "Question_title":"Sagemaker tensorflow endpoint not calling the input_handler when being invoked for a prediction",
        "Question_body":"<p>I'm deploying a <code>tensorflow.serving<\/code> endpoint with a custom <code>inference.py<\/code> script via the <code>entry point<\/code> parameter<\/p>\n<pre><code>model = Model(role='xxx',\n              framework_version='2.2.0',\n              entry_point='inference.py',\n              model_data='xxx')\n\npredictor = model.deploy(instance_type='xxx',\n                         initial_instance_count=1,\n                         endpoint_name='xxx')\n<\/code><\/pre>\n<p>inference.py constains an <code>input_handler<\/code> and an <code>output_handler<\/code> functions, but when i call predict with:<\/p>\n<pre><code>model = Predictor(endpoint_name='xxx')\nurl = 'xxx'\n\ninput = {\n    'instances': [url]\n}\n\npredictions = model.predict(input)\n<\/code><\/pre>\n<p>I'm getting the following <code>error<\/code>:<\/p>\n<p><em>botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;{&quot;error&quot;: &quot;Failed to process element: 0 of 'instances' list. Error: Invalid argument: JSON Value: &quot;xxx&quot; Type: String is not of expected type: float&quot; }&quot;<\/em><\/p>\n<p>It seems the function is never calling the <code>input_handler<\/code> function in inference.py script. Do you know why this might be happening?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1595851293460,
        "Question_score":2,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":474,
        "Owner_creation_time":1402755934690,
        "Owner_last_access_time":1664075083037,
        "Owner_location":"Porto, Portugal",
        "Owner_reputation":5998,
        "Owner_up_votes":2638,
        "Owner_down_votes":56,
        "Owner_views":426,
        "Question_last_edit_time":1595931884943,
        "Answer_body":"<p>Found the problem thanks to AWS support:<\/p>\n<p>I was creating an endpoint that already had an endpoint configuration with the same name and the new configuration wasn't being utilized.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1595952661830,
        "Answer_score":0.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63114905",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54521080,
        "Question_title":"aws sagemaker for detecting text in an image",
        "Question_body":"<p>I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.<\/p>\n\n<p>I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1549300262230,
        "Question_score":4,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":2255,
        "Owner_creation_time":1412300825550,
        "Owner_last_access_time":1663958318850,
        "Owner_location":"West Lafayette, IN, United States",
        "Owner_reputation":463,
        "Owner_up_votes":50,
        "Owner_down_votes":0,
        "Owner_views":111,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The different services will all provide different levels of abstraction for Optical Character Recognition (OCR) depending on what parts of the pipeline you are most comfortable with working with, and what you prefer to have abstracted.<\/p>\n\n<p>Here are a few options:<\/p>\n\n<ul>\n<li><p><strong>Rekognition<\/strong> will provide out of the box OCR with the <a href=\"https:\/\/docs.aws.amazon.com\/rekognition\/latest\/dg\/text-detecting-text-procedure.html\" rel=\"nofollow noreferrer\">DetectText<\/a> feature. However, it seems you will need to perform some sort of pre-processing on your images in your current case in order to get better results. This can be done through any method of your choice (Lambda, EC2, etc).<\/p><\/li>\n<li><p><strong>SageMaker<\/strong> is a tool that will enable you to easily train and deploy your own models (of any type). You have two primary options with SageMaker:<\/p>\n\n<ol>\n<li><p><em>Do-it-yourself option<\/em>: If you're looking to go the route of labeling your own data, gathering a sizable training set, and training your own OCR model, this is possible by training and deploying your own model via SageMaker.<\/p><\/li>\n<li><p><em>Existing OCR algorithm<\/em>: There are many algorithms out there that all have different potential tradeoffs for OCR. One example would be <a href=\"https:\/\/github.com\/tesseract-ocr\/tesseract\" rel=\"nofollow noreferrer\">Tesseract<\/a>. Using this, you can more closely couple your pre-processing step to the text detection.<\/p><\/li>\n<\/ol><\/li>\n<li><p><a href=\"https:\/\/aws.amazon.com\/textract\/\" rel=\"nofollow noreferrer\"><strong>Amazon Textract<\/strong><\/a> (In preview) is a purpose-built dedicated OCR service that may offer better performance depending on what your images look like and the settings you choose. <\/p><\/li>\n<\/ul>\n\n<p>I would personally recommend looking into <a href=\"https:\/\/docparser.com\/blog\/improve-ocr-accuracy\/\" rel=\"nofollow noreferrer\">pre-processing for OCR<\/a> to see if it improves Rekognition accuracy before moving onto the other options. Even if it doesn't improve Rekognition's accuracy, it will still be valuable for most of the other options!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1549313202943,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54521080",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72376872,
        "Question_title":"entry_point file using XGBoost as a framework in sagemaker",
        "Question_body":"<p>Looking at the following source code taken from <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">here<\/a> (SDK v2):<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker.xgboost.estimator import XGBoost\nfrom sagemaker.session import Session\nfrom sagemaker.inputs import TrainingInput\n\n# initialize hyperparameters\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;verbosity&quot;:&quot;1&quot;,\n        &quot;objective&quot;:&quot;reg:linear&quot;,\n        &quot;num_round&quot;:&quot;50&quot;}\n\n# set an output path where the trained model will be saved\nbucket = sagemaker.Session().default_bucket()\nprefix = 'DEMO-xgboost-as-a-framework'\noutput_path = 's3:\/\/{}\/{}\/{}\/output'.format(bucket, prefix, 'abalone-xgb-framework')\n\n# construct a SageMaker XGBoost estimator\n# specify the entry_point to your xgboost training script\nestimator = XGBoost(entry_point = &quot;your_xgboost_abalone_script.py&quot;, \n                    framework_version='1.2-2',\n                    hyperparameters=hyperparameters,\n                    role=sagemaker.get_execution_role(),\n                    instance_count=1,\n                    instance_type='ml.m5.2xlarge',\n                    output_path=output_path)\n\n# define the data type and paths to the training and validation datasets\ncontent_type = &quot;libsvm&quot;\ntrain_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'train'), content_type=content_type)\nvalidation_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'validation'), content_type=content_type)\n\n# execute the XGBoost training job\nestimator.fit({'train': train_input, 'validation': validation_input})\n<\/code><\/pre>\n<p>I wonder where the your_xgboost_abalone_script.py file has to be placed please? So far I used XGBoost as a built-in algorithm from my local machine with similar code (i.e. I span up a training job remotely). Thanks!<\/p>\n<p>PS:<\/p>\n<p>Looking at <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html\" rel=\"nofollow noreferrer\">this<\/a>, and source_dir, I wonder if one can upload Python files to S3. In this case, I take it is has to be tar.gz? Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653478466700,
        "Question_score":0,
        "Question_tags":"python|xgboost|amazon-sagemaker",
        "Question_view_count":74,
        "Owner_creation_time":1267440784443,
        "Owner_last_access_time":1664045779313,
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Question_last_edit_time":1653482394637,
        "Answer_body":"<p><code>your_xgboost_abalone_script.py<\/code> can be created locally. The path you provide is relative to where the code is running.<\/p>\n<p>I.e. <code>your_xgboost_abalone_script.py<\/code> can be located in the same directory where you are running the SageMaker SDK (&quot;source code&quot;).<\/p>\n<p>For example if you have <code>your_xgboost_abalone_script.py<\/code> in the same directory as the source code:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 source_code.py\n\u2514\u2500\u2500 your_xgboost_abalone_script.py\n<\/code><\/pre>\n<p>Then you can point to this file exactly how the documentation depicts:<\/p>\n<pre><code>estimator = XGBoost(entry_point = &quot;your_xgboost_abalone_script.py&quot;, \n.\n.\n.\n)\n<\/code><\/pre>\n<p>The SDK will take <code>your_xgboost_abalone_script.py<\/code> repackage it into a model tar ball and upload it to S3 on your behalf.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1654714910813,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1654796658207,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72376872",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68009703,
        "Question_title":"Can \"Invoke_endpoint\" calls timeout a lambda function?",
        "Question_body":"<p>I am attempting to pass json data into my sagemaker model through a lambda function. Currently, I am using a testing model that makes relatively quick inferences and returns them to the lambda function through the invoke_endpoint call. However, eventually a more advanced model will be implemented which might take longer than a lambda function can fun for (15 minutes maximum) to produce inferences. In the case that I call invoke_endpoint in one lambda function, can I return the response to another lambda function which is invoked by the sagemaker endpoint response? Even better, can I shut down the current lambda function after sending the data to sagemaker, and re-invoke it upon a response? I need to store the inference in DynamoDB, which is why I need a response (Unless I can update the saved model to store inferences directly, in which case I need the lambda function to not expect a response from invoke_endpoint). Sorry for my ignorance, I am a bit new to sagemaker.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1623875736457,
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":181,
        "Owner_creation_time":1622063222450,
        "Owner_last_access_time":1658543544890,
        "Owner_location":null,
        "Owner_reputation":5,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When calling <code>invoke_endpoint<\/code>, the underlying model invocation must take <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-code-how-containe-serves-requests\" rel=\"nofollow noreferrer\">less than 1 minute<\/a>. If a single model execution needs more time to execute, consider running the model in Lambda itself, in SageMaker Training API (if its coldstart is acceptable) or in a custom service. If the invocation is made of several shorter calls you can also chain multiple services together with Step Functions.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1623878015320,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68009703",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50732094,
        "Question_title":"Sagemaker PySpark: Kernel Dead",
        "Question_body":"<p>I followed the instructions <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/\" rel=\"nofollow noreferrer\">here<\/a> to set up an EMR cluster and a SageMaker notebook. I did not have any errors until the last step.<\/p>\n\n<p>When I open a new Notebook in Sagemaker, I get the message:<\/p>\n\n<pre><code>The kernel appears to have died. It will restart automatically.\n<\/code><\/pre>\n\n<p>And then:<\/p>\n\n<pre><code>        The kernel has died, and the automatic restart has failed.\n        It is possible the kernel cannot be restarted. \n        If you are not able to restart the kernel, you will still be able to save the \nnotebook, but running code will no longer work until the notebook is reopened.\n<\/code><\/pre>\n\n<p>This only happens when I use the pyspark\/Sparkmagic kernel. Notebooks opened with the Conda kernel or any other kernel work fine. <\/p>\n\n<p>My EMR cluster is set up exactly as in the instructions, with an added rule:<\/p>\n\n<pre><code>[\n  {\n    \"Classification\": \"spark\",\n    \"Properties\": {\n      \"maximizeResourceAllocation\": \"true\"\n    }\n  }\n]\n<\/code><\/pre>\n\n<p>I'd appreciate any pointers on why this is happening and how I can debug\/fix.<\/p>\n\n<p>P.S.: I've done this successfully in the past without any issues. When I tried re-doing this today, I ran into this issue. I tried re-creating the EMR clusters and Sagemaker notebooks, but that didn't help. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1528337832390,
        "Question_score":2,
        "Question_tags":"pyspark|jupyter|amazon-sagemaker",
        "Question_view_count":1768,
        "Owner_creation_time":1430255275880,
        "Owner_last_access_time":1557543667777,
        "Owner_location":"Pittsburgh, PA",
        "Owner_reputation":125,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Thank you for using Amazon SageMaker.<\/p>\n\n<p>The issue here is Pandas 0.23.0 changed the location of a core class named DataError and SparkMagic has not been updated to require DataError from correct namespace.<\/p>\n\n<p>The workaround for this issue is to downgrade Pandas version in SageMaker Notebook Instance with <code>pip install pandas==0.22.0<\/code>.<\/p>\n\n<p>You can get more information in this open github issue <a href=\"https:\/\/github.com\/jupyter-incubator\/sparkmagic\/issues\/458\" rel=\"noreferrer\">https:\/\/github.com\/jupyter-incubator\/sparkmagic\/issues\/458<\/a>.<\/p>\n\n<p>Let us know if there is any other way we can be of assistance.<\/p>\n\n<p>Thanks,<br>\nNeelam<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1531254842950,
        "Answer_score":5.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50732094",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71385524,
        "Question_title":"Sagemaker training job Fatal error: cannot open file 'train': No such file or directory",
        "Question_body":"<p>I am trying work on bring your own model. I have R code. when i try to run the job its failing.<\/p>\n<p><strong>Training Image:<\/strong><\/p>\n<pre><code>FROM r-base:3.6.3\n\nMAINTAINER Amazon SageMaker Examples &lt;amazon-sagemaker-examples@amazon.com&gt;\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n    wget \\\n    r-base \\\n    r-base-dev \\\n    apt-transport-https \\\n    ca-certificates \\\n    python3 python3-dev pip\n\nENV AWS_DEFAULT_REGION=&quot;us-east-2&quot;\nRUN R -e &quot;install.packages('reticulate', dependencies = TRUE, warning = function(w) stop(w))&quot;\nRUN R -e &quot;install.packages('readr', dependencies = TRUE, warning = function(w) stop(w))&quot;\nRUN R -e &quot;install.packages('dplyr', dependencies = TRUE, warning = function(w) stop(w))&quot;\n\nRUN pip install --quiet --no-cache-dir \\\n    'boto3&gt;1.0&lt;2.0' \\\n    'sagemaker&gt;2.0&lt;3.0'    \n\nENTRYPOINT [&quot;\/usr\/bin\/Rscript&quot;]\n<\/code><\/pre>\n<p><strong>Source code:<\/strong><\/p>\n<pre><code>rcode\n    \u2514\u2500\u2500 train.R\n    \u2514\u2500\u2500 train.tar.gz\n<\/code><\/pre>\n<p>Build<\/p>\n<pre><code>- aws s3 cp $CODEBUILD_SRC_DIR\/rcode\/ s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/code\/training --recursive\n<\/code><\/pre>\n<p><strong>Serverless.com yaml<\/strong><\/p>\n<pre><code>           SagemakerRCodeTrainingStep:\n            Type: Task\n            Resource: ${self:custom.sageMakerTrainingJob}\n            Parameters:\n              TrainingJobName.$: &quot;$.sageMakerTrainingJobName&quot;\n              DebugHookConfig:\n                S3OutputPath: &quot;s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/models\/rmodel&quot;\n              AlgorithmSpecification:\n                TrainingImage: ${self:custom.sagemakerRExecutionContainerURI}\n                TrainingInputMode: &quot;File&quot;\n              OutputDataConfig:\n                S3OutputPath: &quot;s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/models\/rmodel&quot;\n              StoppingCondition:\n                MaxRuntimeInSeconds: ${self:custom.maxRuntime}\n              ResourceConfig:\n                InstanceCount: 1\n                InstanceType: &quot;ml.m5.xlarge&quot;\n                VolumeSizeInGB: 30\n              RoleArn: ${self:custom.stateMachineRoleARN}\n              InputDataConfig:\n                - DataSource:\n                    S3DataSource:\n                      S3DataType: &quot;S3Prefix&quot;\n                      S3Uri: &quot;s3:\/\/${self:custom.datasetsFilePath}\/data\/processed\/train&quot;\n                      S3DataDistributionType: &quot;FullyReplicated&quot;\n                  ChannelName: &quot;train&quot;\n              HyperParameters:\n                sagemaker_submit_directory: &quot;s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/code\/training\/train.tar.gz&quot;\n                sagemaker_program: &quot;train.R&quot;\n                sagemaker_enable_cloudwatch_metrics: &quot;false&quot;\n                sagemaker_container_log_level: &quot;20&quot;\n                sagemaker_job_name: &quot;sagemaker-r-learn-2022-02-28-09-56-33-234&quot;\n                sagemaker_region: ${self:provider.region}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646676784137,
        "Question_score":1,
        "Question_tags":"r|amazon-sagemaker|serverless.com",
        "Question_view_count":369,
        "Owner_creation_time":1285763771347,
        "Owner_last_access_time":1658319981990,
        "Owner_location":"Bangalore, India",
        "Owner_reputation":6958,
        "Owner_up_votes":123,
        "Owner_down_votes":1,
        "Owner_views":787,
        "Question_last_edit_time":1646720798767,
        "Answer_body":"<p>I am not sure which <code>TrainingImage<\/code> you are using and all the files in your container.\nThat being said, I suspect you are using a custom container.<\/p>\n<p>SageMaker Training Jobs look for a <code>train<\/code> file and run your container as <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">follows<\/a>:<\/p>\n<pre><code>docker run image train\n<\/code><\/pre>\n<p>You can change this behavior by setting the <code>ENTRYPOINT<\/code> in your Dockerfile. Please see this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/r_examples\/r_byo_r_algo_hpo\/Dockerfile#L47\" rel=\"nofollow noreferrer\">Dockerfile<\/a> from the <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/main\/r_examples\/r_byo_r_algo_hpo\" rel=\"nofollow noreferrer\">r_byo_r_algo_hpo<\/a> example.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1646690790177,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71385524",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57808963,
        "Question_title":"Sagemaker instance does automatic cd at startup",
        "Question_body":"<p>I have a Sagemaker instance that's linked to a github repo <code>my-repo<\/code>, and every time I open a new terminal, I see this immediately at startup: <\/p>\n\n<pre><code>sh-4.2$ cd \"my-repo\"\nsh: cd: my-repo: No such file or directory\n<\/code><\/pre>\n\n<p>I assumed something was in the .bashrc or .bash_profile that prompted this (failed) <code>cd<\/code> but it's not in there. Any ideas where I should look for what's causing this behavior? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1567698318190,
        "Question_score":0,
        "Question_tags":"shell|command-line|amazon-sagemaker",
        "Question_view_count":304,
        "Owner_creation_time":1386023479737,
        "Owner_last_access_time":1614533016280,
        "Owner_location":null,
        "Owner_reputation":725,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The issue is not specific to SageMaker Notebook instances. Rather, it is a bug in the Git extension of JupyterLab. You can find details around this here: <a href=\"https:\/\/github.com\/jupyterlab\/jupyterlab-git\/issues\/346\" rel=\"nofollow noreferrer\">https:\/\/github.com\/jupyterlab\/jupyterlab-git\/issues\/346<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1567699632857,
        "Answer_score":1.0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57808963",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52940677,
        "Question_title":"AWS Sagemaker: AttributeError: module 'pandas' has no attribute 'core'",
        "Question_body":"<p>Let me prefix this by saying I'm very new to tensorflow and even newer to AWS Sagemaker.<\/p>\n\n<p>I have some tensorflow\/keras code that I wrote and tested on a local dockerized Jupyter notebook and it runs fine. In it, I import a csv file as my input.<\/p>\n\n<p>I use Sagemaker to spin up a jupyter notebook instance with conda_tensorflow_p36. I modified the pandas.read_csv() code to point to my input file, now hosted on a S3 bucket.<\/p>\n\n<p>So I changed this line of code from<\/p>\n\n<pre><code>import pandas as pd\n\ndata = pd.read_csv(\"\/input.csv\", encoding=\"latin1\")\n<\/code><\/pre>\n\n<p>to this<\/p>\n\n<pre><code>import pandas as pd\n\ndata = pd.read_csv(\"https:\/\/s3.amazonaws.com\/my-sagemaker-bucket\/input.csv\", encoding=\"latin1\")\n<\/code><\/pre>\n\n<p>and I get this error<\/p>\n\n<pre><code>AttributeError: module 'pandas' has no attribute 'core'\n<\/code><\/pre>\n\n<p>I'm not sure if it's a permissions issue. I read that as long as I name my bucket with the string \"sagemaker\" it should have access to it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1540264856293,
        "Question_score":0,
        "Question_tags":"pandas|tensorflow|amazon-sagemaker",
        "Question_view_count":1028,
        "Owner_creation_time":1319234288810,
        "Owner_last_access_time":1663375872053,
        "Owner_location":null,
        "Owner_reputation":4966,
        "Owner_up_votes":744,
        "Owner_down_votes":11,
        "Owner_views":304,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Pull our data from S3 for example:<\/p>\n\n<pre><code>import boto3\nimport io\nimport pandas as pd\n\n\n# Set below parameters\nbucket = '&lt;bucket name&gt;'\nkey = 'data\/training\/iris.csv'\nendpointName = 'decision-trees'\n\n# Pull our data from S3\ns3 = boto3.client('s3')\nf = s3.get_object(Bucket=bucket, Key=key)\n\n# Make a dataframe\nshape = pd.read_csv(io.BytesIO(f['Body'].read()), header=None)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1540283269857,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52940677",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69782294,
        "Question_title":"AWS Sagemaker output how to read file with multiple json objects spread out over multiple lines",
        "Question_body":"<p>I have a bunch of json files that look like this<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.01027186680585146, 0.04181386157870293, -0.07363887131214142, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.011699287220835686, 0.04741542786359787, -0.07899319380521774, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n<\/code><\/pre>\n<p>Which I can read in with<\/p>\n<pre><code>f = open(file_name)\ndata = []\nfor line in f:\n   data.append(json.dumps(line))\n<\/code><\/pre>\n<p>But I have another file with output like this<\/p>\n<pre><code>{\n    &quot;predictions&quot;: [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]\n    ]\n}\n<\/code><\/pre>\n<p>I.e. the json is formatted over several lines, so I can't simply read the json in line for line. Is there an easy way to parse this? Or do I have to write something that stitches together each json object line by line and the does json.loads?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635629350997,
        "Question_score":1,
        "Question_tags":"python|json|amazon-sagemaker",
        "Question_view_count":248,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":1635655049277,
        "Answer_body":"<p>Hmm,  as far as I know there's unfortunately no way to load a <a href=\"https:\/\/jsonlines.org\/\" rel=\"nofollow noreferrer\">JSONL<\/a> format data using <code>json.loads<\/code>. One option though, is to come up with a helper function that can convert it to a valid JSON string, as below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import json\n\nstring = &quot;&quot;&quot;\n{\n    &quot;predictions&quot;: [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]\n    ]\n}\n&quot;&quot;&quot;\n\n\ndef json_lines_to_json(s: str) -&gt; str:\n    # replace the first occurrence of '{'\n    s = s.replace('{', '[{', 1)\n\n    # replace the last occurrence of '}\n    s = s.rsplit('}', 1)[0] + '}]'\n\n    # now go in and replace all occurrences of '}' immediately followed\n    # by newline with a '},'\n    s = s.replace('}\\n', '},\\n')\n\n    return s\n\n\nprint(json.loads(json_lines_to_json(string)))\n<\/code><\/pre>\n<p>Prints:<\/p>\n<pre><code>[{'predictions': [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]]}, {'predictions': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]}, {'predictions': [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]]}]\n<\/code><\/pre>\n<p><strong>Note:<\/strong> your first example actually doesn't seem like valid JSON (or at least JSON lines from my understanding). In particular, this part appears to be invalid due to a trailing comma after the last array element:<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664, ], ...}\n<\/code><\/pre>\n<p>To ensure it's valid after calling the helper function, you'd also need to remove the trailing commas, so each line is in the below format:<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], ...},\n<\/code><\/pre>\n<hr \/>\n<p>There also appears to be a <a href=\"https:\/\/stackoverflow.com\/questions\/50475635\/loading-jsonl-file-as-json-objects\/50475669\">similar question<\/a> where they suggest splitting on newlines and calling <code>json.loads<\/code> on each line; actually it should be (slightly) less performant to call <code>json.loads<\/code> multiple times on each object, rather than once on the list, as I show below.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from timeit import timeit\nimport json\n\n\nstring = &quot;&quot;&quot;\\\n{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.01027186680585146, 0.04181386157870293, -0.07363887131214142 ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.011699287220835686, 0.04741542786359787, -0.07899319380521774 ], &quot;word&quot;: &quot;blah blah blah&quot;}\\\n&quot;&quot;&quot;\n\n\ndef json_lines_to_json(s: str) -&gt; str:\n\n    # Strip newlines from end, then replace all occurrences of '}' followed\n    # by a newline, by a '},' followed by a newline.\n    s = s.rstrip('\\n').replace('}\\n', '},\\n')\n\n    # return string value wrapped in brackets (list)\n    return f'[{s}]'\n\n\nn = 10_000\n\nprint('string replace:        ', timeit(r'json.loads(json_lines_to_json(string))', number=n, globals=globals()))\nprint('json.loads each line:  ', timeit(r'[json.loads(line) for line in string.split(&quot;\\n&quot;)]', number=n, globals=globals()))\n<\/code><\/pre>\n<p>Result:<\/p>\n<pre><code>string replace:         0.07599360000000001\njson.loads each line:   0.1078384\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1635633656723,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1635655742577,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69782294",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56835306,
        "Question_title":"Download file using boto3 within Docker container deployed on Sagemaker Endpoint",
        "Question_body":"<p>I have built my own Docker container that provides inference code to be deployed as endpoint on Amazon Sagemaker. However, this container needs to have access to some files from s3. The used IAM role has access to all s3 buckets that I am trying to reach.<\/p>\n\n<p>Code to download files using a boto3 client:<\/p>\n\n<pre><code>import boto3\n\nmodel_bucket = 'my-bucket'\n\ndef download_file_from_s3(s3_path, local_path):\n    client = boto3.client('s3')\n    client.download_file(model_bucket, s3_path, local_path)\n<\/code><\/pre>\n\n<p>The IAM role's policies:<\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::my-bucket\/*\"\n            ]\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>Starting the docker container locally allows me to download files from s3 just like expected. <\/p>\n\n<p>Deploying as an endpoint on Sagemaker, however, the request times out:<\/p>\n\n<pre><code>botocore.vendored.requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='my-bucket.s3.eu-central-1.amazonaws.com', port=443): Max retries exceeded with url: \/path\/to\/my-file (Caused by ConnectTimeoutError(&lt;botocore.awsrequest.AWSHTTPSConnection object at 0x7f66244e69b0&gt;, 'Connection to my-bucket.s3.eu-central-1.amazonaws.com timed out. (connect timeout=60)'))\n<\/code><\/pre>\n\n<p>Any help is appreciated!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1561982365937,
        "Question_score":4,
        "Question_tags":"python-3.x|amazon-web-services|boto3|amazon-sagemaker",
        "Question_view_count":1171,
        "Owner_creation_time":1456411465890,
        "Owner_last_access_time":1663933136733,
        "Owner_location":null,
        "Owner_reputation":105,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For anyone coming across this question, when creating a model, the 'Enable Network Isolation' property defaults to True.\nFrom AWS docs:<\/p>\n<blockquote>\n<p>If you enable network isolation, the containers are not able to make any outbound network calls, even to other AWS services such as Amazon S3. Additionally, no AWS credentials are made available to the container runtime environment.<\/p>\n<\/blockquote>\n<p>So this property needs to be set to False in order to connect to any other AWS service.<\/p>\n<p><img src=\"https:\/\/i.stack.imgur.com\/5qERm.jpg\" alt=\"AWS Sagemaker UI Network Isolation set to False\" \/><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1562142120397,
        "Answer_score":0.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1641299334400,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56835306",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71126832,
        "Question_title":"Amazon Sagemaker: User Input data validation in Inference Endpoint",
        "Question_body":"<p>I have successfully built a Sagemaker endpoint using a Tensorflow model. The pre and post processing is done inside &quot;inference.py&quot; which calls a handler function based on this tutorial: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s<\/a><\/p>\n<p>My questions are:<\/p>\n<ul>\n<li>Which method is good for validating user input data within inference.py?<\/li>\n<li>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/li>\n<li>How is this compatible with the API gateway placed above the endpoint?<\/li>\n<\/ul>\n<p>Here is the structure of the inference.py with the desired validation check as a comment:<\/p>\n<pre><code>import json\nimport requests\n\n\ndef handler(data, context):\n    &quot;&quot;&quot;Handle request.\n    Args:\n        data (obj): the request data\n        context (Context): an object containing request and configuration details\n    Returns:\n        (bytes, string): data to return to client, (optional) response content type\n    &quot;&quot;&quot;\n    processed_input = _process_input(data, context)\n    response = requests.post(context.rest_uri, data=processed_input)\n    return _process_output(response, context)\n\n\ndef _process_input(data, context):\n    if context.request_content_type == 'application\/json':\n        # pass through json (assumes it's correctly formed)\n        d = data.read().decode('utf-8')\n        data_dict = json.loads(data)\n\n\n        # -----&gt;   if data_dict['input_1'] &gt; 25000:\n        # -----&gt;       return some error specific message with status code 123\n\n\n        return some_preprocessing_function(data_dict)\n\n    raise ValueError('{{&quot;error&quot;: &quot;unsupported content type {}&quot;}}'.format(\n        context.request_content_type or &quot;unknown&quot;))\n\n\ndef _process_output(data, context):\n    if data.status_code != 200:\n        raise ValueError(data.content.decode('utf-8'))\n\n    response_content_type = context.accept_header\n    prediction = data.content\n    return prediction, response_content_type\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644929807890,
        "Question_score":0,
        "Question_tags":"python|validation|amazon-sagemaker|endpoint|inference",
        "Question_view_count":245,
        "Owner_creation_time":1613661928947,
        "Owner_last_access_time":1663929308913,
        "Owner_location":null,
        "Owner_reputation":160,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":1644931522733,
        "Answer_body":"<p>I will answer your questions inline below:<\/p>\n<ol>\n<li><em>Which method is good for validating user input data within inference.py?<\/em><\/li>\n<\/ol>\n<p>Seeing that you have a <code>handler<\/code> function, <code>input_handler<\/code> and <code>output_handler<\/code> are ignored. Thus, inside your <code>handler<\/code> function (as you are correctly doing) you can have the validation logic.<\/p>\n<ol start=\"2\">\n<li><em>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/em><\/li>\n<\/ol>\n<p>I like to think of my SageMaker endpoint as a web server. Thus, you can return any valid HTTP response code with a response message. Please see this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_batch_transform\/tensorflow_cifar-10_with_inference_script\/code\/inference.py#L47\" rel=\"nofollow noreferrer\">inference.py<\/a> file that I found as a reference.<\/p>\n<pre><code>_return_error(\n            415, 'Unsupported content type &quot;{}&quot;'.format(context.request_content_type or &quot;Unknown&quot;)\n        )\n\ndef _return_error(code, message):\n    raise ValueError(&quot;Error: {}, {}&quot;.format(str(code), message))\n<\/code><\/pre>\n<ol start=\"3\">\n<li><em>How is this compatible with the API gateway placed above the endpoint?<\/em><\/li>\n<\/ol>\n<p>Please see this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">link<\/a> for details on Creating a machine learning-powered REST API with Amazon API Gateway mapping templates and Amazon SageMaker.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1645567161520,
        "Answer_score":1.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71126832",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71340893,
        "Question_title":"When I get a prediction from sagemaker endpoint, what does the endpoint do?",
        "Question_body":"<p>In sagemaker, the docs talk about inference scripts requiring to have 4 specific functions. When we get a prediction, the python SDK sends a request to the endpoint.<\/p>\n<p>Then the inference script runs. But I cannot find where in the SDK the inference script is run.<\/p>\n<p>When I navigate through the sdk code the <code>Predictor.predict()<\/code> method calls the sagemaker session to post a request to the endpoint and get a response. That is the final step in the sdk. Sagemaker is obviously doing something when it receives that request.<\/p>\n<p>What is the code that it runs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646326739290,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":405,
        "Owner_creation_time":1578932319743,
        "Owner_last_access_time":1663935286190,
        "Owner_location":"Ireland",
        "Owner_reputation":1012,
        "Owner_up_votes":91,
        "Owner_down_votes":96,
        "Owner_views":66,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The endpoint is essentially a Flask web server running in a Docker container<\/p>\n<p>If it's a scikit-learn image, when you invoke the endpoint, it loads your script from S3, then...<\/p>\n<p>It calls <code>input_fn(request_body: bytearray, content_type) -&gt; np.ndarray<\/code> to parse the <code>request_body<\/code> into a numpy array<\/p>\n<p>Then it calls your <code>model_fn(model_dir: str) -&gt; object<\/code> function to load the model from <code>model_dir<\/code> and return the model<\/p>\n<p>Then it calls <code>predict_fn(input_object: np.ndarray, model: object) -&gt; np.array<\/code>, which calls your <code>model.predict()<\/code> function and returns the prediction<\/p>\n<p>Then it calls <code>output_fn(prediction: np.array, accept: str)<\/code> to take the result from <code>predict_fn<\/code> and encode it to the <code>accept<\/code> type<\/p>\n<p>You don't need to implement all of these functions yourself, as there are defaults<\/p>\n<p>You <strong>do<\/strong> need to implement <code>model_fn<\/code><\/p>\n<p>You only need to implement <code>input_fn<\/code> if you have non numeric data<\/p>\n<p>You only need to implement <code>predict_fn<\/code> if your model uses something other than <code>.predict()<\/code><\/p>\n<p>You can see how the default function implementations work <a href=\"https:\/\/github.com\/aws\/sagemaker-scikit-learn-container\/blob\/master\/src\/sagemaker_sklearn_container\/serving.py\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1646329084320,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1646761223947,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71340893",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63305569,
        "Question_title":"How to mount an EFS volume on AWS Sagemaker Studio",
        "Question_body":"<p>I have tried to follow the normal (non-studio) documentation on mounting an EFS file system, as can be found <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/mount-an-efs-file-system-to-an-amazon-sagemaker-notebook-with-lifecycle-configurations\/\" rel=\"nofollow noreferrer\">here<\/a>, however, these steps don't work in a studio notebook. Specifically, the <code>sudo mount -t nfs ...<\/code> does not work in both the Image terminal and the system terminal.<\/p>\n<p>How do I mount an EFS file system that already exists to amazon Sagemaker, so I can access the data\/ datasets I stored in them?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596816839977,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":2331,
        "Owner_creation_time":1483370766803,
        "Owner_last_access_time":1664056213153,
        "Owner_location":"London, UK",
        "Owner_reputation":15819,
        "Owner_up_votes":827,
        "Owner_down_votes":21,
        "Owner_views":1395,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Update: I spoke to an AWS Solutions Architect, and he confirms that EFS is not supported on Sagemaker Studio.<\/p>\n<hr \/>\n<p><strong>Workaround:<\/strong><\/p>\n<p>Instead of mounting your old EFS, you can mount the SageMaker studio EFS onto an EC2 instance, and copy over the data manually. You would need the correct EFS storage volume id, and you'll find your newly copied data available in Sagemaker Studio. <em>I have not actually done this though.<\/em><\/p>\n<p>To find the EFS id, look at the section &quot;Manage your storage volume&quot; <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tasks.html#manage-your-storage-volume\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1596816839977,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1597991730413,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63305569",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66561959,
        "Question_title":"Sagemaker Endpoint BrokenPipeError at DeepAR Prediction",
        "Question_body":"<p>I've created an SageMaker Endpoint from a trained DeepAR-Model using following code:<\/p>\n<pre><code>job_name = estimator.latest_training_job.job_name\n\nendpoint_name = sagemaker_session.endpoint_from_job(\n    job_name=job_name,\n    initial_instance_count=1,\n    instance_type=&quot;ml.m4.xlarge&quot;,\n    image_uri=image_uri,\n    role=role\n)\n<\/code><\/pre>\n<p>Now I want to test my model using a <code>test.json<\/code>-Dataset (<strong>66.2MB<\/strong>).\nI've created that file according to various tutorials\/sample-notebooks (same as <code>train.json<\/code>, but with <code>prediction-length<\/code>-less values.<\/p>\n<p>For that, I've written the following code:<\/p>\n<pre><code>class DeepARPredictor(sagemaker.predictor.Predictor):\n    def set_prediction_parameters(self, freq, prediction_length):\n        self.freq = freq\n        self.prediction_length = prediction_length\n\n    def predict(self, ts, num_samples=100, quantiles=[&quot;0.1&quot;, &quot;0.5&quot;, &quot;0.9&quot;]):\n        prediction_times = [x.index[-1] + pd.Timedelta(1, unit=self.freq) for x in ts]\n        req = self.__encode_request(ts, num_samples, quantiles)\n        res = super(DeepARPredictor, self).predict(req, initial_args={&quot;ContentType&quot;: &quot;application\/json&quot;})\n        return self.__decode_response(res, prediction_times)\n\n    def __encode_request(self, ts, num_samples, quantiles):\n        instances = [{&quot;start&quot;: str(ts[k].index[0]), &quot;target&quot;: list(ts[k])} for k in range(len(ts))]\n        configuration = {\n            &quot;num_samples&quot;: num_samples,\n            &quot;output_types&quot;: [&quot;quantiles&quot;],\n            &quot;quantiles&quot;: quantiles,\n        }\n        http_request_data = {&quot;instances&quot;: instances, &quot;configuration&quot;: configuration}\n        return json.dumps(http_request_data).encode( &quot;utf-8&quot;)\n\n    def __decode_response(self, response, prediction_times):\n        response_data = json.loads(response.decode(&quot;utf-8&quot;))\n        list_of_df = []\n        for k in range(len(prediction_times)):\n            prediction_index = pd.date_range(\n                start=prediction_times[k], freq=self.freq, periods=self.prediction_length\n            )\n            list_of_df.append(\n                pd.DataFrame(data=response_data[&quot;predictions&quot;][k][&quot;quantiles&quot;], index=prediction_index)\n            )\n        return list_of_df\n<\/code><\/pre>\n<p>But after running the following block:<\/p>\n<pre><code>predictor = DeepARPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\npredictor.set_prediction_parameters(freq, prediction_length)\nlist_of_df = predictor.predict(time_series_training)\n<\/code><\/pre>\n<p>I've getting a BrokenPipeError:<\/p>\n<pre><code>---------------------------------------------------------------------------\nBrokenPipeError                           Traceback (most recent call last)\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    676                 headers=headers,\n--&gt; 677                 chunked=chunked,\n    678             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\n    391         else:\n--&gt; 392             conn.request(method, url, **httplib_request_kw)\n    393 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in request(self, method, url, body, headers, encode_chunked)\n   1261         &quot;&quot;&quot;Send a complete request to the server.&quot;&quot;&quot;\n-&gt; 1262         self._send_request(method, url, body, headers, encode_chunked)\n   1263 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_request(self, method, url, body, headers, *args, **kwargs)\n     92         rval = super(AWSConnection, self)._send_request(\n---&gt; 93             method, url, body, headers, *args, **kwargs)\n     94         self._expect_header_set = False\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in _send_request(self, method, url, body, headers, encode_chunked)\n   1307             body = _encode(body, 'body')\n-&gt; 1308         self.endheaders(body, encode_chunked=encode_chunked)\n   1309 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in endheaders(self, message_body, encode_chunked)\n   1256             raise CannotSendHeader()\n-&gt; 1257         self._send_output(message_body, encode_chunked=encode_chunked)\n   1258 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_output(self, message_body, *args, **kwargs)\n    119             message_body = None\n--&gt; 120         self.send(msg)\n    121         if self._expect_header_set:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in send(self, str)\n    203             return\n--&gt; 204         return super(AWSConnection, self).send(str)\n    205 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in send(self, data)\n    995         try:\n--&gt; 996             self.sock.sendall(data)\n    997         except TypeError:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in sendall(self, data, flags)\n    974                 while count &lt; amount:\n--&gt; 975                     v = self.send(byte_view[count:])\n    976                     count += v\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in send(self, data, flags)\n    943                     self.__class__)\n--&gt; 944             return self._sslobj.write(data)\n    945         else:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in write(self, data)\n    641         &quot;&quot;&quot;\n--&gt; 642         return self._sslobj.write(data)\n    643 \n\nBrokenPipeError: [Errno 32] Broken pipe\n\nDuring handling of the above exception, another exception occurred:\n\nProtocolError                             Traceback (most recent call last)\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/httpsession.py in send(self, request)\n    319                 decode_content=False,\n--&gt; 320                 chunked=self._chunked(request.headers),\n    321             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    726             retries = retries.increment(\n--&gt; 727                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n    728             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/util\/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)\n    378             # Disabled, indicate to re-raise the error.\n--&gt; 379             raise six.reraise(type(error), error, _stacktrace)\n    380 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/packages\/six.py in reraise(tp, value, tb)\n    733             if value.__traceback__ is not tb:\n--&gt; 734                 raise value.with_traceback(tb)\n    735             raise value\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    676                 headers=headers,\n--&gt; 677                 chunked=chunked,\n    678             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\n    391         else:\n--&gt; 392             conn.request(method, url, **httplib_request_kw)\n    393 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in request(self, method, url, body, headers, encode_chunked)\n   1261         &quot;&quot;&quot;Send a complete request to the server.&quot;&quot;&quot;\n-&gt; 1262         self._send_request(method, url, body, headers, encode_chunked)\n   1263 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_request(self, method, url, body, headers, *args, **kwargs)\n     92         rval = super(AWSConnection, self)._send_request(\n---&gt; 93             method, url, body, headers, *args, **kwargs)\n     94         self._expect_header_set = False\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in _send_request(self, method, url, body, headers, encode_chunked)\n   1307             body = _encode(body, 'body')\n-&gt; 1308         self.endheaders(body, encode_chunked=encode_chunked)\n   1309 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in endheaders(self, message_body, encode_chunked)\n   1256             raise CannotSendHeader()\n-&gt; 1257         self._send_output(message_body, encode_chunked=encode_chunked)\n   1258 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_output(self, message_body, *args, **kwargs)\n    119             message_body = None\n--&gt; 120         self.send(msg)\n    121         if self._expect_header_set:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in send(self, str)\n    203             return\n--&gt; 204         return super(AWSConnection, self).send(str)\n    205 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in send(self, data)\n    995         try:\n--&gt; 996             self.sock.sendall(data)\n    997         except TypeError:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in sendall(self, data, flags)\n    974                 while count &lt; amount:\n--&gt; 975                     v = self.send(byte_view[count:])\n    976                     count += v\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in send(self, data, flags)\n    943                     self.__class__)\n--&gt; 944             return self._sslobj.write(data)\n    945         else:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in write(self, data)\n    641         &quot;&quot;&quot;\n--&gt; 642         return self._sslobj.write(data)\n    643 \n\nProtocolError: ('Connection aborted.', BrokenPipeError(32, 'Broken pipe'))\n\nDuring handling of the above exception, another exception occurred:\n\nConnectionClosedError                     Traceback (most recent call last)\n&lt;ipython-input-14-95dda20e8a70&gt; in &lt;module&gt;\n      1 predictor = DeepARPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n      2 predictor.set_prediction_parameters(freq, prediction_length)\n----&gt; 3 list_of_df = predictor.predict(time_series_training)\n\n&lt;ipython-input-13-a0fbac2b9b07&gt; in predict(self, ts, num_samples, quantiles)\n      7         prediction_times = [x.index[-1] + pd.Timedelta(1, unit=self.freq) for x in ts]\n      8         req = self.__encode_request(ts, num_samples, quantiles)\n----&gt; 9         res = super(DeepARPredictor, self).predict(req, initial_args={&quot;ContentType&quot;: &quot;application\/json&quot;})\n     10         return self.__decode_response(res, prediction_times)\n     11 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, target_variant)\n    123 \n    124         request_args = self._create_request_args(data, initial_args, target_model, target_variant)\n--&gt; 125         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n    126         return self._handle_response(response)\n    127 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    355                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    356             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 357             return self._make_api_call(operation_name, kwargs)\n    358 \n    359         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    661         else:\n    662             http, parsed_response = self._make_request(\n--&gt; 663                 operation_model, request_dict, request_context)\n    664 \n    665         self.meta.events.emit(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_request(self, operation_model, request_dict, request_context)\n    680     def _make_request(self, operation_model, request_dict, request_context):\n    681         try:\n--&gt; 682             return self._endpoint.make_request(operation_model, request_dict)\n    683         except Exception as e:\n    684             self.meta.events.emit(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in make_request(self, operation_model, request_dict)\n    100         logger.debug(&quot;Making request for %s with params: %s&quot;,\n    101                      operation_model, request_dict)\n--&gt; 102         return self._send_request(request_dict, operation_model)\n    103 \n    104     def create_request(self, params, operation_model=None):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _send_request(self, request_dict, operation_model)\n    135             request, operation_model, context)\n    136         while self._needs_retry(attempts, operation_model, request_dict,\n--&gt; 137                                 success_response, exception):\n    138             attempts += 1\n    139             # If there is a stream associated with the request, we need\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _needs_retry(self, attempts, operation_model, request_dict, response, caught_exception)\n    254             event_name, response=response, endpoint=self,\n    255             operation=operation_model, attempts=attempts,\n--&gt; 256             caught_exception=caught_exception, request_dict=request_dict)\n    257         handler_response = first_non_none_response(responses)\n    258         if handler_response is None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in emit(self, event_name, **kwargs)\n    354     def emit(self, event_name, **kwargs):\n    355         aliased_event_name = self._alias_event_name(event_name)\n--&gt; 356         return self._emitter.emit(aliased_event_name, **kwargs)\n    357 \n    358     def emit_until_response(self, event_name, **kwargs):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in emit(self, event_name, **kwargs)\n    226                  handlers.\n    227         &quot;&quot;&quot;\n--&gt; 228         return self._emit(event_name, kwargs)\n    229 \n    230     def emit_until_response(self, event_name, **kwargs):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in _emit(self, event_name, kwargs, stop_on_response)\n    209         for handler in handlers_to_call:\n    210             logger.debug('Event %s: calling handler %s', event_name, handler)\n--&gt; 211             response = handler(**kwargs)\n    212             responses.append((handler, response))\n    213             if stop_on_response and response is not None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempts, response, caught_exception, **kwargs)\n    181 \n    182         &quot;&quot;&quot;\n--&gt; 183         if self._checker(attempts, response, caught_exception):\n    184             result = self._action(attempts=attempts)\n    185             logger.debug(&quot;Retry needed, action of: %s&quot;, result)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    249     def __call__(self, attempt_number, response, caught_exception):\n    250         should_retry = self._should_retry(attempt_number, response,\n--&gt; 251                                           caught_exception)\n    252         if should_retry:\n    253             if attempt_number &gt;= self._max_attempts:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in _should_retry(self, attempt_number, response, caught_exception)\n    275             # If we've exceeded the max attempts we just let the exception\n    276             # propogate if one has occurred.\n--&gt; 277             return self._checker(attempt_number, response, caught_exception)\n    278 \n    279 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    315         for checker in self._checkers:\n    316             checker_response = checker(attempt_number, response,\n--&gt; 317                                        caught_exception)\n    318             if checker_response:\n    319                 return checker_response\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    221         elif caught_exception is not None:\n    222             return self._check_caught_exception(\n--&gt; 223                 attempt_number, caught_exception)\n    224         else:\n    225             raise ValueError(&quot;Both response and caught_exception are None.&quot;)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in _check_caught_exception(self, attempt_number, caught_exception)\n    357         # the MaxAttemptsDecorator is not interested in retrying the exception\n    358         # then this exception just propogates out past the retry code.\n--&gt; 359         raise caught_exception\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _do_get_response(self, request, operation_model)\n    198             http_response = first_non_none_response(responses)\n    199             if http_response is None:\n--&gt; 200                 http_response = self._send(request)\n    201         except HTTPClientError as e:\n    202             return (None, e)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _send(self, request)\n    267 \n    268     def _send(self, request):\n--&gt; 269         return self.http_session.send(request)\n    270 \n    271 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/httpsession.py in send(self, request)\n    349                 error=e,\n    350                 request=request,\n--&gt; 351                 endpoint_url=request.url\n    352             )\n    353         except Exception as e:\n\nConnectionClosedError: Connection was closed before we received a valid response from endpoint URL\n<\/code><\/pre>\n\n<p>Somebody know's why this happens?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1615369010763,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|machine-learning|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":244,
        "Owner_creation_time":1607069622450,
        "Owner_last_access_time":1662644221550,
        "Owner_location":null,
        "Owner_reputation":101,
        "Owner_up_votes":40,
        "Owner_down_votes":5,
        "Owner_views":63,
        "Question_last_edit_time":1615369103570,
        "Answer_body":"<p>I believe that Tarun might on the right path. The BrokenPipeError that you got is thrown when the connection is abruptly closed. See <a href=\"https:\/\/docs.python.org\/3\/library\/exceptions.html#BrokenPipeError\" rel=\"nofollow noreferrer\">the python docs for BrokenPipeError<\/a>.\nThe SageMaker endpoint probably drops the connection as soon as you go over the limit of 5MB. I suggest you try a smaller dataset. Also the data you send might get enlarged because of how sagemaker.tensorflow.model.TensorFlowPredictor encodes the data according to <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/799#issuecomment-492698717\" rel=\"nofollow noreferrer\">this comment<\/a> on a similar issue.<\/p>\n<p>If that doesn't work I've also seen a couple of people having problems with their networks in general. Specifically firewall\/antivirus (<a href=\"https:\/\/github.com\/aws\/aws-cli\/issues\/3999#issuecomment-531151161\" rel=\"nofollow noreferrer\">for example this comment<\/a>) or network timeout.<\/p>\n<p>Hope this points you in the right direction.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1616165232250,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66561959",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65889143,
        "Question_title":"upload image dataset to S3 sagemaker",
        "Question_body":"<p>my dataset is 3 folders (train, validation and test) of images. each folder has two subfolders (cat1 and cat2). I am using AWS sage maker to preprocess my data and train my model. we all know that we have to upload the training data to S3 bucket before starting the &quot;.fit&quot; process.\nI want to know how to upload my data set to S3<\/p>\n<pre><code># general prefix\nprefix='chest-xray'\n#unique train\/test prefixes\ntrain_prefix   = '{}\/{}'.format(prefix, 'train')\nval_prefix   = '{}\/{}'.format(prefix, 'validation')\ntest_prefix    = '{}\/{}'.format(prefix, 'test')\n\n# uploading data to S3, and saving locations\ntrain_path  = sagemaker_session.upload_data(train_data, bucket=bucket, key_prefix=train_prefix)\n<\/code><\/pre>\n<p>what the train_data parameters should look like<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611594075077,
        "Question_score":0,
        "Question_tags":"amazon-s3|amazon-sagemaker",
        "Question_view_count":444,
        "Owner_creation_time":1477757915557,
        "Owner_last_access_time":1663322043803,
        "Owner_location":null,
        "Owner_reputation":79,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":"<p>According to the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/utility\/session.html#sagemaker.session.Session.upload_data\" rel=\"nofollow noreferrer\">documentation<\/a> <code>train_data<\/code> is the local path of the file to upload to S3, so you need this file locally where you are launching the training job. If you are using a notebook this is not the way to do. You have instead to manually upload your dataset in a S3 bucket. I suggest to preprocess your dataset in a single file (tfrecord for example if you are using TF) and upload that file to S3. You can do it using the AWS web console or using the AWS-CLI with the <code>aws s3 cp yourfile s3:\/\/your-bucket <\/code>command.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1611678605710,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65889143",
        "Question_exclusive_tag":"Amazon SageMaker"
    }
]