[
    {
        "Question_id":65939058,
        "Question_title":"MLflow stores artifacts on GCP buckets but is not able to read them",
        "Question_body":"<p>I've found an almost identical question <a href=\"https:\/\/stackoverflow.com\/questions\/63727235\/mlflow-artifacts-storing-artifactsgoogle-cloud-storage-but-not-displaying-them?newreg=923da08a362547daab64c7d7e2275423\">here<\/a> but don't have enough reputation to add comments so will ask again hoping that someone has found a solution in the mean time.<\/p>\n<p>I am using MLflow (1.13.1) to track model performance and GCP Storage to store model artifacts.\nMLflow is running on a GCP VM instance and my python application uses a service account with Storage Object Creator and Storage Object Viewer roles (and then I've also added storage.buckets.get permissions) to store artifacts in GCP buckets and read from them.\nEverything is working as expected with parameters and metrics correctly displaying in MLflow UI and model artifacts correctly stored in buckets. The problem is that the model artifacts do not show up in MLflow UI because of this error:<\/p>\n<pre><code>Unable to list artifacts stored under gs:\/******\/artifacts for the current run. \nPlease contact your tracking server administrator to notify them of this error, \nwhich can happen when the tracking server lacks permission to list artifacts under the current run's root artifact directory.\n<\/code><\/pre>\n<p>The quoted artifacts location exists and contains the correct model artifacts, and MLflow should be able to read the artifacts because of the Storage Object Viewer role and the storage.buckets.get permissions.<\/p>\n<p>Any suggestion on what could be wrong? Thank you.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611843895217,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|mlflow",
        "Question_view_count":428,
        "Owner_creation_time":1611841996690,
        "Owner_last_access_time":1637883583270,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I've found the problem just after posting the question.\nI had forgotten to install the <code>google-cloud-storage<\/code> library on the GCP VM. Everything works as expected now.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1611845294603,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65939058",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61980244,
        "Question_title":"How to fix Artifacts not showing in MLflow UI",
        "Question_body":"<p>I'd used MLflow and logged parameters using the function below (from pydataberlin).<\/p>\n<pre><code>def train(alpha=0.5, l1_ratio=0.5):\n    # train a model with given parameters\n    warnings.filterwarnings(&quot;ignore&quot;)\n    np.random.seed(40)\n\n    # Read the wine-quality csv file (make sure you're running this from the root of MLflow!)\n    data_path = &quot;data\/wine-quality.csv&quot;\n    train_x, train_y, test_x, test_y = load_data(data_path)\n\n    # Useful for multiple runs (only doing one run in this sample notebook)    \n    with mlflow.start_run():\n        # Execute ElasticNet\n        lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n        lr.fit(train_x, train_y)\n\n        # Evaluate Metrics\n        predicted_qualities = lr.predict(test_x)\n        (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n        # Print out metrics\n        print(&quot;Elasticnet model (alpha=%f, l1_ratio=%f):&quot; % (alpha, l1_ratio))\n        print(&quot;  RMSE: %s&quot; % rmse)\n        print(&quot;  MAE: %s&quot; % mae)\n        print(&quot;  R2: %s&quot; % r2)\n\n        # Log parameter, metrics, and model to MLflow\n        mlflow.log_param(key=&quot;alpha&quot;, value=alpha)\n        mlflow.log_param(key=&quot;l1_ratio&quot;, value=l1_ratio)\n        mlflow.log_metric(key=&quot;rmse&quot;, value=rmse)\n        mlflow.log_metrics({&quot;mae&quot;: mae, &quot;r2&quot;: r2})\n        mlflow.log_artifact(data_path)\n        print(&quot;Save to: {}&quot;.format(mlflow.get_artifact_uri()))\n        \n        mlflow.sklearn.log_model(lr, &quot;model&quot;)\n<\/code><\/pre>\n<p>Once I run <code>train()<\/code> with its parameters, in UI I cannot see Artifacts, but I can see models and its parameters and Metric.<\/p>\n<p>In artifact tab it's written <code>No Artifacts Recorded Use the log artifact APIs to store file outputs from MLflow runs.<\/code> But in finder in models folders all Artifacts existe with models Pickle.<\/p>\n<p>help<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1590280271933,
        "Question_score":2,
        "Question_tags":"python|artifacts|mlflow",
        "Question_view_count":8044,
        "Owner_creation_time":1500490643013,
        "Owner_last_access_time":1663284031840,
        "Owner_location":"France",
        "Owner_reputation":722,
        "Owner_up_votes":703,
        "Owner_down_votes":8,
        "Owner_views":290,
        "Question_last_edit_time":1656334439607,
        "Answer_body":"<p>Had a similar issue. In my case, I solved it by running <code>mlflow ui<\/code> inside the <code>mlruns<\/code> directory of your experiment.<\/p>\n<p>See the full discussion on Github <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/3030\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<p>Hope it helps!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1593697638320,
        "Answer_score":4.0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":1593982070673,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61980244",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":64164367,
        "Question_title":"Nginx authentication issues when building mlflow through docker-compose",
        "Question_body":"<p>I'm trying to dockerize <a href=\"https:\/\/mlflow.org\/\" rel=\"nofollow noreferrer\">mlflow<\/a> with PostgreSQL and nginx configurations for Google Cloud Run (GCR) on the Google Cloud Platform (GCP).<\/p>\n<p>Before deploying anything to GCP however, I wanted to get a local deployment working. I found <a href=\"https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039\" rel=\"nofollow noreferrer\">this<\/a> guide that details the process of setting up the environment. Having followed the guide (excluding the SQL part), I can see the  mlflow UI on <code>localhost:80<\/code> as nginx redirects traffic on port 80 to 5000. To add authentication, I found <a href=\"https:\/\/www.digitalocean.com\/community\/tutorials\/how-to-set-up-password-authentication-with-nginx-on-ubuntu-14-04\" rel=\"nofollow noreferrer\">here<\/a> that I can do it using <code>sudo htpasswd -c .htpasswd &lt;username&gt;<\/code> in the <code>etc\/nginx\/<\/code> directory and then adding<\/p>\n<pre><code>location \\ {\n   auth_basic &quot;Private Property&quot;;\n   auth_basic_user_file .htpasswd;\n}\n<\/code><\/pre>\n<p>to the <code>nginx.conf<\/code> (or <code>mlflow.conf<\/code> in this case) to make it appear online. Trouble is, when I go to <code>localhost:80<\/code> <em>now<\/em> and enter in my username\/password, I continue to see<\/p>\n<pre><code>[error] 6#6: *1 open() &quot;\/etc\/nginx\/.htpasswd&quot; failed (2: No such file or directory)\n<\/code><\/pre>\n<p>in the <code>docker-compose up<\/code> logs as they are printed to the terminal, and as such <em>I'm not able to see the mlflow UI<\/em> on <code>localhost:80<\/code> (either a blank screen or nginx 403 error).<\/p>\n<p>Now, I've looked at several other posts (such as <a href=\"https:\/\/stackoverflow.com\/questions\/2010677\/nginx-and-auth-basic\">this one<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/16510374\/403-forbidden-nginx-using-correct-credentials\">this one<\/a>) and it seems to me that nginx doesn't have the right permissions to read the <code>.htpasswd<\/code> in the <code>etc\/nginx\/<\/code> directory file or that the path of the file isn't correct, i.e. the path has to be in reference to the <code>nginx.conf<\/code> file.<\/p>\n<p>Even though I made these corrections to the above towards-data-science files, the problem still persists.  I've been stuck for a while on this. Any particular reasons why this may be happening?<\/p>\n<p>Edit:\nHere is my directory structure in case it may help:<\/p>\n<pre><code>mlflow-docker\/:\n  mlflow\/:\n    Dockerfile\n  nginx\/:\n    Dockerfile\n    mlflow.conf\n    nginx.conf\n  docker-compose.yml\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1601594496277,
        "Question_score":0,
        "Question_tags":"docker|debugging|nginx|google-cloud-platform|mlflow",
        "Question_view_count":841,
        "Owner_creation_time":1524443788580,
        "Owner_last_access_time":1663973899597,
        "Owner_location":null,
        "Owner_reputation":45,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":1601616621330,
        "Answer_body":"<p>You need to add the .htpasswd file inside your container's file system.<\/p>\n<p>Generate the password file in your project's nginx folder.<\/p>\n<pre><code>sudo htpasswd -c .htpasswd sammy\n<\/code><\/pre>\n<p>Copy the password file to the nginx container's directory. Add following line in nginx dockerfile.<\/p>\n<pre><code>COPY .htpasswd \/etc\/nginx\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1601618748113,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1601619366397,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64164367",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68527422,
        "Question_title":"mlflow.pyfunc.spark_udf and vector struct type",
        "Question_body":"<p>My <em>PySpark<\/em> dataset contains categorical data.<\/p>\n<p>To train a model on this data, I followed this <a href=\"https:\/\/docs.databricks.com\/_static\/notebooks\/binary-classification.html\" rel=\"nofollow noreferrer\">example notebook<\/a>. Especially, see the <em>Preprocess Data<\/em> section for the encoding part.<\/p>\n<p>I now need to use this model somewhere else; hence, I followed <em>Databricks<\/em> recommendation to save and load this model.<\/p>\n<p>It's working fine with <em>Pandas<\/em> (cf. code below).<\/p>\n<pre><code>logged_model = 'runs:\/e905f5759d434a1391bbe1e54a2b\/best-model'\n\n# Load model as a PyFuncModel.\nloaded_model = mlflow.pyfunc.load_model(logged_model)\n\n# Predict on a Pandas DataFrame.\nimport pandas as pd\nloaded_model.predict(pd.DataFrame(data))\n<\/code><\/pre>\n<p>However the dataframe is to big to be converted to <em>Pandas<\/em>. Hence I need to make it work in <em>Spark<\/em>:<\/p>\n<pre><code>import mlflow\nlogged_model = 'runs:\/e905f5759d434a131bbe1e54a2b\/best-model'\n\n# Load model as a Spark UDF.\nloaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model)\n\n# Predict on a Spark DataFrame.\ndf.withColumn('predictions', loaded_model(*columns)).collect()\n<\/code><\/pre>\n<p>But this snippet is producing:<\/p>\n<pre><code>java.lang.UnsupportedOperationException: Unsupported data type: struct&amp;lt;type:tinyint,size:int,indices:array&amp;lt;int&amp;gt;,values:array&amp;lt;double&amp;gt;&amp;gt;\n<\/code><\/pre>\n<p>My feeling is that the udf doesn't accept this type of data as input.\nIs there a way to fix it ?\nAnother solution ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1627291388767,
        "Question_score":1,
        "Question_tags":"pyspark|databricks|mlflow",
        "Question_view_count":790,
        "Owner_creation_time":1480398962230,
        "Owner_last_access_time":1664019145870,
        "Owner_location":null,
        "Owner_reputation":440,
        "Owner_up_votes":21,
        "Owner_down_votes":4,
        "Owner_views":56,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Have you tried using the <code>mlflow.spark.load_model<\/code>?<\/p>\n<p>I'm having a very similar issue over here, but but using the spark method. I tried using the <code>mlflow.spark.load_model('runs:\/run-id\/my-model')<\/code> method and I got this weird error:<\/p>\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '\/dbfs\/tmp\/mlflow\/weird-id-folder'\n<\/code><\/pre>\n<p>Searching for the docs, I see the problem that we are facing (which seems to be different), seems to be a signature problem.<\/p>\n<p>According with other part of the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#model-signature\" rel=\"nofollow noreferrer\">docs<\/a> we have that the signature logged with the model will help to define what type of input the model has. The problem for me here is that my input is a Spark Sparse Vector -- which is not supported... Right now I'm trying to convert that into a column-based signature.<\/p>\n<p>Have you tried something like this?<\/p>\n<hr \/>\n<p>UPDATE:<\/p>\n<p>I would like to add that in my case adding the signature did solve the problem. All I did was ignore the vectors and consider only the input data and output data.<\/p>\n<p>I took a look into the notebook, but haven't seen any mlflow logs, anyway, I do suppose you are logging your experiment according to <a href=\"https:\/\/docs.databricks.com\/applications\/mlflow\/tracking.html#log-runs-to-a-notebook-or-workspace-experiment\" rel=\"nofollow noreferrer\">this<\/a> and using the <code>mlflow.spark<\/code> flavor.<\/p>\n<p>If so, consider using all your data transformation and model fit in the same pipeline, using <code>from pyspark.ml import Pipeline<\/code>. Before logging the model, consider going under signature and registering the model schema.<\/p>\n<pre><code>import mlflow.spark\nfrom mlflow.models.signature import infer_signature\n\nwith mlflow.start_run():\n    [...]\n    # executing train &amp; test pipelines:\n    model = pipeline.fit(train_features) # training model\n    predictions = model.transform(test_features) # testing model\n    train_signature = train_features.select('input_data') # ignores all other features created on the pipeline\n    prediction_signature = predictions.select('input_data', 'prediction') # ignores all other features created on the training pipeline \n    signature = infer_signature(train_signature, prediction_signature) # register model schema\n    mlflow.spark.log_model(model, 'transactions-classification', signature=signature) # logging model to mlflow\n    [...]\n<\/code><\/pre>\n<p>After logging the model to the experiment, in a different notebook, you can use the load_model function as:<\/p>\n<pre><code># importing model\nimport mlflow.spark\nmodel_path = 'runs:\/run-id'\nmodel = mlflow.spark.load_model(model_path)\n<\/code><\/pre>\n<p>And it will work! :D<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1629378460770,
        "Answer_score":3.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":1630447430980,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68527422",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56351452,
        "Question_title":"Connect on-prem jypyter notebook to mlflow tracking server in Azure",
        "Question_body":"<p>Is it possible to connect a notebook running in premises to an mlflow Tracking server that is part of an Azure Databricks workspace? Have all the local logging and tracking saved in Azure?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1559088053237,
        "Question_score":2,
        "Question_tags":"azure-databricks|mlflow",
        "Question_view_count":291,
        "Owner_creation_time":1343948494333,
        "Owner_last_access_time":1662160890913,
        "Owner_location":"Seattle, WA, USA",
        "Owner_reputation":1390,
        "Owner_up_votes":122,
        "Owner_down_votes":2,
        "Owner_views":121,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I had a similar problem, used python and solved it with the following steps:<\/p>\n<ol>\n<li>Install mlflow and datbricks-cli libraries.<\/li>\n<li>Define the following env variables : DATABRICKS_HOST (databricks workspace url: <a href=\"https:\/\/region.azuredatabricks.net\" rel=\"nofollow noreferrer\">https:\/\/region.azuredatabricks.net<\/a>) and DATABRICKS_TOKEN<\/li>\n<li>Define mlflow client:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow_client = mlflow.tracking.MlflowClient(tracking_uri='databricks')\n<\/code><\/pre>\n<ol start=\"5\">\n<li>Use mlflow_client client for logging, saving and etc..<\/li>\n<\/ol>\n<p>for more reference you can look at the &quot;Log to a tracking server from a notebook&quot; section <a href=\"https:\/\/docs.azuredatabricks.net\/applications\/mlflow\/tracking.html#log-to-a-tracking-server-from-a-notebook\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1559118227527,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1592644375060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56351452",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72604450,
        "Question_title":"MLflow load model fails Python",
        "Question_body":"<p>I am trying to build an API using an MLflow model.<\/p>\n<p>the funny thing is it works from one location on my PC and not from another. So, the reason for doing I wanted to change my repo etc.<\/p>\n<p>So, the simple code of<\/p>\n<pre><code>from mlflow.pyfunc import load_model\nMODEL_ARTIFACT_PATH = &quot;.\/model\/model_name\/&quot;\nMODEL = load_model(MODEL_ARTIFACT_PATH)\n<\/code><\/pre>\n<p>now fails with<\/p>\n<pre><code>ERROR:    Traceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 540, in lifespan\n    async for item in self.lifespan_context(app):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 481, in default_lifespan\n    await self.startup()\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 516, in startup\n    await handler()\n  File &quot;\/code\/.\/app\/main.py&quot;, line 32, in startup_load_model\n    MODEL = load_model(MODEL_ARTIFACT_PATH)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 733, in load_model\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/spark.py&quot;, line 737, in _load_pyfunc\n    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/spark.py&quot;, line 656, in _load_model\n    return PipelineModel.load(model_uri)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/util.py&quot;, line 332, in load\n    return cls.read().load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/pipeline.py&quot;, line 258, in load\n    return JavaMLReader(self.cls).load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/util.py&quot;, line 282, in load\n    java_obj = self._jread.load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/py4j\/java_gateway.py&quot;, line 1321, in __call__\n    return_value = get_return_value(\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/sql\/utils.py&quot;, line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.\n<\/code><\/pre>\n<p>The model artifacts are already downloaded to the folder \/model folder which has the following structure.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oqxRW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oqxRW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>the load model call is in the main.py file\nAs I mentioned it works from another directory, but there is no reference to any absolute paths. Also, I have made sure that my package references are identical. e,g I have pinned them all down<\/p>\n<pre><code># Model\nmlflow==1.25.1\nprotobuf==3.20.1\npyspark==3.2.1\nscipy==1.6.2\nsix==1.15.0\n<\/code><\/pre>\n<p>also, the same docker file is used both places, which among other things, makes sure that the final resulting folder structure is the same<\/p>\n<pre><code>......other stuffs\n\nCOPY .\/app \/code\/app\nCOPY .\/model \/code\/model\n<\/code><\/pre>\n<p>what can explain it throwing this exception whereas in another location (on my PC), it works (same model artifacts) ?<\/p>\n<p>Since it uses load_model function, it should be able to read the parquet files ?<\/p>\n<p>Any question and I can explain.<\/p>\n<p>EDIT1: I have debugged this a little more in the docker container and it seems the parquet files in the itemFactors folder (listed in my screenshot above) are not getting copied over to my image , even though I have the copy command to copy all files under the model folder. It is copying the _started , _committed and _SUCCESS files, just not the parquet files. Anyone knows why would that be? I DO NOT have a .dockerignore file. Why are those files ignored while copying?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655130099670,
        "Question_score":2,
        "Question_tags":"python|docker|databricks|mlflow",
        "Question_view_count":109,
        "Owner_creation_time":1428654714763,
        "Owner_last_access_time":1664012257383,
        "Owner_location":null,
        "Owner_reputation":596,
        "Owner_up_votes":53,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Question_last_edit_time":1655134670387,
        "Answer_body":"<p>I found the problem. Like I wrote in the EDIT1 of my post, with further observations, the parquet files were missing in the docker container. That was strange because I was copying the entire folder in my Dockerfile.<\/p>\n<p>I then realized that I was hitting this problem <a href=\"https:\/\/github.com\/moby\/buildkit\/issues\/1366\" rel=\"nofollow noreferrer\">mentioned here<\/a>. File paths exceeding 260 characters, silently fail and do not get copied over to the docker container. This was really frustrating because nothing failed during build and then during run, it gave me that cryptic error of &quot;unable to infer schema for parquet&quot;, essentially because the parquet files were not copied over during docker build.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655191745993,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1655202841060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72604450",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69466354,
        "Question_title":"MLflow S3UploadFailedError: Failed to upload",
        "Question_body":"<p>I've created with docker a MinioS3 artifact storage and a mysql bakend storage using the next docker-compose:<\/p>\n<pre><code>    version: '3.8'\n    services:\n        db:\n           environment:\n              - MYSQL_DATABASE=${MYSQL_DATABASE}\n              - MYSQL_USER=${MYSQL_USER}\n              - MYSQL_PASSWORD=${MYSQL_PASSWORD}\n              - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}\n           expose:\n              - '3306'        \n           volumes:\n              - '(path)\/server_backend:\/var\/lib\/mysql '\n           image: 'mysql'\n           container_name: db\n\n        storage:\n            environment:\n                - MINIO_ACCESS_KEY=${MINIO_USR}\n                - MINIO_SECRET_KEY=${MINIO_PASS}\n            expose:\n                - '9000'\n            ports:\n                - '9000:9000'        \n            depends_on:\n                - db\n            command: server \/data\n            volumes:\n                - '(path)\/server_artifact:\/data'\n            image: minio\/minio:RELEASE.2021-02-14T04-01-33Z\n            container_name: MinIO\n\n        mlflow:\n            build: .\/mlflow\n            environment:\n                - AWS_ACCESS_KEY_ID=${MINIO_USR}\n                - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}       \n            expose:\n                - '5000'\n            ports:\n                - '5000:5000'\n            depends_on:\n                - storage                       \n            image: 'mlflow:Dockerfile'\n            container_name: server\n<\/code><\/pre>\n<p>The Mlflow server docker was created using the next Dockerfile:<\/p>\n<pre><code>    FROM python:3.8-slim-buster\n    WORKDIR \/usr\/src\/app\n    RUN pip install cryptography mlflow psycopg2-binary boto3 pymysql\n    ENV MLFLOW_S3_ENDPOINT_URL=http:\/\/storage:9000\n    CMD mlflow server \\\n        --backend-store-uri mysql+pymysql:\/\/MLFLOW:temporal@db:3306\/DBMLFLOW \\\n        --default-artifact-root s3:\/\/artifacts \\\n        --host 0.0.0.0\n<\/code><\/pre>\n<p>The credantials are defined in a <code>.env<\/code> file.<\/p>\n<p>The results of the <code>docker-compose<\/code> up command :<\/p>\n<pre><code>\n    [+] Running 21\/22\n     - mlflow Error                                                                                                                              5.6s\n     - storage Pulled                                                                                                                           36.9s\n       - a6b97b4963f5 Pull complete                                                                                                             24.6s\n       - 13948a011eec Pull complete                                                                                                             24.7s\n       - 40cdef9976a6 Pull complete                                                                                                             24.7s\n       - f47162848743 Pull complete                                                                                                             24.8s\n       - 5f2758d8e94c Pull complete                                                                                                             24.9s\n       - c2950439edb8 Pull complete                                                                                                             25.0s\n       - 1b08f8a15998 Pull complete                                                                                                             30.7s\n     - db Pulled                                                                                                                                45.8s\n       - 07aded7c29c6 Already exists                                                                                                             0.0s\n       - f68b8cbd22de Pull complete                                                                                                              0.7s\n       - 30c1754a28c4 Pull complete                                                                                                              2.1s\n       - 1b7cb4d6fe05 Pull complete                                                                                                              2.2s\n       - 79a41dc56b9a Pull complete                                                                                                              2.3s\n       - 00a75e3842fb Pull complete                                                                                                              6.7s\n       - b36a6919c217 Pull complete                                                                                                              6.8s\n       - 635b0b84d686 Pull complete                                                                                                              6.8s\n       - 6d24c7242d02 Pull complete                                                                                                             39.4s\n       - 5be6c5edf16f Pull complete                                                                                                             39.5s\n       - cb35eac1242c Pull complete                                                                                                             39.5s\n       - a573d4e1c407 Pull complete                                                                                                             39.6s\n    [+] Building 1.4s (7\/7) FINISHED\n     =&gt; [internal] load build definition from Dockerfile                                                                                         0.0s\n     =&gt; =&gt; transferring dockerfile: 32B                                                                                                          0.0s\n     =&gt; [internal] load .dockerignore                                                                                                            0.0s\n     =&gt; =&gt; transferring context: 2B                                                                                                              0.0s\n     =&gt; [internal] load metadata for docker.io\/library\/python:3.8-slim-buster                                                                    1.3s\n     =&gt; [1\/3] FROM docker.io\/library\/python:3.8-slim-buster@sha256:13a3f2bffb4b18ff7eda2763a3b0ba316dd82e548f52ea8b4fd11c94b97afa7d              0.0s\n     =&gt; CACHED [2\/3] WORKDIR \/usr\/src\/app                                                                                                        0.0s\n     =&gt; CACHED [3\/3] RUN pip install cryptography mlflow psycopg2-binary boto3 pymysql                                                           0.0s\n     =&gt; exporting to image                                                                                                                       0.0s\n     =&gt; =&gt; exporting layers                                                                                                                      0.0s\n     =&gt; =&gt; writing image sha256:76d4e4462b5c7c1826734e59a54488b56660de0dd5ecc188c308202608a8f20b                                                 0.0s\n     =&gt; =&gt; naming to docker.io\/library\/mlflow:Dockerfile                                                                                         0.0s\n    \n    Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\n    [+] Running 3\/3\n     - Container db  Created                                                                                                       0.5s\n     - Container MinIO      Created                                                                                                       0.1s\n     - Container server     Created                                                                                                       0.1s\n    Attaching to server, MinIO, db\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.26-1debian10 started.\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.26-1debian10 started.\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Initializing database files\n    db  | 2021-10-06T12:12:57.679527Z 0 [System] [MY-013169] [Server] \/usr\/sbin\/mysqld (mysqld 8.0.26) initializing of server in progress as process 44\n    db  | 2021-10-06T12:12:57.687748Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.\n    db  | 2021-10-06T12:12:58.230036Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.\n    db  | 2021-10-06T12:12:59.888820Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:12:59.889102Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:12:59.997461Z 6 [Warning] [MY-010453] [Server] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option.\n    MinIO      | Attempting encryption of all config, IAM users and policies on MinIO backend\n    MinIO      | Endpoint: http:\/\/172.18.0.3:9000  http:\/\/127.0.0.1:9000\n    MinIO      |\n    MinIO      | Browser Access:\n    MinIO      |    http:\/\/172.18.0.3:9000  http:\/\/127.0.0.1:9000\n    MinIO      |\n    MinIO      | Object API (Amazon S3 compatible):\n    MinIO      |    Go:         https:\/\/docs.min.io\/docs\/golang-client-quickstart-guide\n    MinIO      |    Java:       https:\/\/docs.min.io\/docs\/java-client-quickstart-guide\n    MinIO      |    Python:     https:\/\/docs.min.io\/docs\/python-client-quickstart-guide\n    MinIO      |    JavaScript: https:\/\/docs.min.io\/docs\/javascript-client-quickstart-guide\n    MinIO      |    .NET:       https:\/\/docs.min.io\/docs\/dotnet-client-quickstart-guide\n    server     | 2021\/10\/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 0.1 seconds\n    server     | 2021\/10\/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 0.3 seconds\n    server     | 2021\/10\/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 0.7 seconds\n    server     | 2021\/10\/06 12:13:03 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 1.5 seconds\n    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Database files initialized\n    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Starting temporary server\n    db  | 2021-10-06T12:13:04.422603Z 0 [System] [MY-010116] [Server] \/usr\/sbin\/mysqld (mysqld 8.0.26) starting as process 93\n    db  | 2021-10-06T12:13:04.439806Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.\n    db  | 2021-10-06T12:13:04.575773Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.\n    db  | 2021-10-06T12:13:04.827307Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:04.827865Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:04.832827Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.\n    db  | 2021-10-06T12:13:04.834132Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.\n    db  | 2021-10-06T12:13:04.841629Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '\/var\/run\/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.\n    db  | 2021-10-06T12:13:04.855748Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Socket: \/var\/run\/mysqld\/mysqlx.sock\n    db  | 2021-10-06T12:13:04.855801Z 0 [System] [MY-010931] [Server] \/usr\/sbin\/mysqld: ready for connections. Version: '8.0.26'  socket: '\/var\/run\/mysqld\/mysqld.sock'  port: 0  MySQL Community Server - GPL.\n    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Temporary server started.\n    server     | 2021\/10\/06 12:13:05 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 3.1 seconds\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/iso3166.tab' as time zone. Skipping it.\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/leap-seconds.list' as time zone. Skipping it.\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/zone.tab' as time zone. Skipping it.\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/zone1970.tab' as time zone. Skipping it.\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Creating database DBMLFLOW\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Creating user MLFLOW\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Giving user MLFLOW access to schema DBMLFLOW\n    db  |\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Stopping temporary server\n    db  | 2021-10-06T12:13:06.948482Z 13 [System] [MY-013172] [Server] Received SHUTDOWN from user root. Shutting down mysqld (Version: 8.0.26).\n    server     | 2021\/10\/06 12:13:08 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 6.3 seconds\n    db  | 2021-10-06T12:13:08.716131Z 0 [System] [MY-010910] [Server] \/usr\/sbin\/mysqld: Shutdown complete (mysqld 8.0.26)  MySQL Community Server - GPL.\n    db  | 2021-10-06 12:13:08+00:00 [Note] [Entrypoint]: Temporary server stopped\n    db  |\n    db  | 2021-10-06 12:13:08+00:00 [Note] [Entrypoint]: MySQL init process done. Ready for start up.\n    db  |\n    db  | 2021-10-06T12:13:09.159115Z 0 [System] [MY-010116] [Server] \/usr\/sbin\/mysqld (mysqld 8.0.26) starting as process 1\n    db  | 2021-10-06T12:13:09.167405Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.\n    db  | 2021-10-06T12:13:09.298925Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.\n    db  | 2021-10-06T12:13:09.488958Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:09.489087Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:09.489934Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.\n    db  | 2021-10-06T12:13:09.490169Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.\n    db  | 2021-10-06T12:13:09.494728Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '\/var\/run\/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.\n    db  | 2021-10-06T12:13:09.509856Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Bind-address: '::' port: 33060, socket: \/var\/run\/mysqld\/mysqlx.sock\n    db  | 2021-10-06T12:13:09.509982Z 0 [System] [MY-010931] [Server] \/usr\/sbin\/mysqld: ready for connections. Version: '8.0.26'  socket: '\/var\/run\/mysqld\/mysqld.sock'  port: 3306  MySQL Community Server - GPL.\n    db  | mbind: Operation not permitted\n    server     | 2021\/10\/06 12:13:14 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n    server     | 2021\/10\/06 12:13:14 INFO mlflow.store.db.utils: Updating database tables\n    server     | INFO  [alembic.runtime.migration] Context impl MySQLImpl.\n    server     | INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n    server     | INFO  [alembic.runtime.migration] Running upgrade  -&gt; 451aebb31d03, add metric step\n    server     | INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -&gt; 90e64c465722, migrate user column to tags\n    server     | INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -&gt; 181f10493468, allow nulls for metric values\n    server     | INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -&gt; df50e92ffc5e, Add Experiment Tags Table\n    server     | INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -&gt; 7ac759974ad8, Update run tags with larger limit\n    server     | INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -&gt; 89d4b8295536, create latest metrics table\n    server     | INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!\n    server     | INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -&gt; 2b4d017a5e9b, add model registry tables to db\n    server     | INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.\n    server     | INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!\n    server     | INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -&gt; cfd24bdc0731, Update run status constraint with killed\n    server     | INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -&gt; 0a8213491aaa, drop_duplicate_killed_constraint\n    server     | INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -&gt; 728d730b5ebd, add registered model tags table\n    server     | INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -&gt; 27a6a02d2cf1, add model version tags table\n    server     | INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -&gt; 84291f40a231, add run_link to model_version\n    server     | INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -&gt; a8c4a736bde6, allow nulls for run_id\n    server     | INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -&gt; 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n    server     | INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -&gt; c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n    server     | INFO  [alembic.runtime.migration] Context impl MySQLImpl.\n    server     | INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n    db  | mbind: Operation not permitted\n    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Starting gunicorn 20.1.0\n    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Listening at: http:\/\/0.0.0.0:5000 (17)\n    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Using worker: sync\n    server     | [2021-10-06 12:13:16 +0000] [19] [INFO] Booting worker with pid: 19\n    server     | [2021-10-06 12:13:16 +0000] [20] [INFO] Booting worker with pid: 20\n    server     | [2021-10-06 12:13:16 +0000] [21] [INFO] Booting worker with pid: 21\n    server     | [2021-10-06 12:13:16 +0000] [22] [INFO] Booting worker with pid: 22\n\n<\/code><\/pre>\n<p>It makes me suspect because on the second line appears <code>- mlflow Error<\/code> but i think that this is why the other builds haven't finished.<\/p>\n<p>Then I've set my environment variables on the client to create the information flow between my script and the storages:<\/p>\n<pre><code>\n    os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http:\/\/localhost:9000\/'\n    os.environ['AWS_ACCESS_KEY_ID'] = 'key'\n    os.environ['AWS_SECRET_ACCESS_KEY'] = 'pw'\n    \n    remote_server_uri = &quot;http:\/\/localhost:5000\/&quot; # server URI\n    mlflow.set_tracking_uri(remote_server_uri)\n    \n    mlflow.set_experiment(&quot;mnist_mLflow_demo&quot;)\n\n<\/code><\/pre>\n<p>finally i trained a tensorflow network and i didn't have problems storing parameters and metrics but gave me some warnings (refering to next error). But the model haven't been auto log, so i tryed to do it manually:<\/p>\n<pre><code>    with mlflow.start_run(run_name = &quot;test0&quot;) as run:\n    \n        mlflow.keras.log_model(model2, 'model2')\n\n    mlflow.end_run()\n<\/code><\/pre>\n<p>It dosen't work and it gives me the next INFO (but essencialy an error):<\/p>\n<pre><code>    INFO:tensorflow:Assets written to: (path)\\Temp\\tmpgr5eaha2\\model\\data\\model\\assets\n    INFO:tensorflow:Assets written to: (path)\\Temp\\tmpgr5eaha2\\model\\data\\model\\assets\n    2021\/10\/06 14:16:00 ERROR mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: (path)\\AppData\\Local\\Temp\\tmpgr5eaha2\\model, flavor: keras)\n    Traceback (most recent call last):\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\environment.py&quot;, line 212, in infer_pip_requirements\n        return _infer_requirements(model_uri, flavor)\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\requirements_utils.py&quot;, line 263, in _infer_requirements\n        modules = _capture_imported_modules(model_uri, flavor)\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\requirements_utils.py&quot;, line 221, in _capture_imported_modules\n        _run_command(\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\requirements_utils.py&quot;, line 163, in _run_command\n        stderr = stderr.decode(&quot;utf-8&quot;)\n    UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in position 349: invalid continuation byte\n\n<\/code><\/pre>\n<p>And the next error:<\/p>\n<pre><code>\n    ClientError                               Traceback (most recent call last)\n    ~\\Python\\Python39\\lib\\site-packages\\boto3\\s3\\transfer.py in upload_file(self, filename, bucket, key, callback, extra_args)\n        278         try:\n    --&gt; 279             future.result()\n        280         # If a client error was raised, add the backwards compatibility layer\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\futures.py in result(self)\n        105             # out of this and propogate the exception.\n    --&gt; 106             return self._coordinator.result()\n        107         except KeyboardInterrupt as e:\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\futures.py in result(self)\n        264         if self._exception:\n    --&gt; 265             raise self._exception\n        266         return self._result\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\tasks.py in __call__(self)\n        125             if not self._transfer_coordinator.done():\n    --&gt; 126                 return self._execute_main(kwargs)\n        127         except Exception as e:\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\tasks.py in _execute_main(self, kwargs)\n        149 \n    --&gt; 150         return_value = self._main(**kwargs)\n        151         # If the task is the final task, then set the TransferFuture's\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\upload.py in _main(self, client, fileobj, bucket, key, extra_args)\n        693         with fileobj as body:\n    --&gt; 694             client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)\n        695 \n    \n    ~\\Python\\Python39\\lib\\site-packages\\botocore\\client.py in _api_call(self, *args, **kwargs)\n        385             # The &quot;self&quot; in this scope is referring to the BaseClient.\n    --&gt; 386             return self._make_api_call(operation_name, kwargs)\n        387 \n    \n    ~\\Python\\Python39\\lib\\site-packages\\botocore\\client.py in _make_api_call(self, operation_name, api_params)\n        704             error_class = self.exceptions.from_code(error_code)\n    --&gt; 705             raise error_class(parsed_response, operation_name)\n        706         else:\n    \n    ClientError: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.\n    \n    During handling of the above exception, another exception occurred:\n    \n    S3UploadFailedError                       Traceback (most recent call last)\n    C:\\Users\\FCAIZA~1\\AppData\\Local\\Temp\/ipykernel_7164\/2476247499.py in &lt;module&gt;\n          1 with mlflow.start_run(run_name = &quot;test0&quot;) as run:\n          2 \n    ----&gt; 3     mlflow.keras.log_model(model2, 'model2')\n          4 \n          5 mlflow.end_run()\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\keras.py in log_model(keras_model, artifact_path, conda_env, custom_objects, keras_module, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, **kwargs)\n        402             mlflow.keras.log_model(keras_model, &quot;models&quot;)\n        403     &quot;&quot;&quot;\n    --&gt; 404     Model.log(\n        405         artifact_path=artifact_path,\n        406         flavor=mlflow.keras,\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\models\\model.py in log(cls, artifact_path, flavor, registered_model_name, await_registration_for, **kwargs)\n        186             mlflow_model = cls(artifact_path=artifact_path, run_id=run_id)\n        187             flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n    --&gt; 188             mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)\n        189             try:\n        190                 mlflow.tracking.fluent._record_logged_model(mlflow_model)\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\tracking\\fluent.py in log_artifacts(local_dir, artifact_path)\n        582     &quot;&quot;&quot;\n        583     run_id = _get_or_start_run().info.run_id\n    --&gt; 584     MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\n        585 \n        586 \n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\tracking\\client.py in log_artifacts(self, run_id, local_dir, artifact_path)\n        975             is_dir: True\n        976         &quot;&quot;&quot;\n    --&gt; 977         self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\n        978 \n        979     @contextlib.contextmanager\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py in log_artifacts(self, run_id, local_dir, artifact_path)\n        332         :param artifact_path: If provided, the directory in ``artifact_uri`` to write to.\n        333         &quot;&quot;&quot;\n    --&gt; 334         self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)\n        335 \n        336     def list_artifacts(self, run_id, path=None):\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\store\\artifact\\s3_artifact_repo.py in log_artifacts(self, local_dir, artifact_path)\n        102                 upload_path = posixpath.join(dest_path, rel_path)\n        103             for f in filenames:\n    --&gt; 104                 self._upload_file(\n        105                     s3_client=s3_client,\n        106                     local_file=os.path.join(root, f),\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\store\\artifact\\s3_artifact_repo.py in _upload_file(self, s3_client, local_file, bucket, key)\n         78         if environ_extra_args is not None:\n         79             extra_args.update(environ_extra_args)\n    ---&gt; 80         s3_client.upload_file(Filename=local_file, Bucket=bucket, Key=key, ExtraArgs=extra_args)\n         81 \n         82     def log_artifact(self, local_file, artifact_path=None):\n    \n    ~\\Python\\Python39\\lib\\site-packages\\boto3\\s3\\inject.py in upload_file(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\n        128     &quot;&quot;&quot;\n        129     with S3Transfer(self, Config) as transfer:\n    --&gt; 130         return transfer.upload_file(\n        131             filename=Filename, bucket=Bucket, key=Key,\n        132             extra_args=ExtraArgs, callback=Callback)\n    \n    ~\\Python\\Python39\\lib\\site-packages\\boto3\\s3\\transfer.py in upload_file(self, filename, bucket, key, callback, extra_args)\n        283         # client error.\n        284         except ClientError as e:\n    --&gt; 285             raise S3UploadFailedError(\n        286                 &quot;Failed to upload %s to %s: %s&quot; % (\n        287                     filename, '\/'.join([bucket, key]), e))\n    \n    S3UploadFailedError: Failed to upload (path)\\AppData\\Local\\Temp\\tmpgr5eaha2\\model\\conda.yaml to artifacts\/1\/5ae5fcef2d07432d811c3d7eb534382c\/artifacts\/model2\/conda.yaml: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.\n\n<\/code><\/pre>\n<p>Do you know how to help me with it? I have been looking all this morning but i did not find a solution. Thank you!!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633525784987,
        "Question_score":1,
        "Question_tags":"python|mysql|docker|minio|mlflow",
        "Question_view_count":969,
        "Owner_creation_time":1580841805373,
        "Owner_last_access_time":1663792613347,
        "Owner_location":"Seville, Spain",
        "Owner_reputation":33,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I found the solution of this issue. It is a tricky problem due to spanish characters, my system's user profile in &quot;C:\/&quot; is &quot;fca\u00f1izares&quot; (Ca\u00f1izares is my first last name). I have created another user named &quot;fcanizares&quot; and all is working fine. Hope you find this solution helpfull.<\/p>\n<p>PS: Moral of the issue, get rid of the extrange characters!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1633680248313,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69466354",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70931309,
        "Question_title":"How to retrieve the model signature from the MLflow Model Registry",
        "Question_body":"<p>I have registered a scikit learn model on my MLflow Tracking server, and I am loading it with <code>sklearn.load_model(model_uri)<\/code>.<\/p>\n<p>Now, I would like to access the signature of the model so I can get a list of the model's required inputs\/features so I can retrieve them from my feature store by name. I can't seem to find any utility or method in the <code>mlflow<\/code> API or the <code>MLFlowClient<\/code> API that will let me access a signature or inputs\/outputs attribute, even though I can see a list of inputs and outputs under each version of the model in the UI.<\/p>\n<p>I know that I can find the input sample and the model configuration in the model's artifacts, but that would require me actually downloading the artifacts and loading them manually in my script. I don't need to avoid that, but I am surprised that I can't just return the signature as a dictionary the same way I can return a run's parameters or metrics.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643655088920,
        "Question_score":4,
        "Question_tags":"mlflow",
        "Question_view_count":904,
        "Owner_creation_time":1466188731113,
        "Owner_last_access_time":1663868994303,
        "Owner_location":"Michigan",
        "Owner_reputation":414,
        "Owner_up_votes":25,
        "Owner_down_votes":5,
        "Owner_views":39,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The way to access the model's signature without downloading the MLModel file is under the loaded model. And then you'll access the model's attributes, such as its signature or even other Pyfunc-defined methods.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmodel = mlflow.pyfunc.load_model(&quot;runs:\/&lt;run_id&gt;\/model&quot;)\nprint(model._model_meta._signature)\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1645469817663,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70931309",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63255631,
        "Question_title":"MLflow: INVALID_PARAMETER_VALUE: Unsupported URI '.\/mlruns' for model registry store",
        "Question_body":"<p>I got this error when I was trying to have a model registered in the model registry. Could someone help me?<\/p>\n<pre><code>RestException: INVALID_PARAMETER_VALUE: Unsupported URI '.\/mlruns' for model registry store. \nSupported schemes are: ['postgresql', 'mysql', 'sqlite', 'mssql']. \nSee https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#storage for how to setup a compatible server.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596578095187,
        "Question_score":10,
        "Question_tags":"python|mlflow",
        "Question_view_count":12594,
        "Owner_creation_time":1419619351727,
        "Owner_last_access_time":1644268036150,
        "Owner_location":null,
        "Owner_reputation":125,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":1598815085667,
        "Answer_body":"<p>Mlflow required DB as datastore for Model Registry\nSo you have to run tracking server with DB as backend-store and log model to this tracking server.\nThe easiest way to use DB is to use SQLite.<\/p>\n<pre><code>mlflow server \\\n    --backend-store-uri sqlite:\/\/\/mlflow.db \\\n    --default-artifact-root .\/artifacts \\\n    --host 0.0.0.0\n<\/code><\/pre>\n<p>And set MLFLOW_TRACKING_URI environment variable to <em>http:\/\/localhost:5000<\/em> or<\/p>\n<pre><code>mlflow.set_tracking_uri(&quot;http:\/\/localhost:5000&quot;)\n<\/code><\/pre>\n<p>After got to http:\/\/localhost:5000 and you can register a logged model from UI or from the code.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1596626369480,
        "Answer_score":27.0,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63255631",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69689266,
        "Question_title":"How to set a tag at the experiment level in MLFlow",
        "Question_body":"<p>I can see that an experiment in MLFlow can have tags (like runs can have tags).\nI'm able to set a run's tag using <code>mlflow.set_tag<\/code>, but how do I set it for an experiment?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1635000756820,
        "Question_score":2,
        "Question_tags":"python|mlflow",
        "Question_view_count":1047,
        "Owner_creation_time":1244984040077,
        "Owner_last_access_time":1663968051750,
        "Owner_location":"New York, NY",
        "Owner_reputation":13408,
        "Owner_up_votes":306,
        "Owner_down_votes":12,
        "Owner_views":687,
        "Question_last_edit_time":1635005765867,
        "Answer_body":"<p>If you look into the Python API, the very <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html\" rel=\"nofollow noreferrer\">first example<\/a> in <code>mlflow.tracking package<\/code> that shows how to create the <code>MLflowClient<\/code> is really showing how to tag experiment using the <code>client.set_experiment_tag<\/code> function (<a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html#mlflow.tracking.MlflowClient.set_experiment_tag\" rel=\"nofollow noreferrer\">doc<\/a>):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from mlflow.tracking import MlflowClient\n\n# Create an experiment with a name that is unique and case sensitive.\nclient = MlflowClient()\nexperiment_id = client.create_experiment(&quot;Social NLP Experiments&quot;)\nclient.set_experiment_tag(experiment_id, &quot;nlp.framework&quot;, &quot;Spark NLP&quot;)\n<\/code><\/pre>\n<p>you can also set it for model version with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html#mlflow.client.MlflowClient.set_model_version_tag\" rel=\"nofollow noreferrer\">set_model_version_tag<\/a> function, and for registered model with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html#mlflow.tracking.MlflowClient.set_registered_model_tag\" rel=\"nofollow noreferrer\">set_registered_model_tag<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1635012053310,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1663958792437,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69689266",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69227917,
        "Question_title":"Connect MLflow server to minio in local",
        "Question_body":"<p>I am trying to connect mlflow with Minio server, both are running on my local machine, I am able to connect my client code to minio by adding the below lines to the code,<\/p>\n<pre><code>os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] =&quot;xxxx&quot;\nos.environ['AWS_SECRET_ACCESS_KEY'] =&quot;xxxxxx&quot; \nos.environ['MLFLOW_TRACKING_URI'] = 'http:\/\/localhost:5000'\n<\/code><\/pre>\n<p>But the mlflow server is not getting connected to Minio. To run Mlflow server, command I use:<\/p>\n<pre><code>mlflow server -h 0.0.0.0 -p 5000 --default-artifact-root s3:\/\/mlbucket --backend-store-uri sqlite:\/\/\/mlflow.db\n<\/code><\/pre>\n<p>The mlflow server runs, but while accessing the artifacts page the server, it throws the error:<\/p>\n<pre><code>raise NoCredentialsError()\nbotocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n<p>So how can I pass the credentials of the Minio to the mlflow server command?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1631902626293,
        "Question_score":1,
        "Question_tags":"python|minio|mlflow",
        "Question_view_count":1136,
        "Owner_creation_time":1310893185210,
        "Owner_last_access_time":1663988189020,
        "Owner_location":"Thiruvananthapuram, Kerala, India",
        "Owner_reputation":2763,
        "Owner_up_votes":373,
        "Owner_down_votes":7,
        "Owner_views":851,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Just add the below environment variables:<\/p>\n<pre><code>export AWS_ACCESS_KEY_ID=&lt;your-aws-access-key-id&gt;\nexport AWS_SECRET_ACCESS_KEY = &lt;your-aws-secret-access-key&gt;\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1634743751773,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69227917",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69126555,
        "Question_title":"how to log KerasClassifier model in a sklearn pipeline mlflow?",
        "Question_body":"<p>I have a set of pre-processing stages in sklearn <code>Pipeline<\/code> and an estimator which is a <code>KerasClassifier<\/code> (<code>from tensorflow.keras.wrappers.scikit_learn import KerasClassifier<\/code>).<\/p>\n<p>My overall goal is to tune and log the whole sklearn pipeline in <code>mlflow<\/code> (in databricks evn). I get a confusing type error which I can't figure out how to reslove:<\/p>\n<blockquote>\n<p>TypeError: can't pickle _thread.RLock objects<\/p>\n<\/blockquote>\n<p>I have the following code (without tuning stage) which returns the above error:<\/p>\n<pre><code>conda_env = _mlflow_conda_env(\n    additional_conda_deps=None,\n    additional_pip_deps=[\n        &quot;cloudpickle=={}&quot;.format(cloudpickle.__version__),\n        &quot;scikit-learn=={}&quot;.format(sklearn.__version__),\n        &quot;numpy=={}&quot;.format(np.__version__),\n        &quot;tensorflow=={}&quot;.format(tf.__version__),\n    ],\n    additional_conda_channels=None,\n)\n\nsearch_space = {\n    &quot;estimator__dense_l1&quot;: 20,\n    &quot;estimator__dense_l2&quot;: 20,\n    &quot;estimator__learning_rate&quot;: 0.1,\n    &quot;estimator__optimizer&quot;: &quot;Adam&quot;,\n}\n\n\ndef create_model(n):\n\n    model = Sequential()\n    model.add(Dense(int(n[&quot;estimator__dense_l1&quot;]), activation=&quot;relu&quot;))\n    model.add(Dense(int(n[&quot;estimator__dense_l2&quot;]), activation=&quot;relu&quot;))\n    model.add(Dense(1, activation=&quot;sigmoid&quot;))\n    model.compile(\n        loss=&quot;binary_crossentropy&quot;,\n        optimizer=n[&quot;estimator__optimizer&quot;],\n        metrics=[&quot;accuracy&quot;],\n    )\n\n    return model\n\n\nmlflow.sklearn.autolog()\nwith mlflow.start_run(nested=True) as run:\n\n    classfier = KerasClassifier(build_fn=create_model, n=search_space)\n    # fit the pipeline\n    clf = Pipeline(steps=[(&quot;preprocessor&quot;, preprocessor), \n                          (&quot;estimator&quot;, classfier)])\n    h = clf.fit(\n        X_train,\n        y_train.values,\n        estimator__validation_split=0.2,\n        estimator__epochs=10,\n        estimator__verbose=2,\n    )\n\n    # log scores\n    acc_score = clf.score(X=X_test, y=y_test)\n    mlflow.log_metric(&quot;accuracy&quot;, acc_score)\n\n    signature = infer_signature(X_test, clf.predict(X_test))\n    # Log the model with a signature that defines the schema of the model's inputs and outputs.\n    mlflow.sklearn.log_model(\n        sk_model=clf, artifact_path=&quot;model&quot;, \n        signature=signature, \n        conda_env=conda_env\n    )\n<\/code><\/pre>\n<p>I also get this warning before the error:<\/p>\n<pre><code>\n    WARNING mlflow.sklearn.utils: Truncated the value of the key `steps`. Truncated value: `[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n                      transformer_weights=None,\n                      transformers=[('num',\n                                   Pipeline(memory=None,\n<\/code><\/pre>\n<p>note the the whole pipeline runs outside mlflow.\ncan someone help?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1631241004103,
        "Question_score":0,
        "Question_tags":"python|tensorflow|scikit-learn|databricks|mlflow",
        "Question_view_count":435,
        "Owner_creation_time":1455667285907,
        "Owner_last_access_time":1663905046563,
        "Owner_location":null,
        "Owner_reputation":283,
        "Owner_up_votes":35,
        "Owner_down_votes":7,
        "Owner_views":85,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I think I find sort of a workaround\/solution for this for now, but I think this issue needs to be addressed in MLFloow anyways.<\/p>\n<p>What I did is not the best way probably.\nI used a python package called <a href=\"https:\/\/scikeras.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">scikeras<\/a> that does this wrapping and then could log the model<\/p>\n<p>The code:<\/p>\n<pre><code>import scikeras \nimport tensorflow as tf \nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Flatten, Activation \n \nfrom scikeras.wrappers import KerasClassifier \n  \n \nclass ModelWrapper(mlflow.pyfunc.PythonModel): \n    def __init__(self, model): \n        self.model = model \n \n    def predict(self, context, model_input): \n        return self.model.predict(model_input) \n \nconda_env =  _mlflow_conda_env( \n      additional_conda_deps=None, \n      additional_pip_deps=[ \n        &quot;cloudpickle=={}&quot;.format(cloudpickle.__version__),  \n        &quot;scikit-learn=={}&quot;.format(sklearn.__version__), \n        &quot;numpy=={}&quot;.format(np.__version__), \n        &quot;tensorflow=={}&quot;.format(tf.__version__), \n        &quot;scikeras=={}&quot;.format(scikeras.__version__), \n      ], \n      additional_conda_channels=None, \n  ) \n \nparam = { \n   &quot;dense_l1&quot;: 20, \n   &quot;dense_l2&quot;: 20, \n   &quot;optimizer__learning_rate&quot;: 0.1, \n   &quot;optimizer&quot;: &quot;Adam&quot;, \n   &quot;loss&quot;:&quot;binary_crossentropy&quot;, \n} \n \n  \ndef create_model(dense_l1, dense_l2, meta): \n  \n  n_features_in_ = meta[&quot;n_features_in_&quot;] \n  X_shape_ = meta[&quot;X_shape_&quot;] \n  n_classes_ = meta[&quot;n_classes_&quot;] \n \n  model = Sequential() \n  model.add(Dense(n_features_in_, input_shape=X_shape_[1:], activation=&quot;relu&quot;)) \n  model.add(Dense(dense_l1, activation=&quot;relu&quot;)) \n  model.add(Dense(dense_l2, activation=&quot;relu&quot;)) \n  model.add(Dense(1, activation=&quot;sigmoid&quot;)) \n \n  return model   \n \nmlflow.sklearn.autolog() \nwith mlflow.start_run(run_name=&quot;sample_run&quot;): \n \n  classfier = KerasClassifier( \n    create_model, \n    loss=param[&quot;loss&quot;], \n    dense_l1=param[&quot;dense_l1&quot;], \n    dense_l2=param[&quot;dense_l2&quot;], \n    optimizer__learning_rate = param[&quot;optimizer__learning_rate&quot;], \n    optimizer= param[&quot;optimizer&quot;], \n) \n \n  # fit the pipeline \n  clf = Pipeline(steps=[('preprocessor', preprocessor), \n                      ('estimator', classfier)])   \n \n  h = clf.fit(X_train, y_train.values) \n  # log scores \n  acc_score = clf.score(X=X_test, y=y_test) \n  mlflow.log_metric(&quot;accuracy&quot;, acc_score) \n  signature = infer_signature(X_test, clf.predict(X_test)) \n  model_nn = ModelWrapper(clf,)  \n \n  mlflow.pyfunc.log_model( \n      python_model= model_nn, \n      artifact_path = &quot;model&quot;,  \n      signature = signature,  \n      conda_env = conda_env \n  ) \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1631593268677,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69126555",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70098779,
        "Question_title":"How to connect to MLFlow tracking server that has auth?",
        "Question_body":"<p>I want to connect to remote tracking server (<a href=\"http:\/\/123.456.78.90\" rel=\"nofollow noreferrer\">http:\/\/123.456.78.90<\/a>) that requires authentication<\/p>\n<p>When I do this:<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>import mlflow\nmlflow.set_tracking_uri(\"http:\/\/123.456.78.90\")\nmlflow.set_experiment(\"my-experiment\")<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p>I get an error<\/p>\n<p><em>MlflowException: API request to endpoint \/api\/2.0\/mlflow\/experiments\/list failed with error code 401 != 200.\nResponse body: 401 Authorization Required<\/em><\/p>\n<p>I understand that I need to log in first but I have no idea how to do it<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637767811310,
        "Question_score":1,
        "Question_tags":"authorization|tracking|mlflow",
        "Question_view_count":2102,
        "Owner_creation_time":1637766437853,
        "Owner_last_access_time":1663839694783,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":"<p><a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#logging-to-a-tracking-server\" rel=\"nofollow noreferrer\">MLflow documentation<\/a> says:<\/p>\n<blockquote>\n<p><code>MLFLOW_TRACKING_USERNAME<\/code> and <code>MLFLOW_TRACKING_PASSWORD<\/code> - username and password to use with HTTP Basic authentication. To use Basic authentication, you must set both environment variables.<\/p>\n<\/blockquote>\n<p>So you just need to set these variables in your code using <code>os.environ<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>os.environ['MLFLOW_TRACKING_USERNAME'] = 'name'\nos.environ['MLFLOW_TRACKING_PASSWORD'] = 'pass'\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1637773273483,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70098779",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60695933,
        "Question_title":"MLflow: Why can't backend-store-uri be an s3 location?",
        "Question_body":"<p>I'm new to mlflow and I can't figure out why the <code>artifact store<\/code> can't be the same as the <code>backend store<\/code>? <\/p>\n\n<p>The only reason I can think of is to be able to query the experiments with SQL syntax... but since we can interact with the runs using <code>mlflow ui<\/code> I just don't understand why all artifacts and parameters can't go to a same location (which is what happens when using local storage).<\/p>\n\n<p>Can anyone shed some light on this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1584294693613,
        "Question_score":4,
        "Question_tags":"amazon-s3|mlflow",
        "Question_view_count":768,
        "Owner_creation_time":1528574640850,
        "Owner_last_access_time":1663976741153,
        "Owner_location":null,
        "Owner_reputation":133,
        "Owner_up_votes":10,
        "Owner_down_votes":2,
        "Owner_views":76,
        "Question_last_edit_time":null,
        "Answer_body":"<p>MLflow's Artifacts are typically ML models, i.e. relatively large binary files. On the other hand, run data are typically a couple of floats.<\/p>\n<p>In the end it is not a question of what is possible or not (many things are possible if you put enough effort into it), but rather to follow good practices:<\/p>\n<ul>\n<li>storing large binary artifacts in an SQL database is possible but is bound the degrade the performance of the database sooner or later, and this in turn will degrade your user experience.<\/li>\n<li>storing a couple of floats from a SQL database for quick retrieval for display in a front-end or via command line is a robust industry-proven classic<\/li>\n<\/ul>\n<p>It remains true that the documentation of MLflow on the architecture design rationale could be improved (as of 2020)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1611045077983,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60695933",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61481147,
        "Question_title":"MLFlow Registry high availability",
        "Question_body":"<p>I am running the mlflow registry using <code>mlflow server<\/code> (<a href=\"https:\/\/mlflow.org\/docs\/latest\/model-registry.html\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/model-registry.html<\/a>). The server runs fine. If the server crashes for any reason it restart automatically. But for the time of restart the server is not available.<\/p>\n\n<p>Is it possible to run multiple isntances in parallel behind a load balancer? Is this safe or could it be possible that there are any inconsistencies?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1588079911073,
        "Question_score":0,
        "Question_tags":"mlflow|mlmodel",
        "Question_view_count":501,
        "Owner_creation_time":1378563249260,
        "Owner_last_access_time":1658919410330,
        "Owner_location":null,
        "Owner_reputation":470,
        "Owner_up_votes":128,
        "Owner_down_votes":7,
        "Owner_views":45,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes, it's possible to have multiple instances of MLflow Tracker Service running behind a load balancer.<\/p>\n\n<p>Because the Tracking server is stateless, you could have multiple instances log to a replicated primary DB as a store. A second hot standby can take over if the primary fails.<\/p>\n\n<p>As for the documentation in how to set up replicated instances of your backend store will vary on which one you elect to use, we cannot definitely document all different scenarios and their configurations.<\/p>\n\n<p>I would check the respective documentation of your backend DB and load balancer for how to federate requests to multiple instances of an MLflow tracking server, how to failover to a hot standby or replicated DB, or how to configure a hot-standby replicated DB instance.<\/p>\n\n<p>The short of it: MLflow tracking server is stateless.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1588382828087,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1588394571917,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61481147",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70111193,
        "Question_title":"How can I load the latest model version from MLflow model registry?",
        "Question_body":"<p>I can load a specific version of a model using the mlflow client:<\/p>\n<pre><code>import mlflow\n\nmodel_version = 1\n\nmodel = mlflow.pyfunc.load_model(\n    model_uri=f&quot;models:\/c3760a15e6ac48f88ad7e5af940047d4\/{model_version}&quot;\n)\n<\/code><\/pre>\n<p>But is there a way to load the latest model version?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637843541287,
        "Question_score":3,
        "Question_tags":"mlflow",
        "Question_view_count":1873,
        "Owner_creation_time":1351080779277,
        "Owner_last_access_time":1664084963950,
        "Owner_location":"Leuven, Belgium",
        "Owner_reputation":3126,
        "Owner_up_votes":1817,
        "Owner_down_votes":2,
        "Owner_views":262,
        "Question_last_edit_time":1637850687890,
        "Answer_body":"<p>There is no such thing, like load <code>latest<\/code>, but:<\/p>\n<ul>\n<li>You can specify the stage (<code>staging<\/code>, <code>production<\/code>) - see <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/concepts.html#referencing-artifacts\" rel=\"nofollow noreferrer\">docs<\/a><\/li>\n<li>You can find latest version using the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_latest_versions\" rel=\"nofollow noreferrer\">get_latest_versions<\/a> function - but it will also return latest per stage<\/li>\n<\/ul>\n<p>So you need to define what <code>latest<\/code> means for you.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1637853200393,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70111193",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":57987999,
        "Question_title":"Delete a run in the experiment of mlflow from the UI so the run does not exist in backend store",
        "Question_body":"<p>I found deleting a <code>run<\/code> only change the state from <code>active<\/code> to <code>deleted<\/code>, because the run is still visible in the UI if searching by <code>deleted<\/code>. <\/p>\n\n<p>Is it possible to remove a <code>run<\/code> from the UI to save the space? \nWhen removing a run, does the artifact correspond to the run is also removed?<\/p>\n\n<p>If not, can the run be removed through rest call?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1568794247973,
        "Question_score":6,
        "Question_tags":"mlflow",
        "Question_view_count":3582,
        "Owner_creation_time":1408370821673,
        "Owner_last_access_time":1663216838707,
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":2521,
        "Owner_up_votes":447,
        "Owner_down_votes":13,
        "Owner_views":197,
        "Question_last_edit_time":1568796468593,
        "Answer_body":"<p>You can't do it via the web UI but you can from a python terminal<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmlflow.delete_experiment(69)\n<\/code><\/pre>\n\n<p>Where 69 is the experiment ID<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1569793174477,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57987999",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72994988,
        "Question_title":"How to mlflow-autolog a sklearn ConfusionMatrixDisplay?",
        "Question_body":"<p>I'm trying to log the plot of a <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator\" rel=\"nofollow noreferrer\">confusion matrix generated with scikit-learn<\/a> for a <em>test<\/em> set using <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.sklearn.html\" rel=\"nofollow noreferrer\">mlflow's support for scikit-learn<\/a>.<\/p>\n<p>For this, I tried something that resemble the code below (I'm using mlflow hosted on Databricks, and <code>sklearn==1.0.1<\/code>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sklearn.datasets\nimport pandas as pd\nimport numpy as np\nimport mlflow\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nmlflow.set_tracking_uri(&quot;databricks&quot;)\nmlflow.set_experiment(&quot;\/Users\/name.surname\/plotcm&quot;)\n\ndata = sklearn.datasets.fetch_20newsgroups(categories=['alt.atheism', 'sci.space'])\n\ndf = pd.DataFrame(data = np.c_[data['data'], data['target']])\\\n       .rename({0:'text', 1:'class'}, axis = 'columns')\n\ntrain, test = train_test_split(df)\n\nmy_pipeline = Pipeline([\n    ('vectorizer', TfidfVectorizer()),\n    ('classifier', SGDClassifier(loss='modified_huber')),\n])\n\nmlflow.sklearn.autolog()\n\nfrom sklearn.metrics import ConfusionMatrixDisplay # should I import this after the call to `.autolog()`?\n\nmy_pipeline.fit(train['text'].values, train['class'].values)\n\ncm = ConfusionMatrixDisplay.from_predictions(\n      y_true=test[&quot;class&quot;], y_pred=my_pipeline.predict(test[&quot;text&quot;])\n  )\n<\/code><\/pre>\n<p>while the confusion matrix for the training set is saved in my mlflow run, no png file is created in the mlflow frontend for the <code>test<\/code> set.<\/p>\n<p>If I try to add<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>cm.figure_.savefig('test_confusion_matrix.png')\nmlflow.log_artifact('test_confusion_matrix.png')\n<\/code><\/pre>\n<p>that does the job, but requires explicitly logging the artifact.<\/p>\n<p>Is there an idiomatic\/proper way to autolog the confusion matrix computed using a test set after <code>my_pipeline.fit()<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657892681357,
        "Question_score":0,
        "Question_tags":"python|scikit-learn|confusion-matrix|mlflow",
        "Question_view_count":157,
        "Owner_creation_time":1415722650717,
        "Owner_last_access_time":1664051478173,
        "Owner_location":"Verona, VR, Italy",
        "Owner_reputation":4811,
        "Owner_up_votes":376,
        "Owner_down_votes":73,
        "Owner_views":713,
        "Question_last_edit_time":1658083880967,
        "Answer_body":"<p>The proper way to do this is to use <code>mlflow.log_figure<\/code> as a fluent API announced in <code>MLflow 1.13.0<\/code>. You can read the documentation <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_figure\" rel=\"nofollow noreferrer\">here<\/a>. This code will do the job.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.log_figure(cm.figure_, 'test_confusion_matrix.png')\n<\/code><\/pre>\n<p>This function implicitly store the image, and then calls <code>log_artifact<\/code> against that path, something like you did.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1658304934100,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72994988",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62711259,
        "Question_title":"Customize metric visualization in MLFlow UI when using mlflow.tensorflow.autolog()",
        "Question_body":"<p>I'm trying to integrate MLFlow to my project. Because I'm using <code>tf.keras.fit_generator()<\/code> for my training so I take advantage of <code>mlflow.tensorflow.autolog()<\/code>(<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a> here) to enable automatic logging of metrics and parameters:<\/p>\n<pre><code>    model = Unet()\n    optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n\n    metrics = [IOUScore(threshold=0.5), FScore(threshold=0.5)]\n    model.compile(optimizer, customized_loss, metrics)\n\n    callbacks = [\n        tf.keras.callbacks.ModelCheckpoint(&quot;model.h5&quot;, save_weights_only=True, save_best_only=True, mode='min'),\n        tf.keras.callbacks.TensorBoard(log_dir='.\/logs', profile_batch=0, update_freq='batch'),\n    ]\n\n\n    train_dataset = Dataset(src_dir=SOURCE_DIR)\n\n    train_data_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n\n   \n    with mlflow.start_run():\n        mlflow.tensorflow.autolog()\n        mlflow.log_param(&quot;batch_size&quot;, BATCH_SIZE)\n\n        model.fit_generator(\n            train_data_loader,\n            steps_per_epoch=len(train_data_loader),\n            epochs=EPOCHS,\n            callbacks=callbacks   \n            )\n<\/code><\/pre>\n<p>I expected something like this (just a demonstration taken from the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#visualizing-metrics\" rel=\"nofollow noreferrer\">docs<\/a>):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" alt=\"Visualization on the docs\" \/><\/a><\/p>\n<p>However, after the training finished, this is what I got:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" alt=\"f1_score visualization\" \/><\/a><\/p>\n<p>How can I configure so that the metric plot will update and display its value at each epoch instead of just showing the latest value?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593764146630,
        "Question_score":2,
        "Question_tags":"tf.keras|mlflow",
        "Question_view_count":1035,
        "Owner_creation_time":1467943515393,
        "Owner_last_access_time":1664083114540,
        "Owner_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Owner_reputation":173,
        "Owner_up_votes":97,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":1594008626393,
        "Answer_body":"<p>After searching around, I found <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/2390\" rel=\"nofollow noreferrer\">this issue<\/a> related to my problem above. Actually, all my metrics just logged once each training (instead of each epoch as my intuitive thought). The reason is I didn't specify the <code>every_n_iter<\/code> parameter in <code>mlflow.tensorflow.autolog()<\/code>, which indicates how many 'iterations' must pass before MLflow logs metric executed (see the <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a>). So, changing my code to:<\/p>\n<p><code>mlflow.tensorflow.autolog(every_n_iter=1)<\/code><\/p>\n<p>fixed the problem.<\/p>\n<p>P\/s: Remember that in TF 2.x, an 'iteration' is an epoch (in TF 1.x it's a batch).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1594008525280,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62711259",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60088889,
        "Question_title":"How Do You \"Permanently\" Delete An Experiment In Mlflow?",
        "Question_body":"<p>Permanent deletion of an experiment isn't documented anywhere. I'm using Mlflow w\/ backend postgres db<\/p>\n\n<p>Here's what I've run: <\/p>\n\n<pre><code>client = MlflowClient(tracking_uri=server)\nclient.delete_experiment(1)\n<\/code><\/pre>\n\n<p>This deletes the the experiment, but when I run a new experiment with the same name as the one I just deleted, it will return this error:<\/p>\n\n<pre><code>mlflow.exceptions.MlflowException: Cannot set a deleted experiment 'cross-sell' as the active experiment. You can restore the experiment, or permanently delete the  experiment to create a new one.\n<\/code><\/pre>\n\n<p>I cannot find anywhere in the documentation that shows how to permanently delete everything.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1580970401043,
        "Question_score":20,
        "Question_tags":"python|mlflow",
        "Question_view_count":13984,
        "Owner_creation_time":1443225809767,
        "Owner_last_access_time":1663898334270,
        "Owner_location":"Vancouver, BC, Canada",
        "Owner_reputation":2332,
        "Owner_up_votes":133,
        "Owner_down_votes":3,
        "Owner_views":560,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Unfortunately it seems there is no way to do this via the UI or CLI at the moment :-\/<\/p>\n\n<p>The way to do it depends on the type of backend file store that you are using.<\/p>\n\n<p><strong>Filestore<\/strong>:<\/p>\n\n<p>If you are using the filesystem as a storage mechanism (the default) then it is easy. The 'deleted' experiments are moved to a <code>.trash<\/code> folder. You just need to clear that out:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>rm -rf mlruns\/.trash\/*\n<\/code><\/pre>\n\n<p>As of the current version of the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/cli.html#mlflow-experiments-delete\" rel=\"noreferrer\">documentation<\/a> (1.7.2), they remark:<\/p>\n\n<blockquote>\n  <p>It is recommended to use a cron job or an alternate workflow mechanism to clear <code>.trash<\/code> folder.<\/p>\n<\/blockquote>\n\n<p><strong>SQL Database:<\/strong><\/p>\n\n<p>This is more tricky, as there are dependencies that need to be deleted. I am using MySQL, and these commands work for me:<\/p>\n\n<pre class=\"lang-sql prettyprint-override\"><code>USE mlflow_db;  # the name of your database\nDELETE FROM experiment_tags WHERE experiment_id=ANY(\n    SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n);\nDELETE FROM latest_metrics WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM metrics WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM tags WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM runs WHERE experiment_id=ANY(\n    SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n);\nDELETE FROM experiments where lifecycle_stage=\"deleted\";\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1585231513453,
        "Answer_score":22.0,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60088889",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70051420,
        "Question_title":"MLFlow projects; bash: python: command not found",
        "Question_body":"<p>I'm running MLflow Project for a model using following command from my ubuntu 20.04 terminal<\/p>\n<pre><code>mlflow run . --no-conda -P alpha=0.5\n<\/code><\/pre>\n<p>My system doesn't have conda or python (It does however have python3). So, I added alias for python using terminal<\/p>\n<pre><code>alias python='python3'\n<\/code><\/pre>\n<p>After which I could open python in terminal using <code>python<\/code>. However, I still got the same error<\/p>\n<pre><code>2021\/11\/21 08:07:34 INFO mlflow.projects.utils: === Created directory \/tmp\/tmpp4h595ql for downloading remote URIs passed to arguments of type 'path' ===\n2021\/11\/21 08:07:34 INFO mlflow.projects.backend.local: === Running command 'python tracking.py 0.5 0.1' in run with ID 'e50ca47b3f8848a083906be6220c26fc' === \nbash: python: command not found\n2021\/11\/21 08:07:34 ERROR mlflow.cli: === Run (ID 'e50ca47b3f8848a083906be6220c26fc') failed ===\n<\/code><\/pre>\n<p>How to get rid of this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637462657297,
        "Question_score":0,
        "Question_tags":"python|bash|ubuntu|terminal|mlflow",
        "Question_view_count":281,
        "Owner_creation_time":1601004709207,
        "Owner_last_access_time":1664003822153,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":1074,
        "Owner_up_votes":239,
        "Owner_down_votes":21,
        "Owner_views":143,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Change <code>python<\/code> to <code>python3<\/code> in the <code>MLproject<\/code> file to the resolve error.<\/p>\n<pre><code>command: &quot;python3 tracking.py {alpha} {l1_ratio}&quot;\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1637463754447,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1637502993977,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70051420",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71708147,
        "Question_title":"MLFlow tracking ui not showing experiments on local machine (laptop)",
        "Question_body":"<p>I am a beginner in mlflow and was trying to set it up locally using Anaconda 3.\nI have created a new environment in anaconda and install mlflow and sklearn in it. Now I am using jupyter notebook to run my sample code for mlflow.<\/p>\n<p>'''<\/p>\n<pre><code>import os\nimport warnings\nimport sys\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet\nfrom urllib.parse import urlparse\nimport mlflow\nimport mlflow.sklearn\n\nimport logging\n\nlogging.basicConfig(level=logging.WARN)\nlogger = logging.getLogger(__name__)\n\nwarnings.filterwarnings(&quot;ignore&quot;)\nnp.random.seed(40)\n\n\nmlflow.set_tracking_uri(&quot;file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun&quot;)\n\nmlflow.get_tracking_uri()\n\nmlflow.get_experiment\n\n#experiment_id = mlflow.create_experiment(&quot;Mlflow_demo&quot;)\nexperiment_id = mlflow.create_experiment(&quot;Demo3&quot;)\nexperiment = mlflow.get_experiment(experiment_id)\nprint(&quot;Name: {}&quot;.format(experiment.name))\nprint(&quot;Experiment_id: {}&quot;.format(experiment.experiment_id))\nprint(&quot;Artifact Location: {}&quot;.format(experiment.artifact_location))\nprint(&quot;Tags: {}&quot;.format(experiment.tags))\nprint(&quot;Lifecycle_stage: {}&quot;.format(experiment.lifecycle_stage))\n\nmlflow.set_experiment(&quot;Demo3&quot;)\n\ndef eval_metrics(actual, pred):\n    rmse = np.sqrt(mean_squared_error(actual, pred))\n    mae = mean_absolute_error(actual, pred)\n    r2 = r2_score(actual, pred)\n    return rmse, mae, r2\n\n# Read the wine-quality csv file from the URL\ncsv_url =\\\n    'http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/wine-quality\/winequality-red.csv'\ntry:\n    data = pd.read_csv(csv_url, sep=';')\nexcept Exception as e:\n    logger.exception(\n        &quot;Unable to download training &amp; test CSV, check your internet connection. Error: %s&quot;, e)\n\ndata.head(2)\n\n\ndef train_model(data, alpha, l1_ratio):\n    \n    # Split the data into training and test sets. (0.75, 0.25) split.\n    train, test = train_test_split(data)\n\n    # The predicted column is &quot;quality&quot; which is a scalar from [3, 9]\n    train_x = train.drop([&quot;quality&quot;], axis=1)\n    test_x = test.drop([&quot;quality&quot;], axis=1)\n    train_y = train[[&quot;quality&quot;]]\n    test_y = test[[&quot;quality&quot;]]\n\n    # Set default values if no alpha is provided\n    alpha = alpha\n    l1_ratio = l1_ratio\n\n\n    # Execute ElasticNet\n    lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n    lr.fit(train_x, train_y)\n\n    # Evaluate Metrics\n    predicted_qualities = lr.predict(test_x)\n    (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n    # Print out metrics\n    print(&quot;Elasticnet model (alpha=%f, l1_ratio=%f):&quot; % (alpha, l1_ratio))\n    print(&quot;  RMSE: %s&quot; % rmse)\n    print(&quot;  MAE: %s&quot; % mae)\n    print(&quot;  R2: %s&quot; % r2)\n    \n    # Log parameter, metrics, and model to MLflow\n    with mlflow.start_run(experiment_id = experiment_id):\n        mlflow.log_param(&quot;alpha&quot;, alpha)\n        mlflow.log_param(&quot;l1_ratio&quot;, l1_ratio)\n        mlflow.log_metric(&quot;rmse&quot;, rmse)\n        mlflow.log_metric(&quot;r2&quot;, r2)\n        mlflow.log_metric(&quot;mae&quot;, mae)\n        mlflow.sklearn.log_model(lr, &quot;model&quot;)\n        \n\ntrain_model(data, 0.5, 0.5)\n\ntrain_model(data, 0.5, 0.3)\n\ntrain_model(data, 0.4, 0.3)\n<\/code><\/pre>\n<p>'''<\/p>\n<p>using above code, I am successfully able to create 3 different experiment as I can see the folders created in my local directory as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jKqgX.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Now, I am trying to run the mlflow ui using the jupyter terminal in my chrome browser and I am able to open the mlflow ui but cannot see and experiments as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6KaQK.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Could you help me in finding where I am going wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1648821712310,
        "Question_score":3,
        "Question_tags":"python|windows|mlflow|mlops",
        "Question_view_count":936,
        "Owner_creation_time":1648820658173,
        "Owner_last_access_time":1663932565810,
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Where do you run <code>mlflow ui<\/code> command?<\/p>\n<p>I think if you pass tracking ui path in the arguments, it would work:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>mlflow ui --backend-store-uri file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1650761211590,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1659992652860,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71708147",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":66320435,
        "Question_title":"mlflow.exceptions.MlflowException: Changing param values is not allowed. Param with key='input_rows' was already logged with value='32205'",
        "Question_body":"<p>I am using Mlflow as a work orchestration tool. I have a Machine Learning pipeline. In this pipeline, I have real-time data. I'm listening this data with Apache Kafka. Also, I'm doing this: Whenever 250 message comes to this topic, I'm gathering them, and I'm appending this message my previous data. After that, my training function is triggered. Thus, I am able to making new training in every 250 new data. With Mlflow, I can show the results, metrics and any other parameters of trained models. But After training occurred one time, the second one doesn't occurs, and It throws me this error which I have shown in title. Here it is my consumer:<\/p>\n<pre><code>topic_name = 'twitterdata'\ntrain_every = 250\n\n\ndef consume_tweets():\n    consumer = KafkaConsumer(\n        topic_name,\n        bootstrap_servers=['localhost:9093'],\n        auto_offset_reset='latest',\n        enable_auto_commit=True,\n        auto_commit_interval_ms=5000,\n        fetch_max_bytes=128,\n        max_poll_records=100,\n        value_deserializer=lambda x: json.loads(x.decode('utf-8')))\n\n    tweet_counter = 0\n    for message in consumer:\n        tweets = json.loads(json.dumps(message.value))\n        # print(tweets['text'])\n        tweet_sentiment = make_prediction(tweets['text'])\n\n        if tweet_counter == train_every:\n            update_df()\n            data_path = 'data\/updated_tweets.csv'\n            train(data_path)\n            print(&quot;\\nTraining with new data is completed!\\n&quot;)\n            tweet_counter = 0\n\n        else:\n            tweet_counter += 1\n\n        publish_prediction(tweet_sentiment, tweets['text'])\n\n<\/code><\/pre>\n<p>And here it is my train.py:<\/p>\n<pre><code>train_tweets = pd.read_csv(DATA_PATH)\n    # train_tweets = train_tweets[:20000]\n\n    tweets = train_tweets.tweet.values\n    labels = train_tweets.label.values\n\n    # Log data params\n    mlflow.log_param('input_rows', train_tweets.shape[0])\n\n    # Do preprocessing and return vectorizer with it\n    vectorizer, processed_features = embedding(tweets)\n\n    # Saving vectorizer\n    save_vectorizer(vectorizer)\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)\n\n    # Handle imbalanced data by using 'Smote' and log to Mlflow\n    smote = SMOTE('minority')\n    mlflow.log_param(&quot;over-sampling&quot;, smote)\n\n    X_train, y_train = smote.fit_sample(X_train, y_train)\n\n    # text_classifier = MultinomialNB()\n    text_classifier = LogisticRegression(max_iter=10000)\n    text_classifier.fit(X_train, y_train)\n    predictions = text_classifier.predict(X_test)\n\n    # Model metrics\n    (rmse, mae, r2) = eval_metrics(y_test, predictions)\n\n    mlflow.log_param('os-row-Xtrain', X_train.shape[0])\n    mlflow.log_param('os-row-ytrain', y_train.shape[0])\n    mlflow.log_param(&quot;model_name&quot;, text_classifier)\n    mlflow.log_metric(&quot;rmse&quot;, rmse)\n    mlflow.log_metric(&quot;r2&quot;, r2)\n    mlflow.log_metric(&quot;mae&quot;, mae)\n    mlflow.log_metric('acc_score', accuracy_score(y_test, predictions))\n\n    mlflow.sklearn.log_model(text_classifier, &quot;model&quot;)\n<\/code><\/pre>\n<p>I couldn't solve the problem. MLflow is one of the newest tool, so issues and examples of Mlflow are very few.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1614014592930,
        "Question_score":1,
        "Question_tags":"python|machine-learning|apache-kafka|mlflow|real-time-data",
        "Question_view_count":2716,
        "Owner_creation_time":1613130263950,
        "Owner_last_access_time":1663846837707,
        "Owner_location":"Turkey",
        "Owner_reputation":44,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":1614015098323,
        "Answer_body":"<p>I think you need an MLflow &quot;run&quot; for every new batch of data, so that your parameters are logged independently for each new training.<\/p>\n<p>So, try the following in your consumer:<\/p>\n<pre><code>if tweet_counter == train_every:\n            update_df()\n            data_path = 'data\/updated_tweets.csv'\n            with mlflow.start_run() as mlrun:\n               train(data_path)\n            print(&quot;\\nTraining with new data is completed!\\n&quot;)\n            tweet_counter = 0\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1614350710747,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66320435",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72694707,
        "Question_title":"Multiple artifact paths when logging a model using mlflow and sklearn",
        "Question_body":"<p>I'm using mlflow to log parameters and artifacts of a Logistic Regression, but when I try to log the model so I can see all the files in the Mlflow UI, I see two folders: one named 'model' and the other one named 'logger' (the one I set).<\/p>\n<pre><code>model = LogisticRegression()\n\nmlflow.set_tracking_uri('file:\/\/\/artifacts')\nmlflow.set_experiment('test')\nmlflow.autolog()\n\nwith mlflow.start_run(run_name=run_name) as run:\n   model.train(X_train, y_train)\n   mlflow.sklearn.log_model(model, 'logreg')\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BtIHo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BtIHo.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Not sure if I'm missing something or if there's a configuration for that.<\/p>\n<p>I hope someone out there can help me!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655778299153,
        "Question_score":0,
        "Question_tags":"python|machine-learning|scikit-learn|mlflow|scikit-learn-pipeline",
        "Question_view_count":122,
        "Owner_creation_time":1544467691223,
        "Owner_last_access_time":1659381626413,
        "Owner_location":"Zacatecas, Mexico",
        "Owner_reputation":131,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You have set <code>autolog<\/code> and you are also logging the model explicitly. Remove one and then try.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655933087183,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72694707",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71262010,
        "Question_title":"Download model artefact from Databricks workspace",
        "Question_body":"<p>How can I download a mlflow model artefact in a docker container from databricks workspace?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645771255510,
        "Question_score":0,
        "Question_tags":"docker|databricks|azure-databricks|mlflow",
        "Question_view_count":370,
        "Owner_creation_time":1411361217027,
        "Owner_last_access_time":1662192778187,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":569,
        "Owner_up_votes":41,
        "Owner_down_votes":3,
        "Owner_views":123,
        "Question_last_edit_time":1645795991323,
        "Answer_body":"<p>To download a model from Databricks workspace you need to do two things:<\/p>\n<ol>\n<li><p>Set MLFlow tracking URI to databricks using python API<\/p>\n<\/li>\n<li><p>Setup databricks authentication. I prefer authenticating by setting the following environment variables, you can also use databricks CLI to authenticate:<\/p>\n<pre><code>DATABRICKS_HOST\n\nDATABRICKS_TOKEN\n<\/code><\/pre>\n<\/li>\n<li><p>Here's a basic code snippet to download a model from Databricks workspace model registry:<\/p>\n<pre><code>import os\nimport mlflow\nfrom mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\n\nmodel_name = &quot;example-model-name&quot;\nmodel_stage = &quot;Staging&quot;  # Should be either 'Staging' or 'Production'\n\nmlflow.set_tracking_uri(&quot;databricks&quot;)\n\nos.makedirs(&quot;model&quot;, exist_ok=True)\nlocal_path = ModelsArtifactRepository(\n    f'models:\/{model_name}\/{model_stage}').download_artifacts(&quot;&quot;, dst_path=&quot;model&quot;)\n\nprint(f'{model_stage} Model {model_name} is downloaded at {local_path}')\n<\/code><\/pre>\n<p>Running above python script will download an ML model in the model directory.<\/p>\n<p><strong>Containerizing MLFlow model serving with Docker<\/strong><\/p>\n<p>The next step is to package this downloaded model in a docker image and serve a model when you run the image.<\/p>\n<\/li>\n<\/ol>\n<p>Here's a basic Dockerfile to do the same:<\/p>\n<pre><code>FROM continuumio\/miniconda3\n\nENV MLFLOW_HOME \/opt\/mlflow\nENV MLFLOW_VERSION 1.12.1\nENV PORT 5000\n\nRUN conda install -c conda-forge mlflow=${MLFLOW_VERSION}\n\nCOPY model\/ ${MLFLOW_HOME}\/model\n\nWORKDIR ${MLFLOW_HOME}\n\nRUN mlflow models prepare-env -m ${MLFLOW_HOME}\/model\n\nRUN useradd -d ${MLFLOW_HOME} mlflow\nRUN chown mlflow: ${MLFLOW_HOME}\nUSER mlflow\n\nCMD mlflow models serve -m ${MLFLOW_HOME}\/model --host 0.0.0.0 --port ${PORT}\n<\/code><\/pre>\n<p>For more information you can follow this <a href=\"https:\/\/dev.to\/itachiredhair\/downloading-mlflow-model-from-databricks-and-serving-with-docker-38ip\" rel=\"nofollow noreferrer\">article<\/a> from Akshay Milmile<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1645776779603,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1645795937627,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71262010",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73501103,
        "Question_title":"Getting Bad request while searching run in mlflow",
        "Question_body":"<p>Training a ml model with mlflow in azure environment.<\/p>\n<pre><code>import mlflow\nfrom mlflow import MlflowClient\nfrom azureml.core import Experiment, Workspace\n\nexperiment_name = 'housing-lin-mlflow'\n\nexperiment = Experiment(ws, experiment_name)\n\nruns = mlflow.search_runs(experiment_ids=[ experiment.id ])\n\n<\/code><\/pre>\n<p>While fetching runs from search_runs getting this error :<\/p>\n<pre><code>RestException: BAD_REQUEST: For input string: &quot;5b649b3c-3b8f-497a-bb4f&quot;\n<\/code><\/pre>\n<p>MLflow version : 1.28.0\nIn Azure studio jobs have been created and successfully run.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661517215980,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-service|mlflow",
        "Question_view_count":56,
        "Owner_creation_time":1582101477803,
        "Owner_last_access_time":1663953873503,
        "Owner_location":"Delhi, India",
        "Owner_reputation":171,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":53,
        "Question_last_edit_time":1661625379893,
        "Answer_body":"<p>The bad request in MLFlow after successful running the job is because of not giving proper API permissions for the application.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Search for <strong>MLFLOW<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Scroll down<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s50AL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s50AL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Click on View API Permissions<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Under API permissions, assign the permissions according to the application running region and requirements. Checkout the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-models-mlflow\" rel=\"nofollow noreferrer\">document<\/a> for further information.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1661603882123,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73501103",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67052295,
        "Question_title":"MLflow unfinished experiment saved as finished",
        "Question_body":"<p>when I create a run using <code>mlflow.start_run()<\/code> ,even if my script is interrupted before executing <code>mlflow.end_run()<\/code>, the run gets tagged as finished instead of unfinished in Status?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1618197962217,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":332,
        "Owner_creation_time":1578750761197,
        "Owner_last_access_time":1664024328047,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When your notebook stops the run gets the status finished. However, if you want to continue logging metrics or artifacts to that run, you just need to use <code>mlflow.start_run(run_id=&quot;YourRunIDYouCanGetItFromUI&quot;)<\/code>. This is explained in the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.start_run\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1618223603527,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67052295",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68812238,
        "Question_title":"How to export a MLFlow Model from Azure Databricks as an Azure DevOps Artifacts for CD phase?",
        "Question_body":"<p>I am trying to create an MLOps Pipeline using Azure DevOps and Azure Databricks. From Azure DevOps, I am submitting a Databricks job to a cluster, which trains a Machine Learning Model and saves it into MLFlow Model Registry with a custom flavour (using PyFunc Custom Model).<\/p>\n<p>Now after the job gets over, I want to export this MLFlow Object (with all dependencies - Conda dependencies, two model files - one <code>.pkl<\/code> and one <code>.h5<\/code>, the Python Class with <code>load_context()<\/code> and <code>predict()<\/code> functions defined so that after exporting I can import it and call predict as we do with MLFlow Models).<\/p>\n<p>How do I export this entire MLFlow Model and save it as an AzureDevOps Artifact to be used in the CD phase (where I will deploy it to an AKS cluster with a custom base image)?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1629179063687,
        "Question_score":1,
        "Question_tags":"azure|azure-devops|azure-databricks|mlflow",
        "Question_view_count":575,
        "Owner_creation_time":1601729162437,
        "Owner_last_access_time":1663774065773,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":887,
        "Owner_up_votes":187,
        "Owner_down_votes":32,
        "Owner_views":130,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There is no official way to export a Databricks MLflow run from one workspace to another. However, there is an &quot;unofficial&quot; tool that does most of the job with the main limitation being that notebook revisions linked to a run cannot be exported due to lack of a REST API endpoint for this.<\/p>\n<p><a href=\"https:\/\/github.com\/amesar\/mlflow-export-import\" rel=\"nofollow noreferrer\">https:\/\/github.com\/amesar\/mlflow-export-import<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1629787454223,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68812238",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":51335594,
        "Question_title":"Error with \"mlflow ui\" when trying to run it on MS Windows",
        "Question_body":"<p>When I run <code>mlflow ui<\/code> the following error occurred:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Anaconda3\\Scripts\\gunicorn.exe\\__main__.py\", line 5, in &lt;module&gt;\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\app\\wsgiapp.py\", line 9, in &lt;module&gt;\n    from gunicorn.app.base import Application\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\app\\base.py\", line 12, in &lt;module&gt;\n    from gunicorn import util\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\util.py\", line 9, in &lt;module&gt;\n    import fcntl\nModuleNotFoundError: No module named 'fcntl'\nTraceback (most recent call last):\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Anaconda3\\Scripts\\mlflow.exe\\__main__.py\", line 9, in &lt;module&gt;\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 1066, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\cli.py\", line 131, in ui\n    mlflow.server._run_server(file_store, file_store, host, port, 1)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\server\\__init__.py\", line 48, in _run_server\n    env=env_map, stream_output=True)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\utils\\process.py\", line 38, in exec_cmd\n    raise ShellCommandException(\"Non-zero exitcode: %s\" % (exit_code))\nmlflow.utils.process.ShellCommandException: Non-zero exitcode: 1\n<\/code><\/pre>\n\n<p>I used anaconda + python 3.6.5 and I installed git and set path with <code>C:\\Program Files\\Git\\bin\\git.exe<\/code> and <code>C:\\Program Files\\Git\\cmd<\/code>.<\/p>\n\n<p>I installed <code>mlflow<\/code> whit <code>pip install mlflow<\/code> and its version is 0.2.1.<\/p>\n\n<p>I set a variable with name <code>GIT_PYTHON_GIT_EXECUTABLE<\/code> and value <code>C:\\Program Files\\Git\\bin\\git.exe<\/code> in Environment Variables. <\/p>\n\n<p>How can I solve this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1531546446273,
        "Question_score":1,
        "Question_tags":"python|windows|fcntl|mlflow",
        "Question_view_count":4688,
        "Owner_creation_time":1308552848513,
        "Owner_last_access_time":1664030809627,
        "Owner_location":null,
        "Owner_reputation":1177,
        "Owner_up_votes":24,
        "Owner_down_votes":0,
        "Owner_views":144,
        "Question_last_edit_time":1531837117033,
        "Answer_body":"<p><a href=\"https:\/\/github.com\/databricks\/mlflow\" rel=\"nofollow noreferrer\">mlflow documentation<\/a> already says that <\/p>\n\n<blockquote>\n  <p>Note 2: We <strong>do not currently support running MLflow on Windows<\/strong>.\n  Despite this, we would appreciate any contributions to make MLflow\n  work better on Windows.<\/p>\n<\/blockquote>\n\n<p>You're hitting <code>fcntl<\/code> problem: it's not available on MS Windows platform because it's a \"wrapper\" around the <a href=\"http:\/\/man7.org\/linux\/man-pages\/man2\/fcntl.2.html\" rel=\"nofollow noreferrer\">fcntl function<\/a> that's available on POSIX-compatible systems. (See <a href=\"https:\/\/stackoverflow.com\/a\/1422436\/236007\">https:\/\/stackoverflow.com\/a\/1422436\/236007<\/a> for more details.)<\/p>\n\n<p>Solving this requires modifying the source code of mlflow accordingly. <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1531837039907,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51335594",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65494496,
        "Question_title":"How to load a model using the object \"mlflow.tracking.client.MlflowClient\"?",
        "Question_body":"<p>I'm stuck with the MLFlow model registry. Does anyone know how to load a model using the object &quot;mlflow.tracking.client.MlflowClient&quot;?<\/p>\n<p>I would like to do a predict after with that. I'm sure I'm wrong somewhere because I've already done that in the past. I'm not able to find it in the doc, in the web.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1609255468723,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":608,
        "Owner_creation_time":1423640080283,
        "Owner_last_access_time":1663943557963,
        "Owner_location":"Lyon, France",
        "Owner_reputation":457,
        "Owner_up_votes":19,
        "Owner_down_votes":2,
        "Owner_views":125,
        "Question_last_edit_time":1611139575950,
        "Answer_body":"<p>You'll have to make use of <code>mlflow.&lt;model_flavor&gt;.load_model()<\/code> to load a given model from the Model Registry. For example:<\/p>\n<pre><code>import mlflow.pyfunc\n\nmodel = mlflow.pyfunc.load_model(\n          model_uri=&quot;models:\/&lt;model_name&gt;\/&lt;model_version&gt;&quot;\n          )\n\nmodel.predict(...)\n<\/code><\/pre>\n<p>With <code>mlflow.tracking.client.MlflowClient<\/code> you can retrieve metadata about a model from the model registry, but for retrieving the actual model you will need to use <code>mlflow.&lt;model_flavor&gt;.load_model<\/code>. For example, you could use the MlflowClient to get the download URI for a given model, and then use <code>mlflow.&lt;flavor&gt;.load_model<\/code> to retrieve that model.<\/p>\n<pre><code>model_uri = client.get_model_version_download_uri(&quot;&lt;model_name&gt;&quot;, &lt;version&gt;)\nmodel = mlflow.pyfunc.load_model(model_uri)\n\nmodel.predict(...)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1609791959680,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65494496",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62296590,
        "Question_title":"MLFlow and Hydra causing crash when used together",
        "Question_body":"<p>I'm trying to utilize Hydra with MLFlow, so I wrote the bare minimum script to see if they worked together (importing etc.). Both work fine on their own, but when put together I get a weird outcome.<\/p>\n\n<p>I have the script below:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import hydra\nfrom omegaconf import DictConfig\nfrom mlflow import log_metric, log_param, log_artifact,start_run\n\n@hydra.main(config_path=\"config.yaml\")\ndef my_app(cfg : DictConfig):\n    # print(cfg.pretty())\n    # print(cfg['coordinates']['x0'])\n    log_param(\"a\",2)\n    log_metric(\"b\",3)\n\nif __name__ == \"__main__\":\n    my_app()\n<\/code><\/pre>\n\n<p>However when ran, I get the error below:<\/p>\n\n<pre><code>ilknull@nurmachine:~\/Files\/Code\/Python\/MLFlow_test$ python3 hydra_temp.py \nError in atexit._run_exitfuncs:\nTraceback (most recent call last):\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/fluent.py\", line 164, in end_run\n    MlflowClient().set_terminated(_active_run_stack[-1].info.run_id, status)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 311, in set_terminated\n    self._tracking_client.set_terminated(run_id, status, end_time)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 312, in set_terminated\n    end_time=end_time)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/file_store.py\", line 377, in update_run_info\n    run_info = self._get_run_info(run_id)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/file_store.py\", line 442, in _get_run_info\n    databricks_pb2.RESOURCE_DOES_NOT_EXIST)\nmlflow.exceptions.MlflowException: Run '9066793c02604a6783d081ed965d5eff' not found\n<\/code><\/pre>\n\n<p>Again, they work perfectly fine when used separately, but together they cause this error. Any ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591767515617,
        "Question_score":4,
        "Question_tags":"python|docker|exception|mlflow|fb-hydra",
        "Question_view_count":718,
        "Owner_creation_time":1583072555673,
        "Owner_last_access_time":1663970229957,
        "Owner_location":null,
        "Owner_reputation":301,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Thanks for reporting this. I was not aware of this issue.<\/p>\n\n<p>This is because Hydra is changing your current working directory for each run.<\/p>\n\n<p>I did some digging, this is what you can do:<\/p>\n\n<ol>\n<li>Set the MLFLOW_TRACKING_URI environment variable:<\/li>\n<\/ol>\n\n<pre><code>MLFLOW_TRACKING_URI=file:\/\/\/$(pwd)\/.mlflow  python3 hydra_temp.py\n<\/code><\/pre>\n\n<ol start=\"2\">\n<li>Call set_tracking_url() before hydra.main() starts:<\/li>\n<\/ol>\n\n<pre><code>import hydra\nfrom omegaconf import DictConfig\nfrom mlflow import log_metric, log_param, set_tracking_uri\nimport os\n\nset_tracking_uri(f\"file:\/\/\/{os.getcwd()}\/.mlflow\")\n\n@hydra.main(config_name=\"config\")\ndef my_app(cfg: DictConfig):\n    log_param(\"a\", 2)\n    log_metric(\"b\", 3)\n\n\nif __name__ == \"__main__\":\n    my_app()\n<\/code><\/pre>\n\n<ol start=\"3\">\n<li>Wait for my <a href=\"https:\/\/github.com\/facebookresearch\/hydra\/issues\/664\" rel=\"nofollow noreferrer\">new issue<\/a> to get resolved, then there will have a proper plugin to integrate with mlflow.\n(This will probably take a while).<\/li>\n<\/ol>\n\n<p>By the way, Hydra 1.0 has new support for setting environment variables:<\/p>\n\n<p>This <em>ALMOST<\/em> works:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>hydra:\n  job:\n    env_set:\n      MLFLOW_TRACKING_DIR: file:\/\/${hydra:runtime.cwd}\/.mlflow\n      MLFLOW_TRACKING_URI: file:\/\/${hydra:runtime.cwd}\/.mlflow\n<\/code><\/pre>\n\n<p>Unfortunately Hydra is cleaning up the env variables when your function exits, and MLFlow is making the final save when the process exits so the env variable is no longer set.\nMLFlow also keeps re-initializing the FileStore object used to store the experiments data. If they would have initialized it just once and reused the same object the above should would have worked.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1591773861797,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1591782103620,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62296590",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56647549,
        "Question_title":"MLflow Error while deploying the Model to local REST server",
        "Question_body":"<blockquote>\n  <p><strong>System Details:<\/strong><\/p>\n  \n  <p>Operating System: Ubuntu 19.04<\/p>\n  \n  <p>Anaconda version: 2019.03<\/p>\n  \n  <p>Python version: 3.7.3<\/p>\n  \n  <p>mlflow version: 1.0.0<\/p>\n<\/blockquote>\n\n<p><strong>Steps to Reproduce:<\/strong> <a href=\"https:\/\/mlflow.org\/docs\/latest\/tutorial.html\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tutorial.html<\/a><\/p>\n\n<p><strong>Error at line\/command:<\/strong> <code>mlflow models serve -m [path_to_model] -p 1234<\/code><\/p>\n\n<p><strong>Error:<\/strong>\nCommand 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1>&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1<\/p>\n\n<p><strong>Terminal Log:<\/strong><\/p>\n\n<pre><code>(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# mlflow models serve -m $(pwd) -p 1234\n2019\/06\/18 16:15:16 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n2019\/06\/18 16:15:17 INFO mlflow.pyfunc.backend: === Running command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app'\nbash: activate: No such file or directory\nTraceback (most recent call last):\n  File \"\/root\/anaconda3\/envs\/mlflow\/bin\/mlflow\", line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/models\/cli.py\", line 43, in serve\n    host=host)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 76, in serve\n    command_env=command_env)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 147, in _execute_in_conda_env\n    command, rc\nException: Command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1\n(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# \n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1560855399150,
        "Question_score":0,
        "Question_tags":"python-3.x|deployment|mlflow",
        "Question_view_count":1840,
        "Owner_creation_time":1504001058090,
        "Owner_last_access_time":1630952509900,
        "Owner_location":null,
        "Owner_reputation":2101,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":41,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Following the steps mentioned in the GitHub Issue <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">1507<\/a> (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/1507<\/a>) I was able to resolve this issue.<\/p>\n\n<p>In reference to this post, the \"<strong>anaconda\/bin\/<\/strong>\" directory is never added to the list of environment variables i.e. PATH variable. In order to resolve this issue, add the \"<strong>else<\/strong>\" part of conda initialize code block from ~\/.bashrc file to your PATH variable.<\/p>\n\n<pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('\/home\/atulk\/anaconda3\/bin\/conda' 'shell.bash' 'hook' 2&gt; \/dev\/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\" ]; then\n        . \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\"\n    else\n        export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n<\/code><\/pre>\n\n<p>In this case, I added <strong>export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"<\/strong> to the PATH variable. However, this is just a temporary fix until the issue is fixed in the project.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1561730574530,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56647549",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72250896,
        "Question_title":"PowerShell Get request with body",
        "Question_body":"<p>I am trying <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#search-modelversions\" rel=\"nofollow noreferrer\">this api endpoint<\/a>.\nI can call this in python, no problem, like the below<\/p>\n<pre><code>get_model_versions={\n    &quot;filter&quot;:&quot;name='model_name'&quot;,\n    &quot;order_by&quot;:[&quot;version DESC&quot;],\n    &quot;max_results&quot;:1\n}\n\ninit_get = requests.get(&quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search&quot;,headers=header_read,json=get_model_versions)\n<\/code><\/pre>\n<p>However, I just can't seem to find a way to make it work in Powershell.<\/p>\n<p>First the powershell &quot;get&quot; Invoke-RestMethod does not accept a body<\/p>\n<p>and then I can't seem to find a way to append it in Powershell as a query string.<\/p>\n<p>I have tried (among other failed attempts), the following<\/p>\n<pre><code>$get_model_versions=([PSCustomObject]@{\n  filter = &quot;name=`'model_name`'&quot;\n  order_by = @(&quot;version desc&quot;)\n} | ConvertTo-Json)\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get -Body $get_model_versions\n<\/code><\/pre>\n<p>But that gives me an error that body can't be used with a get method<\/p>\n<p>trying to append it as a query string (like if I even just keep the name filter and remove the others), also fails<\/p>\n<pre><code>$searchuri= &quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search?filter=&quot;&quot;name==model_name&quot;&quot;&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get\n<\/code><\/pre>\n<p>fails with<\/p>\n<pre><code>{&quot;error_code&quot;:&quot;INVALID_PARAMETER_VALUE&quot;,&quot;message&quot;:&quot;Unsupported filter query : `\\&quot;name==model_name\\&quot;`. Unsupported operator.&quot;}\n<\/code><\/pre>\n<p>How can I mimic the same behaviour in Powershell, as I do in Python?<\/p>\n<p>EDIT 1: I did try to encode the query param (maybe I did it wrong), but here's how my failed attempt looked like<\/p>\n<pre><code>$encodedvalue = [System.Web.HttpUtility]::UrlEncode(&quot;`&quot;name='model_name'`&quot;&quot;)\n$searchuri= &quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search?filter=$encodedvalue&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get\n<\/code><\/pre>\n<p>But that too gives me<\/p>\n<pre><code>&quot;Unsupported filter query : `\\&quot;name='model_name'\\&quot;`. Unsupported operator.&quot;\n<\/code><\/pre>\n<p>I have also tried it successfully in Postman by passing a raw json body (the same as python) and when I look at the generated PowerShell code in Postman I see this<\/p>\n<pre><code>$headers = New-Object &quot;System.Collections.Generic.Dictionary[[String],[String]]&quot;\n$headers.Add(&quot;Authorization&quot;, &quot;Bearer token&quot;)\n$headers.Add(&quot;Content-Type&quot;, &quot;application\/json&quot;)\n\n$body = &quot;{\n`n    `&quot;filter`&quot;:`&quot;name='model_name'`&quot;,\n`n    `&quot;order_by`&quot;:[`&quot;version DESC`&quot;],\n`n    `&quot;max_results`&quot;:1\n`n}\n`n&quot;\n\n$response = Invoke-RestMethod 'baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search' -Method 'GET' -Headers $headers -Body $body\n$response | ConvertTo-Json\n<\/code><\/pre>\n<p>But of course that fails (if you copy that in an powershell editor and run it<\/p>\n<pre><code>Invoke-RestMethod : Cannot send a content-body with this verb-type\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1652637573657,
        "Question_score":1,
        "Question_tags":"powershell|rest|python-requests|mlflow",
        "Question_view_count":281,
        "Owner_creation_time":1428654714763,
        "Owner_last_access_time":1664012257383,
        "Owner_location":null,
        "Owner_reputation":596,
        "Owner_up_votes":53,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Question_last_edit_time":1652640935860,
        "Answer_body":"<p>Finally, after struggling for a long time, I found the answer !<\/p>\n<p>The crux is in the documentation <a href=\"https:\/\/docs.microsoft.com\/en-us\/powershell\/module\/microsoft.powershell.utility\/invoke-restmethod?view=powershell-7.2\" rel=\"nofollow noreferrer\">here<\/a>.\nEspecially this section<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/h0gwk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/h0gwk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So, if you want to pass on a body for your &quot;get&quot; method in powershell, pass it as a hashtable.<\/p>\n<p>So, finally the answer is<\/p>\n<pre><code>$query=@{&quot;filter&quot;=&quot;name='model_name'&quot;;&quot;order_by&quot;=@(&quot;version DESC&quot;); &quot;max_results&quot;=1};\n$searchuri=&quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get -Body $query\n<\/code><\/pre>\n<p>Hope this helps someone looking for something similar.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1652649592320,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72250896",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60641337,
        "Question_title":"mlflow R installation MLFLOW_PYTHON_BIN",
        "Question_body":"<p>I am trying to install mlflow in R and im getting this error message saying <\/p>\n\n<blockquote>\n  <p>mlflow::install_mlflow()\n  Error in mlflow_conda_bin() :\n    Unable to find conda binary. Is Anaconda installed?\n    If you are not using conda, you can set the environment variable MLFLOW_PYTHON_BIN to the path of yourpython executable.<\/p>\n<\/blockquote>\n\n<p>I have tried the following<\/p>\n\n<pre><code>export MLFLOW_PYTHON_BIN=\"\/usr\/bin\/python\" \nsource ~\/.bashrc\necho $MLFLOW_PYTHON_BIN  -&gt; this prints the \/usr\/bin\/python.\n<\/code><\/pre>\n\n<p>or in R,<\/p>\n\n<pre><code>sys.setenv(MLFLOW_PYTHON_BIN=\"\/usr\/bin\/python\")\nsys.getenv() -&gt; prints MLFLOW_PYTHON_BIN is set to \/usr\/bin\/python.\n<\/code><\/pre>\n\n<p>however, it still does not work<\/p>\n\n<p>I do not want to use conda environment.<\/p>\n\n<p>how to I get past this error?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1583947052940,
        "Question_score":5,
        "Question_tags":"r|mlflow|system-variable",
        "Question_view_count":1141,
        "Owner_creation_time":1539211301843,
        "Owner_last_access_time":1663982305137,
        "Owner_location":null,
        "Owner_reputation":117,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":1584403666973,
        "Answer_body":"<p>The install_mlflow command only works with conda right now, sorry about the confusing message. You can either:<\/p>\n<ul>\n<li>install conda - this is the recommended way of installing and using mlflow<\/li>\n<\/ul>\n<p>or<\/p>\n<ul>\n<li>install mlflow python package yourself via pip<\/li>\n<\/ul>\n<p>To install mlflow yourself, pip install correct (matching the the R package) python version of mlflow and set the MLFLOW_PYTHON_BIN environment variable as well as MLFLOW_BIN evn variable: e.g.<\/p>\n<pre><code>library(mlflow)\nsystem(paste(&quot;pip install -U mlflow==&quot;, mlflow:::mlflow_version(), sep=&quot;&quot;))\nSys.setenv(MLFLOW_BIN=system(&quot;which mlflow&quot;))\nSys.setenv(MLFLOW_PYTHON_BIN=system(&quot;which python&quot;))\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1584554585177,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1624202175903,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60641337",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72886409,
        "Question_title":"MLflow proxied artifact access: Unable to locate credentials",
        "Question_body":"<p>I am using MLflow to track my experiments. I am using an S3 bucket as an artifact store. For acessing it, I want to use <em>proxied artifact access<\/em>, as described in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>, however this does not work for me, since it locally looks for credentials (but the server should handle this).<\/p>\n<h2>Expected Behaviour<\/h2>\n<p>As described in the docs, I would expect that locally, I do not need to specify my AWS credentials, since the server handles this for me. From <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n<blockquote>\n<p>This eliminates the need to allow end users to have direct path access to a remote object store (e.g., s3, adls, gcs, hdfs) for artifact handling and eliminates the need for an end-user to provide access credentials to interact with an underlying object store.<\/p>\n<\/blockquote>\n<h2>Actual Behaviour \/ Error<\/h2>\n<p>Whenever I run an experiment on my machine, I am running into the following error:<\/p>\n<p><code>botocore.exceptions.NoCredentialsError: Unable to locate credentials<\/code><\/p>\n<p>So the error is local. However, this should not happen since the server should handle the auth instead of me needing to store my credentials locally. Also, I would expect that I would not even need library <code>boto3<\/code> locally.<\/p>\n<h2>Solutions Tried<\/h2>\n<p>I am aware that I need to create a new experiment, because existing experiments might still use a different artifact location which is proposed in <a href=\"https:\/\/stackoverflow.com\/a\/71417933\/10465165\">this SO answer<\/a> as well as in the note in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>. Creating a new experiment did not solve the error for me. Whenever I run the experiment, I get an explicit log in the console validating this:<\/p>\n<p><code>INFO mlflow.tracking.fluent: Experiment with name 'test' does not exist. Creating a new experiment.<\/code><\/p>\n<p>Related Questions (<a href=\"https:\/\/stackoverflow.com\/questions\/72206086\/cant-log-mlflow-artifacts-to-s3-with-docker-based-tracking-server\">#1<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/72236258\/mlflow-unable-to-store-artifacts-to-s3\/72261826#comment128726676_72261826\">#2<\/a>) refer to a different scenario, which is also <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-4-mlflow-with-remote-tracking-server-backend-and-artifact-stores\" rel=\"nofollow noreferrer\">described in the docs<\/a><\/p>\n<h2>Server Config<\/h2>\n<p>The server runs on a kubernetes pod with the following config:<\/p>\n<pre><code>mlflow server \\\n    --host 0.0.0.0 \\\n    --port 5000 \\\n    --backend-store-uri postgresql:\/\/user:pw@endpoint \\\n    --artifacts-destination s3:\/\/my_bucket\/artifacts \\\n    --serve-artifacts \\\n    --default-artifact-root s3:\/\/my_bucket\/artifacts \\\n<\/code><\/pre>\n<p>I would expect my config to be correct, looking at doc <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">page 1<\/a> and <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#using-the-tracking-server-for-proxied-artifact-access\" rel=\"nofollow noreferrer\">page 2<\/a><\/p>\n<p>I am able to see the mlflow UI if I forward the port to my local machine. I also see the experiment runs as failed, because of the error I sent above.<\/p>\n<h2>My Code<\/h2>\n<p>The relevant part of my code which fails is the logging of the model:<\/p>\n<pre><code>mlflow.set_tracking_uri(&quot;http:\/\/localhost:5000&quot;)\nmlflow.set_experiment(&quot;test2)\n\n...\n\n# this works\nmlflow.log_params(hyperparameters)\n                        \nmodel = self._train(model_name, hyperparameters, X_train, y_train)\ny_pred = model.predict(X_test)\nself._evaluate(y_test, y_pred)\n\n# this fails with the error from above\nmlflow.sklearn.log_model(model, &quot;artifacts&quot;)\n\n<\/code><\/pre>\n<h2>Question<\/h2>\n<p>I am probably overlooking something. Is there a need to locally indicate that I want to use proxied artified access? If yes, how do I do this? Is there something I have missed?<\/p>\n<h2>Full Traceback<\/h2>\n<pre><code>  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/models\/model.py&quot;, line 295, in log\n    mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 726, in log_artifacts\n    MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py&quot;, line 1001, in log_artifacts\n    self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 346, in log_artifacts\n    self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/store\/artifact\/s3_artifact_repo.py&quot;, line 141, in log_artifacts\n    self._upload_file(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/store\/artifact\/s3_artifact_repo.py&quot;, line 117, in _upload_file\n    s3_client.upload_file(Filename=local_file, Bucket=bucket, Key=key, ExtraArgs=extra_args)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/boto3\/s3\/inject.py&quot;, line 143, in upload_file\n    return transfer.upload_file(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/boto3\/s3\/transfer.py&quot;, line 288, in upload_file\n    future.result()\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/futures.py&quot;, line 103, in result\n    return self._coordinator.result()\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/futures.py&quot;, line 266, in result\n    raise self._exception\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/tasks.py&quot;, line 139, in __call__\n    return self._execute_main(kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/tasks.py&quot;, line 162, in _execute_main\n    return_value = self._main(**kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/upload.py&quot;, line 758, in _main\n    client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 508, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 898, in _make_api_call\n    http, parsed_response = self._make_request(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 921, in _make_request\n    return self._endpoint.make_request(operation_model, request_dict)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 119, in make_request\n    return self._send_request(request_dict, operation_model)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 198, in _send_request\n    request = self.create_request(request_dict, operation_model)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 134, in create_request\n    self._event_emitter.emit(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 412, in emit\n    return self._emitter.emit(aliased_event_name, **kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 256, in emit\n    return self._emit(event_name, kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 239, in _emit\n    response = handler(**kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/signers.py&quot;, line 103, in handler\n    return self.sign(operation_name, request)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/signers.py&quot;, line 187, in sign\n    auth.add_auth(request)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/auth.py&quot;, line 407, in add_auth\n    raise NoCredentialsError()\nbotocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1657122030593,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|amazon-s3|boto3|mlflow",
        "Question_view_count":237,
        "Owner_creation_time":1538816771613,
        "Owner_last_access_time":1663856763733,
        "Owner_location":"Berlin",
        "Owner_reputation":647,
        "Owner_up_votes":971,
        "Owner_down_votes":39,
        "Owner_views":61,
        "Question_last_edit_time":1657122771947,
        "Answer_body":"<p>The problem is that the server is running on wrong run parameters, the <code>--default-artifact-root<\/code> needs to either be removed or set to <code>mlflow-artifacts:\/<\/code>.<\/p>\n<p>From <code>mlflow server --help<\/code>:<\/p>\n<pre><code>  --default-artifact-root URI  Directory in which to store artifacts for any\n                               new experiments created. For tracking server\n                               backends that rely on SQL, this option is\n                               required in order to store artifacts. Note that\n                               this flag does not impact already-created\n                               experiments with any previous configuration of\n                               an MLflow server instance. By default, data\n                               will be logged to the mlflow-artifacts:\/ uri\n                               proxy if the --serve-artifacts option is\n                               enabled. Otherwise, the default location will\n                               be .\/mlruns.\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1657186814370,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72886409",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56967364,
        "Question_title":"Keep track of all the parameters of spark-submit",
        "Question_body":"<p>I have a team where many member has permission to submit Spark tasks to YARN (the resource management) by command line. It's hard to track who is using how much cores, who is using how much memory...e.g. Now I'm looking for a software, framework or something could help me monitor the parameters that each member used. It will be a bridge between client and YARN. Then I could used it to filter the submit commands.<\/p>\n\n<p>I did take a look at <a href=\"http:\/\/www.mlflow.org\" rel=\"nofollow noreferrer\">mlflow<\/a> and I really like the MLFlow Tracking but it was designed for ML training process. I wonder if there is an alternative for my purpose? Or there is any other solution for the problem.<\/p>\n\n<p>Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1562750084187,
        "Question_score":0,
        "Question_tags":"apache-spark|parameters|hadoop-yarn|spark-submit|mlflow",
        "Question_view_count":93,
        "Owner_creation_time":1413431014113,
        "Owner_last_access_time":1566529894493,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>My recommendation would be to build such a tool yourself as its not too complicated,\nhave a wrapper script to spark submit which logs the usage in a DB and after the spark job finishes the wrapper will know to release information. could be done really easily.\nIn addition you can even block new spark submits if your team already asked for too much information.<\/p>\n\n<p>And as you build it your self its really flexible as you can even create \"sub teams\" or anything you want.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1562766864887,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56967364",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58600732,
        "Question_title":"Is there a way to hide mlflow ui header when start the server with mlflow server?",
        "Question_body":"<p>I want to integrate mlflow ui to our website by using an iframe, but with the header hidden if possible. I found there is an environment variable setting in the source code \/mlflow\/server\/js\/components\/HomeView.js:\n<code>const headerHeight = process.env.HIDE_HEADER === 'true' ? 0 : 60;<\/code> But how can I specify this environment by running the server with <code>mlflow server<\/code>? I tried with <code>HIDE_HEADER=true mlflow server<\/code>, but this doesn't work. Or is there any other way to solve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1572316204850,
        "Question_score":0,
        "Question_tags":"node.js|reactjs|mlflow",
        "Question_view_count":296,
        "Owner_creation_time":1441151447410,
        "Owner_last_access_time":1663903618597,
        "Owner_location":null,
        "Owner_reputation":626,
        "Owner_up_votes":75,
        "Owner_down_votes":1,
        "Owner_views":79,
        "Question_last_edit_time":null,
        "Answer_body":"<p>@Jason good question, those environment variables are read at build-time for the MLflow UI's Javascript assets. Since the PyPI MLflow wheel comes with pre-built Javascript assets, it's difficult to achieve your use case using a PyPI installation of <code>mlflow<\/code>.<\/p>\n\n<p>However, you can build a custom MLflow wheel from source with the UI header hidden by following the instructions <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/branch-1.3\/CONTRIBUTING.rst#building-a-distributable-artifact\" rel=\"nofollow noreferrer\">here<\/a>, replacing the <code>npm run build<\/code> step with <code>HIDE_HEADER=true npm run build<\/code> (basically, the idea is to set the desired environment variables prior to building Javascript assets via <code>npm run build<\/code>). You can then pip-install that wheel on the node hosting your MLflow server &amp; launch the server via <code>mlflow server<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1572468752163,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58600732",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58541794,
        "Question_title":"MLflow saving weights after each epoch",
        "Question_body":"<p>I have been testing some small examples with MLflow tracking but for my usecase I would like to have the weights saved after each epoch. \nSometimes I kill the runs before they are completely finished (I cannot use earlystopping), but what I experience now is that the weights do not get saved to the tracking ui server.\nIs there a way to do this after each epoch?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1571921401100,
        "Question_score":1,
        "Question_tags":"keras|mlflow",
        "Question_view_count":1121,
        "Owner_creation_time":1534367348473,
        "Owner_last_access_time":1663928245177,
        "Owner_location":"Leuven, Belgium",
        "Owner_reputation":344,
        "Owner_up_votes":74,
        "Owner_down_votes":2,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Save the weights to disk and then log them as an artifact.  As long as the checkpoints\/weights are saved to disk, you can log them with <code>mlflow_log_artifact()<\/code> or <code>mlflow_log_artifacts()<\/code>.  From the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#logging-functions\" rel=\"nofollow noreferrer\">docs<\/a>,<\/p>\n\n<blockquote>\n  <p><strong>mlflow.log_artifact()<\/strong> logs a local file or directory as an artifact,\n  optionally taking an artifact_path to place it in within the run\u2019s\n  artifact URI. Run artifacts can be organized into directories, so you\n  can place the artifact in a directory this way.<\/p>\n  \n  <p><strong>mlflow.log_artifacts()<\/strong> logs all the files in a given directory as\n  artifacts, again taking an optional artifact_path.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1572525493020,
        "Answer_score":1.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58541794",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63973530,
        "Question_title":"Convert an instance of xgboost.Booster into a model that implements the scikit-learn API",
        "Question_body":"<p>I am trying to use <code>mlflow<\/code> to save a model and then load it later to make predictions.<\/p>\n<p>I'm using a <code>xgboost.XGBRegressor<\/code> model and its sklearn functions <code>.predict()<\/code> and <code>.predict_proba()<\/code> to make predictions but it turns out that <code>mlflow<\/code> doesn't support models that implements the sklearn API, so when loading the model later from mlflow, mlflow returns an instance of <code>xgboost.Booster<\/code>, and it doesn't implements the <code>.predict()<\/code> or <code>.predict_proba()<\/code> functions.<\/p>\n<p>Is there a way to convert a <code>xgboost.Booster<\/code> back into a <code>xgboost.sklearn.XGBRegressor<\/code> object that implements the sklearn API functions?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1600550858547,
        "Question_score":3,
        "Question_tags":"scikit-learn|save|xgboost|mlflow|xgbclassifier",
        "Question_view_count":1317,
        "Owner_creation_time":1592264086427,
        "Owner_last_access_time":1663962090790,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Have you tried wrapping up your model in custom class, logging and loading it using <code>mlflow.pyfunc.PythonModel<\/code>?\nI put up a simple example and upon loading back the model it correctly shows <code>&lt;class 'xgboost.sklearn.XGBRegressor'&gt;<\/code> as a type.<\/p>\n<p>Example:<\/p>\n<pre><code>import xgboost as xgb\nxg_reg = xgb.XGBRegressor(...)\n\nclass CustomModel(mlflow.pyfunc.PythonModel):\n    def __init__(self, xgbRegressor):\n        self.xgbRegressor = xgbRegressor\n\n    def predict(self, context, input_data):\n        print(type(self.xgbRegressor))\n        \n        return self.xgbRegressor.predict(input_data)\n\n# Log model to local directory\nwith mlflow.start_run():\n     custom_model = CustomModel(xg_reg)\n     mlflow.pyfunc.log_model(&quot;custome_model&quot;, python_model=custom_model)\n\n\n# Load model back\nfrom mlflow.pyfunc import load_model\nmodel = load_model(&quot;\/mlruns\/0\/..\/artifacts\/custome_model&quot;)\nmodel.predict(X_test)\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>&lt;class 'xgboost.sklearn.XGBRegressor'&gt;\n[ 9.107417 ]\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1600607182583,
        "Answer_score":4.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63973530",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59642900,
        "Question_title":"Unable to connect Mlflow server to my mlflow project image",
        "Question_body":"<p>My final purpose is to run experiment from  an Api.<\/p>\n\n<p>the experiment come from :\n<a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/tensorflow\/tf2\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/tensorflow\/tf2<\/a>\nbut export the file in my custom git where I clone it, in the image below -><\/p>\n\n<p>I have 2 images in my docker compose :\ntree project : <\/p>\n\n<pre><code>|_app\/\n| |_Dockerfile\n|\n|_mlflow\/\n| |_Dockerfile\n|\n|_docker-compose.yml\n\n<\/code><\/pre>\n\n<p>app\/Dockerfile<\/p>\n\n<pre><code>FROM continuumio\/anaconda3\n\nENV APP_HOME .\/\nWORKDIR ${APP_HOME}\nRUN conda config --append channels conda-forge\nRUN conda install --quiet --yes \\\n    'mlflow' \\\n    'psycopg2' \\\n    'tensorflow'\nRUN pip install pylint\nRUN pwd;ls \\\n&amp;&amp; git clone https:\/\/github.com\/MChrys\/QuickSign.git \nRUN pwd;ls \\\n    &amp;&amp; cd QuickSign \\\n    &amp;&amp; pwd;ls\n\nCOPY . .\n\n#RUN conda install jupyter \n#CMD jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root --no-browser\nCMD cd QuickSign &amp;&amp; mlflow run .\n<\/code><\/pre>\n\n<p>mlflow\/Dockerfile<\/p>\n\n<pre><code>FROM python:3.7.0\n\nRUN pip install mlflow\n\nRUN mkdir \/mlflow\/\n\nCMD mlflow server \\\n    --backend-store-uri \/mlflow \\\n    --host 0.0.0.0\n<\/code><\/pre>\n\n<p>docker-compose.yml<\/p>\n\n<pre><code>version: '3'\nservices:\n  notebook:\n    build:\n      context: .\/app\n    ports:\n      - \"8888:8888\"\n    depends_on: \n      - mlflow\n    environment: \n      MLFLOW_TRACKING_URI: 'http:\/\/mlflow:5000'\n  mlflow:\n    build:\n      context: .\/mlflow\n    expose: \n      - \"5000\"\n    ports:\n      - \"5000:5000\"\n<\/code><\/pre>\n\n<p>when I <code>docker-compose up<\/code> the image I obtain  :<\/p>\n\n<pre><code>notebook_1_74059cdc20ce |     response = requests.request(**kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/api.py\", line 60, in request\nnotebook_1_74059cdc20ce |     return session.request(method=method, url=url, **kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 533, in request\nnotebook_1_74059cdc20ce |     resp = self.send(prep, **send_kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 646, in send\nnotebook_1_74059cdc20ce |     r = adapter.send(request, **kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/adapters.py\", line 516, in send\nnotebook_1_74059cdc20ce |     raise ConnectionError(e, request=request)\nnotebook_1_74059cdc20ce | requests.exceptions.ConnectionError: HTTPConnectionPool(host='mlflow', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/create (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7fd5db4edc50&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n\n<\/code><\/pre>\n\n<p>The problem look like that I run a project which is not found in the server images, as I run it in the app image, but I don't know how figure it out I have to trigger  the experiment from a futur flask app <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1578475698630,
        "Question_score":0,
        "Question_tags":"docker|docker-compose|mlflow",
        "Question_view_count":2937,
        "Owner_creation_time":1459625723203,
        "Owner_last_access_time":1648342504593,
        "Owner_location":"Grenoble, France",
        "Owner_reputation":56,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Question_last_edit_time":1578477326563,
        "Answer_body":"<p>The problem came from  docker for windows, I was unable to make working docker compose on it but there are no problem to build it when I run it on virtual machine with ubuntu.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1586436837590,
        "Answer_score":0.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59642900",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56984854,
        "Question_title":"fcntl error with \u201cmlflow ui\u201d on windows - mlflow 1.0",
        "Question_body":"<p>I am getting the following error message when trying mlflow examples and running 'mlflow ui'.<\/p>\n\n<p><strong>Error:<\/strong><\/p>\n\n<blockquote>\n  <p>ModuleNotFoundError: No module named 'fcntl' Running the mlflow server\n  failed. Please see the logs above for details<\/p>\n<\/blockquote>\n\n<p>Is anyone aware of a solution to this issue?<\/p>\n\n<p>I have tried the solutions suggested at <a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/1080\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/pull\/1080<\/a><\/p>\n\n<p>without success. Replacing the modified files in mlflow source code, it raises other issues for not finding what it is looking for with the following:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\thesis_mlflow\\Scripts\\mlflow.exe\\__main__.py\", line 9, in &lt;module&gt;\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\mlflow\\cli.py\", line 198, in ui\n    _run_server(backend_store_uri, default_artifact_root, \"127.0.0.1\", port, None, 1)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\mlflow\\server\\__init__.py\", line 90, in _run_server\n    exec_cmd(full_command, env=env_map, stream_output=True)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\mlflow\\utils\\process.py\", line 34, in exec_cmd\n    stdin=subprocess.PIPE, **kwargs)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\subprocess.py\", line 729, in __init__\n    restore_signals, start_new_session)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\subprocess.py\", line 1017, in _execute_child\n    startupinfo)\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1562833873410,
        "Question_score":1,
        "Question_tags":"python|windows|mlflow",
        "Question_view_count":725,
        "Owner_creation_time":1529408888483,
        "Owner_last_access_time":1649930832187,
        "Owner_location":"London, UK",
        "Owner_reputation":79,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1562909693510,
        "Answer_body":"<p>Just solved the issue: for some reason, waitress was not installed in the running environment. After installing it, everything seems working fine with the solution #1080 linked above in the question.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1562919127687,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56984854",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61258979,
        "Question_title":"EKS Docker Image Pull CrashLoopBackOff",
        "Question_body":"<p>I'm trying to deploy a Docker image from ECR to my EKS. When attempting to deploy my docker image to a pod, I get the following events from a CrashLoopBackOff:<\/p>\n\n<pre><code>Events:\n  Type     Reason                  Age               From                                   Message\n  ----     ------                  ----              ----                                   -------\n  Normal   Scheduled               62s               default-scheduler                      Successfully assigned default\/mlflow-tracking-server to &lt;EC2 IP&gt;.internal\n  Normal   SuccessfulAttachVolume  60s               attachdetach-controller                AttachVolume.Attach succeeded for volume \"&lt;PVC&gt;\"\n  Normal   Pulling                 56s               kubelet, &lt;IP&gt;.ec2.internal             Pulling image \"&lt;ECR Image UI&gt;\"\n  Normal   Pulled                  56s               kubelet, &lt;IP&gt;.ec2.internal             Successfully pulled image \"&lt;ECR Image UI&gt;\"\n  Normal   Created                 7s (x4 over 56s)  kubelet, &lt;IP&gt;.ec2.internal             Created container mlflow-tracking-server\n  Normal   Pulled                  7s (x3 over 54s)  kubelet, &lt;IP&gt;.ec2.internal             Container image \"&lt;ECR Image UI&gt;\" already present on machine\n  Normal   Started                 6s (x4 over 56s)  kubelet, &lt;IP&gt;.ec2.internal             Started container mlflow-tracking-server\n  Warning  BackOff                 4s (x5 over 52s)  kubelet, &lt;IP&gt;.ec2.internal             Back-off restarting failed container\n<\/code><\/pre>\n\n<p>I don't understand why it keeps looping like this and failing. Would anyone know why this is happening?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1587067011163,
        "Question_score":1,
        "Question_tags":"docker|amazon-eks|mlflow",
        "Question_view_count":604,
        "Owner_creation_time":1417012835813,
        "Owner_last_access_time":1628782664573,
        "Owner_location":null,
        "Owner_reputation":945,
        "Owner_up_votes":35,
        "Owner_down_votes":1,
        "Owner_views":148,
        "Question_last_edit_time":null,
        "Answer_body":"<p><code>CrashLoopBackError<\/code> can be related to these possible reasons:<\/p>\n\n<ul>\n<li><p>the application inside your pod is not starting due to an error;<\/p><\/li>\n<li><p>the image your pod is based on is not present in the registry, or the\nnode where your pod has been scheduled cannot pull from the registry;<\/p><\/li>\n<li><p>some parameters of the pod has not been configured correctly.<\/p><\/li>\n<\/ul>\n\n<p>In your case it seems an application error, inside the container.\nTry to view the logs with:<\/p>\n\n<pre><code>kubectl logs &lt;your_pod&gt; -n &lt;namespace&gt;\n<\/code><\/pre>\n\n<p>For more info on how to troubleshoot this kind of error refer to:<\/p>\n\n<p><a href=\"https:\/\/pillsfromtheweb.blogspot.com\/2020\/05\/troubleshooting-kubernetes.html\" rel=\"nofollow noreferrer\">https:\/\/pillsfromtheweb.blogspot.com\/2020\/05\/troubleshooting-kubernetes.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1588674359533,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61258979",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69479488,
        "Question_title":"Hi. I am very new to MLFlow, and want to implement MLFlow project on my own ML model. However I am getting \"\"Could not find main among entry points\"\"",
        "Question_body":"<p>The full error message is as below:<\/p>\n<pre><code>ERROR mlflow.cli: === Could not find main among entry points [] or interpret main as a runnable script. Supported script file extensions: ['.py', '.sh'] ===\n\n<\/code><\/pre>\n<p>I also try the solutions suggested here <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code>, but the result is the same.<\/p>\n<p>Below I provide all the required files to run <code>MLflow<\/code> project.<\/p>\n<p>The <code>conda.yaml<\/code> file<\/p>\n<pre><code>name: lightgbm-example\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.6\n  - pip\n  - pip:\n      - mlflow&gt;=1.6.0\n      - lightgbm\n      - pandas\n      - numpy\n<\/code><\/pre>\n<p>The MLProject file<\/p>\n<pre><code>name: lightgbm-example\nconda_env: ~\/Desktop\/MLflow\/conda.yaml\nentry-points:\n    main:\n      parameters:\n        learning_rate: {type: float, default: 0.1}\n        colsample_bytree: {type: float, default: 1.0}\n        subsample: {type: float, default: 1.0} \n      command: |\n          python3 ~\/Desktop\/MLflow\/Test.py \\\n            --learning-rate={learning_rate} \\\n            --colsample-bytree={colsample_bytree} \\\n            --subsample={subsample}\n<\/code><\/pre>\n<p>My Test.py file<\/p>\n<pre><code>import pandas as pd\nimport lightgbm as lgb\nimport numpy as np\nimport mlflow\nimport mlflow.lightgbm\nimport argparse\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=&quot;LightGBM example&quot;)\n    parser.add_argument(\n        &quot;--learning-rate&quot;,\n        type=float,\n        default=0.1,\n        help=&quot;learning rate to update step size at each boosting step (default: 0.3)&quot;,\n    )\n    parser.add_argument(\n        &quot;--colsample-bytree&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of columns when constructing each tree (default: 1.0)&quot;,\n    )\n    parser.add_argument(\n        &quot;--subsample&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of the training instances (default: 1.0)&quot;,\n    )\n    return parser.parse_args()\n\ndef find_specificity(c_matrix):\n    specificity = c_matrix[1][1]\/(c_matrix[1][1]+c_matrix[0][1])\n    return specificity\n    \n    \ndef main():\n\n    args = parse_args()\n\n    df = pd.read_csv('~\/Desktop\/MLflow\/Churn_demo.csv')\n    train_df = df.sample(frac=0.8, random_state=25)\n    test_df = df.drop(train_df.index)\n\n\n        \n    train_df.drop(['subscriberid'], axis = 1, inplace = True)\n    test_df.drop(['subscriberid'], axis = 1, inplace = True)\n\n    TrainX = train_df.iloc[:,:-1]\n    TrainY = train_df.iloc[:,-1]\n\n    TestX = test_df.iloc[:,:-1]\n    TestY = test_df.iloc[:,-1]\n    \n    mlflow.lightgbm.autolog()\n    \n    dtrain = lgb.Dataset(TrainX, label=TrainY)\n    dtest = lgb.Dataset(TestX, label=TestY)\n    \n    with mlflow.start_run():\n\n        parameters = {\n            'objective': 'binary',\n            'device':'cpu',\n            'num_threads': 6,\n            'num_leaves': 127,\n            'metric' : 'binary',\n            'lambda_l2':5,\n            'max_bin': 63,\n            'bin_construct_sample_cnt' :2*1000*1000,\n            'learning_rate': args.learning_rate,\n            'colsample_bytree': args.colsample_bytree,\n            'subsample': args.subsample,\n            'verbose': 1\n        }\n\n\n\n        model = lgb.train(parameters,\n                       dtrain,\n                       valid_sets=dtest,\n                       num_boost_round=10000,\n                       early_stopping_rounds=10)\n                       \n               \n        y_proba=model.predict(TestX)\n        pred=np.where(y_proba&gt;0.25,1,0) \n        conf_matrix = confusion_matrix(TestY,pred)\n        \n        specificity = find_specificity(conf_matrix)\n        acc = accuracy_score(TestY,pred)\n        \n        mlflow.log_metric({&quot;specificity&quot; : specificity, &quot;accuracy&quot; : acc})\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n        \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633602279323,
        "Question_score":2,
        "Question_tags":"python|yaml|mlflow",
        "Question_view_count":418,
        "Owner_creation_time":1583491811220,
        "Owner_last_access_time":1663774319043,
        "Owner_location":"Baku, Azerbaijan",
        "Owner_reputation":23,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Fortunately, I have been resolved my problem. I list some solutions for the same error which can help you in the future if you face the same problem.<\/p>\n<ol>\n<li>File names. The file names should be the same suggested in MLFlow docs <code>https:\/\/mlflow.org\/ <\/code>. For example not <code>conda.yamp<\/code>, but <code>conda.yaml<\/code>, as there was such problem in <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/3856<\/code><\/li>\n<li>The <code>conda.yaml<\/code> file does not support Tab, please consider using spaces instead<\/li>\n<li>In the MLProject file name 'P' should be the upper case before MLFlow 1.4. But the later versions it does not matter as explained there <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code><\/li>\n<li>(In my case) MLProject file is space sensitive. Let the <code> https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples<\/code> GitHub examples guide you.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1633946464143,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69479488",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70539698,
        "Question_title":"MlFlow - Unable to run with S3 as default-artifact-root",
        "Question_body":"<p>I am trying to store my model artifacts using mlflow to s3. In the API services, we use <code>MLFLOW_S3_ENDPOINT_URL<\/code> as the s3 bucket. In the mlflow service, we pass it as an environment variable. But, the mlflow container servicer fails with the below exception:<\/p>\n<pre><code>mflow_server  | botocore.exceptions.HTTPClientError: An HTTP Client raised an unhandled exception: Not supported URL scheme s3\n<\/code><\/pre>\n<p>docker-compose file as below:<\/p>\n<pre><code>version: &quot;3.3&quot;\nservices:\n  prisim-api:\n    image: prisim-api:latest\n    container_name: prisim-api\n    expose:\n      - &quot;8000&quot;\n    environment: \n    - S3_URL=s3:\/\/mlflow-automation-artifacts\/\n    - MLFLOW_SERVER=http:\/\/mlflow:5000\n    - AWS_ID=xyz+\n    - AWS_KEY=xyz\n\n    networks:\n      - prisim \n    depends_on:\n      - mlflow\n    links:\n            - mlflow\n    volumes:\n      - app_data:\/usr\/data\n  mlflow:\n    image: mlflow_server:latest\n    container_name: mflow_server\n    ports:\n      - &quot;5000:5000&quot;    \n    environment:\n      - AWS_ACCESS_KEY_ID=xyz+\n      - AWS_SECRET_ACCESS_KEY=xyz\n      - MLFLOW_S3_ENDPOINT_URL=s3:\/\/mlflow-automation-artifacts\/\n    healthcheck:\n      test: [&quot;CMD&quot;, &quot;echo&quot;, &quot;mlflow server is running&quot;]\n      interval: 1m30s\n      timeout: 10s\n      retries: 3\n    networks:\n       - prisim \nnetworks:\n prisim:\nvolumes:\n  app_data:\n<\/code><\/pre>\n<p>Why the scheme s3 is not supported?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1640933189943,
        "Question_score":1,
        "Question_tags":"amazon-s3|docker-compose|mlflow",
        "Question_view_count":932,
        "Owner_creation_time":1310893185210,
        "Owner_last_access_time":1663988189020,
        "Owner_location":"Thiruvananthapuram, Kerala, India",
        "Owner_reputation":2763,
        "Owner_up_votes":373,
        "Owner_down_votes":7,
        "Owner_views":851,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I found the solution.<\/p>\n<p>I have added <code>[&quot;AWS_DEFAULT_REGION&quot;]<\/code> to the environment variables and it worked.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1641275216080,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70539698",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59856641,
        "Question_title":"How can I throw an exception from within an MLflow project?",
        "Question_body":"<p>I have an Mlflow project that raises an exception. I execute that function using <code>mlflow.run<\/code>, but I get <code>mlflow.exceptions.ExecutionException(\"Run (ID '&lt;run_id&gt;') failed\")<\/code>. <\/p>\n\n<p>Is there any way I could get the exception that is being raised where I am executing <code>mlflow.run<\/code>? <\/p>\n\n<p>Or is it possible to send an <code>mlflow.exceptions.ExecutionException<\/code> with custom message set from within the project?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579686060327,
        "Question_score":0,
        "Question_tags":"python|exception|mlflow",
        "Question_view_count":428,
        "Owner_creation_time":1472932425400,
        "Owner_last_access_time":1623748857057,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Unfortunately not at the moment. mlflow run starts a new process and there is no protocol for exception passing right now. In general the other project does not even have to be in the same language. <\/p>\n\n<p>One workaround I can think of is to pass the exception via mlflow by setting run tag. E.g.:<\/p>\n\n<pre><code>try:\n    ...\nexcept Exception as ex:\n    mlflow.set_tag(\"exception\", str(ex))\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1579719419647,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1579728648287,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59856641",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60616430,
        "Question_title":"MLflow: how to read metrics or params from an existing run?",
        "Question_body":"<p>I try to read metrics in this way:<\/p>\n\n<pre><code> data, info = mlflow.get_run(run_id)\n print(data[1].metrics)\n # example of output: {'loss': 0.01}\n<\/code><\/pre>\n\n<p>But it get only last value. It is possible to read manually all steps of a particular metric?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":2,
        "Question_creation_time":1583838649443,
        "Question_score":3,
        "Question_tags":"python|metrics|mlflow",
        "Question_view_count":2995,
        "Owner_creation_time":1547467399037,
        "Owner_last_access_time":1663856296190,
        "Owner_location":"Busto Arsizio, VA, Italia",
        "Owner_reputation":171,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Question_last_edit_time":1647258268603,
        "Answer_body":"<p>I ran into this same problem and was able to do get all of the values for the metric by using using <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_metric_history\" rel=\"noreferrer\"><code>mlflow.tracking.MlflowClient().get_metric_history<\/code><\/a>. This will return every value you logged using <code>mlflow.log_metric(key, value)<\/code>.<\/p>\n<p>Quick example (untested)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\ntrackingDir = 'file:\/\/\/....'\nregistryDir = 'file:\/\/\/...'\nrunID = 'my run id'\nmetricKey = 'loss'\n\nclient = mlflow.tracking.MlflowClient(\n            tracking_uri=trackingDir,\n            registry_uri=registryDir,\n        )\n\nmetrics = client.get_metric_history(runID, metricKey)\n<\/code><\/pre>\n<p><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_metric_history\" rel=\"noreferrer\">From the docs<\/a><\/p>\n<blockquote>\n<p>get_metric_history(run_id, key)[source] Return a list of metric\nobjects corresponding to all values logged for a given metric.<\/p>\n<p>Parameters run_id \u2013 Unique identifier for run<\/p>\n<p>key \u2013 Metric name within the run<\/p>\n<p>Returns A list of mlflow.entities.Metric entities if logged, else\nempty list<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from mlflow.tracking import MlflowClient\n\ndef print_metric_info(history):\n    for m in history:\n        print(&quot;name: {}&quot;.format(m.key))\n        print(&quot;value: {}&quot;.format(m.value))\n        print(&quot;step: {}&quot;.format(m.step))\n        print(&quot;timestamp: {}&quot;.format(m.timestamp))\n        print(&quot;--&quot;)\n\n# Create a run under the default experiment (whose id is &quot;0&quot;). Since this is low-level\n# CRUD operation, the method will create a run. To end the run, you'll have\n# to explicitly end it. \nclient = MlflowClient() \nexperiment_id = &quot;0&quot; \nrun = client.create_run(experiment_id) \nprint(&quot;run_id:{}&quot;.format(run.info.run_id))\nprint(&quot;--&quot;)\n\n# Log couple of metrics, update their initial value, and fetch each\n# logged metrics' history. \nfor k, v in [(&quot;m1&quot;, 1.5), (&quot;m2&quot;, 2.5)]:\n    client.log_metric(run.info.run_id, k, v, step=0)\n    client.log_metric(run.info.run_id, k, v + 1, step=1)\n    print_metric_info(client.get_metric_history(run.info.run_id, k))\nclient.set_terminated(run.info.run_id) \n<\/code><\/pre>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1606884707693,
        "Answer_score":5.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60616430",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59464404,
        "Question_title":"COPY files - next to Dockerfile - don't work and block docker build",
        "Question_body":"<p>I'm trying to building a docker container with mlflow server inside, with poetry toml file for dependency.(the two toml are exactly the same, it was just a way to try  to figure out)<br>\ntree:<\/p>\n\n<p>\u251c\u2500\u2500 docker-entrypoint.sh <br>\n\u251c\u2500\u2500 Dockerfile<br>\n\u251c\u2500\u2500 files<br>\n\u2502   \u2514\u2500\u2500 pyproject.toml<br>\n\u251c\u2500\u2500 git.sh<br>\n\u251c\u2500\u2500 pyproject.toml<br>\n\u2514\u2500\u2500 README.md<br><\/p>\n\n<p>as you can see, my toml file is next to Dockerfile <code>COPY pyproject.toml .\/<\/code> don't work nevertheless<\/p>\n\n<p><strong>Dockerfile<\/strong><\/p>\n\n<pre><code>FROM python:3.6.10-alpine3.10 as base\nLABEL maintainer=\"\"\n\nENV PYTHONFAULTHANDLER 1 \nENV    PYTHONHASHSEED random \nENV    PYTHONUNBUFFERED 1\n\nENV MLFLOW_HOME .\/ \nENV SERVER_PORT 5000   \nENV    MLFLOW_VERSION 0.7.0 \nENV    SERVER_HOST 0.0.0.0  \nENV    FILE_STORE ${MLFLOW_HOME}\/fileStore  \nENV    ARTIFACT_STORE ${MLFLOW_HOME}\/artifactStore \nENV PIP_DEFAULT_TIMEOUT 100\nENV    PIP_DISABLE_PIP_VERSION_CHECK on\nENV    PIP_NO_CACHE_DIR  off \nENV    POETRY_VERSION  1.0.0 \n\nWORKDIR ${MLFLOW_HOME}\n\nFROM base as builder\n\nRUN apk update  \\\n    &amp;&amp; apk add --no-cache make gcc musl-dev python3-dev libffi-dev openssl-dev subversion\n#download project file from github  repo \nRUN    svn export https:\/\/github.com\/MChrys\/QuickSign\/trunk\/  \\\n    &amp;&amp; pip install poetry==${POETRY_VERSION} \\\n    &amp;&amp; mkdir -p ${FILE_STORE}  \\\n    &amp;&amp; mkdir -p ${ARTIFACT_STORE}\\\n    &amp;&amp; python -m venv \/venv\n\nCOPY  pyproject.toml .\/\nRUN poetry export -f requirements.txt | \/venv\/bin\/pip install -r  --allow-root-install \/dev\/stdin \n\nCOPY . .\nRUN poetry build &amp;&amp; \/venv\/bin\/pip install dist\/*.whl\n\nFROM base as final\n\nRUN apk add --no-cache libffi libpq\nCOPY --from=builder \/venv \/venv\nCOPY docker-entrypoint.sh .\/\n\nEXPOSE $SERVER_PORT\n\nVOLUME [\"${FILE_STORE}\", \"${ARTIFACT_STORE}\"]\n\nCMD [\".\/docker-entrypoint.sh\"]\n<\/code><\/pre>\n\n<p>the build command :<\/p>\n\n<pre><code>docker build - &lt; Dockerfile\n<\/code><\/pre>\n\n<p>I get this error  :<\/p>\n\n<pre><code>Step 21\/32 : COPY  pyproject.toml .\/\nCOPY failed: stat \/var\/lib\/docker\/tmp\/docker-builder335195979\/pyproject.toml: no such file or   directory\n<\/code><\/pre>\n\n<p><strong>pyproject.toml<\/strong><\/p>\n\n<pre><code>requires = [\"poetry&gt;=1.0.0\", \"mlflow&gt;=0.7.0\", \"python&gt;=3.6\"]\nbuild-backend = \"poetry.masonry.api\"\n\n[tool.poetry]\nname = \"Sign\"\ndescription = \"\"\nversion = \"1.0.0\"\nreadme = \"README.md\"\nauthors = [\n  \"\"\n]\n\nlicense = \"MIT\"\n\n\n[tool.poetry.dependencies]\npython = \"3.6\"\nnumpy = \"1.14.3\"\nscipy = \"*\"\npandas = \"0.22.0\"\nscikit-learn = \"0.19.1\"\ncloudpickle = \"*\"\nmlflow =\"0.7.0\"\ntensorflow = \"^2.0.0\"\n\n\n[tool.poetry.dev-dependencies]\n\npylint = \"*\"\ndocker-compose = \"^1.25.0\"\ndocker-image-size-limit = \"^0.2.0\"\ntomlkit = \"^0.5.8\"\n\n<\/code><\/pre>\n\n<p><strong>docker-entrypoint.sh<\/strong><\/p>\n\n<pre><code>#!\/bin\/sh\n\nset -e\n\n. \/venv\/bin\/activate\n\nmlflow server \\\n    --file-store $FILE_STORE \\\n    --default-artifact-root $ARTIFACT_STORE \\\n    --host $SERVER_HOST \\\n    --port $SERVER_PORT\n<\/code><\/pre>\n\n<hr>\n\n<hr>\n\n<p>if i add <code>RUN pwd; ls<\/code> just befor the first <code>COPY<\/code> I obtain :<\/p>\n\n<pre><code>Step 20\/31 : RUN pwd; ls\n ---&gt; Running in e8ec36dd6ca8\n\/\nartifactStore\nbin\ndev\netc\nfileStore\nhome\nlib\nmedia\nmnt\nopt\nproc\nroot\nrun\nsbin\nsrv\nsys\ntmp\ntrunk\nusr\nvar\nvenv\nRemoving intermediate container e8ec36dd6ca8\n ---&gt; d7bba641bd7c\nStep 21\/31 : COPY  pyproject.toml .\/\nCOPY failed: stat \/var\/lib\/docker\/tmp\/docker-builder392824737\/pyproject.toml: no such file or directory\n\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1577166859160,
        "Question_score":2,
        "Question_tags":"docker|dockerfile|mlflow|python-poetry",
        "Question_view_count":940,
        "Owner_creation_time":1459625723203,
        "Owner_last_access_time":1648342504593,
        "Owner_location":"Grenoble, France",
        "Owner_reputation":56,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Question_last_edit_time":1577172041963,
        "Answer_body":"<p>Try \n<code>docker build -t test .<\/code><\/p>\n\n<p>instead of\n<code>docker build - &lt; Dockerfile<\/code><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1577175350593,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59464404",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65789715,
        "Question_title":"MLFlow sklearn autologging prints too many info messages in colab",
        "Question_body":"<p>I am trying mlflow sklearn auto logging, in colab, mlflow prints a lot of info messages and at times it crashes the browser. Attaching the pic of info logs<a href=\"https:\/\/i.stack.imgur.com\/RqvNM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RqvNM.png\" alt=\"mlflow info logs\" \/><\/a><\/p>\n<p>codes are in <a href=\"https:\/\/colab.research.google.com\/drive\/1wvHSgYk6boKW0AMPqIt-AByyFHSO26wm?usp=sharing\" rel=\"nofollow noreferrer\">this colab file<\/a><\/p>\n<p>Am not sure what am missing here, but the same code works fine without producing these info logs on my local computer.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611052310507,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":194,
        "Owner_creation_time":1608633925527,
        "Owner_last_access_time":1657433687843,
        "Owner_location":"Dubai - United Arab Emirates",
        "Owner_reputation":25,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This is a known issue with MLFlow package, in which a hotfix has been raised.<\/p>\n<p>See here: <a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/3978\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/pull\/3978<\/a><\/p>\n<p><strong>Description of fault<\/strong><\/p>\n<p>In MLflow 1.13.0 and 1.13.1, the following Python event logging message is emitted when a patched ML training function begins execution within a preexisting MLflow run.<\/p>\n<p>Unfortunately, for patched ML training routines that make child calls to other patched ML training routines (e.g. sklearn random forests that call fit() on a collection of sklearn DecisionTree instances), this event log is printed to stdout every time a child is called.<\/p>\n<p>This can produce hundreds of redundant event logging calls that don't provide value to the user.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1611053713910,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65789715",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65937623,
        "Question_title":"Unable to serve an mlflow model locally",
        "Question_body":"<p>I have created an mlflow model with custom pyfunc. It shows the results when I send input to the loaded model in Jupyter notebook.\nHowever if I am trying to serve it to a local port<\/p>\n<pre><code>!mlflow models serve -m Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001\n<\/code><\/pre>\n<p>I am getting this error<\/p>\n<pre><code> Traceback (most recent call last):\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/bin\/mlflow&quot;, line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 829, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 782, in main\n    rv = self.invoke(ctx)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 610, in invoke\n    return callback(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 56, in serve\n    install_mlflow=install_mlflow).serve(model_uri=model_uri, port=port,\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 163, in _get_flavor_backend\n    append_to_uri_path(underlying_model_uri, &quot;MLmodel&quot;), output_path=tmp.path())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/tracking\/artifact_utils.py&quot;, line 76, in _download_artifact_from_uri\n    artifact_path=artifact_path, dst_path=output_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 67, in download_artifacts\n    return super(LocalArtifactRepository, self).download_artifacts(artifact_path, dst_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 140, in download_artifacts\n    return download_file(artifact_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 105, in download_file\n    self._download_file(remote_file_path=fullpath, local_path=local_file_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 95, in _download_file\n    shutil.copyfile(remote_file_path, local_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/shutil.py&quot;, line 120, in copyfile\n    with open(src, 'rb') as fsrc:\nFileNotFoundError: [Errno 2] No such file or directory: 'Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model\/MLmodel'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611838887777,
        "Question_score":0,
        "Question_tags":"deployment|localhost|mlflow",
        "Question_view_count":1277,
        "Owner_creation_time":1573739890560,
        "Owner_last_access_time":1663922252563,
        "Owner_location":null,
        "Owner_reputation":115,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>From your error traceback, the model artifact can't be located. In your code, you are executing the 'mlflow' command from within a Jupyter Notebook. I would suggest trying the following:<\/p>\n<ol>\n<li>Check if your models artifacts are on the path you are using Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model<\/li>\n<li>Try opening a terminal, then <code>cd \/Home\/miniconda3\/envs<\/code> and  execute <code>mlflow models serve -m .\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001<\/code><\/li>\n<li>MLFlow offers different solutions to serve a model, you can try to register your model and refer to it as &quot;models:\/{model_name}\/{stage}&quot; as mentioned in the Model Registry <a href=\"https:\/\/mlflow.org\/docs\/latest\/model-registry.html#serving-an-mlflow-model-from-model-registry\" rel=\"nofollow noreferrer\">docs<\/a><\/li>\n<\/ol>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1611840603100,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65937623",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65627039,
        "Question_title":"MLflow stores tags but does not return them",
        "Question_body":"<p>I am running the below code to store tags and then to retrieve them. As you can see below, Mlflow is storing one set of tags and returning another.<\/p>\n<pre><code>import mlflow\nwith mlflow.start_run() as active_run:\n    tw = { &quot;run_id&quot;: 1}\n    mlflow.set_tags(tw)            \n    print(&quot;Tags are &quot;, active_run.data.tags)\n    print(type(active_run.data.tags))\n<\/code><\/pre>\n<p>Output<\/p>\n<pre><code>Tags are  {'mlflow.source.name': '\/media\/Space\/AI\/anaconda4\/lib\/python3.7\/site-packages\/ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'adeel'}\n<\/code><\/pre>\n<p>Looking at the stored tags through mlflow ui, I can see that the tag &quot;run_id&quot; set by the code is actually stored in the run. However, only the header information of the run seems to be getting returned by active_run.data.tags.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610100575263,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":173,
        "Owner_creation_time":1445990517173,
        "Owner_last_access_time":1663982428387,
        "Owner_location":"Sydney, New South Wales, Australia",
        "Owner_reputation":689,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":87,
        "Question_last_edit_time":1611139529420,
        "Answer_body":"<p>At the moment, you have to query your run again in MLflow to get the run with all the info that you logged. In the example below, I call <code>mlflow.get_run(&lt;run_id&gt;)<\/code> to achieve this.<\/p>\n<pre><code>import mlflow\n\n\nwith mlflow.start_run() as active_run:\n  tags = { &quot;my_tag&quot;: 1}\n  mlflow.set_tags(tags)            \n  # Keep track of the run ID of the active run\n  run_id = active_run.info.run_id\n\nrun = mlflow.get_run(run_id)\nprint(&quot;The tags are &quot;, run.data.tags)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1610128097450,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65627039",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73320708,
        "Question_title":"Set run description programmatically in mlflow",
        "Question_body":"<p>Similar to <a href=\"https:\/\/stackoverflow.com\/questions\/57199472\/is-it-possible-to-set-change-mlflow-run-name-after-run-initial-creation#:%7E:text=It%20is%20possible%20to%20edit,you%27d%20like%20to%20edit.&amp;text=There%27s%20currently%20no%20stable%20public,the%20tag%20with%20key%20mlflow.\">this question<\/a>, I'd like to edit\/set the description of a run via code, instead of editing it via UI.<\/p>\n<p>To clarify, I don't want to set the description of my entire experiment, only of a single run.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ogUgu.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ogUgu.png\" alt=\"Image showing what I want to edit\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660221143727,
        "Question_score":1,
        "Question_tags":"python|artificial-intelligence|mlflow",
        "Question_view_count":89,
        "Owner_creation_time":1527525798183,
        "Owner_last_access_time":1664059587093,
        "Owner_location":"Sarajevo, Bosnia and Herzegovina",
        "Owner_reputation":736,
        "Owner_up_votes":829,
        "Owner_down_votes":8,
        "Owner_views":57,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There are two ways to set the description.<\/p>\n<h3>1. <code>description<\/code> parameter<\/h3>\n<p>You can set a description using a markdown string for your run in <code>mlflow.start_run()<\/code> using <code>description<\/code> parameter. Here is an example.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if __name__ == &quot;__main__&quot;:\n    # load dataset and other stuff\n\n    run_description = &quot;&quot;&quot;\n### Header\nThis is a test **Bold**, *italic*, ~~strikethrough~~ text.\n[And this is an example hayperlink](http:\/\/example.com\/).\n    &quot;&quot;&quot;\n\n    with mlflow.start_run(description=run_description) as run:\n        # train model and other stuff\n<\/code><\/pre>\n<h3>2. <code>mlflow.note.content<\/code> tag<\/h3>\n<p>You can set\/edit run names by setting the tag with the key <code>mlflow.note.content<\/code>, which is what the UI (currently) does under the hood.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if __name__ == &quot;__main__&quot;:\n    # load dataset and other stuff\n\n    run_description = &quot;&quot;&quot;\n### Header\nThis is a test **Bold**, *italic*, ~~strikethrough~~ text.\n[And this is an example hayperlink](http:\/\/example.com\/).\n    &quot;&quot;&quot;\n\n    tags = {\n        'mlflow.note.content': run_description\n    }\n\n    with mlflow.start_run(tags=tags) as run:\n        # train model and other stuff\n<\/code><\/pre>\n<h3>Result<\/h3>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4zZa9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4zZa9.png\" alt=\"output of the given example\" \/><\/a><\/p>\n<hr \/>\n<p>If you set <code>description<\/code> parameter and <code>mlflow.note.content<\/code> tag in <code>mlflow.start_run()<\/code>, you'll get this error.<\/p>\n<pre><code>Description is already set via the tag mlflow.note.content in tags.\nRemove the key mlflow.note.content from the tags or omit the description.\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1660231264753,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1660307815687,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73320708",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73074887,
        "Question_title":"'mlflow' has no attribute 'last_active_run'",
        "Question_body":"<p>For the first time, it is proceeding mlflow with port 5000.<\/p>\n<p>Testing Mlflow, problem is no attribute last_active_run in mlflow<\/p>\n<p>But, It was an example provided by Mlflow. <br \/>\nlink is here <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/sklearn_autolog\" rel=\"nofollow noreferrer\">mlflow<\/a><\/p>\n<p>What is problem and how can I change code?<\/p>\n<p>shell<\/p>\n<pre><code>wget https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/master\/examples\/sklearn_autolog\/utils.py\nwget https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/master\/examples\/sklearn_autolog\/pipeline.py\n<\/code><\/pre>\n<p>pipeline.py<\/p>\n<pre><code>from pprint import pprint\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nimport mlflow\nfrom utils import fetch_logged_data\n\n\ndef main():\n    # enable autologging\n    mlflow.sklearn.autolog()\n\n    # prepare training data\n    X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n    y = np.dot(X, np.array([1, 2])) + 3\n\n    # train a model\n    pipe = Pipeline([(&quot;scaler&quot;, StandardScaler()), (&quot;lr&quot;, LinearRegression())])\n    pipe.fit(X, y)\n    run_id = mlflow.last_active_run().info.run_id\n    print(&quot;Logged data and model in run: {}&quot;.format(run_id))\n\n    # show logged data\n    for key, data in fetch_logged_data(run_id).items():\n        print(&quot;\\n---------- logged {} ----------&quot;.format(key))\n        pprint(data)\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n<\/code><\/pre>\n<p>utils.py<\/p>\n<pre><code>import mlflow\nfrom mlflow.tracking import MlflowClient\n\n\ndef yield_artifacts(run_id, path=None):\n    &quot;&quot;&quot;Yield all artifacts in the specified run&quot;&quot;&quot;\n    client = MlflowClient()\n    for item in client.list_artifacts(run_id, path):\n        if item.is_dir:\n            yield from yield_artifacts(run_id, item.path)\n        else:\n            yield item.path\n\n\ndef fetch_logged_data(run_id):\n    &quot;&quot;&quot;Fetch params, metrics, tags, and artifacts in the specified run&quot;&quot;&quot;\n    client = MlflowClient()\n    data = client.get_run(run_id).data\n    # Exclude system tags: https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#system-tags\n    tags = {k: v for k, v in data.tags.items() if not k.startswith(&quot;mlflow.&quot;)}\n    artifacts = list(yield_artifacts(run_id))\n    return {\n        &quot;params&quot;: data.params,\n        &quot;metrics&quot;: data.metrics,\n        &quot;tags&quot;: tags,\n        &quot;artifacts&quot;: artifacts,\n    }\n<\/code><\/pre>\n<p>Error message<\/p>\n<pre><code>INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '8cc3f4e03b4e417b95a64f1a9a41be63', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\nTraceback (most recent call last):\n  File &quot;\/Users\/taein\/Desktop\/mlflow\/pipeline.py&quot;, line 33, in &lt;module&gt;\n    main()\n  File &quot;\/Users\/taein\/Desktop\/mlflow\/pipeline.py&quot;, line 23, in main\n    run_id = mlflow.last_active_run().info.run_id\nAttributeError: module 'mlflow' has no attribute 'last_active_run'\n<\/code><\/pre>\n<p>Thanks for your helping<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1658461256963,
        "Question_score":1,
        "Question_tags":"python|python-3.x|scikit-learn|mlflow",
        "Question_view_count":215,
        "Owner_creation_time":1598180751547,
        "Owner_last_access_time":1663903316017,
        "Owner_location":"Seoul, Repulic of Korea",
        "Owner_reputation":158,
        "Owner_up_votes":4,
        "Owner_down_votes":2,
        "Owner_views":29,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It's because of the mlflow version that you mentioned in the comments. <code>mlflow.last_active_run()<\/code> API was introduced in <a href=\"https:\/\/github.com\/mlflow\/mlflow\/releases\/tag\/v1.25.0\" rel=\"nofollow noreferrer\">mlflow 1.25.0\n<\/a>. So you should upgrade the mlflow or you can use the previous version of the code available <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/5e2cb3baef544b00a972dff9dd6fb764be20510b\/examples\/sklearn_autolog\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<pre><code>wget https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/5e2cb3baef544b00a972dff9dd6fb764be20510b\/examples\/sklearn_autolog\/utils.py\nwget https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/5e2cb3baef544b00a972dff9dd6fb764be20510b\/examples\/sklearn_autolog\/pipeline.py\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1658706466923,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73074887",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56700687,
        "Question_title":"Installing dependencies from (Conda) environment.yml without Conda?",
        "Question_body":"<p>I currently use Conda to capture my dependencies for a python project in a <code>environment.yml<\/code>.<\/p>\n\n<p>When I build a docker service from the project I need to reinstall these dependencies. I would like to get around, having to add (mini-)conda to my docker image.<\/p>\n\n<p>Is it possible to parse <code>environment.yml<\/code> with pip\/pipenv or transform this into a corresponding <code>requirements.txt<\/code>?<\/p>\n\n<p>(I don't want to leave conda just yet, as this is what MLflow captures, when I log models)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1561109630463,
        "Question_score":1,
        "Question_tags":"python|docker|pip|conda|mlflow",
        "Question_view_count":1601,
        "Owner_creation_time":1336936830510,
        "Owner_last_access_time":1663079651823,
        "Owner_location":null,
        "Owner_reputation":948,
        "Owner_up_votes":592,
        "Owner_down_votes":1,
        "Owner_views":132,
        "Question_last_edit_time":1561111068257,
        "Answer_body":"<p>Nope.<\/p>\n\n<ol>\n<li><p><code>conda<\/code> automatically installs dependencies of conda packages. These are resolved differently by <code>pip<\/code>, so you'd have to resolve the Anaconda dependency tree in your transformation script.<\/p><\/li>\n<li><p>Many <code>conda<\/code> packages are non-Python. You couldn't install those dependencies with <code>pip<\/code> at all.<\/p><\/li>\n<li><p>Some <code>conda<\/code> packages contain binaries that were compiled with the Anaconda compiler toolchain. Even if the corresponding <code>pip<\/code> package can compile such binaries on installation, it wouldn't be using the Anaconda toolchain. What you'd get would be fundamentally different from the corresponding <code>conda<\/code> package.<\/p><\/li>\n<li><p>Some <code>conda<\/code> packages have fixes applied, which are missing from corresponding <code>pip<\/code> packages.<\/p><\/li>\n<\/ol>\n\n<p>I hope this is enough to convince you that your idea won't fly.<\/p>\n\n<p>Installing Miniconda isn't really a big deal. Just do it :-)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1561110403227,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56700687",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69944447,
        "Question_title":"How to change the directory of mlflow logs?",
        "Question_body":"<p>I am using MLflow to log the metrics but I want to change the default saving logs directory. So, instead of writing log files besides my main file, I want to store them to <code>\/path\/outputs\/lg <\/code>. I don't know how to change it. I use it without in the <code>Model<\/code>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nfrom time import time\n\nimport mlflow\nimport numpy as np\nimport torch\nimport tqdm\n\n# from segmentation_models_pytorch.utils import metrics\nfrom AICore.emergency_landing.metrics import IoU, F1\nfrom AICore.emergency_landing.utils import AverageMeter\nfrom AICore.emergency_landing.utils import TBLogger\n\n\nclass Model:\n    def __init__(self, model, num_classes=5, ignore_index=0, optimizer=None, scheduler=None, criterion=None,\n                 device=None, epochs=30, train_loader=None, val_loader=None, tb_logger: TBLogger = None,\n                 logger=None,\n                 best_model_path=None,\n                 model_check_point_path=None,\n                 load_from_best_model=None,\n                 load_from_model_checkpoint=None,\n                 early_stopping=None,\n                 debug=False):\n\n        self.debug = debug\n\n        self.early_stopping = {\n            'init': early_stopping,\n            'changed': 0\n        }\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.criterion = criterion\n        self.device = device\n        self.epochs = epochs\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n        self.model = model.to(device)\n\n        self.tb_logger = tb_logger\n        self.logger = logger\n\n        self.best_loss = np.Inf\n\n        if not os.path.exists(best_model_path):\n            os.makedirs(best_model_path)\n        self.best_model_path = best_model_path\n\n        if not os.path.exists(model_check_point_path):\n            os.makedirs(model_check_point_path)\n        self.model_check_point_path = model_check_point_path\n\n        self.load_from_best_model = load_from_best_model\n        self.load_from_model_checkpoint = load_from_model_checkpoint\n\n        if self.load_from_best_model is not None:\n            self.load_model(path=self.load_from_best_model)\n        if self.load_from_model_checkpoint is not None:\n            self.load_model_checkpoint(path=self.load_from_model_checkpoint)\n\n        self.train_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n        self.val_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n        self.test_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n\n        self.train_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n        self.val_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n        self.test_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n\n    def metrics(self, is_train=True):\n        if is_train:\n            train_losses = AverageMeter('Training Loss', ':.4e')\n            train_iou = AverageMeter('Training iou', ':6.2f')\n            train_f_score = AverageMeter('Training F_score', ':6.2f')\n\n            return train_losses, train_iou, train_f_score\n        else:\n            val_losses = AverageMeter('Validation Loss', ':.4e')\n            val_iou = AverageMeter('Validation mean iou', ':6.2f')\n            val_f_score = AverageMeter('Validation F_score', ':6.2f')\n\n            return val_losses, val_iou, val_f_score\n\n    def fit(self):\n\n        self.logger.info(&quot;\\nStart training\\n\\n&quot;)\n        start_training_time = time()\n\n        with mlflow.start_run():\n            for e in range(self.epochs):\n                start_training_epoch_time = time()\n                self.model.train()\n                train_losses_avg, train_iou_avg, train_f_score_avg = self.metrics(is_train=True)\n                with tqdm.tqdm(self.train_loader, unit=&quot;batch&quot;) as tepoch:\n                    tepoch.set_description(f&quot;Epoch {e}&quot;)\n                    for image, target in tepoch:\n                        # Transfer Data to GPU if available\n                        image = image.to(self.device)\n                        target = target.to(self.device)\n                        # Clear the gradients\n                        self.optimizer.zero_grad()\n                        # Forward Pass\n                        # out = self.model(image)['out']\n                        # if unet == true =&gt; remove ['out']\n                        out = self.model(image)\n                        # Find the Loss\n                        loss = self.criterion(out, target)\n                        # Calculate Loss\n                        train_losses_avg.update(loss.item(), image.size(0))\n                        # Calculate gradients\n                        loss.backward()\n                        # Update Weights\n                        self.optimizer.step()\n\n                        iou = self.train_iou(out.cpu(), target.cpu()).item()\n                        train_iou_avg.update(iou)\n\n                        f1_score = self.train_f1(out.cpu(), target.cpu()).item()\n                        train_f_score_avg.update(f1_score)\n\n                        tepoch.set_postfix(loss=train_losses_avg.avg,\n                                           iou=train_iou_avg.avg,\n                                           f_score=train_f_score_avg.avg)\n                        if self.debug:\n                            break\n\n                self.tb_logger.log(log_type='criterion\/training', value=train_losses_avg.avg, epoch=e)\n                self.tb_logger.log(log_type='iou\/training', value=train_iou_avg.avg, epoch=e)\n                self.tb_logger.log(log_type='f_score\/training', value=train_f_score_avg.avg, epoch=e)\n\n                mlflow.log_metric('criterion\/training', train_losses_avg.avg, step=e)\n                mlflow.log_metric('iou\/training', train_iou_avg.avg, step=e)\n                mlflow.log_metric('f_score\/training', train_f_score_avg.avg, step=e)\n\n                end_training_epoch_time = time() - start_training_epoch_time\n                print('\\n')\n                self.logger.info(\n                    f'Training Results - [{end_training_epoch_time:.3f}s] Epoch: {e}:'\n                    f' f_score: {train_f_score_avg.avg:.3f},'\n                    f' IoU: {train_iou_avg.avg:.3f},'\n                    f' Loss: {train_losses_avg.avg:.3f}')\n\n                # validation step\n                val_loss = self.evaluation(e)\n                # apply scheduler\n                if self.scheduler:\n                    self.scheduler.step()\n                # early stopping\n                if self.early_stopping['init'] &gt;= self.early_stopping['changed']:\n                    self._early_stopping_model(val_loss=val_loss)\n                else:\n                    print(f'The model can not learn more, Early Stopping at epoch[{e}]')\n                    break\n\n                # save best model\n                if self.best_model_path is not None:\n                    self._best_model(val_loss=val_loss, path=self.best_model_path)\n\n                # model check points\n                if self.model_check_point_path is not None:\n                    self.save_model_check_points(path=self.model_check_point_path, epoch=e, net=self.model,\n                                                 optimizer=self.optimizer, loss=self.criterion,\n                                                 avg_loss=train_losses_avg.avg)\n\n                # log mlflow\n                if self.scheduler:\n                    mlflow.log_param(&quot;get_last_lr&quot;, self.scheduler.get_last_lr())\n                    mlflow.log_param(&quot;scheduler&quot;, self.scheduler.state_dict())\n\n                self.tb_logger.flush()\n                if self.debug:\n                    break\n\n            end_training_time = time() - start_training_time\n            print(f'Finished Training after {end_training_time:.3f}s')\n            self.tb_logger.close()\n\n    def evaluation(self, epoch):\n        print('Validating...')\n        start_validation_epoch_time = time()\n        self.model.eval()  # Optional when not using Model Specific layer\n        with torch.no_grad():\n            val_losses_avg, val_iou_avg, val_f_score_avg = self.metrics(is_train=False)\n            with tqdm.tqdm(self.val_loader, unit=&quot;batch&quot;) as tepoch:\n                for image, target in tepoch:\n                    # Transfer Data to GPU if available\n                    image = image.to(self.device)\n                    target = target.to(self.device)\n                    # out = self.model(image)['out']\n                    # if unet == true =&gt; remove ['out']\n                    out = self.model(image)\n                    # Find the Loss\n                    loss = self.criterion(out, target)\n                    # Calculate Loss\n                    val_losses_avg.update(loss.item(), image.size(0))\n\n                    iou = self.val_iou(out.cpu(), target.cpu()).item()\n                    val_iou_avg.update(iou)\n\n                    f1_score = self.val_f1(out.cpu(), target.cpu()).item()\n                    val_f_score_avg.update(f1_score)\n\n                    tepoch.set_postfix(loss=val_losses_avg.avg,\n                                       iou=val_iou_avg.avg,\n                                       f_score=val_f_score_avg.avg)\n                    if self.debug:\n                        break\n            print('\\n')\n            self.tb_logger.log(log_type='criterion\/validation', value=val_losses_avg.avg, epoch=epoch)\n            self.tb_logger.log(log_type='iou\/validation', value=val_iou_avg.avg, epoch=epoch)\n            self.tb_logger.log(log_type='f_score\/validation', value=val_f_score_avg.avg, epoch=epoch)\n\n            mlflow.log_metric('criterion\/validation', val_losses_avg.avg, step=epoch)\n            mlflow.log_metric('iou\/validation', val_iou_avg.avg, step=epoch)\n            mlflow.log_metric('f_score\/validation', val_f_score_avg.avg, step=epoch)\n\n            end_validation_epoch_time = time() - start_validation_epoch_time\n            self.logger.info(\n                f'validation Results - [{end_validation_epoch_time:.3f}s] Epoch: {epoch}:'\n                f' f_score: {val_f_score_avg.avg:.3f},'\n                f' IoU: {val_iou_avg.avg:.3f},'\n                f' Loss: {val_losses_avg.avg:.3f}')\n            print('\\n')\n            return val_losses_avg.avg\n\n    def _save_model(self, name, path, params):\n        torch.save(params, path)\n\n    def _early_stopping_model(self, val_loss):\n        if self.best_loss &lt; val_loss:\n            self.early_stopping['changed'] += 1\n        else:\n            self.early_stopping['changed'] = 0\n\n    def _best_model(self, val_loss, path):\n        if self.best_loss &gt; val_loss:\n            self.best_loss = val_loss\n            name = f'\/best_model_loss_{self.best_loss:.2f}'.replace('.', '_')\n            self._save_model(name, path=f'{path}\/{name}.pt', params={\n                'model_state_dict': self.model.state_dict(),\n            })\n\n            print(f'The best model is saved with criterion: {self.best_loss:.2f}')\n\n    def save_model_check_points(self, path, epoch, net, optimizer, loss, avg_loss):\n        name = f'\/model_epoch_{epoch}_loss_{avg_loss:.2f}'.replace('.', '_')\n        self._save_model(name, path=f'{path}\/{name}.pt', params={\n            'epoch': epoch,\n            'model_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'criterion': loss,\n        })\n        print(f'model checkpoint is saved at model_epoch_{epoch}_loss_{avg_loss:.2f}')\n\n    def load_model_checkpoint(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        epoch = checkpoint['epoch']\n        self.criterion = checkpoint['criterion']\n\n        return epoch\n\n    def load_model(self, path):\n        best_model = torch.load(path)\n        self.model.load_state_dict(best_model['model_state_dict'])\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1636727119830,
        "Question_score":0,
        "Question_tags":"machine-learning|deep-learning|pytorch|mlflow",
        "Question_view_count":436,
        "Owner_creation_time":1410333105327,
        "Owner_last_access_time":1662489968593,
        "Owner_location":"Turin, Metropolitan City of Turin, Italy",
        "Owner_reputation":477,
        "Owner_up_votes":133,
        "Owner_down_votes":0,
        "Owner_views":130,
        "Question_last_edit_time":1636755379243,
        "Answer_body":"<p>The solution is:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.set_tracking_uri(uri=f'file:\/\/{hydra.utils.to_absolute_path(&quot;..\/output\/mlruns&quot;)}')\nexp = mlflow.get_experiment_by_name(name='Emegency_landing')\nif not exp:\n    experiment_id = mlflow.create_experiment(name='Emegency_landing',\n                                                 artifact_location=f'file:\/\/{hydra.utils.to_absolute_path(&quot;..\/output\/mlruns&quot;)}')\nelse:\n    experiment_id = exp.experiment_id\n<\/code><\/pre>\n<p>And then you should pass the experiment Id to:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with mlflow.start_run(experiment_id=experiment_id):\n     pass \n<\/code><\/pre>\n<p>If you don't mention the <code>\/path\/mlruns<\/code>, when you run the command of <code>mlflow ui<\/code>, it will create another folder automatically named <code>mlruns<\/code>. so, pay attention to this point to have the same name as <code>mlruns<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1636732235590,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1636755540677,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69944447",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":66792575,
        "Question_title":"Log text in mlflow",
        "Question_body":"<p>I know that you can log metrics as your experiment progresses. For example the training loss over epochs for your DL model.<\/p>\n<p>I was wondering if it was possible to do something similar for text. In my particular case I have a text model that generates some example text after each epoch and I wish to see what it's like. For example:<\/p>\n<pre><code>Epoch 1:\ntHi is RubisH\nEpoch 2:\nOk look slight better\nEpoch 3:\nI can speak English better than William Shakespeare\n<\/code><\/pre>\n<p>The workaround I can think of is to log this to a text file and push that as an artifact in mlflow. Was wondering if there was something else more native to mlflow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616642359947,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":1295,
        "Owner_creation_time":1372398800110,
        "Owner_last_access_time":1664007625063,
        "Owner_location":"Sydney NSW, Australia",
        "Owner_reputation":9065,
        "Owner_up_votes":1204,
        "Owner_down_votes":0,
        "Owner_views":952,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_param\" rel=\"nofollow noreferrer\">log_param\/log_params<\/a> for that. For long texts maybe it's better to use <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_text\" rel=\"nofollow noreferrer\">log_text<\/a> instead...<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1616671469503,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66792575",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56851463,
        "Question_title":"How do I specify mlflow MLproject with zero parameters?",
        "Question_body":"<p>I tried to create MLproject with zero parameters as:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    parameters:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>when I get an error:<\/p>\n\n<pre><code>  Traceback (most recent call last):\n File \"\/home\/ubuntu\/.local\/bin\/mlflow\", line 11, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/cli.py\", line 137, in run\n    run_id=run_id,\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 230, in run\n    use_conda=use_conda, storage_dir=storage_dir, synchronous=synchronous, run_id=run_id)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 85, in _run\n    project = _project_spec.load_project(work_dir)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 40, in load_project\n    entry_points[name] = EntryPoint(name, parameters, command)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 87, in __init__\n    self.parameters = {k: Parameter(k, v) for (k, v) in parameters.items()}\nAttributeError: 'NoneType' object has no attribute 'items'\n<\/code><\/pre>\n\n<p>Am I missing something or mlflow does not allow project with  zero parameters?<\/p>\n\n<p>I have also posted this at my public repo of: <a href=\"https:\/\/github.com\/sameermahajan\/mlflow-try\" rel=\"nofollow noreferrer\">https:\/\/github.com\/sameermahajan\/mlflow-try<\/a> if someone would like to try out:<\/p>\n\n<pre><code>mlflow run https:\/\/github.com\/sameermahajan\/mlflow-try.git\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1562066976100,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":353,
        "Owner_creation_time":1384343462317,
        "Owner_last_access_time":1663916912330,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":478,
        "Owner_up_votes":65,
        "Owner_down_votes":4,
        "Owner_views":118,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For this, you completely drop the 'parameters' section as below:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>(I thought I had tried it earlier but I was trying too many different ways to may be miss out on this one)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1562240543613,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56851463",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67062145,
        "Question_title":"Continue stopped run in MLflow",
        "Question_body":"<p>We run our experiment on AWS spot instances. Sometimes the experiments are stopped, and we would prefer to continue logging to the same run. How can you set the run-id of the active run?<\/p>\n<p>Something like this pseudocode (not working):<\/p>\n<pre><code>if new:\n    mlflow.start_run(experiment_id=1, run_name=x)\nelse:\n    mlflow.set_run(run_id)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618244956103,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":131,
        "Owner_creation_time":1484838464573,
        "Owner_last_access_time":1663858511743,
        "Owner_location":"Amsterdam, Nederland",
        "Owner_reputation":3937,
        "Owner_up_votes":672,
        "Owner_down_votes":27,
        "Owner_views":387,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can pass the run_id directly to <code>start_run<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.start_run(experiment_id=1,\n                 run_name=x,\n                 run_id=&lt;run_id_of_interrupted_run&gt; # pass None to start a new run\n                 ) \n<\/code><\/pre>\n<p>Of course, you have to store the run_id for this. You can get it with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.entities.html#mlflow.entities.RunInfo.run_id\" rel=\"nofollow noreferrer\"><code>run.info.run_id<\/code><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1631884865500,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67062145",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59773167,
        "Question_title":"When will experiment be deleted with lifecycle_stage is set as deleted",
        "Question_body":"<p>I can see experiment 2 is in deleted, but when it will be deleted actually?<\/p>\n\n<pre><code>2   test    hdfs:\/\/\/1234\/mlflow deleted\n<\/code><\/pre>\n\n<p>If the experiment is not deleted automatically, how can I delete it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579189296547,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":412,
        "Owner_creation_time":1408370821673,
        "Owner_last_access_time":1663216838707,
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":2521,
        "Owner_up_votes":447,
        "Owner_down_votes":13,
        "Owner_views":197,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I am assuming you use sql store?<\/p>\n\n<p>Currently there is no way to tell mlflow to hard-delete experiments. We are working with open source contributors to add a cli command that would perform garbage-collection of deleted experiments. This should be added soon in one of the upcoming mlflow releases. In the meantime, you can connect to your sql store and delete the experiments manually.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1579736829917,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59773167",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69703225,
        "Question_title":"Can I change the port of my MLflow tracking server?",
        "Question_body":"<p>I would like to know if I can change the port of my MLflow server.<\/p>\n<p>By default it is running on port 5000, but my company's VPN only allows HTTP (port 80) and HTTPS (port 443) traffic.<\/p>\n<p>This might be a very beginner's question, but is it possible, and if yes, is there any problem on running the MLflow server on port 83 (HTTP) ?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635139632427,
        "Question_score":1,
        "Question_tags":"http|port|mlflow",
        "Question_view_count":540,
        "Owner_creation_time":1561106497313,
        "Owner_last_access_time":1661059074150,
        "Owner_location":null,
        "Owner_reputation":133,
        "Owner_up_votes":28,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes, you can do that by passing the <code>-p port_number<\/code> command-line switch when starting MLflow server (see <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#cmdoption-mlflow-server-p\" rel=\"nofollow noreferrer\">docs<\/a>). Please note, that to be able to use ports below 1024, the server needs to be run as root.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1635152056290,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69703225",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70005957,
        "Question_title":"Logging the git_sha as a parameter on Mlflow using Kedro hooks",
        "Question_body":"<p>I would like to log the git_sha parameter on Mlflow as shown in the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/02_hooks.html?highlight=run_params#add-metrics-tracking-to-your-model\" rel=\"nofollow noreferrer\">documentation<\/a>. What appears to me here, is that simply running the following portion of code should be enough to get git_sha logged in the Mlflow UI. Am I right ?<\/p>\n<pre><code>@hook_impl\n    def before_pipeline_run(self, run_params: Dict[str, Any]) -&gt; None:\n        &quot;&quot;&quot;Hook implementation to start an MLflow run\n        with the same run_id as the Kedro pipeline run.\n        &quot;&quot;&quot;\n        mlflow.start_run(run_name=run_params[&quot;run_id&quot;])\n        mlflow.log_params(run_params)\n<\/code><\/pre>\n<p>But this does not work as I get all but the git_sha parameter. And when I look at the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/_modules\/kedro\/framework\/hooks\/specs.html?highlight=run_params#\" rel=\"nofollow noreferrer\">hooks specs<\/a>, it seems that this param is not part of run_params (anymore?)<\/p>\n<p>Is there a way I could get the git sha (maybe from the context journal ?) and add it to the logged parameters ?<\/p>\n<p>Thank you in advance !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637158421443,
        "Question_score":0,
        "Question_tags":"python|mlflow|kedro|mlops",
        "Question_view_count":172,
        "Owner_creation_time":1586517832390,
        "Owner_last_access_time":1660328393330,
        "Owner_location":null,
        "Owner_reputation":127,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":1637158766987,
        "Answer_body":"<p>Whilst it's heavily encouraged to use git with Kedro it's not required and as such no part of Kedro (except <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro-starters\" rel=\"nofollow noreferrer\">kedro-starters<\/a> if we're being pedantic) is 'aware' of git.<\/p>\n<p>In your <code>before_pipeline_hook<\/code> there it is pretty easy for you to retrieve the info <a href=\"https:\/\/stackoverflow.com\/questions\/14989858\/get-the-current-git-hash-in-a-python-script\">via the techniques documented here<\/a>. It seems trivial for the whole codebase, a bit more involved if you want to say provide pipeline specific hashes.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1637159193837,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70005957",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73621446,
        "Question_title":"Log Any Type of Model in MLflow",
        "Question_body":"<p>I am trying to create a wrapper function that allows my Data Scientists to log their models in MLflow.<\/p>\n<p>This is what the function looks like,<\/p>\n<pre><code>def log_model(self, params, metrics, model, run_name, artifact_path, artifacts=None):\n\n    with mlflow.start_run(run_name=run_name):\n        run_id = mlflow.active_run().info.run_id\n        mlflow.log_params(params)\n        mlflow.log_metrics(metrics)\n\n        if model:\n            mlflow.lightgbm.log_model(model, artifact_path=artifact_path)\n\n        if artifacts:\n            for artifact in artifacts:\n                mlflow.log_artifact(artifact, artifact_path=artifact_path)\n\n    return run_id\n<\/code><\/pre>\n<p>It can be seen here that the model is being logged as a <code>lightgbm<\/code> model, however, the <code>model<\/code> parameter that is passed into this function can be of any type.<\/p>\n<p>How can I update this function, so that it will be able to log any kind of model?<\/p>\n<p>As far as I know, there is no <code>log_model<\/code> function that comes with <code>mlflow<\/code>. It's always <code>mlflow.&lt;model_type&gt;.log_model<\/code>.<\/p>\n<p>How can I go about handling this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662464447387,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":39,
        "Owner_creation_time":1521856385820,
        "Owner_last_access_time":1664037995903,
        "Owner_location":"Sri Lanka",
        "Owner_reputation":820,
        "Owner_up_votes":389,
        "Owner_down_votes":1,
        "Owner_views":165,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I was able to solve this using the following approach,<\/p>\n<pre><code>def log_model(model, artifact_path):\n    model_class = get_model_class(model).split('.')[0]\n\n    try:\n        log_model = getattr(mlflow, model_class).log_model\n        log_model(model, artifact_path)\n    except AttributeError:\n        logger.info('The log_model function is not available as expected!')\n\ndef get_model_class(model):\n    klass = model.__class__\n    module = klass.__module__\n\n    if module == 'builtins':\n        return klass.__qualname__\n    return module + '.' + klass.__qualname__\n<\/code><\/pre>\n<p>From what I have seen, this will be able to handle most cases. The <code>get_model_class()<\/code> method will return the class used to develop the model and based on this, we can use the <code>getattr()<\/code> method to extract the relevant <code>log_model()<\/code> method.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1663636318523,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73621446",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67953241,
        "Question_title":"mlflow run git-uri clone to specific directory",
        "Question_body":"<p>I am using mlflow run with a GitHub uri.<\/p>\n<p>When I run using the below command<\/p>\n<pre><code>mlflow run &lt;git-uri&gt;\n<\/code><\/pre>\n<p>The command sets up a conda environment and then <em>clones the Git repo into a <strong>temp<\/strong> directory, But I need it setup in a <strong>specific<\/strong> directory<\/em><\/p>\n<p>I checked the entire document, but I can't find it. Is there no such option to do so in one shot?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1623534343453,
        "Question_score":1,
        "Question_tags":"python-3.x|mlflow",
        "Question_view_count":239,
        "Owner_creation_time":1575348765723,
        "Owner_last_access_time":1663079223380,
        "Owner_location":"Chennai, Tamil Nadu, India",
        "Owner_reputation":1049,
        "Owner_up_votes":55,
        "Owner_down_votes":68,
        "Owner_views":192,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For non-local URIs, MLflow uses the Python's <code>tempfile.mkdtemp<\/code> function (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/1c43176cefb5531fbb243975b9c8c5bfb9775e66\/mlflow\/projects\/utils.py#L140\" rel=\"nofollow noreferrer\">source code<\/a>), that creates the temporary directory.  You may have some control over it by setting the <code>TMPDIR<\/code> environment variable as described in <a href=\"https:\/\/docs.python.org\/3\/library\/tempfile.html#tempfile.mkstemp\" rel=\"nofollow noreferrer\">Python docs<\/a> (it lists <code>TMP<\/code> &amp; <code>TEMP<\/code> as well, but they didn't work for me on MacOS) - but it will set only &quot;base path&quot; for temporary directories and files, the directory\/file names are still will be random.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1623570743820,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1623614468897,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67953241",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71687131,
        "Question_title":"How to import MLflow tracking server WSGI application via Flask or FastAPI?",
        "Question_body":"<p>MLflow provides a very cool tracking server, however, this server does not provide authentication or RBAC which is required for my needs.<\/p>\n<p>I would like to add my own authentication and RBAC functionality. I think one way to accomplish this is to import the MLflow WSGI application object and add some middleware layers to perform authentication \/ authorization before passing requests through to the tracking server, essentially proxying requests through my custom middleware stack.<\/p>\n<p>How do I go about doing this? I can see from <a href=\"https:\/\/fastapi.tiangolo.com\/advanced\/wsgi\/\" rel=\"nofollow noreferrer\">these docs<\/a> that I can use FastAPI to import another WSGI application and add custom middleware, but I'm not sure of a few things<\/p>\n<ol>\n<li>Where do I find the MLflow tracking server WSGI app (where can it be imported from)?<\/li>\n<li>How do I pass through the relevant arguments to the MLflow tracking server? I.e. the tracking server expects params to configure the backend storage layer, host, and port. If I just import the application object, how do I pass those parameters to it?<\/li>\n<\/ol>\n<p>edit - it looks like the Flask application can be found here <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/server\/__init__.py#L28\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/server\/__init__.py#L28<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648703157780,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":394,
        "Owner_creation_time":1425965839877,
        "Owner_last_access_time":1663961190290,
        "Owner_location":"Santa Cruz, CA",
        "Owner_reputation":3256,
        "Owner_up_votes":81,
        "Owner_down_votes":0,
        "Owner_views":164,
        "Question_last_edit_time":1648705670280,
        "Answer_body":"<p>This was actually very simple, below is an example using FastAPI to import and mount the MLflow WSGI application.<\/p>\n<pre><code>import os\nimport subprocess\nfrom fastapi import FastAPI\nfrom fastapi.middleware.wsgi import WSGIMiddleware\n\nfrom mlflow.server import app as mlflow_app\n\napp = FastAPI()\napp.mount(&quot;\/&quot;, WSGIMiddleware(mlflow_app))\n\nBACKEND_STORE_URI_ENV_VAR = &quot;_MLFLOW_SERVER_FILE_STORE&quot;\nARTIFACT_ROOT_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACT_ROOT&quot;\nARTIFACTS_DESTINATION_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACT_DESTINATION&quot;\nPROMETHEUS_EXPORTER_ENV_VAR = &quot;prometheus_multiproc_dir&quot;\nSERVE_ARTIFACTS_ENV_VAR = &quot;_MLFLOW_SERVER_SERVE_ARTIFACTS&quot;\nARTIFACTS_ONLY_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACTS_ONLY&quot;\n\ndef parse_args():\n    a = argparse.ArgumentParser()\n    a.add_argument(&quot;--host&quot;, type=str, default=&quot;0.0.0.0&quot;)\n    a.add_argument(&quot;--port&quot;, type=str, default=&quot;5000&quot;)\n    a.add_argument(&quot;--backend-store-uri&quot;, type=str, default=&quot;sqlite:\/\/\/mlflow.db&quot;)\n    a.add_argument(&quot;--serve-artifacts&quot;, action=&quot;store_true&quot;, default=False)\n    a.add_argument(&quot;--artifacts-destination&quot;, type=str)\n    a.add_argument(&quot;--default-artifact-root&quot;, type=str)\n    a.add_argument(&quot;--gunicorn-opts&quot;, type=str, default=&quot;&quot;)\n    a.add_argument(&quot;--n-workers&quot;, type=str, default=1)\n    return a.parse_args()\n\ndef run_command(cmd, env, cwd=None):\n    cmd_env = os.environ.copy()\n    if cmd_env:\n        cmd_env.update(env)\n    child = subprocess.Popen(\n        cmd, env=cmd_env, cwd=cwd, text=True, stdin=subprocess.PIPE\n    )\n    child.communicate()\n    exit_code = child.wait()\n    if exit_code != 0:\n        raise Exception(&quot;Non-zero exitcode: %s&quot; % (exit_code))\n    return exit_code\n\ndef run_server(args):\n    env_map = dict()\n    if args.backend_store_uri:\n        env_map[BACKEND_STORE_URI_ENV_VAR] = args.backend_store_uri\n    if args.serve_artifacts:\n        env_map[SERVE_ARTIFACTS_ENV_VAR] = &quot;true&quot;\n    if args.artifacts_destination:\n        env_map[ARTIFACTS_DESTINATION_ENV_VAR] = args.artifacts_destination\n    if args.default_artifact_root:\n        env_map[ARTIFACT_ROOT_ENV_VAR] = args.default_artifact_root\n\n    print(f&quot;Envmap: {env_map}&quot;)\n\n    #opts = args.gunicorn_opts.split(&quot; &quot;) if args.gunicorn_opts else []\n    opts = args.gunicorn_opts if args.gunicorn_opts else &quot;&quot;\n\n    cmd = [\n        &quot;gunicorn&quot;, &quot;-b&quot;, f&quot;{args.host}:{args.port}&quot;, &quot;-w&quot;, f&quot;{args.n_workers}&quot;, &quot;-k&quot;, &quot;uvicorn.workers.UvicornWorker&quot;, &quot;server:app&quot;\n    ]\n    run_command(cmd, env_map)\n\ndef main():\n    args = parse_args()\n    run_server(args)\n\nif __name__ == &quot;__main__&quot;:\n    main()\n<\/code><\/pre>\n<p>Run like<\/p>\n<pre><code>python server.py --artifacts-destination s3:\/\/mlflow-mr --default-artifact-root s3:\/\/mlflow-mr --serve-artifacts\n<\/code><\/pre>\n<p>Then navigate to your browser and see the tracking server running! This allows you to insert custom FastAPI middleware in front of the tracking server<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1648708838090,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71687131",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72454747,
        "Question_title":"Problem when loading a xgboost model from mlflow registry",
        "Question_body":"<p>I create a xgboost classifier:<\/p>\n<pre><code>   xg_reg = xgb.XGBClassifier(objective ='reg:squarederror',  learning_rate = 0.1,\n                max_depth = 20, alpha = 10, n_estimators = 50, use_label_encoder=False)\n<\/code><\/pre>\n<p>After training the model, I am logging it to the MLFLow registry:<\/p>\n<pre><code>   mlflow.xgboost.log_model(\n        xgb_model = xg_reg, \n        artifact_path = &quot;xgboost-models&quot;,\n        registered_model_name = &quot;xgb-regression-model&quot;\n    )\n<\/code><\/pre>\n<p>In the remote UI, I can see the logged model:<\/p>\n<pre><code>artifact_path: xgboost-models\nflavors:\n  python_function:\n    data: model.xgb\n    env: conda.yaml\n    loader_module: mlflow.xgboost\n    python_version: 3.7.9\n  xgboost:\n    code: null\n    data: model.xgb\n    model_class: xgboost.sklearn.XGBClassifier\n    xgb_version: 1.5.2\nmlflow_version: 1.25.1\nmodel_uuid: 5fd42554cf184d8d96afae34dbb96de2\nrun_id: acdccd9f610b4c278b624fca718f76b4\nutc_time_created: '2022-05-17 17:54:53.039242\n<\/code><\/pre>\n<p>Now, on the server side, to load the logged model:<\/p>\n<pre><code>   model = mlflow.xgboost.load_model(model_uri=model_path)\n<\/code><\/pre>\n<p>which loads OK, but the model type is<\/p>\n<blockquote>\n<p>&lt;xgboost.core.Booster object at 0x00000234DBE61D00&gt;<\/p>\n<\/blockquote>\n<p>and the predictions are numpy.float32 (eg 0.5) instead of int64 (eg 0, 1) for the original model.<\/p>\n<p>Any ideas what can be wrong? Many thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654036554297,
        "Question_score":1,
        "Question_tags":"python|xgboost|mlflow",
        "Question_view_count":163,
        "Owner_creation_time":1369252294280,
        "Owner_last_access_time":1663789810223,
        "Owner_location":null,
        "Owner_reputation":324,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It turns out this was caused by using different versions of mlflow. The model was uploaded to registry with the newest version but was loaded with a previous one. When updated the server to load it, it now works! :)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655934198727,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72454747",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68718719,
        "Question_title":"How can I retrive the model.pkl in the experiment in Databricks",
        "Question_body":"<p>I want to retrieve the pickle off my trained model, which I know is in the run file inside my experiments in Databricks.<\/p>\n<p>It seems that the <code>mlflow.pyfunc.load_model<\/code> can only do the <code>predict<\/code> method.<\/p>\n<p>There is an option to directly access the pickle?<\/p>\n<p>I also tried to use the path in the run using the <code>pickle.load(path)<\/code> (example of path: dbfs:\/databricks\/mlflow-tracking\/20526156406\/92f3ec23bf614c9d934dd0195\/artifacts\/model\/model.pkl).<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1628544411170,
        "Question_score":1,
        "Question_tags":"python|azure|databricks|datastore|mlflow",
        "Question_view_count":3081,
        "Owner_creation_time":1522076554450,
        "Owner_last_access_time":1663901377960,
        "Owner_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Owner_reputation":96,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I recently found the solution which can be done by the following two approaches:<\/p>\n<ol>\n<li>Use the customized predict function at the moment of saving the model (check <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">databricks<\/a> documentation for more details).<\/li>\n<\/ol>\n<p>example give by Databricks<\/p>\n<pre><code>class AddN(mlflow.pyfunc.PythonModel):\n\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input):\n        return model_input.apply(lambda column: column + self.n)\n# Construct and save the model\nmodel_path = &quot;add_n_model&quot;\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n<\/code><\/pre>\n<ol start=\"2\">\n<li>Load the model artefacts as we are downloading the artefact:<\/li>\n<\/ol>\n<pre><code>from mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\n\ntmp_path = client.download_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path='model\/model.pkl')\n\nf = open(tmp_path,'rb')\n\nmodel = pickle.load(f)\n\nf.close()\n\n \n\nclient.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;&quot;)\n\nclient.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;model&quot;)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1629748421287,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1643054905513,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68718719",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70544873,
        "Question_title":"mlflow serving r models failed if use LDA instead of linear regression",
        "Question_body":"<p>I have a mlflow question regarding to serve R models, below is my code to train and serialize the R model.<\/p>\n<pre><code>library(mlflow)\nlibrary(caret)\nlibrary(carrier)\n\nset.seed(101)\nsample &lt;- createDataPartition(iris$Species, p=0.80, list=FALSE)\niris_train &lt;- iris[sample,]\niris_test &lt;- iris[-sample,]\n\n\nwith(mlflow_start_run()  , {\n  control &lt;- trainControl(method='cv', number=10)\n  metric &lt;- 'Accuracy'\n\n  # Linear Discriminant Analysis (LDA)\n  model &lt;- train(Species~., data=iris_train, method='lda', trControl=control, metric=metric,\n                preProcess=c(&quot;center&quot;, &quot;scale&quot;))\n  fn &lt;- crate(~ caret::predict.train(model, .x), model = model)\n  # fn &lt;- crate(~ stats::predict(model, .x), model = model)\n  iris_prediction &lt;- predict(model, iris_test)\n\n  mlflow_log_model(model = fn, artifact_path=&quot;model&quot;)\n})\n\n<\/code><\/pre>\n<p>However, when I serve it with the API request,<\/p>\n<pre><code>curl --location --request POST 'localhost:5000\/invocations' \\\n--header 'Content-Type: application\/json' \\\n--data-raw '{&quot;columns&quot;: [&quot;Sepal.Length&quot;, &quot;Sepal.With&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;], &quot;data&quot;: [[5.2, 4.1, 1.5, 0.1], [6.4, 2.8, 5.6, 2.1], [7.9, 3.8, 6.4, 2.0], [6.7, 3.1, 5.6, 2.4], [6.3, 3.4, 5.6, 2.4]]}'\n<\/code><\/pre>\n<p>I have the following error:\n<code>Invalid Request.  object 'Sepal.Width' not found<\/code><\/p>\n<p>Note, if I change the training to use linear regression model,<\/p>\n<pre><code>with(mlflow_start_run(), {\n  model &lt;- lm(Sepal.Width ~ Sepal.Length, iris)\n  mlflow_log_model(\n    crate(~ stats::predict(model, .x), model=model), &quot;model&quot;)\n})\n<\/code><\/pre>\n<p>The same API request, everything works fine.<\/p>\n<p>Can anyone help me find any clue? I am not that familiar with R.<\/p>\n<p>Thanks a lot!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640975828923,
        "Question_score":1,
        "Question_tags":"r|mlflow",
        "Question_view_count":57,
        "Owner_creation_time":1383535151877,
        "Owner_last_access_time":1663898770570,
        "Owner_location":null,
        "Owner_reputation":502,
        "Owner_up_votes":24,
        "Owner_down_votes":1,
        "Owner_views":40,
        "Question_last_edit_time":null,
        "Answer_body":"<p>it turns out the issue is a typo in curl script. it's Sepal.Width not Sepal.With.<\/p>\n<pre><code>curl --location --request POST 'localhost:5000\/invocations' \\\n--header 'Content-Type: application\/json' \\\n--data-raw '{&quot;columns&quot;: [&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;], &quot;data&quot;: [[5.2, 4.1, 1.5, 0.1], [6.4, 2.8, 5.6, 2.1], [7.9, 3.8, 6.4, 2.0], [6.7, 3.1, 5.6, 2.4], [6.3, 3.4, 5.6, 2.4]]}'\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1641228071910,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70544873",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":57199472,
        "Question_title":"Is it possible to set\/change mlflow run name after run initial creation?",
        "Question_body":"<p>I could not find a way yet of setting the runs name after the first start_run for that run (we can pass a name there). <\/p>\n\n<p>I Know we can use tags but that is not the same thing. I would like to add a run relevant name, but very often we know the name only after run evaluation or while we're running the run interactively in notebook for example.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1564049391490,
        "Question_score":11,
        "Question_tags":"mlflow",
        "Question_view_count":7689,
        "Owner_creation_time":1393062808773,
        "Owner_last_access_time":1618438983790,
        "Owner_location":"Portugal",
        "Owner_reputation":133,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1564533527097,
        "Answer_body":"<p>It is possible to edit run names from the MLflow UI. First, click into the run whose name you'd like to edit.<\/p>\n\n<p>Then, edit the run name by clicking the dropdown next the run name (i.e. the downward-pointing caret in this image):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/sl6Qs.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sl6Qs.png\" alt=\"Rename run dropdown\"><\/a><\/p>\n\n<p>There's currently no stable public API for setting run names - however, you can programmatically set\/edit run names by setting the tag with key <code>mlflow.runName<\/code>, which is what the UI (currently) does under the hood.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1564380155503,
        "Answer_score":9.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57199472",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58234777,
        "Question_title":"MLflow remote execution on databricks from windows creates an invalid dbfs path",
        "Question_body":"<p>I'm researching the use of MLflow as part of our data science initiatives and I wish to set up a minimum working example of remote execution on databricks from windows.<\/p>\n\n<p>However, when I perform the remote execution a path is created locally on windows in the MLflow package which is sent to databricks. This path specifies the upload location of the '.tar.gz' file corresponding to the Github repo containing the MLflow Project. In cmd this has a combination of '\\' and '\/', but on databricks there are no separators at all in this path, which raises the 'rsync: No such file or directory (2)' error.<\/p>\n\n<p>To be more general, I reproduced the error using an MLflow standard example and following this <a href=\"https:\/\/docs.databricks.com\/applications\/mlflow\/projects.html\" rel=\"nofollow noreferrer\">guide<\/a> from databricks. The MLflow example is the <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/sklearn_elasticnet_wine\" rel=\"nofollow noreferrer\">sklearn_elasticnet_wine<\/a>, but I had to add a default value to a parameter so I forked it and the MLproject which can be executed remotely can be found at (<a href=\"https:\/\/github.com\/aestene\/mlflow\/tree\/master\/examples\/sklearn_elasticnet_wine\" rel=\"nofollow noreferrer\">forked repo<\/a>).<\/p>\n\n<p>The Project can be executed remotely by the following command (assuming a databricks instance has been set up)<\/p>\n\n<pre><code>mlflow run https:\/\/github.com\/aestene\/mlflow#examples\/sklearn_elasticnet_wine -b databricks -c db-clusterconfig.json --experiment-id &lt;insert-id-here&gt;\n<\/code><\/pre>\n\n<p>where \"db-clusterconfig.json\" correspond to the cluster to set up in databricks and is in this example set to<\/p>\n\n<pre><code>{\n    \"autoscale\": {\n        \"min_workers\": 1,\n        \"max_workers\": 2\n    },\n    \"spark_version\": \"5.5.x-scala2.11\",\n    \"node_type_id\": \"Standard_DS3_v2\",\n    \"driver_node_type_id\": \"Standard_DS3_v2\",\n    \"ssh_public_keys\": [],\n    \"custom_tags\": {},\n    \"spark_env_vars\": {\n        \"PYSPARK_PYTHON\": \"\/databricks\/python3\/bin\/python3\"\n    }\n}\n<\/code><\/pre>\n\n<p>When running the project remotely, this is the output in cmd:<\/p>\n\n<pre><code>2019\/10\/04 10:09:50 INFO mlflow.projects: === Fetching project from https:\/\/github.com\/aestene\/mlflow#examples\/sklearn_elasticnet_wine into C:\\Users\\ARNTS\\AppData\\Local\\Temp\\tmp2qzdyq9_ ===\n2019\/10\/04 10:10:04 INFO mlflow.projects.databricks: === Uploading project to DBFS path \/dbfs\\mlflow-experiments\\3947403843428882\\projects-code\\aa5fbb4769e27e1be5a983751eb1428fe998c3e65d0e66eb9b4c77355076f524.tar.gz ===\n2019\/10\/04 10:10:05 INFO mlflow.projects.databricks: === Finished uploading project to \/dbfs\\mlflow-experiments\\3947403843428882\\projects-code\\aa5fbb4769e27e1be5a983751eb1428fe998c3e65d0e66eb9b4c77355076f524.tar.gz ===\n2019\/10\/04 10:10:05 INFO mlflow.projects.databricks: === Running entry point main of project https:\/\/github.com\/aestene\/mlflow#examples\/sklearn_elasticnet_wine on Databricks ===\n2019\/10\/04 10:10:06 INFO mlflow.projects.databricks: === Launched MLflow run as Databricks job run with ID 8. Getting run status page URL... ===\n2019\/10\/04 10:10:18 INFO mlflow.projects.databricks: === Check the run's status at https:\/\/&lt;region&gt;.azuredatabricks.net\/?o=&lt;databricks-id&gt;#job\/8\/run\/1 ===\n<\/code><\/pre>\n\n<p>Where the DBFS path has a leading '\/' before the remaining are '\\'. <\/p>\n\n<p>The command spins up a cluster in databricks and is ready to execute the job, but ends up with the following error message on the databricks side:<\/p>\n\n<pre><code>rsync: link_stat \"\/dbfsmlflow-experiments3947403843428882projects-codeaa5fbb4769e27e1be5a983751eb1428fe998c3e65d0e66eb9b4c77355076f524.tar.gz\" failed: No such file or directory (2)\nrsync error: some files\/attrs were not transferred (see previous errors) (code 23) at main.c(1183) [sender=3.1.1]\n<\/code><\/pre>\n\n<p>Where we can see the same path but without the '\\' inserted. I narrowed down the creation of this path to this <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/projects\/databricks.py\" rel=\"nofollow noreferrer\">file<\/a> in the MLflow Github repo, where the following code creates the path (line 133):<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>dbfs_path = os.path.join(DBFS_EXPERIMENT_DIR_BASE, str(experiment_id),\n                                     \"projects-code\", \"%s.tar.gz\" % tarfile_hash)\ndbfs_fuse_uri = os.path.join(\"\/dbfs\", dbfs_path)\n<\/code><\/pre>\n\n<p>My current hypothesis is that <code>os.path.join()<\/code> in the first line joins the string together in a \"windows fashion\" such that they have backslashes. Then the following call to <code>os.path.join()<\/code> adds a '\/'. The databricks file system is then unable to handle this path and something causes the 'tar.gz' file to not be properly uploaded or to be accessed at the wrong path. <\/p>\n\n<p>It should also be mentioned that the project runs fine locally.<\/p>\n\n<p>I'm running the following versions:<\/p>\n\n<p>Windows 10<\/p>\n\n<p>Python 3.6.8<\/p>\n\n<p>MLflow 1.3.0 (also replicated the fault with 1.2.0)<\/p>\n\n<p>Any feedback or suggestions are greatly appreciated!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1570185592420,
        "Question_score":2,
        "Question_tags":"databricks|mlflow",
        "Question_view_count":350,
        "Owner_creation_time":1570173110493,
        "Owner_last_access_time":1663917124557,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Thanks for the catch, you're right that using <code>os.path.join<\/code> when working with DBFS paths is incorrect, resulting in a malformed path that breaks project execution. I've filed to <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1926\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/1926<\/a> track this, if you're interested in making a bugfix PR (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst\" rel=\"nofollow noreferrer\">see the MLflow contributor guide for info on how to do this<\/a>) to replace <code>os.path.join<\/code> here with <code>os.posixpath.join<\/code> I'd be happy to review :)<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1570752025483,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58234777",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59194004,
        "Question_title":"MLflow - Serving model by reference to model registry",
        "Question_body":"<p>I'm having an issue to serve a model with reference to model registry. According to help, the path should look like this: <\/p>\n\n<p>models:\/model_name\/stage<\/p>\n\n<p>When I type in terminal: <br>\n<code>mlflow models serve -m models:\/ml_test_model1\/Staging --no-conda -h 0.0.0.0 -p 5003<\/code><\/p>\n\n<p>I got the error: <br>\n<code>mlflow.exceptions.MlflowException: Not a proper models:\/ URI: models:\/ml_test_model1\/Staging\/MLmodel. Models URIs must be of the form 'models:\/&lt;model_name&gt;\/&lt;version or stage&gt;'.<\/code><\/p>\n\n<p>Model is registered and visible in db and server. <br> \nIf I put absolute path, it works (experiment_id\/run_id\/artifacts\/model_name).<\/p>\n\n<p>mlflow version: 1.4 <br>\nPython version: 3.7.3<\/p>\n\n<p>Is it matter of some environmental settings or something different?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1575544859307,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":1225,
        "Owner_creation_time":1575543357813,
        "Owner_last_access_time":1596719149423,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>That style of referencing model artefacts is fixed from mlflow v1.5 (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/2067\" rel=\"nofollow noreferrer\">Bug Fix<\/a>).<\/p>\n\n<p>You'll need to run <code>mlflow db upgrade &lt;db uri&gt;<\/code> to refresh your schemas before restarting your mlflow server.<\/p>\n\n<p>You may find listing registered models helpful:<\/p>\n\n<p><code>&lt;server&gt;:&lt;port&gt;\/api\/2.0\/preview\/mlflow\/registered-models\/list<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1577232243980,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1577251898637,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59194004",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63920599,
        "Question_title":"PowerBI and MLflow integration (through AzureML)",
        "Question_body":"<p>I'm currently trying to integrate an ML model currently deployed as a webservice on AzureML with PowerBI.<\/p>\n<p>I see that it can be <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#invoking-the-azure-ml-model-in-power-bi\" rel=\"nofollow noreferrer\">integrated<\/a> but the model requires the addition of a schema file when it is <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#schema-discovery-for-machine-learning-models\" rel=\"nofollow noreferrer\">being deployed as a webservice<\/a>. Without this, the model can't be viewed in PowerBI.<\/p>\n<p>The problem that I have come up against is that I use MLflow to log ML model performances and subsequently to deploy a selected model onto AzureML as a webservice using MLflow's AzureML integration - mlflow.azureml.deploy(). This unfortunately doesn't have the option to define a schema file before the model is deployed, thus resulting in no model being available in PowerBI as it lacks the required schema file.<\/p>\n<p>My options seem to be:<\/p>\n<ol>\n<li>Find a workaround, possibly using the working <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-serving\" rel=\"nofollow noreferrer\">REST api of the model in a power query<\/a>.<\/li>\n<li>Rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow.<\/li>\n<\/ol>\n<p>I thought I would ask to see if I am maybe missing something as I can't find a workaround using my current code to define a schema file in MLflow when deploying with mlflow.azureml.deploy().<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1600261190477,
        "Question_score":0,
        "Question_tags":"azure|powerbi|mlflow|azure-machine-learning-service",
        "Question_view_count":405,
        "Owner_creation_time":1600260166047,
        "Owner_last_access_time":1615561616230,
        "Owner_location":null,
        "Owner_reputation":15,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1600855880503,
        "Answer_body":"<p>Point number 2 is the way we solved this issue. Instead of using MLflow to deploy to a scoring service on Azure, we wrote a custom code which load MLflow model when container is initialised.<\/p>\n<p>Scoring code is something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nfrom mlflow.pyfunc import load_model\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\ndef init():\n    global model\n    model = load_model(os.path.join(os.environ.get(&quot;AZUREML_MODEL_DIR&quot;), &quot;awesome_model&quot;))\n\n@input_schema('data', NumpyParameterType(input_sample))\n@output_schema(NumpyParameterType(output_sample))\n\ndef run(data):\n    return model.predict(data)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1600604920243,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1600855957377,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63920599",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71846804,
        "Question_title":"How to I track loss at epoch using mlflow\/tensorflow?",
        "Question_body":"<p>I want to use mlflow to track the development of a TensorFlow model. How do I log the loss at each epoch? I have written the following code:<\/p>\n<pre><code>mlflow.set_tracking_uri(tracking_uri)\n\nmlflow.set_experiment(&quot;\/deep_learning&quot;)\nwith mlflow.start_run():\n    mlflow.log_param(&quot;batch_size&quot;, batch_size)\n    mlflow.log_param(&quot;learning_rate&quot;, learning_rate)\n    mlflow.log_param(&quot;epochs&quot;, epochs)\n    mlflow.log_param(&quot;Optimizer&quot;, opt)\n    mlflow.log_metric(&quot;train_loss&quot;, train_loss)\n    mlflow.log_metric(&quot;val_loss&quot;, val_loss)\n    mlflow.log_metric(&quot;test_loss&quot;, test_loss)\n    mlflow.log_metric(&quot;test_mse&quot;, test_mse)\n    mlflow.log_artifacts(&quot;.\/model&quot;)\n<\/code><\/pre>\n<p>If I change the train_loss and val_loss to<\/p>\n<pre><code>train_loss = history.history['loss']\nval_loss = history.history['val_loss']\n<\/code><\/pre>\n<p>I get the following error:<\/p>\n<pre><code>mlflow.exceptions.MlflowException: Got invalid value [12.041399002075195] for metric 'train_loss' (timestamp=1649783654667). Please specify value as a valid double (64-bit floating point)\n<\/code><\/pre>\n<p>How to I save the the loss and the val_loss at all epochs, so I can visualise a learning curve within mlflow?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649783963393,
        "Question_score":1,
        "Question_tags":"python|tensorflow|keras|tf.keras|mlflow",
        "Question_view_count":320,
        "Owner_creation_time":1507645492477,
        "Owner_last_access_time":1655115322463,
        "Owner_location":"Liverpool, UK",
        "Owner_reputation":77,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As you can read <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.keras.html#module-mlflow.keras\" rel=\"nofollow noreferrer\">here<\/a>. You can use <code>mlflow.tensorflow.autolog()<\/code> and this, (from doc):<\/p>\n<blockquote>\n<p>Enables (or disables) and configures autologging from Keras to MLflow. Autologging captures the following information:<\/p>\n<blockquote>\n<p>fit() or fit_generator() parameters; optimizer name; learning rate; epsilon\n...<\/p>\n<\/blockquote>\n<\/blockquote>\n<p>For example:<\/p>\n<pre><code># !pip install mlflow\nimport tensorflow as tf\nimport mlflow\nimport numpy as np\n\n\nX_train = np.random.rand(100,100)\ny_train = np.random.randint(0,10,100)\n    \n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.Input(100,))\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(rate=.4))\nmodel.add(tf.keras.layers.Dense(10, activation='sigmoid'))        \nmodel.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n              optimizer='Adam', \n              metrics=['accuracy'])\nmodel.summary()\n\n\nmlflow.tensorflow.autolog()\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=50)\n<\/code><\/pre>\n<p>Or as you mention in the comment you can use <code>mlflow.set_tracking_uri()<\/code> like below:<\/p>\n<pre><code>mlflow.set_tracking_uri('http:\/\/127.0.0.1:5000')\ntracking_uri = mlflow.get_tracking_uri()\nwith mlflow.start_run(run_name='PARENT_RUN') as parent_run:\n    batch_size=50\n    history = model.fit(X_train, y_train, epochs=2, batch_size=batch_size)\n    mlflow.log_param(&quot;batch_size&quot;, batch_size)  \n<\/code><\/pre>\n<p>For getting results:<\/p>\n<pre><code>!mlflow ui\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>[....] [...] [INFO] Starting gunicorn 20.1.0\n[....] [...] [INFO] Listening at: http:\/\/127.0.0.1:5000 (****)\n[....] [...] [INFO] Using worker: sync\n[....] [...] [INFO] Booting worker with pid: ****\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9XXoi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9XXoi.png\" alt=\"enter image description here\" \/><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/V2tvM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/V2tvM.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1649809588953,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1649895732377,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71846804",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":64513552,
        "Question_title":"How to have multiple MLFlow runs in parallel?",
        "Question_body":"<p>I'm not very familiar with parallelization in Python and I'm getting an error when trying to train a model on multiple training folds in parallel. Here's a simplified version of my code:<\/p>\n<pre><code>def train_test_model(fold):\n    # here I train the model etc...\n    \n    # now I want to save the parameters and metrics\n    with mlflow.start_run():\n        mlflow.log_param(&quot;run_name&quot;, run_name)\n        mlflow.log_param(&quot;modeltype&quot;, modeltype)\n        # and so on...\n\nif __name__==&quot;__main__&quot;:\n    pool = ThreadPool(processes = num_trials)\n    # run folds in parallel\n    pool.map(lambda fold:train_test_model(fold), folds)\n<\/code><\/pre>\n<p>I'm getting the following error:<\/p>\n<pre><code>Exception: Run with UUID 23e9bb6d22674a518e48af9c51252860 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True\n<\/code><\/pre>\n<p>The <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.start_run\" rel=\"nofollow noreferrer\">documentation<\/a> says that <code>mlflow.start_run()<\/code> starts a new run and makes it active which is the root of my problem. Every thread starts a MLFlow run for its corresponding fold and makes it active while I need the runs to run in parallel i.e. all be active(?) and save parameters\/metrics of the corresponding fold. How can I solve that issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603544320573,
        "Question_score":1,
        "Question_tags":"python|pyspark|parallel-processing|mlflow",
        "Question_view_count":2275,
        "Owner_creation_time":1515931819517,
        "Owner_last_access_time":1664047231110,
        "Owner_location":null,
        "Owner_reputation":177,
        "Owner_up_votes":126,
        "Owner_down_votes":1,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I found a solution, maybe it will be useful for someone else. You can see details with code examples here: <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/3592\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/3592<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1603877162673,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64513552",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68533916,
        "Question_title":"what is features and how to interpret in RFormula",
        "Question_body":"<p>I am trying to understand what an RFormula is in MLflow or spark.<\/p>\n<p>I have found these:<\/p>\n<p><a href=\"https:\/\/george-jen.gitbook.io\/data-science-and-apache-spark\/rformula\" rel=\"nofollow noreferrer\">https:\/\/george-jen.gitbook.io\/data-science-and-apache-spark\/rformula<\/a>\n<a href=\"https:\/\/spark.apache.org\/docs\/latest\/api\/python\/reference\/api\/pyspark.ml.feature.RFormula.html\" rel=\"nofollow noreferrer\">https:\/\/spark.apache.org\/docs\/latest\/api\/python\/reference\/api\/pyspark.ml.feature.RFormula.html<\/a><\/p>\n<p>but still cannot understand how to interpret an RFormula fully. I am not sure how to interpret the below table\n<a href=\"https:\/\/i.stack.imgur.com\/guLyU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/guLyU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>based on the formula &quot;y ~ x+ s&quot;, y is related to x and s, but in the table when y=0 and x=0 and s =a (i.e. third row), then the features is [0,1] and label is 0, so how shall I interpret this.<\/p>\n<p>I have found <a href=\"https:\/\/stackoverflow.com\/questions\/61290042\/spark-rformula-interpretation\">this<\/a> but still cannot understand my way through this problem.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1627319315683,
        "Question_score":0,
        "Question_tags":"r|pyspark|mlflow",
        "Question_view_count":78,
        "Owner_creation_time":1375186444010,
        "Owner_last_access_time":1662797989807,
        "Owner_location":"Auckland, New Zealand",
        "Owner_reputation":912,
        "Owner_up_votes":334,
        "Owner_down_votes":1,
        "Owner_views":288,
        "Question_last_edit_time":1627371880353,
        "Answer_body":"<p>So your label is y. You parse x and s in rformula.<\/p>\n<p>x stays the same:<\/p>\n<pre><code>+-----------+---+\n|      x    | x |\n+-----------+---+\n|     1.0   |1.0|\n|     2.0   |2.0|\n|     0.0   |0.0|\n+-----------+---+\n<\/code><\/pre>\n<p>s:<\/p>\n<pre><code>+-----------+---+\n|       s   | s |\n+-----------+---+\n|       a   |1.0|\n|       b   |0.0|\n|       a   |1.0|\n+-----------+---+\n<\/code><\/pre>\n<p>I hope I could answer you question.\nRformula just converts the strings, standarize them and parse them into a vector.<\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_time":1627334164713,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1627334932660,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68533916",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65316586,
        "Question_title":"get the run id for an mlflow experiment with the name?",
        "Question_body":"<p>I currently created an experiment in mlflow and created multiple runs in the experiment.<\/p>\n<pre><code>from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nimport mlflow\n\nexperiment_name=&quot;experiment-1&quot;\nmlflow.set_experiment(experiment_name)\n\nno_of_trees=[100,200,300]\ndepths=[2,3,4]\nfor trees in no_of_trees:\n    for depth in depths:\n        with mlflow.start_run() as run:\n            model=RandomForestRegressor(n_estimators=trees, criterion='mse',max_depth=depth)\n            model.fit(x_train, y_train)\n            predictions=model.predict(x_cv)\n            mlflow.log_metric('rmse',mean_squared_error(y_cv, predictions))\n<\/code><\/pre>\n<p>after creating the runs, I wanted to get the best run_id for this experiment. for now, I can get the best run by looking at the UI of mlflow but how can we do right the program?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1608086727487,
        "Question_score":6,
        "Question_tags":"python|mlflow",
        "Question_view_count":6374,
        "Owner_creation_time":1479190327737,
        "Owner_last_access_time":1660584310400,
        "Owner_location":"R G U K T , basar, Andhra Pradesh, India",
        "Owner_reputation":2470,
        "Owner_up_votes":265,
        "Owner_down_votes":22,
        "Owner_views":251,
        "Question_last_edit_time":null,
        "Answer_body":"<p>we can get the experiment id from the experiment name and we can use python API to get the best runs.<\/p>\n<pre><code>experiment_name = &quot;experiment-1&quot;\ncurrent_experiment=dict(mlflow.get_experiment_by_name(experiment_name))\nexperiment_id=current_experiment['experiment_id']\n<\/code><\/pre>\n<p>By using the experiment id, we can get all the runs and we can sort them based on metrics like below. In the below code, rmse is my metric name (so it may be different for you based on metric name)<\/p>\n<pre><code>df = mlflow.search_runs([experiment_id], order_by=[&quot;metrics.rmse DESC&quot;])\nbest_run_id = df.loc[0,'run_id']\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1608086727487,
        "Answer_score":15.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65316586",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71679081,
        "Question_title":"How can I connect mlflow server via nginx ssl authentication?",
        "Question_body":"<p>System information\nOS Platform and Distribution: Windows 10\nMLflow installed: using pip\nMLflow version: version 1.24.0\n**Python version: Python 3.9.7 **<\/p>\n<p>Describe the problem\nI have created a docker-compose system with a backend\/artifact storages, mlflow server and nginx to add an authentication layer.<\/p>\n<pre><code>...\nmlflow:\n        restart: always\n        build: .\n        environment:\n            - AWS_ACCESS_KEY_ID=${MINIO_USR}\n            - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}       \n        expose:\n            - '5000'\n        networks:\n            - frontend\n            - backend\n        depends_on:\n            - storage                       \n        image: 'mlflow:Dockerfile'\n        container_name: mlflow_server_nginx\n\n    nginx:\n        restart: always\n        build: .\/nginx\n        container_name: mlflow_nginx\n        ports:\n            - 5043:443\n        links:\n            - mlflow:mlflow\n        volumes:\n            - 'path\/to\/nginx\/auth:\/etc\/nginx\/conf.d'\n            - 'path\/to\/nginx\/nginx.conf:\/etc\/nginx\/nginx.conf:ro'\n        networks:\n            - frontend\n        depends_on:\n            - mlflow\n<\/code><\/pre>\n<p>I have created an user\/password via htpasswd and a custom SSL CA (.pem\/.key) using openssl and my-mlflow.com server-name.<\/p>\n<p>When the docker-compose system is built i can access to mlflow UI via my browser. But when i try to create a new experiment using python trying diferent approaches, i get next errors:\nExecuted code 1:<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\n#os.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n#os.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate (_ssl.c:1108)')))\n<\/code><\/pre>\n<p>After read some notes in the documentation and realated issues I tryed next<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\n#os.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\nos.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLError(9, '[SSL] PEM lib (_ssl.c:4012)')))\n<\/code><\/pre>\n<p>Finally<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\nos.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n#os.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLCertVerificationError(&quot;hostname 'localhost' doesn't match '*.my-mlflow.com'&quot;)))\n<\/code><\/pre>\n<p>Can you give me some hints about how to solve it?<\/p>\n<p>Thank you very much!\nFernando....<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648650339347,
        "Question_score":1,
        "Question_tags":"python-3.x|docker|nginx|docker-compose|mlflow",
        "Question_view_count":625,
        "Owner_creation_time":1580841805373,
        "Owner_last_access_time":1663792613347,
        "Owner_location":"Seville, Spain",
        "Owner_reputation":33,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can set:<\/p>\n<pre><code>os.environ['MLFLOW_TRACKING_INSECURE_TLS'] = 'true'\n<\/code><\/pre>\n<p>And then try to get your cert-chain straight from there for production use.<\/p>\n<p>Also see Documentation: <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#id19\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tracking.html#id19<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1652276299263,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1652448943407,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71679081",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60667610,
        "Question_title":"How to deploy mlflow model with data preprocessing(text data)",
        "Question_body":"<p>I have developed keras text classification model. I have preprocessed data(tokenization). I have logged trained model successfully(mlflow.keras.log_model). I have served model using mlflow serve. Now while doing prediction on text data I need to do preprocessing using same tokenizer object used for training.\nHow to preprocess test data and get predictions from served model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1584090517703,
        "Question_score":3,
        "Question_tags":"mlflow",
        "Question_view_count":1996,
        "Owner_creation_time":1498470936987,
        "Owner_last_access_time":1614914388693,
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can log a custom python model: \n<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1584552358667,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60667610",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73127303,
        "Question_title":"Get Experiment that Created Model in MLflow",
        "Question_body":"<p>I want to get the name of the experiment that contains the run that created a registered MLflow model. How can I do this using MLflow, if I just have the name of the model and the version?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1658855429667,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":44,
        "Owner_creation_time":1521856385820,
        "Owner_last_access_time":1664037995903,
        "Owner_location":"Sri Lanka",
        "Owner_reputation":820,
        "Owner_up_votes":389,
        "Owner_down_votes":1,
        "Owner_views":165,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As @Andre has said, I had to write my own function to achieve this,<\/p>\n<pre><code>def get_model_experiment(model_name, model_version):\n    # get run_id of the model version\n    run_id = mlflow_client.get_model_version(model_name, model_version).run_id\n\n    # get the experiment_id from the run_id\n    experiment_id = mlflow_client.get_run(run_id).info.experiment_id\n\n    # get the experiment name from the experiment_id\n    return mlflow_client.get_experiment(experiment_id).name\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1659073607990,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73127303",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71752458,
        "Question_title":"MLflow Experiments Tracking : local (dev tools - vscode) to databricks workspace",
        "Question_body":"<p>I had configured my databricks workspace in local using,<\/p>\n<p><code>databricks configure --profile &lt;profile_name&gt; --token<\/code><\/p>\n<p>by which I am able to list the clusters and create secret scope.<\/p>\n<p>But I am unable to create mlflow experiments. I had set the tracking uri to &quot;databricks&quot; and also tested with &quot;databricks\/&lt;profile_name&quot; and tested but i am unable to create or track any experiments on my databricks workspace.<\/p>\n<p>I get this following error;<\/p>\n<p><code>from mlflow.tracking import MlflowClient client = MlflowClient() mlflow.set_tracking_uri(&quot;databricks&quot;) experiment =  client.get_experiment_by_name('\/Shared\/test')<\/code><\/p>\n<p>MlflowException: API request to endpoint was successful but the response body was not in a valid JSON format. Response body: '&lt;!doctype html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;\/&gt;&lt;meta http-equiv=&quot;Content-Language&quot; content=&quot;en&quot;\/&gt;&lt;title&gt;Databricks - Sign In&lt;\/title&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=960&quot;\/&gt;&lt;link rel=&quot;icon&quot; type=&quot;image\/png&quot; href=&quot;\/favicon.ico&quot;\/&gt;&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text\/html; charset=UTF8&quot;\/&gt;&lt;link rel=&quot;icon&quot; href=&quot;\/favicon.ico&quot;&gt;&lt;script defer=&quot;defer&quot; src=&quot;\/login\/login.0ceb14c0.js&quot;&gt;&lt;\/script&gt;&lt;\/head&gt;&lt;body class=&quot;light-mode&quot;&gt;&lt;uses-legacy-bootstrap&gt;&lt;div id=&quot;login-page&quot;&gt;&lt;\/div&gt;&lt;\/uses-legacy-bootstrap&gt;&lt;\/body&gt;&lt;\/html&gt;'<\/p>\n<p>Could someone help me on what I am missing here?<\/p>\n<p>I am expecting to create\/track mlflow experiements in databricks workspace via dev-tools(vscode).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649164485623,
        "Question_score":0,
        "Question_tags":"databricks|azure-databricks|mlflow",
        "Question_view_count":220,
        "Owner_creation_time":1515569029903,
        "Owner_last_access_time":1655721276247,
        "Owner_location":null,
        "Owner_reputation":5,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I had the same problem while trying to load a model from model registry with mismatching versions (client 1.22.0).<\/p>\n<p>I had to downgrade the client version to make it work.<\/p>\n<p>Downgraded first the client to 1.21 and then server to 1.20<\/p>\n<p>Refer - <a href=\"https:\/\/docs.databricks.com\/dev-tools\/api\/latest\/mlflow.html#operation\/transition-model-version-stage\" rel=\"nofollow noreferrer\">https:\/\/docs.databricks.com\/dev-tools\/api\/latest\/mlflow.html#operation\/transition-model-version-stage<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1649225844710,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71752458",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":66814885,
        "Question_title":"Serve online learning models with mlflow",
        "Question_body":"<p>It is not clear to me if one could use mlflow to serve a model that is evolving continuously based on its previous predictions.<\/p>\n<p>I need to be able to query a model in order to make a prediction on a sample of data which is the basic use of mlflow serve. However I also want the model to be updated internaly now that it has seen new data.<\/p>\n<p>Is it possible or does it need a FR ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616753577110,
        "Question_score":1,
        "Question_tags":"python-3.x|mlflow",
        "Question_view_count":360,
        "Owner_creation_time":1515156959093,
        "Owner_last_access_time":1630697909597,
        "Owner_location":"Fairbanks, AK, United States",
        "Owner_reputation":76,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I think that you should be able to do that by implementing the custom python model or custom flavor, as it's described in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">documentation<\/a>.  In this case you need to create a class that is inherited from <code>mlflow.pyfunc.PythonModel<\/code>, and implement the <code>predict<\/code> method, and inside that method you're free to do anything.  Here is just simple example from documentation:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class AddN(mlflow.pyfunc.PythonModel):\n\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input):\n        return model_input.apply(lambda column: column + self.n)\n<\/code><\/pre>\n<p>and this model is then could be saved &amp; loaded again just as normal models:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Construct and save the model\nmodel_path = &quot;add_n_model&quot;\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1618124640630,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66814885",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67786052,
        "Question_title":"Log Pickle files as a part of Mlflow run",
        "Question_body":"<p>I am running an MLflow experiment as a part of it I would like to log a few artifacts as a python pickle.<\/p>\n<p>Ex: Trying out different categorical encoders, so wanted to log the encoder objects as a pickle file.<\/p>\n<p>Is there a way to achieve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622538922663,
        "Question_score":3,
        "Question_tags":"python|databricks|azure-databricks|mlflow",
        "Question_view_count":1843,
        "Owner_creation_time":1411361217027,
        "Owner_last_access_time":1662192778187,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":569,
        "Owner_up_votes":41,
        "Owner_down_votes":3,
        "Owner_views":123,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There are two functions for there:<\/p>\n<ol>\n<li><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifact\" rel=\"nofollow noreferrer\">log_artifact<\/a> - to log a local file or directory as an artifact<\/li>\n<li><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifacts\" rel=\"nofollow noreferrer\">log_artifacts<\/a> - to log a contents of a local directory<\/li>\n<\/ol>\n<p>so it would be as simple as:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with mlflow.start_run():\n    mlflow.log_artifact(&quot;encoder.pickle&quot;)\n<\/code><\/pre>\n<p>And you will need to use the <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">custom MLflow model<\/a> to use that pickled file, something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow.pyfunc\n\nclass my_model(mlflow.pyfunc.PythonModel):\n    def __init__(self, encoders):\n        self.encoders = encoders\n\n    def predict(self, context, model_input):\n        _X = ...# do encoding using self.encoders.\n        return str(self.ctx.predict([_X])[0])\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1622542563553,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67786052",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65145994,
        "Question_title":"Saving an Matlabplot as an MLFlow artifact",
        "Question_body":"<p>I am using DataBricks and Spark 7.4ML,<\/p>\n<p>The following code successfully logs the params and metrics, and I can see the ROCcurve.png in the MLFLOW gui (just the item in the tree below the model). But the actually plot is blank. Why?<\/p>\n<pre><code>with mlflow.start_run(run_name=&quot;logistic-regression&quot;) as run:\n  pipeModel = pipe.fit(trainDF)\n  mlflow.spark.log_model(pipeModel, &quot;model&quot;)\n  predTest = pipeModel.transform(testDF)\n  predTrain = pipeModel.transform(trainDF)\n  evaluator=BinaryClassificationEvaluator(labelCol=&quot;arrivedLate&quot;)\n  trainROC = evaluator.evaluate(predTrain)\n  testROC = evaluator.evaluate(predTest)\n  print(f&quot;Train ROC: {trainROC}&quot;)\n  print(f&quot;Test ROC: {testROC}&quot;)\n  mlflow.log_param(&quot;Dataset Name&quot;, &quot;Flights &quot; + datasetName)\n  mlflow.log_metric(key=&quot;Train ROC&quot;, value=trainROC)\n  mlflow.log_metric(key=&quot;Test ROC&quot;, value=testROC)\n\n  lrModel = pipeModel.stages[3]\n  trainingSummary = lrModel.summary\n  roc = trainingSummary.roc.toPandas()\n  plt.plot(roc['FPR'],roc['TPR'])\n  plt.ylabel('False Positive Rate')\n  plt.xlabel('True Positive Rate')\n  plt.title('ROC Curve')\n  plt.show()\n  plt.savefig(&quot;ROCcurve.png&quot;)\n  mlflow.log_artifact(&quot;ROCcurve.png&quot;)\n  plt.close()\n  \n  display(predTest.select(stringCols + [&quot;arrivedLate&quot;, &quot;prediction&quot;]))\n<\/code><\/pre>\n<p>What the notebook shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What the MLFlow shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1607094596640,
        "Question_score":8,
        "Question_tags":"apache-spark|matplotlib|pyspark|databricks|mlflow",
        "Question_view_count":5219,
        "Owner_creation_time":1316705139197,
        "Owner_last_access_time":1663085821243,
        "Owner_location":"Boston, MA",
        "Owner_reputation":6711,
        "Owner_up_votes":353,
        "Owner_down_votes":3,
        "Owner_views":819,
        "Question_last_edit_time":1607191847983,
        "Answer_body":"<p>Put <code>plt.show()<\/code> after <code>plt.savefig()<\/code> - <code>plt.show()<\/code> will remove your plot because it is shown already.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1607094854147,
        "Answer_score":7.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65145994",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71985302,
        "Question_title":"How to see results via `mlflow ui` for experiments logged on a other server?",
        "Question_body":"<p>I was running ML experiments on a ssh server, the experiments were logged via mlflow and stored in local <code>mlruns<\/code> on the server.<\/p>\n<p>The code were just basic usage of mlflow and looks like this<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nimport mlflow\nimport matplotlib.pyplot as plt\n\nfrom pathlib import Path\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n\nif __name__ == '__main__':\n    Path('.\/plot').mkdir(exist_ok = True)\n    mlflow.set_experiment('demo-exp')\n    with mlflow.start_run():\n        x     = np.random.randn(10, 1)\n        y     = np.random.randn(10, 1)\n        model = LinearRegression().fit(x, y)\n        py    = model.predict(x)\n        r2    = r2_score(y, py)\n        plt.plot(y, py, '+')\n        plt.savefig('.\/plot\/result.png')\n        plt.close()\n\n        mlflow.log_metric('r2', r2)\n        mlflow.log_artifact('.\/plot\/result.png')\n<\/code><\/pre>\n<p>Now I want to see the logged metrics and artifactos from my own laptop, so I tried<\/p>\n<pre><code>mlflow ui \\\n    --default-artifact-root=sftp:\/\/\/USER_NAME@ADDRESS\/path\/to\/experiment\/mlruns \\\n    --port=8888 \\\n\n<\/code><\/pre>\n<p>However, it looks like nothing showed in <code>localhost:8888<\/code> on my own laptop.<\/p>\n<p>Anything I did wrong about the code and <code>mlflow ui<\/code> command?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650771165373,
        "Question_score":0,
        "Question_tags":"python|mlflow|mlops",
        "Question_view_count":358,
        "Owner_creation_time":1366379833543,
        "Owner_last_access_time":1663851694857,
        "Owner_location":"China,Shanghai.",
        "Owner_reputation":3157,
        "Owner_up_votes":52,
        "Owner_down_votes":7,
        "Owner_views":163,
        "Question_last_edit_time":null,
        "Answer_body":"<ul>\n<li>Run mlflow on a remote server<\/li>\n<\/ul>\n<pre><code>mlflow server --host 0.0.0.0 --port 8888\n<\/code><\/pre>\n<ul>\n<li>Configure ssh local port forward on your local machine, like this:\n<a href=\"https:\/\/stackoverflow.com\/questions\/9146457\/ssh-port-forwarding-in-a-ssh-config-file\">SSH Port forwarding in a ~\/.ssh\/config file?<\/a><\/li>\n<\/ul>\n<pre><code>LocalForward 8888 your_remote_machine_addr:8888\n<\/code><\/pre>\n<ul>\n<li>Connect to localhost:8888 on your browser<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1650951347493,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71985302",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67499339,
        "Question_title":"Create mlflow experiment: Run with UUID is already active",
        "Question_body":"<p>I'm trying to create a new experiment on mlflow but I have this problem:<\/p>\n<pre><code>Exception: Run with UUID l142ae5a7cf04a40902ae9ed7326093c is already active.\n\n<\/code><\/pre>\n<p>This is my code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\nimport mlflow.sklearn\nimport sys\n\nmlflow.set_experiment(&quot;New experiment 2&quot;)\n\nmlflow.set_tracking_uri('http:\/\/mlflow:5000')\nst= mlflow.start_run(run_name='Test2')\nid = st.info.run_id\nmlflow.log_metric(&quot;score&quot;, score)\nmlflow.sklearn.log_model(model, &quot;wineModel&quot;)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620805743553,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":675,
        "Owner_creation_time":1620748388617,
        "Owner_last_access_time":1620921887890,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You have to run mlflow.end_run() to finish the first experiment. Then you can create another<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620805914963,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67499339",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67456172,
        "Question_title":"Unable to connect to MLFLOW_TRACKING_URI when running MLflow run in Docker container",
        "Question_body":"<p>I have setup a mlflow server locally at http:\/\/localhost:5000<\/p>\n<p>I followed the instructions at <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker<\/a> and tried to run the example docker with<\/p>\n<pre><code>\/mlflow\/examples\/docker$ mlflow run . -P alpha=0.5\n<\/code><\/pre>\n<p>but I encountered the following error.<\/p>\n<pre><code>2021\/05\/09 17:11:20 INFO mlflow.projects.docker: === Building docker image docker-example:7530274 ===\n2021\/05\/09 17:11:20 INFO mlflow.projects.utils: === Created directory \/tmp\/tmp9wpxyzd_ for downloading remote URIs passed to arguments of type 'path' ===\n2021\/05\/09 17:11:20 INFO mlflow.projects.backend.local: === Running command 'docker run --rm -v \/home\/mlf\/mlf\/0\/ae69145133bf49efac22b1d390c354f1\/artifacts:\/home\/mlf\/mlf\/0\/ae69145133bf49efac22b1d390c354f1\/artifacts -e MLFLOW_RUN_ID=ae69145133bf49efac22b1d390c354f1 -e MLFLOW_TRACKING_URI=http:\/\/localhost:5000 -e MLFLOW_EXPERIMENT_ID=0 docker-example:7530274 python train.py --alpha 0.5 --l1-ratio 0.1' in run with ID 'ae69145133bf49efac22b1d390c354f1' === \n\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/__init__.py:55: DeprecationWarning: MLflow support for Python 2 is deprecated and will be dropped in a future release. At that point, existing Python 2 workflows that use MLflow will continue to work without modification, but Python 2 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3 - see https:\/\/docs.python.org\/3\/howto\/pyporting.html for a migration guide.\n  &quot;for a migration guide.&quot;, DeprecationWarning)\nTraceback (most recent call last):\n  File &quot;train.py&quot;, line 56, in &lt;module&gt;\n    with mlflow.start_run():\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/client.py&quot;, line 96, in get_run\n    return self._tracking_client.get_run(run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 49, in get_run\n    return self.store.get_run(run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py&quot;, line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py&quot;, line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 133, in call_endpoint\n    host_creds=host_creds, endpoint=endpoint, method=method, params=json_body)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 70, in http_request\n    url=url, headers=headers, verify=verify, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 51, in request_with_ratelimit_retries\n    response = requests.request(**kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/api.py&quot;, line 58, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/sessions.py&quot;, line 508, in request\n    resp = self.send(prep, **send_kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/sessions.py&quot;, line 618, in send\n    r = adapter.send(request, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/adapters.py&quot;, line 508, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get?run_uuid=ae69145133bf49efac22b1d390c354f1&amp;run_id=ae69145133bf49efac22b1d390c354f1 (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f5cbd80d690&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))\n2021\/05\/09 17:11:22 ERROR mlflow.cli: === Run (ID 'ae69145133bf49efac22b1d390c354f1') failed ===\n<\/code><\/pre>\n<p>Any ideas how to fix this? I tried adding the following in MLproject file but it doesn't help<\/p>\n<pre><code>environment: [[&quot;network&quot;, &quot;host&quot;], [&quot;add-host&quot;, &quot;host.docker.internal:host-gateway&quot;]]\n<\/code><\/pre>\n<p>Thanks for your help! =)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620552530280,
        "Question_score":0,
        "Question_tags":"docker|mlflow",
        "Question_view_count":1151,
        "Owner_creation_time":1316620102630,
        "Owner_last_access_time":1643692547643,
        "Owner_location":null,
        "Owner_reputation":1308,
        "Owner_up_votes":177,
        "Owner_down_votes":1,
        "Owner_views":151,
        "Question_last_edit_time":1620554070857,
        "Answer_body":"<p>Run MLflow server such was that it will use your machine IP instead of <code>localhost<\/code>.  Then point the <code>mlflow run<\/code> to that IP instead of <code>http:\/\/localhost:5000<\/code>.   The main reason is that <code>localhost<\/code> of Docker process is its own, not your machine.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1620627546970,
        "Answer_score":1.0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67456172",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70538098,
        "Question_title":"Databricks MLFlow AutoML XGBoost can't predict_proba()",
        "Question_body":"<p>I used AutoML in Databricks Notebooks for a binary classification problem and the winning model flavor was XGBoost (big surprise).<\/p>\n<p>The outputted model is of this variety:<\/p>\n<pre><code>mlflow.pyfunc.loaded_model:\n      artifact_path: model\n      flavor: mlflow.sklearn\n      run_id: 123456789\n<\/code><\/pre>\n<p>Any idea why when I use <code>model.predict_proba(X)<\/code>, I get this response?<\/p>\n<p><code>AttributeError: 'PyFuncModel' object has no attribute 'predict_proba'<\/code><\/p>\n<p>I know it is possible to get the probabilities because ROC\/AUC is a metric used for tuning the model. Any help would be amazing!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640912523883,
        "Question_score":1,
        "Question_tags":"pandas|scikit-learn|databricks|xgboost|mlflow",
        "Question_view_count":451,
        "Owner_creation_time":1562828557217,
        "Owner_last_access_time":1663796637890,
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":77,
        "Owner_up_votes":35,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I had the same issue with catboost model.\nThe way I solved it was by saving the artifacts in a local dir<\/p>\n<pre><code>import os\nfrom mlflow.tracking import MlflowClient\nclient = MlflowClient()\nlocal_dir = &quot;\/dbfs\/FileStore\/user\/models&quot;\nlocal_path = client.download_artifacts('run_id', &quot;model&quot;, local_dir)```\n\n```model_path = '\/dbfs\/FileStore\/user\/models\/model\/model.cb'\nmodel = CatBoostClassifier()\nmodel = model.load_model(model_path)\nmodel.predict_proba(test_set)```\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1646744187860,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70538098",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71135260,
        "Question_title":"Python assert a function is called within a certain `with` statement's context",
        "Question_body":"<p>In python I would like to check that a given function is called within a <code>with<\/code> statement of a given type<\/p>\n<pre><code>class Bar:\n def __init__(self, x):\n  self.x = x\n def __enter__(self):\n  return self\n def __exit__(self, *a, **k):\n  pass\n\ndef foo(x):\n # assert that the enclosing context is an instance of bar\n # assert isinstance('enclosed context', Bar)\n print(x*2)\n\nwith Bar(1) as bar:\n foo(bar.x)\n<\/code><\/pre>\n<p>I could do something like enforcing an arg passed into <code>foo<\/code> and wrapping functions in a decorator i.e.<\/p>\n<pre><code>class Bar:\n def __init__(self, x):\n  self.x = x\n def __enter__(self):\n  return self\n def __exit__(self, *a, **k):\n  pass\n\ndef assert_bar(func):\n def inner(bar, *a, **k):\n  assert isinstance(bar, Bar)\n  return func(*a, **k)\n return inner\n\n\n@assert_bar\ndef foo(x):\n print(x*2)\n\nwith Bar(1) as bar:\n foo(bar, bar.x)\n\n<\/code><\/pre>\n<p>but then I would have to pass around <code>bar<\/code> everywhere.<\/p>\n<p>As a result I'm trying to see if there's a way to access the <code>with<\/code> context<\/p>\n<p>Note: The real world application of this is ensuring that <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.pyfunc.html#mlflow.pyfunc.log_model\" rel=\"nofollow noreferrer\"><code>mlflow.pyfunc.log_model<\/code><\/a> is called within an <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.ActiveRun\" rel=\"nofollow noreferrer\"><code>mlflow.ActiveRun<\/code><\/a> context, or it leaves an <code>ActiveRun<\/code> open, causing problems later on<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1644973389773,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":77,
        "Owner_creation_time":1473193743197,
        "Owner_last_access_time":1657404267333,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":617,
        "Owner_up_votes":21,
        "Owner_down_votes":2,
        "Owner_views":75,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Here's an ugly way to do it: global state.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class Bar:\n    active = 0\n    def __init__(self, x):\n        self.x = x\n    def __enter__(self):\n        Bar.active += 1\n        return self\n    def __exit__(self, *a, **k):\n        Bar.active -= 1\n\nfrom functools import wraps\n\ndef assert_bar(func):\n    @wraps(func)\n    def wrapped(*vargs, **kwargs):\n        if Bar.active &lt;= 0:\n            # raises even if asserts are disabled\n            raise AssertionError()\n        return func(*vargs, **kwargs)\n    return wrapped\n<\/code><\/pre>\n<p>Unfortunately I don't think there is any non-ugly way to do it. If you aren't going to pass around a <code>Bar<\/code> instance yourself then you must rely on some state existing somewhere else to tell you that a <code>Bar<\/code> instance exists and is currently being used as a context manager.<\/p>\n<p>The only way you can avoid that global state is to store the state in the instance, which means the decorator needs to be an instance method and the instance needs to exist before the function is declared:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from functools import wraps\n\nclass Bar:\n    def __init__(self, x):\n        self.x = x\n        self.active = 0\n    def __enter__(self):\n        self.active += 1\n        return self\n    def __exit__(self, *a, **k):\n        self.active -= 1\n    def assert_this(self, func):\n        @wraps(func)\n        def wrapped(*vargs, **kwargs):\n            if self.active &lt;= 0:\n                raise AssertionError()\n            return func(*vargs, **kwargs)\n        return wrapped\n\nbar = Bar(1)\n\n@bar.assert_this\ndef foo(x):\n    print(x + 1)\n\nwith bar:\n    foo(1)\n<\/code><\/pre>\n<p>This is still &quot;global state&quot; in the sense that the function <code>foo<\/code> now holds a reference to the <code>Bar<\/code> instance that holds the state. But it may be more palatable if <code>foo<\/code> is only ever going to be a local function.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1644974017813,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1644974478490,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71135260",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72780610,
        "Question_title":"MLFlow in container is not mapped to host port",
        "Question_body":"<p>I am learning about MLFlow and Docker Containers.\nI created an ubuntu container and mapped port 5001 of the host to 5000 of the container.<\/p>\n<pre><code>docker run -it -p 5001:5000 -v D:\\Docker\\mlflow:\/home --name mlflow ubuntu:18.04 bash\n<\/code><\/pre>\n<p>Inside the container, I installed the mlflow using pip<\/p>\n<pre><code>pip install mlflow\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/mmQVx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mmQVx.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>When I run the mlflow UI it's running but I can't access it from my host PC (localhost:5001) is not working.<\/p>\n<p>Did I do any mistakes anywhere?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656388899483,
        "Question_score":1,
        "Question_tags":"docker|mlflow",
        "Question_view_count":51,
        "Owner_creation_time":1539781583973,
        "Owner_last_access_time":1663923376093,
        "Owner_location":"Bangkok, Thailand",
        "Owner_reputation":434,
        "Owner_up_votes":141,
        "Owner_down_votes":3,
        "Owner_views":77,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The problem is that you are starting the server on <code>127.0.0.1<\/code> and the port mapping was not pointing to this interface (socket hang up). Starting it on all interfaces <code>0.0.0.0<\/code> works.<\/p>\n<p>You should just run this command in the container.<\/p>\n<pre><code>mlflow ui -h 0.0.0.0\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1657057254010,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72780610",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":55822637,
        "Question_title":"Is there a way to get log the descriptive stats of a dataset using MLflow?",
        "Question_body":"<p>Is there a way to get log the descriptive stats of a dataset using MLflow? If any could you please share the details?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1556081529530,
        "Question_score":3,
        "Question_tags":"python|mlflow",
        "Question_view_count":4592,
        "Owner_creation_time":1411361217027,
        "Owner_last_access_time":1662192778187,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":569,
        "Owner_up_votes":41,
        "Owner_down_votes":3,
        "Owner_views":123,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Generally speaking you can log arbitrary output from your code using the mlflow_log_artifact() function.  From <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifact\" rel=\"noreferrer\">the docs<\/a>:<\/p>\n<blockquote>\n<p><strong>mlflow.log_artifact(local_path, artifact_path=None)<\/strong>\nLog a local file or directory as an artifact of the currently active run.<\/p>\n<\/blockquote>\n<blockquote>\n<p><strong>Parameters:<\/strong><br \/>\n<em>local_path<\/em> \u2013 Path to the file to write.\n<em>artifact_path<\/em> \u2013 If provided, the directory in artifact_uri to write to.<\/p>\n<\/blockquote>\n<p>As an example, say you have your statistics in a pandas dataframe, <code>stat_df<\/code>.<\/p>\n<pre><code>## Write csv from stats dataframe\nstat_df.to_csv('dataset_statistics.csv')\n\n## Log CSV to MLflow\nmlflow.log_artifact('dataset_statistics.csv')\n<\/code><\/pre>\n<p>This will show up under the artifacts section of this MLflow run in the Tracking UI.  If you explore the docs further you'll see that you can also log an entire directory and the objects therein.  In general, MLflow provides you a lot of flexibility - anything you write to your file system you can track with MLflow.  Of course that doesn't mean you should. :)<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1557279162457,
        "Answer_score":9.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1611635690617,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55822637",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62664183,
        "Question_title":"MLflow: find model version with best metric using python code",
        "Question_body":"<p>I am trying to use API workflow (python code) to find a model version that has the best metric (for instance, \u201caccuracy\u201d) among several model versions. I understand we can use web UI to do so, but I would love to write python code to achieve this. Could someone help me?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593542659820,
        "Question_score":2,
        "Question_tags":"python|mlflow",
        "Question_view_count":445,
        "Owner_creation_time":1419619351727,
        "Owner_last_access_time":1644268036150,
        "Owner_location":null,
        "Owner_reputation":125,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":null,
        "Answer_body":"<pre><code>import mlflow \nclient = mlflow.tracking.MlflowClient()\nruns = client.search_runs(&quot;my_experiment_id&quot;, &quot;&quot;, order_by=[&quot;metrics.rmse DESC&quot;], max_results=1)\nbest_run = runs[0]\n<\/code><\/pre>\n<p><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.search_runs\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.search_runs<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1595480605940,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62664183",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72780102,
        "Question_title":"Treatment of Existing Models in a Stage When Transitioning Models",
        "Question_body":"<p>With the MLflow client library, it is possible to transition models through stages as shown below,<\/p>\n<pre><code>client = MlflowClient()\nclient.transition_model_version_stage(\n    name=&quot;sk-learn-random-forest-reg-model&quot;,\n    version=3,\n    stage=&quot;Production&quot;\n)\n<\/code><\/pre>\n<p>Upon doing some testing, I noticed that this does not in any way affect the model(s) that are currently in the stage that the model was transitioned to.<\/p>\n<p>For example, let's say version 2 of a given model is in Production. This will remain tagged as a Production model, even if I were to move version 3 to Production as well.<\/p>\n<p>Is there any way that I can control what happens to models that exist in a stage when making transitions using the above code?<\/p>\n<p>Basically, I only want one version of a model to be in a given stage at a time.<\/p>\n<p>This functionality is available when transitioning models through the Databricks UI,\n<a href=\"https:\/\/i.stack.imgur.com\/aycvC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aycvC.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656383133240,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":26,
        "Owner_creation_time":1521856385820,
        "Owner_last_access_time":1664037995903,
        "Owner_location":"Sri Lanka",
        "Owner_reputation":820,
        "Owner_up_votes":389,
        "Owner_down_votes":1,
        "Owner_views":165,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I have missed the <code>archive_existing_versions=True<\/code> that comes with the <code>transition_model_version_stage<\/code> function.<\/p>\n<p>This flag defaults to <code>False<\/code>.<\/p>\n<p>The documentation is available here,\n<br>\n<a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.transition_model_version_stage\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.transition_model_version_stage<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1656613905027,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72780102",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70445997,
        "Question_title":"MLflow run within a docker container - Running with \"docker_env\" in MLflow project file",
        "Question_body":"<p>We are trying to develop an MLflow pipeline. We have our developing environment in a series of dockers (no local python environment &quot;whatsoever&quot;). This means that we have set up a docker container with MLflow and all requirements necessary to run pipelines. The issue we have is that when we write our MLflow project file we need to use &quot;docker_env&quot; to specify the environment. This figure illustrates what we want to achieve:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1tLuw.jpg\" rel=\"nofollow noreferrer\">MLflow run dind<\/a><\/p>\n<p>MLflow inside the docker needs to access the docker daemon\/service so that it can either use the &quot;docker-image&quot; in the MLflow project file or pull it from docker hub. We are aware of the possibility of using &quot;conda_env&quot; in the MLflow project file but wish to avoid this.<\/p>\n<p>Our question is,<\/p>\n<p>Do we need to set some sort of &quot;docker in docker&quot; solution to achieve our goal?<\/p>\n<p>Is it possible to set up the docker container in which MLflow is running so that it can access the &quot;host machine&quot; docker daemon?<\/p>\n<p>I have been all over Google and MLflow's documentation but I can seem to find anything that can guide us. Thanks a lot in advance for any help or pointers!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1640160788793,
        "Question_score":1,
        "Question_tags":"docker|mlflow|docker-in-docker",
        "Question_view_count":779,
        "Owner_creation_time":1546431264350,
        "Owner_last_access_time":1663848875057,
        "Owner_location":"Norway",
        "Owner_reputation":31,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1640184686953,
        "Answer_body":"<p>I managed to create my pipeline using docker and docker_env in MLflow. It is not necessary to run d-in-d, the &quot;sibling approach&quot; is sufficient. This approach is described here:<\/p>\n<p><a href=\"https:\/\/jpetazzo.github.io\/2015\/09\/03\/do-not-use-docker-in-docker-for-ci\/\" rel=\"nofollow noreferrer\">https:\/\/jpetazzo.github.io\/2015\/09\/03\/do-not-use-docker-in-docker-for-ci\/<\/a><\/p>\n<p>and it is the preferred method to avoid d-in-d.<\/p>\n<p>One needs to be very careful when mounting volumes within the primary and secondary docker environments: all volume mounts happen in the host machine.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1640385689187,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70445997",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65035488,
        "Question_title":"Running mlflow as a systemd service - gunicorn not found",
        "Question_body":"<p>I am trying to run a mlflow tracking server that is installed inside of a virtualenv as a systemd service on Ubuntu 20.04 but I am getting an error indicating that it is unable to find gunicorn. Here is my journal<\/p>\n<pre><code>nov 27 10:37:17 Atrium-Power mlflow[81375]: Traceback (most recent call last):\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/bin\/mlflow&quot;, line 8, in &lt;module&gt;\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     sys.exit(cli())\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 829, in __call__\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     return self.main(*args, **kwargs)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 782, in main\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     rv = self.invoke(ctx)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 1259, in invoke\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     return _process_result(sub_ctx.command.invoke(sub_ctx))\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 1066, in invoke\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     return ctx.invoke(self.callback, **ctx.params)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 610, in invoke\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     return callback(*args, **kwargs)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/mlflow\/cli.py&quot;, line 392, in server\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     _run_server(\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/mlflow\/server\/__init__.py&quot;, line 138, in _run_server\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     exec_cmd(full_command, env=env_map, stream_output=True)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/mlflow\/utils\/process.py&quot;, line 34, in exec_cmd\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     child = subprocess.Popen(\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/subprocess.py&quot;, line 947, in __init__\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     self._execute_child(args, executable, preexec_fn, close_fds,\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/subprocess.py&quot;, line 1819, in _execute_child\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     raise child_exception_type(errno_num, err_msg, err_filename)\nnov 27 10:37:17 Atrium-Power mlflow[81375]: FileNotFoundError: [Errno 2] No such file or directory: 'gunicorn'\n<\/code><\/pre>\n<p>and my systemd is this:<\/p>\n<pre><code>[Unit]\nStartLimitBurst=5\nStartLimitIntervalSec=33\n\n[Service]\nUser=praxasense\nWorkingDirectory=\/home\/praxasense\nRestart=always\nRestartSec=5\nExecStart=\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/bin\/mlflow server --port 3569 --backend-store-uri .mlruns\n\n[Install]\nWantedBy=multi-user.target\n<\/code><\/pre>\n<p>The strange thing is that if I run the command from <code>ExecStart<\/code> in my terminal it works fine in fish shell, but not in bash, <em>but<\/em> if I do <code>conda activate mlflow-server<\/code> and then do <code>mlflow ...<\/code> it <em>does<\/em> work. As far as I understood the Python interpreter should be aware of it's virtual environment and so it should work as I tried it, but apparently I am missing something that makes it not able to find the gunicon package, which is obviously there.<\/p>\n<p>Any ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1606471859393,
        "Question_score":1,
        "Question_tags":"python|systemd|ubuntu-20.04|mlflow",
        "Question_view_count":1018,
        "Owner_creation_time":1314532759333,
        "Owner_last_access_time":1662567260447,
        "Owner_location":"Rotterdam, Netherlands",
        "Owner_reputation":2732,
        "Owner_up_votes":414,
        "Owner_down_votes":15,
        "Owner_views":609,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Try adding the venv's bin path to the environment that systemd runs in:<\/p>\n<pre><code>[Service]\n...\nEnvironment=&quot;PATH=\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/bin&quot;\n...\n<\/code><\/pre>\n<p>I also recommend setting <code>KillMode=mixed<\/code>, since MLFlow will spawn gunicorn instances that won't be terminated if you terminate the service otherwise. <code>mixed<\/code> means that child processes will also be terminated.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1607442042567,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65035488",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61351024,
        "Question_title":"Kubernetes MLflow Service Pod Connection",
        "Question_body":"<p>I have deployed a build of mlflow to a pod in my kubernetes cluster. I'm able to port forward to the mlflow ui, and now I'm attempting to test it. To do this, I am running the following test on a jupyter notebook that is running on another pod in the same cluster.<\/p>\n<pre><code>import mlflow\n\nprint(&quot;Setting Tracking Server&quot;)\ntracking_uri = &quot;http:\/\/mlflow-tracking-server.default.svc.cluster.local:5000&quot;\n\nmlflow.set_tracking_uri(tracking_uri)\n\nprint(&quot;Logging Artifact&quot;)\nmlflow.log_artifact('\/home\/test\/mlflow-example-artifact.png')\n\nprint(&quot;DONE&quot;)\n<\/code><\/pre>\n<p>When I run this though, I get<\/p>\n<pre><code>ConnectionError: HTTPConnectionPool(host='mlflow-tracking-server.default.svc.cluster.local', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get? (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n<\/code><\/pre>\n<p>The way I have deployed the mlflow pod is shown below in the yaml and docker:<\/p>\n<p>Yaml:<\/p>\n<pre><code>---\napiVersion: apps\/v1\nkind: Deployment\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: mlflow-tracking-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mlflow-tracking-server\n    spec:\n      containers:\n      - name: mlflow-tracking-server\n        image: &lt;ECR_IMAGE&gt;\n        ports:\n        - containerPort: 5000\n        env:\n        - name: AWS_MLFLOW_BUCKET\n          value: &lt;S3_BUCKET&gt;\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_SECRET_ACCESS_KEY\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\n  labels:\n    app: mlflow-tracking-server\n  annotations:\n    service.beta.kubernetes.io\/aws-load-balancer-type: nlb\nspec:\n  externalTrafficPolicy: Local\n  type: LoadBalancer\n  selector:\n    app: mlflow-tracking-server\n  ports:\n    - name: http\n      port: 5000\n      targetPort: http\n<\/code><\/pre>\n<p>While the dockerfile calls a script that executes the mlflow server command: <code>mlflow server --default-artifact-root ${AWS_MLFLOW_BUCKET} --host 0.0.0.0 --port 5000<\/code>, I cannot connect to the service I have created using that mlflow pod.<\/p>\n<p>I have tried using the tracking uri <code>http:\/\/mlflow-tracking-server.default.svc.cluster.local:5000<\/code>, I've tried using the service EXTERNAL-IP:5000, but everything I tried cannot connect and log using the service. Is there anything that I have missed in deploying my mlflow server pod to my kubernetes cluster?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":6,
        "Question_creation_time":1587495286493,
        "Question_score":2,
        "Question_tags":"kubernetes|kubernetes-service|mlflow",
        "Question_view_count":855,
        "Owner_creation_time":1417012835813,
        "Owner_last_access_time":1628782664573,
        "Owner_location":null,
        "Owner_reputation":945,
        "Owner_up_votes":35,
        "Owner_down_votes":1,
        "Owner_views":148,
        "Question_last_edit_time":1599477403817,
        "Answer_body":"<p>Your <strong>mlflow-tracking-server<\/strong> service should have <em>ClusterIP<\/em> type, not <em>LoadBalancer<\/em>. <\/p>\n\n<p>Both pods are inside the same Kubernetes cluster, therefore, there is no reason to use <em>LoadBalancer<\/em> Service type.<\/p>\n\n<blockquote>\n  <p>For some parts of your application (for example, frontends) you may want to expose a Service onto an external IP address, that\u2019s outside of your cluster.\n  Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default is ClusterIP.<\/p>\n  \n  <p>Type values and their behaviors are:<\/p>\n  \n  <ul>\n  <li><p><strong>ClusterIP<\/strong>: Exposes the Service on a cluster-internal IP. Choosing this\n  value makes the Service only reachable from within the cluster. This\n  is the default ServiceType. <\/p><\/li>\n  <li><p><strong>NodePort<\/strong>: Exposes the Service on each Node\u2019s IP at a static port (the NodePort). A > ClusterIP Service, to which the NodePort Service routes, is automatically created. You\u2019ll > be able to contact the NodePort Service, from outside the cluster, by\n  requesting :. <\/p><\/li>\n  <li><strong>LoadBalancer<\/strong>: Exposes the Service\n  externally using a cloud provider\u2019s load balancer. NodePort and\n  ClusterIP Services, to which the external load balancer routes, are\n  automatically created. <\/li>\n  <li><strong>ExternalName<\/strong>: Maps the Service to the contents\n  of the externalName field (e.g. foo.bar.example.com), by returning a\n  CNAME record with its value. No proxying of any kind is set up.<\/li>\n  <\/ul>\n  \n  <p><a href=\"https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#publishing-services-service-types\" rel=\"nofollow noreferrer\">kubernetes.io<\/a><\/p>\n<\/blockquote>",
        "Answer_comment_count":6.0,
        "Answer_creation_time":1587498737657,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1587499353943,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61351024",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72518344,
        "Question_title":"Logging and Fetching Run Parameters in AzureML",
        "Question_body":"<p>I am able to log and fetch metrics to AzureML using Run.log, however, I need a way to also log run parameters, like Learning Rate, or Momentum. I can't seem to find anything in the AzureML Python SDK documentation to achieve this. However, if I use MLflow's mlflow.log_param, I am able to log parameters, and they even nicely show up on the AzureML Studio Dashboard (bottom right of the image):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Again, I am able to fetch this using MLflow's get_params() function, but I can't find a way to do this using just AzureML's Python SDK. Is there a way to do this directly using <code>azureml<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654521747343,
        "Question_score":0,
        "Question_tags":"azure|mlflow|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":73,
        "Owner_creation_time":1554497484963,
        "Owner_last_access_time":1664005308093,
        "Owner_location":"Hyderabad, Telangana, India",
        "Owner_reputation":438,
        "Owner_up_votes":15,
        "Owner_down_votes":2,
        "Owner_views":120,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The retrieving of log run parameters like <strong>Learning Rate, or Momentum<\/strong> is not possible with <strong>AzureML<\/strong> alone. Because it was tied with <strong>MLFlow<\/strong> and <strong>azureml-core<\/strong>. without those two involvements, we cannot retrieve the log run parameters.<\/p>\n<pre><code>pip install azureml-core mlflow azureml-mlflow\n<\/code><\/pre>\n<p>Need to install these three for getting run parameters. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics\" rel=\"nofollow noreferrer\">Link<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1656998954310,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72518344",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62320331,
        "Question_title":"Sagemaker API to list Hyperparameters",
        "Question_body":"<p>I'm currently trying to implement MLFlow Tracking into my training pipeline and would like to log the hyperparameters of my hyperparameter Tuning of each training job.<\/p>\n\n<p>Does anyone know, how to pull the list of hyperparameters that can be seen on the sagemaker training job interface (on the AWS console)? Is there any other smarter way to list how models perform in comparison in Sagemaker (and displayed)?<\/p>\n\n<p>I would assume there must be an easy and Pythonic way to do this (either boto3 or the sagemaker api) to get this data. I wasn't able to find it in Cloudwatch.<\/p>\n\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1591864702897,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|mlflow",
        "Question_view_count":484,
        "Owner_creation_time":1469720117217,
        "Owner_last_access_time":1648482744487,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":"<p>there is indeed a rather pythonic way in the SageMaker python SDK:<\/p>\n\n<pre><code>tuner = sagemaker.tuner.HyperparameterTuner.attach('&lt; your tuning jobname&gt;')\n\nresults = tuner.analytics().dataframe()  # all your tuning metadata, in pandas!\n<\/code><\/pre>\n\n<p>See full example here <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1592173650467,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62320331",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60597319,
        "Question_title":"Running MLFlow on GCP VM",
        "Question_body":"<p>I have installed mlflow on GCP VM instance, \nnow I want to access mlflow UI with external IP.\nI tried setting up a firewall rule and opening the default port for mlflow, but not able to access it.\nCan someone give step by step process for just running mlflow on VM instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1583744387613,
        "Question_score":4,
        "Question_tags":"python|google-cloud-platform|mlflow",
        "Question_view_count":1537,
        "Owner_creation_time":1451124057623,
        "Owner_last_access_time":1664026764720,
        "Owner_location":"India",
        "Owner_reputation":736,
        "Owner_up_votes":69,
        "Owner_down_votes":2,
        "Owner_views":234,
        "Question_last_edit_time":1583832420660,
        "Answer_body":"<p>I've decided to check on my test VM and run mlflow server on GCE VM. Have a look at my steps below:<\/p>\n\n<ol>\n<li>create VM instance based on Ubuntu Linux 18.04 LTS<\/li>\n<li><p><a href=\"https:\/\/www.mlflow.org\/docs\/latest\/quickstart.html\" rel=\"noreferrer\">install MLflow<\/a>:<\/p>\n\n<pre><code>$ sudo apt update\n$ sudo apt upgrade\n$ cd ~\n$ git clone https:\/\/github.com\/mlflow\/mlflow\n$ cd mlflow\n$ sudo apt install python3-pip\n$ pip3 install mlflow\n$ python3 setup.py build\n$ sudo python3 setup.py install\n$ mlflow --version\nmlflow, version 1.7.1.dev0\n<\/code><\/pre><\/li>\n<li><p>run mlflow server on internal IP of VM instance (default 127.0.0.1):<\/p>\n\n<pre><code>$ ifconfig \nens4: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1460\ninet 10.XXX.15.XXX  netmask 255.255.255.255  broadcast 0.0.0.0\n...\n\n$ mlflow server --host 10.XXX.15.XXX\n[2020-03-09 15:05:50 +0000] [8631] [INFO] Starting gunicorn 20.0.4\n[2020-03-09 15:05:50 +0000] [8631] [INFO] Listening at: http:\/\/10.128.15.211:5000 (8631)\n[2020-03-09 15:05:50 +0000] [8631] [INFO] Using worker: sync\n[2020-03-09 15:05:50 +0000] [8634] [INFO] Booting worker with pid: 8634\n[2020-03-09 15:05:51 +0000] [8635] [INFO] Booting worker with pid: 8635\n[2020-03-09 15:05:51 +0000] [8636] [INFO] Booting worker with pid: 8636\n[2020-03-09 15:05:51 +0000] [8638] [INFO] Booting worker with pid: 8638\n<\/code><\/pre><\/li>\n<li><p>check from VM instance (from second connection):<\/p>\n\n<pre><code>$ curl -I http:\/\/10.XXX.15.XXX:5000\nHTTP\/1.1 200 OK\nServer: gunicorn\/20.0.4\nDate: Mon, 09 Mar 2020 15:06:08 GMT\nConnection: close\nContent-Length: 853\nContent-Type: text\/html; charset=utf-8\nLast-Modified: Mon, 09 Mar 2020 14:57:11 GMT\nCache-Control: public, max-age=43200\nExpires: Tue, 10 Mar 2020 03:06:08 GMT\nETag: \"1583765831.3202355-853-3764264575\"\n<\/code><\/pre><\/li>\n<li><p><a href=\"https:\/\/cloud.google.com\/vpc\/docs\/add-remove-network-tags\" rel=\"noreferrer\">set network tag<\/a> <code>mlflow-server<\/code> <\/p><\/li>\n<li><p><a href=\"https:\/\/cloud.google.com\/vpc\/docs\/using-firewalls#creating_firewall_rules\" rel=\"noreferrer\">create firewall rule<\/a> to allow access on port 5000<\/p>\n\n<pre><code>$ gcloud compute --project=test-prj firewall-rules create mlflow-server --direction=INGRESS --priority=999 --network=default --action=ALLOW --rules=tcp:5000 --source-ranges=0.0.0.0\/0 --target-tags=mlflow-server\n<\/code><\/pre><\/li>\n<li><p>check from on-premises Linux machine <code>nmap -Pn 35.225.XXX.XXX<\/code><\/p>\n\n<pre><code>Starting Nmap 7.80 ( https:\/\/nmap.org ) at 2020-03-09 16:20 CET\nNmap scan report for 74.123.225.35.bc.googleusercontent.com (35.225.XXX.XXX)\nHost is up (0.20s latency).\nNot shown: 993 filtered ports\nPORT     STATE  SERVICE\n...\n5000\/tcp open   upnp\n...\n<\/code><\/pre><\/li>\n<li><p>go to web browser <a href=\"http:\/\/35.225.XXX.XXX:5000\/\" rel=\"noreferrer\">http:\/\/35.225.XXX.XXX:5000\/<\/a><\/p><\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/u2aFt.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/u2aFt.png\" alt=\"mlflow\"><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1583767598370,
        "Answer_score":5.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60597319",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62778020,
        "Question_title":"Embarrassingly parallel hyperparameter search via Azure + DataBricks + MLFlow",
        "Question_body":"<p>Conceptual question.  My company is pushing Azure + DataBricks.  I am trying to understand where this can take us.<\/p>\n<p>I am porting some work I've done locally to the Azure + Databricks platform.  I want to run an experiment with a large number of hyperparameter combinations using Azure + Databricks + MLfLow.  I am using PyTorch to implement my models.<\/p>\n<p>I have a cluster with 8 nodes.  I want to kick off the parameter search across all of the nodes in an embarrassingly parallel manner (one run per node, running independently).  Is this as simple as creating a MLflow project and then using the mlflow.projects.run command for each hyperparameter combination and Databricks + MLflow will take care of the rest?<\/p>\n<p>Is this technology capable of this?  I'm looking for some references I could use to make this happen.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1594133526057,
        "Question_score":0,
        "Question_tags":"databricks|azure-databricks|mlflow",
        "Question_view_count":262,
        "Owner_creation_time":1328818302350,
        "Owner_last_access_time":1663339031437,
        "Owner_location":"Sioux City, IA",
        "Owner_reputation":325,
        "Owner_up_votes":254,
        "Owner_down_votes":4,
        "Owner_views":51,
        "Question_last_edit_time":1594134725200,
        "Answer_body":"<p>The short answer is yes, it's possible, but won't be exactly as easy as running a single mlflow command. You can paralelize single-node workflows using spark Python UDFs, a good example of this is this <a href=\"https:\/\/pages.databricks.com\/rs\/094-YMS-629\/images\/Fine-Grained-Time-Series-Forecasting.html?_ga=2.64430959.1760852900.1593769579-972789996.1561118598\" rel=\"nofollow noreferrer\">notebook<\/a><\/p>\n<p>I'm not sure if this will work with pytorch, but there is hyperopt library that lets you parallelize search across parameters using Spark - it's integrated with mlflow and available in databricks ML runtime. I've been using it only with scikit-learn, but it may be <a href=\"https:\/\/docs.databricks.com\/applications\/machine-learning\/automl\/hyperopt\/hyperopt-model-selection.html\" rel=\"nofollow noreferrer\">worth checking out<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1594986774773,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62778020",
        "Question_exclusive_tag":"MLFlow"
    }
]