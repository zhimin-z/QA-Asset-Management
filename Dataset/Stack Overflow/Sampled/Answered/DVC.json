[
    {
        "Question_id":52871630,
        "Question_title":"Resolving paths in mingw fails with Data Version Control",
        "Question_body":"<p>I am following the <a href=\"https:\/\/blog.dataversioncontrol.com\/data-version-control-tutorial-9146715eda46\" rel=\"nofollow noreferrer\">tutorial<\/a> about <a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"nofollow noreferrer\">Data Version Control<\/a> using <code>mingw32<\/code> on Windows 7.<\/p>\n\n<p>I am getting very strange error when I try to use <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/run\" rel=\"nofollow noreferrer\">run<\/a>:<\/p>\n\n<pre><code>$ dvc run -v echo \"hello\"\nDebug: updater is not old enough to check for updates\nDebug: PRAGMA user_version;\nDebug: fetched: [(2,)]\nDebug: CREATE TABLE IF NOT EXISTS state (inode INTEGER PRIMARY KEY, mtime TEXT NOT NULL, md5 TEXT NOT NULL, timestamp TEXT NOT NULL)\nDebug: CREATE TABLE IF NOT EXISTS state_info (count INTEGER)\nDebug: CREATE TABLE IF NOT EXISTS link_state (path TEXT PRIMARY KEY, inode INTEGER NOT NULL, mtime TEXT NOT NULL)\nDebug: INSERT OR IGNORE INTO state_info (count) SELECT 0 WHERE NOT EXISTS (SELECT * FROM state_info)\nDebug: PRAGMA user_version = 2;\nRunning command:\n        echo hello\n\/c: \/c: Is a directory\nDebug: SELECT count from state_info WHERE rowid=1\nDebug: fetched: [(1,)]\nDebug: UPDATE state_info SET count = 1 WHERE rowid = 1\nError: Traceback (most recent call last):\n  File \"dvc\\command\\run.py\", line 18, in run\n  File \"dvc\\project.py\", line 265, in run\n  File \"dvc\\stage.py\", line 435, in run\nStageCmdFailedError: Stage 'Dvcfile' cmd echo hello failed\n\nError: Failed to run command: Stage 'Dvcfile' cmd echo hello failed\n<\/code><\/pre>\n\n<h3>Question:<\/h3>\n\n<p>Where does the <code>\/c: \/c: Is a directory<\/code> come from?  How can I fix it? <\/p>\n\n<h3>My findings<\/h3>\n\n<ol>\n<li><p>I supposed that it was resolving path to echo, but ech is a builtin.<\/p>\n\n<pre><code>$ type echo\necho is a shell builtin\n<\/code><\/pre>\n\n<p>I tried also with <code>exit<\/code> and <code>cd<\/code> but I am getting the same error.<\/p><\/li>\n<li><p>Calling commands without dvc works fine.<\/p><\/li>\n<li><p><code>dvc<\/code> with <code>--no-exec<\/code> flag works fine, but when later executed with <code>repro<\/code> gives the same error. <\/p><\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1539857086453,
        "Question_score":4,
        "Question_tags":"windows|mingw|dvc",
        "Question_view_count":109,
        "Owner_creation_time":1508231047660,
        "Owner_last_access_time":1663539768267,
        "Owner_location":"Krak\u00f3w, Poland",
        "Owner_reputation":860,
        "Owner_up_votes":658,
        "Owner_down_votes":18,
        "Owner_views":118,
        "Question_last_edit_time":1539881078543,
        "Answer_body":"<p>I'm one of the dvc developers. Similar error has affected dvc running on cygwin. We've released a fix for it in <code>0.20.0<\/code>. Please upgrade.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1540629268433,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52871630",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56818930,
        "Question_title":"\"dvc push\" after several local commits",
        "Question_body":"<p>I work on a project with DVC (Data version control). Let's say I make a lot of local commits. Something like this:<\/p>\n\n<pre><code># make changes for experiment 1\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 1\"\n\n# make changes for experiment 2\n# which change both code and data\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 2\"\n\n# make changes for experiment 3\n# which change both code and data\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 3\"\n\n# Finally I'm done\n# push changes:\ndvc push\ngit push\n<\/code><\/pre>\n\n<p>However there is one problem: <code>dvc push<\/code> will only push data from experiment 3. Is there any way to push data from all local commits (i.e. starting from the first commit diverged from remote branch)?<\/p>\n\n<p>Currently I see two options:<\/p>\n\n<ol>\n<li>Tag each commit and push it with <code>dvc push -T<\/code><\/li>\n<li>After \"expermient 3\" commit execute <code>git checkout commit-hash &amp;&amp; dvc push<\/code> for all local commits not yet pushed to remote.<\/li>\n<\/ol>\n\n<p>Both these options seem cumbersome and error-prone. Is there any better way to do it?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1561823734517,
        "Question_score":2,
        "Question_tags":"git|dvc",
        "Question_view_count":916,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":null,
        "Answer_body":"<p>@NShiny, there is a related ticket:<\/p>\n\n<p><a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1691\" rel=\"nofollow noreferrer\">support push\/pull\/metrics\/gc, etc across different commits<\/a>.<\/p>\n\n<p>Please, give it a vote so that we know how to prioritize it.<\/p>\n\n<p>As a workaround, I would recommend to run <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/install\" rel=\"nofollow noreferrer\"><code>dvc install<\/code><\/a>. It installs a <code>pre-push<\/code> GIt hook and runs <code>dvc push<\/code> automatically:<\/p>\n\n<pre><code>Git pre-push hook executes dvc push before git push to upload files and directories under DVC control to remote.\n<\/code><\/pre>\n\n<p>It means, though you need to run <code>git push<\/code> after every <code>git commit<\/code> :(<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1561842649417,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1561847539150,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56818930",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73651050,
        "Question_title":"DVC imports authentication to blob storage",
        "Question_body":"<p>I'm using <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> to track and version data that is stored locally on the file system and in Azure Blob storage.<\/p>\n<p>My setup is as follows:<\/p>\n<ul>\n<li><p><code>DataProject1<\/code>, it uses a local file location as a remote therefore it does not require any authentication.<\/p>\n<\/li>\n<li><p><code>DataProject2<\/code>, it uses Azure Blob Storage as a remote, it is using sas_token for authentication, I can push pull data to\/from the remote when I'm within this project.<\/p>\n<\/li>\n<li><p><code>MLProject<\/code>, it uses dvc import to import data from <code>DataProjec1<\/code> and <code>DataProject2<\/code>.<\/p>\n<\/li>\n<\/ul>\n<p>When I run the import with the command against <code>DataProject1<\/code> everything works fine:<\/p>\n<p><code>dvc import -o 'data\/project1' 'https:\/\/company.visualstudio.com\/DefaultCollection\/proj\/_git\/DataProject1' 'data\/project1'<\/code> - Successful<\/p>\n<p>Howevever when I run a similar command against <code>DataProject2<\/code> the command fails:<\/p>\n<p><code>dvc import -o 'data\/project2' 'https:\/\/company.visualstudio.com\/DefaultCollection\/proj\/_git\/DataProject2' 'data\/project2'<\/code> - it fails with:<\/p>\n<blockquote>\n<p>ERROR: unexpected error - Operation returned an invalid status 'This\nrequest is not authorized to perform this operation using this\npermission.'  ErrorCode:AuthorizationPermissionMismatch.<\/p>\n<\/blockquote>\n<p>I would like to configure the <code>dvc import<\/code> so that I can set the required <code>sas_token<\/code> but I cannot find a way to do that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1662648529560,
        "Question_score":1,
        "Question_tags":"python|dvc|dvc-import",
        "Question_view_count":34,
        "Owner_creation_time":1248452771430,
        "Owner_last_access_time":1664006575380,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":3317,
        "Owner_up_votes":466,
        "Owner_down_votes":8,
        "Owner_views":296,
        "Question_last_edit_time":1662650337133,
        "Answer_body":"<p>This happens since DVC is not using <code>MLProject<\/code>'s config when it clones and does <code>dvc fetch<\/code> in the <code>DataProject2<\/code> during the <code>import<\/code>. And it doesn't know where it can find the token (clearly, it's not in the Git repo, right?).<\/p>\n<p>There are a few ways to specify it: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#--system\" rel=\"nofollow noreferrer\"><code>global\/system<\/code> configs<\/a> and\/or <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#authenticate-with-environment-variables\" rel=\"nofollow noreferrer\">environment variables<\/a>.<\/p>\n<p>To implement the first option:<\/p>\n<p>On a machine where you do <code>dvc import<\/code>, you could create a remote in the <code>--global<\/code>, or <code>--system<\/code> configs with the same name and specify the token there. Global config fields will be merged with the config in the <code>DataProject2<\/code> repo when DVC is pulling data to import.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>dvc remote add --global &lt;DataProject2-remote-name&gt; azure:\/\/DataProject2\/storage\ndvc remote modify --global &lt;DataProject2-remote-name&gt; account_name &lt;name&gt;\ndvc remote modify --global &lt;DataProject2-remote-name&gt; sas_token &lt;token&gt;\n<\/code><\/pre>\n<p>The second option:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>export AZURE_STORAGE_SAS_TOKEN='mysecret'\nexport AZURE_STORAGE_ACCOUNT='myaccount'\n<\/code><\/pre>\n<p>Please give it a try, let me know if that works or not.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1662656090387,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73651050",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56285351,
        "Question_title":"Updating tracked dir in DVC",
        "Question_body":"<p>According to <a href=\"https:\/\/dvc.org\/doc\/user-guide\/update-tracked-file\" rel=\"nofollow noreferrer\">this tutorial<\/a> when I update file I should remove file from under DVC control first (i.e. execute <code>dvc unprotect &lt;myfile&gt;.dvc<\/code> or <code>dvc remove &lt;myfile&gt;.dvc<\/code>) and then add it again via <code>dvc add &lt;mifile&gt;<\/code>. However It's not clear if I should apply the same workflow for the directories.<\/p>\n\n<p>I have the directory under DVC control with the following structure:<\/p>\n\n<pre><code>data\/\n    1.jpg\n    2.jpg\n<\/code><\/pre>\n\n<p>Should I run <code>dvc unprotect data<\/code> every time the directory content is updated?<\/p>\n\n<p>More specifically I'm interested if I should run <code>dvc unprotect data<\/code> in the following use cases:<\/p>\n\n<ul>\n<li><strong>New file is added.<\/strong> For example if I put <code>3.jpg<\/code> image in the data dir<\/li>\n<li><strong>File is deleted.<\/strong> For example if I delete <code>2.jpg<\/code> image in the <code>data<\/code> dir<\/li>\n<li><strong>File is updated.<\/strong> For example if I edit <code>1.jpg<\/code> image via graphic editor.<\/li>\n<li>A combination of the previous use cases (i.e. some files are updated, other deleted and new files are added)<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1558667422490,
        "Question_score":4,
        "Question_tags":"dvc",
        "Question_view_count":995,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":1558708772617,
        "Answer_body":"<p>Only when file is updated - i.e. edit <code>1.jpg<\/code> with your editor <strong>AND<\/strong> only if hadrlink or symlink cache type is enabled.<\/p>\n\n<p>Please, check this <a href=\"https:\/\/dvc.org\/doc\/user-guide\/update-tracked-file\" rel=\"nofollow noreferrer\">link<\/a>:<\/p>\n\n<blockquote>\n  <p>updating tracked files has to be carried out with caution to avoid data corruption when the DVC config option cache.type is set to hardlink or\/and symlink<\/p>\n<\/blockquote>\n\n<p>I would strongly recommend reading this document: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/cache-file-linking\" rel=\"nofollow noreferrer\">Performance Optimization for Large Files<\/a> it explains benefits of using hardlinks\/symlinks.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1558674266680,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56285351",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":66409283,
        "Question_title":"updating data in dvc registry from other projects",
        "Question_body":"<p>I have a couple of projects that are using and updating the same data sources. I recently learned about <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries\" rel=\"nofollow noreferrer\">dvc's data registries<\/a>, which sound like a great way of versioning data across these different projects (e.g. scrapers, computational pipelines).<\/p>\n<p>I have put all of the relevant data into <code>data-registry<\/code> and then I imported the relevant files into the scraper project with:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ poetry run dvc import https:\/\/github.com\/username\/data-registry raw\n<\/code><\/pre>\n<p>where <code>raw<\/code> is a directory that stores the scraped data. This seems to have worked properly, but then when I went to build <a href=\"https:\/\/dvc.org\/doc\/start\/data-pipelines\" rel=\"nofollow noreferrer\">a dvc pipeline<\/a> that <em>outputted<\/em> data into a file that was already tracked by dvc, I got an error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ dvc run -n menu_items -d src\/ -o raw\/menu_items\/restaurant.jsonl scrapy crawl restaurant\nERROR: Paths for outs:                                                \n'raw'('raw.dvc')\n'raw\/menu_items\/restaurant.jsonl'('menu_items')\noverlap. To avoid unpredictable behaviour, rerun command with non overlapping outs paths.\n<\/code><\/pre>\n<p>Can someone help me understand what is going on here? <strong>What is the best way to use data registries to share and update data across projects?<\/strong><\/p>\n<p>I would ideally like to update the data-registry with new data from the scraper project and then allow other dependent projects to update their data when they are ready to do so.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1614516713937,
        "Question_score":3,
        "Question_tags":"data-management|dvc",
        "Question_view_count":388,
        "Owner_creation_time":1294268936687,
        "Owner_last_access_time":1661618392827,
        "Owner_location":"Chicago, IL",
        "Owner_reputation":2893,
        "Owner_up_votes":294,
        "Owner_down_votes":1,
        "Owner_views":168,
        "Question_last_edit_time":1614699218993,
        "Answer_body":"<p>When you <code>import<\/code> (or <code>add<\/code>) something into your project, a .dvc file is created with that lists that something (in this case the <code>raw\/<\/code> dir) as an &quot;output&quot;.<\/p>\n<p>DVC doesn't allow overlapping outputs among .dvc files or dvc.yaml stages, meaning that your &quot;menu_items&quot; stage shouldn't write to <code>raw\/<\/code> since it's already under the control of <code>raw.dvc<\/code>.<\/p>\n<p>Can you make a separate directory for the pipeline outputs? E.g. use <code>processed\/menu_items\/restaurant.jsonl<\/code><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1614537291720,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1614698988013,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66409283",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":71155959,
        "Question_title":"How to merge data (CSV) files from multiple branches (Git and DVC)?",
        "Question_body":"<p><strong>Background<\/strong>: In my projects I'm using GIT and <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> to keep track of versions:<\/p>\n<ul>\n<li>GIT - only for source codes<\/li>\n<li>DVC - for dataset, model objects and outputs<\/li>\n<\/ul>\n<p>I'm testing different approaches in separate branches, i.e:<\/p>\n<ul>\n<li>random_forest<\/li>\n<li>neural_network_1<\/li>\n<li>...<\/li>\n<\/ul>\n<p>Typically as an output I'm keeping predictions in csv file with standarised name (i.e.: pred_test.csv). As a consequence in different branches I've different pred_test.csv files. The structure of the file is very simple, it contains two columns:<\/p>\n<ul>\n<li>ID<\/li>\n<li>Prediction<\/li>\n<\/ul>\n<p><strong>Question<\/strong>: What is the best way to merge those prediction files into single big file?<\/p>\n<p>I would like to obtain a file with structure:<\/p>\n<ul>\n<li>ID<\/li>\n<li>Prediction_random_forest<\/li>\n<li>Prediction_neural_network_1<\/li>\n<li>Prediction_...<\/li>\n<\/ul>\n<p>My main issue is how to access files with predictions which are in different branches?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1645092064283,
        "Question_score":0,
        "Question_tags":"git|data-science|dvc",
        "Question_view_count":274,
        "Owner_creation_time":1265742671200,
        "Owner_last_access_time":1658834085253,
        "Owner_location":null,
        "Owner_reputation":2735,
        "Owner_up_votes":190,
        "Owner_down_votes":7,
        "Owner_views":552,
        "Question_last_edit_time":1645132430277,
        "Answer_body":"<p>I would try to use <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\" rel=\"nofollow noreferrer\"><code>dvc get<\/code><\/a> in this case:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc get -o random_forest_pred.csv --rev random_forest . pred_test.csv\n<\/code><\/pre>\n<p>It should bring the <code>pred_test.csv<\/code> from the <code>random_forest<\/code> branch.<\/p>\n<blockquote>\n<p>Mind the <code>.<\/code> before the <code>pred_test.csv<\/code> please, it's needed and it means that &quot;use the current repo&quot;, since <code>dvc get<\/code> could also be used on other repos (e.g. GitHub URL)<\/p>\n<\/blockquote>\n<p>Then I think you could use some CLI or write a script to join the files:<\/p>\n<p><a href=\"https:\/\/unix.stackexchange.com\/questions\/293775\/merging-contents-of-multiple-csv-files-into-single-csv-file\">https:\/\/unix.stackexchange.com\/questions\/293775\/merging-contents-of-multiple-csv-files-into-single-csv-file<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1645113091963,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71155959",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73580703,
        "Question_title":"DVC | Permission denied ERROR: failed to reproduce stage: failed to run: .py, exited with 126",
        "Question_body":"<p>Goal: run <code>.py<\/code> files via. <code>dvc.yaml<\/code>.<\/p>\n<p>There are stages before it, in <code>dvc.yaml<\/code>, that don't produce the error.<\/p>\n<p><code>dvc exp run<\/code>:<\/p>\n<pre><code>(venv) me@ubuntu-pcs:~\/PycharmProjects\/project$ dvc exp run\nStage 'inference' didn't change, skipping\nRunning stage 'load_data':\n&gt; load_data.py\n\/bin\/bash: line 1: load_data.py: Permission denied\nERROR: failed to reproduce 'load_data': failed to run: load_data.py, exited with 126\n<\/code><\/pre>\n<p><code>dvc repro<\/code>:<\/p>\n<pre><code>(venv) me@ubuntu-pcs:~\/PycharmProjects\/project$ dvc repro\nStage 'predict' didn't change, skipping                                                                                                                                                                                                                        \nStage 'evaluate' didn't change, skipping\nStage 'inference' didn't change, skipping\nRunning stage 'load_data':\n&gt; load_data.py\n\/bin\/bash: line 1: load_data.py: Permission denied\nERROR: failed to reproduce 'load_data': failed to run: pdl1_lung_model\/load_data.py, exited with 126\n<\/code><\/pre>\n<hr \/>\n<p><code>dvc doctor<\/code>:<\/p>\n<pre><code>DVC version: 2.10.2 (pip)\n---------------------------------\nPlatform: Python 3.9.12 on Linux-5.15.0-46-generic-x86_64-with-glibc2.35\nSupports:\n        webhdfs (fsspec = 2022.5.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        s3 (s3fs = 2022.5.0, boto3 = 1.21.21)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/nvme0n1p5\nCaches: local\nRemotes: s3\nWorkspace directory: ext4 on \/dev\/nvme0n1p5\nRepo: dvc, git\n<\/code><\/pre>\n<p><code>dvc exp run -v<\/code>:<\/p>\n<p><a href=\"https:\/\/gist.github.com\/danielbellsa\/3f2fe05c1535d494a8677e54cddf684a\" rel=\"nofollow noreferrer\">output.txt<\/a><\/p>\n<p><code>dvc exp run -vv<\/code>:<\/p>\n<p><a href=\"https:\/\/gist.github.com\/danielbellsa\/a124cf28b3f0252556deb90b042b7cec\" rel=\"nofollow noreferrer\">output2.txt<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1662111917277,
        "Question_score":1,
        "Question_tags":"python-3.x|permission-denied|dvc",
        "Question_view_count":77,
        "Owner_creation_time":1631019482980,
        "Owner_last_access_time":1663946675073,
        "Owner_location":null,
        "Owner_reputation":234,
        "Owner_up_votes":708,
        "Owner_down_votes":14,
        "Owner_views":155,
        "Question_last_edit_time":1662368809863,
        "Answer_body":"<h3>Solution 1<\/h3>\n<p><code>.py<\/code> files weren't running as scripts.<\/p>\n<p>They need to be; if you want to run one <code>.py<\/code> file per <code>stage<\/code> in <code>dvc.yaml<\/code>.<\/p>\n<p>To do so, you want to append <strong>Boiler-plate code<\/strong>, at the bottom of each <code>.py<\/code> file.<\/p>\n<pre><code>if __name__ == &quot;__main__&quot;:\n    # invoke primary function() in .py file, w\/ params\n<\/code><\/pre>\n<h3>Solution 2<\/h3>\n<pre><code>chmod 777 ....py\n<\/code><\/pre>\n<h3>Soution 3<\/h3>\n<p>I forgot the <code>python<\/code> in <code>cmd:<\/code><\/p>\n<pre><code>  load_data:\n    cmd: python pdl1_lung_model\/load_data.py\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1662114844133,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1662368753113,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73580703",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":61245284,
        "Question_title":"Is it necessary to commit DVC files from our CI pipelines?",
        "Question_body":"<p>DVC uses git commits to save the experiments and navigate between experiments.<\/p>\n<p>Is it possible to avoid making auto-commits in CI\/CD (to save data artifacts after <code>dvc repro<\/code> in CI\/CD side).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1587023782283,
        "Question_score":6,
        "Question_tags":"git|machine-learning|continuous-integration|dvc|mlops",
        "Question_view_count":1047,
        "Owner_creation_time":1558529684193,
        "Owner_last_access_time":1642243398997,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":79,
        "Owner_up_votes":107,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":1594640021920,
        "Answer_body":"<blockquote>\n  <p>will you make it part of CI pipeline<\/p>\n<\/blockquote>\n\n<p>DVC often serves as a part of MLOps infrastructure. There is a popular <a href=\"https:\/\/martinfowler.com\/articles\/cd4ml.html\" rel=\"noreferrer\">blog post about CI\/CD for ML<\/a> where DVC is used under the hood. <a href=\"https:\/\/blog.codecentric.de\/en\/2020\/01\/remote-training-gitlab-ci-dvc\/\" rel=\"noreferrer\">Another example<\/a> but with GitLab CI\/CD.<\/p>\n\n<blockquote>\n  <p>scenario where you will integrate dvc commit command with CI\n  pipelines?<\/p>\n<\/blockquote>\n\n<p>If you mean <code>git commit<\/code> of DVC files (not <code>dvc commit<\/code>) then yes, you need to commit dvc-files into Git during CI\/CD process. Auto-commit is not the best practice.<\/p>\n\n<p>How to avoid Git commit in CI\/CD:<\/p>\n\n<ol>\n<li>After ML model training in CI\/CD, save changed dvc-files in external storage (for example GitLab artifact\/releases), then get the files to a developer machine and commit there. Users usually write scripts to automate it.<\/li>\n<li>Wait for DVC 1.0 release when <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1234\" rel=\"noreferrer\">run-cache (like build-cache)<\/a> will be implemented. Run-cache makes dvc-files ephemeral and no additional Git commits will be required. Technically, run-cache is an associative storage <code>repo state --&gt; run results<\/code> outside of Git repo (in data remote).<\/li>\n<\/ol>\n\n<p>Disclaimer: I'm one of the creators of DVC.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1587029761967,
        "Answer_score":6.0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61245284",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72665109,
        "Question_title":"dvc.exceptions.CyclicGraphError: Pipeline has a cycle involving: load_extract_save",
        "Question_body":"<pre><code>stages:\n  load_extract_save: \n    cmd: python src\/stage_01_load_extract_save.py --config=config\/config.yaml\n    deps:\n      - config\/config.yaml\n      - src\/utils\/all_utils.py\n      - src\/stage_01_load_extract_save.py\n      - artifacts\/data\n    outs:\n      - artifacts\/data\n      - artifacts\/clean_data\/X.npy\n      - artifacts\/clean_data\/Y.npy\n\n  train_test_split_save:\n    cmd: python src\/stage_02_train_test_split_save.py --config=config\/config.yaml --params=params.yaml\n    deps:\n      - artifacts\/clean_data\/X.npy\n      - artifacts\/clean_data\/Y.npy\n      - src\/utils\/all_utils.py\n      - params.yaml\n      - config\/config.yaml\n      - src\/stage_02_train_test_split_save.py\n    outs:\n      - artifacts\/train_data\/X_train.npy\n      - artifacts\/train_data\/Y_train.npy\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n  \n  train_model:\n    cmd:  python src\/stage_03_train.py --config=config\/config.yaml --params=params.yaml\n    deps:\n      - artifacts\/train_data\/X_train.npy\n      - artifacts\/train_data\/Y_train.npy\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n      - src\/stage_03_train.py\n      - src\/utils\/all_utils.py\n      - config\/config.yaml\n      - params.yaml\n    outs:\n      - artifacts\/checkpoints\n      - artifacts\/model\n  \n  metrics:\n    cmd: python src\/stage_04_metrics.py --config=config\/config.yaml\n    deps:\n      - src\/stage_04_metrics.py\n      - config\/config.yaml\n      - src\/utils\/all_utils.py\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n      - artifacts\/checkpoints\n      - artifacts\/model\n    outs:\n      - confusion_matrix.png\n<\/code><\/pre>\n<p>This is my DVC.yaml.<\/p>\n<p>I have created Github workflow to reproduce it, but whenever I run it it gives me the following error - <code>... ERROR: Pipeline has a cycle involving: load_extract_save.<\/code><\/p>\n<p>The error looks <a href=\"https:\/\/i.stack.imgur.com\/1mt1Y.png\" rel=\"nofollow noreferrer\">like this<\/a>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655502527920,
        "Question_score":2,
        "Question_tags":"python|dvc",
        "Question_view_count":38,
        "Owner_creation_time":1587308895270,
        "Owner_last_access_time":1663873987327,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1655524559503,
        "Answer_body":"<p>Stage <code>load_extract_save<\/code> both outputs and depends on the same path (<code>artifacts\/data<\/code>). That's a cycle.<\/p>\n<p>Pipeline structures should be <a href=\"https:\/\/dvc.org\/doc\/command-reference\/dag#directed-acyclic-graph\" rel=\"nofollow noreferrer\">directed <strong>acyclical<\/strong> graphs<\/a>, otherwise <code>dvc repro<\/code> could execute that stage over and over forever.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655524277717,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72665109",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":62441146,
        "Question_title":"Revert a dvc remove -p command",
        "Question_body":"<p>I have just removed a DVC tracking file by mistake using the command <code>dvc remove training_data.dvc -p<\/code>, which led to all my training dataset gone completely. I know in Git, we can easily revert a deleted branch based on its hash. Does anyone know how to revert all my lost data in DVC?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592445622650,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":687,
        "Owner_creation_time":1467943515393,
        "Owner_last_access_time":1664083114540,
        "Owner_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Owner_reputation":173,
        "Owner_up_votes":97,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You should be safe (at least data is not gone) most likely. From the <code>dvc remove<\/code> <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remove\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>Note that it does not remove files from the DVC cache or remote storage (see dvc gc). However, remember to run <code>dvc push<\/code> to save the files you actually want to use or share in the future.<\/p>\n<\/blockquote>\n\n<p>So, if you created <code>training_data.dvc<\/code> as with <code>dvc add<\/code> and\/or <code>dvc run<\/code> and <code>dvc remove -p<\/code> didn't ask\/warn you about anything, means that data is cached similar to Git in the <code>.dvc\/cache<\/code>. <\/p>\n\n<p>There are ways to retrieve it, but I would need to know a little bit more details - how exactly did you add your dataset? Did you commit <code>training_data.dvc<\/code> or it's completely gone? Was it the only data you have added so far? (happy to help you in comments).<\/p>\n\n<h2>Recovering a directory<\/h2>\n\n<p>First of all, <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files-and-directories#structure-of-cache-directory\" rel=\"nofollow noreferrer\">here<\/a> is the document that describes briefly how DVC stores directories in the cache.<\/p>\n\n<p>What we can do is to find all <code>.dir<\/code> files in the <code>.dvc\/cache<\/code>:<\/p>\n\n<p><code>find .dvc\/cache -type f -name \"*.dir\"<\/code><\/p>\n\n<p>outputs something like:<\/p>\n\n<pre><code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir\n.dvc\/cache\/00\/db872eebe1c914dd13617616bb8586.dir\n.dvc\/cache\/2d\/1764cb0fc973f68f31f5ff90ee0883.dir\n<\/code><\/pre>\n\n<p>(if the local cache is lost and we are restoring data from the remote storage, the same logic applies, commands (e.g. to find files on S3 with .dir extension) look different)<\/p>\n\n<p>Each <code>.dir<\/code> file is a JSON with a content of one version of a directory (file names, hashes, etc). It has all the information needed to restore it. The next thing we need to do is to understand which one do we need. There is no one single rule for that, what I would recommend to check (and pick depending on your use case):<\/p>\n\n<ul>\n<li>Check the date modified (if you remember when this data was added).<\/li>\n<li>Check the content of those files - if you remember a specific file name that was present only in the directory you are looking for - just grep it.<\/li>\n<li>Try to restore them one by one and check the directory content.<\/li>\n<\/ul>\n\n<p>Okay, now let's imagine we decided that we want to restore <code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir<\/code>, (e.g. because content of it looks like:<\/p>\n\n<pre><code>[\n{\"md5\": \"6f597d341ceb7d8fbbe88859a892ef81\", \"relpath\": \"test.tsv\"}, {\"md5\": \"32b715ef0d71ff4c9e61f55b09c15e75\", \"relpath\": \"train.tsv\"}\n]\n<\/code><\/pre>\n\n<p>and we want to get a directory with <code>train.tsv<\/code>).<\/p>\n\n<p>The only thing we need to do is to create a <code>.dvc<\/code> file that references this directory:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n- md5: 20b786b6e6f80e2b3fcf17827ad18597.dir\n  path: my-directory\n<\/code><\/pre>\n\n<p>(note, that path \/20\/b786b6e6f80e2b3fcf17827ad18597.dir became a hash value: 20b786b6e6f80e2b3fcf17827ad18597.dir)<\/p>\n\n<p>And run <code>dvc pull<\/code> on this file.<\/p>\n\n<p>That should be it.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1592457436920,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1592496929890,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62441146",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":65824766,
        "Question_title":"SSH automation in jenkins",
        "Question_body":"<p>So I've been working on the automation of processes and it includes fetching data from an external source through DVC(data version control) for which I am using SSH client to pull and push changes. For automation, I'm using <strong>Jenkins<\/strong> and the problem I'm facing is that for ssh we need to give a password on runtime, and in automation that's not an option. I've tried multiple ways to specify passwords for ssh like sshpass and ssh config but it turns out Jenkins when building creates some file name <strong>script.sh<\/strong> in a directory <em>repoName@tmp<\/em> in var\/lib\/jenkins\/.... and therefore it is giving permission denied error. no matter what I try. If anyone could give any suggestions to this problem it would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611222255303,
        "Question_score":2,
        "Question_tags":"jenkins|ssh|dvc",
        "Question_view_count":121,
        "Owner_creation_time":1493101921290,
        "Owner_last_access_time":1663968152030,
        "Owner_location":"Pakistan",
        "Owner_reputation":133,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You could use key-based auth for SSH instead instead of password auth so that your Jenkins user can access your SSH DVC remote without needing to specify a password.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1611228029327,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65824766",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":64653042,
        "Question_title":"Control tracked version of external dependency",
        "Question_body":"<p>I am trying to set up a DVC repository for machine learning data with different tagged versions of the dataset. I do this with something like:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd \/raid\/ml_data  # folder on a data drive\n$ git init\n$ dvc init\n$ [add data]\n$ [commit to dvc, git]\n$ git tag -a 1.0.0\n$ [add or change data]\n$ [commit to dvc, git]\n$ git tag -a 1.1.0\n<\/code><\/pre>\n<p>I have multiple projects that each need to reference some version of this dataset. The problem is I can't figure out how to set up those projects to reference a specific version. I'm able to track the <code>HEAD<\/code> of the repo with something like:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd ~\/my_proj  # different drive than the remote\n$ mkdir data\n$ git init\n$ dvc init\n$ dvc remote add -d local \/raid\/ml_data  # add the remote on my data drive\n$ dvc cache dir \/raid\/ml_data\/.dvc\/cache  # tell DVC to use the remote cache\n$ dvc checkout\n$ dvc run --external -d \/raid\/ml_data -o data\/ cp -r \/raid\/ml_data data\n<\/code><\/pre>\n<p>This gets me the latest version of the dataset, symlinked into my <code>data<\/code> folder, but what if I want some projects to use the <code>1.0.0<\/code> version and some to use the <code>1.1.0<\/code> version, or another version? Or for that matter, if I update the dataset to <code>2.0.0<\/code> but don't want my existing projects to necessarily track <code>HEAD<\/code> and instead keep the version with which they were set up?<\/p>\n<p>It's important to me to not create a ton of local copies of my dataset as the <code>\/home<\/code> drive is much smaller than the <code>\/raid<\/code> drive and some of these datasets are huge.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1604349754297,
        "Question_score":2,
        "Question_tags":"version-control|dvc",
        "Question_view_count":139,
        "Owner_creation_time":1370629593700,
        "Owner_last_access_time":1663178436650,
        "Owner_location":"Colorado Springs, CO",
        "Owner_reputation":11685,
        "Owner_up_votes":2855,
        "Owner_down_votes":47,
        "Owner_views":1329,
        "Question_last_edit_time":1604361023337,
        "Answer_body":"<p>I think you are looking for the <a href=\"https:\/\/dvc.org\/doc\/start\/data-access\" rel=\"nofollow noreferrer\">data access<\/a> set of commands.<\/p>\n<p>In your particular case, <code>dvc import<\/code> makes sense:<\/p>\n<pre><code>$ dvc import \/raid\/ml_data data\n<\/code><\/pre>\n<p>if you want to get the most recent version (HEAD). Then you will be able to update it with the <code>dvc update<\/code> command (if 2.0.0 is released, for example).<\/p>\n<pre><code>$ dvc import \/raid\/ml_data data --rev 1.0.0\n<\/code><\/pre>\n<p>if you'd like to &quot;fix&quot; it to the specific version.<\/p>\n<h3>Avoiding copies<\/h3>\n<p>Make sure also, that <code>symlinks<\/code> are set for the second project, as described in the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"nofollow noreferrer\">Large Dataset Optimization<\/a>:<\/p>\n<pre><code>$ dvc config cache.type reflink,hardlink,symlink,copy\n<\/code><\/pre>\n<p>(there are config modifiers <code>--global<\/code>, <code>--local<\/code>, <code>--system<\/code> to set this setting for everyone at once, or just for one project, etc)<\/p>\n<p>Check the details instruction <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization#configuring-dvc-cache-file-link-type\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<hr \/>\n<p>Overall, it's a great setup, and looks like you got pretty much everything right. Please, don't hesitate to follow up and\/or create other questions here- we'll help you with this.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1604351561433,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1604362563343,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64653042",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72473641,
        "Question_title":"How do launch experiments in DVC?",
        "Question_body":"<p>I want to launch some experiments in DVC. But when I set values of experiment parameters, DVC deletes file 'params.yaml', and experiment doesn't set in queue.<\/p>\n<p>Simplified code for example:\nPython file 'test.py':<\/p>\n<pre><code>import numpy as np\nimport json\nimport yaml\n\nparams = yaml.safe_load(open('params.yaml'))[&quot;test&quot;]\n\nprecision = np.random.random()\nrecall = params['value']\naccuracy = np.random.random()\n \n\nrows = {'precision': precision,\n        'recall': recall,\n        'accuracy': accuracy}\n\n\nwith open(params['metrics_path'], 'w') as outfile:\n    json.dump(rows, outfile)\n\nfpr = 10*np.random.random((1,10)).tolist()\ntpr = 10*np.random.random((1,10)).tolist()\n\nwith open('plot.json', 'w') as outfile2:\n    json.dump(\n      {\n        &quot;roc&quot;: [ {&quot;fpr&quot;: f, &quot;tpr&quot;: t} for f, t in zip(fpr, tpr) ]\n      }, \n      outfile2\n      )\n<\/code><\/pre>\n<p>params.yaml:<\/p>\n<pre><code>test:\n  metrics_path: &quot;scores.json&quot;\n  value: 1\n<\/code><\/pre>\n<p>dvc.yaml:<\/p>\n<pre><code>stages:\n  test:\n    cmd: python test.py\n    deps:\n    - test.py\n    params:\n    - test.metrics_path\n    - test.value\n    metrics:\n    - scores.json:\n        cache: false\n    plots:\n    - plot.json:\n        cache: false\n        x: fpr\n        y: tpr\n<\/code><\/pre>\n<p>It is strange behavior. Is it possible to fix it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_time":1654161042840,
        "Question_score":2,
        "Question_tags":"python|dvc",
        "Question_view_count":167,
        "Owner_creation_time":1623510794510,
        "Owner_last_access_time":1663932931300,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I solved my problem. It is necessary, that all files (executable scripts, 'dvc.yaml', 'params.yaml') be tracked by git. In this case <code>dvc exp run<\/code> command works correctly.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1654244934587,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72473641",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":57966851,
        "Question_title":"Undo 'dvc add' operation",
        "Question_body":"<p>I <code>dvc add<\/code>-ed a file I did not mean to add. I have not yet committed.<\/p>\n\n<p>How do I undo this operation? In Git, you would do <code>git rm --cached &lt;filename&gt;<\/code>.<\/p>\n\n<p>To be clear: I want to make DVC forget about the file, and I want the file to remain untouched in my working tree. This is the opposite of what <code>dvc remove<\/code> does.<\/p>\n\n<p>One <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1524\" rel=\"nofollow noreferrer\">issue<\/a> on the DVC issue tracker suggests that <code>dvc unprotect<\/code> is the right command. But reading the <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/unprotect\" rel=\"nofollow noreferrer\">manual page<\/a> suggests otherwise.<\/p>\n\n<p>Is this possible with DVC?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1568689927047,
        "Question_score":4,
        "Question_tags":"dvc",
        "Question_view_count":1304,
        "Owner_creation_time":1383611307000,
        "Owner_last_access_time":1664061570950,
        "Owner_location":"New York",
        "Owner_reputation":10846,
        "Owner_up_votes":1581,
        "Owner_down_votes":95,
        "Owner_views":984,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As per mroutis on the DVC Discord server:<\/p>\n\n<ol>\n<li><code>dvc unprotect<\/code> the file; this won't be necessary if you don't use <code>symlink<\/code> or <code>hardlink<\/code> caching, but it can't hurt.<\/li>\n<li>Remove the .dvc file<\/li>\n<li>If you need to delete the cache entry itself, run <code>dvc gc<\/code>, or look up the MD5 in <code>data.dvc<\/code> and manually remove it from <code>.dvc\/cache<\/code>.<\/li>\n<\/ol>\n\n<p><em>Edit<\/em> -- there is now an issue on their Github page to add this to the manual: <a href=\"https:\/\/github.com\/iterative\/dvc.org\/issues\/625\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc.org\/issues\/625<\/a><\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_time":1568693889197,
        "Answer_score":7.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1568725966083,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57966851",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":68559059,
        "Question_title":"DVC connect to Min.IO to access S3",
        "Question_body":"<p>What is the proper way to connect DVC to Min.IO that is connected to some buckets on S3.<\/p>\n<pre><code>AWS-S3(My_Bucket) &gt; Min.io(MY_Bucket aliased as S3)\n<\/code><\/pre>\n<p>Right now i am accessing my bucket by using mc for example <code>mc cp s3\/my_bucket\/datasets datasets<\/code> to copy stuff from there. But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC so i can use for example <code>&quot;DVC mc-S3 pull&quot;<\/code> and <code>&quot;DVC AWS-S3 pull&quot;<\/code>.<\/p>\n<p>How do i got for it because while googling i couldn't find anything that i could easily follow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1627469557323,
        "Question_score":1,
        "Question_tags":"amazon-s3|minio|dvc",
        "Question_view_count":375,
        "Owner_creation_time":1578574709920,
        "Owner_last_access_time":1648636957603,
        "Owner_location":"Poland",
        "Owner_reputation":85,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It looks like you are looking for a combination of things.<\/p>\n<p>First, Jorge mentioned you can set <code>endpointurl<\/code> to access Minio the same way as you would access regular S3:<\/p>\n<pre><code>dvc remote add -d minio-remote s3:\/\/mybucket\/path\ndvc remote modify minio-remote endpointurl https:\/\/minio.example.com                          \n<\/code><\/pre>\n<p>Second, it seems you can create <em>two<\/em> remotes - one for S3, one for Minio and use <code>-r<\/code> option that is available for many data management related commands:<\/p>\n<pre><code>dvc pull -r minio-remote\ndvc pull -r s3-remote\ndvc push -r minio-remote\n...\n<\/code><\/pre>\n<p>This way you could <code>push<\/code>\/<code>pull<\/code> data to\/from a specific storage.<\/p>\n<blockquote>\n<p>But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC<\/p>\n<\/blockquote>\n<p>There are other possible ways, I think to organize this. It indeed depends on what semantics you expect from <code>DVC mc-S3 pull<\/code>. Please let us know if <code>-r<\/code> is not enough and clarify the question- that would help us here.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_creation_time":1627513406643,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68559059",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73700203,
        "Question_title":"ERROR: Cannot add 'folder-path', because it is overlapping with other DVC tracked output:",
        "Question_body":"<p>Goal: <code>add<\/code> <code>commit<\/code> <code>push<\/code> all contents of <code>project_model\/data\/<\/code> to <strong>dvcstore<\/strong>.<\/p>\n<p>I don't have any <code>.dvc<\/code> files in my project.<\/p>\n<pre><code>$ dvc add .\/project_model\/data\/\nERROR: Cannot add '\/home\/me\/PycharmProjects\/project\/project_model\/data\/images', because it is overlapping with other DVC tracked output: '\/home\/me\/PycharmProjects\/project\/project_model\/data'.\nTo include '\/home\/me\/PycharmProjects\/project\/project_model\/data\/images' in '\/home\/me\/PycharmProjects\/project\/project_model\/data', run 'dvc commit project_model\/data.dvc'\n\n$ dvc commit project_model\/data.dvc\nERROR: failed to commit project_model\/data.dvc - 'project_model\/data.dvc' does not exist\n<\/code><\/pre>\n<p>I've deleted contents from <code>.dvc\/cache\/<\/code> and <strong>S3<\/strong> <code>s3:\/\/foo\/bar\/dvcstore\/<\/code>, with no luck.<\/p>\n<hr \/>\n<pre><code>$ dvc -V\n2.10.2\n<\/code><\/pre>\n<pre><code>$ dvc doctor\nDVC version: 2.10.2 (pip)\n---------------------------------\nPlatform: Python 3.9.12 on Linux-5.15.0-47-generic-x86_64-with-glibc2.35\nSupports:\n        webhdfs (fsspec = 2022.5.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        s3 (s3fs = 2022.5.0, boto3 = 1.21.21)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/nvme0n1p5\nCaches: local\nRemotes: s3\nWorkspace directory: ext4 on \/dev\/nvme0n1p5\nRepo: dvc, git\n<\/code><\/pre>\n<p>Please let me know if there's anything else I can add to post.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1663059618623,
        "Question_score":1,
        "Question_tags":"git|amazon-s3|caching|pycharm|dvc",
        "Question_view_count":31,
        "Owner_creation_time":1631019482980,
        "Owner_last_access_time":1663946675073,
        "Owner_location":null,
        "Owner_reputation":234,
        "Owner_up_votes":708,
        "Owner_down_votes":14,
        "Owner_views":155,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In my case, the problem was in <code>dvc.yaml<\/code>.<\/p>\n<p>For a few <code>stages<\/code>, I had cyclical dependencies, where a file-path was mentioned in both the <code>deps<\/code> and <code>outs<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1663149911323,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73700203",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":71338160,
        "Question_title":"DVC Experiment management workflow",
        "Question_body":"<p>I'm struggling with the DVC experiment management. Suppose the following scenario:<\/p>\n<p>I have <code>params.yaml<\/code> file:<\/p>\n<pre><code>recommendations:\n  k: 66\n  q: 5\n<\/code><\/pre>\n<p>I run the experiment with <code>dvc exp run -n exp_66<\/code>, and then I do <code>dvc exp push origin exp_66<\/code>. After this, I modify <code>params.yaml<\/code> file:<\/p>\n<pre><code>recommendations:\n  k: 99\n  q: 5\n<\/code><\/pre>\n<p>and then run another experiment <code>dvc exp run -n exp_99<\/code>, after which I commit with <code>dvc exp push origin exp_99<\/code>.<\/p>\n<p>Now, when I pull the corresponding branch with Git, I try to pull <code>exp_66<\/code> from dvc by running <code>dvc exp pull origin exp_66<\/code>. This does the pull (no error messages), but the content of the <code>params.yaml<\/code> file is with <code>k: 99<\/code> (and I would expect <code>k: 66<\/code>). What am I doing wrong? Does <code>git push<\/code> have to be executed after <code>dvc push<\/code>? Apart from that, I also found <code>dvc exp apply exp_66<\/code>, but I'm not sure what it does (it is suggested that after <code>apply<\/code> one should execute <code>git add .<\/code>, then <code>git commit<\/code>?<\/p>\n<p>I would really appreciate if you could write down the workflow with committing different experiments, pushing, pulling, applying, etc.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1646314733127,
        "Question_score":1,
        "Question_tags":"git|dvcs|dvc",
        "Question_view_count":152,
        "Owner_creation_time":1643373769633,
        "Owner_last_access_time":1663766086480,
        "Owner_location":null,
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You did everything alright. In the end, after pulling, you can see that when using <code>dvc exp show<\/code> your experiments will be there. To restore the experiment available from your experiment list into your workspace, you simply need to run <code>dvc exp apply exp_66<\/code>. DVC will make sure that the changes corresponding to this experiment will be checked out.<\/p>\n<p>Your workflow seems correct so far. One addition: once you make sure one of the experiments is what you want to &quot;keep&quot; in git history, you can use <code>dvc exp branch {exp_id} {branch_name}<\/code> to create a separate branch for this experiment. Then you can use <code>git<\/code> commands to save the changes.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1646319959073,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71338160",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":66925614,
        "Question_title":"How to access DVC-controlled files from Oracle?",
        "Question_body":"<p>I have been storing my large files in CLOBs within Oracle, but I am thinking of storing my large files in a shared drive, then having a column in Oracle contain pointers to the files. This would use DVC.<\/p>\n<p>When I do this,<\/p>\n<p>(a) are the paths in Oracle paths that point to the files in my shared drive, as in, the actual files themselves?<\/p>\n<p>(b) or do the paths in Oracle point somehow to the DVC metafile?<\/p>\n<p>Any insight would help me out!<\/p>\n<p>Thanks :)\nJustin<\/p>\n<hr \/>\n<p>EDIT to provide more clarity:<\/p>\n<p>I checked here (<a href=\"https:\/\/dvc.org\/doc\/api-reference\/open\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/api-reference\/open<\/a>), and it helped, but I'm not fully there yet ...<\/p>\n<p>I want to pull a file from a remote dvc repository using python (which I have connected to the Oracle database). So, if we can make that work, I think I will be good. But, I am confused. If I specify 'remote' below, then how do I name the file (e.g., 'activity.log') when the remote files are all encoded?<\/p>\n<pre><code>with dvc.api.open(\n        'activity.log',\n        repo='location\/of\/dvc\/project',\n        remote='my-s3-bucket'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p>(NOTE: For testing purposes, my &quot;remote&quot; DVC directory is just another folder on my MacBook.)<\/p>\n<p>I feel like I'm missing a key concept about getting remote files ...<\/p>\n<p>I hope that adds more clarity. Any help figuring out remote file access is appreciated! :)<\/p>\n<p>Justin<\/p>\n<hr \/>\n<p>EDIT to get insights on 'rev' parameter:<\/p>\n<p>Before my question, some background\/my setup:\n(a) I have a repo on my MacBook called 'basics'.\n(b) I copied into 'basics' a directory of 501 files (called 'surface_files') that I subsequently pushed to a remote storage folder called 'gss'. After the push, 'gss' contains 220 hash directories.<\/p>\n<p>The steps I used to get here are as follows:<\/p>\n<pre><code>&gt; cd ~\/Desktop\/Work\/basics\n&gt; git init\n&gt; dvc init\n&gt; dvc add ~\/Desktop\/Work\/basics\/surface_files\n&gt; git add .gitignore surface_files.dvc\n&gt; git commit -m &quot;Add raw data&quot;\n&gt; dvc remote add -d remote_storage ~\/Desktop\/Work\/gss\n&gt; git commit .dvc\/config -m &quot;Configure remote storage&quot;\n&gt; dvc push\n&gt; rm -rf .\/.dvc\/cache\n&gt; rm -rf .\/surface_files\n<\/code><\/pre>\n<p>Next, I ran the following Python code to take one of my surface files, named <code>surface_100141.dat<\/code>, and used <code>dvc.api.get_url()<\/code> to get the corresponding remote storage file name. I then copied this remote storage file into my desktop under the file's original name, i.e., <code>surface_100141.dat<\/code>.<\/p>\n<p>The code that does all this is as follows, but FIRST, MY QUESTION --- when I run the code as it is shown below, no problems; but when I uncomment the 'rev=' line, it fails. I am not sure why this is happening. I used <code>git log<\/code> and <code>cat .git\/refs\/heads\/master<\/code> to make sure that I was getting the right hash. WHY IS THIS FAILING? That is my question.<\/p>\n<p>(In full disclosure, my git knowledge is not too strong yet. I'm getting there, but it's still a work in progress! :))<\/p>\n<pre><code>import dvc.api\nimport os.path\nfrom os import path\nimport shutil\n\nfilename = 'surface_100141.dat' # This file name would be stored in my Oracle database\nhome_dir = os.path.expanduser('~')+'\/' # This simply expanding '~' into '\/Users\/ricej\/'\n\nresource_url = dvc.api.get_url(\n    path=f'surface_files\/{filename}', # Works when 'surface_files.dvc' exists, even when 'surface_files' directory and .dvc\/cache do not\n    repo=f'{home_dir}Desktop\/Work\/basics',\n    # rev='5c92710e68c045d75865fa24f1b56a0a486a8a45', # Commit hash, found using 'git log' or 'cat .git\/refs\/heads\/master'\n    remote='remote_storage')\nresource_url = home_dir+resource_url\nprint(f'Remote file: {resource_url}')\n\nnew_dir = f'{home_dir}Desktop\/' # Will copy fetched file to desktop, for demonstration\nnew_file = new_dir+filename\nprint(f'Remote file copy: {new_file}')\n\nif path.exists(new_file):\n    os.remove(new_file)\n    \ndest = shutil.copy(resource_url, new_file) # Check your desktop after this to see remote file copy\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1617399923477,
        "Question_score":1,
        "Question_tags":"python|oracle|dvc",
        "Question_view_count":389,
        "Owner_creation_time":1407091594730,
        "Owner_last_access_time":1654654035420,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1617643343263,
        "Answer_body":"<p>I'm not 100% sure that I understand the question (it would be great to expand it a bit on the actual use case you are trying to solve with this database), but I can share a few thoughts.<\/p>\n<p>When we talk about DVC, I think you need to specify a few things to identify the file\/directory:<\/p>\n<ol>\n<li>Git commit + path (actual path like <code>data\/data\/xml<\/code>). Commit (or to be precise any Git revision) is needed to identify the version of the data file.<\/li>\n<li>Or path in the DVC storage (<code>\/mnt\/shared\/storage\/00\/198493ef2343ao<\/code> ...<code>) + actual name of this file. This way you would be saving info that <\/code>.dvc` files have.<\/li>\n<\/ol>\n<p>I would say that second way is <em>not<\/em> recommended since to some extent it's an implementation detail - how does DVC store files internally. The public interface to DVC organized data storage is its repository URL + commit + file name.<\/p>\n<p>Edit (example):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with dvc.api.open(\n        'activity.log',\n        repo='location\/of\/dvc\/project',\n        remote='my-s3-bucket'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p><code>location\/of\/dvc\/project<\/code> this path must point to an actual Git repo. This repo should have a <code>.dvc<\/code> or <code>dvc.lock<\/code> file that has <code>activity.log<\/code> name in it + its hash in the remote storage:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n  - md5: a304afb96060aad90176268345e10355\n    path: activity.log\n<\/code><\/pre>\n<p>By reading this Git repo and analyzing let's say <code>activity.log.dvc<\/code> DVC will be able to create the right path <code>s3:\/\/my-bucket\/storage\/a3\/04afb96060aad90176268345e10355<\/code><\/p>\n<p><code>remote='my-s3-bucket'<\/code> argument is optional. By default it will use the one that is defined in the repo itself.<\/p>\n<p>Let's take another real example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with dvc.api.open(\n        'get-started\/data.xml',\n        repo='https:\/\/github.com\/iterative\/dataset-registry'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p>In the <code>https:\/\/github.com\/iterative\/dataset-registry<\/code> you could find the <a href=\"https:\/\/github.com\/iterative\/dataset-registry\/blob\/master\/get-started\/data.xml.dvc\" rel=\"nofollow noreferrer\"><code>.dvc<\/code> file<\/a> that is enough for DVC to create a path to the file by also analyzing its <a href=\"https:\/\/github.com\/iterative\/dataset-registry\/blob\/master\/.dvc\/config\" rel=\"nofollow noreferrer\">config<\/a><\/p>\n<pre><code>https:\/\/remote.dvc.org\/dataset-registry\/a3\/04afb96060aad90176268345e10355\n<\/code><\/pre>\n<p>you could run <code>wget<\/code> on this file to download it<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1617404822540,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1617478114567,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66925614",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":69725612,
        "Question_title":"Is the default DVC behavior to store connection data in git?",
        "Question_body":"<p>I've recently started to play with <a href=\"https:\/\/dvc.org\" rel=\"nofollow noreferrer\">DVC<\/a>, and I was a bit surprised to see the <a href=\"https:\/\/dvc.org\/doc\/start\/data-and-model-versioning#storing-and-sharing\" rel=\"nofollow noreferrer\">getting started docs<\/a> are suggesting to store <code>.dvc\/config<\/code> in git.<\/p>\n<p>This seemed like a fine idea at first, but then I noticed that my Azure Blob Storage account (i.e. my Azure username) is also stored in .dvc\/config, which means it would end up in git. Making it not ideal for team collaboration scenarios.<\/p>\n<p>What's even less ideal (read: really scary) is that connection strings entered using <code>dvc remote modify blah connection_string ...<\/code> also end up in <code>.dvc\/config<\/code>, making them end up in git and, in the case of open source projects, making them end up in <strong>very<\/strong> interesting places.<\/p>\n<p>Am I doing something obviously wrong? I wouldn't expect the getting started docs to go very deep into security issues, but I wouldn't expect them to store connection strings in source control either.<\/p>\n<p>My base assumption is that I'm misunderstanding\/misconfiguring something, I'd be curious to know what.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635260868803,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":77,
        "Owner_creation_time":1250158552417,
        "Owner_last_access_time":1663847198323,
        "Owner_location":"Romania",
        "Owner_reputation":7916,
        "Owner_up_votes":1735,
        "Owner_down_votes":33,
        "Owner_views":801,
        "Question_last_edit_time":null,
        "Answer_body":"<p>DVC has few &quot;levels&quot; of config, that can be controlled with proper flag:<\/p>\n<ul>\n<li><code>--local<\/code> - repository level, ignored by git by default - designated for project-scope, sensitive data<\/li>\n<li>project - same as above, not ignored - designated to specify non-sensitive data (it is the default)<\/li>\n<li><code>--global<\/code> \/ <code>--system<\/code> - for common config for more repositories.<\/li>\n<\/ul>\n<p>More information can be found in the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#description\" rel=\"nofollow noreferrer\">docs<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1635332461183,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1635332764020,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69725612",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72551630,
        "Question_title":"How to setup a DVC shared cache without git repository between different services in minikube?",
        "Question_body":"<p>I need to setup a shared cache in minikube in such a way that different services can use that cache to pull and update DVC models and data needed for training Machine Learning models. The structure of the project is to use 1 pod to periodically update the cache with new models and outputs. Then, multiple pods can read the cache to recreate the updated models and data. So I need to be able to update the local cache directory and pull from it using DVC commands, so that all the services have consistent view on the latest models and data created by a service.<\/p>\n<p>More specifically, I have a docker image called <code>inference-service<\/code> that should only <code>dvc pull<\/code> or some how use the info in the shared dvc cache to get the latest model and data locally in <code>models<\/code> and <code>data<\/code> folders (see dockerfile) in minikube. I have another image called <code>test-service<\/code> that\nruns the ML pipeline using <code>dvc repro<\/code> which creates the models and data that DVC needs (dvc.yaml) to track and store in the shared cache. So <code>test-service<\/code> should push created outputs from the ML pipeline into the shared cache so that <code>inference-service<\/code> can pull it and use it instead of running dvc repro by itself. <code>test-service<\/code> should only re-train and write the updated models and data into the shared cache while <code>inference-service<\/code> should only read and recreate the updated\/latest models and data from the shared cache.<\/p>\n<p><em><strong>Problem: the cache does get mounted on the minikube VM, but the inference service does not pull (using <code>dvc pull -f<\/code>) the data and models after the test service is done with <code>dvc repro<\/code> and results the following warnings and failures:<\/strong><\/em><\/p>\n<p><em>relevant kubernetes pod log of inference-service<\/em><\/p>\n<pre><code>WARNING: Output 'data\/processed\/train_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/train_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/validation_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/validation_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/test_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/test_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/interim\/train_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/train_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/validation_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/validation_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/test_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/test_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'models\/mlb.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/tfidf_vectorizer.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/model.pkl'(stage: 'train') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'reports\/scores.json'(stage: 'evaluate') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: No file hash info found for '\/root\/models\/model.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/reports\/scores.json'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/train_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/validation_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/test_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/train_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/validation_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/test_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/mlb.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/tfidf_vectorizer.pkl'. It won't be created.\n10 files failed\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\n\/root\/models\/model.pkl\n\/root\/reports\/scores.json\n\/root\/data\/processed\/train_preprocessed.pkl\n\/root\/data\/processed\/validation_preprocessed.pkl\n\/root\/data\/processed\/test_preprocessed.pkl\n\/root\/data\/interim\/train_featurized.pkl\n\/root\/data\/interim\/validation_featurized.pkl\n\/root\/data\/interim\/test_featurized.pkl\n\/root\/models\/mlb.pkl\n\/root\/models\/tfidf_vectorizer.pkl\nIs your cache up to date?\n<\/code><\/pre>\n<p><em>relevant kubernetes pod log of test-service<\/em><\/p>\n<pre><code>Stage 'preprocess' is cached - skipping run, checking out outputs\nGenerating lock file 'dvc.lock'\nUpdating lock file 'dvc.lock'\nStage 'featurize' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'train' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'evaluate' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nUse `dvc push` to send your updates to remote storage.\n<\/code><\/pre>\n<p><strong>Project Tree<\/strong><\/p>\n<pre><code>\u251c\u2500 .dvc\n\u2502  \u251c\u2500 .gitignore\n\u2502  \u251c\u2500 config\n\u2502  \u2514\u2500 tmp\n\u251c\u2500 deployment\n\u2502  \u251c\u2500 docker-compose\n\u2502  \u2502  \u251c\u2500 docker-compose.yml\n\u2502  \u251c\u2500 minikube-dep\n\u2502  \u2502  \u251c\u2500 inference-test-services_dep.yaml\n\u2502  \u251c\u2500 startup_minikube_with_mount.sh.sh\n\u251c\u2500 Dockerfile # for inference service\n\u251c\u2500 dvc-cache # services should push and pull from this cache folder and see this as the DVC repo\n\u251c- dvc.yaml\n\u251c- params.yaml\n\u251c\u2500 src\n\u2502  \u251c\u2500 build_features.py\n|  \u251c\u2500 preprocess_data.py\n|  \u251c\u2500 serve_model.py\n|  \u251c\u2500 startup.sh  \n|  \u251c\u2500 requirements.txt\n\u251c\u2500 test_dep\n\u2502  \u251c\u2500 .dvc # same as .dvc in the root folder\n|  |  \u251c\u2500...\n\u2502  \u251c\u2500 Dockerfile # for test service\n\u2502  \u251c\u2500 dvc.yaml\n|  \u251c\u2500 params.yaml\n\u2502  \u2514\u2500 src\n\u2502     \u251c\u2500 build_features.py # same as root src folder\n|     \u251c\u2500 preprocess_data.py # same as root src folder\n|     \u251c\u2500 serve_model.py # same as root src folder\n|     \u251c\u2500 startup_test.sh  \n|     \u251c\u2500 requirements.txt  # same as root src folder\n<\/code><\/pre>\n<p><strong>dvc.yaml<\/strong><\/p>\n<pre><code>stages:\n  preprocess:\n    cmd: python ${preprocess.script}\n    params:\n      - preprocess\n    deps:\n      - ${preprocess.script}\n      - ${preprocess.input_train}\n      - ${preprocess.input_val}\n      - ${preprocess.input_test}\n    outs:\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n  featurize:\n    cmd: python ${featurize.script}\n    params:\n      - preprocess\n      - featurize\n    deps:\n      - ${featurize.script}\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n    outs:\n      - ${featurize.output_train}\n      - ${featurize.output_val}\n      - ${featurize.output_test}\n      - ${featurize.mlb_out}\n      - ${featurize.tfidf_vectorizer_out}\n  train:\n    cmd: python ${train.script}\n    params:\n      - featurize\n      - train\n    deps:\n      - ${train.script}\n      - ${featurize.output_train}\n    outs:\n      - ${train.model_out}\n  evaluate:\n    cmd: python ${evaluate.script}\n    params:\n      - featurize\n      - train\n      - evaluate\n    deps:\n      - ${evaluate.script}\n      - ${train.model_out}\n      - ${featurize.output_val}\n    metrics:\n      - ${evaluate.scores_path}\n<\/code><\/pre>\n<p><strong>params.yaml<\/strong><\/p>\n<pre><code>preprocess:\n  script: src\/preprocess\/preprocess_data.py\n  input_train: data\/raw\/train.tsv\n  input_val: data\/raw\/validation.tsv\n  input_test: data\/raw\/test.tsv\n  output_train: data\/processed\/train_preprocessed.pkl\n  output_val: data\/processed\/validation_preprocessed.pkl\n  output_test: data\/processed\/test_preprocessed.pkl\n\nfeaturize:\n  script: src\/features\/build_features.py\n  output_train: data\/interim\/train_featurized.pkl\n  output_val: data\/interim\/validation_featurized.pkl\n  output_test: data\/interim\/test_featurized.pkl\n  mlb_out: models\/mlb.pkl\n  tfidf_vectorizer_out: models\/tfidf_vectorizer.pkl\n\ntrain:\n  script: src\/models\/train_model.py\n  model_out: models\/model.pkl\n\nevaluate:\n  script: src\/models\/evaluate_model.py\n  scores_path: reports\/scores.json\n  roc_json: reports\/roc_plot.json\n  prc_json: reports\/prc_plot.json\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654718868090,
        "Question_score":0,
        "Question_tags":"docker|kubernetes|minikube|dvc",
        "Question_view_count":196,
        "Owner_creation_time":1562403039337,
        "Owner_last_access_time":1659463717997,
        "Owner_location":null,
        "Owner_reputation":298,
        "Owner_up_votes":14,
        "Owner_down_votes":1,
        "Owner_views":47,
        "Question_last_edit_time":1655157056467,
        "Answer_body":"<p>After running <code>dvc repro<\/code> in <code>test-service<\/code>, a new <code>dvc.lock<\/code> will be created, containing the file hashes relative to your pipeline (i.e. the hash for <code>models\/model.pkl<\/code> etc).<\/p>\n<p>If you're running a shared cache, <code>inference-service<\/code> should have access to the updated <code>dvc.lock<\/code>. If that is present, it will be sufficient to run <code>dvc checkout<\/code> to populate the workspace with the files corresponding to the hashes in the shared cache.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1654875420463,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72551630",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70928144,
        "Question_title":"Multiple users in DVC",
        "Question_body":"<p>I would like to ask if it is possible to use DVC with several accounts on the same machine. At the moment, all commands (<code>dvc pull<\/code>, <code>dvc push<\/code>, ...) are executed under my name. But after several people joined this project too, I do not want them to execute commands under my name.<\/p>\n<p>When I was alone on this project I generated ssh key:<\/p>\n<pre><code>ssh-keygen\n<\/code><\/pre>\n<p>Connected to server where DVC remote data is stored:<\/p>\n<pre><code>ssh-copy-id username@server_IP\n<\/code><\/pre>\n<p>Created config file which lets me execute all <code>dvc<\/code> commands using ssh:<\/p>\n<pre><code>[core]\n    remote = storage_server\n['remote &quot;storage_server&quot;']\n    url = ssh:\/\/username@server_IP:\/home\/DVC_remote\/DVC_project\n<\/code><\/pre>\n<p>What I should do so that several people could execute commands on their own name?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643641422880,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":181,
        "Owner_creation_time":1457469301700,
        "Owner_last_access_time":1663223040440,
        "Owner_location":null,
        "Owner_reputation":585,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You need to make the &quot;username&quot; part of the config personalized based on who is running the command. There are a few options to do this (based on <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#available-parameters-per-storage-type\" rel=\"nofollow noreferrer\">this document<\/a>, see the SSH part):<\/p>\n<h2>Basic options are:<\/h2>\n<ul>\n<li>User defined in the SSH config file (e.g. <code>~\/.ssh\/config<\/code>) for this host (URL);<\/li>\n<li>Current system user;<\/li>\n<\/ul>\n<p>So, the simplest even options could be just remove it from the URL and rely on the current system user?<\/p>\n<h2>Local (git-ignored or per-project DVC config) config<\/h2>\n<p>You could do is to remove the <code>username<\/code> part from the <code>url<\/code> and run something like this:<\/p>\n<pre><code>dvc remote modify --local storage_server user username\n<\/code><\/pre>\n<p><code>--local<\/code> here means that DVC will create a separate additional config that will be ignored by Git. This way if every user runs this command in every project they use they will customize the username.<\/p>\n<hr \/>\n<p>Let me know if that helps or something doesn't work. I'll try to help.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1643651143317,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70928144",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67209146,
        "Question_title":"DVC - make scheduled csv dumps",
        "Question_body":"<p>Suppose we got some database (any database, that supports csv dumping), collecting raw data in real time for further usage in ML.\nOn the other side, we got DVC, that can work with csv files.<\/p>\n<p>I want to organize a scheduled run of stored SELECT to that DB with datetime parameters (and also support a manual run), to make a new csv files, and send them to DVC.<\/p>\n<p>In DVC documentation and examples I found, csv file already exists.<\/p>\n<p>Can I make this interaction with database with DVC itself, or I got something wrong, and there is a separate tool for csv dump?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619078487020,
        "Question_score":2,
        "Question_tags":"export-to-csv|dvc",
        "Question_view_count":65,
        "Owner_creation_time":1580932981007,
        "Owner_last_access_time":1663744754923,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There are 3 steps in this process:<\/p>\n<ol>\n<li>Create a CSV dump. Many DBs have these tools but DVC does not support this natively.<\/li>\n<li>Version the CSV dump and move it to some storage. DVC does this job.<\/li>\n<li>Schedule periodical dump. You can use Cron (easy), AirFlow (not easy) or <a href=\"https:\/\/docs.github.com\/en\/actions\/reference\/events-that-trigger-workflows\" rel=\"nofollow noreferrer\">periodical jobs in GitHub Actions<\/a>\/<a href=\"https:\/\/docs.gitlab.com\/ee\/ci\/pipelines\/schedules.html\" rel=\"nofollow noreferrer\">GitLab CI\/CD<\/a>. Another project from the DVC team can help with CI\/CD option: <a href=\"https:\/\/cml.dev\" rel=\"nofollow noreferrer\">https:\/\/cml.dev<\/a>.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1619081803750,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1619084940030,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67209146",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70560288,
        "Question_title":"DVC Shared Windows Directory Setup",
        "Question_body":"<p>I have one Linux machine and one Windows machine for developments. For data sharing, we have set up a shared Windows directory in another Windows machine, which both my Linux and Windows can access.<\/p>\n<p>I am now using <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> for version control of the shared data. To make it easy, I mount the shared Windows folder both in Windows and in Linux development machine. In Windows, it looks like<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>In Linux, it looks like:<\/p>\n<pre><code>[core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>As you can see, Windows and Linux have different mounting points. So my question is: is there a way to make that both Windows and Linux have the same <code>\u00f9rl<\/code> in the DVC configuration file?<\/p>\n<p>If this is impossible, is there another alternative solution for DVC keeps data in remote shared Windows folder? Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641163263100,
        "Question_score":1,
        "Question_tags":"linux|dvc",
        "Question_view_count":128,
        "Owner_creation_time":1331553057367,
        "Owner_last_access_time":1663939436043,
        "Owner_location":null,
        "Owner_reputation":10643,
        "Owner_up_votes":1174,
        "Owner_down_votes":7,
        "Owner_views":504,
        "Question_last_edit_time":1641199464667,
        "Answer_body":"<p>If you are using a local remote this way, you won't be able to have to the same <code>url<\/code> on both platforms since the mount points are different (as you already realized).<\/p>\n<p>The simplest way to configure this would be to pick one (Linux or Windows) <code>url<\/code> to use as your default case that gets git-committed into <code>.dvc\/config<\/code>. On the other platform you (or your users) can override that <code>url<\/code> in the local configuration file: <code>.dvc\/config.local<\/code>.<\/p>\n<p>(Note that <code>.dvc\/config.local<\/code> is a git-ignored file and will not be included in any commits)<\/p>\n<p>So if you wanted Windows to be the default case, in <code>.dvc\/config<\/code> you would have:<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>and on your Linux machine you would add the file <code>.dvc\/config.local<\/code> containing:<\/p>\n<pre><code>['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>See the DVC docs for <code>dvc config --local<\/code> and <code>dvc remote modify --local<\/code> for more details:<\/p>\n<ul>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#description\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/config#description<\/a><\/li>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1641179335767,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70560288",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":66477468,
        "Question_title":"Data Version Control with Google Drive Remote: \"googleapiclient.errors.UnknownApiNameOrVersion: name: drive version: v2\"",
        "Question_body":"<p>I'm trying to setup DVC with Google Drive storage as shown <a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote#url-format\" rel=\"nofollow noreferrer\">here<\/a>. So far, I've been unsuccessful in pushing data to the remote. I tried both with and without the Google App setup.<\/p>\n<p>After running a <code>dvc push -v<\/code>, the following exception is shown:<\/p>\n<pre><code>  File &quot;(...)\/anaconda3\/lib\/python3.8\/site-packages\/googleapiclient\/discovery.py&quot;, line 387, in _retrieve_discovery_doc\n    raise UnknownApiNameOrVersion(&quot;name: %s  version: %s&quot; % (serviceName, version))\ngoogleapiclient.errors.UnknownApiNameOrVersion: name: drive  version: v2\n<\/code><\/pre>\n<p>DVC was installed via <code>pip install dvc[gdrive]<\/code>. The <code>pip freeze<\/code> of the concerning packages is:<\/p>\n<pre><code>oauth2client==4.1.3\ngoogle-api-python-client==2.0.1\ndvc==2.0.1\n<\/code><\/pre>\n<p>Any help is thoroughly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1614869465907,
        "Question_score":4,
        "Question_tags":"google-client|dvc",
        "Question_view_count":979,
        "Owner_creation_time":1593006899210,
        "Owner_last_access_time":1664068381880,
        "Owner_location":null,
        "Owner_reputation":704,
        "Owner_up_votes":53,
        "Owner_down_votes":6,
        "Owner_views":33,
        "Question_last_edit_time":1614874338737,
        "Answer_body":"<p>Can you try to install <code>google-api-python-client==1.12.8<\/code> and test in that way?<\/p>\n<p>Edit:<\/p>\n<p>It appears to be that, this was a bug in the 2.0.0-2.0.1 of google-api-client and resolved in 2.0.2. So this should also work <code>google-api-python-client&gt;=2.0.2<\/code><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1614873489453,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1614927407287,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66477468",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":58952962,
        "Question_title":"How to use different remotes for different folders?",
        "Question_body":"<p>I want my data and models stored in separate Google Cloud buckets. The idea is that I want to be able to share the data with others without sharing the models.<\/p>\n\n<p>One idea I can think of is using separate git submodules for data and models. But that feels cumbersome and imposes some additional requirements from the end user (e.g. having to do <code>git submodule update<\/code>).<\/p>\n\n<p>So can I do this without using git submodules?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1574248229420,
        "Question_score":12,
        "Question_tags":"dvc",
        "Question_view_count":1984,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can first add the different <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\" rel=\"nofollow noreferrer\">DVC remotes<\/a> you want to establish (let's say you call them <code>data<\/code> and <code>models<\/code>, each one pointing to a different <a href=\"https:\/\/cloud.google.com\/storage\/docs\/json_api\/v1\/buckets\" rel=\"nofollow noreferrer\">GC bucket<\/a>). <strong>But don't set any remote as the project's default<\/strong>; This way, <a href=\"https:\/\/dvc.org\/doc\/command-reference\/push\" rel=\"nofollow noreferrer\"><code>dvc push<\/code><\/a> won't work without the <code>-r<\/code> (or <code>--remote<\/code>) option.<\/p>\n<p>You would then need to push each directory or file individually to the appropriate remote, like <code>dvc push data\/ -r data<\/code> and <code>dvc push model.dat -r models<\/code>.<\/p>\n<p>Note that a feature request to configure this exists on the DVC repo too. See <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2095\" rel=\"nofollow noreferrer\">Specify file types that can be pushed to remote<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1574267475363,
        "Answer_score":13.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1642527991693,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58952962",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67744934,
        "Question_title":"Is it possible to check that the version of a file tracked by a DVC metadata file exists in remote storage without pulling the file?",
        "Question_body":"<p>My team has a set up wherein we track datasets and models in DVC, and have a GitLab repository for tracking our code and DVC metadata files. We have a job in our dev GitLab pipeline (run on each push to a merge request) that has the goal of checking to be sure that the developer remembered to run <code>dvc push<\/code> to keep DVC remote storage up-to-date. Right now, the way we do this is by running <code>dvc pull<\/code> on the GitLab runner, which will fail with errors telling you which files (new files or latest versions of existing files) were not found.<\/p>\n<p>The downside to this approach is that we are loading the entirety of our data stored in DVC onto a GitLab runner, and we've run into out-of-memory issues, not to mention lengthy run time to download all that data. Since the path and md5 hash of the objects are stored in the DVC metadata files, I would think that's all the information that DVC would need to be able to answer the question &quot;is the remote storage system up-to-date&quot;.<\/p>\n<p>It seems like <code>dvc status<\/code> is similar to what I'm asking for, but compares the cache or workspace and remote storage. In other words, it requires the files to actually be present on whatever filesystem is making the call.<\/p>\n<p>Is there some way to achieve the goal I laid out above (&quot;inform the developer that they need to run <code>dvc push<\/code>&quot;) without pulling everything from DVC?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622232629793,
        "Question_score":5,
        "Question_tags":"git|gitlab|continuous-integration|dvc",
        "Question_view_count":488,
        "Owner_creation_time":1618255062697,
        "Owner_last_access_time":1645555346683,
        "Owner_location":null,
        "Owner_reputation":75,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1622257491983,
        "Answer_body":"<blockquote>\n<p>It seems like dvc status is similar to what I'm asking for<\/p>\n<\/blockquote>\n<p><code>dvc status --cloud<\/code> will give you a list of &quot;new&quot; files if they that haven't been pushed to the (default) remote. It won't error out though, so your CI script should fail depending on the stdout message.<\/p>\n<p>More info: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/status#options\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/status#options<\/a><\/p>\n<p>I'd also ask everyone to run <code>dvc install<\/code>, which will setup some Git hooks, including automatic <code>dvc push<\/code> with <code>git push<\/code>.<\/p>\n<p>See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/install\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/install<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1622257759210,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1622503453297,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67744934",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":69265000,
        "Question_title":"DVC - Forbidden: An error occurred (403) when calling the HeadObject operation",
        "Question_body":"<p>I just started with DVC. following are the steps I am doing to push my models on S3<\/p>\n<p>Initialize<\/p>\n<pre><code>dvc init\n<\/code><\/pre>\n<p>Add bucket url<\/p>\n<pre><code>dvc remote add -d storage s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>\n<p>add some files<\/p>\n<pre><code>dvc add somefiles\n<\/code><\/pre>\n<p>Add aws keys<\/p>\n<pre><code>dvc remote modify storage access_key_id AWS_ACCESS_KEY_ID\ndvc remote modify storage secret_access_key AWS_SECRET_ACCESS_KEY\n<\/code><\/pre>\n<p>now when I push<\/p>\n<pre><code>dvc push\n<\/code><\/pre>\n<p>it shows<\/p>\n<pre><code>ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n<p>Am i missing something?<\/p>\n<p><strong>update1<\/strong><\/p>\n<p>result of <code>dvc doctor<\/code><\/p>\n<pre><code>C:\\my-server&gt;dvc doctor\nDVC version: 2.7.4 (pip)\n---------------------------------\nPlatform: Python 3.8.0 on Windows-10-10.0.19041-SP0\nSupports:\n        http (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        https (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        s3 (s3fs = 2021.8.1, boto3 = 1.17.106)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: s3\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n<\/code><\/pre>\n<p>and the <code>dvc push-vv<\/code><\/p>\n<pre><code>C:\\my-server&gt;dvc push -vv  \n2021-09-21 13:21:38,382 TRACE: Namespace(all_branches=False, all_commits=False, all_tags=False, cd='.', cmd='push', cprofile=False, cprofile_dump=None, func=&lt;class 'dvc.command.data_sync.CmdDataPush'&gt;, glob=False, instrument=False, instrument_open=False, jobs=None, pdb=False, quiet=0, recursive=False, remote=None, run_cache=False, targets=[], verbose=2, version=None, with_deps=False)\n2021-09-21 13:21:39,293 TRACE: Assuming 'C:\\my-server\\.dvc\\cache\\02\\5b196462b86d2f10a9f659e2224da8.dir' is unchanged since \nit is read-only\n2021-09-21 13:21:39,296 TRACE: Assuming 'C:\\my-server\\.dvc\\cache\\02\\5b196462b86d2f10a9f659e2224da8.dir' is unchanged since \nit is read-only\n2021-09-21 13:21:40,114 DEBUG: Preparing to transfer data from '.dvc\\cache' to 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,117 DEBUG: Preparing to collect status from 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,119 DEBUG: Collecting status from 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,121 DEBUG: Querying 1 hashes via object_exists\n2021-09-21 13:21:44,840 ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 248, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\aiobotocore\\client.py&quot;, line 155, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (InvalidAccessKeyId) when calling the ListObjectsV2 operation: The AWS Access Key Id you provided does not exist in our records.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1080, in _info\n    out = await self._simple_info(path)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 993, in _simple_info\n    out = await self._call_s3(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 268, in _call_s3\n    raise err\nPermissionError: The AWS Access Key Id you provided does not exist in our records.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 248, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\aiobotocore\\client.py&quot;, line 155, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\main.py&quot;, line 55, in main\n    ret = cmd.do_run()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\command\\base.py&quot;, line 45, in do_run\n    return self.run()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\command\\data_sync.py&quot;, line 57, in run\n    processed_files_count = self.repo.push(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\repo\\__init__.py&quot;, line 50, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\repo\\push.py&quot;, line 48, in push\n    pushed += self.cloud.push(obj_ids, jobs, remote=remote, odb=odb)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\data_cloud.py&quot;, line 85, in push\n    return transfer(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\transfer.py&quot;, line 153, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 160, in compare_status\n    dest_exists, dest_missing = status(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 122, in status\n    exists = hashes.intersection(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 48, in _indexed_dir_hashes\n    dir_exists.update(odb.list_hashes_exists(dir_hashes - dir_exists))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\db\\base.py&quot;, line 415, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 611, in result_iterator\n    yield fs.pop().result()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 439, in result\n    return self.__get_result()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 388, in __get_result\n    raise self._exception\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\db\\base.py&quot;, line 406, in exists_with_progress\n    ret = self.fs.exists(path_info)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\fs\\fsspec_wrapper.py&quot;, line 97, in exists\n    return self.fs.exists(self._with_bucket(path_info))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 88, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 69, in sync\n    raise result[0]\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 25, in _runner\n    result[0] = await coro\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 820, in _exists\n    await self._info(path, bucket, key, version_id=version_id)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1084, in _info\n    out = await self._version_aware_info(path, version_id)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1027, in _version_aware_info\n    out = await self._call_s3(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 268, in _call_s3\n    raise err\nPermissionError: Forbidden\n------------------------------------------------------------\n2021-09-21 13:21:45,178 DEBUG: Version info for developers:\nDVC version: 2.7.4 (pip)\n---------------------------------\nPlatform: Python 3.8.0 on Windows-10-10.0.19041-SP0\nSupports:\n        http (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        https (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        s3 (s3fs = 2021.8.1, boto3 = 1.17.106)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: s3\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2021-09-21 13:21:45,185 DEBUG: Analytics is enabled.\n2021-09-21 13:21:45,446 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\sgarg\\\\AppData\\\\Local\\\\Temp\\\\tmpm_p9f3eq']'\n2021-09-21 13:21:45,456 DEBUG: Spawned '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\sgarg\\\\AppData\\\\Local\\\\Temp\\\\tmpm_p9f3eq']'\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1632209467140,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":816,
        "Owner_creation_time":1363322632587,
        "Owner_last_access_time":1664084323070,
        "Owner_location":"Chandigarh, India",
        "Owner_reputation":13237,
        "Owner_up_votes":2454,
        "Owner_down_votes":19,
        "Owner_views":2675,
        "Question_last_edit_time":1632236873617,
        "Answer_body":"<p>Could you please run <code>dvc doctor<\/code> and rerun <code>dvc push<\/code> and add <code>-vv<\/code> flag. And give the two results?<\/p>\n<pre><code>PermissionError: The AWS Access Key Id you provided does not exist in our records.\n<\/code><\/pre>\n<p>Does the <code>aws cli<\/code> works correctly for you? First setup <code>AWS_ACCESS_KEY_ID<\/code> and <code>AWS_SECRET_ACCESS_KEY<\/code> in envs then<\/p>\n<pre><code>aws s3 ls s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1632210565513,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1632211831430,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69265000",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":68082912,
        "Question_title":"git-ignore dvc.lock in repositories where only the DVC pipelines are used",
        "Question_body":"<p>I want to use the pipeline functionality of dvc in a git repository. The data is managed otherwise and should not be versioned by dvc. The only functionality which is needed is that dvc reproduces the needed steps of the pipeline when <code>dvc repro<\/code> is called. Checking out the repository on a new system should lead to an 'empty' repository, where none of the pipeline steps are stored.<\/p>\n<p>Thus, - if I understand correctly - there is no need to track the dvc.lock file in the repository. However, adding dvc.lock to the .gitginore file leads to an error message:<\/p>\n<pre><code>ERROR: 'dvc.lock' is git-ignored.\n<\/code><\/pre>\n<p>Is there any way to disable the dvc.lock in .gitignore check for this usecase?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624362030233,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":493,
        "Owner_creation_time":1542537900087,
        "Owner_last_access_time":1663940500713,
        "Owner_location":null,
        "Owner_reputation":147,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This is definitely possible, as DVC features are loosely coupled to one another. You can do pipelining by writing your dvc.yaml file(s), but avoid data management\/versioning by using <code>cache: false<\/code> in the stage outputs (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#output-subfields\" rel=\"nofollow noreferrer\"><code>outs<\/code> field<\/a>). See also helper <code>dvc stage add -O<\/code> (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/stage\/add#options\" rel=\"nofollow noreferrer\">big O<\/a>, alias of <code>--outs-no-cache<\/code>).<\/p>\n<p>And the same for initial data dependencies, you can <code>dvc add --no-commit<\/code> them (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#options\" rel=\"nofollow noreferrer\">ref<\/a>).<\/p>\n<p>You do want to track <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#dvclock-file\" rel=\"nofollow noreferrer\">dvc.lock<\/a> in Git though, so that DVC can determine the latest stage of the pipeline associated with the Git commit in every repo copy or branch.<\/p>\n<p>You'll be responsible for placing the right data files\/dirs (matching .dvc files and dvc.lock) in the workspace for <code>dvc repro<\/code> or <code>dvc exp run<\/code> to behave as expected. <code>dvc checkout<\/code> won't be able to help you.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_creation_time":1624393487733,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68082912",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":64456396,
        "Question_title":"How do I unit test a function in the CI pipeline that uses model files that are not part of the git remote?",
        "Question_body":"<p>I am developing machine learning repositories that require fairly large trained model files to run. These files are not part of the git remote but is tracked by DVC and is saved in a separate remote storage. I am running into issues when I am trying to run unit tests in the CI pipeline for functions that require these model files to make their prediction. Since I don't have access them in the git remote, I can't test them.<\/p>\n<p>What is the best practice that people usually do in this situation? I can think of couple of options -<\/p>\n<ul>\n<li>Pull the models from the DVC remote inside the CI pipeline. I don't want to do this becasue downloading models every time you want to run push some code will quickly eat up my usage minutes for CI and is an expensive option.<\/li>\n<li>Use <code>unittest.mock<\/code> to simulate the output of from the model prediction and test other parts of my code. This is what I am doing now but it's sort of a pain with unittest's mock functionalities. That module wasn't really developed with ML in mind from what I can tell. It's missing (or is hard to find) some functionalities that I would have really liked. Are there any good tools for doing this geared specifically towards ML?<\/li>\n<li>Do weird reformatting of the function definition that allows me to essentially do option 2 but without a mock module. That is, just test the surrounding logic and don't worry about the model output.<\/li>\n<li>Just put the model files in the git remote and be done with it. Only use DVC to track data.<\/li>\n<\/ul>\n<p>What do people usually do in this situation?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603252093370,
        "Question_score":4,
        "Question_tags":"unit-testing|machine-learning|mocking|continuous-integration|dvc",
        "Question_view_count":289,
        "Owner_creation_time":1446746840593,
        "Owner_last_access_time":1664083975707,
        "Owner_location":null,
        "Owner_reputation":2545,
        "Owner_up_votes":845,
        "Owner_down_votes":386,
        "Owner_views":382,
        "Question_last_edit_time":1603305923907,
        "Answer_body":"<p>If we talk about unit tests, I think it's indeed better to do a mock. It's best to have unit tests small, testing actual logic of the unit, etc. It's good to have other tests though that would pull the model and run some logic on top of that - I would call them integration tests.<\/p>\n<p>It's not black and white though. If you for some reason see that it's easier to use an actual model (e.g. it changes a lot and it is easier to use it instead of maintaining and updating stubs\/fixtures), you could potentially cache it.<\/p>\n<p>I think, to help you with the mock, you would need to share some technical details- how does the function look like, what have you tried, what breaks, etc.<\/p>\n<blockquote>\n<p>to do this because downloading models every time you want to run push some code will quickly eat up my usage minutes for CI and is an expensive option.<\/p>\n<\/blockquote>\n<p>I think you can potentially utilize CI systems cache to avoid downloading it over and over again. This is the GitHub Actions related <a href=\"https:\/\/github.com\/actions\/cache#cache-limits\" rel=\"nofollow noreferrer\">repository<\/a>, this is <a href=\"https:\/\/circleci.com\/docs\/2.0\/caching\" rel=\"nofollow noreferrer\">CircleCI<\/a>. The idea is the same across all common CI providers. Which one are considering to use, btw?<\/p>\n<blockquote>\n<p>Just put the model files in the git remote and be done with it. Only use DVC to track data.<\/p>\n<\/blockquote>\n<p>This can be the way, but if models are large enough you will pollute Git history significantly. On some CI systems it can become even slower since they will be fetching this with regular <code>git clone<\/code>. Effectively, downloading models anyway.<\/p>\n<p>Btw, if you use DVC or not take a look at another open-source project that is made specifically to do CI\/CD for ML - <a href=\"https:\/\/cml.dev\" rel=\"nofollow noreferrer\">CML<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1603314290770,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1603325349590,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64456396",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67407702,
        "Question_title":"Corrupted dvc.lock",
        "Question_body":"<p>I'm using DAGsHub storage as a remote and running into the following error message (when trying to DVC pull):<\/p>\n<blockquote>\n<p>ERROR: Lockfile 'bias_tagging_model\/dvc.lock' is corrupted.<\/p>\n<\/blockquote>\n<p>I thought I might have messed something up, but when cloning the git repo again and DVC pulling I am still running into this.\nThe data looks ok when viewed in the browser.\nIf you have any ideas, I would appreciate your help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1620243318287,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":362,
        "Owner_creation_time":1620132740443,
        "Owner_last_access_time":1641927840700,
        "Owner_location":"New York, NY, USA",
        "Owner_reputation":75,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Usually, the reason for this error is the DVC version.<\/p>\n<p>If the dvc.lock file has a DVC 2.* schema and you are using a lower version, it will throw this error.<\/p>\n<p>Upgrade your DVC version, and it should work.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620292401493,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67407702",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":57132106,
        "Question_title":"Expanding environment variables in the command part of a dvc run",
        "Question_body":"<p><strong>Summary<\/strong>: I am trying to define a <code>dvc<\/code> step using <code>dvc-run<\/code> where the command depends on some environment variables (for instance <code>$HOME<\/code>). The problem is that when I'm defining the step on machine A, then the variable is expanded when stored in the <code>.dvc<\/code> file. In this case, it won't be possible to reproduce the step on machine B. Did I hit a limitation of <code>dvc<\/code>? If that's not the case, what's the right approach?<\/p>\n\n<p><strong>More details<\/strong>: I faced the issue when trying to define a step for which the command is a <code>docker run<\/code>. Say that:<\/p>\n\n<ul>\n<li>on machine A <code>myrepo<\/code> is located at <code>\/Users\/user\/myrepo<\/code> and <\/li>\n<li>on machine B it is to be found at <code>\/home\/ubuntu\/myrepo<\/code>. <\/li>\n<\/ul>\n\n<p>Furthermore, assume I have a script <code>myrepo\/script.R<\/code> which processes a data file to be found at <code>myrepo\/data\/mydata.txt<\/code>. Lastly, assume that my step's command is something like: <\/p>\n\n<pre><code>docker run -v $HOME\/myrepo\/:\/prj\/ my_docker_image \/prj\/script.R \/prj\/data\/mydata.txt\n<\/code><\/pre>\n\n<p>If I'm running <code>dvc run -f step.dvc -d ... -d ... [cmd]<\/code> where <code>cmd<\/code> is the <code>docker<\/code> execution above, then in <code>step.dvc<\/code> the environment variable <code>$HOME<\/code> will be expanded. In this case, the step will be broken on machine B.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1563703426437,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":329,
        "Owner_creation_time":1300789717227,
        "Owner_last_access_time":1663941101560,
        "Owner_location":null,
        "Owner_reputation":11410,
        "Owner_up_votes":2846,
        "Owner_down_votes":6,
        "Owner_views":1782,
        "Question_last_edit_time":null,
        "Answer_body":"<p>From <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/run\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>Use single quotes ' instead of \" to wrap the command if there are environment variables in it, that you want to be evaluated dynamically. E.g. dvc run -d script.sh '.\/myscript.sh $MYENVVAR'<\/p>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1563988872483,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1564319630557,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57132106",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":58163305,
        "Question_title":"dvc gc and files in remote cache",
        "Question_body":"<p>dvc documentation for <code>dvc gc<\/code> command states, the <code>-r<\/code> option indicates \"Remote storage to collect garbage in\" but I'm not sure if I understand it correctly. For example I execute this command:<\/p>\n\n<pre><code>dvc gc -r myremote\n<\/code><\/pre>\n\n<p>What exactly happens if I execute this command? I have 2 possible answers:<\/p>\n\n<ol>\n<li>dvc checks which files should be deleted, then moves these files to \"myremote\" and then deletes all these files in local cache but not in remote.<\/li>\n<li>dvc checks which files should be deleted and deletes these files both in local cache and \"myremote\"<\/li>\n<\/ol>\n\n<p>Which one of them is correct?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1569828438393,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":1617,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":null,
        "Answer_body":"<p>one of DVC maintainers here.<\/p>\n\n<p>Short answer: 2. is correct.<\/p>\n\n<p>A bit of additional information:\nPlease be careful when using <code>dvc gc<\/code>. It will clear your cache from all dependencies that are not mentioned in the current HEAD of your git repository. \nWe are working on making <code>dvc gc<\/code> preserving whole history by default. <\/p>\n\n<p>So if you don't want to delete files from your history commits, it would be better to wait for completion of <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2325\" rel=\"nofollow noreferrer\">this<\/a> task.<\/p>\n\n<p>[EDIT]\nPlease see comment below.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1569833846627,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1569837069043,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58163305",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":65416056,
        "Question_title":"Data Version Control: Absolute Paths and Project Paths in the Pipeline Parameters?",
        "Question_body":"<p>In DVC one may define pipelines.  In Unix, one typically does not work at the root level.  Further, DVC expects files to be inside the git repository.<\/p>\n<p>So, this seems like a typical problem.<\/p>\n<p>Suppose I have the following:<\/p>\n<pre><code>\/home\/user\/project\/content-folder\/data\/data-type\/cfg.json\n\/home\/user\/project\/content-folder\/app\/foo.py\n<\/code><\/pre>\n<p>Git starts at <code>\/home\/user\/project\/<\/code><\/p>\n<pre><code>cd ~\/project\/content-folder\/data\/data-type\n..\/..\/app\/foo.py do-this --with cfg.json --dest $(pwd) \n<\/code><\/pre>\n<p>Seems reasonable to me: the script takes a configuration, which is stored in a particular location, runs it against some encapsulated functionality, and outputs it to the destination using an absolute path.<\/p>\n<p>The default behavior of <code>--dest<\/code> is to output to the current working directory.  This seems like another reasonable default.<\/p>\n<hr \/>\n<p>Next, I go to configure the <code>params.yaml<\/code> file for <code>dvc<\/code>, and I am immediately confusing and unsure what is going to happen.  I write:<\/p>\n<pre><code>foodoo:\n  params: do-this --with ????\/cfg.json --dest ????\n<\/code><\/pre>\n<p>What I want to write (and would in a shell script):<\/p>\n<pre><code>#!\/usr\/bin\/env bash\norigin:=$(git rev-parse --show-toplevel)\n\nverb=do-this\nparams=--with $(origin)\/content-folder\/data\/data-type\/cfg.json --dest $(origin)\/content-folder\/data\/data-type\n<\/code><\/pre>\n<hr \/>\n<p>But, in DVC, the pathing seems to be implicit, and I do not know where to start as either:<\/p>\n<ol>\n<li>DVC will calculate the path to my script locally<\/li>\n<li>Not calculate the path to my script locally<\/li>\n<\/ol>\n<p>Which is fine -- I can discover that.  But I am reasonably sure that DVC will absolutely not prefix the directory and file params in my params.yaml with the path to my project.<\/p>\n<hr \/>\n<p>How does one achieve path control that does not assume a fixed project location, like I would in BASH?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1608672730887,
        "Question_score":3,
        "Question_tags":"git|path|dvc",
        "Question_view_count":448,
        "Owner_creation_time":1405262190020,
        "Owner_last_access_time":1663968225423,
        "Owner_location":"Atlanta, GA",
        "Owner_reputation":26244,
        "Owner_up_votes":434,
        "Owner_down_votes":35,
        "Owner_views":1383,
        "Question_last_edit_time":null,
        "Answer_body":"<p>By default, DVC will run your stage command from the same directory as the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files#dvcyaml-file\" rel=\"nofollow noreferrer\">dvc.yaml<\/a> file. If you need to run the command from a different location, you can specify an alternate working directory via <code>wdir<\/code>, which should be a path relative to <code>dvc.yaml<\/code>'s location.<\/p>\n<p>Paths for everything else in your stage (like <code>params.yaml<\/code>) should be specified as relative to <code>wdir<\/code> (or relative to <code>dvc.yaml<\/code> if <code>wdir<\/code> is not provided).<\/p>\n<p>Looking at your example, there also seems to be a bit of confusion on parameters in DVC. In a DVC stage, <code>params<\/code> is for specifying <a href=\"https:\/\/dvc.org\/doc\/command-reference\/params\" rel=\"nofollow noreferrer\">parameter dependencies<\/a>, not used for specifying command-line flags. The full command including flags\/options should be included  the <code>cmd<\/code> section for your stage. If you wanted to make sure that your stage was rerun every time certain values in <code>cfg.json<\/code> have changed, your stage's <code>params<\/code> section would look something like:<\/p>\n<pre><code>params:\n  &lt;relpath from dvc.yaml&gt;\/cfg.json:\n    - param1\n    - param2\n    ...\n<\/code><\/pre>\n<p>So your example <code>dvc.yaml<\/code> would look something like:<\/p>\n<pre><code>stages:\n  foodoo:\n    cmd: &lt;relpath from dvc.yaml&gt;\/foo.py do-this --with &lt;relpath from dvc.yaml&gt;\/cfg.json --dest &lt;relpath from dvc.yaml&gt;\/...\n    deps:\n      &lt;relpath from dvc.yaml&gt;\/foo.py\n    params:\n      &lt;relpath from dvc.yaml&gt;\/cfg.json:\n        ...\n    ...\n<\/code><\/pre>\n<p>This would make the command <code>dvc repro<\/code> rerun your stage any time that the code in foo.py has changed, or the specified parameters in <code>cfg.json<\/code> have changed.<\/p>\n<p>You may also want to refer to the docs for <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#run\" rel=\"nofollow noreferrer\">dvc run<\/a>, which can be used to generate or update a <code>dvc.yaml<\/code> stage (rather than writing <code>dvc.yaml<\/code> by hand)<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1608686869860,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65416056",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73435172,
        "Question_title":"Is there any way to log 'git hash' in hydra?",
        "Question_body":"<p>I want to control version of experiment configuration files with hydra and dvc without uploading original config files to git.<br \/>\nHydra does control config, and dvc controls version. But Hydra does not specify which 'code version' is needed to reproduce experiment. And I don't want to add 'git hash logging code' in every experiments.<\/p>\n<p>Is there any way to log git hash to hydra log in default? thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661090494277,
        "Question_score":1,
        "Question_tags":"git|fb-hydra|dvc",
        "Question_view_count":43,
        "Owner_creation_time":1603378831587,
        "Owner_last_access_time":1663547314893,
        "Owner_location":null,
        "Owner_reputation":161,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Good timing! A DVC-Hydra integration is in development. You can see the proposal in <a href=\"https:\/\/github.com\/iterative\/dvc\/discussions\/7044#discussioncomment-3271855\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/discussions\/7044#discussioncomment-3271855<\/a> and the development progress in <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/8093\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/pull\/8093<\/a>. This should allow you to take a Hydra config, pass your Hydra overrides via <code>dvc exp run --set-param=&lt;hydra_overrides&gt;<\/code>, and capture the output with DVC.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1661173080163,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73435172",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67122683,
        "Question_title":"DVC Files Incomplete",
        "Question_body":"<p>I'm in a team using dvc with git to version-control data files. We are using dvc 1.3.1, with the an S3 bucket remote. I'm getting this error when executing <code>dvc fetch<\/code> or <code>dvc pull<\/code> on a colleague's branch:<\/p>\n<pre><code>ERROR: failed to fetch data from the cloud - DVC-file 'C:\\Users\\blah\\Documents\\repo\\data\\processed_data.dvc' format error: extra keys not allowed @ data['outs'][0]['size']\n<\/code><\/pre>\n<p>When I check the dvc file for a cached file with which I have no problem I see this:<\/p>\n<pre><code>md5: ded591aacbe363f0518ceb9c3bc1836b\nouts:\n- md5: efdab20e8b59903b9523cc188ff727e5\n  path: completion_header.p\n  cache: true\n  metric: false\n  persist: false\n<\/code><\/pre>\n<p>but a problematic file only has this:<\/p>\n<pre><code>outs:\n- md5: f4e15187d9a0bbb328e629eabd8d1784.dir\n  size: 112007\n  nfiles: 3\n  path: processed_data\n<\/code><\/pre>\n<p>In all cases, files are added to dvc with the command <code>dvc add %dirname%<\/code>. This is the second time I've seen this on a colleague's branch (2 different people).<\/p>\n<p>Since posting, I have realized that my colleague dvc'd a directory. I have attempted creating the directory first, then calling <code>dvc fetch<\/code>, but get the same error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618565400113,
        "Question_score":2,
        "Question_tags":"git|dvc",
        "Question_view_count":548,
        "Owner_creation_time":1348150034833,
        "Owner_last_access_time":1663852884933,
        "Owner_location":"Glasgow, UK",
        "Owner_reputation":2400,
        "Owner_up_votes":66,
        "Owner_down_votes":12,
        "Owner_views":263,
        "Question_last_edit_time":1618826341417,
        "Answer_body":"<blockquote>\n<p>In all cases, files are added to dvc with the command dvc add %filename%.<\/p>\n<\/blockquote>\n<p>It seems like there is a high chance that one of the dvc files created in newer versions of dvc and you are trying to operate with an older version. Are all of your colleagues use the same dvc version when adding new files?<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1618566517710,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67122683",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67454531,
        "Question_title":"Git bash command prompt hanging when running dvc push to DAGsHub",
        "Question_body":"<p>I'm having problems pushing files with DVC to DAGsHub.<\/p>\n<p>Workflow:<\/p>\n<ul>\n<li>I used my email to signup to DAGsHub.<\/li>\n<li>I created a repo and clone it to my computer.<\/li>\n<li>I added files to the repo and track them using DVC and Git to track the pointer files.<\/li>\n<li>Running DVC push -r origin, it asks me for my password. When I enter the password and hit enter - nothing happens.<\/li>\n<\/ul>\n<p>It sits and waits, barring me from even canceling the operation with Ctrl+C.\nI'm forced to manually close the terminal, open a new one, ending the &quot;Python&quot; process in task manager and delete the lock file in .dvc\/tmp\/lock.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1620537862843,
        "Question_score":5,
        "Question_tags":"dvc",
        "Question_view_count":254,
        "Owner_creation_time":1620537484800,
        "Owner_last_access_time":1642059984007,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1620625906413,
        "Answer_body":"<p><strong>Short answer<\/strong><\/p>\n<p>Do not use <code>ask_password<\/code>.\nInstead, save your token in the local config by running once:<\/p>\n<pre><code>dvc remote modify origin --local --unset ask_password\ndvc remote modify origin --local password &lt;--access token--&gt;\n<\/code><\/pre>\n<p><code>dvc push -r origin<\/code> should work then.<\/p>\n<p><strong>Long answer<\/strong><\/p>\n<p><a href=\"https:\/\/www.atlassian.com\/git\/tutorials\/git-bash#:%7E:text=What%20is%20Git%20Bash%3F,operating%20system%20through%20written%20commands.\" rel=\"nofollow noreferrer\">Git Bash<\/a> is not running the regular Windows command prompt but an emulated Unix-style bash prompt. From the information in your question, I cannot know for sure, but this is probably causing the <code>msvcrt<\/code> package used by DVC to prompt the password on windows machines to fail\/hang.<\/p>\n<p>There are potentially 3 ways to deal with the issue:<\/p>\n<ol>\n<li>Run <code>dvc pull<\/code> from the regular Windows cmd prompt.<\/li>\n<li>Find a way to make Git Bash wrap Python calls with <code>winpty<\/code> - I am not 100% positive about how to do this, but not using <code>winpty<\/code> seems to be the reason <code>msvcrt<\/code> fails at prompting for your password.<\/li>\n<li>The simplest solution - Do not use <code>ask_password<\/code>.\nInstead, save your token in the local config by running once:\n<pre><code>dvc remote modify origin --local --unset ask_password\ndvc remote modify origin --local password &lt;--access token--&gt;\n<\/code><\/pre>\nYou can get your access token by clicking on the question mark beside the DVC\nremote of your DAGsHub repository, then click on &quot;Reveal my token&quot;.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620591578073,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67454531",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67393339,
        "Question_title":"dvc push, change the names of files on the remote storage",
        "Question_body":"<p>I'm working on a project with DVC (Data Version Control), when I push files in my remote storage, the name of the files are changed. How I can conserve the names?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620170453717,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":447,
        "Owner_creation_time":1379685720450,
        "Owner_last_access_time":1663797728603,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1620171022370,
        "Answer_body":"<p>Short answer: there is no way to do that.<\/p>\n<p>Long answer:\nDvc remote is a content-based storage, so names are not preserved. Dvc creates metafiles (*.dvc files) in your workspace that contain names and those files are usually tracked by git, so you need to use git remote and dvc remote together to have both filenames and their contents. Here is a more detailed explanation about the format of local and remote storage <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory<\/a> . Also, checkout <a href=\"https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1620171651067,
        "Answer_score":5.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67393339",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":58541260,
        "Question_title":"Difference between git-lfs and dvc",
        "Question_body":"<p>What is the difference between these two? We used git-lfs in my previous job and we are starting to use dvc alongside git in my current one. They both place some kind of index instead of file and can be downloaded on demand. Has dvc some improvements over the former one?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1571919586097,
        "Question_score":27,
        "Question_tags":"git|git-lfs|dvc",
        "Question_view_count":6255,
        "Owner_creation_time":1373630643250,
        "Owner_last_access_time":1664025275130,
        "Owner_location":null,
        "Owner_reputation":382,
        "Owner_up_votes":185,
        "Owner_down_votes":1,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":"<p>DVC is a better replacement for <code>git-lfs<\/code>. <\/p>\n\n<p>Unlike git-lfs, DVC doesn't require installing a dedicated server; It can be used on-premises (NAS, SSH, for example) or with any major cloud provider (S3, Google Cloud, Azure).<\/p>\n\n<p>For more information: <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-and-model-files-versioning\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/use-cases\/data-and-model-files-versioning<\/a><\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1571925277763,
        "Answer_score":10.0,
        "Question_favorite_count":5.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58541260",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":68004538,
        "Question_title":"Is dvc.yaml supposed to be written or generated by dvc run command?",
        "Question_body":"<p>Trying to understand <a href=\"https:\/\/dvc.org\/doc\/start\" rel=\"nofollow noreferrer\">dvc<\/a>, most tutorials mention generation of dvc.yaml by running <code>dvc run<\/code> command.<\/p>\n<p>But at the same time, dvc.yaml which defines the DAG is also <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files\" rel=\"nofollow noreferrer\">well documented<\/a>. Also the fact that it is a yaml format and human readable\/writable would point to the fact that it is meant to be a DSL for specifying your data pipeline.<\/p>\n<p>Can somebody clarify which is the better practice?\nWriting the dvc.yaml or let it be generated by <code>dvc run<\/code> command?\nOr is it left to user's choice and there is no technical difference?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1623853195940,
        "Question_score":5,
        "Question_tags":"directed-acyclic-graphs|data-pipeline|dvc",
        "Question_view_count":1101,
        "Owner_creation_time":1243446134993,
        "Owner_last_access_time":1663913878327,
        "Owner_location":"Gothenburg, Sweden",
        "Owner_reputation":1547,
        "Owner_up_votes":28,
        "Owner_down_votes":9,
        "Owner_views":212,
        "Question_last_edit_time":1623873485370,
        "Answer_body":"<p>I'd recommend manual editing as the main route! (I believe that's officially recommended since <a href=\"https:\/\/dvc.org\/blog\/dvc-2-0-release\" rel=\"nofollow noreferrer\">DVC 2.0<\/a>)<\/p>\n<p><code>dvc stage add<\/code> can still be very helpful for programmatic generation of pipelines files, but it doesn't support all the features of <code>dvc.yaml<\/code>, for example setting <code>vars<\/code> values or defining <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#foreach-stages\" rel=\"nofollow noreferrer\"><code>foreach<\/code> stages<\/a>.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_creation_time":1623859205940,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1623875977407,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68004538",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":53213596,
        "Question_title":"How to execute python from conda environment by dvc run",
        "Question_body":"<p>I have an environment of conda configurated with python 3.6 and dvc is installed there, but when I try to execute dvc run with python, dvc call the python version of main installation of conda and not find the installed libraries.<\/p>\n\n<pre><code>$ conda activate py36\n$ python --version\nPython 3.6.6 :: Anaconda custom (64-bit)\n$ dvc run python --version\nRunning command:\n    python --version\nPython 3.7.0\nSaving information to 'Dvcfile'.\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1541699972677,
        "Question_score":6,
        "Question_tags":"python|anaconda|conda|dvc",
        "Question_view_count":351,
        "Owner_creation_time":1420765480790,
        "Owner_last_access_time":1663614206113,
        "Owner_location":null,
        "Owner_reputation":340,
        "Owner_up_votes":477,
        "Owner_down_votes":2,
        "Owner_views":61,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The version 0.24.3 of dvc correct this problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1549414531500,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53213596",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67215839,
        "Question_title":"Use parameters from additional configs in dvc 2.0",
        "Question_body":"<p>Using dvc version 2.0.18 and python 3.9.2 I want to use parameters defined in a config file different from params.yaml when configuring the parameters of the stages in <code>dvc.yaml<\/code>. However, it does not work as I expected.<\/p>\n<p>MWE:\nGit repo + dvc init:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 dvc.yaml\n\u251c\u2500\u2500 preproc.yaml\n\u2514\u2500\u2500 test.py\n<\/code><\/pre>\n<p>dvc.yaml:<\/p>\n<pre><code>vars:\n  - preproc.yaml\nstages:\n  test:\n    cmd: python test.py\n    deps:\n      - test.py\n    params:\n      - important_parameter\n<\/code><\/pre>\n<p>preproc.yaml:<\/p>\n<pre><code>important_parameter: 123\n<\/code><\/pre>\n<p>Running <code>dvc repro<\/code> lead to the following error:<\/p>\n<pre><code>ERROR: failed to reproduce 'dvc.yaml': dependency 'params.yaml' does not exist\n<\/code><\/pre>\n<p>Creating a dummy params.yaml without content gives:<\/p>\n<pre><code>WARNING: 'params.yaml' is empty.\nERROR: failed to reproduce 'dvc.yaml': Parameters 'important_parameter' are missing from 'params.yaml'.\n<\/code><\/pre>\n<p>What am I missing? Is this possible at all with the templating feature?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1619103654467,
        "Question_score":2,
        "Question_tags":"python|dvc",
        "Question_view_count":588,
        "Owner_creation_time":1542537900087,
        "Owner_last_access_time":1663940500713,
        "Owner_location":null,
        "Owner_reputation":147,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I think you don't need the templating feature in this case. As shown in this <a href=\"https:\/\/dvc.org\/doc\/command-reference\/params#examples-python-parameters-file\" rel=\"nofollow noreferrer\">example<\/a>:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>stages:\n  train:\n    cmd: python train.py\n    deps:\n      - users.csv\n    params:\n      - params.py:\n          - BOOL\n          - INT\n          - TrainConfig.EPOCHS\n          - TrainConfig.layers\n    outs:\n      - model.pkl\n<\/code><\/pre>\n<p>The way to redefine the default <code>params.yaml<\/code> is to specify the file name explicitly in the <code>params:<\/code> section:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>params:\n  - preproc.yaml:\n    - important_parameter\n<\/code><\/pre>\n<p>Also, when you create a stage either with <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run\" rel=\"nofollow noreferrer\"><code>dvc run<\/code><\/a> (not recommended) or <a href=\"https:\/\/dvc.org\/doc\/command-reference\/stage\/add\" rel=\"nofollow noreferrer\"><code>dvc stage add<\/code><\/a>, you can provide the params file name explicitly as a prefix:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc run -n train -d train.py -d logs\/ -o users.csv -f \\\n          -p parse_params.yaml:threshold,classes_num \\\n          python train.py\n<\/code><\/pre>\n<p>Here ^^ <code>parse_params.yaml<\/code> is a custom params file.<\/p>\n<p>Please, let me know if it solves the problem and if you have any other questions :)<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1619127242127,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67215839",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72651603,
        "Question_title":"Adding files that rely on pipeline outputs",
        "Question_body":"<p>In my workflow, I do the following:<\/p>\n<ol>\n<li>Acquire raw data (e.g. a video containing people)<\/li>\n<li>Transform it (e.g. automatically extract all crops with faces)<\/li>\n<li>Manually label them (e.g. identify the person in each crop). The labels are stored in json files along with the crops.<\/li>\n<li>Train a model on these data.<\/li>\n<\/ol>\n<p><strong>How should I track this pipeline with DVC?<\/strong><\/p>\n<p>My concerns:<\/p>\n<ol>\n<li>If stage 2 is changed (e.g. crops are extracted with a different size), the manual data should be invalidated (and so should the final model).<\/li>\n<li>The 3rd step is manual and therefore not precisely reproducible. But I do need its input to be reproducible.<\/li>\n<li>Stage 4 has an element of randomness, so it's not precisely reproducible either.<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1655411665790,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":39,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":1655415427550,
        "Answer_body":"<p>Stage 3 is manual so you can't really codify it or automate it, nor guarantee its reproducibility (due to possible human error). But there's a way to get you as close as possible:<\/p>\n<p>You could replace it with a helper script that just checks whether all the labels are annotated. If so, output a text file with content &quot;green&quot;, otherwise &quot;red&quot; (for example) and error out.<\/p>\n<p>Stage 4 should depend on both the inputs from stages 2 and 3, so it will only run if BOTH the face crops changed AND if they are thoroughly annotated.\nInternally, it first checks the semaphore file (from 3) and dies on red. On green, it trains the model :)<\/p>\n<p>The <a href=\"https:\/\/dvc.org\/doc\/command-reference\/dag#directed-acyclic-graph\" rel=\"nofollow noreferrer\">DAG<\/a> looks like this:<\/p>\n<pre><code>          +-----------+       \n          | 1-acquire |       \n          +-----------+       \n                *          \n                *          \n                *          \n          +---------+       \n          | 2-xform |       \n          +---------+       \n you      **        **     \n   --&gt;  **            **   \n       *                ** \n+---------+               *\n| 3-check |             ** \n+---------+           **   \n          **        **     \n            **    **       \n              *  *         \n          +---------+      \n          | 4-train |      \n          +---------+      \n<\/code><\/pre>\n<blockquote>\n<p>re randomness: while not ideal, non-determinism technically only <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#avoiding-unexpected-behavior\" rel=\"nofollow noreferrer\">affects intermediate stages<\/a> of the pipeline, because it causes everything after that to always run. In this case, since it's in the last stage, it won't affect DVC's job.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655523571800,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1655706111940,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72651603",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56456463,
        "Question_title":"Unable to ignore .DS_Store files in DVC",
        "Question_body":"<p>I use DVC to track my media files. I use MacOS and I want\".DS_Store\" files to be ignored by DVC. According to DVC documentation I can achieve it with  <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvcignore\" rel=\"nofollow noreferrer\">.dvcignore<\/a>. I created <code>.dvcignore<\/code> file with \".DS_Store\" rule. However every time \".DS_Store\" is created <code>dvc status<\/code> still says that content has changed<\/p>\n\n<p>Here is the little test to reproduce my issue:<\/p>\n\n<pre><code>$ git init\n$ dvc init\n\n# create directory to store data\n# and track it's content with DVC\n$ mkdir data\n$ dvc add data\n\n# Ignore .DS_Store files created by MacOS\n$ echo \".DS_Store\" &gt; .dvcignore\n\n# create .DS_Store in data dir\n$ touch \"data\/.DS_Store\"\n<\/code><\/pre>\n\n<p>If I understand DVC documentation correctly then <code>dvc status<\/code> should print something like \"Pipeline is up to date. Nothing to reproduce\". However <code>dvc status<\/code> gives me:<\/p>\n\n<pre><code>data.dvc:\n        changed outs:\n                modified:           data\n<\/code><\/pre>\n\n<p>How I can really ignore \".DS_Store\" files?<\/p>\n\n<p><strong>UPDATE:<\/strong> The .dvcignore support noticeably improved in latest versions and the problem is no more relevant.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1559722020690,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":326,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":1568948355943,
        "Answer_body":"<p>The current implementation of <code>.dvcignore<\/code> is very limited. Read more on it <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvcignore\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Please, mention that you are interested in this feature here - <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1876\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/issues\/1876<\/a>. That would help our team to prioritize issues properly.<\/p>\n\n<p>The possible workaround for now would be to use one of these approaches - <a href=\"https:\/\/stackoverflow.com\/questions\/18015978\/how-to-stop-creating-ds-store-on-mac\">How to stop creating .DS_Store on Mac?<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1559758177417,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56456463",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":71378280,
        "Question_title":"Error with DVC on Google Colab - dvc.scm.CloneError: Failed to clone repo",
        "Question_body":"<p>I'm having a problem trying to run &quot;dvc pull&quot; on Google Colab. I have two repositories (let's call them A and B) where repository A is for my machine learning codes and repository B is for my dataset.<\/p>\n<p>I've successfully pushed my dataset to repository B with DVC (using gdrive as my remote storage) and I also managed to successfully run &quot;dvc import&quot; (as well as &quot;dvc pull\/update&quot;) on my local project of repository A.<\/p>\n<p>The problem comes when I use colab to run my project. So what I did was the following:<\/p>\n<ol>\n<li>Created a new notebook on colab<\/li>\n<li>Successfully git-cloned my machine learning project (repository A)<\/li>\n<li>Ran &quot;!pip install dvc&quot;<\/li>\n<li>Ran &quot;!dvc pull -v&quot; (This is what causes the error)<\/li>\n<\/ol>\n<p>On step 4, I got the error (this is the full stack trace. Note that I changed the repo URL in the stack trace for confidentiality reasons)<\/p>\n<pre><code>2022-03-08 08:53:31,863 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/config.local' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/tmp' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/cache' to gitignore file.\n2022-03-08 08:53:31,916 DEBUG: Creating external repo https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git@3a3f4559efabff8ec74486da39b86688d1b98d75\n2022-03-08 08:53:31,916 DEBUG: erepo: git clone 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to a temporary dir\nEverything is up to date.\n2022-03-08 08:53:32,154 ERROR: failed to pull data from the cloud - Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 185, in clone\n    tmp_repo = clone_from()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1148, in clone_from\n    return cls._clone(git, url, to_path, GitCmdObjectDB, progress, multi_options, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1079, in _clone\n    finalize_process, decode_streams=False)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 176, in handle_process_output\n    return finalizer(process)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/util.py&quot;, line 386, in finalize_process\n    proc.wait(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 502, in wait\n    raise GitCommandError(remove_password_if_present(self.args), status, errstr)\ngit.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v --no-single-branch --progress https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git \/tmp\/tmp2x7y7xgedvc-clone\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 104, in clone\n    return Git.clone(url, to_path, progress=pbar.update_git, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/__init__.py&quot;, line 121, in clone\n    backend.clone(url, to_path, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 190, in clone\n    raise CloneError(url, to_path) from exc\nscmrepo.exceptions.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/command\/data_sync.py&quot;, line 41, in run\n    glob=self.args.glob,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/pull.py&quot;, line 38, in pull\n    run_cache=run_cache,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/fetch.py&quot;, line 50, in fetch\n    revs=revs,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 437, in used_objs\n    with_deps=with_deps,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/index.py&quot;, line 190, in used_objs\n    filter_info=filter_info,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/stage\/__init__.py&quot;, line 660, in get_used_objs\n    for odb, objs in out.get_used_objs(*args, **kwargs).items():\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 918, in get_used_objs\n    return self.get_used_external(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 973, in get_used_external\n    return dep.get_used_objs(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 94, in get_used_objs\n    used, _ = self._get_used_and_obj(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 108, in _get_used_and_obj\n    locked=locked, cache_dir=local_odb.cache_dir\n  File &quot;\/usr\/lib\/python3.7\/contextlib.py&quot;, line 112, in __enter__\n    return next(self.gen)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 35, in external_repo\n    path = _cached_clone(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 155, in _cached_clone\n    clone_path, shallow = _clone_default_branch(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 45, in wrapper\n    return deco(call, *dargs, **dkwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/flow.py&quot;, line 274, in wrap_with\n    return call()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 66, in __call__\n    return self._func(*self._args, **self._kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 220, in _clone_default_branch\n    git = clone(url, clone_path)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 106, in clone\n    raise CloneError(str(exc))\ndvc.scm.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\n2022-03-08 08:53:32,161 DEBUG: Analytics is enabled.\n2022-03-08 08:53:32,192 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n2022-03-08 08:53:32,193 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n<\/code><\/pre>\n<p>And btw this is how I cloned my git repository (repo A)<\/p>\n<pre><code>!git config - global user.name &quot;Zharfan&quot;\n!git config - global user.email &quot;zharfan@myemail.com&quot;\n!git clone https:\/\/&lt;MyTokenName&gt;:&lt;MyToken&gt;@link-to-my-repo-A.git\n<\/code><\/pre>\n<p>Does anyone know why? Any help would be greatly appreciated. Thank you in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":12,
        "Question_creation_time":1646641948613,
        "Question_score":1,
        "Question_tags":"git|dataset|google-colaboratory|dvc",
        "Question_view_count":707,
        "Owner_creation_time":1525227015313,
        "Owner_last_access_time":1663842020310,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1652856778060,
        "Answer_body":"<p>To summarize the discussion in the comments thread.<\/p>\n<p>Most likely it's happening since DVC can't get access to a private repo on GitLab. (The error message is obscure and should be fixed.)<\/p>\n<p>The same way you would not be able to run:<\/p>\n<pre><code>!git clone https:\/\/gitlab.com\/org\/&lt;private-repo&gt;\n<\/code><\/pre>\n<p>It also returns a pretty obscure error:<\/p>\n<pre><code>Cloning into '&lt;private-repo&gt;'...\nfatal: could not read Username for 'https:\/\/gitlab.com': No such device or address\n<\/code><\/pre>\n<p>(I think it's something related to how tty is setup in Colab?)<\/p>\n<p>The best approach to solve this is to use SSH like described <a href=\"https:\/\/medium.com\/@sadiaafrinpurba\/how-to-clone-private-github-repo-in-google-colab-using-ssh-77384cfef18f\" rel=\"nofollow noreferrer\">here<\/a> for example.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1647022114533,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71378280",
        "Question_exclusive_tag":"DVC"
    }
]