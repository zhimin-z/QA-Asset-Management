[
    {
        "Question_id":60299478,
        "Question_title":"Setting parameters in Kedro Notebook",
        "Question_body":"<p>Is it possible to overwrite properties taken from the parameters.yaml file within a Kedro notebook?<\/p>\n\n<p>I am trying to dynamically change parameter values within a notebook. I would like to be able to give users the ability to run a standard pipeline but with customizable parameters. I don't want to change the YAML file, I just want to change the parameter for the life of the notebook.<\/p>\n\n<p>I have tried editing the params within the context but this has no affect.<\/p>\n\n<pre><code>context.params.update({\"test_param\": 2})\n<\/code><\/pre>\n\n<p>Am I missing something or is this not an intended use case?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1582112145783,
        "Question_last_edit_time":1583420995670,
        "Question_score":3,
        "Question_tags":"python|kedro",
        "Question_view_count":586,
        "Owner_creation_time":1582111403050,
        "Owner_last_access_time":1661262270877,
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":"<p>Kedro supports specifying <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/03_configuration.html#specifying-extra-parameters\" rel=\"nofollow noreferrer\">extra parameters<\/a> from the command line by running<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>kedro run --params \"key1:value1,key2:value2\"\n<\/code><\/pre>\n\n<p>which solves your second use case.<\/p>\n\n<p>As for the notebook use case, updating <code>context.params<\/code> does not have any effect since the context does not store the parameters on <code>self<\/code> but rather pulls them from the config every time the property is being called.<\/p>\n\n<p>However you can still add extra parameters to the context object after it being instantiated:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>extra_params = context._extra_params or {}\nextra_params.update({\"test_param\": 2})\ncontext._extra_params = extra_params\n<\/code><\/pre>\n\n<p>This will update extra parameters that are applied on top of regular parameters coming from the config.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1582114523347,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60299478",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":58443788,
        "Question_title":"Where to perform the saving of an nodeoutput in Kedro?",
        "Question_body":"<p>In Kedro, we can pipeline different nodes and partially run some nodes. When we are partially running some nodes, we need to save some inputs from the nodes somewhere so that when another node is run it can access the data that the previous node has generated. However, in which file do we write the code for this - pipeline.py, run.py or nodes.py?<\/p>\n\n<p>For instance, I am trying to save a dir path directly to the DataCatalog under a variable name 'model_path'. <\/p>\n\n<p>Snippet from pipeline.py:<\/p>\n\n<pre><code>    # A mapping from a pipeline name to a ``Pipeline`` object.\ndef create_pipelines(**kwargs) -&gt; Dict[str, Pipeline]:\nio = DataCatalog(dict(\n    model_path=MemoryDataSet()\n))\n\nio.save('model_path', \"data\/06_models\/model_test\")\nprint('****', io.exists('model_path'))\n\npipeline = Pipeline([\n    node(\n        split_files,\n        [\"data_csv\", \"parameters\"],\n        [\"train_filenames\", \"val_filenames\", \"train_labels\", \"val_labels\"],\n        name=\"splitting filenames\"\n    ),\n    # node(\n    #     create_and_train,\n    #     [\"train_filenames\", \"val_filenames\", \"train_labels\", \"val_labels\", \"parameters\"],\n    #     \"model_path\",\n    #     name=\"Create Dataset, Train and Save Model\"\n    # ),\n    node(\n        validate_model,\n        [\"val_filenames\", \"val_labels\", \"model_path\"],\n        None,\n        name=\"Validate Model\",\n    )\n\n]).decorate(decorators.log_time, decorators.mem_profile)\n\nreturn {\n    \"__default__\": pipeline\n}\n<\/code><\/pre>\n\n<p>However, I get the following error when I Kedro run:<\/p>\n\n<pre><code>ValueError: Pipeline input(s) {'model_path'} not found in the DataCatalog\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1571371429583,
        "Question_last_edit_time":1571671894437,
        "Question_score":4,
        "Question_tags":"python|tensorflow|kedro",
        "Question_view_count":1606,
        "Owner_creation_time":1545311054090,
        "Owner_last_access_time":1663916183127,
        "Owner_location":null,
        "Owner_reputation":170,
        "Owner_up_votes":113,
        "Owner_down_votes":2,
        "Owner_views":33,
        "Answer_body":"<p>Node inputs are automatically loaded by Kedro from the <code>DataCatalog<\/code> before being passed to the node function. Node outputs are consequently saved to the DataCatalog after the node successfully produces some data. DataCatalog configuration by default is taken from <code>conf\/base\/catalog.yml<\/code>. <\/p>\n\n<p>In your example <code>model_path<\/code> is produced by <code>Create Dataset, Train and Save Model<\/code> node and then consumed by <code>Validate Model<\/code>. If required dataset definition is not found in the <code>conf\/base\/catalog.yml<\/code>, Kedro will try to store this dataset in memory using <code>MemoryDataSet<\/code>. This will work if you run the pipeline that contains both <code>Create Dataset...<\/code> and <code>Validate Model<\/code> nodes (given no other issues arise). However, when you are trying to run <code>Validate Model<\/code> node alone, Kedro attempts to read <code>model_path<\/code> dataset from memory, which doesn't exist there.<\/p>\n\n<p>So, <strong>TLDR<\/strong>:<\/p>\n\n<p>To mitigate this, you need to:<\/p>\n\n<p>a) persist <code>model_path<\/code> by adding something like the following to your <code>conf\/base\/catalog.yml<\/code>:<\/p>\n\n<pre><code>model_path:\n  type: TextLocalDataSet\n  filepath: data\/02_intermediate\/model_path.txt\n<\/code><\/pre>\n\n<p>b) run <code>Create Dataset, Train and Save Model<\/code> node (and its dependencies) at least once<\/p>\n\n<p>After completing a) and b) you should be able to start running <code>Validate Model<\/code> separately.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1571671635460,
        "Answer_last_edit_time":1571672436533,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58443788",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":60355240,
        "Question_title":"Pipeline can't find nodes in kedro",
        "Question_body":"<p>I was following <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/03_tutorial\/04_create_pipelines.html\" rel=\"nofollow noreferrer\">pipelines tutorial<\/a>, create all needed files, started the kedro with <code>kedro run --node=preprocessing_data<\/code> but got stuck with such error message:<\/p>\n\n<pre><code>ValueError: Pipeline does not contain nodes named ['preprocessing_data'].\n<\/code><\/pre>\n\n<p>If I run kedro without <code>node<\/code> parameter, I receive<\/p>\n\n<pre><code>kedro.context.context.KedroContextError: Pipeline contains no nodes\n<\/code><\/pre>\n\n<p>Contents of the files:<\/p>\n\n<pre><code>src\/project\/pipelines\/data_engineering\/nodes.py\ndef preprocess_data(data: SparkDataSet) -&gt; None:\n    print(data)\n    return\n<\/code><\/pre>\n\n<pre><code>src\/project\/pipelines\/data_engineering\/pipeline.py\ndef create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                func=preprocess_data,\n                inputs=\"data\",\n                outputs=\"preprocessed_data\",\n                name=\"preprocessing_data\",\n            ),\n        ]\n    )\n<\/code><\/pre>\n\n<pre><code>src\/project\/pipeline.py\ndef create_pipelines(**kwargs) -&gt; Dict[str, Pipeline]:\n    de_pipeline = de.create_pipeline()\n    return {\n        \"de\": de_pipeline,\n        \"__default__\": Pipeline([])\n    }\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1582395101757,
        "Question_last_edit_time":1583175186660,
        "Question_score":4,
        "Question_tags":"python|kedro",
        "Question_view_count":2155,
        "Owner_creation_time":1324477592580,
        "Owner_last_access_time":1663840044463,
        "Owner_location":null,
        "Owner_reputation":1315,
        "Owner_up_votes":85,
        "Owner_down_votes":5,
        "Owner_views":91,
        "Answer_body":"<p>I think it looks like you need to have the pipeline in <code>__default__<\/code>.\ne.g.<\/p>\n\n<pre><code>def create_pipelines(**kwargs) -&gt; Dict[str, Pipeline]:\n    de_pipeline = de.create_pipeline()\n    return {\n        \"de\": data_engineering_pipeline,\n        \"__default__\": data_engineering_pipeline\n    }\n<\/code><\/pre>\n\n<p>Then <code>kedro run --node=preprocessing_data<\/code> works for me.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1582427680590,
        "Answer_last_edit_time":null,
        "Answer_score":8.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60355240",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":62492785,
        "Question_title":"Kedro: How to pass \"list\" parameters from command line?",
        "Question_body":"<p>I'd like to control kedro parameters via command line.<\/p>\n<p>According to <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/03_configuration.html#specifying-parameters-at-runtime\" rel=\"nofollow noreferrer\">docs<\/a>, kedro can specify runtime parameters as follows:<\/p>\n<pre><code>kedro run --params key:value\n&gt; {'key': 'value'}\n<\/code><\/pre>\n<p>It works. In the same way, I try to specify <strong>list<\/strong> parameters like this:<\/p>\n<pre><code>kedro run --params keys:['value1']\n&gt; {'keys': '[value1]'}\n<\/code><\/pre>\n<p>It doesn't work because kedro interplets <strong>not list but str<\/strong>. Probably, <a href=\"https:\/\/stackoverflow.com\/a\/61455703\/9489217\">this answer<\/a> could be related.<\/p>\n<p>Hope to mention a couple things to make kedro evaluates list parameters like yaml.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1592695157830,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_tags":"python|kedro",
        "Question_view_count":1413,
        "Owner_creation_time":1521002414380,
        "Owner_last_access_time":1663833806703,
        "Owner_location":"Japan",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>By default the <code>kedro<\/code> command line won't typecast parameters beyond simpler numeric types. More complicated parameters should be handled via the <code>parameters.yml<\/code> file.<\/p>\n<p>That said, if you <em>really<\/em> want to do this, you can modify your <code>kedro_cli.py<\/code> to support this. Specifically, you'd want to modify the <code>_split_params<\/code> callback function in the file. The easiest thing here would likely be to change the line that reads<\/p>\n<pre><code>result[key] = _try_convert_to_numeric(value)\n<\/code><\/pre>\n<p>which handles parsing simple numeric types to<\/p>\n<pre><code>result[key] = json.loads(value)\n<\/code><\/pre>\n<p>to make it parse a wider range of types. That is, parse the CLI parameter you pass in as <code>json<\/code> (so you'll also need to be mindful of quotes and making sure that you pass in valid <code>json<\/code> syntax.<\/p>\n<p>If that doesn't work, you can try adding your own syntax and parsing it in that function. However, my recommendation is to avoid depending on fragile string parameter evaluation from CLI and use the <code>parameters.yml<\/code> instead.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1592700529053,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62492785",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":73460511,
        "Question_title":"How to use generators with kedro?",
        "Question_body":"<p>Thanks to <a href=\"https:\/\/www.dabeaz.com\/generators\/\" rel=\"nofollow noreferrer\">David Beazley's slides on Generators<\/a> I'm quite taken with using generators for data processing in order to keep memory consumption minimal. Now I'm working on my first kedro project, and my question is how I can use generators in kedro. When I have a node that yields a generator, and then run it with <code>kedro run --node=example_node<\/code>, I get the following error:<\/p>\n<pre><code>DataSetError: Failed while saving data to data set MemoryDataSet().\ncan't pickle generator objects\n<\/code><\/pre>\n<p>Am I supposed to always load all my data into memory when working with kedro?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661264859883,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_tags":"python-3.x|pipeline|kedro|generator-expression",
        "Question_view_count":46,
        "Owner_creation_time":1481393494750,
        "Owner_last_access_time":1663852667270,
        "Owner_location":null,
        "Owner_reputation":19,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>Hi @ilja to do this you may need to change the type of <code>assignment<\/code> operation that <code>MemoryDataSet<\/code> applies.<\/p>\n<p>In your catalog, declare your datasets explicitly, change the <code>copy_mode<\/code> to one of <code>copy<\/code> or <code>assign<\/code>. I think <code>assign<\/code> may be your best bet here...<\/p>\n<p><a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.MemoryDataSet.html\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.MemoryDataSet.html<\/a><\/p>\n<p>I hope this works, but am not 100% sure.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1661267379900,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73460511",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":66158536,
        "Question_title":"What does this python function signature means in Kedro Tutorial?",
        "Question_body":"<p>I am looking at Kedro Library as my team are looking into using it for our data pipeline.<\/p>\n<p>While going to the offical tutorial - Spaceflight.<\/p>\n<p>I came across this function:<\/p>\n<pre><code>def preprocess_companies(companies: pd.DataFrame) -&gt; pd.DataFrame:\n&quot;&quot;&quot;Preprocess the data for companies.\n\n    Args:\n        companies: Source data.\n    Returns:\n        Preprocessed data.\n\n&quot;&quot;&quot;\n\ncompanies[&quot;iata_approved&quot;] = companies[&quot;iata_approved&quot;].apply(_is_true)\n\ncompanies[&quot;company_rating&quot;] = companies[&quot;company_rating&quot;].apply(_parse_percentage)\n\nreturn companies\n<\/code><\/pre>\n<ul>\n<li>companies is the name of the csv file containing the data<\/li>\n<\/ul>\n<p>Looking at the function, my assumption is that <code>(companies: pd.Dafarame)<\/code> is the shorthand to read the &quot;companies&quot; dataset as a dataframe. If so, I do not understand what does <code>-&gt; pd.Dataframe<\/code> at the end means<\/p>\n<p>I tried looking at python documentation regarding such style of code but I did not managed to find any<\/p>\n<p>Much help is appreciated to assist me in understanding this.<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1613060367020,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_tags":"python-3.x|kedro",
        "Question_view_count":135,
        "Owner_creation_time":1504515330837,
        "Owner_last_access_time":1652156152220,
        "Owner_location":null,
        "Owner_reputation":5,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>The <code>-&gt;<\/code> notation is <a href=\"https:\/\/docs.python.org\/3\/library\/typing.html\" rel=\"nofollow noreferrer\">type hinting<\/a>, as is the <code>:<\/code> part in the <code>companies: pd.DataFrame<\/code> function definition. This is not essential to do in Python but many people like to include it. The function definition would work exactly the same if it didn't contain this but instead read:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def preprocess_companies(companies):\n<\/code><\/pre>\n<p>This is a general Python thing rather than anything kedro-specific.<\/p>\n<p>The way that kedro registers <code>companies<\/code> as a kedro dataset is completely separate from this function definition and is done through the catalog.yml file:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>companies:\n  type: pandas.CSVDataSet\n  filepath: data\/01_raw\/companies.csv\n<\/code><\/pre>\n<p>There will then a <em>node<\/em> defined (in pipeline.py) to specify that the <code>preprocess_companies<\/code> function should take as input the kedro dataset <code>companies<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>node(\n    func=preprocess_companies,\n    inputs=&quot;companies&quot;,  # THIS LINE REFERS TO THE DATASET NAME\n    outputs=&quot;preprocessed_companies&quot;,\n    name=&quot;preprocessing_companies&quot;,\n),\n<\/code><\/pre>\n<p>In theory the name of the parameter in the function itself could be completely different, e.g.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def preprocess_companies(anything_you_want):\n<\/code><\/pre>\n<p>... although it is very common to give it the same name as the dataset.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1613062868173,
        "Answer_last_edit_time":1613067465070,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66158536",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":68353241,
        "Question_title":"Kedro can not find SQL Server table",
        "Question_body":"<p>I have these two datasets defined:<\/p>\n<pre><code>flp_test_query:\n  type: pandas.SQLQueryDataSet\n  credentials: dw_dev_credentials\n  sql: select numero from dwdb.dwschema.flp_tst\n  load_args:\n    index_col: [numero]\n\nflp_test:\n  type: pandas.SQLTableDataSet\n  credentials: dw_dev_credentials\n  table_name: flp_tst\n  load_args:\n    index_col: ['numero']\n    columns: ['numero']\n  save_args:\n    if_exists: 'append'\n<\/code><\/pre>\n<p>However, I only manged to get <code>flp_test_query<\/code> working, as when I try to access <code>flp_tst<\/code> I get this error:<\/p>\n<blockquote>\n<p>ValueError: Table flp_tst not found<\/p>\n<\/blockquote>\n<p>I did try to define table name as <code>table_name: dwschema.flp_tst<\/code> and <code>table_name: dwdb.dwschema.flp_tst<\/code> but all trew the same error. What am I missing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1626119761090,
        "Question_last_edit_time":1644169948123,
        "Question_score":0,
        "Question_tags":"sql-server|pymssql|kedro",
        "Question_view_count":117,
        "Owner_creation_time":1271930452580,
        "Owner_last_access_time":1664057428157,
        "Owner_location":null,
        "Owner_reputation":5469,
        "Owner_up_votes":168,
        "Owner_down_votes":3,
        "Owner_views":232,
        "Answer_body":"<p>From the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.extras.datasets.pandas.SQLTableDataSet.html\" rel=\"nofollow noreferrer\">docs<\/a> it looks like you can specify the schema in <code>load_args<\/code>, eg<\/p>\n<pre><code>  load_args:\n    index_col: ['numero']\n    columns: ['numero']\n    schema: 'dwschema'\n<\/code><\/pre>\n<p>or<\/p>\n<pre><code>load_args = {&quot;schema&quot;,&quot;dwschema&quot;}\ndata_set = SQLTableDataSet(table_name=table_name,\n                           credentials=credentials,\n                            load_args=load_args)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1626124577253,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68353241",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":61645397,
        "Question_title":"How do I add many CSV files to the catalog in Kedro?",
        "Question_body":"<p>I have hundreds of CSV files that I want to process similarly. For simplicity, we can assume that they are all in <code>.\/data\/01_raw\/<\/code> (like <code>.\/data\/01_raw\/1.csv<\/code>, <code>.\/data\/02_raw\/2.csv<\/code>) etc. I would much rather not give each file a different name and keep track of them individually when building my pipeline. I would like to know if there is any way to read all of them in bulk by specifying something in the <code>catalog.yml<\/code> file?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1588799145203,
        "Question_last_edit_time":1588803043230,
        "Question_score":4,
        "Question_tags":"python|kedro",
        "Question_view_count":806,
        "Owner_creation_time":1453233461910,
        "Owner_last_access_time":1663631340010,
        "Owner_location":null,
        "Owner_reputation":299,
        "Owner_up_votes":35,
        "Owner_down_votes":1,
        "Owner_views":16,
        "Answer_body":"<p>You are looking for <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html#partitioned-dataset\" rel=\"nofollow noreferrer\">PartitionedDataSet<\/a>. In your example, the <code>catalog.yml<\/code> might look like this:<\/p>\n<pre><code>my_partitioned_dataset:\n  type: &quot;PartitionedDataSet&quot;\n  path: &quot;data\/01_raw&quot;\n  dataset: &quot;pandas.CSVDataSet&quot;\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1588804881827,
        "Answer_last_edit_time":1600445892403,
        "Answer_score":8.0,
        "Question_favorite_count":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61645397",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":60382704,
        "Question_title":"Convert csv into parquet in kedro",
        "Question_body":"<p>I have pretty big CSV that would not fit into memory, and I need to convert it into .parquet file to work with vaex.<\/p>\n\n<p>Here is my catalog:<\/p>\n\n<pre><code>raw_data:\n    type: kedro.contrib.io.pyspark.SparkDataSet\n    filepath: data\/01_raw\/data.csv\n    file_format: csv\n\nparquet_data:\n    type: ParquetLocalDataSet\n    filepath: data\/02_intermediate\/data.parquet\n<\/code><\/pre>\n\n<p>node:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def convert_to_parquet(data: SparkDataSet) -&gt; ParquetLocalDataSet:\n    return data.coalesce(1)\n<\/code><\/pre>\n\n<p>and a pipeline:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                func=convert_to_parquet,\n                inputs=\"raw_data\",\n                outputs=\"parquet_data\",\n                name=\"data_to_parquet\",\n            ),\n        ]\n    )\n<\/code><\/pre>\n\n<p>But if I do <code>kedro run<\/code> I receive this error <code>kedro.io.core.DataSetError: Failed while saving data to data set ParquetLocalDataSet(engine=auto, filepath=data\/02_intermediate\/data.parquet, save_args={}).\n'DataFrame' object has no attribute 'to_parquet'<\/code><\/p>\n\n<p>What should I fix to get my dataset converted?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1582572566660,
        "Question_last_edit_time":1583421031537,
        "Question_score":2,
        "Question_tags":"python|kedro",
        "Question_view_count":498,
        "Owner_creation_time":1324477592580,
        "Owner_last_access_time":1663840044463,
        "Owner_location":null,
        "Owner_reputation":1315,
        "Owner_up_votes":85,
        "Owner_down_votes":5,
        "Owner_views":91,
        "Answer_body":"<p>You could try the following. This has worked for me in the past.<\/p>\n\n<pre><code>parquet_data:\n    type: kedro.contrib.io.pyspark.SparkDataSet\n    file_format: 'parquet'\n    filepath: data\/02_intermediate\/data.parquet\n    save_args:\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1582574347453,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60382704",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":72752043,
        "Question_title":"Load existing data catalog programmatically",
        "Question_body":"<ol>\n<li><p>I want to write pytest unit test in <strong>Kedro 0.17.5<\/strong>. They need to perform integrity checks on dataframes created by the pipeline.\nThese dataframes are specified in the <code>catalog.yml<\/code> and already persisted successfully using <code>kedro run<\/code>. The <code>catalog.yml<\/code> is in <code>conf\/base<\/code>.<\/p>\n<\/li>\n<li><p>I have a test module <code>test_my_dataframe.py<\/code> in <code>src\/tests\/pipelines\/my_pipeline\/<\/code>.<\/p>\n<\/li>\n<\/ol>\n<p>How can I load the data catalog based on my <code>catalog.yml<\/code> programmatically from within <code>test_my_dataframe.py<\/code> in order to properly access my specified dataframes?<\/p>\n<p>Or, for that matter, how can I programmatically load the whole project context (including the data catalog) in order to also execute nodes etc.?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656142241737,
        "Question_last_edit_time":1656265981920,
        "Question_score":2,
        "Question_tags":"python|unit-testing|pytest|kedro",
        "Question_view_count":107,
        "Owner_creation_time":1258185382660,
        "Owner_last_access_time":1663666121847,
        "Owner_location":null,
        "Owner_reputation":333,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Answer_body":"<ol>\n<li><p>For unit testing, we test just the function which we are testing, and everything external to the function we should mock\/patch. Check if you really need kedro project context while writing the unit test.<\/p>\n<\/li>\n<li><p>If you really need project context in test, you can do something like following<\/p>\n<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from kedro.framework.project import configure_project\nfrom kedro.framework.session import KedroSession\n\nwith KedroSession.create(package_name=&quot;demo&quot;, project_path=Path.cwd()) as session:\n    context = session.load_context()\n    catalog = context.catalog\n<\/code><\/pre>\n<p>or you can also create pytest fixture to use it again and again with scope of your choice.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@pytest.fixture\ndef get_project_context():\n    session = KedroSession.create(\n        package_name=&quot;demo&quot;,\n        project_path=Path.cwd()\n    )\n    _activate_session(session, force=True)\n    context = session.load_context()\n    return context\n<\/code><\/pre>\n<p>Different args supported by KedroSession create you can check it here <a href=\"https:\/\/kedro.readthedocs.io\/en\/0.17.5\/kedro.framework.session.session.KedroSession.html#kedro.framework.session.session.KedroSession.create\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/0.17.5\/kedro.framework.session.session.KedroSession.html#kedro.framework.session.session.KedroSession.create<\/a><\/p>\n<p>To read more about pytest fixture you can refer to <a href=\"https:\/\/docs.pytest.org\/en\/6.2.x\/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session\" rel=\"nofollow noreferrer\">https:\/\/docs.pytest.org\/en\/6.2.x\/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1656143487270,
        "Answer_last_edit_time":1656143791537,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72752043",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":65900415,
        "Question_title":"How do I add a directory of .wav files to the Kedro data catalogue?",
        "Question_body":"<p>This is my first time trying to use the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Kedro<\/a> package.<\/p>\n<p>I have a list of .wav files in an s3 bucket, and I'm keen to know how I can have them available within the Kedro data catalog.<\/p>\n<p>Any thoughts?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1611660152193,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_tags":"amazon-s3|kedro",
        "Question_view_count":294,
        "Owner_creation_time":1500383313377,
        "Owner_last_access_time":1643748878623,
        "Owner_location":"London, UK",
        "Owner_reputation":851,
        "Owner_up_votes":39,
        "Owner_down_votes":3,
        "Owner_views":86,
        "Answer_body":"<p>I don't believe there's currently a dataset format that handles <code>.wav<\/code> files. You'll need to build a <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/03_custom_datasets.html\" rel=\"nofollow noreferrer\">custom dataset<\/a> that uses something like <a href=\"https:\/\/docs.python.org\/3\/library\/wave.html\" rel=\"nofollow noreferrer\">Wave<\/a> - not as much work as it sounds!<\/p>\n<p>This will enable you to do something like this in your catalog:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>dataset:\n  type: my_custom_path.WaveDataSet\n  filepath: path\/to\/individual\/wav_file.wav # this can be a s3:\/\/url\n<\/code><\/pre>\n<p>and you can then access your WAV data natively within your Kedro pipeline. You can do this for each <code>.wav<\/code> file you have.<\/p>\n<p>If you wanted to be able to access a whole folders worth of wav files, you might want to explore the notion of a &quot;wrapper&quot; dataset like the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.PartitionedDataSet.html\" rel=\"nofollow noreferrer\">PartitionedDataSet<\/a> whose <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html#partitioned-dataset-definition\" rel=\"nofollow noreferrer\">usage guide<\/a> can be found in the documentation.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1611661436393,
        "Answer_last_edit_time":1611661825370,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65900415",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":68796641,
        "Question_title":"Building an autoencoder with Keras and Kedro",
        "Question_body":"<p>I'm trying to build an autoencoder, which I'm sure I'm doing something wrong. I tried separating the creation of the model from the actual training but this is not really working out for me and is giving me the following error.<\/p>\n<pre><code>AssertionError: Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 310), dtype=tf.float32, name=None), name='dense_7\/Sigmoid:0', description=&quot;created by layer 'dense_7'&quot;)\n<\/code><\/pre>\n<p>I'm doing this all using the Kedro framework. I have a pipeline.py file with the pipeline definition and a nodes.py with the functions that I want to use. So far, this is my project structure:<\/p>\n<p>pipelines.py:<\/p>\n<pre><code>from kedro.pipeline import Pipeline, node\nfrom .nodes.autoencoder_nodes import *\n\ndef train_autoencoder_pipeline():\n    return Pipeline([\n        # Build neural network\n        node(\n            build_models, \n            inputs=[\n                &quot;train_x&quot;, \n                &quot;params:autoencoder_n_hidden_layers&quot;,\n                &quot;params:autoencoder_latent_space_size&quot;,\n                &quot;params:autoencoder_regularization_strength&quot;,\n                &quot;params:seed&quot;\n                ],\n            outputs=dict(\n                pre_train_autoencoder=&quot;pre_train_autoencoder&quot;,\n                pre_train_encoder=&quot;pre_train_encoder&quot;,\n                pre_train_decoder=&quot;pre_train_decoder&quot;\n            ), name=&quot;autoencoder-create-models&quot;\n        ),\n        # Scale features\n        node(fit_scaler, inputs=&quot;train_x&quot;, outputs=&quot;autoencoder_scaler&quot;, name=&quot;autoencoder-fit-scaler&quot;),\n        node(tranform_scaler, inputs=[&quot;autoencoder_scaler&quot;, &quot;train_x&quot;], outputs=&quot;autoencoder_scaled_train_x&quot;, name=&quot;autoencoder-scale-train&quot;),\n        node(tranform_scaler, inputs=[&quot;autoencoder_scaler&quot;, &quot;test_x&quot;], outputs=&quot;autoencoder_scaled_test_x&quot;, name=&quot;autoencoder-scale-test&quot;),\n\n        # Train autoencoder\n        node(\n            train_autoencoder, \n            inputs=[\n                &quot;autoencoder_scaled_train_x&quot;,\n                &quot;autoencoder_scaled_test_x&quot;,\n                &quot;pre_train_autoencoder&quot;, \n                &quot;pre_train_encoder&quot;, \n                &quot;pre_train_decoder&quot;,\n                &quot;params:autoencoder_epochs&quot;,\n                &quot;params:autoencoder_batch_size&quot;,\n                &quot;params:seed&quot;\n            ],\n            outputs= dict(\n                autoencoder=&quot;autoencoder&quot;,\n                encoder=&quot;encoder&quot;,\n                decoder=&quot;decoder&quot;,\n                autoencoder_history=&quot;autoencoder_history&quot;,\n            ),\n            name=&quot;autoencoder-train-model&quot;\n        )])\n<\/code><\/pre>\n<p>nodes.py:<\/p>\n<pre><code>from sklearn.preprocessing import MinMaxScaler\nfrom tensorflow import keras\nimport tensorflow as tf\n\nfrom typing import Dict, Any, Tuple\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport logging\n\n\ndef build_models(data: pd.DataFrame, n_hidden_layers: int, latent_space_size: int, retularization_stregth: float, seed: int) -&gt; Tuple[keras.Model, keras.Model, keras.Model]:\n    assert n_hidden_layers &gt;= 1, &quot;There must be at least 1 hidden layer for the autoencoder&quot;\n    \n    n_features = data.shape[1]\n    tf.random.set_seed(seed)\n    input_layer = keras.Input(shape=(n_features,))\n    \n    hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(input_layer)\n    hidden = keras.layers.LeakyReLU()(hidden)\n    \n    for _ in range(n_hidden_layers - 1):\n        hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(hidden)\n        hidden = keras.layers.LeakyReLU()(hidden)\n    \n    encoded = keras.layers.Dense(latent_space_size, activation=&quot;sigmoid&quot;)(hidden)\n\n    hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(encoded)\n    hidden = keras.layers.LeakyReLU()(hidden)\n    \n    for _ in range(n_hidden_layers - 1):\n        hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(hidden)\n        hidden = keras.layers.LeakyReLU()(hidden)\n    \n\n    decoded = keras.layers.Dense(n_features, activation=&quot;sigmoid&quot;)(hidden)\n\n    # Defines the neural networks\n    autoencoder = keras.models.Model(inputs=input_layer, outputs=decoded)\n    encoder = keras.models.Model(inputs=input_layer, outputs=encoded)\n    decoder = keras.models.Model(inputs=input_layer, outputs=decoded)\n    autoencoder.compile(optimizer=&quot;adam&quot;, loss=&quot;mean_absolute_error&quot;)\n\n    return dict(\n        pre_train_autoencoder=autoencoder,\n        pre_train_encoder=encoder,\n        pre_train_decoder=decoder\n    )\n\ndef fit_scaler(data: pd.DataFrame) -&gt; MinMaxScaler:\n    scaler = MinMaxScaler()\n    scaler.fit(data)\n    return scaler\n\ndef tranform_scaler(scaler: MinMaxScaler, data: pd.DataFrame) -&gt; np.array:\n    return scaler.transform(data)\n\ndef train_autoencoder(\n    train_x: pd.DataFrame, test_x: pd.DataFrame, \n    autoencoder: keras.Model, encoder: keras.Model, decoder: keras.Model, \n    epochs: int, batch_size: int, seed: int) -&gt; Dict[str, Any]:\n\n    tf.random.set_seed(seed)\n    callbacks = [\n        keras.callbacks.History(),\n        keras.callbacks.EarlyStopping(patience=3)\n    ]\n    logging.info(train_x.shape)\n    logging.info(test_x.shape)\n\n    history = autoencoder.fit(\n        train_x, train_x,\n        validation_data=(test_x, test_x),\n        callbacks=callbacks, \n        epochs=epochs,\n        batch_size=batch_size\n    )\n\n    return dict(\n        autoencoder=autoencoder,\n        encoder=encoder,\n        decoder=decoder,\n        autoencoder_history=history,\n    )\n<\/code><\/pre>\n<p>catalog.yaml:<\/p>\n<pre><code>autoencoder_scaler:\n  type: pickle.PickleDataSet\n  filepath: data\/06_models\/autoencoder_scaler.pkl\n\nautoencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/autoencoder.h5\n\nencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/encoder.h5\n\ndecoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/decoder.h5\n\nautoencoder_train_x:\n  type: pandas.CSVDataSet\n  filepath: data\/04_feature\/autoencoder_train_x.csv\n\nautoencoder_test_x:\n  type: pandas.CSVDataSet\n  filepath: data\/04_feature\/autoencoder_test_x.csv\n<\/code><\/pre>\n<p>And finally parameters.yaml:<\/p>\n<pre><code>seed: 200\n# Autoencoder\nautoencoder_n_hidden_layers: 3\nautoencoder_latent_space_size: 15\nautoencoder_epochs: 100\nautoencoder_batch_size: 32\nautoencoder_regularization_strength: 0.001\n<\/code><\/pre>\n<p>I believe that Keras is not seeing the whole graph since they will be out of the scope for the buld_models function, but I'm not sure whether this is the case, or how to fix it. Any help would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629078707297,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_tags":"python|keras|deep-learning|autoencoder|kedro",
        "Question_view_count":245,
        "Owner_creation_time":1423164285360,
        "Owner_last_access_time":1663938741010,
        "Owner_location":"Itabira, Brazil",
        "Owner_reputation":856,
        "Owner_up_votes":60,
        "Owner_down_votes":3,
        "Owner_views":106,
        "Answer_body":"<p>I was able to set up your project locally and reproduce the error. To fix it, I had to add the <code>pre_train_*<\/code> outputs to the catalog as well. Therefore, it's my <code>catalog.yaml<\/code> file:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>autoencoder_scaler:\n  type: pickle.PickleDataSet\n  filepath: data\/06_models\/autoencoder_scaler.pkl\n\npre_train_autoencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/pre_train_autoencoder.h5\n\npre_train_encoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/pre_train_encoder.h5\n\npre_train_decoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/pre_train_decoder.h5\n\nautoencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/autoencoder.h5\n\nencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/encoder.h5\n\ndecoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/decoder.h5\n<\/code><\/pre>\n<p>Also, I changed the return of <code>train_autoencoder<\/code> node to:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>return dict(\n    autoencoder=autoencoder,\n    autoencoder_history=history.history,\n)\n<\/code><\/pre>\n<p>Note that I changed the <code>autoencoder_history<\/code> to return <code>history.history<\/code> since <code>MemoryDataset<\/code> can't pickle the object <code>history<\/code> by itself. The <code>history.history<\/code> is a dictionary with losses of train and validation sets.<\/p>\n<p>You can find the complete code <a href=\"https:\/\/gist.github.com\/arnaldog12\/a3f7801fe3910c02f4bcea8be61b910c\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1629132605447,
        "Answer_last_edit_time":1629217805313,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68796641",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":58911398,
        "Question_title":"Are S3 Kedro datasets thread-safe?",
        "Question_body":"<p><code>CSVS3DataSet<\/code>\/<code>HDFS3DataSet<\/code>\/<code>HDFS3DataSet<\/code> use <code>boto3<\/code>, which is known to be not thread-safe <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/guide\/resources.html?highlight=multithreading#multithreading-multiprocessing\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/guide\/resources.html?highlight=multithreading#multithreading-multiprocessing<\/a><\/p>\n\n<p>Is it OK to use these datasets with the ParallelRunner?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1574069164940,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_tags":"python|amazon-s3|boto3|kedro",
        "Question_view_count":146,
        "Owner_creation_time":1395230906503,
        "Owner_last_access_time":1652366319603,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":129,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":51,
        "Answer_body":"<p><code>Kedro<\/code> uses <code>s3fs<\/code>, which uses <code>boto3<\/code> library to access S3. <code>Boto3<\/code> is not thread-safe indeed, but only if you are trying to reuse the same Session object.<\/p>\n\n<p>All <code>Kedro<\/code> S3 datasets maintain separate instances of <code>S3FileSystem<\/code>, which means separate boto sessions, so it's safe.<\/p>\n\n<p>It's probably not great in terms of performance, and if you work with hundreds of S3 data sets in parallel, or thousands of small S3 datasets sequentially - the pipeline might run quite long and even fail on connection errors, but you are totally safe with a few dozens of them.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1574069164940,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58911398",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":58766724,
        "Question_title":"How do I select which columns to load in a Kedro CSVLocalDataSet?",
        "Question_body":"<p>I have a <code>csv<\/code> file that looks like<\/p>\n\n<pre><code>a,b,c,d\n1,2,3,4\n5,6,7,8\n<\/code><\/pre>\n\n<p>and I want to load it in as a Kedro <code>CSVLocalDataSet<\/code>, but I don't want to read the entire file. I only want a few columns (say <code>a<\/code> and <code>b<\/code> for example).<\/p>\n\n<p>Is there any way for me to specify the list of columns to read\/load?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1573216572997,
        "Question_last_edit_time":1573220304527,
        "Question_score":0,
        "Question_tags":"python|pandas|csv|kedro",
        "Question_view_count":306,
        "Owner_creation_time":1395230906503,
        "Owner_last_access_time":1652366319603,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":129,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":51,
        "Answer_body":"<p>CSVLocalDataSet uses <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.read_csv.html\" rel=\"nofollow noreferrer\">pandas.read_csv<\/a>, which takes \"usecols\" parameter. It can easily be proxied by using <code>load_args<\/code> dataset parameter (all datasets support additional parameters passing via <code>load_args<\/code> and <code>save_args<\/code>):<\/p>\n\n<pre><code>my_cool_data:\n  type: CSVLocalDataSet\n  filepath: data\/path.csv\n  load_args: \n    usecols: ['a', 'b']\n<\/code><\/pre>\n\n<p>Also note the same parameters would work for any pandas-based dataset.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1573216728420,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58766724",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":69940562,
        "Question_title":"Azure Data Lake Storage Gen2 (ADLS Gen2) as a data source for Kedro pipeline",
        "Question_body":"<p>According to Kedro's <a href=\"https:\/\/kedro.readthedocs.io\/en\/0.15.7\/kedro.extras.datasets.html\" rel=\"nofollow noreferrer\">documentation<\/a>, Azure Blob Storage is one of the available data sources. Does this extend to ADLS Gen2 ?<\/p>\n<p>Haven't tried Kedro yet, but before I invest some time on it, I wanted to make sure I could connect to ADLS Gen2.<\/p>\n<p>Thank you in advance !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636709020827,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_tags":"python|kedro|mlops",
        "Question_view_count":350,
        "Owner_creation_time":1586517832390,
        "Owner_last_access_time":1660328393330,
        "Owner_location":null,
        "Owner_reputation":127,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Answer_body":"<p>Yes this works with Kedro. You're actually pointing a really old version of the docs, nowadays all filesystem based datasets in Kedro use <a href=\"https:\/\/github.com\/fsspec\/filesystem_spec\" rel=\"nofollow noreferrer\">fsspec<\/a> under the hood which means they work with S3, HDFS, local and many more filesystems seamlessly.<\/p>\n<p>The ADLS Gen2 is supported by <code>ffspec<\/code> via the underlying <code>adlfs<\/code> library which is <a href=\"https:\/\/github.com\/fsspec\/adlfs\" rel=\"nofollow noreferrer\">documented here<\/a>.<\/p>\n<p>From a Kedro point of view all you need to do is declare your catalog entry like so:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code> motorbikes:\n     type: pandas.CSVDataSet\n     filepath: abfs:\/\/your_bucket\/data\/02_intermediate\/company\/motorbikes.csv\n     credentials: dev_az\n<\/code><\/pre>\n<p>We also have more examples <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/01_data_catalog.html\" rel=\"nofollow noreferrer\">here<\/a>, particularly example 15.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1636712829167,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69940562",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":66505695,
        "Question_title":"kedro run as a python command instead of command line",
        "Question_body":"<p>I am getting started with Kedro, so I created the new kedro project for default iris dataset.<\/p>\n<p>I am able to succesfully run it with <code>kedro run<\/code> command. My question now is how do I run it as a python command? From the documentation I read that the command <code>kedro run<\/code>  runs the <code>src\/project-name\/run.py<\/code>. However, if I run the <code>run.py<\/code> I get <code>ModuleNotFoundError: No module named 'iris_workflow'<\/code>. I get the same error if I run the <code>run<\/code> method from <code>src\/project-name\/cli.py<\/code>.<\/p>\n<p>Everything works fine If I run <code>kedro run<\/code> in terminal.<\/p>\n<p>How do I run <code>kedro run<\/code> from a python script without <code>subprocess.run()<\/code>. If I import the <code>run.py<\/code> or <code>cli.py<\/code> in a script and run it, I get the same error <code>ModuleNotFoundError: No module named 'iris_workflow'<\/code>.<\/p>\n<p>This is the default workflow I created with <code>kedro new --starter=pandas-iris<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615034237927,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_tags":"python|kedro",
        "Question_view_count":805,
        "Owner_creation_time":1519724643533,
        "Owner_last_access_time":1646603612997,
        "Owner_location":null,
        "Owner_reputation":453,
        "Owner_up_votes":41,
        "Owner_down_votes":0,
        "Owner_views":79,
        "Answer_body":"<p>The problem is that your <code>src\/<\/code> folder which is where your project python package lives isn't on your Python path, so if you modify your <code>PYTHONPATH<\/code> first, you'll be able to run <code>run.py<\/code>:<\/p>\n<pre><code>~\/code\/kedro\/test-project\ntest-project \u276f PYTHONPATH=$PYTHONPATH:$pwd\/src python3 src\/test_project\/run.py\n<\/code><\/pre>\n<p>To more concretely answer your question, if you wanted to run Kedro from a Python script, you'd do something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\n\nsys.path.append(&quot;&lt;path-to-your-project-src&quot;)\nwith KedroSession.create(package_path.name) as session:\n    session.run()\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1615038763867,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66505695",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":68442999,
        "Question_title":"Waiting for nodes to finish in Kedro",
        "Question_body":"<p>I have a pipeline in Kedro that looks like this:<\/p>\n<pre><code>from kedro.pipeline import Pipeline, node\nfrom .nodes import *\n\ndef foo():\n    return Pipeline([\n        node(a, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=dict(bar_a=&quot;bar_a&quot;), name=&quot;A&quot;),\n        node(b, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=dict(bar_b=&quot;bar_b&quot;), name=&quot;B&quot;),\n        node(c, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=dict(bar_c=&quot;bar_c&quot;), name=&quot;C&quot;),\n        node(d, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=dict(bar_d=&quot;bar_d&quot;), name=&quot;D&quot;),\n        \n    ])\n<\/code><\/pre>\n<p>The nodes A, B, and C are not very resource-intensive, but they take a while so I want to run them in parallel, node D, on the other hand, uses pretty much all my memory, and it will fail if it's executed alongside the other nodes. Is there a way that I can tell Kedro to wait for A, B, and C to finish before executing node D and keep the code organized?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626707886057,
        "Question_last_edit_time":1626708364910,
        "Question_score":2,
        "Question_tags":"python|artificial-intelligence|kedro|mlops",
        "Question_view_count":245,
        "Owner_creation_time":1423164285360,
        "Owner_last_access_time":1663938741010,
        "Owner_location":"Itabira, Brazil",
        "Owner_reputation":856,
        "Owner_up_votes":60,
        "Owner_down_votes":3,
        "Owner_views":106,
        "Answer_body":"<p>Kedro determines the execution order based on the interdependencies between the inputs\/outputs of different nodes. In your case, node D doesn't depend on any of the other nodes, so execution order cannot be guaranteed. Similarly, it cannot be ensured that node D will <em>not<\/em> run in parallel to A, B and C if using a parallel runner.<\/p>\n<p>That said, there are a couple of workarounds one could use achieve a particular execution order.<\/p>\n<h5 id=\"preferred-run-the-nodes-separately-62tl\">1 [Preferred] Run the nodes separately<\/h5>\n<p>Instead of doing <code>kedro run --parallel<\/code>, you could do:<\/p>\n<pre><code>kedro run --pipeline foo --node A --node B --node C --parallel; kedro run --pipeline foo --node D\n<\/code><\/pre>\n<p>This is arguably the preferred solution because it requires no code changes (which is good in case you ever run the same pipeline on a different machine). You could do <code>&amp;&amp;<\/code> instead of <code>;<\/code> if you want node D to run only if A, B and C succeded. If the running logic gets more complex, you could store it in a Makefile\/bash script.<\/p>\n<h5 id=\"using-dummy-inputsoutputs-j7un\">2 Using dummy inputs\/outputs<\/h5>\n<p>You could also force the execution order by introducing dummy datasets. Something like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def foo():\n    return Pipeline([\n        node(a, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=[dict(bar_a=&quot;bar_a&quot;), &quot;a_done&quot;], name=&quot;A&quot;),\n        node(b, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=[dict(bar_b=&quot;bar_b&quot;), &quot;b_done&quot;], name=&quot;B&quot;),\n        node(c, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=[dict(bar_c=&quot;bar_c&quot;), &quot;c_done&quot;], name=&quot;C&quot;),\n        node(d, inputs=[&quot;train_x&quot;, &quot;test_x&quot;, &quot;a_done&quot;, &quot;b_done&quot;, &quot;c_done&quot;], outputs=dict(bar_d=&quot;bar_d&quot;), name=&quot;D&quot;),     \n    ])\n<\/code><\/pre>\n<p>Empty lists could do for the dummy datasets. The underlying functions would also have to return\/take the additional arguments.<\/p>\n<p>The advantage of this approach is that <code>kedro run --parallel<\/code> will immediately result in the desired execution logic. The disadvantage is that it pollutes the definition of nodes and underlying functions.<\/p>\n<p>If you go down this road, you'll also have to decide whether you want to store the dummy datasets in the data catalog (pollutes even more, but allows to run node D on its own) or not (node D cannot run on its own).<\/p>\n<hr \/>\n<p>Related discussions [<a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/issues\/132\" rel=\"nofollow noreferrer\">1<\/a>, <a href=\"https:\/\/stackoverflow.com\/questions\/58686533\/how-to-run-the-nodes-in-sequence-as-declared-in-kedro-pipeline\">2<\/a>]<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1626713371123,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68442999",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":61547494,
        "Question_title":"How to snap a python package with plugin packages?",
        "Question_body":"<p>I'd like to bundle the Python package <code>kedro<\/code> which provides a command line interface (<code>kedro<\/code>). In addition I'd like to put the Python package <code>kedro-docker<\/code> into the snap as well. This second package extends the first package's command line interface (<code>kedro docker<\/code>). But when I create a snap with the <code>snapcraft.yaml<\/code> below I get only the command line interface of the first package:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>name: kedro\nbase: core18\nversion: latest\ndescription: |\n    Kedro is a development workflow framework that implements software\n    engineering best-practice for data pipelines with an eye towards\n    productionising machine learning models.\n\ngrade: devel\nconfinement: devmode\n\narchitectures:\n  - build-on: [amd64]\n\napps:\n  kedro:\n    command: kedro\n    plugs:\n      - home\n      - network\n      - network-bind\n      - docker\n    environment: {\n      LANG: C.UTF-8,\n      LC_ALL: C.UTF-8\n    }\n\nparts:\n  kedro:\n    plugin: python\n    python-version: python3\n    python-packages:\n      - kedro==0.15.9\n      - kedro-docker==0.1.1\n<\/code><\/pre>\n\n<p>How can I get the extended command line interface (<code>kedro docker<\/code>) into the snap?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1588354211093,
        "Question_last_edit_time":1588354692553,
        "Question_score":0,
        "Question_tags":"python|snapcraft|kedro",
        "Question_view_count":246,
        "Owner_creation_time":1441627039650,
        "Owner_last_access_time":1663951876837,
        "Owner_location":"Augsburg, Germany",
        "Owner_reputation":3635,
        "Owner_up_votes":1076,
        "Owner_down_votes":3,
        "Owner_views":383,
        "Answer_body":"<p>I'm no expert and never used <code>snapcraft<\/code>, therefore just a hypothesis here. Kedro-Docker exposes only project-specific commands that won't show up unless you are in the root of the project. So if you run <code>kedro new<\/code> and then <code>cd &lt;project-dir&gt; &amp;&amp; kedro<\/code>, you should (ideally) see a <code>docker<\/code> group of commands:<\/p>\n\n<pre><code>Global commands from Kedro\nCommands:\n  docs  See the kedro API docs and introductory tutorial.\n  info  Get more information about kedro.\n  new   Create a new kedro project.\n\nProject specific commands from Docker\nCommands:\n  docker  Dockerize your Kedro project.\n\nProject specific commands from &lt;project-dir&gt;\/kedro_cli.py\nCommands:\n  activate-nbstripout  Install the nbstripout git hook to automatically...\n  build-docs           Build the project documentation.\n  build-reqs           Build the project dependency requirements.\n  install              Install project dependencies from both...\n  ipython              Open IPython with project specific variables loaded.\n  jupyter              Open Jupyter Notebook \/ Lab with project specific...\n  lint                 Run flake8, isort and (on Python &gt;=3.6) black.\n  package              Package the project as a Python egg and wheel.\n  run                  Run the pipeline.\n  test                 Run the test suite.\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1588362730510,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61547494",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":62386291,
        "Question_title":"Data versioning of \"Hello_World\" tutorial",
        "Question_body":"<p>i have added \"versioned: true\" in the \"catalog.yml\" file of the \"hello_world\" tutorial.<\/p>\n\n<pre><code>example_iris_data:\n  type: pandas.CSVDataSet\n  filepath: data\/01_raw\/iris.csv\n  versioned: true\n<\/code><\/pre>\n\n<p>Then when I used \n\"kedro run\" to run the tutorial, it has error as below:\n\"VersionNotFoundError: Did not find any versions for CSVDataSet\".<\/p>\n\n<p>May i know what is the right way for me to do versioning for the \"iris.csv\" file? thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1592216898480,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_tags":"kedro",
        "Question_view_count":338,
        "Owner_creation_time":1517455831447,
        "Owner_last_access_time":1611727283283,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":"<p>Try versioning one of the downstream outputs. For example, add this entry in your <code>catalog.yml<\/code>, and run <code>kedro run<\/code><\/p>\n\n<pre><code>example_train_x:\n  type: pandas.CSVDataSet\n  filepath: data\/02_intermediate\/example_iris_data.csv\n  versioned: true\n<\/code><\/pre>\n\n<p>And you will see <code>example_iris.data.csv<\/code> directory (not a file) under <code>data\/02_intermediate<\/code>. The reason <code>example_iris_data<\/code> gives you an error is that it's the starting data and there's already <code>iris.csv<\/code> in <code>data\/01_raw<\/code> so, Kedro cannot create <code>data\/01_raw\/iris.csv\/<\/code> directory because of the name conflict with the existing <code>iris.csv<\/code> file. <\/p>\n\n<p>Hope this helps :) <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1592218684390,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62386291",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":61452211,
        "Question_title":"Kedro - how to pass nested parameters directly to node",
        "Question_body":"<p><a href=\"https:\/\/stackoverflow.com\/questions\/tagged\/kedro\"><code>kedro<\/code><\/a> recommends storing parameters in <code>conf\/base\/parameters.yml<\/code>. Let's assume it looks like this:<\/p>\n\n<pre><code>step_size: 1\nmodel_params:\n    learning_rate: 0.01\n    test_data_ratio: 0.2\n    num_train_steps: 10000\n<\/code><\/pre>\n\n<p>And now imagine I have some <code>data_engineering<\/code> pipeline whose <code>nodes.py<\/code> has a function that looks something like this:<\/p>\n\n<pre><code>def some_pipeline_step(num_train_steps):\n    \"\"\"\n    Takes the parameter `num_train_steps` as argument.\n    \"\"\"\n    pass\n<\/code><\/pre>\n\n<p>How would I go about and pass that nested parameters straight to this function in <code>data_engineering\/pipeline.py<\/code>? I unsuccessfully tried:<\/p>\n\n<pre><code>from kedro.pipeline import Pipeline, node\n\nfrom .nodes import split_data\n\n\ndef create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                some_pipeline_step,\n                [\"params:model_params.num_train_steps\"],\n                dict(\n                    train_x=\"train_x\",\n                    train_y=\"train_y\",\n                ),\n            )\n        ]\n    )\n<\/code><\/pre>\n\n<p>I know that I could just pass all parameters into the function by using <code>['parameters']<\/code> or just pass all <code>model_params<\/code> parameters with <code>['params:model_params']<\/code> but it seems unelegant and I feel like there must be a way. Would appreciate any input!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1587965065520,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_tags":"machine-learning|yaml|pipeline|kedro",
        "Question_view_count":1403,
        "Owner_creation_time":1525290575943,
        "Owner_last_access_time":1655397273760,
        "Owner_location":"Toronto, ON, Canada",
        "Owner_reputation":143,
        "Owner_up_votes":28,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":"<p>(Disclaimer: I'm part of the Kedro team)<\/p>\n\n<p>Thank you for your question. Current version of Kedro, unfortunately, does not support nested parameters. The interim solution would be to use top-level keys inside the node (as you already pointed out) or decorate your node function with some sort of a parameter filter, which is not elegant either.<\/p>\n\n<p>Probably the most viable solution would be to customise your <code>ProjectContext<\/code> (in <code>src\/&lt;package_name&gt;\/run.py<\/code>) class by overwriting <code>_get_feed_dict<\/code> method as follows:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>class ProjectContext(KedroContext):\n    # ...\n\n\n    def _get_feed_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Get parameters and return the feed dictionary.\"\"\"\n        params = self.params\n        feed_dict = {\"parameters\": params}\n\n        def _add_param_to_feed_dict(param_name, param_value):\n            \"\"\"This recursively adds parameter paths to the `feed_dict`,\n            whenever `param_value` is a dictionary itself, so that users can\n            specify specific nested parameters in their node inputs.\n\n            Example:\n\n                &gt;&gt;&gt; param_name = \"a\"\n                &gt;&gt;&gt; param_value = {\"b\": 1}\n                &gt;&gt;&gt; _add_param_to_feed_dict(param_name, param_value)\n                &gt;&gt;&gt; assert feed_dict[\"params:a\"] == {\"b\": 1}\n                &gt;&gt;&gt; assert feed_dict[\"params:a.b\"] == 1\n            \"\"\"\n            key = \"params:{}\".format(param_name)\n            feed_dict[key] = param_value\n\n            if isinstance(param_value, dict):\n                for key, val in param_value.items():\n                    _add_param_to_feed_dict(\"{}.{}\".format(param_name, key), val)\n\n        for param_name, param_value in params.items():\n            _add_param_to_feed_dict(param_name, param_value)\n\n        return feed_dict\n<\/code><\/pre>\n\n<p>Please also note that this issue has already been <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/commit\/529606273e201a736f10338ada73ac6206081730\" rel=\"nofollow noreferrer\">addressed on develop<\/a> and will become available in the next release. The fix uses the approach from the snippet above.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1587979890917,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_favorite_count":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61452211",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":64921833,
        "Question_title":"DataBricks + Kedro Vs GCP + Kubeflow Vs Server + Kedro + Airflow",
        "Question_body":"<p>We are deploying a data consortium between more than 10 companies. Wi will deploy several machine learning models (in general advanced analytics models) for all the companies and we will administrate all the models. We are looking for a solution that administrates several servers, clusters and data science pipelines. I love kedro, but not sure what is the best option to administrate all while using kedro.<\/p>\n<p>In summary, we are looking for the best solution to administrate several models, tasks and pipelines in different servers and possibly Spark clusters. Our current options are:<\/p>\n<ul>\n<li><p>AWS as our data warehouse and Databricks for administrating servers, clusters and tasks. I don't feel that the notebooks of databricks are a good solution for building pipelines and to work collaboratively, so I would like to connect kedro to databricks (is it good? is it easy to schedule the run of the kedro pipelines using databricks?)<\/p>\n<\/li>\n<li><p>Using GCP for data warehouse and use kubeflow (iin GCP) for deploying models and the administration and the schedule of the pipelines and the needed resources<\/p>\n<\/li>\n<li><p>Setting up servers from ASW or GCP, install kedro and schedule the pipelines with airflow (I see a big problem administrating 20 servers and 40 pipelines)<\/p>\n<\/li>\n<\/ul>\n<p>I would like to know if someone knows what is the best option between these alternatives, their  downsides and advantages, or if there are more possibilities.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1605830422657,
        "Question_last_edit_time":1605836750283,
        "Question_score":4,
        "Question_tags":"google-cloud-platform|databricks|kedro",
        "Question_view_count":967,
        "Owner_creation_time":1605828724553,
        "Owner_last_access_time":1618356797460,
        "Owner_location":null,
        "Owner_reputation":59,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":"<p>I'll try and summarise what I know, but be aware that I've not been part of a KubeFlow project.<\/p>\n<h2>Kedro on Databricks<\/h2>\n<p>Our approach was to build our project with CI and then execute the pipeline from a notebook. We <em>did not<\/em> use the <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/11_tools_integration\/03_databricks.html\" rel=\"nofollow noreferrer\">kedro recommended approach<\/a> of using databricks-connect due to the <a href=\"https:\/\/databricks.com\/product\/aws-pricing\" rel=\"nofollow noreferrer\">large price difference<\/a> between Jobs and Interactive Clusters (which are needed for DB-connect). If you're working on several TB's of data, this quickly becomes relevant.<\/p>\n<p>As a DS, this approach may feel natural, as a SWE though it does not. Running pipelines in notebooks feels hacky. It works but it feels non-industrialised. Databricks performs well in automatically spinning up and down clusters &amp; taking care of the runtime for you. So their value add is abstracting IaaS away from you (more on that later).<\/p>\n<h2>GCP &amp; &quot;Cloud Native&quot;<\/h2>\n<p><strong>Pro<\/strong>: GCP's main selling point is BigQuery. It is an incredibly powerful platform, simply because you can be productive from day 0. I've seen people build entire web API's on top of it. KubeFlow isn't tied to GCP so you could port this somewhere else later on. Kubernetes will also allow you to run anything else you wish on the cluster, API's, streaming, web services, websites, you name it.<\/p>\n<p><strong>Con<\/strong>: Kubernetes is complex. If you have 10+ engineers to run this project long-term, you should be OK. But don't underestimate the complexity of Kubernetes. It is to the cloud what Linux is to the OS world. Think log management, noisy neighbours (one cluster for web APIs + batch spark jobs), multi-cluster management (one cluster per department\/project), security, resource access etc.<\/p>\n<h2>IaaS server approach<\/h2>\n<p>Your last alternative, the manual installation of servers is one I would recommend only if you have a large team, extremely large data and are building a long-term product who's revenue can sustain the large maintenance costs.<\/p>\n<h2>The people behind it<\/h2>\n<p>How does the talent market look like in your region? If you can hire experienced engineers with GCP knowledge, I'd go for the 2nd solution. GCP is a mature, &quot;native&quot; platform in the sense that it abstracts a lot away for customers. If your market has mainly AWS engineers, that may be a better road to take. If you have a number of kedro engineers, that also has relevance. Note that kedro is agnostic enough to run anywhere. It's really just python code.<\/p>\n<p><strong>Subjective advise<\/strong>:<\/p>\n<p>Having worked mostly on AWS projects and a few GCP projects, I'd go for GCP. I'd use the platform's components (BigQuery, Cloud Run, PubSub, Functions, K8S) as a toolbox to choose from and build an organisation around that. Kedro can run in any of these contexts, as a triggered job by the Scheduler, as a container on Kubernetes or as a ETL pipeline bringing data into (or out of) BigQuery.<\/p>\n<p>While Databricks is &quot;less management&quot; than raw AWS, it's still servers to think about and VPC networking charges to worry over. BigQuery is simply GB queried. Functions are simply invocation count. These high level components will allow you to quickly show value to customers and you only need to go deeper (RaaS -&gt; PaaS -&gt; IaaS) as you scale.<\/p>\n<p>AWS also has these higher level abstractions over IaaS but in general, it appears (to me) that Google's offering is the most mature. Mainly because they have published tools they've been using internally for almost a decade whereas AWS has built new tools for the market. AWS is the king of IaaS though.<\/p>\n<p>Finally, a bit of content, <a href=\"https:\/\/youtu.be\/kjhXMTOLtac?t=618\" rel=\"nofollow noreferrer\">two former colleagues have discussed ML industrialisation frameworks earlier this fall<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1605891844630,
        "Answer_last_edit_time":1605947117163,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64921833",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":63518174,
        "Question_title":"using gunicorn for nested folders",
        "Question_body":"<p>I'm new to gunicorn and heroku so I would appreciate any help. I want to deploy my python Dash app on to heroku and I know I need a Procfile. The thing is that my project structure uses the Kedro structure and my structure looks like this:<\/p>\n<pre><code>myproject\n    .... # Kedro-generated files\n    src\/\n        package1\/\n            package2\/\n                __init__.py\n                index.py\n    Procfile\n<\/code><\/pre>\n<p>index.py is a Dash application like so<\/p>\n<pre><code>#imports up here\n\napp = dash.Dash(__name__, external_stylesheets=external_stylesheets)\nserver = app.server\n\n.......  # main code chunk\n\nif __name__ == '__main__':\napp.run_server(debug=True)\n<\/code><\/pre>\n<p>Currently, my Procfile looks like this:<\/p>\n<pre><code>web: gunicorn src frontend.index:app\n<\/code><\/pre>\n<p>My project uploads to heroku just fine but I'm getting this error in my log:<\/p>\n<pre><code>2020-08-21T06:46:46.433935+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 941, in _find_and_load_unlocked\n2020-08-21T06:46:46.433935+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n2020-08-21T06:46:46.433936+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import\n2020-08-21T06:46:46.433936+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load\n2020-08-21T06:46:46.433936+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 953, in _find_and_load_unlocked\n2020-08-21T06:46:46.433962+00:00 app[web.1]: ModuleNotFoundError: No module named 'frontend'\n2020-08-21T06:46:46.434082+00:00 app[web.1]: [2020-08-21 06:46:46 +0000] [11] [INFO] Worker exiting (pid: 11)\n2020-08-21T06:46:46.464346+00:00 app[web.1]: Traceback (most recent call last):\n2020-08-21T06:46:46.464367+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 202, in run\n2020-08-21T06:46:46.464715+00:00 app[web.1]: self.manage_workers()\n2020-08-21T06:46:46.464732+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 545, in manage_workers\n2020-08-21T06:46:46.465049+00:00 app[web.1]: self.spawn_workers()\n2020-08-21T06:46:46.465054+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 617, in spawn_workers\n2020-08-21T06:46:46.465412+00:00 app[web.1]: time.sleep(0.1 * random.random())\n2020-08-21T06:46:46.465417+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n2020-08-21T06:46:46.465617+00:00 app[web.1]: self.reap_workers()\n2020-08-21T06:46:46.465622+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n2020-08-21T06:46:46.465905+00:00 app[web.1]: raise HaltServer(reason, self.WORKER_BOOT_ERROR)\n2020-08-21T06:46:46.465950+00:00 app[web.1]: gunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\n2020-08-21T06:46:46.465964+00:00 app[web.1]: \n2020-08-21T06:46:46.465965+00:00 app[web.1]: During handling of the above exception, another exception occurred:\n2020-08-21T06:46:46.465965+00:00 app[web.1]: \n2020-08-21T06:46:46.465969+00:00 app[web.1]: Traceback (most recent call last):\n2020-08-21T06:46:46.465969+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/bin\/gunicorn&quot;, line 8, in &lt;module&gt;\n2020-08-21T06:46:46.466103+00:00 app[web.1]: sys.exit(run())\n2020-08-21T06:46:46.466107+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in run\n2020-08-21T06:46:46.466254+00:00 app[web.1]: WSGIApplication(&quot;%(prog)s [OPTIONS] [APP_MODULE]&quot;).run()\n2020-08-21T06:46:46.466258+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 228, in run\n2020-08-21T06:46:46.466464+00:00 app[web.1]: super().run()\n2020-08-21T06:46:46.466470+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 72, in run\n2020-08-21T06:46:46.466601+00:00 app[web.1]: Arbiter(self).run()\n2020-08-21T06:46:46.466606+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 229, in run\n2020-08-21T06:46:46.466790+00:00 app[web.1]: self.halt(reason=inst.reason, exit_status=inst.exit_status)\n2020-08-21T06:46:46.466794+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 342, in halt\n2020-08-21T06:46:46.467031+00:00 app[web.1]: self.stop()\n2020-08-21T06:46:46.467032+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 393, in stop\n2020-08-21T06:46:46.467262+00:00 app[web.1]: time.sleep(0.1)\n2020-08-21T06:46:46.467267+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n2020-08-21T06:46:46.467468+00:00 app[web.1]: self.reap_workers()\n2020-08-21T06:46:46.467469+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n2020-08-21T06:46:46.467750+00:00 app[web.1]: raise HaltServer(reason, self.WORKER_BOOT_ERROR)\n2020-08-21T06:46:46.467754+00:00 app[web.1]: gunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\n2020-08-21T06:46:46.559947+00:00 heroku[web.1]: Process exited with status 1\n2020-08-21T06:46:46.610907+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:47:03.000000+00:00 app[api]: Build succeeded\n2020-08-21T06:49:12.915422+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/&quot; host=protected-coast-07061.herokuapp.com request_id=781ff03f-db0d-40ad-996f-1d25ff3fd026 fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n2020-08-21T06:49:13.357185+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/&quot; host=protected-coast-07061.herokuapp.com request_id=51bff951-d0fa-4e01-ba38-60f44cbe373b fwd=&quot;18.217.223.118&quot; dyno= connect= service= status=503 bytes= protocol=http\n2020-08-21T06:49:13.955353+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/favicon.ico&quot; host=protected-coast-07061.herokuapp.com request_id=632cc0a9-e052-43c9-a90c-62f99dfbba5c fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n<\/code><\/pre>\n<pre><code>2020-08-21T06:52:18.372623+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T06:52:32.487313+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index`\n2020-08-21T06:52:34.595212+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T06:52:34.595933+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:17241 (4)\n2020-08-21T06:52:34.596051+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Using worker: sync\n2020-08-21T06:52:34.600183+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T06:52:34.603725+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:52:34.603887+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T06:52:34.626625+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [11] [INFO] Booting worker with pid: 11\n2020-08-21T06:52:34.629877+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:52:34.629978+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [11] [INFO] Worker exiting (pid: 11)\n2020-08-21T06:52:34.733270+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T06:52:34.733356+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Reason: App failed to load.\n2020-08-21T06:52:34.800675+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T06:52:34.837697+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:52:34.839731+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T06:52:49.188229+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index`\n2020-08-21T06:52:50.000000+00:00 app[api]: Build succeeded\n2020-08-21T06:52:51.154243+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T06:52:51.154956+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:46031 (4)\n2020-08-21T06:52:51.155075+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Using worker: sync\n2020-08-21T06:52:51.158999+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T06:52:51.162147+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:52:51.162261+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T06:52:51.189291+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T06:52:51.189375+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Reason: App failed to load.\n2020-08-21T06:52:51.249579+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T06:52:51.281288+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:53:27.313026+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/&quot; host=protected-coast-07061.herokuapp.com request_id=67b1f83d-37a8-4ad1-b522-2e7cc0bd7b7d fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n2020-08-21T06:53:28.196639+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/favicon.ico&quot; host=protected-coast-07061.herokuapp.com request_id=0d13d80e-ca9b-4857-970e-47cfcf602017 fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n2020-08-21T06:57:12.000000+00:00 app[api]: Build started by user \n2020-08-21T06:58:51.667324+00:00 app[api]: Deploy 1f77e9e8 by user \n2020-08-21T06:58:51.667324+00:00 app[api]: Release v12 created by user \n2020-08-21T06:58:51.832220+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T06:59:07.062252+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index:app`\n2020-08-21T06:59:10.383357+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T06:59:10.384213+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:54641 (4)\n2020-08-21T06:59:10.384357+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Using worker: sync\n2020-08-21T06:59:10.388913+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T06:59:10.392276+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:59:10.392426+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T06:59:10.403239+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [11] [INFO] Booting worker with pid: 11\n2020-08-21T06:59:10.407880+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:59:10.408006+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [11] [INFO] Worker exiting (pid: 11)\n2020-08-21T06:59:10.525402+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T06:59:10.525558+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Reason: App failed to load.\n2020-08-21T06:59:10.607473+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T06:59:11.643239+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:59:54.000000+00:00 app[api]: Build succeeded\n2020-08-21T07:08:53.300472+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T07:09:16.319403+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index:app`\n2020-08-21T07:09:19.182910+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T07:09:19.228761+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T07:09:19.057971+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T07:09:19.058760+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:25408 (4)\n2020-08-21T07:09:19.058888+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Using worker: sync\n2020-08-21T07:09:19.063236+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T07:09:19.066629+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T07:09:19.066758+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T07:09:19.102247+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T07:09:19.102349+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Reason: App failed to load.\n<\/code><\/pre>\n<p>Apologies, I am new to this so I am not sure where to even start with the debugging as well. To summarise: I think my gunicorn is not firing as my line may be wrong; and I am not sure what is causing my app to not launch. How do I solve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597994063423,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_tags":"python|heroku|gunicorn|kedro",
        "Question_view_count":586,
        "Owner_creation_time":1597993100673,
        "Owner_last_access_time":1613368758793,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":"<p>The logic of gunicorn is the following:\n<code>.<\/code> (dot) for directories, <code>:<\/code> (column) for objects defined inside a file.<\/p>\n<p>Assuming the given structure, you should have something like this:<\/p>\n<pre><code>$ cat Procfile\nweb: gunicorn src.package1.package2.index:app\n<\/code><\/pre>\n<p>[EDIT] If you get an error, you should consider using <code>server<\/code> instead of <code>app<\/code>. As an example, these files are from <a href=\"https:\/\/gitlab.com\/qmeeus\/datathon\" rel=\"nofollow noreferrer\">one of my old projects<\/a> (also a Dash app):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># app.py\n\nimport flask\nfrom src import dashboard\n\nserver = flask.Flask(__name__)\nserver.secret_key = os.environ.get('secret_key', str(randint(0, 1000000)))\napp = dashboard.main(server)\n\nif __name__ == '__main__':\n    app.server.run(debug=True, threaded=True)\n<\/code><\/pre>\n<pre class=\"lang-sh prettyprint-override\"><code># Procfile\nweb: gunicorn app:server --timeout 300\n<\/code><\/pre>\n<pre class=\"lang-sh prettyprint-override\"><code>$ ls *\nProcfile app.py\n\nsrc:\nconfig.py  dashboard.py ...\n \n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1597994468467,
        "Answer_last_edit_time":1597995299663,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63518174",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":61791713,
        "Question_title":"How can I read\/write data from\/to network attached storage with kedro?",
        "Question_body":"<p>In the API docs about <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.html#\" rel=\"nofollow noreferrer\"><code>kedro.io<\/code><\/a> and <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.contrib.io.html\" rel=\"nofollow noreferrer\"><code>kedro.contrib.io<\/code><\/a> I could not find info about how to read\/write data from\/to network attached storage such as e.g. <a href=\"https:\/\/en.avm.de\/guide\/using-the-fritzbox-nas-function\/\" rel=\"nofollow noreferrer\">FritzBox NAS<\/a>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1589441422903,
        "Question_last_edit_time":1589442901067,
        "Question_score":3,
        "Question_tags":"python|kedro",
        "Question_view_count":675,
        "Owner_creation_time":1441627039650,
        "Owner_last_access_time":1663951876837,
        "Owner_location":"Augsburg, Germany",
        "Owner_reputation":3635,
        "Owner_up_votes":1076,
        "Owner_down_votes":3,
        "Owner_views":383,
        "Answer_body":"<p>So I'm a little rusty on network attached storage, but:<\/p>\n\n<ol>\n<li><p>If you can mount your network attached storage onto your OS and access it like a regular folder, then it's just a matter of providing the right <code>filepath<\/code> when writing the config for a given catalog entry. See for example: <a href=\"https:\/\/stackoverflow.com\/questions\/7169845\/using-python-how-can-i-access-a-shared-folder-on-windows-network\">Using Python, how can I access a shared folder on windows network?<\/a><\/p><\/li>\n<li><p>Otherwise, if accessing the network attached storage requires anything special, you might want to <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/14_create_a_new_dataset.html\" rel=\"nofollow noreferrer\">create a custom dataset<\/a> that uses a Python library for interfacing with your network attached storage. Something like <a href=\"https:\/\/pysmb.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">pysmb<\/a> comes to mind.<\/p><\/li>\n<\/ol>\n\n<p>The custom dataset could borrow heavily from the logic in existing <code>kedro.io<\/code> or <code>kedro.extras.datasets<\/code> datasets, but you replace the filepath\/fsspec handling code with <code>pysmb<\/code> instead.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1589448276967,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61791713",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":70080915,
        "Question_title":"kedro context and catalog missing from ipython session",
        "Question_body":"<p>I launched ipython session and trying to load a dataset.<br \/>\nI am running<br \/>\ndf = catalog.load(&quot;test_dataset&quot;)<br \/>\nFacing the below error<br \/>\n<code>NameError: name 'catalog' is not defined<\/code><\/p>\n<p>I also tried %reload_kedro but got the below error<\/p>\n<p><code>UsageError: Line magic function `%reload_kedro` not found.<\/code><\/p>\n<p>Even not able to load context either.\nI am running the kedro environment from a Docker container.\nI am not sure where I am going wrong.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637670434953,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_tags":"ipython|kedro",
        "Question_view_count":617,
        "Owner_creation_time":1495105930730,
        "Owner_last_access_time":1663779078467,
        "Owner_location":null,
        "Owner_reputation":29,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":"<p>new in 0.17.5 there is a fallback option, please run the following commands in your Jupyter\/IPython session:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>%load_ext kedro.extras.extensions.ipython\n%reload_kedro &lt;path_to_project_root&gt;\n<\/code><\/pre>\n<p>This should help you get up and running.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1637671420497,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_favorite_count":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70080915",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":72093004,
        "Question_title":"Setup a base dir for the Data Catalog in Kedro",
        "Question_body":"<p>I'm working on a project that, because of the company's compliance rules, the data has to stay in a shared directory, that is synchronized among the programmers. The project's code on the other hand cannot be on that shared directory otherwise we wouldn't be able to version it and work together since it's all synchronized. The path to the shared folder is pretty much the same <code>C:\\Users\\&lt;employee name&gt;\\&lt;path to data&gt;<\/code>, is there a way that I can setup <code>C:\\Users\\&lt;employee name&gt;<\/code> as a base path for my data catalog in Kedro?<\/p>\n<p>I tried creating a <code>catalog.py<\/code> file that has the following code:<\/p>\n<pre><code>from kedro.io import DataCatalog\nfrom kedro.extras.datasets.pandas import (\n    CSVDataSet,\n    ExcelDataSet,\n)\nfrom pathlib import Path\n\nDEFAULT_DATA_PATH = Path.expanduser(\n    Path(\n        &quot;~&quot;, \n        &quot;Path to Data&quot;\n    )\n)\n\nDATA_CATALOG = DataCatalog(\n    {\n        &quot;data&quot;: ExcelDataSet(\n            filepath=Path(EXTERNAL_DATA_PATH, &quot;data.xlsx&quot;).as_uri()\n        )\n            \n    }\n)\n<\/code><\/pre>\n<p>And then on the <code>setting.py<\/code> I've added this:<\/p>\n<pre><code>from .catalog import DATA_CATALOG\nDATA_CATALOG_CLASS = DATA_CATALOG\n<\/code><\/pre>\n<p>but then I get the following error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;...\\Miniconda3\\Scripts\\kedro-script.py&quot;, line 9, in &lt;module&gt;\n    sys.exit(main())\n  File &quot;...\\Miniconda3\\lib\\site-packages\\kedro\\framework\\cli\\cli.py&quot;, line 205, in main \n    cli_collection = KedroCLI(project_path=Path.cwd())\n  File &quot;...\\Miniconda3\\lib\\site-packages\\kedro\\framework\\cli\\cli.py&quot;, line 114, in __init__\n    self._metadata = bootstrap_project(project_path)\n  File &quot;...\\Miniconda3\\lib\\site-packages\\kedro\\framework\\startup.py&quot;, line 155, in bootstrap_project\n    configure_project(metadata.package_name)\n  File &quot;...\\Miniconda3\\lib\\site-packages\\kedro\\framework\\project\\__init__.py&quot;, line 166, in configure_project\n    settings.configure(settings_module)\n  File &quot;...\\Miniconda3\\lib\\site-packages\\dynaconf\\base.py&quot;, line 223, in configure      \n    self._wrapped = Settings(settings_module=settings_module, **kwargs)\n  File &quot;...\\Miniconda3\\lib\\site-packages\\dynaconf\\base.py&quot;, line 271, in __init__       \n    self.validators.validate()\n  File &quot;...\\Miniconda3\\lib\\site-packages\\dynaconf\\validator.py&quot;, line 318, in validate  \n    validator.validate(self.settings)\n  File &quot;...\\Miniconda3\\lib\\site-packages\\kedro\\framework\\project\\__init__.py&quot;, line 34, \nin validate\n    if not issubclass(setting_value, default_class):\nTypeError: issubclass() arg 1 must be a class\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651530165163,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_tags":"python|kedro",
        "Question_view_count":86,
        "Owner_creation_time":1423164285360,
        "Owner_last_access_time":1663938741010,
        "Owner_location":"Itabira, Brazil",
        "Owner_reputation":856,
        "Owner_up_votes":60,
        "Owner_down_votes":3,
        "Owner_views":106,
        "Answer_body":"<p><code>DATA_CATALOG_CLASS<\/code> is expecting a class while you are providing an instance of data catalog, thus the error.<\/p>\n<p>I think the way to go here to use <code>TemplatedConfigLoader<\/code>, and pass the share directory as a variable. You would supply this <code>SHARE_DIR<\/code> either through a <code>global.yml<\/code> or just a variable.<\/p>\n<p>In your <code>catalog.yml<\/code>\nsome_data:\ntype: pandas.CSVDataSet<\/p>\n<p>See more documentation here.\n<a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.config.TemplatedConfigLoader.html\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.config.TemplatedConfigLoader.html<\/a>\npath: ${SHARE_DIR}\/file_name<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1651566013013,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72093004",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":58686533,
        "Question_title":"How to run the nodes in sequence as declared in kedro pipeline?",
        "Question_body":"<p>In Kedro pipeline, nodes (something like python functions) are declared sequentially. In some cases, the input of one node is the output of the previous node. However, sometimes, when kedro run API is called in the commandline, the nodes are not run sequentially.<\/p>\n\n<p>In kedro documentation, it says that by default the nodes are ran in sequence. <\/p>\n\n<p>My run.py code:<\/p>\n\n<pre><code>def main(\ntags: Iterable[str] = None,\nenv: str = None,\nrunner: Type[AbstractRunner] = None,\nnode_names: Iterable[str] = None,\nfrom_nodes: Iterable[str] = None,\nto_nodes: Iterable[str] = None,\nfrom_inputs: Iterable[str] = None,\n):\n\nproject_context = ProjectContext(Path.cwd(), env=env)\nproject_context.run(\n    tags=tags,\n    runner=runner,\n    node_names=node_names,\n    from_nodes=from_nodes,\n    to_nodes=to_nodes,\n    from_inputs=from_inputs,\n)\n<\/code><\/pre>\n\n<p>Currently my last node is sometimes ran before my first few nodes.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1572835098980,
        "Question_last_edit_time":null,
        "Question_score":5,
        "Question_tags":"python|machine-learning|kedro",
        "Question_view_count":1741,
        "Owner_creation_time":1545311054090,
        "Owner_last_access_time":1663916183127,
        "Owner_location":null,
        "Owner_reputation":170,
        "Owner_up_votes":113,
        "Owner_down_votes":2,
        "Owner_views":33,
        "Answer_body":"<p>The answer that I recieved from Kedro github:<\/p>\n\n<blockquote>\n  <p>Pipeline determines the node execution order exclusively based on\n  dataset dependencies (node inputs and outputs) at the moment. So the\n  only option to dictate that the node A should run before node B is to\n  put a dummy dataset as an output of node A and an input of node B.<\/p>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1572835437910,
        "Answer_last_edit_time":1572865397610,
        "Answer_score":5.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58686533",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":70230262,
        "Question_title":"kedro DataSetError while loading PartitionedDataSet",
        "Question_body":"<p>I am using PartitionedDataSet to load multiple csv files from azure blob storage. I defined my data set in the datacatalog as below.<\/p>\n<pre><code>my_partitioned_data_set:\n          type: PartitionedDataSet\n          path: my\/azure\/folder\/path\n          credentials: my credentials\n          dataset: pandas.CSVDataSet\n          load_args:\n                sep: &quot;;&quot;\n                encoding: latin1\n<\/code><\/pre>\n<p>I also defined a node to combine all the partitions. But while loading each file as a CSVDataSet kedro is not considering the load_args, so I am getting the below error.<\/p>\n<pre><code>Failed while loading data from data set CSVDataSet(filepath=my\/azure\/folder\/path, load_args={}, protocol=abfs, save_args={'index': False}).\n'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte \n<\/code><\/pre>\n<p>The error shows that while loading the CSVDataSet kedro is not considering the load_args defined in the PartitionedDataSet. And passing an empty dict as a load_args parameter to CSVDataSet.\nI am following the documentation\n<code>https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html#partitioned-dataset<\/code>\nI am not getting where I am doing mistakes.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638659712850,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_tags":"kedro",
        "Question_view_count":381,
        "Owner_creation_time":1495105930730,
        "Owner_last_access_time":1663779078467,
        "Owner_location":null,
        "Owner_reputation":29,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":"<p>Move <code>load_args<\/code> inside dataset<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>my_partitioned_data_set:\n  type: PartitionedDataSet\n  path: my\/azure\/folder\/path\n  credentials: my credentials\n  dataset:\n    type: pandas.CSVDataSet\n    load_args:\n      sep: &quot;;&quot;\n      encoding: latin1\n<\/code><\/pre>\n<ul>\n<li><p><code>load_args<\/code> mentioned outside dataset is passed into <code>find()<\/code> method of the corresponding filesystem implementation<\/p>\n<\/li>\n<li><p>To pass granular configuration to underlying dataset put it inside <code>dataset<\/code> as above.<\/p>\n<\/li>\n<\/ul>\n<p>You can check out the details in the docs<\/p>\n<p><a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html?highlight=partitoned%20dataset#partitioned-dataset-definition\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html?highlight=partitoned%20dataset#partitioned-dataset-definition<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1638684474153,
        "Answer_last_edit_time":1638685522183,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70230262",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":67941614,
        "Question_title":"Kedro install fail to install, but few attempt later it is successful",
        "Question_body":"<p>I have to test if my kedro project works from github so I create a new environment, then :<\/p>\n<pre><code>git clone &lt;my_project&gt;\npip install kedro kedro[pandas] kedro-viz jupyter\nkedro build-reqs\nkedro install\n<\/code><\/pre>\n<p>and the install fails, then I retry a few time (sometimes 2 or 3) then the next attempt it is successful<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4KTui.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4KTui.png\" alt=\"see image\" \/><\/a><\/p>\n<p>EDIT:\npython -V : Python 3.7.10\nkedro --version : kedro, version 0.17.3<\/p>\n<p>i cant post my requirement.txt (post is mostly code) so here is my requirement.in<\/p>\n<pre><code>black==v19.10b0\nflake8&gt;=3.7.9, &lt;4.0\nipython==7.10\nisort&gt;=4.3.21, &lt;5.0\njupyter~=1.0\njupyter_client&gt;=5.1, &lt;7.0\njupyterlab==0.31.1\nkedro==0.17.3\nnbstripout==0.3.3\npytest-cov~=2.5\npytest-mock&gt;=1.7.1, &lt;2.0\npytest~=6.1.2\nwheel==0.32.2\nspacy&gt;=3.0.0,&lt;4.0.0\nscikit-learn == 0.24.2\nkedro-viz==3.11.0\nwordcloud== 1.8.1\nhttps:\/\/github.com\/explosion\/spacy-models\/releases\/download\/fr_core_news_sm-3.0.0\/fr_core_news_sm-3.0.0.tar.gz#egg=fr_core_news_sm\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1623434458303,
        "Question_last_edit_time":1623788215210,
        "Question_score":2,
        "Question_tags":"python|pipeline|kedro",
        "Question_view_count":594,
        "Owner_creation_time":1596486679830,
        "Owner_last_access_time":1635947969527,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>As indicated in the comment, I think there are two issues at play.<\/p>\n<h4>1. Decoding error<\/h4>\n<p>This is the main exception you're getting, i.e.:<\/p>\n<pre><code>UnicodeDecodeError: \u2018utf-8\u2019 codec can\u2019t decode byte 0xe8 in position 69: invalid continuation byte\n<\/code><\/pre>\n<p>This is unexpectedly raised while Kedro itself is handling the errors from <code>pip install<\/code> (see <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/blob\/master\/kedro\/framework\/cli\/project.py#L172\" rel=\"nofollow noreferrer\">this line of Kedro's source code<\/a>). I believe the cause might be that you have accented characters in your working directory, which can't be interpreted by Python's standard <code>decode()<\/code> (see <a href=\"https:\/\/stackoverflow.com\/questions\/49898909\/reading-a-file-with-french-characters-in-python\">this<\/a>). Example:<\/p>\n<pre><code>b'acc\u00e9l\u00e9ration'.decode()\n&gt;&gt; SyntaxError: bytes can only contain ASCII literal characters.\n<\/code><\/pre>\n<p>The decoding error is obscuring the actual <code>pip install<\/code> error.<\/p>\n<h4>2. <code>pip install<\/code> error<\/h4>\n<p>As you correctly pointed out, <code>kedro install<\/code> uses <code>pip install<\/code> under the hood. It's a bit difficult to pinpoint the exact cause without seeing the actual error. I could however reproduce a similar issue, in my case getting the following error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\users\\\\&lt;mu-user&gt;\\\\anaconda3\\\\&lt;my-env&gt;\\\\kedro_project_tests\\\\lib\\\\site-packages\\\\~ydantic\\\\annotated_types.cp37-win_amd64.pyd'\nConsider using the `--user` option or check the permissions.\n<\/code><\/pre>\n<p>I believe this is caused by interactions caused by between different versions Kedro and Kedro-Viz. Simply not <code>pip install<\/code>ing <code>kedro-viz<\/code> <em>before<\/em> doing <code>kedro install<\/code> fixed it for me.<\/p>\n<hr \/>\n<p><em>Note: Related to this, there will surely be an error if the version of Kedro installed through <code>pip<\/code> <em>before<\/em> doing <code>kedro install<\/code> is not the same as the version of Kedro specified in <code>requirements.in<\/code> or <code>requirements.txt<\/code>. This is obvious, as the package currently handling execution will be uninstalled. The error in this case will be something like this:<\/em><\/p>\n<pre><code>ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'c:\\\\users\\\\&lt;my-user&gt;\\\\anaconda3\\\\envs\\\\&lt;my-env&gt;\\\\scripts\\\\kedro.exe\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1623941002453,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67941614",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":58764792,
        "Question_title":"How to run functions from a Class in the nodes.py file?",
        "Question_body":"<p>I want to organize the node functions by Classes in the nodes.py file. For example, functions related to cleaning data are in the \"CleanData\" Class, with a @staticmethod decorator, while other functions will stay in the \"Other\" Class, without any decorator (the names of these classes are merely representative). In the pipeline file, I tried importing the names of the classes, the names of the nodes and the following way: CleanData.function1 (which gave an error) and none of them worked. How can I call the nodes from the classes, if possible, please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1573208985150,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_tags":"python|python-3.x|class|pipeline|kedro",
        "Question_view_count":289,
        "Owner_creation_time":1572973807453,
        "Owner_last_access_time":1663947835340,
        "Owner_location":null,
        "Owner_reputation":29,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Answer_body":"<p>I'm not entirely certain what the error you're getting is. If you're literally trying to do <code>from .nodes import CleanData.function1<\/code> that won't work. Imports don't work like that in Python. If you do something like this:<\/p>\n\n<p><code>nodes.py<\/code> has:<\/p>\n\n<pre><code>class CleanData:\n    def clean(arg1):\n        pass\n<\/code><\/pre>\n\n<p>and <code>pipeline.py<\/code> has:<\/p>\n\n<pre><code>from kedro.pipeline import Pipeline, node\nfrom .nodes import CleanData\n\ndef create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                CleanData.clean,\n                \"example_iris_data\",\n                None,\n            )\n        ]\n    )\n<\/code><\/pre>\n\n<p>that should work.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1573209953537,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58764792",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":62215724,
        "Question_title":"Does Kedro support Checkpointing\/Caching of Results?",
        "Question_body":"<p>Let's say we have multiple long running pipeline nodes.\nIt seems quite straight forward to checkpoint or cache the intermediate results, so when nodes after a checkpoint are changed or added only these nodes must be executed again.<\/p>\n\n<p>Does Kedro provide functionality to make sure, that when I run the pipeline only those steps are \nexecuted that have changed?\nAlso the reverse, is there a way to make sure, that all steps that have changed are executed?<\/p>\n\n<p>Let's say a pipeline producing some intermediate result changed, will it be executed, when i execute a pipeline depending on the output of the first?<\/p>\n\n<p><strong>TL;DR:<\/strong> Does Kedro have <code>makefile<\/code>-like tracking of what needs to be done and what not?<\/p>\n\n<p>I think my question is similar to <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/issues\/341\" rel=\"nofollow noreferrer\">issue #341<\/a>, but I do not require support of cyclic graphs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591361290233,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_tags":"kedro",
        "Question_view_count":367,
        "Owner_creation_time":1494502461177,
        "Owner_last_access_time":1621336242960,
        "Owner_location":null,
        "Owner_reputation":83,
        "Owner_up_votes":40,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>You might want to have a look at the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.IncrementalDataSet.html\" rel=\"nofollow noreferrer\">IncrementalDataSet<\/a> alongside the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/08_advanced_io.html#partitioned-dataset\" rel=\"nofollow noreferrer\">partitioned dataset<\/a> documentation, specifically the section on <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/08_advanced_io.html#incremental-loads-with-incrementaldataset\" rel=\"nofollow noreferrer\">incremental loads with the incremental dataset<\/a> which has a notion of \"checkpointing\", although checkpointing is a manual step and not automated like <code>makefile<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1591362515297,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62215724",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":72275283,
        "Question_title":"Kedro using wrong conda environment",
        "Question_body":"<p>I have created a conda environment called <code>Foo<\/code>. After activating this environment I installed Kedro with <code>pip<\/code>, since <code>conda<\/code> was giving me a conflict. Even though I'm inside the <code>Foo<\/code> environment, when I run:<\/p>\n<pre><code>kedro jupyter lab\n<\/code><\/pre>\n<p>It picks up the modules from my <code>base<\/code> environment, not the <code>Foo<\/code> environment. Any idea, why this is happening, and how I can change what modules my notebook detect?<\/p>\n<p><strong>Edit<\/strong><\/p>\n<p>By mangling with my code I found out that on the <code>\\AppData\\Roaming\\jupyter\\kernels\\kedro_project\\kernel.json<\/code> it was calling the python from the base environment, not the <code>Foo<\/code> environment. I changed it manually, but is there a mode automatic way of setting the <code>\\AppData\\Roaming\\jupyter\\kernels\\kedro_project\\kernel.json<\/code> to use the current environment I'm on?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1652795556263,
        "Question_last_edit_time":1652825440370,
        "Question_score":0,
        "Question_tags":"python|jupyter-notebook|conda|kedro",
        "Question_view_count":115,
        "Owner_creation_time":1423164285360,
        "Owner_last_access_time":1663938741010,
        "Owner_location":"Itabira, Brazil",
        "Owner_reputation":856,
        "Owner_up_votes":60,
        "Owner_down_votes":3,
        "Owner_views":106,
        "Answer_body":"<p>The custom Kedro kernel spec is a feature that I recently added to Kedro. When you run <code>kedro jupyter lab\/notebook<\/code> it should automatically pick up on the conda environment without you needing to manually edit the kernel.json file. I tested this myself to check that it worked so I'm very interested in understanding what's going on here!<\/p>\n<p>The function <a href=\"https:\/\/github.com\/kedro-org\/kedro\/blob\/58c57c384f5257b998edebb99d94bff46574ae1e\/kedro\/framework\/cli\/jupyter.py#L99\" rel=\"nofollow noreferrer\"><code>_create_kernel<\/code><\/a> is what makes the the Kedro kernel spec. The docstring for that explains what's going on, but in short we delegate to <a href=\"https:\/\/github.com\/ipython\/ipykernel\/blob\/d02f4371348187c3e5e87a46388bbef92615c110\/ipykernel\/kernelspec.py#L92\" rel=\"nofollow noreferrer\"><code>ipykernel.kernelspec.install<\/code><\/a>. This generates a kernelspec that points towards the Python path given by <code>sys.executable<\/code> (see <a href=\"https:\/\/github.com\/ipython\/ipykernel\/blob\/d02f4371348187c3e5e87a46388bbef92615c110\/ipykernel\/kernelspec.py#L27\" rel=\"nofollow noreferrer\"><code>make_ipkernel_cmd<\/code><\/a>). In theory this should already point towards the correct Python path, which takes account of the conda environment.<\/p>\n<p>It's worth checking <code>which kedro<\/code> to see which conda environment that points to, and if we need to debug further then please do raise an issue on our <a href=\"https:\/\/github.com\/kedro-org\/kedro\" rel=\"nofollow noreferrer\">Github repo<\/a>. I'd definitely like to get to the bottom of this and understand where the problem is.<\/p>\n<p>P.S. you can also do a plain <code>jupyter lab\/notebook<\/code> to launch a kernel with the right conda environment and then run <code>%load_ext kedro.extras.extensions.ipython<\/code> in the first cell. This is basically equivalent to using the Kedro kernelspec, which loads the Kedro IPython extension automatically.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1652879216830,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72275283",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":70757448,
        "Question_title":"How to save kedro dataset in azure and still have it in memory",
        "Question_body":"<p>I want to save Kedro memory dataset in azure as a file and still want to have it in memory as my pipeline will be using this later in the pipeline. Is this possible in Kedro. I tried to look at Transcoding datasets but looks like not possible. Is there any other way to acheive this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1642516914763,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_tags":"kedro",
        "Question_view_count":231,
        "Owner_creation_time":1495105930730,
        "Owner_last_access_time":1663779078467,
        "Owner_location":null,
        "Owner_reputation":29,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":"<p>This may be a good opportunity to use <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.CachedDataSet.html\" rel=\"nofollow noreferrer\">CachedDataSet<\/a> this allows you to wrap any other dataset, but once it's read into memory - make it available to downstream nodes without re-performing the IO operations.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1642520713270,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70757448",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":70355869,
        "Question_title":"How to access environment name in kedro pipeline",
        "Question_body":"<p>Is there any way to access the kedro pipeline environment name? Actually below is my problem.<\/p>\n<p>I am loading the config paths as below<\/p>\n<pre><code>conf_paths = [&quot;conf\/base&quot;, &quot;conf\/local&quot;]  \nconf_loader = ConfigLoader(conf_paths)\nparameters = conf_loader.get(&quot;parameters*&quot;, &quot;parameters*\/**&quot;)\ncatalog = conf_loader.get(&quot;catalog*&quot;)\n\n<\/code><\/pre>\n<p>But  I have few environments like  <code>&quot;conf\/server&quot; <\/code>, <code>&quot;conf\/test&quot;<\/code> etc, So if I have env name available I can add it to conf_paths as <code>&quot;conf\/&lt;env_name&gt;&quot;<\/code>  so that kedro will read the files from the respective env folder.\nBut now if the env path is not added to conf_paths, the files are not being read by kedro even if i specify the env name while I  run kedro like    <code>kedro run --env=server <\/code>\nI searched for all the docs but was not able to find any solution.<\/p>\n<p>EDIT:\nElaborating more on the problem.\nI am using the above-given parameters and catalog dicts in the nodes. I only have keys that are common for all runs in <code>conf\/base\/parameters.yml<\/code> and the environment specific keys in <code>conf\/server\/parameters.yml<\/code> but when i do <code>kedro run --env=server<\/code> I am getting <code>keyerror<\/code> which means the keys in <code>conf\/server\/parameters.yml<\/code> is not available in the parameters dict. If I add  <code>conf\/server<\/code> to config_paths kedro is running well without keyerror.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_time":1639518159733,
        "Question_last_edit_time":1639559950497,
        "Question_score":1,
        "Question_tags":"python|kedro",
        "Question_view_count":712,
        "Owner_creation_time":1495105930730,
        "Owner_last_access_time":1663779078467,
        "Owner_location":null,
        "Owner_reputation":29,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":"<p>You don't need to define config paths, config loader etc unless you are trying to override something.<\/p>\n<p>If you are using kedro 0.17.x, the hooks.py will look something like this.<\/p>\n<p>Kedro will pass, base, local and the env you specified during runtime in <code>conf_paths<\/code> into <code>ConfigLoader<\/code>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class ProjectHooks:\n    @hook_impl\n    def register_config_loader(\n        self, conf_paths: Iterable[str], env: str, extra_params: Dict[str, Any]\n    ) -&gt; ConfigLoader:\n        return ConfigLoader(conf_paths)\n\n    @hook_impl\n    def register_catalog(\n        self,\n        catalog: Optional[Dict[str, Dict[str, Any]]],\n        credentials: Dict[str, Dict[str, Any]],\n        load_versions: Dict[str, str],\n        save_version: str,\n        journal: Journal,\n    ) -&gt; DataCatalog:\n        return DataCatalog.from_config(\n            catalog, credentials, load_versions, save_version, journal\n        )\n<\/code><\/pre>\n<p>In question, I can see you have defined <code>conf_paths<\/code> and <code>conf_loader<\/code> and the env path is not present. So kedro will ignore the env passed during runtime.<\/p>",
        "Answer_comment_count":8.0,
        "Answer_creation_time":1639600021140,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70355869",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":67925860,
        "Question_title":"Kedro Data Modelling",
        "Question_body":"<p>We are struggling to model our data correctly for use in Kedro - we are using the recommended Raw\\Int\\Prm\\Ft\\Mst model but are struggling with some of the concepts....e.g.<\/p>\n<ul>\n<li>When is a dataset a feature rather than a primary dataset? The distinction seems vague...<\/li>\n<li>Is it OK for a primary dataset to consume data from another primary dataset?<\/li>\n<li>Is it good practice to build a feature dataset from the INT layer? or should it always pass through Primary?<\/li>\n<\/ul>\n<p>I appreciate there are no hard &amp; fast rules with data modelling but these are big modelling decisions &amp; any guidance or best practice on Kedro modelling would be really helpful, I can find just one table defining the layers in the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/12_faq\/01_faq.html#what-is-data-engineering-convention\" rel=\"nofollow noreferrer\">Kedro docs<\/a><\/p>\n<p>If anyone can offer any further advice or blogs\\docs talking about Kedro Data Modelling that would be awesome!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1623345871000,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_tags":"kedro",
        "Question_view_count":180,
        "Owner_creation_time":1369054667740,
        "Owner_last_access_time":1663769401193,
        "Owner_location":"United Kingdom",
        "Owner_reputation":1445,
        "Owner_up_votes":107,
        "Owner_down_votes":1,
        "Owner_views":104,
        "Answer_body":"<p>Great question. As you say, there are no hard and fast rules here and opinions do vary, but let me share my perspective as a QB data scientist and kedro maintainer who has used the layering convention you referred to several times.<\/p>\n<p>For a start, let me emphasise that there's absolutely no reason to stick to the data engineering convention suggested by kedro if it's not suitable for your needs. 99% of users don't change the folder structure in <code>data<\/code>. This is not because the kedro default is the right structure for them but because they just don't think of changing it. You should absolutely add\/remove\/rename layers to suit yourself. The most important thing is to choose a set of layers (or even a non-layered structure) that works for your project rather than trying to shoehorn your datasets to fit the kedro default suggestion.<\/p>\n<p>Now, assuming you are following kedro's suggested structure - onto your questions:<\/p>\n<blockquote>\n<p>When is a dataset a feature rather than a primary dataset? The distinction seems vague...<\/p>\n<\/blockquote>\n<p>In the case of simple features, a feature dataset can be very similar to a primary one. The distinction is maybe clearest if you think about more complex features, e.g. formed by aggregating over time windows. A primary dataset would have a column that gives a cleaned version of the original data, but without doing any complex calculations on it, just simple transformations. Say the raw data is the colour of all cars driving past your house over a week. By the time the data is in primary, it will be clean (e.g. correcting &quot;rde&quot; to &quot;red&quot;, maybe mapping &quot;crimson&quot; and &quot;red&quot; to the same colour). Between primary and the feature layer, we will have done some less trivial calculations on it, e.g. to find one-hot encoded most common car colour each day.<\/p>\n<blockquote>\n<p>Is it OK for a primary dataset to consume data from another primary dataset?<\/p>\n<\/blockquote>\n<p>In my opinion, yes. This might be necessary if you want to join multiple primary tables together. In general if you are building complex pipelines it will become very difficult if you don't allow this. e.g. in the feature layer I might want to form a dataset containing <code>composite_feature = feature_1 * feature_2<\/code> from the two inputs <code>feature_1<\/code> and <code>feature_2<\/code>. There's no way of doing this without having multiple sub-layers within the feature layer.<\/p>\n<p>However, something that is generally worth avoiding is a node that consumes data from many different layers. e.g. a node that takes in one dataset from the feature layer and one from the intermediate layer. This seems a bit strange (why has the latter dataset not passed through the feature layer?).<\/p>\n<blockquote>\n<p>Is it good practice to build a feature dataset from the INT layer? or should it always pass through Primary?<\/p>\n<\/blockquote>\n<p>Building features from the intermediate layer isn't unheard of, but it seems a bit weird. The primary layer is typically an important one which forms the basis for all feature engineering. If your data is in a shape that you can build features then that means it's probably primary layer already. In this case, maybe you don't need an intermediate layer.<\/p>\n<p>The above points might be summarised by the following rules (which should no doubt be broken when required):<\/p>\n<ol>\n<li>The input datasets for a node in layer <code>L<\/code> should all be in the same layer, which can be either <code>L<\/code> or <code>L-1<\/code><\/li>\n<li>The output datasets for a node in layer <code>L<\/code> should all be in the same layer <code>L<\/code>, which can be either <code>L<\/code> or <code>L+1<\/code><\/li>\n<\/ol>\n<blockquote>\n<p>If anyone can offer any further advice or blogs\\docs talking about Kedro Data Modelling that would be awesome!<\/p>\n<\/blockquote>\n<p>I'm also interested in seeing what others think here! One possibly useful thing to note is that kedro was inspired by cookiecutter data science, and the kedro layer structure is an extended version of <a href=\"http:\/\/drivendata.github.io\/cookiecutter-data-science\/#directory-structure\" rel=\"nofollow noreferrer\">what's suggested there<\/a>. Maybe other projects have taken this directory structure and adapted it in different ways.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1623349806340,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67925860",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":58716474,
        "Question_title":"How to run a pipeline except for a few nodes?",
        "Question_body":"<p>I want to run a pipeline for different files, but some of them don't need all of the defined nodes. How can I pass them?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_time":1572974635783,
        "Question_last_edit_time":1572982969067,
        "Question_score":0,
        "Question_tags":"python|pipeline|kedro",
        "Question_view_count":859,
        "Owner_creation_time":1572973807453,
        "Owner_last_access_time":1663947835340,
        "Owner_location":null,
        "Owner_reputation":29,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Answer_body":"<p>To filter out a few lines of a pipeline you can simply filter the pipeline list from inside of python, my favorite way is to use a list comprehension.<\/p>\n\n<p><strong>by name<\/strong><\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>nodes_to_run = [node for node in pipeline.nodes if 'dont_run_me' not in node.name]\nrun(nodes_to_run, io)\n<\/code><\/pre>\n\n<p><strong>by tag<\/strong><\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>nodes_to_run = [node for node in pipeline.nodes if 'dont_run_tag' not in node.tags]\nrun(nodes_to_run, io)\n<\/code><\/pre>\n\n<p>It's possible to filter by any attribute tied to the pipeline node, (name, inputs, outputs, short_name, tags)<\/p>\n\n<p>If you need to run your pipeline this way in production or from the command line, you can either tag your pipeline to run with tags, or add a custom <code>click.option<\/code> to your <code>run<\/code> function inside of <code>kedro_cli.py<\/code> then run this filter when the flag is <code>True<\/code>.<\/p>\n\n<p><strong>Note<\/strong>\nThis assumes that you have your pipeline loaded into memory as <code>pipeline<\/code> and catalog loaded in as <code>io<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1573135218943,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58716474",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":69378898,
        "Question_title":"How to dynamically pass save_args to kedro catalog?",
        "Question_body":"<p>I'm trying to write delta tables in Kedro. Changing file format to delta makes the write as delta tables with mode as overwrite.<\/p>\n<p>Previously, a node in the raw layer (meta_reload) creates a dataset that determines what's the start date for incremental load for each dataset. each node uses that raw dataset to filter the working dataset to apply the transformation logic and write partitioned parquet tables incrementally.<\/p>\n<p>But now writing delta with mode as overwrite with just file type change to delta makes current incremental data overwrite all the past data instead of just those partitions. So I need to use replaceWhere option in save_args in the catalog.\nHow would I determine the start date for replaceWhere in the catalog when I need to read the meta_reload raw dataset to determine the date.\nIs there a way to dynamically pass the save_args from inside the node?<\/p>\n<pre><code>my_dataset:\n  type: my_project.io.pyspark.SparkDataSet\n  filepath: &quot;s3:\/\/${bucket_de_pipeline}\/${data_environment_project}\/${data_environment_intermediate}\/my_dataset\/&quot;\n  file_format: delta\n  layer: intermediate\n  save_args:\n    mode: &quot;overwrite&quot;\n    replaceWhere: &quot;DATE_ID &gt; xyz&quot;  ## what I want to implement dynamically\n    partitionBy: [ &quot;DATE_ID&quot; ]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1632927672467,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_tags":"databricks|delta-lake|delta|kedro",
        "Question_view_count":341,
        "Owner_creation_time":1477986647030,
        "Owner_last_access_time":1663781097037,
        "Owner_location":"Atlanta, GA, USA",
        "Owner_reputation":171,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":"<p>I've answered this on the GH <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/discussions\/910\" rel=\"nofollow noreferrer\">discussion<\/a>. In short you would need to subclass and define your own <code>SparkDataSet<\/code> we avoid changing the underlying API of the datasets at a Kedro level, but you're encouraged to alter and remix this for your own purposes.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1632930071223,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69378898",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":62283931,
        "Question_title":"Dynamic instance of pipeline execution based on dataset partition\/iterator logic",
        "Question_body":"<p>Not sure if this is possible or not, but this is what I am trying to do: -<\/p>\n\n<p>I want to extract out portions (steps) of a function as individual nodes (ok so far), but the catch is I have an iterator on top of steps, which is dependent on some logic on dataset i.e. repeating the same operation (which are independent) on logical partitions of a dataset. <\/p>\n\n<h3>Example code<\/h3>\n\n<pre class=\"lang-py prettyprint-override\"><code>def single_node(list_of_numbers):\n   modified_list = [] # to store all output\n   for x in list_of_numbers: # iteration logic\n      x+=1 # Step 1\n      x=str(x) # Step 2\n      x+=\"_suffix\" # Step 3\n      modified_list.append(x) # append to final output\n   return modified_list # return\n<\/code><\/pre>\n\n<h3>Context<\/h3>\n\n<ol>\n<li>In the provided example, suppose currently I have a single node which performs all of the steps.<\/li>\n<li>So the current pipeline has one node which takes 1 input and returns 1 output.<\/li>\n<li>As the complexity of my steps increases, I want to expose them as individual nodes. So I create another pipeline with these 3 steps as individual nodes and connected them together. (their input and output)<\/li>\n<li>But my overall requirement is unchanged, I want to iterate over all values in <code>list_of_numbers<\/code>, and for each element in this list I want to call this new pipeline. Finally I want to merge the output of all run's and generate one output.<\/li>\n<\/ol>\n\n<p>Seems somewhat similar to dynamic graph (multiple dynamic instance of a pipeline) which expands based on the dataset. <\/p>\n\n<h3>Additional points to consider,<\/h3>\n\n<ol>\n<li>My input is a single file. Say I do the partition of dataset based on some logic defined as a node. So this node could have multiple outputs. (the exact count totally depends on the dataset, here the size of list) <\/li>\n<li>For each output of the data iterator node, I need to \"spawn\" one pipeline.<\/li>\n<li>Finally, merge the outputs of all \"spawned\" pipelines. (this logic could again be defined in a merge node with multiple dynamic inputs).<\/li>\n<\/ol>\n\n<p>Is there a way to do this? Thank you! <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591709703053,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_tags":"python|kedro",
        "Question_view_count":561,
        "Owner_creation_time":1338197185853,
        "Owner_last_access_time":1664043570243,
        "Owner_location":"India",
        "Owner_reputation":1025,
        "Owner_up_votes":162,
        "Owner_down_votes":1,
        "Owner_views":228,
        "Answer_body":"<p>This looks like the PartitionedDataSet or IncrementalDataSet might be of use to you.<\/p>\n\n<p>They allow you to segregate your similar data into separate chunks, determined by files, and repeat operations on those chunks as you see fit.<\/p>\n\n<p>So, rather than kick of x pipelines containing y nodes, you would have one pipeline that contains y nodes which processes x chunks of your data.<\/p>\n\n<p>More on IncrementalDataSet in this video: <a href=\"https:\/\/www.youtube.com\/watch?v=v7JSSiYgqpg\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=v7JSSiYgqpg<\/a><\/p>\n\n<pre><code># nodes.py\n\nfrom typing import Any, Dict, Callable\n\ndef _dict_mapper(dictionary: Dict[str, Any], fun: Callable):\n  # Apply your function to the dictionary mapping\n  return {k: fun(v) for k, v in dictionary.items()}\n\ndef node_0(list_of_strings: Dict[str, str]):\n  return _dict_mapper(list_of_strings, lambda x: int(x))\n\ndef node_1(list_of_numbers: Dict[str, int]):\n  return _dict_mapper(list_of_numbers, lambda x: x+1)\n\ndef node_2(list_of_numbers: Dict[str, int]):\n  return _dict_mapper(list_of_numbers, lambda x: str(x))\n\ndef node_3(list_of_strings: Dict[str, str]):\n  return _dict_mapper(list_of_strings, lambda x: f'{x}_suffix')\n\n\n# catalog.yml\ndata:\n  type: IncrementalDataSet\n  dataset: text.TextDataSet\n  path: folder\/with\/text_files\/each\/containing\/single\/number\/\n  filename_suffix: .txt\n\n# pipeline.py\n\nPipeline([\n  node(node_0, inputs='data', outputs='0'),\n  node(node_1, inputs='0', outputs='1'),\n  node(node_2, inputs='1', outputs='2'),\n  node(node_3, inputs='2', outputs='final_output'),\n])\n\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1591785478973,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62283931",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":61825202,
        "Question_title":"Accessing Kedro CLI from an existing project",
        "Question_body":"<p>I have an existing project, cloned with <code>git clone<\/code>.<\/p>\n\n<p>After I <code>pip install kedro<\/code> I can run <code>kedro info<\/code> fine but I dont seem to have access to the projects CLI for example if I try to run<code>kedro install<\/code> I get the following error:<\/p>\n\n<pre><code>Usage: kedro [OPTIONS] COMMAND [ARGS]...\nTry 'kedro -h' for help.\n\nError: No such command 'install'.\n<\/code><\/pre>\n\n<p>Any clues on what to do for existing projects are much appreciated.<\/p>\n\n<p>Not sure if this matters but I am working inside a conda environment which is inside a docker container.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1589564204173,
        "Question_last_edit_time":1589721006283,
        "Question_score":2,
        "Question_tags":"installation|kedro",
        "Question_view_count":1175,
        "Owner_creation_time":1387377887347,
        "Owner_last_access_time":1641059204137,
        "Owner_location":"Edinburgh, United Kingdom",
        "Owner_reputation":1240,
        "Owner_up_votes":124,
        "Owner_down_votes":2,
        "Owner_views":135,
        "Answer_body":"<p>Project CLIs are available if you run <code>kedro<\/code> at your Kedro project directory. <\/p>\n\n<ol>\n<li><p>Run <code>kedro new<\/code> to create a Kedro project<\/p><\/li>\n<li><p><code>cd &lt;your-kedro-project&gt;<\/code><\/p><\/li>\n<li><p><code>kedro<\/code> at the project directory<\/p><\/li>\n<\/ol>\n\n<p>And you should see the project level CLIs<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9NnAN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9NnAN.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Also for your existing project, check if you have <code>kedro_cli.py<\/code> at your project directory.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1589565622150,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61825202",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":63585717,
        "Question_title":"In Kedro, How to specify layer to parameters.yml?",
        "Question_body":"<p>Currently, I'm using kedro and kedro-viz.<\/p>\n<p>I can specify a layer of dataset from catalog.yml.<\/p>\n<pre><code>hoge:\n  type: MemoryDataSet\n  layer: raw\n<\/code><\/pre>\n<p>but I don't know how to do it with parameters.yml<\/p>\n<pre><code>step_size: 1\nlearning_rate: 0.01\n<\/code><\/pre>\n<p>if it can be done not in parameters.yml but in run.py, I want to see example code.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1598383098480,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_tags":"kedro",
        "Question_view_count":225,
        "Owner_creation_time":1545209959320,
        "Owner_last_access_time":1607654795560,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>At the moment layers can only be specified for datasets, not for nodes or parameters.<\/p>\n<p>If you have a specific use case for adding layers to nodes\/parameters, please let us know by opening a feature request in the Kedro repo: <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/issues\" rel=\"nofollow noreferrer\">https:\/\/github.com\/quantumblacklabs\/kedro\/issues<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1598442196703,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63585717",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":59067349,
        "Question_title":"How to run parts of your Kedro pipeline conditionally?",
        "Question_body":"<p>I have a big pipeline, taking a few hours to run. A small part of it needs to run quite often, how do I run it without triggering the entire pipeline?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1574848206347,
        "Question_last_edit_time":null,
        "Question_score":3,
        "Question_tags":"python|pipeline|kedro",
        "Question_view_count":4724,
        "Owner_creation_time":1457555855467,
        "Owner_last_access_time":1661507382820,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":86,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Answer_body":"<p>There are multiple ways to specify which nodes or parts of your pipeline to run. <\/p>\n\n<ol>\n<li><p>Use <code>kedro run<\/code> parameters like <code>--to-nodes<\/code>\/<code>--from-nodes<\/code>\/<code>--node<\/code> to explicitly define what needs to be run.<\/p><\/li>\n<li><p>In <code>kedro&gt;=0.15.2<\/code> you can define multiple pipelines, and then run only one of them with <code>kedro run --pipeline &lt;name&gt;<\/code>. If no <code>--pipeline<\/code> parameter is specified, the default pipeline is run. The default pipeline might combine several other pipelines. More information about using modular pipelines: <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/06_pipelines.html#modular-pipelines\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/06_pipelines.html#modular-pipelines<\/a><\/p><\/li>\n<li><p>Use tags. Tag a small portion of your pipeline with something like \"small\", and then do <code>kedro run --tag small<\/code>. Read more here: <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/05_nodes.html#tagging-nodes\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/05_nodes.html#tagging-nodes<\/a><\/p><\/li>\n<\/ol>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1574848206347,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_favorite_count":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59067349",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":63238607,
        "Question_title":"Override nested parameters using kedro run CLI command",
        "Question_body":"<p>I am using nested parameters in my <code>parameters.yml<\/code> and would like to override these using runtime parameters for the <code>kedro run<\/code> CLI command:<\/p>\n<pre><code>train:\n    batch_size: 32\n    train_ratio: 0.9\n    epochs: 5\n<\/code><\/pre>\n<p>The following doesn't seem to work:<\/p>\n<pre><code>kedro run --params  train.batch_size:64,train.epochs:50 \n<\/code><\/pre>\n<p>the values for epoch and batch_size are those from the <code>parameters.yml<\/code>. How can I override these parameters with the cli command?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1596500312837,
        "Question_last_edit_time":null,
        "Question_score":3,
        "Question_tags":"python|command-line-interface|kedro",
        "Question_view_count":549,
        "Owner_creation_time":1362514672823,
        "Owner_last_access_time":1663952544520,
        "Owner_location":"Vorarlberg, Austria",
        "Owner_reputation":1570,
        "Owner_up_votes":595,
        "Owner_down_votes":1,
        "Owner_views":159,
        "Answer_body":"<p>The additional parameters get passed into the <code>KedroContext<\/code> object via <code>load_context(Path.cwd(), env=env, extra_params=params)<\/code> in <code>kedro_cli.py<\/code>. Here you can see that there's a callback (protected) function called <code>_split_params<\/code> which splits the key-value pairs on <code>:<\/code>.<\/p>\n<p>This <code>_split_params<\/code> first splits string on commas (to get multiple params) and then on colons. Actually adding a print\/logging statement of what gets passed into <code>extra_params<\/code> will show you something like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>{'train.batch_size': 64, 'train.epochs': 50}\n<\/code><\/pre>\n<p>I think you have a couple options:<\/p>\n<ol>\n<li>Un-nesting the params. That way you will override them correctly.<\/li>\n<li>Adding custom logic to <code>_split_params<\/code> in <code>kedro_cli.py<\/code> to create a nested dictionary on <code>.<\/code> characters which gets passed into the func mentioned above. I think you can reuse a lot of the existing logic.<\/li>\n<\/ol>\n<p>NB: This was tested on <code>kedro==0.16.2<\/code>.<\/p>\n<p>NB2: The way <code>kedro<\/code> splits out nested params is using the <code>_get_feed_dict<\/code> and <code>_add_param_to_feed_dict<\/code> functions in <code>context.py<\/code>. Specifically, <code>_add_param_to_feed_dict<\/code> is a recursive function that unpacks a dictionary and formats as <code>&quot;{}.{}&quot;.format(key, value)<\/code>. IMO you can use the logic from here.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1596531353283,
        "Answer_last_edit_time":1596532757497,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63238607",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":68923747,
        "Question_title":"Saving data with DataCatalog",
        "Question_body":"<p>I was looking at <code>iris<\/code> project example provided by kedro. Apart from logging the accuracy I also wanted to save the <code>predictions<\/code> and <code>test_y<\/code> as a csv.<\/p>\n<p>This is the example node provided by kedro.<\/p>\n<pre><code>def report_accuracy(predictions: np.ndarray, test_y: pd.DataFrame) -&gt; None:\n    &quot;&quot;&quot;Node for reporting the accuracy of the predictions performed by the\n    previous node. Notice that this function has no outputs, except logging.\n    &quot;&quot;&quot;\n    # Get true class index\n    target = np.argmax(test_y.to_numpy(), axis=1)\n    # Calculate accuracy of predictions\n    accuracy = np.sum(predictions == target) \/ target.shape[0]\n    # Log the accuracy of the model\n    log = logging.getLogger(__name__)\n    log.info(&quot;Model accuracy on test set: %0.2f%%&quot;, accuracy * 100)\n<\/code><\/pre>\n<p>I added the following to save the data.<\/p>\n<pre><code>data = pd.DataFrame({&quot;target&quot;: target , &quot;prediction&quot;: predictions})\ndata_set = CSVDataSet(filepath=&quot;data\/test.csv&quot;)\ndata_set.save(data)\n<\/code><\/pre>\n<p>This works as intended, however, my question is &quot;is it the kedro way of doing thing&quot; ? Can I provide the <code>data_set <\/code> in <code>catalog.yml<\/code> and later save <code>data<\/code> to it? If I want to do it, how do I access the <code>data_set<\/code> from <code>catalog.yml<\/code> inside a node.<\/p>\n<p>Is there a way to save data without creating a catalog inside a node like this <code>data_set = CSVDataSet(filepath=&quot;data\/test.csv&quot;)<\/code> ? I want this in <code>catalog.yml<\/code>, if possible and if it follows kedro convention!.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629897723887,
        "Question_last_edit_time":1629897818943,
        "Question_score":0,
        "Question_tags":"python|kedro",
        "Question_view_count":333,
        "Owner_creation_time":1519724643533,
        "Owner_last_access_time":1646603612997,
        "Owner_location":null,
        "Owner_reputation":453,
        "Owner_up_votes":41,
        "Owner_down_votes":0,
        "Owner_views":79,
        "Answer_body":"<p>Kedro actually abstracts this part for you. You don't need to access the datasets via their Python API.<\/p>\n<p>Your <code>report_accuracy<\/code> method does need to be tweaked to return the <code>DataFrame<\/code> instead of <code>None<\/code>.<\/p>\n<p>Your node needs to be defined as such:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>node(\n  func=report_accuracy,\n  inputs='dataset_a',\n  outputs='dataset_b'\n)\n<\/code><\/pre>\n<p>Kedro then looks at your catalog and will load\/save <code>dataset_a<\/code> and <code>dataset_b<\/code> as required:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>dataset_a:\n   type: pandas.CSVDataSet\n   path: xxxx.csv\n\ndataset_b:\n   type: pandas.ParquetDataSet\n   path: yyyy.pq\n<\/code><\/pre>\n<p>As you run the node\/pipeline Kedro will handle the load\/save operations for you. You also don't need to save every dataset if it's only used mid-way in a pipeline, you can read about <code>MemoryDataSet<\/code>s <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/11_tools_integration\/01_pyspark.html#use-memorydataset-for-intermediary-dataframe\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1629898742263,
        "Answer_last_edit_time":1643038306147,
        "Answer_score":7.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68923747",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":63182406,
        "Question_title":"Does kedro support tfrecord?",
        "Question_body":"<p>To train tensorflow keras models on AI Platform using Docker containers, we convert our raw images stored on GCS to a tfrecord dataset using <code>tf.data.Dataset<\/code>. Thereby the data is never stored locally. Instead the raw images are transformed directly to tfrecords to another bucket. Is it possible to make use of <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\" rel=\"nofollow noreferrer\">kedro<\/a> with a tfrecord dataset and the streaming capability of <code>tf.data.Dataset<\/code>? According to the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.extras.datasets.html\" rel=\"nofollow noreferrer\">docs<\/a> kedro doesn't seem to support tfrecord datasets.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596148880400,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|tfrecord|gcp-ai-platform-training|kedro|tf.data.dataset",
        "Question_view_count":107,
        "Owner_creation_time":1362514672823,
        "Owner_last_access_time":1663952544520,
        "Owner_location":"Vorarlberg, Austria",
        "Owner_reputation":1570,
        "Owner_up_votes":595,
        "Owner_down_votes":1,
        "Owner_views":159,
        "Answer_body":"<p>Only TF related dataset we have at the moment is <code>TensorFlowModelDataset<\/code> (<a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/_modules\/kedro\/extras\/datasets\/tensorflow\/tensorflow_model_dataset.html\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/latest\/_modules\/kedro\/extras\/datasets\/tensorflow\/tensorflow_model_dataset.html<\/a>), but you can easily <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/07_extend_kedro\/01_custom_datasets.html#custom-datasets\" rel=\"nofollow noreferrer\">add your own custom dataset<\/a>, or please add a feature request\/your contribution in <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/issues\" rel=\"nofollow noreferrer\">the repo<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1596184916010,
        "Answer_last_edit_time":1596272833650,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63182406",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":69592125,
        "Question_title":"Kedro - Memory management",
        "Question_body":"<p>I am working on a Kedro 0.17.2 project that is running on out-of-memory issues and I'm trying to reduce the memory footprint.<\/p>\n<p>I'm doing the profiling by using <code>mprof<\/code> from the <code>memory-profiler<\/code> library and I noticed that there is always a child process and memory seems to duplicate in the main process after the first computation in the node that is running. Is it possible that Kedro is duplicating the dataframes in memory? And, if so, is there a way to avoid this?<\/p>\n<p>Notes:<\/p>\n<ul>\n<li>I'm using the <code>SequentialRunner<\/code><\/li>\n<li>I'm not using the <code>is_async<\/code> cli option<\/li>\n<li>I'm not using either multithreading or multiprocessing in the node execution<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/87hhM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/87hhM.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1634349281137,
        "Question_last_edit_time":null,
        "Question_score":3,
        "Question_tags":"python|pandas|out-of-memory|kedro",
        "Question_view_count":308,
        "Owner_creation_time":1389700864877,
        "Owner_last_access_time":1664081150257,
        "Owner_location":"Sydney, New South Wales, Australia",
        "Owner_reputation":193,
        "Owner_up_votes":48,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Answer_body":"<p>It turns out this issue is caused by a possible bug in the <code>memory-profiler<\/code> library that is used in the <code>kedro.extras.decorators.memory_profiler.mem_profile<\/code> decorator.<\/p>\n<p>The kedro decorator makes use of the <code>memory_usage<\/code> function in the <code>memory-profiler<\/code> module. It is used to sample the total memory being used by the running function from within the python process.<\/p>\n<p>There is an open issue about this problem but with no solution yet.\n<a href=\"https:\/\/github.com\/pythonprofilers\/memory_profiler\/issues\/332\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pythonprofilers\/memory_profiler\/issues\/332<\/a><\/p>\n<p>For the moment I have just removed the decorator.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1634968179750,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69592125",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":68363180,
        "Question_title":"How to use SQL Server Bulk Insert in Kedro Node?",
        "Question_body":"<p>I am managing a data pipeline using Kedro and at the last step I have a huge csv file stored in a S3 bucket and I need to load it back to SQL Server.<\/p>\n<p>I'd normally go about that with a <a href=\"https:\/\/towardsdatascience.com\/use-python-and-bulk-insert-to-quickly-load-data-from-csv-files-into-sql-server-tables-ba381670d376\" rel=\"nofollow noreferrer\">bulk insert<\/a>, but not quite sure how to fit that into the <strong>kedro<\/strong> templates. This are the destination table and the S3 Bucket as configured in the <code>catalog.yml<\/code><\/p>\n<pre><code>flp_test:\n  type: pandas.SQLTableDataSet\n  credentials: dw_dev_credentials\n  table_name: flp_tst\n  load_args:\n    schema: 'dwschema'\n  save_args:\n    schema: 'dwschema'\n    if_exists: 'replace'\n\nbulk_insert_input:\n   type: pandas.CSVDataSet\n   filepath: s3:\/\/your_bucket\/data\/02_intermediate\/company\/motorbikes.csv\n   credentials: dev_s3\n\n\ndef insert_data(self, conn, csv_file_nm, db_table_nm):\n    qry = &quot;BULK INSERT &quot; + db_table_nm + &quot; FROM '&quot; + csv_file_nm + &quot;' WITH (FORMAT = 'CSV')&quot;\n    # Execute the query\n    cursor = conn.cursor()\n    success = cursor.execute(qry)\n    conn.commit()\n    cursor.close\n<\/code><\/pre>\n<ul>\n<li>How do I point <code>csv_file_nm<\/code> to my <code>bulk_insert_input<\/code> S3 catalog?<\/li>\n<li>Is there a proper way to indirectly access <code>dw_dev_credentials<\/code> to do the insert?<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626182174053,
        "Question_last_edit_time":1626187242810,
        "Question_score":0,
        "Question_tags":"sql-server|bulkinsert|bulk-load|kedro",
        "Question_view_count":241,
        "Owner_creation_time":1271930452580,
        "Owner_last_access_time":1664057428157,
        "Owner_location":null,
        "Owner_reputation":5469,
        "Owner_up_votes":168,
        "Owner_down_votes":3,
        "Owner_views":232,
        "Answer_body":"<p>Kedro's <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.extras.datasets.pandas.SQLTableDataSet.html\" rel=\"nofollow noreferrer\">pandas.SQLTableDataSet.html<\/a> uses the <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.23.4\/generated\/pandas.DataFrame.to_sql.html\" rel=\"nofollow noreferrer\">pandas.to_sql<\/a> method as is. To use this as is you would need one <code>pandas.CSVDataSet<\/code> into a <code>node<\/code> which then writes to a target <code>pandas.SQLDataTable<\/code> dataset in order to write it to SQL. If you have Spark available this will be faster than Pandas.<\/p>\n<p>In order to use the built in <code>BULK INSERT<\/code> query I think you will need to define a <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/03_custom_datasets.html\" rel=\"nofollow noreferrer\">custom dataset<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1626182900783,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68363180",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":59829640,
        "Question_title":"Kedro deployment to databricks",
        "Question_body":"<p>Maybe I misunderstand the purpose of packaging but it doesn't seem to helpful in creating an artifact for production deployment because it only packages code. It leaves out the conf, data, and other directories that make the kedro project reproducible.<\/p>\n\n<p>I understand that I can use docker or airflow plugins for deployment but what about deploying to databricks. Do you have any advice here?<\/p>\n\n<p>I was thinking about making a wheel that could be installed on the cluster but I would need to package the conf first. Another option is to just sync a git workspace to the cluster and run kedro via a notebook.<\/p>\n\n<p>Any thoughts on a best practice?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1579548059213,
        "Question_last_edit_time":null,
        "Question_score":5,
        "Question_tags":"kedro",
        "Question_view_count":1705,
        "Owner_creation_time":1283979840287,
        "Owner_last_access_time":1662405629080,
        "Owner_location":null,
        "Owner_reputation":1152,
        "Owner_up_votes":127,
        "Owner_down_votes":8,
        "Owner_views":101,
        "Answer_body":"<p>If you are not using <code>docker<\/code> and just using kedro to deploy directly on a databricks cluster. This is how we have been deploying kedro to databricks.<\/p>\n\n<ol>\n<li><p>CI\/CD pipeline builds using <code>kedro package<\/code>. Creates a wheel file.<\/p><\/li>\n<li><p>Upload <code>dist<\/code> and <code>conf<\/code> to dbfs or AzureBlob file copy (if using Azure Databricks)<\/p><\/li>\n<\/ol>\n\n<p>This will upload everything to databricks on every <code>git push<\/code><\/p>\n\n<p>Then you can have a notebook with the following:<\/p>\n\n<ol>\n<li>You can have an init script in databricks something like:<\/li>\n<\/ol>\n\n<pre><code>from cargoai import run\nfrom cargoai.pipeline import create_pipeline\n\nbranch = dbutils.widgets.get(\"branch\")\n\nconf = run.get_config(\n    project_path=f\"\/dbfs\/project_name\/build\/cicd\/{branch}\"\n)\ncatalog = run.create_catalog(config=conf)\npipeline = create_pipeline()\n\n<\/code><\/pre>\n\n<p>Here <code>conf<\/code>, <code>catalog<\/code>, and <code>pipeline<\/code> will be available<\/p>\n\n<ol start=\"2\">\n<li><p>Call this init script when you want to run a branch or a <code>master<\/code> branch in production like: <br\/><code>%run \"\/Projects\/InitialSetup\/load_pipeline\" $branch=\"master\"<\/code><\/p><\/li>\n<li><p>For development and testing, you can run specific nodes<br\/><code>pipeline = pipeline.only_nodes_with_tags(*tags)<\/code><\/p><\/li>\n<li><p>Then run a full or a partial pipeline with just <code>SequentialRunner().run(pipeline, catalog)<\/code><\/p><\/li>\n<\/ol>\n\n<p>In production, this notebook can be scheduled by databricks. If you are on Azure Databricks, you can use <code>Azure Data Factory<\/code> to schedule and run this. <\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1579581192127,
        "Answer_last_edit_time":1579618548797,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59829640",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":70719080,
        "Question_title":"AttributeError: Object ParquetDataSet cannot be loaded from kedro.extras.datasets.pandas",
        "Question_body":"<p>I'm quite new using Kedro and after installing kedro in my conda environment, I'm getting the following error when trying to list my catalog:<\/p>\n<p>Command performed: <code>kedro catalog list<\/code><\/p>\n<p>Error:<\/p>\n<blockquote>\n<p>kedro.io.core.DataSetError: An exception occurred when parsing config\nfor DataSet <code>df_medinfo_raw<\/code>: Object <code>ParquetDataSet<\/code> cannot be loaded\nfrom <code>kedro.extras.datasets.pandas<\/code>. Please see the documentation on\nhow to install relevant dependencies for\nkedro.extras.datasets.pandas.ParquetDataSet:<\/p>\n<\/blockquote>\n<p>I installed kedro trough conda-forge: <code>conda install -c conda-forge &quot;kedro[pandas]&quot;<\/code>. As far as I understand, this way to install kedro also installs the pandas dependencies.<\/p>\n<p>I tried to read the kedro documentation for dependencies, but it's not really clear how to solve this kind of issue.<\/p>\n<p>My kedro version is <strong>0.17.6<\/strong>.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1642224358493,
        "Question_last_edit_time":1642264400917,
        "Question_score":3,
        "Question_tags":"python|pip|conda|kedro",
        "Question_view_count":863,
        "Owner_creation_time":1492098397317,
        "Owner_last_access_time":1657650559800,
        "Owner_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Owner_reputation":135,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":"<p>Kedro uses Pandas to load <code>ParquetDataSet<\/code> objects, and Pandas requires additional dependencies to accomplish this (see <a href=\"https:\/\/pandas.pydata.org\/docs\/getting_started\/install.html#other-data-sources\" rel=\"nofollow noreferrer\">&quot;Installation: Other data sources&quot;<\/a>). That is, in addition to Pandas, one must also install either <code>fastparquet<\/code> or <code>pyarrow<\/code>.<\/p>\n<p>For Conda you either want:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>## use pyarrow for parquet\nconda install -c conda-forge kedro pandas pyarrow\n<\/code><\/pre>\n<p>or<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>## or use fastparquet for parquet\nconda install -c conda-forge kedro pandas fastparquet\n<\/code><\/pre>\n<p>Note that the syntax used in the question <code>kedro[pandas]<\/code> is meaningless to Conda (i.e., it ultimately parses to just <code>kedro<\/code>). Conda package specification uses <a href=\"https:\/\/stackoverflow.com\/a\/57734390\/570918\">a custom grammar called <code>MatchSpec<\/code><\/a>, where anything inside a <code>[...]<\/code> is parsed for a <code>[key1=value1;key2=value2;...]<\/code> syntax. Essentially, the <code>[pandas]<\/code> is treated as an unknown key, which is ignored.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1642280220677,
        "Answer_last_edit_time":1642280575967,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70719080",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":58733422,
        "Question_title":"How can we get the pipeline to read columns with special characters?",
        "Question_body":"<p>I am using the \"usecols\" parameter to get some columns of a .xlsx file (I am using the xls_local.py file from the Kedro tutorial) but the program says that \"usecols do not match columns, columns expected but not found:\" and it only shows the columns that have special characters. How can I fix this, please? Thank you very much for your attention.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1573053746820,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_tags":"pipeline|kedro",
        "Question_view_count":157,
        "Owner_creation_time":1572973807453,
        "Owner_last_access_time":1663947835340,
        "Owner_location":null,
        "Owner_reputation":29,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Answer_body":"<p>As far as I can tell, this isn't a <code>kedro<\/code> issue, but a <code>pandas.read_excel<\/code> issue, which is what <code>kedro<\/code> uses under the hood. This seems to be broken in <code>pandas<\/code> itself, and a workaround is to reference the columns using letters instead, so something like <code>usecols='A:D'<\/code> and then you can rename the columns to what they should be by doing <code>df.columns = [\"colname with special characters\", \"b\", \"c\", \"d\"]<\/code> for example.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1573057563320,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58733422",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":70365836,
        "Question_title":"Is there a package in R that mimics KEDRO as a modular collaborative framework for development?",
        "Question_body":"<p>I currently work with Kedro (from quantum black <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/01_introduction\/01_introduction.html\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/stable\/01_introduction\/01_introduction.html<\/a>) as a framework for deployment oriented framework to code collaboratively. It is a great framework to develop machine learning in a team.<\/p>\n<p>I am looking for an R equivalent.<\/p>\n<p>My main issue is that I have teams of data scientists that develop in R, but each team is developing in different formats.<\/p>\n<p>I wanted to make them follow a common framework to develop deployment ready R code, easy to work on in 2 or 3-people teams.<\/p>\n<p>Any suggestions are welcome<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1639580284993,
        "Question_last_edit_time":1639581270160,
        "Question_score":1,
        "Question_tags":"r|frameworks|collaboration|kedro",
        "Question_view_count":246,
        "Owner_creation_time":1460063521157,
        "Owner_last_access_time":1661517013243,
        "Owner_location":"S\u00e3o Paulo - SP, Brasil",
        "Owner_reputation":2472,
        "Owner_up_votes":256,
        "Owner_down_votes":18,
        "Owner_views":186,
        "Answer_body":"<p>member of the Kedro team here. We've heard good things about the <a href=\"https:\/\/github.com\/ropensci\/targets\" rel=\"nofollow noreferrer\">Targets<\/a> library doing similar things in the R world.<\/p>\n<p>It would be remiss for me to not try and covert you and your team to the dark side too :)<\/p>\n<p>Before Kedro our teams internally were writing a mix of Python, SQL, Scala and R. Part of the drive to write the framework was to get our teams internally speaking the same language. Python felt like the best compromise available at the time and I'd argue this still holds. We also had trouble productionising R projects and felt Python is more manageable in that respect.<\/p>\n<p>Whilst not officially documented - I've also seen some people on the Kedro <a href=\"https:\/\/discord.com\/channels\/778216384475693066\/778998585454755870\/901111920290070588\" rel=\"nofollow noreferrer\">Discord play with r2py<\/a> so that they can use specific R functionality within their Python pipelines.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1639581358650,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70365836",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":61737613,
        "Question_title":"Is there IO functionality to store trained models in kedro?",
        "Question_body":"<p>In the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.html#\" rel=\"nofollow noreferrer\">IO section of the kedro API docs<\/a> I could not find functionality w.r.t. storing trained models (e.g. <code>.pkl<\/code>, <code>.joblib<\/code>, <code>ONNX<\/code>, <code>PMML<\/code>)? Have I missed something?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1589224779627,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_tags":"python|kedro",
        "Question_view_count":795,
        "Owner_creation_time":1441627039650,
        "Owner_last_access_time":1663951876837,
        "Owner_location":"Augsburg, Germany",
        "Owner_reputation":3635,
        "Owner_up_votes":1076,
        "Owner_down_votes":3,
        "Owner_views":383,
        "Answer_body":"<p>There is the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.PickleLocalDataSet.html#kedro.io.PickleLocalDataSet\" rel=\"nofollow noreferrer\"><code>pickle<\/code><\/a> dataset in <code>kedro.io<\/code>, that you can use to save trained models and\/or anything you want to pickle and is serialisable (models being a common object). It accepts a <code>backend<\/code> that defaults to <code>pickle<\/code> but can be set to <code>joblib<\/code> if you want to use <code>joblib<\/code> instead.<\/p>\n\n<p>I'm just going to quickly note that Kedro is moving to <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.extras.datasets.html\" rel=\"nofollow noreferrer\"><code>kedro.extras.datasets<\/code><\/a> for its datasets and moving away from having non-core datasets in <code>kedro.io<\/code>. You might want to look at <code>kedro.extras.datasets<\/code> and in Kedro 0.16 onwards <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/kedro.extras.datasets.pickle.PickleDataSet.html#kedro.extras.datasets.pickle.PickleDataSet\" rel=\"nofollow noreferrer\"><code>pickle.PickleDataSet<\/code><\/a> with <code>joblib<\/code> support.<\/p>\n\n<p>The Kedro <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/03_tutorial\/02_tutorial_template.html\" rel=\"nofollow noreferrer\"><code>spaceflights<\/code><\/a> tutorial in the documentation actually saves the trained linear regression model using the <code>pickle<\/code> dataset if you want to see an example of it. The relevant section is <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/03_tutorial\/04_create_pipelines.html#working-with-multiple-pipelines\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1589225919163,
        "Answer_last_edit_time":1589277661210,
        "Answer_score":3.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61737613",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":66009324,
        "Question_title":"How to load a specific catalog dataset instance in kedro 0.17.0?",
        "Question_body":"<p>We were using kedro version 0.15.8 and we were loading one specific item from the catalog this way:<\/p>\n<pre><code>from kedro.context import load_context\nget_context().catalog.datasets.__dict__[key]\n<\/code><\/pre>\n<p>Now, we are changing to kedro 0.17.0 and trying to load the catalogs datasets the same way(using the framework context):<\/p>\n<pre><code>from kedro.framework.context import load_context\nget_context().catalog.datasets.__dict__[key]\n<\/code><\/pre>\n<p>And now we get the error:<\/p>\n<blockquote>\n<p>kedro.framework.context.context.KedroContextError: Expected an instance of <code>ConfigLoader<\/code>, got <code>NoneType<\/code> instead.<\/p>\n<\/blockquote>\n<p>It's because the hook register_config_loader from the project it's not being used by the hook_manager that calls the function.<\/p>\n<p>The project hooks are the defined the following way:<\/p>\n<pre><code>class ProjectHooks:\n\n    @hook_impl\n\n    def register_pipelines(self) -&gt; Dict[str, Pipeline]:\n\n        &quot;&quot;&quot;Register the project's pipeline.\n\n        Returns:\n\n            A mapping from a pipeline name to a ``Pipeline`` object.\n\n        &quot;&quot;&quot;\n\n        pm = pre_master.create_pipeline()\n\n        return {\n\n            &quot;pre_master&quot;: pm,\n\n            &quot;__default__&quot;: pm\n\n        }\n\n    @hook_impl\n\n    def register_config_loader(self, conf_paths: Iterable[str]) -&gt; ConfigLoader:\n\n        return ConfigLoader(conf_paths)\n\n    @hook_impl\n\n    def register_catalog(\n\n        self,\n\n        catalog: Optional[Dict[str, Dict[str, Any]]],\n\n        credentials: Dict[str, Dict[str, Any]],\n\n        load_versions: Dict[str, str],\n\n        save_version: str,\n\n        journal: Journal,\n\n    ) -&gt; DataCatalog:\n\n        return DataCatalog.from_config(\n\n            catalog, credentials, load_versions, save_version, journal\n\n        )\n\nproject_hooks = ProjectHooks()\n<\/code><\/pre>\n<p>And the settings are called the following way:\n&quot;&quot;&quot;Project settings.&quot;&quot;&quot;<\/p>\n<pre><code>from price_based_trading.hooks import ProjectHooks\n\n\nHOOKS = (ProjectHooks(),)\n<\/code><\/pre>\n<p>How can we configure that in a way that the hooks are used calling the method load_context(_working_dir).catalog.datasets ?<\/p>\n<p>I posted the same question in the kedro community: <a href=\"https:\/\/discourse.kedro.community\/t\/how-to-load-a-specific-catalog-item-in-kedro-0-17-0\/310\" rel=\"nofollow noreferrer\">https:\/\/discourse.kedro.community\/t\/how-to-load-a-specific-catalog-item-in-kedro-0-17-0\/310<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1612267210303,
        "Question_last_edit_time":1612461225273,
        "Question_score":4,
        "Question_tags":"hook|kedro",
        "Question_view_count":1298,
        "Owner_creation_time":1462800865783,
        "Owner_last_access_time":1663944240730,
        "Owner_location":null,
        "Owner_reputation":316,
        "Owner_up_votes":15,
        "Owner_down_votes":1,
        "Owner_views":18,
        "Answer_body":"<p>It was a silly mistake because I was not creating the Kedro session. To load an item of the catalog it can be done with the following code:<\/p>\n<pre><code>from kedro.framework.session import get_current_session\nfrom kedro.framework.session import KedroSession\n\nKedroSession.create(&quot;name_of_proyect&quot;) as session:\n    key = &quot;item_of_catalog&quot;\n    session = get_current_session()\n    context = session.load_context()\n    kedro_connector = context.catalog.datasets.__dict__[key] \n    \/\/ or kedro_connector = context.catalog._get_datasets(key)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1612452830193,
        "Answer_last_edit_time":1616756019583,
        "Answer_score":2.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66009324",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":70005957,
        "Question_title":"Logging the git_sha as a parameter on Mlflow using Kedro hooks",
        "Question_body":"<p>I would like to log the git_sha parameter on Mlflow as shown in the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/02_hooks.html?highlight=run_params#add-metrics-tracking-to-your-model\" rel=\"nofollow noreferrer\">documentation<\/a>. What appears to me here, is that simply running the following portion of code should be enough to get git_sha logged in the Mlflow UI. Am I right ?<\/p>\n<pre><code>@hook_impl\n    def before_pipeline_run(self, run_params: Dict[str, Any]) -&gt; None:\n        &quot;&quot;&quot;Hook implementation to start an MLflow run\n        with the same run_id as the Kedro pipeline run.\n        &quot;&quot;&quot;\n        mlflow.start_run(run_name=run_params[&quot;run_id&quot;])\n        mlflow.log_params(run_params)\n<\/code><\/pre>\n<p>But this does not work as I get all but the git_sha parameter. And when I look at the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/_modules\/kedro\/framework\/hooks\/specs.html?highlight=run_params#\" rel=\"nofollow noreferrer\">hooks specs<\/a>, it seems that this param is not part of run_params (anymore?)<\/p>\n<p>Is there a way I could get the git sha (maybe from the context journal ?) and add it to the logged parameters ?<\/p>\n<p>Thank you in advance !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637158421443,
        "Question_last_edit_time":1637158766987,
        "Question_score":0,
        "Question_tags":"python|mlflow|kedro|mlops",
        "Question_view_count":172,
        "Owner_creation_time":1586517832390,
        "Owner_last_access_time":1660328393330,
        "Owner_location":null,
        "Owner_reputation":127,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Answer_body":"<p>Whilst it's heavily encouraged to use git with Kedro it's not required and as such no part of Kedro (except <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro-starters\" rel=\"nofollow noreferrer\">kedro-starters<\/a> if we're being pedantic) is 'aware' of git.<\/p>\n<p>In your <code>before_pipeline_hook<\/code> there it is pretty easy for you to retrieve the info <a href=\"https:\/\/stackoverflow.com\/questions\/14989858\/get-the-current-git-hash-in-a-python-script\">via the techniques documented here<\/a>. It seems trivial for the whole codebase, a bit more involved if you want to say provide pipeline specific hashes.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1637159193837,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70005957",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":70610418,
        "Question_title":"Why doesn't my Kedro starter prompt for input?",
        "Question_body":"<p>I would like to create my own Kedro starter. I have tried to replicate the relevant portions of the pandas iris starter. I have a <code>cookiecutter.json<\/code> file with what I believe are appropriate mappings, and I have changed the repo and package directory names as well as any references to Kedro version such that they work with cookie cutter.<\/p>\n<p>I am able to generate a new project from my starter with <code>kedro new --starter=path\/to\/my\/starter<\/code>. <strong>However, the newly created project uses the default values for the project, package, and repo names, without prompting me for any input in the terminal<\/strong>.<\/p>\n<p>Have I misconfigured something? How can I create a starter that will prompt users to override the defaults when creating new projects?<\/p>\n<p>Here are the contents of <code>cookiecutter.json<\/code> in the top directory of my starter project:<\/p>\n<pre><code>{\n    &quot;project_name&quot;: &quot;default&quot;,\n    &quot;repo_name&quot;: &quot;{{ cookiecutter.project_name }}&quot;,\n    &quot;python_package&quot;: &quot;{{ cookiecutter.repo_name }}&quot;,\n    &quot;kedro_version&quot;: &quot;{{ cookiecutter.kedro_version }}&quot;\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641486876077,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_tags":"python|reusability|kedro",
        "Question_view_count":79,
        "Owner_creation_time":1416091573813,
        "Owner_last_access_time":1659708520190,
        "Owner_location":"Texas, USA",
        "Owner_reputation":197,
        "Owner_up_votes":36,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Answer_body":"<p>I think you may be missing <code>prompts.yml<\/code>\n<a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/blob\/main\/kedro\/templates\/project\/prompts.yml\" rel=\"nofollow noreferrer\">https:\/\/github.com\/quantumblacklabs\/kedro\/blob\/main\/kedro\/templates\/project\/prompts.yml<\/a><\/p>\n<p>Full instructions can be found here:\n<a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/05_create_kedro_starters.html\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/05_create_kedro_starters.html<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1641488156140,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_favorite_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70610418",
        "Question_exclusive_tag":"Kedro"
    },
    {
        "Question_id":52847777,
        "Question_title":"Customer Error: Additional hyperparameters are not allowed - Image classification training- Sagemaker",
        "Question_body":"<p>I'm learning image classification with Amazon SageMaker. I was trying to follow their  learning demo <strong>Image classification transfer learning demo<\/strong> (<code>Image-classification-transfer-learning-highlevel.ipynb<\/code>)<\/p>\n\n<p>I got up to Start the training. Executed below.<\/p>\n\n<pre><code>ic.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n\n<p>Set the hyper parameters as given in the demo<\/p>\n\n<pre><code>ic.set_hyperparameters(num_layers=18,\n                             use_pretrained_model=1,\n                             image_shape = \"3,224,224\",\n                             num_classes=257,\n                             num_training_samples=15420,\n                             mini_batch_size=128,\n                             epochs=1,\n                             learning_rate=0.01,\n                             precission_dtype='float32')\n<\/code><\/pre>\n\n<p>Got the client error<\/p>\n\n<pre><code>ERROR 140291262150464] Customer Error: Additional hyperparameters are not allowed (u'precission_dtype' was unexpected) (caused by ValidationError)\n\nCaused by: Additional properties are not allowed (u'precission_dtype' was unexpected)\n<\/code><\/pre>\n\n<p>Does anyone know how to overcome this? I'm also reporting this to aws support. Posting here for sharing and get a fix. Thanks !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1539753916957,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|jupyter-notebook|classification|amazon-sagemaker",
        "Question_view_count":146,
        "Owner_creation_time":1403185902747,
        "Owner_last_access_time":1547107848667,
        "Owner_location":"Colombo, Sri Lanka",
        "Owner_reputation":169,
        "Owner_up_votes":215,
        "Owner_down_votes":0,
        "Owner_views":81,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Assuming you are referring to <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb<\/a> - you have a typo, it's <code>precision_dtype<\/code>, not <code>precission_dtype<\/code>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1539831993640,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52847777",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71827884,
        "Question_title":"create sagemaker notebook instance via Terraform",
        "Question_body":"<p>I am taking my first steps into the Terraform world so please be gentle with me. I have a user with AmazonSageMakerFullAccess, which I stored via AWS CLI in a profile called terraform. I can create an S3 bucket as follows no problem referring this user in Windows in VSC:<\/p>\n<pre><code>provider &quot;aws&quot; {\n    region = &quot;eu-west-2&quot;\n    shared_credentials_files = [&quot;C:\\\\Users\\\\amazinguser\\\\.aws\\\\credentials&quot;]\n    profile = &quot;terraform&quot;\n}\n\nresource &quot;aws_s3_bucket&quot; &quot;b&quot; {\n  bucket = &quot;blabla-test-bucket&quot;\n\n  tags = {\n    Name        = &quot;amazing_tag&quot;\n    Environment = &quot;dev&quot;\n  }\n}\n<\/code><\/pre>\n<p>I try to implement <a href=\"https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\/tree\/main\/Part%202a%20-%20Creating%20a%20SageMaker%20Notebook\" rel=\"nofollow noreferrer\">this<\/a> documented <a href=\"https:\/\/towardsdatascience.com\/terraform-sagemaker-part-2a-creating-a-custom-sagemaker-notebook-instance-1d68c90b192b\" rel=\"nofollow noreferrer\">here<\/a> and try to this:<\/p>\n<pre><code>resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot; {\n  name = &quot;titanic-sagemaker-byoc-notebook&quot;\n  role_arn = aws_iam_role.notebook_iam_role.arn\n  instance_type = &quot;ml.t2.medium&quot;\n  #lifecycle_config_name = aws_sagemaker_notebook_instance_lifecycle_configuration.notebook_config.name\n  #default_code_repository = aws_sagemaker_code_repository.git_repo.code_repository_name\n}\n<\/code><\/pre>\n<p>I am a bit confused about the role_arn which is defined here:<\/p>\n<p><a href=\"https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\/blob\/main\/Part%202a%20-%20Creating%20a%20SageMaker%20Notebook\/terraform\/iam.tf\" rel=\"nofollow noreferrer\">https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\/blob\/main\/Part%202a%20-%20Creating%20a%20SageMaker%20Notebook\/terraform\/iam.tf<\/a><\/p>\n<p>Can I not use the above user? Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_time":1649680247017,
        "Question_score":0,
        "Question_tags":"amazon-web-services|terraform|amazon-sagemaker",
        "Question_view_count":338,
        "Owner_creation_time":1267440784443,
        "Owner_last_access_time":1664045779313,
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Question_last_edit_time":null,
        "Answer_body":"<p>AWS services trying to call other AWS services and perform actions are not allowed to do so by default. For example, SageMaker Notebooks are basically EC2 instances. In order for SageMaker to create EC2 instances, it has to have a policy which allows e.g., injecting ENIs to a VPC. Since you probably do not want to do all that by yourself (it is a managed Notebook service after all), you have to give SageMaker permissions to perform actions on your behalf. Enter <strong>execution roles<\/strong>. For SageMaker, you can read more in [1]. Other services that you will commonly find using execution roles are Lambda, ECS and many others. An IAM role usually consists of two parts:<\/p>\n<ol>\n<li>Trust relationship (I like to call it trust policy)<\/li>\n<li>Permissions policy<\/li>\n<\/ol>\n<p>The first one decides which principal (AWS identifier, Service etc. [2]) will be able to assume the role. In your example, that is:<\/p>\n<pre><code>data &quot;aws_iam_policy_document&quot; &quot;sm_assume_role_policy&quot; {\n  statement {\n    actions = [&quot;sts:AssumeRole&quot;]\n    \n    principals {\n      type = &quot;Service&quot;\n      identifiers = [&quot;sagemaker.amazonaws.com&quot;]\n    }\n  }\n}\n<\/code><\/pre>\n<p>What this policy says is &quot;I am going to allow SageMaker (which is of type <code>Service<\/code>) to assume any role to which this policy is attached and perform actions that are defined in the permissions policy&quot;. The permissions policy is:<\/p>\n<pre><code># Attaching the AWS default policy, &quot;AmazonSageMakerFullAccess&quot;\nresource &quot;aws_iam_policy_attachment&quot; &quot;sm_full_access_attach&quot; {\n  name = &quot;sm-full-access-attachment&quot;\n  roles = [aws_iam_role.notebook_iam_role.name]\n  policy_arn = &quot;arn:aws:iam::aws:policy\/AmazonSageMakerFullAccess&quot;\n}\n<\/code><\/pre>\n<p>Without going into too much details about what the AWS managed policy for SageMaker does, it is enough to see the <code>FullAccess<\/code> part for it to be clear. What you could do if you want to be extra careful is to define a customer managed policy [3] for SageMaker notebooks. This permissions policy will be attached to the IAM role(s) defined in the <code>roles<\/code> argument. Note that it is a list, so multiple roles can have the same permissions policy attached.<\/p>\n<p>Last, but not the least, the glue between the trust and permissions policy is the role itself:<\/p>\n<pre><code>resource &quot;aws_iam_role&quot; &quot;notebook_iam_role&quot; {\n  name = &quot;sm_notebook_role&quot;\n  assume_role_policy = data.aws_iam_policy_document.sm_assume_role_policy.json\n}\n<\/code><\/pre>\n<p>As you can see, the <code>assume_role_policy<\/code> is the policy which will allow SageMaker to perform actions in the AWS account based on the permissions defined in the permissions policy.<\/p>\n<p>This topic is much more complex than in this answer, but it should give you a fair amount of information.<\/p>\n<p>NOTE: In theory, the same role accessing information in AWS and running the AWS API actions when using Terraform could be used for SageMaker, but I would strongly advise against it. Always keep in mind separation of concerns and principle of least privilege.<\/p>\n<hr \/>\n<p>[1] <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html#sagemaker-roles-create-execution-role\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html#sagemaker-roles-create-execution-role<\/a><\/p>\n<p>[2] <a href=\"https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/reference_policies_elements_principal.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/reference_policies_elements_principal.html<\/a><\/p>\n<p>[3] <a href=\"https:\/\/docs.aws.amazon.com\/acm\/latest\/userguide\/authen-custmanagedpolicies.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/acm\/latest\/userguide\/authen-custmanagedpolicies.html<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1649687933130,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71827884",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59091944,
        "Question_title":"Create a Multi Model Endpoint using AWS Sagemaker Boto",
        "Question_body":"<p>I have created 2 models which are not too complex and renamed them and placed them into a same location in S3 bucket.<\/p>\n\n<p>I need to create a multi model endpoint such that the 2 models have a same end point. \nThe model i am using is AWS in built Linear-learner model type regressor. <\/p>\n\n<p>I am stuck as to how they should be deployed. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1574954456480,
        "Question_score":1,
        "Question_tags":"amazon-web-services|linear-regression|endpoint|amazon-sagemaker|multi-model-forms",
        "Question_view_count":364,
        "Owner_creation_time":1574954057270,
        "Owner_last_access_time":1645458529123,
        "Owner_location":"Carlow, Ireland",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>SageMaker's Linear Learner algorithm container does not currently implement the requirements for <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">multi-model endpoints<\/a>. You could request support in the <a href=\"https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285&amp;start=0\" rel=\"nofollow noreferrer\">AWS Forums<\/a>.<\/p>\n\n<p>You could also build your own version of the Linear Learner algorithm. To deploy the models to a multi-model endpoint you would need to build your own container that meets the requirements for multi-model endpoints and implement your own version of the Linear Learner algorithm. This sample notebook gives an example of how you would create your multi-model compatible container that serves MxNet models, but you could adapt it to implement a Linear Learner algorithm:<\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1582590253857,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1582591338969,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59091944",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53636589,
        "Question_title":"AWS SageMaker: CapacityError: Unable to provision requested ML compute capacity.",
        "Question_body":"<p>We were running two TrainingJob instances of type (1) <em>ml.p3.8xlarge<\/em> and (2) <em>ml.p3.2xlarge<\/em> . <\/p>\n\n<p>Each training job is running a custom algorithm with Tensorflow plus a Keras backend.<\/p>\n\n<p>The instance (1) is running ok, while the instance (2) after a reported time of training of 1 hour, with any logging in CloudWatch (any text tow log), exits with this error:<\/p>\n\n<pre><code>Failure reason\nCapacityError: Unable to provision requested ML compute capacity. Please retry using a different ML instance type.\n<\/code><\/pre>\n\n<p>I'm not sure what this message mean.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1544027022437,
        "Question_score":3,
        "Question_tags":"tensorflow|keras|amazon-sagemaker",
        "Question_view_count":3875,
        "Owner_creation_time":1305708350447,
        "Owner_last_access_time":1663804694443,
        "Owner_location":"Bologna, Italy",
        "Owner_reputation":14823,
        "Owner_up_votes":4638,
        "Owner_down_votes":85,
        "Owner_views":1847,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This message mean SageMaker tried to launch the instance but EC2 was not having enough capacity of this instance hence after waiting for some time(in this case 1 hour) SageMaker gave up and failed the training job.<\/p>\n\n<p>For more information about capacity issue from ec2, please visit: \n<a href=\"https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/troubleshooting-launch.html#troubleshooting-launch-capacity\" rel=\"noreferrer\">troubleshooting-launch-capacity<\/a><\/p>\n\n<p>To solve this, you can either try running jobs with different instance type as suggested in failure reason or wait a few minutes and then submit your request again as suggested by EC2.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1544569877893,
        "Answer_score":5,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1544571385672,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53636589",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66817781,
        "Question_title":"What are the differences between AWS sagemaker and sagemaker_pyspark?",
        "Question_body":"<p>I'm currently running a quick Machine Learning proof of concept on AWS with SageMaker, and I've come across two libraries: <code>sagemaker<\/code> and <code>sagemaker_pyspark<\/code>. I would like to work with distributed data. My questions are:<\/p>\n<ol>\n<li><p>Is using <code>sagemaker<\/code> the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented <code>sagemaker_pyspark<\/code>? Based on this assumption, I do not understand what it would offer regarding using <code>scikit-learn<\/code> on a SageMaker notebook (in terms of computing capabilities).<\/p>\n<\/li>\n<li><p>Is it normal for something like <code>model = xgboost_estimator.fit(training_data)<\/code> to take 4 minutes to run with <code>sagemaker_pyspark<\/code> for a small set of test data? I see that what it does below is to train the model and also create an Endpoint to be able to offer its predictive services, and I assume that this endpoint is deployed on an EC2 instance that is created and started at the moment. Correct me if I'm wrong. I assume this from how the estimator is defined:<\/p>\n<\/li>\n<\/ol>\n<pre><code>from sagemaker import get_execution_role\nfrom sagemaker_pyspark.algorithms import XGBoostSageMakerEstimator\n\n\nxgboost_estimator = XGBoostSageMakerEstimator (\n    trainingInstanceType = &quot;ml.m4.xlarge&quot;,\n    trainingInstanceCount = 1,\n    endpointInstanceType = &quot;ml.m4.xlarge&quot;,\n    endpointInitialInstanceCount = 1,\n    sagemakerRole = IAMRole(get_execution_role())\n)\n\nxgboost_estimator.setNumRound(1)\n<\/code><\/pre>\n<p>If so, is there a way to reuse the same endpoint with different training jobs so that I don't have to wait for a new endpoint to be created each time?<\/p>\n<ol start=\"3\">\n<li><p>Does <code>sagemaker_pyspark<\/code> support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/p>\n<\/li>\n<li><p>Do you know if <code>sagemaker_pyspark<\/code> can perform hyperparameter optimization? From what I see, <code>sagemaker<\/code> offers the <code>HyperparameterTuner<\/code> class, but I can't find anything like it in <code>sagemaker_pyspark<\/code>. I suppose it is a more recent library and there is still a lot of functionality to implement.<\/p>\n<\/li>\n<li><p>I am a bit confused about the concept of <code>entry_point<\/code> and <code>container<\/code>\/<code>image_name<\/code> (both possible input arguments for the <code>Estimator<\/code> object from the <code>sagemaker<\/code> library): can you deploy models with and without containers? why would you use model containers? Do you always need to define the model externally with the <code>entry_point<\/code> script? It is also confusing that the class <code>AlgorithmEstimator<\/code> allows the input argument <code>algorithm_arn<\/code>; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/li>\n<li><p>I see the <code>sagemaker<\/code> library offers SageMaker Pipelines, which seem to be very handy for deploying properly structured ML workflows. However, I don't think this is available with <code>sagemaker_pyspark<\/code>, so in that case, I would rather create my workflows with a combination of Step Functions (to orchestrate the entire thing), Glue processes (for ETL, preprocessing and feature\/target engineering) and SageMaker processes using <code>sagemaker_pyspark<\/code>.<\/p>\n<\/li>\n<li><p>I also found out that <code>sagemaker<\/code> has the <code>sagemaker.sparkml.model.SparkMLModel<\/code> object. What is the difference between this and what <code>sagemaker_pyspark<\/code> offers?<\/p>\n<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616764992750,
        "Question_score":1,
        "Question_tags":"amazon-web-services|pyspark|aws-glue|amazon-sagemaker|aws-step-functions",
        "Question_view_count":242,
        "Owner_creation_time":1376052312530,
        "Owner_last_access_time":1619635368283,
        "Owner_location":"Australia",
        "Owner_reputation":135,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":75,
        "Question_last_edit_time":1618398204670,
        "Answer_body":"<p><code>sagemaker<\/code> is the SageMaker Python SDK. It calls SageMaker-related AWS service APIs on your behalf. You don't need to use it, but it can make life easier<\/p>\n<blockquote>\n<ol>\n<li>Is using sagemaker the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented sagemaker_pyspark?<\/li>\n<\/ol>\n<\/blockquote>\n<p>No. You can run distributed training jobs using <code>sagemaker<\/code> (see <code>instance_count<\/code> parameter)<\/p>\n<p><code>sagemaker_pyspark<\/code> facilitates calling SageMaker-related AWS service APIs from Spark. Use it if you want to use SageMaker services from Spark<\/p>\n<blockquote>\n<ol start=\"2\">\n<li>Is it normal for something like model = xgboost_estimator.fit(training_data) to take 4 minutes to run with sagemaker_pyspark for a small set of test data?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, it takes a few minutes for an EC2 instance to spin-up. Use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">Local Mode<\/a> if you want to iterate more quickly locally. Note: Local Mode won't work with SageMaker built-in algorithms, but you can prototype with (non AWS) XGBoost\/SciKit-Learn<\/p>\n<blockquote>\n<ol start=\"3\">\n<li>Does sagemaker_pyspark support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, but you'd probably want to extend <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/api.html#sagemakerestimator\" rel=\"nofollow noreferrer\">SageMakerEstimator<\/a>. Here you can provide the <code>trainingImage<\/code> URI<\/p>\n<blockquote>\n<ol start=\"4\">\n<li>Do you know if sagemaker_pyspark can perform hyperparameter optimization?<\/li>\n<\/ol>\n<\/blockquote>\n<p>It does not appear so. It'd probably be easier just to do this from SageMaker itself though<\/p>\n<blockquote>\n<p>can you deploy models with and without containers?<\/p>\n<\/blockquote>\n<p>You can certainly host your own models any way you want. But if you want to use SageMaker model inference hosting, then containers are required<\/p>\n<blockquote>\n<p>why would you use model containers?<\/p>\n<\/blockquote>\n<blockquote>\n<p>Do you always need to define the model externally with the entry_point script?<\/p>\n<\/blockquote>\n<p>The whole Docker thing makes bundling dependencies easier, and also makes things language\/runtime-neutral. SageMaker doesn't care if your algorithm is in Python or Java or Fortran. But it needs to know how to &quot;run&quot; it, so you tell it a working directory and a command to run. This is the entry point<\/p>\n<blockquote>\n<p>It is also confusing that the class AlgorithmEstimator allows the input argument algorithm_arn; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/blockquote>\n<p>Please clarify which &quot;three&quot; you are referring to<\/p>\n<p>6 is not a question, so no answer required :)<\/p>\n<blockquote>\n<ol start=\"7\">\n<li>What is the difference between this and what sagemaker_pyspark offers?<\/li>\n<\/ol>\n<\/blockquote>\n<p>sagemaker_pyspark lets you call SageMaker services from Spark, whereas SparkML Serving lets you use Spark ML services from SageMaker<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1617119678127,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1634230982412,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66817781",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56134165,
        "Question_title":"Render hyperlink in pandas df in jupyterlab",
        "Question_body":"<p>I am trying to render a url inside a pandas dataframe output.  I followed along with some of the other examples out there, here is my implementation:<\/p>\n\n<pre><code>def create_url(product_name):\n    search = 'http:\/\/www.example.com\/search'\n    url = 'http:\/\/www.example.com\/search\/'+product_name\n    return url\n\ndef make_clickable(url):\n    return '&lt;a target=\"_blank\" href=\"{}\"&gt;{}&lt;\/a&gt;'.format(url, url)\n\n...\n\ndf['url'] = df['product_name'].apply(format_url)\ndf.style.format({'url': make_clickable})\n<\/code><\/pre>\n\n<p>This produces a correctly formatted raw text hyperlink, however its not clickable within the output.<\/p>\n\n<p>I should add that I'm doing this in an AWS sagemaker jupyterlab notebook which potentially disables hyperlinking in the output.  Not sure how I would check that though.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1557848494950,
        "Question_score":1,
        "Question_tags":"pandas|formatting|amazon-sagemaker",
        "Question_view_count":241,
        "Owner_creation_time":1376684625670,
        "Owner_last_access_time":1663967616407,
        "Owner_location":null,
        "Owner_reputation":1105,
        "Owner_up_votes":658,
        "Owner_down_votes":2,
        "Owner_views":222,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If this doesn't work, I'm guessing it's an AWS thing<\/p>\n\n<ol>\n<li><code>IPython.display.HTML<\/code><\/li>\n<li><code>pandas.DataFrame.to_html<\/code> with <code>escape=False<\/code><\/li>\n<li><code>pandas.set_option('display.max_colwidth', 2000)<\/code> Must be a large number to accommodate length of link tag.  I'll say that I think this is broken.  It shouldn't be necessary to set <code>'display.max_colwidth'<\/code> in order to make sure <code>to_html<\/code> outputs properly.  But it is :-\/<\/li>\n<\/ol>\n\n<hr>\n\n<pre><code>from IPython import display\n\npd.set_option('display.max_colwidth', 2000)\n\ndisplay.HTML(df.assign(url=[*map(make_clickable, df.url)]).to_html(escape=False))\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YcVj6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YcVj6.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1557849422180,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1557931098116,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56134165",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66086605,
        "Question_title":"Use AWS ML model Random Cut Forest locally",
        "Question_body":"<p>I wonder if it is possible to deploy Random Cut Forest (RCF) built-in algorithm of SageMaker to the local mode. I haven't come across any sample implementation about it. If not, can we simply say that models trained using RCF are limited to be consumed inside the platform via Inference Endpoints?<\/p>\n<p>I got this error when I tried to do so.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/YFlRf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YFlRf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ywlQU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ywlQU.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1612692155937,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|rcf",
        "Question_view_count":195,
        "Owner_creation_time":1594550494350,
        "Owner_last_access_time":1650824104303,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":"<p>indeed you're right, <strong>SageMaker Random Cut Forest cannot be trained and deployed locally. The 18 Amazon SageMaker Built-in algorithms are designed to be trained and deployed on Amazon SageMaker.<\/strong> There are 2 exceptions: SageMaker BlazingText and SageMaker XGBoost, which can be read with their open-source counterparts (fastText and XGBoost) and used for  inference out of SageMaker (eg EC2, Lambda, on-prem or on your laptop - as long as you can install those libraries)<\/p>\n<p>There is an open-source attempt to implement the Random Cut Forest here <a href=\"https:\/\/github.com\/kLabUM\/rrcf\" rel=\"nofollow noreferrer\">https:\/\/github.com\/kLabUM\/rrcf<\/a> ; I don't think it has any connection to SageMaker RCF codebase so results, speed and scalability may differ.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1612734092367,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1612737023060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66086605",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72914046,
        "Question_title":"Sagemaker Project successfully creates but there are no linked pipelines or repositories",
        "Question_body":"<p>I created a sagemaker project with a terraform template which successfully created with a stack successfully created and associated with it. However, there is no repository associated or pipeline associated with the sagemaker project despite there being both in the cloudformation template I used. Can someone help with this?<\/p>\n<p>Is there a way to manually link a sagemaker project with a code commit repository? I see that succesfully linked repositories have the tag: <code>sagemaker:project-name<\/code> with the correct project name.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1657295938437,
        "Question_score":0,
        "Question_tags":"amazon-web-services|terraform|amazon-cloudformation|amazon-sagemaker",
        "Question_view_count":124,
        "Owner_creation_time":1653511725307,
        "Owner_last_access_time":1663251784530,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":1657311059267,
        "Answer_body":"<p>Using a different cloudformation template fixed the issue. Not sure why.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1657906025030,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72914046",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57946451,
        "Question_title":"Incremental learning with a built-in sagemaker algorithm",
        "Question_body":"<p>I am training the DeepAR AWS SageMaker built-in algorithm. With the sagemaker SDK, I can train the model with particular specified hyper-parameters:<\/p>\n\n<pre><code>estimator = sagemaker.estimator.Estimator(\n    sagemaker_session=sagemaker_session,\n    image_name=image_name,\n    role=role,\n    train_instance_count=1,\n    train_instance_type='ml.c4.2xlarge',\n    base_job_name='wfp-deepar',\n    output_path=join(s3_path, 'output')\n)\n\nestimator.set_hyperparameters(**{\n    'time_freq': 'M',\n    'epochs': '50',\n    'mini_batch_size': '96',\n    'learning_rate': '1E-3',\n    'context_length': '12',\n    'dropout_rate': 0,\n    'prediction_length': '12'\n})\n\nestimator.fit(inputs=data_channels, wait=True, job_name='wfp-deepar-job-level-5')\n<\/code><\/pre>\n\n<p>I would like to train the resulting model again with a <strong>smaller learning rate<\/strong>. I followed the incremental training method described here: <a href=\"https:\/\/docs.aws.amazon.com\/en_pv\/sagemaker\/latest\/dg\/incremental-training.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/en_pv\/sagemaker\/latest\/dg\/incremental-training.html<\/a>, but it does not work, apparently (according to the link), only two built-in models support incremental learning. <\/p>\n\n<p>Has anyone found a workaround for this so that they can train a built-in algorithm with a scheduled learning rate?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1568567773507,
        "Question_score":1,
        "Question_tags":"amazon-web-services|machine-learning|deep-learning|aws-sdk|amazon-sagemaker",
        "Question_view_count":225,
        "Owner_creation_time":1445974115333,
        "Owner_last_access_time":1664071374770,
        "Owner_location":"Washington, United States",
        "Owner_reputation":5939,
        "Owner_up_votes":1063,
        "Owner_down_votes":11,
        "Owner_views":324,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Unfortunately, the SageMaker built-in DeepAR model doesn't support learning rate scheduling nor incremental learning.  If you want to implement learning rate plateau schedule on a DeepAR architecture I recommend to consider:<\/p>\n\n<ul>\n<li>using the open-source DeepAR implementation (<a href=\"https:\/\/gluon-ts.mxnet.io\/api\/gluonts\/gluonts.model.deepar.html\" rel=\"nofollow noreferrer\">code<\/a>, <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-neural-time-series-models-with-gluon-time-series\/\" rel=\"nofollow noreferrer\">demo<\/a>)<\/li>\n<li>or using the <a href=\"https:\/\/docs.aws.amazon.com\/forecast\/latest\/dg\/aws-forecast-recipe-deeparplus.html\" rel=\"nofollow noreferrer\">DeepAR+ algo of the Amazon Forecast service<\/a>, that features learning rate scheduling ability.<\/li>\n<\/ul>",
        "Answer_comment_count":0,
        "Answer_creation_time":1568630636313,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57946451",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55987935,
        "Question_title":"How do I pull the pre-built docker images for SageMaker?",
        "Question_body":"<p>I'm trying to pull the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pre-built-docker-containers-frameworks.html\" rel=\"noreferrer\">pre-built docker images<\/a> for SageMaker. I am able to successfully <code>docker login<\/code> to ECR (my AWS credentials). When I try to pull the image I get the standard <code>no basic auth credentials<\/code>.<\/p>\n\n<p>Maybe I'm misunderstanding... I assumed those ECR URLs were public.<\/p>\n\n<pre><code>$(aws ecr get-login --region us-west-2 --no-include-email)\n\ndocker pull 246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-scikit-learn\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1557016435310,
        "Question_score":5,
        "Question_tags":"amazon-web-services|docker|amazon-sagemaker|amazon-ecr",
        "Question_view_count":2738,
        "Owner_creation_time":1343237570417,
        "Owner_last_access_time":1661366012027,
        "Owner_location":"California",
        "Owner_reputation":1325,
        "Owner_up_votes":303,
        "Owner_down_votes":10,
        "Owner_views":131,
        "Question_last_edit_time":1557175542127,
        "Answer_body":"<p>Could you show your ECR login command and pull command in the question?<\/p>\n\n<p>For SageMaker pre-built image 520713654638.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.3.0-cpu-py3<\/p>\n\n<p>What I do is:<\/p>\n\n<ol>\n<li>Log in ECR<\/li>\n<\/ol>\n\n<p><code>$(aws ecr get-login --no-include-email --registry-ids 520713654638 --region us-west-2)<\/code><\/p>\n\n<ol start=\"2\">\n<li>Pull the image<\/li>\n<\/ol>\n\n<p><code>docker pull 520713654638.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.3.0-cpu-py3<\/code><\/p>\n\n<p>These images are public readable so you can pull them from any AWS account. I guess the reason you failed is that you did not specify --registry-ids in your login. But it's better if you can provide your scripts for others to identify what's wrong.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1557174090813,
        "Answer_score":3,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55987935",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72448994,
        "Question_title":"Train Amazon SageMaker object detection model on local PC",
        "Question_body":"<p>I wonder if it's possible to run training Amazon SageMaker object detection model on a local PC?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1654004815903,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|training-data|custom-training",
        "Question_view_count":70,
        "Owner_creation_time":1442786553537,
        "Owner_last_access_time":1664039462827,
        "Owner_location":"Kyiv",
        "Owner_reputation":71,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You're probably referring to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection.html\" rel=\"nofollow noreferrer\">this<\/a> object detection algorithm which is part of of <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">Amazon SageMaker built-in algorithms<\/a>. <strong>Built-in algorithms must be trained on the cloud<\/strong>.<br \/>\nIf you're bringing your own Tensorflow or PyTorch model, you could use SageMaker training jobs to train either on the cloud or locally as @kirit noted.<\/p>\n<p>I would also look at <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-jumpstart.html\" rel=\"nofollow noreferrer\">SageMaker JumpStart<\/a> for a wide variety of object detection algorithm which are TF\/PT based.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1654073043173,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72448994",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63035151,
        "Question_title":"When calling a SageMaker deploy_endpoint function with an a1.small instance, I'm given an error that I can't open a m5.xlarge instance",
        "Question_body":"<p>So while executing through a notebook generated by Autopilot, I went to execute the final code cell:<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>pipeline_model.deploy(initial_instance_count=1,\n                      instance_type='a1.small',\n                      endpoint_name=pipeline_model.name,\n                      wait=True)\n<\/code><\/pre>\n<p>I get this error<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m5.2xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p>The most important part of that is the last line where it mentions resource limits.  I'm not trying to open the type of instance it's giving me an error about opening.<\/p>\n<p>Does the endpoint NEED to be on an ml.m5.2xlarge instance?  Or is the code acting up?<\/p>\n<p>Thanks in advance guys and gals.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1595423975777,
        "Question_score":2,
        "Question_tags":"python|boto3|amazon-sagemaker",
        "Question_view_count":91,
        "Owner_creation_time":1495758291317,
        "Owner_last_access_time":1650398234917,
        "Owner_location":"Temple University, West Berks Street, Philadelphia, PA, USA",
        "Owner_reputation":138,
        "Owner_up_votes":3,
        "Owner_down_votes":1,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You should use one of on-demand ML hosting instances supported as detailed at <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/\" rel=\"nofollow noreferrer\">this link<\/a>. I think non-valid <code>instance_type='a1.small'<\/code> is replaced by a valid one (ml.m5.2xlarge), and that is not in your AWS service quota. The weird part is that seeing <code>instance_type='a1.small'<\/code> was generated by SageMaker Autopilot.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1595539712143,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63035151",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61832086,
        "Question_title":"Having issues reading S3 bucket when transitioning a tensorflow model from local machine to AWS SageMaker",
        "Question_body":"<p>When testing on a local machine in Python I would normally use the following to read a training set with sub-directories of all the classes and files\/class:<\/p>\n\n<pre><code>train_path = r\"C:\\temp\\coins\\PCGS - Gold\\train\"\n\ntrain_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(100,100), classes=['0','1',2','3' etc...], batch_size=32)\n<\/code><\/pre>\n\n<p><strong>Found 4100 images belonging to 22 classes.<\/strong><\/p>\n\n<p>but on AWS SageMaker's Jupyter notebook I am now pulling the files from an S3 bucket.  I tried the following: <\/p>\n\n<pre><code>bucket = \"coinpath\"\n\ntrain_path = 's3:\/\/{}\/{}\/train'.format(bucket, \"v1\")   #note that the directory structure is coinpath\/v1\/train where coinpath is the bucket\n\ntrain_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(100,100), classes=\n['0','1',2','3' etc...], batch_size=32)\n<\/code><\/pre>\n\n<p>but I get: ** Found 0 images belonging to 22 classes.**<\/p>\n\n<p>Looking for some guidance on the right way to pull training data from S3.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1589605066783,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":608,
        "Owner_creation_time":1583123749267,
        "Owner_last_access_time":1663877641340,
        "Owner_location":"Washington D.C., DC, USA",
        "Owner_reputation":317,
        "Owner_up_votes":60,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":"<p>From <a href=\"https:\/\/stackoverflow.com\/questions\/54736505\/ideal-way-to-read-data-in-bucket-stored-batches-of-data-for-keras-ml-training-in\">Ideal way to read data in bucket stored batches of data for Keras ML training in Google Cloud Platform?<\/a> \"ImageDataGenerator.flow_from_directory() currently does not allow you to stream data directly from a GCS bucket. \"<\/p>\n\n<p>I had to download the image from S3 first.  This is best for latency reasons as well. <\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1589768612457,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61832086",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68083831,
        "Question_title":"AWS Sagemaker inference endpoint not utilizing all vCPUs",
        "Question_body":"<p>I have deployed a custom model on sagemaker inference endpoint (single instance) and while I was load testing, I have observed that CPU utilization metric is maxing out at 100% but according to <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/sagemaker-cpu-gpu-utilization-100\/\" rel=\"nofollow noreferrer\">this post<\/a> it should max out at #vCPU*100 %. I have confirmed that the inference endpoint is not using all cores in clowdwatch logs.<\/p>\n<p>So if one prediction call requires one second to be processed to give response, the deployed model is only able to handle one API call per second which could have been increased to 8 calls per second if all vCPUs would have been used.<\/p>\n<p>Are there any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency?<\/p>\n<p>Or could we use multiprocessing python package inside <code>inference.py<\/code> file while deploying such that each call comes to the default core and from there all calculations\/prediction is done in any other core whichever is empty at that instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1624366085347,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":661,
        "Owner_creation_time":1473938483227,
        "Owner_last_access_time":1664046020100,
        "Owner_location":"https:\/\/dzone.com\/articles\/machine-learning-provides-360-degree-view-of-the-c",
        "Owner_reputation":909,
        "Owner_up_votes":41,
        "Owner_down_votes":1,
        "Owner_views":81,
        "Question_last_edit_time":1628443120180,
        "Answer_body":"<p>UPDATE<\/p>\n<ul>\n<li><p>Set three environment variables<\/p>\n<ol>\n<li>ENABLE_MULTI_MODEL as &quot;true&quot; (make sure it is string and not bool) and set <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L74\" rel=\"nofollow noreferrer\">SAGEMAKER_HANDLER<\/a> as custom model handler python module path if custom service else dont define it. Also make sure model name <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L94\" rel=\"nofollow noreferrer\">model.mar<\/a>, before compressing it as tar ball and storing in s3<\/li>\n<li>TS_DEFAULT_WORKERS_PER_MODEL as number of vcpus<\/li>\n<li>First environment variable makes sure torch serve env_vars are enabled and second one uses first setting and loads requested number of workers<\/li>\n<li>Setting can be done by passing env dictionary argument to <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-an-estimator\" rel=\"nofollow noreferrer\">PyTorch function<\/a>. Below is explanation as to why it works<\/li>\n<\/ol>\n<\/li>\n<li><p>From the looks of it, sagemaker deployment for pytorch model as given in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-an-estimator\" rel=\"nofollow noreferrer\">Sagemaker SDK guide<\/a>, uses <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu\" rel=\"nofollow noreferrer\">this dockerfile<\/a>. In this docker, entrypoint is <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">torchserve-entrypoint.py<\/a> as in <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu#L124\" rel=\"nofollow noreferrer\">Dockerfile line#124<\/a>.<\/p>\n<\/li>\n<li><p>This <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">torchserve-entrypoint.py<\/a> calls <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">serving.main()<\/a> from <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/serving.py\" rel=\"nofollow noreferrer\">serving.py<\/a>. Which ends up calling <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/serving.py#L34\" rel=\"nofollow noreferrer\">torchserve.start_torchserve(handler_service=HANDLER_SERVICE)<\/a> from <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py\" rel=\"nofollow noreferrer\">torchserve.py<\/a>.<\/p>\n<\/li>\n<li><p><a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L34\" rel=\"nofollow noreferrer\">At line 34 in torchserve.py<\/a> it defines &quot;\/etc\/default-ts.properties&quot; as DEFAULT_TS_CONFIG_FILE. This file is located <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/etc\/default-ts.properties\" rel=\"nofollow noreferrer\">here<\/a>. In this file <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/etc\/default-ts.properties#L2\" rel=\"nofollow noreferrer\">enable_envvars_config=true<\/a> is set. It will use this file setting IFF Environment variable &quot;ENABLE_MULTI_MODEL&quot; is set to &quot;false&quot; as refered <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L167\" rel=\"nofollow noreferrer\">here<\/a>. If it is set to &quot;true&quot; then it will use \/etc\/mme-ts.properties<\/p>\n<\/li>\n<\/ul>\n<hr \/>\n<p>As for the question <code>Are there any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency?<\/code>\nThere are various settings you can use\nFor models you can set <code>default_workers_per_model<\/code> in config.properties <code>TS_DEFAULT_WORKERS_PER_MODEL=$(nproc --all)<\/code> in environment variables. Environment variables take top priority.<\/p>\n<p>Other than that for each model, you can set the number of workers by using management API, but sadly it is not possible to curl to management API in sagemaker. SO TS_DEFAULT_WORKERS_PER_MODEL is the best bet.\nSetting this should make sure all cores are used.<\/p>\n<p>But if you are using docker file then in entrypoint you can setup scripts which wait for model loading and curl to it to set number of workers<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code># load the model\ncurl -X POST localhost:8081\/models?url=model_1.mar&amp;batch_size=8&amp;max_batch_delay=50\n# after loading the model it is possible to set min_worker, etc\ncurl -v -X PUT http:\/\/localhost:8081\/models\/model_1?min_worker=1\n<\/code><\/pre>\n<p>About the other issue that logs confirm that not all cores are used, I face the same issue and believe that is a problem in the logging system. Please look at this issue <a href=\"https:\/\/github.com\/pytorch\/serve\/issues\/782\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pytorch\/serve\/issues\/782<\/a>. The community itself agrees that if threads are not set, then by default then it prints 0, even if by default it uses 2*num_cores.<\/p>\n<p><strong>For an exhaustive set of all configs possible<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Reference: https:\/\/github.com\/pytorch\/serve\/blob\/master\/docs\/configuration.md\n# Variables that can be configured through config.properties and Environment Variables\n# NOTE: Variables which can be configured through environment variables **SHOULD** have a\n# &quot;TS_&quot; prefix\n# debug\ninference_address=http:\/\/0.0.0.0:8080\nmanagement_address=http:\/\/0.0.0.0:8081\nmetrics_address=http:\/\/0.0.0.0:8082\nmodel_store=\/opt\/ml\/model\nload_models=model_1.mar\n# blacklist_env_vars\n# default_workers_per_model\n# default_response_timeout\n# unregister_model_timeout\n# number_of_netty_threads\n# netty_client_threads\n# job_queue_size\n# number_of_gpu\n# async_logging\n# cors_allowed_origin\n# cors_allowed_methods\n# cors_allowed_headers\n# decode_input_request\n# keystore\n# keystore_pass\n# keystore_type\n# certificate_file\n# private_key_file\n# max_request_size\n# max_response_size\n# default_service_handler\n# service_envelope\n# model_server_home\n# snapshot_store\n# prefer_direct_buffer\n# allowed_urls\n# install_py_dep_per_model\n# metrics_format\n# enable_metrics_api\n# initial_worker_port\n\n# Configuration which are not documented or enabled through environment variables\n\n# When below variable is set true, then the variables set in environment have higher precedence.\n# For example, the value of an environment variable overrides both command line arguments and a property in the configuration file. The value of a command line argument overrides a value in the configuration file.\n# When set to false, environment variables are not used at all\n# use_native_io=\n# io_ratio=\n# metric_time_interval=\nenable_envvars_config=true\n# model_snapshot=\n# version=\n<\/code><\/pre>",
        "Answer_comment_count":6,
        "Answer_creation_time":1624416005593,
        "Answer_score":5,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1624470584452,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68083831",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73140531,
        "Question_title":"Deploy Pre-Trained model to SageMaker Endpoint from CloudFormation",
        "Question_body":"<p>This seems to be a tricky thing to do, as I haven't found too much documentation for it. I'm trying to deploy a Huggingface pre-trained model for NLU to a SageMaker endpoint. Naturally, I don't want to do this manually, I'd like to automate it through CloudFormation. I found a somewhat <a href=\"https:\/\/faun.pub\/mastering-the-mystical-art-of-model-deployment-part-2-deploying-amazon-sagemaker-endpoints-with-cf9539dc2579\" rel=\"nofollow noreferrer\">useful article<\/a> on how to deploy, but the name of the training model is confusing and I don't know where I would find the right name for the model I want to deploy or where I would put that name (I want to deploy an <a href=\"https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L6-v2\" rel=\"nofollow noreferrer\">all-MiniLM-L6-v2<\/a> model).<\/p>\n<p>Is this possible to do? Do I need to deploy a container? If so, how do I set up the container to process requests and return the text embeddings from the model? I've looked into doing this with just a lambda (which would satisfy the automated deployment process), but the packages I need to use greatly exceed the 250MB limit for lambda+layers.<\/p>\n<p>How do I deploy an endpoint from CloudFormation? Does anyone have experience doing this? If so, please share your wisdom.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1658934967237,
        "Question_score":1,
        "Question_tags":"aws-lambda|amazon-cloudformation|amazon-sagemaker",
        "Question_view_count":103,
        "Owner_creation_time":1605283363407,
        "Owner_last_access_time":1663456472463,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>To anyone curious, this is how I ended up solving this issue:<\/p>\n<p>I ran a Jupyter notebook locally to create the model artifact. Once complete, I zipped the model artifact into a tar.gz file.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModel, AutoTokenizer\nfrom os import makedirs\n\nsaved_model_dir = 'saved_model_dir'\nmakedirs(saved_model_dir, exist_ok=True)\n\n# models were obtained from https:\/\/huggingface.co\/models\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers\/multi-qa-MiniLM-L6-cos-v1')\nmodel = AutoModel.from_pretrained('sentence-transformers\/multi-qa-MiniLM-L6-cos-v1')\n\ntokenizer.save_pretrained(saved_model_dir)\nmodel.save_pretrained(saved_model_dir)\n<\/code><\/pre>\n<pre class=\"lang-bash prettyprint-override\"><code>cd saved_model_dir &amp;&amp; tar czvf ..\/model.tar.gz *\n<\/code><\/pre>\n<p>I included a script in my pipeline to then upload that artifact to S3.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>aws s3 cp path\/to\/model.tar.gz s3:\/\/bucket-name\/prefix\n<\/code><\/pre>\n<p>I also created a CloudFormation template that would stand up my SageMaker resources. The tricky part of this was finding a container image to use, and a colleague was able to point me to <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">this repo<\/a> that contained a massive list of AWS-maintained container images for deep learning and inference. From there, I just needed to select the one that fit my needs.<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>Resources:\n  SageMakerModel:\n    Type: AWS::SageMaker::Model\n    Properties:\n      PrimaryContainer:\n        Image: 763104351884.dkr.ecr.us-east-1.amazonaws.com\/pytorch-inference:1.12.0-cpu-py38-ubuntu20.04-sagemaker # image resource found at https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\n        Mode: SingleModel\n        ModelDataUrl: s3:\/\/path\/to\/model.tar.gz\n      ExecutionRole: \n      ModelName: inference-model\n\n  SageMakerEndpointConfig:\n    Type: AWS::SageMaker::EndpointConfig\n    Properties:\n      EndpointConfigName: endpoint-config-name\n      ProductionVariants:\n        - ModelName: inference-model\n          InitialInstanceCount: 1\n          InstanceType: ml.t2.medium\n          VariantName: dev\n  \n  SageMakerEndpoint:\n    Type: AWS::SageMaker::Endpoint\n    Properties:\n      EndpointName: endpoint-name\n      EndpointConfigName: !GetAtt SageMakerEndpointConfig.EndpointConfigName\n<\/code><\/pre>\n<p>Once the PyTorch model is created locally, this solution essentially automates the process of provisioning and deploying a SageMaker endpoint for inference. If I need to switch the model, I just need to run my notebook code and it will overwrite my existing model artifact. Then I can redeploy and my pipeline will re-upload the artifact to S3, modify the existing SageMaker resources, and the solution will begin operating using the new model.<\/p>\n<p>This may not be the most elegant solution out there, so any suggestions or pointers would be much appreciated!<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1659976220937,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1659976426276,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73140531",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67899421,
        "Question_title":"Training keras model in AWS Sagemaker",
        "Question_body":"<p>I have keras training script on my machine. I am experimenting to run my script on AWS sagemaker container. For that I have used below code.<\/p>\n<pre><code>from sagemaker.tensorflow import TensorFlow\nest = TensorFlow(\n    entry_point=&quot;caller.py&quot;,\n    source_dir=&quot;.\/&quot;,\n    role='role_arn',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_type='ml.m5.large',\n    instance_count=1,\n    hyperparameters={'batch': 8, 'epochs': 10},\n)\n\nest.fit()\n<\/code><\/pre>\n<p>here <code>caller.py<\/code> is my entry point. After executing the above code I am getting <code>keras is not installed<\/code>. Here is the stacktrace.<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;executor.py&quot;, line 14, in &lt;module&gt;\n    est.fit()\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 682, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 1625, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3681, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3240, in _check_job_status\n    raise exceptions.UnexpectedStatusException(\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job tensorflow-training-2021-06-09-07-14-01-778: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/local\/bin\/python3.7 caller.py --batch 4 --epochs 10\n\nModuleNotFoundError: No module named 'keras'\n\n<\/code><\/pre>\n<ol>\n<li>Which instance has pre-installed keras?<\/li>\n<li>Is there any way I can install the python package to the AWS container? or any workaround for the issue?<\/li>\n<\/ol>\n<p>Note: I have tried with my own container uploading to ECR and successfully run my code. I am looking for AWS's existing container capability.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1623223568407,
        "Question_score":0,
        "Question_tags":"amazon-web-services|keras|amazon-ec2|amazon-sagemaker",
        "Question_view_count":316,
        "Owner_creation_time":1426675778223,
        "Owner_last_access_time":1664028138917,
        "Owner_location":"Coimbatore, Tamil Nadu, India",
        "Owner_reputation":10189,
        "Owner_up_votes":1483,
        "Owner_down_votes":261,
        "Owner_views":1471,
        "Question_last_edit_time":1623224188496,
        "Answer_body":"<p>Keras is now part of tensorflow, so you can just reformat your code to use <code>tf.keras<\/code> instead of <code>keras<\/code>. Since version 2.3.0 of tensorflow they are in sync, so it should not be that difficult.\nYou container is <a href=\"https:\/\/aws.amazon.com\/releasenotes\/aws-deep-learning-containers-for-tensorflow-2-3-1-with-cuda-11-0\/\" rel=\"nofollow noreferrer\">this<\/a>, as you can see from the list of the packages, there is no <code>Keras<\/code>.\nIf you instead want to extend a pre-built container you can take a look <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/prebuilt-containers-extend.html\" rel=\"nofollow noreferrer\">here<\/a> but I don't recommend in this specific use-case, because also for future code maintainability you should go for <code>tf.keras<\/code><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1623320398007,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67899421",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53770876,
        "Question_title":"AWS Sagemaker, InvokeEndpoint operation, Model error: \"setting an array element with a sequence.\"",
        "Question_body":"<p>I am trying to Invoke Endpoint, previously deployed on Amazon SageMaker.\nHere is my code:<\/p>\n\n<pre><code>import numpy as np\nimport boto3\n\nclient = boto3.client('sagemaker-runtime')\n\ndef np2csv(arr):\n    csv = io.BytesIO()\n    np.savetxt(csv, arr, delimiter=',', fmt='%g')\n    return csv.getvalue().decode().rstrip()\n\nendpoint_name = 'DEMO-XGBoostEndpoint-2018-12-12-22-07-28'\ntest_vector = np.array([3.60606061e+00, \n                        3.91395664e+00, \n                        1.34200000e+03, \n                        4.56100000e+03,\n                        2.00000000e+02, \n                        2.00000000e+02]) \ncsv_test_vector = np2csv(test_vector)\n\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                               ContentType='text\/csv',\n                               Body=csv_test_vector)\n<\/code><\/pre>\n\n<p>And here is the error I get:<\/p>\n\n<blockquote>\n  <p>ModelErrorTraceback (most recent call last)\n   in ()\n        1 response = client.invoke_endpoint(EndpointName=endpoint_name,\n        2                                    ContentType='text\/csv',\n  ----> 3                                    Body=csv_test_vector)<\/p>\n  \n  <p>\/home\/ec2-user\/anaconda3\/envs\/python2\/lib\/python2.7\/site-packages\/botocore\/client.pyc\n  in _api_call(self, *args, **kwargs)\n      318                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n      319             # The \"self\" in this scope is referring to the BaseClient.\n  --> 320             return self._make_api_call(operation_name, kwargs)\n      321 \n      322         _api_call.<strong>name<\/strong> = str(py_operation_name)<\/p>\n  \n  <p>\/home\/ec2-user\/anaconda3\/envs\/python2\/lib\/python2.7\/site-packages\/botocore\/client.pyc\n  in _make_api_call(self, operation_name, api_params)\n      621             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n      622             error_class = self.exceptions.from_code(error_code)\n  --> 623             raise error_class(parsed_response, operation_name)\n      624         else:\n      625             return parsed_response<\/p>\n  \n  <p>ModelError: An error occurred (ModelError) when calling the\n  InvokeEndpoint operation: Received client error (415) from model with\n  message \"setting an array element with a sequence.\". See\n  <a href=\"https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/DEMO-XGBoostEndpoint-2018-12-12-22-07-28\" rel=\"nofollow noreferrer\">https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/DEMO-XGBoostEndpoint-2018-12-12-22-07-28<\/a>\n  in account 249707424405 for more information.<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1544739509967,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":2561,
        "Owner_creation_time":1324988509370,
        "Owner_last_access_time":1663847888160,
        "Owner_location":"Moscow, Russia",
        "Owner_reputation":1593,
        "Owner_up_votes":55,
        "Owner_down_votes":2,
        "Owner_views":93,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This works:<\/p>\n\n<pre><code>import numpy as np\nimport boto3\n\nclient = boto3.client('sagemaker-runtime')\nendpoint_name = 'DEMO-XGBoostEndpoint-2018-12-12-22-07-28'\ntest_vector = [3.60606061e+00, \n               3.91395664e+00, \n               1.34200000e+03, \n               4.56100000e+03,\n               2.00000000e+02, \n               2.00000000e+02]) \n\nbody = ',',join([str(item) for item in test_vector])\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                               ContentType='text\/csv',\n                               Body=body)\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1547665192370,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53770876",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71260306,
        "Question_title":"How to access\/invoke a sagemaker endpoint without lambda?",
        "Question_body":"<p>based on the aws documentation, maximum timeout limit is less that 30 seconds in api gateway.so hooking up an sagemaker endpoint with api gateway wouldn't make sense, if the request\/response is going to take more than 30 seconds. is there any workaround ? adding a lambda in between api gateway and sagemaker endpoint is going to add more time to process request\/response, which i would like to avoid. also, there will be added time for lambda cold starts and sagemaker serverless endpoints are built on top of lambda so that will also add cold start time. is there a way to invoke the serverless sagemaker endpoints , without these overhead?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1645755688313,
        "Question_score":3,
        "Question_tags":"serverless|amazon-sagemaker",
        "Question_view_count":1122,
        "Owner_creation_time":1590797441983,
        "Owner_last_access_time":1664049080543,
        "Owner_location":null,
        "Owner_reputation":525,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":98,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It is indeed possible to invoke sagemaker endpoints from sagemaker without using any other AWS services and that is also manifested by the fact that they have invocation URLs.<\/p>\n<p>Here's how you set it up:<\/p>\n<ol>\n<li>create an user with only programmatic access and attach a policy json that should look something like below:<\/li>\n<\/ol>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;VisualEditor0&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:sagemaker:&lt;region&gt;:&lt;account-id&gt;:endpoint\/&lt;endpoint-name&gt;&quot;\n        }\n    ]\n} \n<\/code><\/pre>\n<p>you can replace <code>&lt;endpoint-name&gt;<\/code> with <code>*<\/code> to let this user invoke all endpoints.<\/p>\n<ol start=\"2\">\n<li><p>use the ACCESS-KEY and SECRET-ACCESS-KEY to configure authorisation in postman like shown in this screenshot. also add the parameters in advanced tab like shown in the screenshot.\n<a href=\"https:\/\/i.stack.imgur.com\/cYkTf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cYkTf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>then fill up your body with the relevant content type.<\/p>\n<\/li>\n<li><p>then add or remove additional headers like variant-name or model-name, if you have them set up and the headers should look like shown in this screenshot: <a href=\"https:\/\/i.stack.imgur.com\/NLqkV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NLqkV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>send the request to receive reponse like this\n<a href=\"https:\/\/i.stack.imgur.com\/uA4kF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uA4kF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<\/ol>\n<p><em><strong>URL and credentials in the above screenshots doesn't work anymore, duh!<\/strong><\/em><\/p>\n<p>and if you want code to invoke the endpoint directly using some back-end language, <a href=\"https:\/\/stackoverflow.com\/a\/70803026\/11814996\">here's code for python<\/a>.<\/p>",
        "Answer_comment_count":5,
        "Answer_creation_time":1645795642143,
        "Answer_score":3,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1649698499116,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71260306",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58487710,
        "Question_title":"User Docker Hub registry containers in AWS Sagemaker",
        "Question_body":"<p>Is there any way to load containers stored in docker hub registry in Amazon Sagemaker.\nAccording to some documentation, it should be possible, but I have not been able to find any relevan example or guide for it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1571665775480,
        "Question_score":0,
        "Question_tags":"amazon-web-services|docker|amazon-sagemaker",
        "Question_view_count":135,
        "Owner_creation_time":1382876091093,
        "Owner_last_access_time":1640773246957,
        "Owner_location":"Valencia, Espa\u00f1a",
        "Owner_reputation":149,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":null,
        "Answer_body":"<p>While you can use any registry when working with Docker on a SageMaker notebook, as of this writing other SageMaker components presently only support images from Amazon ECR repositories.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1582242007587,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58487710",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67093041,
        "Question_title":"Aws Sagemaker - ModuleNotFoundError: No module named 'cv2'",
        "Question_body":"<p>I am trying to run a object detection code in Aws. Although opencv is listed in the requirement file, i have the error &quot;no module named cv2&quot;. I am not sure how to fix this error. could someone help me please.<\/p>\n<p>My requirement.txt file has<\/p>\n<ul>\n<li>opencv-python<\/li>\n<li>numpy&gt;=1.18.2<\/li>\n<li>scipy&gt;=1.4.1<\/li>\n<li>wget&gt;=3.2<\/li>\n<li>tensorflow==2.3.1<\/li>\n<li>tensorflow-gpu==2.3.1<\/li>\n<li>tqdm==4.43.0<\/li>\n<li>pandas<\/li>\n<li>boto3<\/li>\n<li>awscli<\/li>\n<li>urllib3<\/li>\n<li>mss<\/li>\n<\/ul>\n<p>I tried installing &quot;imgaug&quot; and &quot;opencv-python headless&quot; as well.. but still not able to get rid of this error.<\/p>\n<pre><code>sh-4.2$ python train_launch.py \n[INFO-ROLE] arn:aws:iam::021945294007:role\/service-role\/AmazonSageMaker-ExecutionRole-20200225T145269\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_count has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\n2021-04-14 13:29:58 Starting - Starting the training job...\n2021-04-14 13:30:03 Starting - Launching requested ML instances......\n2021-04-14 13:31:11 Starting - Preparing the instances for training......\n2021-04-14 13:32:17 Downloading - Downloading input data...\n2021-04-14 13:32:41 Training - Downloading the training image..WARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n2021-04-14 13:33:03,970 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\n2021-04-14 13:33:05,030 sagemaker-containers INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {\n        &quot;training&quot;: &quot;\/opt\/ml\/input\/data\/training&quot;\n    },\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;unfreezed_epochs&quot;: 2,\n        &quot;freezed_batch_size&quot;: 8,\n        &quot;freezed_epochs&quot;: 1,\n        &quot;unfreezed_batch_size&quot;: 8,\n        &quot;model_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {\n        &quot;training&quot;: {\n            &quot;TrainingInputMode&quot;: &quot;File&quot;,\n            &quot;S3DistributionType&quot;: &quot;FullyReplicated&quot;,\n            &quot;RecordWrapperType&quot;: &quot;None&quot;\n        }\n    },\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;yolov4-2021-04-14-15-29&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;train_indu&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 8,\n    &quot;num_gpus&quot;: 1,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;train_indu.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2}\nSM_USER_ENTRY_POINT=train_indu.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[&quot;training&quot;]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=train_indu\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=8\nSM_NUM_GPUS=1\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{&quot;training&quot;:&quot;\/opt\/ml\/input\/data\/training&quot;},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;yolov4-2021-04-14-15-29&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;train_indu&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:8,&quot;num_gpus&quot;:1,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;train_indu.py&quot;}\nSM_USER_ARGS=[&quot;--freezed_batch_size&quot;,&quot;8&quot;,&quot;--freezed_epochs&quot;,&quot;1&quot;,&quot;--model_dir&quot;,&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;--unfreezed_batch_size&quot;,&quot;8&quot;,&quot;--unfreezed_epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_CHANNEL_TRAINING=\/opt\/ml\/input\/data\/training\nSM_HP_UNFREEZED_EPOCHS=2\nSM_HP_FREEZED_BATCH_SIZE=8\nSM_HP_FREEZED_EPOCHS=1\nSM_HP_UNFREEZED_BATCH_SIZE=8\nSM_HP_MODEL_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages\n\nInvoking script with the following command:\n\n\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2\n\n\nWARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n[name: &quot;\/device:CPU:0&quot;\ndevice_type: &quot;CPU&quot;\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 4667030854237447206\n, name: &quot;\/device:XLA_CPU:0&quot;\ndevice_type: &quot;XLA_CPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 3059419181456814147\nphysical_device_desc: &quot;device: XLA_CPU device&quot;\n, name: &quot;\/device:XLA_GPU:0&quot;\ndevice_type: &quot;XLA_GPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 6024475084695919958\nphysical_device_desc: &quot;device: XLA_GPU device&quot;\n, name: &quot;\/device:GPU:0&quot;\ndevice_type: &quot;GPU&quot;\nmemory_limit: 14949928141\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 13034103301168381073\nphysical_device_desc: &quot;device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5&quot;\n]\nTraceback (most recent call last):\n  File &quot;train_indu.py&quot;, line 12, in &lt;module&gt;\n    from yolov3.dataset import Dataset\n  File &quot;\/opt\/ml\/code\/yolov3\/dataset.py&quot;, line 3, in &lt;module&gt;\n    import cv2\nModuleNotFoundError: No module named 'cv2'\n2021-04-14 13:33:08,453 sagemaker-containers ERROR    ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n\n2021-04-14 13:33:11 Uploading - Uploading generated training model\n2021-04-14 13:33:54 Failed - Training job failed\nTraceback (most recent call last):\n  File &quot;train_launch.py&quot;, line 41, in &lt;module&gt;\n    estimator.fit(s3_data_path, logs=True, job_name=job_name) #the argument logs is crucial if you want to see what happends\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 535, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 1210, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 3365, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 2957, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job yolov4-2021-04-14-15-29: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1618407986697,
        "Question_score":0,
        "Question_tags":"opencv|image-processing|amazon-ec2|object-detection|amazon-sagemaker",
        "Question_view_count":1218,
        "Owner_creation_time":1558009697177,
        "Owner_last_access_time":1618755603610,
        "Owner_location":"Cergy-Pontoise, Cergy, France",
        "Owner_reputation":53,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Make sure your estimator has<\/p>\n<ul>\n<li>framework_version = '2.3',<\/li>\n<li>py_version = 'py37',<\/li>\n<\/ul>",
        "Answer_comment_count":0,
        "Answer_creation_time":1618410091900,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67093041",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68374280,
        "Question_title":"Assigning name to AWS SageMaker Training job",
        "Question_body":"<p>This may be a very basic question (I imagine it is)<\/p>\n<pre><code>estimator = PyTorch(entry_point='train.py',\n                   source_dir = 'code',\n                    role = role,\n                   framework_version = '1.5.0',\n                   py_version = 'py3',\n                   instance_count = 2,\n                   instance_type = 'ml.g4dn.2xlarge',\n                   hyperparameters={&quot;epochs&quot;: 2,\n                                     &quot;num_labels&quot;: 46,\n                                     &quot;backend&quot;: &quot;gloo&quot;,    \n                                    },\n                   profiler_config=profiler_config,\n                    debugger_hook_config=debugger_hook_config,\n                    rules=rules\n                   )\n<\/code><\/pre>\n<p>I declare my estimator as above, and put this into training using fit().<br \/>\nI have done several of these on my sagemaker, and there are several training jobs in the aws training job log.<br \/>\nBut they all appear in the form 'pytorch-training-2021 ....'. <br \/>\nIs there anyway I could declare the name of the training job like 'custom-model-xgboost-ver1' ?<br \/>\nI thought it would be possible as one of the parameter of estimator, but i couldn't find it.<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1626249953723,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":445,
        "Owner_creation_time":1600718448277,
        "Owner_last_access_time":1663311569497,
        "Owner_location":"Seoul, South Korea",
        "Owner_reputation":69,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When you call <code>fit()<\/code> you can pass this parameter <code>job_name=yourJobName<\/code><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1626250161860,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68374280",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72874937,
        "Question_title":"Is it possible set up an endpoint for a model I created in AWS SageMaker without using the SageMaker SDK",
        "Question_body":"<p>I've created my own model on a AWS SageMaker instance, with my own training and inference loops. I want to deploy it so that I can call the model for inference from AWS Lambda.<\/p>\n<p>I didn't use the SageMaker package to develop at all, but every tutorial (here is <a href=\"https:\/\/towardsdatascience.com\/using-aws-sagemaker-and-lambda-function-to-build-a-serverless-ml-platform-f14b3ec5854a%3E\" rel=\"nofollow noreferrer\">one<\/a>) I've looked at does so.<\/p>\n<p>How do I create an endpoint without using the SageMaker package.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657051299687,
        "Question_score":0,
        "Question_tags":"python|aws-lambda|amazon-sagemaker|endpoint",
        "Question_view_count":66,
        "Owner_creation_time":1657050754840,
        "Owner_last_access_time":1663796977727,
        "Owner_location":null,
        "Owner_reputation":15,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use the boto3 library to do this.<\/p>\n<p>Here is an example of pseudo code for this -<\/p>\n<pre><code>import boto3\nsm_client = boto3.client('sagemaker')\ncreate_model_respose = sm_client.create_model(ModelName=model_name, ExecutionRoleArn=role, Containers=[container] )\n\ncreate_endpoint_config_response = sm_client.create_endpoint_config(EndpointConfigName=endpoint_config_name)\n\ncreate_endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1657053745440,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72874937",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71152047,
        "Question_title":"how to create a serverless endpoint in sagemaker?",
        "Question_body":"<p>I followed the aws documentation ( <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config<\/a>) to create a model and to use that model, i coded for a serverless endpoint config (sample code below) ,I have all the required values  but this throws an error below and i'm not sure why<\/p>\n<p>parameter validation failed unknown parameter inProductVariants [ 0 ]: &quot;ServerlessConfig&quot;, must be one of : VairantName, ModelName, InitialInstanceCount , Instancetype...<\/p>\n<pre><code>response = client.create_endpoint_config(\n   EndpointConfigName=&quot;abc&quot;,\n   ProductionVariants=[\n        {\n            &quot;ModelName&quot;: &quot;foo&quot;,\n            &quot;VariantName&quot;: &quot;variant-1&quot;,\n            &quot;ServerlessConfig&quot;: {\n                &quot;MemorySizeInMB&quot;: 1024,\n                &quot;MaxConcurrency&quot;: 2\n            }\n        } \n    ]\n)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645067620433,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":161,
        "Owner_creation_time":1590797441983,
        "Owner_last_access_time":1664049080543,
        "Owner_location":null,
        "Owner_reputation":525,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":98,
        "Question_last_edit_time":1645073711769,
        "Answer_body":"<p>You are probably using <strong>old boto3<\/strong> version. <code>ServerlessConfig<\/code> is a very new configuration option. You need to upgrade to the latest version (1.21.1) if possible.<\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1645069539320,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71152047",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73292975,
        "Question_title":"Sagemaker Data Capture does not write files",
        "Question_body":"<p>I want to enable data capture for a specific endpoint (so far, only via the console). The endpoint works fine and also logs &amp; returns the desired results. However, no files are written to the specified S3 location.<\/p>\n<h3>Endpoint Configuration<\/h3>\n<p>The endpoint is based on a training job with a scikit learn classifier. It has only one variant which is a <code>ml.m4.xlarge<\/code> instance type. Data Capture is enabled with a sampling percentage of 100%. As data capture storage locations I tried <code>s3:\/\/&lt;bucket-name&gt;<\/code> as well as <code>s3:\/\/&lt;bucket-name&gt;\/&lt;some-other-path&gt;<\/code>. With the &quot;Capture content type&quot; I tried leaving everything blank, setting <code>text\/csv<\/code> in &quot;CSV\/Text&quot; and <code>application\/json<\/code> in &quot;JSON&quot;.<\/p>\n<h3>Endpoint Invokation<\/h3>\n<p>The endpoint is invoked in a Lambda function with a client. Here's the call:<\/p>\n<pre><code>sagemaker_body_source = {\n            &quot;segments&quot;: segments,\n            &quot;language&quot;: language\n        }\npayload = json.dumps(sagemaker_body_source).encode()\nresponse = self.client.invoke_endpoint(EndpointName=endpoint_name,\n                                       Body=payload,\n                                       ContentType='application\/json',\n                                       Accept='application\/json')\nresult = json.loads(response['Body'].read().decode())\nreturn result[&quot;predictions&quot;]\n<\/code><\/pre>\n<p>Internally, the endpoint uses a Flask API with an <code>\/invocation<\/code> path that returns the result.<\/p>\n<h3>Logs<\/h3>\n<p>The endpoint itself works fine and the Flask API is logging input and output:<\/p>\n<pre><code>INFO:api:body: {'segments': [&lt;strings...&gt;], 'language': 'de'}\n<\/code><\/pre>\n<pre><code>INFO:api:output: {'predictions': [{'text': 'some text', 'label': 'some_label'}, ....]}\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1660052882547,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":65,
        "Owner_creation_time":1394703217223,
        "Owner_last_access_time":1663244105390,
        "Owner_location":"Cologne, Germany",
        "Owner_reputation":486,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":null,
        "Answer_body":"<p>So the issue seemed to be related to the IAM role. The default role (<code>ModelEndpoint-Role<\/code>) does not have access to write S3 files. It worked via the SDK since it uses another role in the sagemaker studio. I did not receive any error message about this.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1660654017400,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73292975",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57379173,
        "Question_title":"Where do I store my model's training data, artifacts, etc?",
        "Question_body":"<p>I'm trying to build and push a custom ML model with docker to Amazon SageMaker. I know things are supposed to follow the general structure of being in opt\/ml. But there's no such bucket in Amazon S3??? Am I supposed to create this directory within my container before I build and push the image to AWS? I just have no idea where to put my training data, etc.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1565104546377,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1022,
        "Owner_creation_time":1399251405187,
        "Owner_last_access_time":1651704417890,
        "Owner_location":null,
        "Owner_reputation":145,
        "Owner_up_votes":35,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":null,
        "Answer_body":"<p>SageMaker is automating the deployment of the Docker image with your code using the convention of channel->local-folder. Everything that you define with a channel in your <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig\" rel=\"nofollow noreferrer\">input data configuration<\/a>, will be copied to the local Docker file system under <em>\/opt\/ml\/<\/em> folder, using the name of the channel as the name of the sub-folder.<\/p>\n\n<pre><code>{\n\"train\" : {\"ContentType\":  \"trainingContentType\", \n           \"TrainingInputMode\": \"File\", \n           \"S3DistributionType\": \"FullyReplicated\", \n           \"RecordWrapperType\": \"None\"},\n\"evaluation\" : {\"ContentType\":  \"evalContentType\", \n                \"TrainingInputMode\": \"File\", \n                \"S3DistributionType\": \"FullyReplicated\", \n                \"RecordWrapperType\": \"None\"},\n\"validation\" : {\"TrainingInputMode\": \"File\", \n                \"S3DistributionType\": \"FullyReplicated\", \n                \"RecordWrapperType\": \"None\"}\n} \n<\/code><\/pre>\n\n<p>to:<\/p>\n\n<pre><code>\/opt\/ml\/input\/data\/training\n\/opt\/ml\/input\/data\/validation\n\/opt\/ml\/input\/data\/testing\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1565167576867,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57379173",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71467176,
        "Question_title":"How can I verify that my training job is reading the augmented manifest file?",
        "Question_body":"<p>Apologies for the long post.<\/p>\n<p>Originally, I had data in one location on an S3 bucket and used to train deep learning image classification models on this data using the typical 'File' mode and passing the S3 uri where the data is stored as training input. To try and accelerate training, I wanted to switch to using:<\/p>\n<ol>\n<li>Pipe mode, to stream data and not download all the data at the beginning of the training, starting training faster and saving disk space.<\/li>\n<li>Augmented Manifest File coupled with 1., so that I don't have to place my data in a single location on S3, so I avoid moving data around when I train models.<\/li>\n<\/ol>\n<p>I was making my script similar to <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=934156#934156\" rel=\"nofollow noreferrer\">the one in this example<\/a>. I printed the steps done when parsing the data, however I noticed that the data might not have been read because when printing it shows the following:<\/p>\n<pre><code>step 1 Tensor(&quot;ParseSingleExample\/ParseExample\/ParseExampleV2:0&quot;, shape=(), dtype=string)\nstep 2 Tensor(&quot;DecodePng:0&quot;, shape=(None, None, 3), dtype=uint8)\nstep 3 Tensor(&quot;Cast:0&quot;, shape=(None, None, 3), dtype=float32)\n<\/code><\/pre>\n<p>I guess the image is not being read\/found since the shape is <code>[None, None, 3]<\/code> when it should be <code>[224, 224, 3]<\/code>, so maybe the problem is from the Augmented Manifest file?<\/p>\n<p>Here's an example of how my Augmented Manifest file is written:<\/p>\n<pre><code>{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image1.png&quot;, &quot;label&quot;: 1}\n{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image2.png&quot;, &quot;label&quot;: 2}\n{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image3.png&quot;, &quot;label&quot;: 3}\n<\/code><\/pre>\n<p>Some other details I should probably mention:<\/p>\n<ol>\n<li>When I create the Training Input I pass <code>'content_type': 'application\/x-recordio', 'record_wrapping': 'RecordIO'<\/code>, even though my data are in .png format, but I assumed that as the augmented manifest file is read the data get wrapped in the RecordIO format.<\/li>\n<li>Following my first point, I pass <code>PipeModeDataset(channel=channel, record_format='RecordIO')<\/code>, so also not sure about the RecordIO thing.<\/li>\n<\/ol>\n<p>There isn't an actual error that is raised, just when I start fitting the model nothing happens, it keeps on running but nothing actually runs so I'm trying to find the issue.<\/p>\n<hr \/>\n<p>EDIT: It now reads the shape correctly, but there's still the issue where it enters the .fit method and does nothing, just keeps running without doing anything. Find part of the script below.<\/p>\n<pre><code>def train_input_fn(train_channel):\n    &quot;&quot;&quot;Returns input function that feeds the model during training&quot;&quot;&quot;\n    return _input_fn(train_channel)\n\ndef _input_fn(channel):\n    &quot;&quot;&quot;\n        Returns a Dataset which reads from a SageMaker PipeMode channel.\n    &quot;&quot;&quot;\n    \n    features = {\n        'image-ref': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([3], tf.int64),\n    }\n \n    def combine(records):\n        return records[0], records[1]\n \n    def parse(record):\n        \n        parsed = tf.io.parse_single_example(record, features)\n        \n                 \n\n        image = tf.io.decode_png(parsed[&quot;image-ref&quot;], channels=3, dtype=tf.uint8)\n        image = tf.reshape(image, [224, 224, 3])\n        \n        lbl = parsed['label']\n        print(image, lbl)\n        return (image, lbl)\n \n    ds = PipeModeDataset(channel=channel, record_format='RecordIO')\n    ds = ds.map(parse, num_parallel_calls=AUTOTUNE)\n    ds = ds.prefetch(AUTOTUNE)\n \n    return ds\n\ndef model(dataset):\n    &quot;&quot;&quot;Generate a simple model&quot;&quot;&quot;\n    inputs = Input(shape=(224, 224, 3))\n    prediction_layer = Dense(2, activation = 'softmax')\n\n\n    x = inputs\n    x = tf.keras.applications.mobilenet.MobileNet(include_top=False, input_shape=(224,224,3), weights='imagenet')(x)\n    outputs = prediction_layer(x)\n    rec_model = tf.keras.Model(inputs, outputs)    \n    \n    rec_model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n        metrics=['accuracy']\n    )\n    \n    \n    rec_model.fit(\n        dataset\n    )\n\n    return rec_model\n\ndef main(params):\n    \n    epochs = params['epochs']\n    train_channel = params['train_channel']\n    record_format = params['record_format']\n    batch_size = params['batch_size']\n        \n    train_spec = train_input_fn(train_channel)\n    model_classifier = model(train_spec)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647258085310,
        "Question_score":0,
        "Question_tags":"python|tensorflow|amazon-s3|manifest|amazon-sagemaker",
        "Question_view_count":127,
        "Owner_creation_time":1576016596283,
        "Owner_last_access_time":1660210631827,
        "Owner_location":"Beirut, Lebanon",
        "Owner_reputation":15,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1647268897627,
        "Answer_body":"<p>From <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<blockquote>\n<p>A PipeModeDataset can read TFRecord, RecordIO, or text line records.<\/p>\n<\/blockquote>\n<p>While your'e trying to read binary (PNG) files. I don't see a relevant <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions\/tree\/tf-2\/src\/pipemode_op\/RecordReader\" rel=\"nofollow noreferrer\">record reader here<\/a> to help you do that.<br \/>\nYou could build your own format pipe implementation like shown <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">here<\/a>, but it's considerably more effort.<\/p>\n<p>Alternatively, you mentioned your files are scattered in different folders, but if your files common path contains less than 2M files, you could use <a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/10\/amazon-sagemaker-fast-file-mode\/\" rel=\"nofollow noreferrer\">FastFile mode<\/a> to <strong>stream<\/strong> data. Currently, FastFile only supports an S3 Prefix, so you won't be able to use a manifest.<\/p>\n<p>Also see this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/choose-the-best-data-source-for-your-amazon-sagemaker-training-job\/\" rel=\"nofollow noreferrer\">general pros\/cons discussion of the different available storage and input types available in SageMaker<\/a>.<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1647607100133,
        "Answer_score":3,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71467176",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49168673,
        "Question_title":"How to load a training set in AWS SageMaker to build a model?",
        "Question_body":"<p>I am very new to SageMaker. Upon my first interaction, it looks like the AWS SageMaker requires you to start from its Notebook. I have a training set which is ready. Is there a way to bypass setting the Notebook and just to start by upload the training set? Or it should be done through the Notebook. If anyone knows some example fitting my need above, that will be great. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1520498020543,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":700,
        "Owner_creation_time":1305269513437,
        "Owner_last_access_time":1664081804010,
        "Owner_location":null,
        "Owner_reputation":10317,
        "Owner_up_votes":268,
        "Owner_down_votes":1,
        "Owner_views":595,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Amazon SageMaker is a combination of multiple services that each is independent of the others. You can use the notebook instances if you want to develop your models in the familiar Jupyter environment. But if just need to train a model, you can use the training jobs without opening a notebook instance. <\/p>\n\n<p>There a few ways to launch a training job:<\/p>\n\n<ul>\n<li>Use the high-level SDK for Python that is similar to the way that you start a training step in your python code<\/li>\n<\/ul>\n\n<p><code>kmeans.fit(kmeans.record_set(train_set[0]))<\/code><\/p>\n\n<p>Here is the link to the python library: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a><\/p>\n\n<ul>\n<li>Use the low-level API to Create-Training-Job, and you can do that using various SDK (Java, Python, JavaScript, C#...) or the CLI. <\/li>\n<\/ul>\n\n<p><code>sagemaker = boto3.client('sagemaker')\n sagemaker.create_training_job(**create_training_params)<\/code><\/p>\n\n<p>Here is a link to the documentation on these options: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a> <\/p>\n\n<ul>\n<li>Use Spark interface to launch it using a similar interface to creating an MLLib training job<\/li>\n<\/ul>\n\n<p><code>val estimator = new KMeansSageMakerEstimator(\n  sagemakerRole = IAMRole(roleArn),\n  trainingInstanceType = \"ml.p2.xlarge\",\n  trainingInstanceCount = 1,\n  endpointInstanceType = \"ml.c4.xlarge\",\n  endpointInitialInstanceCount = 1)\n  .setK(10).setFeatureDim(784)<\/code><\/p>\n\n<p><code>val model = estimator.fit(trainingData)<\/code><\/p>\n\n<p>Here is a link to the spark-sagemaker library: <a href=\"https:\/\/github.com\/aws\/sagemaker-spark\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-spark<\/a><\/p>\n\n<ul>\n<li>Create a training job in the Amazon SageMaker console using the wizard there: <a href=\"https:\/\/console.aws.amazon.com\/sagemaker\/home?region=us-east-1#\/jobs\" rel=\"nofollow noreferrer\">https:\/\/console.aws.amazon.com\/sagemaker\/home?region=us-east-1#\/jobs<\/a><\/li>\n<\/ul>\n\n<p>Please note that there a few options also to train models, either using the built-in algorithms such as K-Means, Linear Learner or XGBoost (see here for the complete list: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html<\/a>). But you can also bring your own models for pre-baked Docker images such as TensorFlow (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf.html<\/a>) or MXNet (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet.html<\/a>), your own Docker image (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html<\/a>).  <\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1520631374253,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49168673",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56927813,
        "Question_title":"Using of Estamator.evaluate() on trained sagemaker tensorflow model",
        "Question_body":"<p>After I've trained and deployed the model with AWS SageMaker, I want to evaluate it on several csv files:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>- category-1-eval.csv (~700000 records)\n- category-2-eval.csv (~500000 records)\n- category-3-eval.csv (~800000 records)\n...\n<\/code><\/pre>\n\n<p>The right way to do this is with using <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/estimator\/Estimator#evaluate\" rel=\"nofollow noreferrer\">Estimator.evaluate()<\/a> method, as it is fast.<\/p>\n\n<p>The problem is - I cannot find the way to restore SageMaker model into Tensorflow Estimator, is it possible?<\/p>\n\n<p>I've tried to restore a model like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>tf.estimator.DNNClassifier(\n    feature_columns=...,\n    hidden_units=[...],\n    model_dir=\"s3:\/\/&lt;bucket_name&gt;\/checkpoints\",\n)\n<\/code><\/pre>\n\n<p>In AWS SageMaker documentation a different approach is described - <a href=\"https:\/\/docs.aws.amazon.com\/en_us\/sagemaker\/latest\/dg\/how-it-works-model-validation.html\" rel=\"nofollow noreferrer\">to test the actual endpoint from the Notebook<\/a> - but it takes to much time and requires a lot of API calls to the endpoint.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1562553970617,
        "Question_score":0,
        "Question_tags":"tensorflow|neural-network|amazon-sagemaker",
        "Question_view_count":155,
        "Owner_creation_time":1530642335903,
        "Owner_last_access_time":1622266440183,
        "Owner_location":"Vancouver, BC, Canada",
        "Owner_reputation":53,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":"<p>if you used the built-in Tensorflow container, your model has been saved in Tensorflow Serving format, e.g.:<\/p>\n\n<pre><code>$ tar tfz model.tar.gz\nmodel\/\nmodel\/1\/\nmodel\/1\/saved_model.pb\nmodel\/1\/variables\/\nmodel\/1\/variables\/variables.index\nmodel\/1\/variables\/variables.data-00000-of-00001\n<\/code><\/pre>\n\n<p>You can easily load it with Tensorflow Serving on your local machine, and send it samples to predict. More info at <a href=\"https:\/\/www.tensorflow.org\/tfx\/guide\/serving\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/tfx\/guide\/serving<\/a><\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1562591149540,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1562591826680,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56927813",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64169189,
        "Question_title":"How can I deploy a re-trained Sagemaker model to an endpoint?",
        "Question_body":"<p>With an <code>sagemaker.estimator.Estimator<\/code>, I want to re-<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#sagemaker.estimator.EstimatorBase.deploy\" rel=\"nofollow noreferrer\">deploy<\/a> a model after retraining (calling <code>fit<\/code> with new data).<\/p>\n<p>When I call this<\/p>\n<pre><code>estimator.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')\n<\/code><\/pre>\n<p>I get an error<\/p>\n<pre><code>botocore.exceptions.ClientError: An error occurred (ValidationException) \nwhen calling the CreateEndpoint operation: \nCannot create already existing endpoint &quot;arn:aws:sagemaker:eu-east- \n1:1776401913911:endpoint\/zyx&quot;.\n<\/code><\/pre>\n<p>Apparently I want to use functionality like <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateEndpoint.html\" rel=\"nofollow noreferrer\">UpdateEndpoint<\/a>. How do I access that functionality from this API?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1601630651783,
        "Question_score":3,
        "Question_tags":"python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":1958,
        "Owner_creation_time":1227171471293,
        "Owner_last_access_time":1664047108080,
        "Owner_location":"Israel",
        "Owner_reputation":17500,
        "Owner_up_votes":463,
        "Owner_down_votes":87,
        "Owner_views":1561,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes, under the hood the <code>model.deploy<\/code> creates a model, an endpoint configuration and an endpoint. When you call again the method from an already-deployed, trained estimator it will create an error because a similarly-configured endpoint is already deployed. What I encourage you to try:<\/p>\n<ul>\n<li><p>use the <code>update_endpoint=True<\/code> parameter. From the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"noreferrer\">SageMaker SDK doc<\/a>:\n<em>&quot;Additionally, it is possible to deploy a different endpoint configuration, which links to your model, to an already existing\nSageMaker endpoint. This can be done by specifying the existing\nendpoint name for the <code>endpoint_name<\/code> parameter along with the\n<code>update_endpoint<\/code> parameter as True within your <code>deploy()<\/code> call.&quot;<\/em><\/p>\n<\/li>\n<li><p>Alternatively, if you want to create a separate model you can specify a new <code>model_name<\/code> in your <code>deploy<\/code><\/p>\n<\/li>\n<\/ul>",
        "Answer_comment_count":2,
        "Answer_creation_time":1601891469683,
        "Answer_score":6,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1602140796110,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64169189",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59717227,
        "Question_title":"AWS SageMaker - submit button is not working with custom template",
        "Question_body":"<p>When I create a new job on AWS SageMaker, using my custom template with crowd form (see attached sample) the SUBMIT button is not working and is not even clickable. Is there anyway to make this work? Haven`t see a good response on AWS support.<\/p>\n\n<pre><code>$('#submitButton').onclick = function() {\n   $('crowd-form').submit(); \n};\n\n\n &lt;body&gt;\n    &lt;h2 id=\"hit\"&gt;test&lt;\/h2&gt;\n        &lt;canvas id=\"canvas\" width=1210 height=687&gt;&lt;\/canvas&gt;    \n        &lt;crowd-button id=\"submitButton3\"&gt;Test button&lt;\/crowd-button&gt;\n\n    &lt;crowd-form&gt;\n\n        &lt;input type=\"hidden\" name=\"path0\" id=\"input0123\" value=\"{{task.input.metadata.images.path0}}\" \/&gt;\n        &lt;crowd-input label=\"Please input the character you see in the image\" max-length=\"1\" name=\"workerInput0\"&gt;&lt;\/crowd-input&gt;\n\n        &lt;crowd-button id=\"submitButto3223n\"&gt;Submit123&lt;\/crowd-button&gt;\n\n    &lt;\/div&gt;&lt;\/div&gt;\n\n    &lt;crowd-button id=\"submitButton\"&gt;Submit123&lt;\/crowd-button&gt;\n\n    &lt;\/crowd-form&gt;\n    &lt;crowd-button id=\"submitButton1\"&gt;Submit1232&lt;\/crowd-button&gt;\n\n    &lt;script src=\"http:\/\/code.jquery.com\/jquery-1.11.0.min.js\"&gt;&lt;\/script&gt;\n &lt;\/body&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1578920666227,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker|mechanicalturk",
        "Question_view_count":917,
        "Owner_creation_time":1544346330127,
        "Owner_last_access_time":1588231544867,
        "Owner_location":"Lviv, Lviv Oblast, Ukraine",
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":1578989573556,
        "Answer_body":"<p>There are few issues with you code snippet.<\/p>\n<p>Here are the links to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-ui-template-reference.html\" rel=\"nofollow noreferrer\">SageMaker's HTML Reference<\/a> and <a href=\"https:\/\/awsfeed.com\/whats-new\/machine-learning\/build-a-custom-data-labeling-workflow-with-amazon-sagemaker-ground-truth\/\" rel=\"nofollow noreferrer\">Example for building custom Labeling template<\/a><\/p>\n<p>First remove all those submit buttons (<code>&lt;crowd-button&gt;<\/code> elements) and the <code>onClick<\/code> event handler. From here you have two options use default SageMaker submit button or create your own in the template.<\/p>\n<h2>Use SageMaker's Submit Button<\/h2>\n<p>Leave out submit buttons (<code>crowd-button<\/code>) and SageMaker will automatically append one inside <code>crowd-form<\/code>. According to documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-ui-template-crowd-form.html\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<h2>Use custom Submit Button<\/h2>\n<p>In this case you need to:<\/p>\n<ol>\n<li>Prevent SageMaker adding button by including <code>crowd-button<\/code> <strong>inside<\/strong> the <code>crowd-form<\/code> element and setting <code>style=&quot;display: none;<\/code><\/li>\n<li>Add your own Submit button elsewhere on the template and add <code>onclick<\/code> even handler that will execute <code>form.submit()<\/code><\/li>\n<\/ol>\n<p>Here is the working example of the template (taken from the Example mentioned above).<\/p>\n<pre><code>&lt;script src=&quot;https:\/\/assets.crowd.aws\/crowd-html-elements.js&quot;&gt;&lt;\/script&gt;\n\n&lt;link rel=&quot;stylesheet&quot; href=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/css\/1.3fc3007b.chunk.css&quot;&gt;\n&lt;link rel=&quot;stylesheet&quot; href=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/css\/main.9504782e.chunk.css&quot;&gt;\n\n&lt;div id='document-text' style=&quot;display: none;&quot;&gt;\n  {{ task.input.text }}\n&lt;\/div&gt;\n&lt;div id='document-image' style=&quot;display: none;&quot;&gt;\n        {{ task.input.taskObject | grant_read_access }}\n&lt;\/div&gt;\n&lt;div id=&quot;metadata&quot; style=&quot;display: none;&quot;&gt;\n  {{ task.input.metadata }}\n&lt;\/div&gt;\n\n&lt;crowd-form&gt;\n    &lt;input name=&quot;annotations&quot; id=&quot;annotations&quot; type=&quot;hidden&quot;&gt;\n\n     &lt;!-- Prevent crowd-form from creating its own button --&gt;\n    &lt;crowd-button form-action=&quot;submit&quot; style=&quot;display: none;&quot;&gt;&lt;\/crowd-button&gt;\n&lt;\/crowd-form&gt;\n\n&lt;!-- Custom annotation user interface is rendered here --&gt;\n&lt;div id=&quot;root&quot;&gt;&lt;\/div&gt;\n\n&lt;crowd-button id=&quot;submitButton&quot;&gt;Submit&lt;\/crowd-button&gt;\n\n&lt;script&gt;\n    document.querySelector('crowd-form').onsubmit = function() {\n        document.getElementById('annotations').value = JSON.stringify(JSON.parse(document.querySelector('pre').innerText));\n    };\n\n    document.getElementById('submitButton').onclick = function() {\n        document.querySelector('crowd-form').submit();\n    };\n&lt;\/script&gt;\n\n&lt;script src=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/js\/1.3e5a6849.chunk.js&quot;&gt;&lt;\/script&gt;\n&lt;script src=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/js\/main.96e12312.chunk.js&quot;&gt;&lt;\/script&gt;\n&lt;script src=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/js\/runtime~main.229c360f.js&quot;&gt;&lt;\/script&gt;\n<\/code><\/pre>\n<p>Code source<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1579189368977,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1592644375060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59717227",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70306493,
        "Question_title":"View train error metrics for Hugging Face Sagemaker model",
        "Question_body":"<p>I have trained a model using Hugging Face's integration with Amazon Sagemaker <a href=\"https:\/\/huggingface.co\/docs\/sagemaker\/train\" rel=\"nofollow noreferrer\">and their Hello World example<\/a>.<\/p>\n<p>I can easily calculate and view the metrics generated on the evaluation test set: accuracy, f-score, precision, recall etc. by calling <code>training_job_analytics<\/code> on the trained model: <code>huggingface_estimator.training_job_analytics.dataframe()<\/code><\/p>\n<p>How can I also see the same metrics on training sets (or even training error for each epoch)?<\/p>\n<p>Training code is basically the same as the link with extra parts of the docs added:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.huggingface import HuggingFace\n\n# optionally parse logs for key metrics\n# from the docs: https:\/\/huggingface.co\/docs\/sagemaker\/train#sagemaker-metrics\nmetric_definitions = [\n    {'Name': 'loss', 'Regex': &quot;'loss': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'learning_rate', 'Regex': &quot;'learning_rate': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_loss', 'Regex': &quot;'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_accuracy', 'Regex': &quot;'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_f1', 'Regex': &quot;'eval_f1': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_precision', 'Regex': &quot;'eval_precision': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_recall', 'Regex': &quot;'eval_recall': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_runtime', 'Regex': &quot;'eval_runtime': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_samples_per_second', 'Regex': &quot;'eval_samples_per_second': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'epoch', 'Regex': &quot;'epoch': ([0-9]+(.|e\\-)[0-9]+),?&quot;}\n]\n\n# hyperparameters, which are passed into the training job\nhyperparameters={\n    'epochs': 5,\n    'train_batch_size': batch_size,\n    'model_name': model_checkpoint,\n    'task': task,\n}\n\n# init the model (but not yet trained)\nhuggingface_estimator = HuggingFace(\n    entry_point='train.py',\n    source_dir='.\/scripts',\n    instance_type='ml.p3.2xlarge',\n    instance_count=1,\n    role=role,\n    transformers_version='4.6',\n    pytorch_version='1.7',\n    py_version='py36',\n    hyperparameters = hyperparameters,\n    metric_definitions=metric_definitions\n)\n# starting the train job with our uploaded datasets as input\nhuggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})\n\n# does not return metrics on training - only on eval!\nhuggingface_estimator.training_job_analytics.dataframe()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639148719370,
        "Question_score":0,
        "Question_tags":"python|nlp|amazon-sagemaker|huggingface-transformers",
        "Question_view_count":184,
        "Owner_creation_time":1437078651387,
        "Owner_last_access_time":1663950070697,
        "Owner_location":null,
        "Owner_reputation":901,
        "Owner_up_votes":150,
        "Owner_down_votes":10,
        "Owner_views":145,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This can be solved by increasing the number of epochs in training to a more realistic value.<\/p>\n<p>Currently, the model trains in fewer than 300 seconds (which is when the following timestamp would be recorded) and presumably the loss function.<\/p>\n<p>Changes to make:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>hyperparameters={\n    'epochs': 100, # increase the number of epochs to realistic value!\n    'train_batch_size': batch_size,\n    'model_name': model_checkpoint,\n    'task': task,\n}\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1639502011653,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1645119168563,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70306493",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63096583,
        "Question_title":"SageMaker: An error occurred (ModelError) when calling the InvokeEndpoint operation: unable to evaluate payload provided",
        "Question_body":"<p>I have a endpoint in Amazon SageMaker (Image-classification algorithm) in Jupyter notebook that works fine. In Lambda function works fine too, when I call the Lambda function from API Gateway, from test of API Gateway, works fine too.<\/p>\n<p>The problem is when I call the API from Postman according this answer: <a href=\"https:\/\/stackoverflow.com\/questions\/39660074\/post-image-data-using-postman\">&quot;Post Image data using POSTMAN&quot;<\/a><\/p>\n<p>The code in Lambda is:<\/p>\n<pre><code>import boto3\nimport json\nimport base64\n\nENDPOINT_NAME = &quot;DEMO-XGBoostEndpoint-Multilabel&quot;\nruntime= boto3.client(&quot;runtime.sagemaker&quot;)\nimagen_ = &quot;\/tmp\/imageToProcess.jpg&quot;\n\ndef write_to_file(save_path, data):\n    with open(save_path, &quot;wb&quot;) as f:\n        f.write(base64.b64decode(data))\n\ndef lambda_handler(event, context):\n    img_json = json.loads(json.dumps(event))\n\n    write_to_file(imagen_, json.dumps(event, indent=2))\n\n    with open(imagen_, &quot;rb&quot;) as image:\n        f = image.read()\n        b = bytearray(f)\n\n    payload = b\n\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                       ContentType=&quot;application\/x-image&quot;,\n                                       Body=payload)\n\n    #print(response)\n    result = json.loads(response[&quot;Body&quot;].read().decode())\n    print(result)\n    predicted_label=[]\n    classes = [&quot;chair&quot;, &quot;handbag&quot;, &quot;person&quot;, &quot;traffic light&quot;, &quot;clock&quot;]\n    for idx, val in enumerate(classes):\n        print(&quot;%s:%f &quot;%(classes[idx], result[idx]), end=&quot;&quot;)\n        predicted_label += (classes[idx], result[idx])\n\n    return {\n      &quot;statusCode&quot;: 200,\n      &quot;headers&quot;: { &quot;content-type&quot;: &quot;application\/json&quot;},\n      &quot;body&quot;:  predicted_label\n}\n<\/code><\/pre>\n<p>The error is:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/var\/task\/lambda_function.py&quot;, line 26, in lambda_handler\n    Body=payload)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 316, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 626, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;unable to evaluate payload provided&quot;. See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/DEMO-XGBoostEndpoint-Multilabel in account 866341179300 for more information. ```\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1595742856227,
        "Question_score":2,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":3996,
        "Owner_creation_time":1565376125573,
        "Owner_last_access_time":1612402865987,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1595997749087,
        "Answer_body":"<p>I resolved with <a href=\"https:\/\/medium.com\/swlh\/upload-binary-files-to-s3-using-aws-api-gateway-with-aws-lambda-2b4ba8c70b8e\" rel=\"nofollow noreferrer\">this<\/a> post:<\/p>\n<p>Thank all<\/p>\n<p>Finally the code in lambda function is:<\/p>\n<pre><code>import os\nimport boto3\nimport json\nimport base64\n\nENDPOINT_NAME = os.environ['endPointName']\nCLASSES = &quot;[&quot;chair&quot;, &quot;handbag&quot;, &quot;person&quot;, &quot;traffic light&quot;, &quot;clock&quot;]&quot;\nruntime= boto3.client(&quot;runtime.sagemaker&quot;)\n\ndef lambda_handler(event, context):\n    file_content = base64.b64decode(event['content'])\n\n    payload = file_content\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType=&quot;application\/x-image&quot;, Body=payload)\n\n    result = json.loads(response[&quot;Body&quot;].read().decode())\n    print(result)\n    predicted_label=[]\n    classes = CLASSES\n    for idx, val in enumerate(classes):\n       print(&quot;%s:%f &quot;%(classes[idx], result[idx]), end=&quot;&quot;)\n       predicted_label += (classes[idx], result[idx])\n\n    return {\n      &quot;statusCode&quot;: 200,\n      &quot;headers&quot;: { &quot;content-type&quot;: &quot;application\/json&quot;},\n      &quot;body&quot;:  predicted_label\n    }\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1597003004373,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1634524535896,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63096583",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53408927,
        "Question_title":"How to pass a bigger .csv files to amazon sagemaker for predictions using batch transform jobs",
        "Question_body":"<p>I created a custom model and deployed it on sagemaker. I am invoking the endpoint using batch transform jobs. It works if the input file is small, i.e, number of rows in the csv file is less. If I upload a file with around 200000 rows, I am getting this error in the cloudwatch logs.<\/p>\n\n<pre><code>2018-11-21 09:11:52.666476: W external\/org_tensorflow\/tensorflow\/core\/framework\/allocator.cc:113]\nAllocation of 2878368000 exceeds 10% of system memory.\n2018-11-21 09:11:53.166493: W external\/org_tensorflow\/tensorflow\/core\/framework\/allocator.cc:113] \nAllocation of 2878368000 exceeds 10% of system memory.\n[2018-11-21 09:12:02,544] ERROR in serving: &lt;_Rendezvous of RPC that \nterminated with:\n#011status = StatusCode.DEADLINE_EXCEEDED\n#011details = \"Deadline Exceeded\"\n#011debug_error_string = \"\n{\n\"created\": \"@1542791522.543282048\",\n\"description\": \"Error received from peer\",\n\"file\": \"src\/core\/lib\/surface\/call.cc\",\n\"file_line\": 1017,\n\"grpc_message\": \"Deadline Exceeded\",\n\"grpc_status\": 4\n}\n\"\n<\/code><\/pre>\n\n<p>Any ideas what might be going wrong. This is the transform function which I am using to create the transform job.<\/p>\n\n<pre><code>transformer =sagemaker.transformer.Transformer(\nbase_transform_job_name='Batch-Transform',\nmodel_name='sagemaker-tensorflow-2018-11-21-07-58-15-887',\ninstance_count=1,\ninstance_type='ml.m4.xlarge',\noutput_path='s3:\/\/2-n2m-sagemaker-json-output\/out_files\/'\n\n)\ninput_location = 's3:\/\/1-n2m-n2g-csv-input\/smal_sagemaker_sample.csv'\ntransformer.transform(input_location, content_type='text\/csv', split_type='Line')\n<\/code><\/pre>\n\n<p>The .csv file contains 2 columns for first and last name of customer, which I am then preprocessing it in the sagemaker itself using input_fn().<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1542792620897,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":1941,
        "Owner_creation_time":1444454434270,
        "Owner_last_access_time":1652071645660,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":140,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Question_last_edit_time":1542799396316,
        "Answer_body":"<p>The error looks to be coming from a GRPC client closing the connection before the server is able to respond. (There looks to be an existing feature request for the sagemaker tensorflow container on <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/issues\/46\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/issues\/46<\/a> to make this timeout configurable)<\/p>\n\n<p>You could try out a few things with the sagemaker Transformer to limit the size of each individual request so that it fits within the timeout:<\/p>\n\n<ul>\n<li>Set a <code>max_payload<\/code> to a smaller value, say 2-3 MB (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB\" rel=\"nofollow noreferrer\">the default is 6 MB<\/a>)<\/li>\n<li>If your instance metrics indicate it has compute \/ memory resources to spare, try <code>max_concurrent_transforms<\/code> > 1 to make use of multiple workers<\/li>\n<li>Split up your csv file into multiple input files. With a bigger dataset, you could also increase the instance count to fan out processing<\/li>\n<\/ul>",
        "Answer_comment_count":0,
        "Answer_creation_time":1544503147007,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53408927",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56740609,
        "Question_title":"Randomforest in amazon aws sagemaker?",
        "Question_body":"<p>I am looking to recreate a randomforest model built locally, and deploy it through sagemaker. The model is very basic, but for comparison I would like to use the same in sagemaker. I don't see randomforest among sagemaker's built in algorithms (which seems weird) - is my only option to go the route of <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/train-and-host-scikit-learn-models-in-amazon-sagemaker-by-building-a-scikit-docker-container\/\" rel=\"nofollow noreferrer\">deploying my own custom model<\/a>? Still learning about containers, and it seems like a lot of work for something that is just a simple randomforestclassifier() call locally. I just want to baseline against the out of the box randomforest model, and show that it works the same when deployed through AWS sagemaker.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1561393633193,
        "Question_score":4,
        "Question_tags":"amazon-web-services|docker|containers|random-forest|amazon-sagemaker",
        "Question_view_count":4295,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":null,
        "Answer_body":"<p><em>edit 03\/30\/2020: adding a link to the the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"noreferrer\">SageMaker Sklearn random forest demo<\/a><\/em><\/p>\n\n<p><br\/><\/p>\n\n<p>in SageMaker you have 3 options to write scientific code:<\/p>\n\n<ul>\n<li><strong>Built-in algorithms<\/strong><\/li>\n<li><strong>Open-source pre-written containers<\/strong> (available\nfor sklearn, tensorflow, pytorch, mxnet, chainer. Keras can be\nwritten in the tensorflow and mxnet containers)<\/li>\n<li><strong>Bring your own container<\/strong> (for R for example)<\/li>\n<\/ul>\n\n<p><strong>At the time of writing this post there is no random forest classifier nor regressor in the built-in library<\/strong>. There is an algorithm called <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-built-in-amazon-sagemaker-random-cut-forest-algorithm-for-anomaly-detection\/\" rel=\"noreferrer\">Random Cut Forest<\/a> in the built-in library but it is an unsupervised algorithm for anomaly detection, a different use-case than the <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\" rel=\"noreferrer\">scikit-learn random forest<\/a> used in a supervised fashion (also <a href=\"https:\/\/stackoverflow.com\/questions\/56728230\/aws-sagemaker-randomcutforest-rcf-vs-scikit-lean-randomforest-rf?noredirect=1&amp;lq=1\">answered in StackOverflow here<\/a>). But it is easy to use the open-source pre-written scikit-learn container to implement your own. There is a <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"noreferrer\">demo showing how to use Sklearn's random forest in SageMaker<\/a>, with training orchestration bother from the high-level SDK and <code>boto3<\/code>. You can also use this other <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_iris\/Scikit-learn%20Estimator%20Example%20With%20Batch%20Transform.ipynb\" rel=\"noreferrer\">public sklearn-on-sagemaker demo<\/a> and change the model. A benefit of the pre-written containers over the \"Bring your own\" option is that the dockerfile is already written, and web serving stack too.<\/p>\n\n<p>Regarding your surprise that Random Forest is not featured in the built-in algos, the library and its 18 algos already cover a rich set of use-cases. For example for supervised learning over structured data (the usual use-case for the random forest), if you want to stick to the built-ins, depending on your priorities (accuracy, inference latency, training scale, costs...) you can use SageMaker XGBoost (XGBoost has been winning tons of datamining competitions - every winning team in the top10 of the KDDcup 2015 used XGBoost <a href=\"https:\/\/arxiv.org\/pdf\/1603.02754.pdf\" rel=\"noreferrer\">according to the XGBoost paper<\/a> - and scales well) and linear learner, which is extremely fast at inference and can be trained at scale, in mini-batch fashion over GPU(s). <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/fact-machines-howitworks.html\" rel=\"noreferrer\">Factorization Machines<\/a> (linear + 2nd degree interaction with weights being column embedding dot-products) and <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-supports-knn-classification-and-regression\/\" rel=\"noreferrer\">SageMaker kNN<\/a> are other options. Also, things are not frozen in stone, and the list of built-in algorithms is being improved fast.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1561491486350,
        "Answer_score":10,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":1585586806547,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56740609",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58892606,
        "Question_title":"Sagemaker can't find paths in container",
        "Question_body":"<p>This is a hard situation to describe.<\/p>\n\n<p>I have a python model train script at:<\/p>\n\n<p><code>myproject\/opt\/program\/train<\/code><\/p>\n\n<p>This gets a file at <code>.\/opt\/ml\/input\/data\/external\/train.csv<\/code><\/p>\n\n<p>When I do <code>python3 opt\/program\/train<\/code> the training runs fine locally.<\/p>\n\n<p>Then I containerize the project and copy <code>opt<\/code> to <code>\/opt<\/code> in my Dockerfile.<\/p>\n\n<p>Now when I run <code>docker run &lt;image name&gt; train<\/code> it also trains fine.<\/p>\n\n<p>Then I deploy the image to SageMaker, create an estimator, and call <code>model.fit(my_data)<\/code> I get:<\/p>\n\n<p><code>Exception during training: [Errno 2] File b'.\/opt\/ml\/input\/data\/external\/train.csv' does not exist<\/code><\/p>\n\n<p>It's definitely there, I was able to train by running the container myself.  Also running the container and exploring the file system I can find the file.<\/p>\n\n<p>So I think I have some filesystem misunderstanding.  From the root of the container, all of these seem to have equivalent outputs.<\/p>\n\n<pre><code>root@798ffe7364c6:\/# ls opt\nml  program\nroot@798ffe7364c6:\/# ls \/opt\nml  program\nroot@798ffe7364c6:\/# ls .\/opt\nml  program\n<\/code><\/pre>\n\n<p>I'm trying to come up with a way to have one path that will work locally, in the container, and on AWS.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1573920322537,
        "Question_score":0,
        "Question_tags":"python|docker|amazon-sagemaker",
        "Question_view_count":2448,
        "Owner_creation_time":1360164540017,
        "Owner_last_access_time":1663899252397,
        "Owner_location":"Columbus, OH",
        "Owner_reputation":11190,
        "Owner_up_votes":136,
        "Owner_down_votes":11,
        "Owner_views":365,
        "Question_last_edit_time":1573920635263,
        "Answer_body":"<p>I was missing the fact that SageMaker looks for your data channels in S3 and copies those to your container at <code>\/opt\/ml\/input\/data<\/code><\/p>\n\n<p>By default it seems to use <code>training<\/code> and <code>validation<\/code> as the channel names.  Therefore, in my example above, it would have never copied data from my <code>external<\/code> folder on S3 to the right <code>external<\/code> folder in my container.  In fact, I discovered it was copying it instead to <code>\/opt\/ml\/input\/data\/training\/external\/train.csv<\/code>.<\/p>\n\n<p>To resolve this, I would have either had to change my folder names, or use <code>InputDataConfig<\/code> to define other channels.  I chose the later and was able to get it working.<\/p>\n\n<p>More info on <code>InputDataConfig<\/code> here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1574100008957,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58892606",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49064183,
        "Question_title":"How to set the percentage of inference calls when performing A\/B testing using AWS sagemaker?",
        "Question_body":"<p>I'm new to sagemaker. I'm trying to figure out how to perform A\/B testing using AWS sagemaker. I understand setting the train_instance_count will distribute the training across two instances. But how do I specify the set the percentage of inference calls each model will handle and perform A\/B testing? \nThis is all I could find from the docs <\/p>\n\n<blockquote>\n  <p>\"Amazon SageMaker can also manage model A\/B testing for you. You can\n  configure the endpoint to spread traffic across as many as five\n  different models and set the percentage of inference calls you want\n  each one to handle. You can change all of this on the fly, giving you\n  a lot of flexibility to run experiments and determine which model\n  produces the most accurate results in the real world.\"<\/p>\n<\/blockquote>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1519974037750,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-ec2|amazon-sagemaker",
        "Question_view_count":1042,
        "Owner_creation_time":1450260166773,
        "Owner_last_access_time":1663955203357,
        "Owner_location":null,
        "Owner_reputation":1587,
        "Owner_up_votes":123,
        "Owner_down_votes":8,
        "Owner_views":540,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can have multiple Production Variants behind an Amazon SageMaker endpoint. Each production variant has an initial variant weight and based on the ratio of each variant weight to the total sum of weights, SageMaker can distribute the calls to each of the models. For example, if you have only one production variant with a weight of 1, all traffic will go to this variant. If you add another production variant with an initial weight of 2, the new variant will get 2\/3 of the traffic and the first variant will get 1\/3. <\/p>\n\n<p>You can see more details on ProductionVariant on Amazon SageMaker documentations here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_ProductionVariant.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_ProductionVariant.html<\/a> <\/p>\n\n<p>You can provide an array of ProductionVariants when you \"Create Endpoint Configuration\": <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpointConfig.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpointConfig.html<\/a> , and you can update the variants with \"Update Endpoint Weights and Capacities\" call: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_UpdateEndpointWeightsAndCapacities.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_UpdateEndpointWeightsAndCapacities.html<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1520190662820,
        "Answer_score":3,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49064183",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73728499,
        "Question_title":"How to update an existing model in AWS sagemaker >= 2.0",
        "Question_body":"<p>I have an XGBoost model currently in production using AWS sagemaker and making real time inferences. After a while, I would like to update the model with a newer one trained on more data and keep everything as is (e.g. same endpoint, same inference procedure, so really no changes aside from the model itself)<\/p>\n<p>The current deployment procedure is the following :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.xgboost.model import XGBoostModel\nfrom sagemaker.xgboost.model import XGBoostPredictor\n\nxgboost_model = XGBoostModel(\n    model_data = &lt;S3 url&gt;,\n    role = &lt;sagemaker role&gt;,\n    entry_point = 'inference.py',\n    source_dir = 'src',\n    code_location = &lt;S3 url of other dependencies&gt;\n    framework_version='1.5-1',\n    name = model_name)\n\nxgboost_model.deploy(\n    instance_type='ml.c5.large',\n    initial_instance_count=1,\n    endpoint_name = model_name)\n<\/code><\/pre>\n<p>Now that I updated the model a few weeks later, I would like to re-deploy it. I am aware that the <code>.deploy()<\/code> method creates an endpoint and an endpoint configuration so it does it all. I cannot simply re-run my script again since I would encounter an error.<\/p>\n<p>In previous versions of sagemaker I could have updated the model with an extra argument passed to the <code>.deploy()<\/code> method called <code>update_endpoint = True<\/code>. In sagemaker &gt;=2.0 this is a no-op. Now, in sagemaker &gt;= 2.0, I need to use the predictor object as stated in the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html\" rel=\"nofollow noreferrer\">documentation<\/a>. So I try the following :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>predictor = XGBoostPredictor(model_name)\npredictor.update_endpoint(model_name= model_name)\n<\/code><\/pre>\n<p>Which actually updates the endpoint according to a new endpoint configuration. However, I do not know what it is updating... I do not specify in the above 2 lines of code that we need to considering the new <code>xgboost_model<\/code> trained on more data...  so where do I tell the update to take a more recent model?<\/p>\n<p>Thank you!<\/p>\n<p><strong>Update<\/strong><\/p>\n<p>I believe that I need to be looking at production variants as stated in their documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-ab-testing.html\" rel=\"nofollow noreferrer\">here<\/a>. However, their whole tutorial is based on the amazon sdk for python (boto3) which has artifacts that are hard to manage when I have difference entry points for each model variant (e.g. different <code>inference.py<\/code> scripts).<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1663233254363,
        "Question_score":0,
        "Question_tags":"python-3.x|deployment|xgboost|amazon-sagemaker",
        "Question_view_count":41,
        "Owner_creation_time":1640956373383,
        "Owner_last_access_time":1663928794863,
        "Owner_location":null,
        "Owner_reputation":309,
        "Owner_up_votes":56,
        "Owner_down_votes":1,
        "Owner_views":15,
        "Question_last_edit_time":1663319367852,
        "Answer_body":"<p>Since I found an answer to my own question I will post it here for those who encounter the same problem.<\/p>\n<p>I ended up re-coding all my deployment script using the boto3 SDK rather than the sagemaker SDK (or a mix of both as some documentation suggest).<\/p>\n<p>Here's the whole script that shows how to create a sagemaker model object, an endpoint configuration and an endpoint to deploy the model on for the first time. In addition, it shows what to do how to update the endpoint with a newer model (which was my main question)<\/p>\n<p>Here's the code to do all 3 in case you want to bring your own model and update it safely in production using sagemaker :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\nimport time\nfrom datetime import datetime\nfrom sagemaker import image_uris\nfrom fileManager import *  # this is a local script for helper functions\n\n# name of zipped model and zipped inference code\nCODE_TAR = 'your_inference_code_and_other_artifacts.tar.gz'\nMODEL_TAR = 'your_saved_xgboost_model.tar.gz'\n\n# sagemaker params\nsmClient = boto3.client('sagemaker')\nsmRole = &lt;your_sagemaker_role&gt;\nbucket = sagemaker.Session().default_bucket()\n\n# deploy algorithm\nclass Deployer:\n\n    def __init__(self, modelName, deployRetrained=False):\n        self.modelName=modelName\n        self.deployRetrained = deployRetrained\n        self.prefix = &lt;S3_model_path_prefix&gt;\n    \n    def deploy(self):\n        '''\n        Main method to create a sagemaker model, create an endpoint configuration and deploy the model. If deployRetrained\n        param is set to True, this method will update an already existing endpoint.\n        '''\n        # define model name and endpoint name to be used for model deployment\/update\n        model_name = self.modelName + &lt;any_suffix&gt;\n        endpoint_config_name = self.modelName + '-%s' %datetime.now().strftime('%Y-%m-%d-%HH%M')\n        endpoint_name = self.modelName\n        \n        # deploy model for the first time\n        if not self.deployRetrained:\n            print('Deploying for the first time')\n\n            # here you should copy and zip the model dependencies that you may have (such as preprocessors, inference code, config code...)\n            # mine were zipped into the file called CODE_TAR\n\n            # upload model and model artifacts needed for inference to S3\n            uploadFile(list_files=[MODEL_TAR, CODE_TAR], prefix = self.prefix)\n\n            # create sagemaker model and endpoint configuration\n            self.createSagemakerModel(model_name)\n            self.createEndpointConfig(endpoint_config_name, model_name)\n\n            # deploy model and wait while endpoint is being created\n            self.createEndpoint(endpoint_name, endpoint_config_name)\n            self.waitWhileCreating(endpoint_name)\n        \n        # update model\n        else:\n            print('Updating existing model')\n\n            # upload model and model artifacts needed for inference (here the old ones are replaced)\n            # make sure to make a backup in S3 if you would like to keep the older models\n            # we replace the old ones and keep the same names to avoid having to recreate a sagemaker model with a different name for the update!\n            uploadFile(list_files=[MODEL_TAR, CODE_TAR], prefix = self.prefix)\n\n            # create a new endpoint config that takes the new model\n            self.createEndpointConfig(endpoint_config_name, model_name)\n\n            # update endpoint\n            self.updateEndpoint(endpoint_name, endpoint_config_name)\n\n            # wait while endpoint updates then delete outdated endpoint config once it is InService\n            self.waitWhileCreating(endpoint_name)\n            self.deleteOutdatedEndpointConfig(model_name, endpoint_config_name)\n\n    def createSagemakerModel(self, model_name):\n        ''' \n        Create a new sagemaker Model object with an xgboost container and an entry point for inference using boto3 API\n        '''\n        # Retrieve that inference image (container)\n        docker_container = image_uris.retrieve(region=region, framework='xgboost', version='1.5-1')\n\n        # Relative S3 path to pre-trained model to create S3 model URI\n        model_s3_key = f'{self.prefix}\/'+ MODEL_TAR\n\n        # Combine bucket name, model file name, and relate S3 path to create S3 model URI\n        model_url = f's3:\/\/{bucket}\/{model_s3_key}'\n\n        # S3 path to the necessary inference code\n        code_url = f's3:\/\/{bucket}\/{self.prefix}\/{CODE_TAR}'\n        \n        # Create a sagemaker Model object with all its artifacts\n        smClient.create_model(\n            ModelName = model_name,\n            ExecutionRoleArn = smRole,\n            PrimaryContainer = {\n                'Image': docker_container,\n                'ModelDataUrl': model_url,\n                'Environment': {\n                    'SAGEMAKER_PROGRAM': 'inference.py', #inference.py is at the root of my zipped CODE_TAR\n                    'SAGEMAKER_SUBMIT_DIRECTORY': code_url,\n                }\n            }\n        )\n    \n    def createEndpointConfig(self, endpoint_config_name, model_name):\n        ''' \n        Create an endpoint configuration (only for boto3 sdk procedure) and set production variants parameters.\n        Each retraining procedure will induce a new variant name based on the endpoint configuration name.\n        '''\n        smClient.create_endpoint_config(\n            EndpointConfigName=endpoint_config_name,\n            ProductionVariants=[\n                {\n                    'VariantName': endpoint_config_name,\n                    'ModelName': model_name,\n                    'InstanceType': INSTANCE_TYPE,\n                    'InitialInstanceCount': 1\n                }\n            ]\n        )\n\n    def createEndpoint(self, endpoint_name, endpoint_config_name):\n        '''\n        Deploy the model to an endpoint\n        '''\n        smClient.create_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=endpoint_config_name)\n    \n    def deleteOutdatedEndpointConfig(self, name_check, current_endpoint_config):\n        '''\n        Automatically detect and delete endpoint configurations that contain a string 'name_check'. This method can be used\n        after a retrain procedure to delete all previous endpoint configurations but keep the current one named 'current_endpoint_config'.\n        '''\n        # get a list of all available endpoint configurations\n        all_configs = smClient.list_endpoint_configs()['EndpointConfigs']\n\n        # loop over the names of endpoint configs\n        names_list = []\n        for config_dict in all_configs:\n            endpoint_config_name = config_dict['EndpointConfigName']\n\n            # get only endpoint configs that contain name_check in them and save names to a list\n            if name_check in endpoint_config_name:\n                names_list.append(endpoint_config_name)\n        \n        # remove the current endpoint configuration from the list (we do not want to detele this one since it is live)\n        names_list.remove(current_endpoint_config)\n\n        for name in names_list:\n            try:\n                smClient.delete_endpoint_config(EndpointConfigName=name)\n                print('Deleted endpoint configuration for %s' %name)\n            except:\n                print('INFO : No endpoint configuration was found for %s' %endpoint_config_name)\n\n    def updateEndpoint(self, endpoint_name, endpoint_config_name):\n        ''' \n        Update existing endpoint with a new retrained model\n        '''\n        smClient.update_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=endpoint_config_name,\n            RetainAllVariantProperties=True)\n    \n    def waitWhileCreating(self, endpoint_name):\n        ''' \n        While the endpoint is being created or updated sleep for 60 seconds.\n        '''\n        # wait while creating or updating endpoint\n        status = smClient.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n        print('Status: %s' %status)\n        while status != 'InService' and status !='Failed':\n            time.sleep(60)\n            status = smClient.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n            print('Status: %s' %status)\n        \n        # in case of a deployment failure raise an error\n        if status == 'Failed':\n            raise ValueError('Endpoint failed to deploy')\n\nif __name__==&quot;__main__&quot;:\n    deployer = Deployer('churnmodel', deployRetrained=True)\n    deployer.deploy()\n<\/code><\/pre>\n<p>Final comments :<\/p>\n<ul>\n<li><p>The sagemaker <a href=\"https:\/\/docs.amazonaws.cn\/en_us\/sagemaker\/latest\/dg\/realtime-endpoints-deployment.html\" rel=\"nofollow noreferrer\">documentation<\/a> mentions all this but fails to state that you can provide an 'entry_point' to the <code>create_model<\/code> method as well as a 'source_dir' for inference dependencies (e.g. normalization artifacts). It can be done as seen in <code>PrimaryContainer<\/code> argument.<\/p>\n<\/li>\n<li><p>my <code>fileManager.py<\/code> script just contains basic functions to make tar files, upload and download to and from my S3 paths. To simplify the class, I have not included them in.<\/p>\n<\/li>\n<li><p>The method deleteOutdatedEndpointConfig may seem like an overkill with an unnecessary loop and checks, I do so because I have multiple endpoint configurations to handle and wanted to remove the ones that weren't live AND contain the string <code>name_check<\/code> (I do not know the exact name of the configuration since there is a datetime suffix). Feel free to simplify it or remove it all together if you feel like it.<\/p>\n<\/li>\n<\/ul>\n<p>Hope it helps.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1663924957330,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1663925308150,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73728499",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66899120,
        "Question_title":"Validation error on Role name when running AWS SageMaker linear-learner locally",
        "Question_body":"<p>I'm trying to build a machine learning model locally using AWS SageMaker, but I got a validation error on IAM Role name. Although it's the exact role name that I created on the console.<\/p>\n<p>This is my code<\/p>\n<pre><code>    import boto3\n    import sagemaker\n    from sagemaker import get_execution_role\n    from sagemaker.amazon.amazon_estimator import image_uris\n    from sagemaker.amazon.amazon_estimator import RecordSet\n\n    sess = sagemaker.Session()\n\n\n    bucket = sagemaker.Session().default_bucket()\n    prefix = 'sagemaker\/ccard19'\n\n    role ='arn:aws:iam::94911111111542:role\/SageMaker-Full-Access '\n\n    container = image_uris.retrieve('linear-learner',boto3.Session().region_name)\n    \n    # Some other code\n\n   linear = sagemaker.LinearLearner(role=role,\n                                               instance_count=1,\n                                               instance_type='ml.m4.xlarge',\n                                               predictor_type='binary_classifier')\n  \n  # Some other code\n\n  ### Fit the classifier\n  linear.fit([train_records,val_records,test_records], wait=True, logs='All')\n\n<\/code><\/pre>\n<p>And this is the error message<\/p>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: 1 validation error detected: Value 'arn:aws:iam::949010940542:role\/SageMaker-Full-Access ' at 'roleArn' failed to satisfy constraint: Member must satisfy regular expression pattern: ^arn:aws[a-z\\-]*:iam::\\d{12}:role\/?[a-zA-Z_0-9+=,.@\\-_\/]+$\n<\/code><\/pre>\n<p>Any Help please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617256728180,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":560,
        "Owner_creation_time":1378039539503,
        "Owner_last_access_time":1663912967613,
        "Owner_location":null,
        "Owner_reputation":340,
        "Owner_up_votes":78,
        "Owner_down_votes":1,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You have <strong>space<\/strong> in the name. It should be:<\/p>\n<pre><code>role ='arn:aws:iam::94911111111542:role\/SageMaker-Full-Access'\n<\/code><\/pre>",
        "Answer_comment_count":1,
        "Answer_creation_time":1617256809810,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66899120",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70156631,
        "Question_title":"How to pass additional parameters (as a dict) to sagemeker custom inference container?",
        "Question_body":"<p>Status:<\/p>\n<ul>\n<li>Custom container is built using the doc - <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own<\/a><\/li>\n<li>predict.py is coded to accommodate the custom inference script and its working well<\/li>\n<li>Using the classsagemaker.model.Model() class to pass the trained model.tar.gz and custom container image inorder to deploy the model<\/li>\n<\/ul>\n<p>Challenge:<\/p>\n<ul>\n<li>In the same Model class there is a ENV  parameter through which we can apparently send the environment variables to the custom image<\/li>\n<li>Tried passing a python dict to this , but facing difficulty to read this json dict inide the predict.py script<\/li>\n<\/ul>\n<p>Somebody faced the same difficulty ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1638197351547,
        "Question_score":0,
        "Question_tags":"python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":183,
        "Owner_creation_time":1604146329127,
        "Owner_last_access_time":1662133039343,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can pass your environment dict in your Model as:<\/p>\n<pre><code>Model(\n.\n.\nenv= {&quot;my_env&quot;: &quot;my_env_value&quot;}\n.\n.\n)\n<\/code><\/pre>\n<p>SageMaker will pass the enviroments dict to your container and you can access it in your predict.py script for example with:<\/p>\n<pre><code>my_env = os.environ.get('my_env',&quot;env key not set in Model&quot;)\nprint(my_env)\n<\/code><\/pre>\n<p>If your env dict was passed to your Model containing they <code>my_env<\/code> then you will receive the output : <code>my_env_value<\/code>. Else, then you will receive <code>env key not set in Model<\/code><\/p>\n<p>I work for AWS and my opinions are my own.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1645489319000,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70156631",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62187748,
        "Question_title":"Change datacapture encoding data to csv",
        "Question_body":"<p>I'm using sagemaker model monitor.<\/p>\n\n<p>When capturing data, it outputs the following json file.<\/p>\n\n<pre><code>{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text\/csv\",\"mode\":\"INPUT\",\"data\":\"MSwwLjUzLDAuNDIsMC4xMzUsMC42NzcsMC4yNTY1LDAuMTQxNSwwLjIx\",\"encoding\":\"BASE64\"},\"endpointOutput\":{\"observedContentType\":\"text\/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"MTEuNjQzNDU1NTA1MzcxMDk0\",\"encoding\":\"BASE64\"}},\"eventMetadata\":{\"eventId\":\"33404924-c0d4-4044-9dc2-1e1f5575cb0a\",\"inferenceTime\":\"2020-06-04T05:45:45Z\"},\"eventVersion\":\"0\"}\n<\/code><\/pre>\n\n<p>I want the encoding to be csv but somehow it outputs base64.<br>\nWhen or where do we change the setting of the encoding?<br>\nIs it during the invoking the endpoint? or set when making endpoint config.<br>\nI looked for some documents but I couldn't find it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591250114543,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":620,
        "Owner_creation_time":1532422348877,
        "Owner_last_access_time":1606441872017,
        "Owner_location":null,
        "Owner_reputation":27,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I just came across this same problem! Seems like you need to specify <code>CaptureContentTypeHeader<\/code> params to tell SageMaker which content type headers to treat as CSV (or JSON), versus the default which is to base64 encode the payload!<\/p>\n<p>So e.g. adding the following to your <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpointConfig.html\" rel=\"nofollow noreferrer\">CreateEndpointConfig<\/a> call or boto3\/sagemaker SDK equivalent should fix it:<\/p>\n<pre><code>{\n   &quot;DataCaptureConfig&quot;: { \n      &quot;CaptureContentTypeHeader&quot;: { \n         &quot;CsvContentTypes&quot;: [ &quot;text\/csv&quot; ]\n      },\n   }\n}\n<\/code><\/pre>\n<p>I guess this is to allow for non-standard Content-Type headers? Providing a layer of config to resolve e.g:<\/p>\n<ul>\n<li><code>application\/x-mycoolmodel<\/code> -&gt; <code>JSON<\/code>, versus<\/li>\n<li><code>application\/x-secretsauce<\/code> -&gt; <code>BASE64<\/code><\/li>\n<\/ul>",
        "Answer_comment_count":0,
        "Answer_creation_time":1593091560010,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62187748",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71197045,
        "Question_title":"How to pass the experiment configuration to a SagemakerTrainingOperator while training?",
        "Question_body":"<p>Idea:<\/p>\n<ul>\n<li>To use experiments and trials to log the training parameters and artifacts in sagemaker while using MWAA as the pipeline orchestrator<\/li>\n<\/ul>\n<p>I am using the training_config to create the dict to pass the training configuration to the Tensorflow estimator, but there is no parameter to pass the experiment configuration<\/p>\n<pre><code>tf_estimator = TensorFlow(entry_point='train_model.py',\n                                      source_dir= source\n                                      role=sagemaker.get_execution_role(),\n                                      instance_count=1,\n                                      framework_version='2.3.0',\n                                      instance_type=instance_type,\n                                      py_version='py37',\n                                      script_mode=True,\n                                      enable_sagemaker_metrics = True,\n                                      metric_definitions=metric_definitions,\n                                      output_path=output\n\nmodel_training_config = training_config(\n                    estimator=tf_estimator,\n                    inputs=input\n                    job_name=training_jobname,\n                )\n    \n\n\n\ntraining_task = SageMakerTrainingOperator(\n                    task_id=test_id,\n                    config=model_training_config,\n                    aws_conn_id=&quot;airflow-sagemaker&quot;,  \n                    print_log=True,\n                    wait_for_completion=True,\n                    check_interval=60  \n                )\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1645378809553,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|mwaa",
        "Question_view_count":128,
        "Owner_creation_time":1604146329127,
        "Owner_last_access_time":1662133039343,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The only way that i found right now is to use the CreateTrainigJob API (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html#sagemaker-CreateTrainingJob-request-RoleArn\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html#sagemaker-CreateTrainingJob-request-RoleArn<\/a>). The following steps are needed:<\/p>\n<ul>\n<li>I am not sure if this will work with Bring your own script method for E.g with a Tensorflow estimator<\/li>\n<li>it works with a build your own container approach<\/li>\n<li>Using the CreateTrainigJob API i created the configs which in turn includes all the needed configs like - training, experiment, algporthm etc and passed that to SagemakerTrainingOperator<\/li>\n<\/ul>",
        "Answer_comment_count":0,
        "Answer_creation_time":1646393704707,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71197045",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51517103,
        "Question_title":"Deploy my own tensorflow model on a virtual machine with AWS",
        "Question_body":"<p>I have a Tensorflow model which is working perfectly fine on my laptop (Tf 1.8 on OS HighSierra). However, I wanted to scale my operations up and use Amazon's Virtual Machine to run predictions faster. What is the best way to use my saved model and classify images in jpeg format which are stored locally? Thank you! <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1532515840557,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-ec2|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":196,
        "Owner_creation_time":1530739110150,
        "Owner_last_access_time":1649240672773,
        "Owner_location":null,
        "Owner_reputation":391,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":57,
        "Question_last_edit_time":null,
        "Answer_body":"<p>you have two options:<\/p>\n\n<p>1) Start a virtual machine on AWS (known as an Amazon EC2 instance). You can pick from many different instance types, including GPU instances. You'll have full administrative access on this machine, meaning that you can copy you TF model to it and predict just like you would on your own machine. <\/p>\n\n<p>More details on getting started with EC2 here: <a href=\"https:\/\/aws.amazon.com\/ec2\/getting-started\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/ec2\/getting-started\/<\/a> <\/p>\n\n<p>I would also recommend using the Deep Learning Amazon Machine Image, which bundles all the popular ML\/DL tools as well as the NVIDIA environment for GPU training\/prediction : <a href=\"https:\/\/aws.amazon.com\/machine-learning\/amis\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/machine-learning\/amis\/<\/a><\/p>\n\n<p>2) If you don't want to manage virtual machines, I'd recommend looking at Amazon SageMaker. You'll be able to import your TF model and to deploy it on fully-managed infrastructure for prediction. <\/p>\n\n<p>Here's a sample notebook showing you how to bring your own TF model to SageMaker: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/tensorflow_iris_byom\/tensorflow_BYOM_iris.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/tensorflow_iris_byom\/tensorflow_BYOM_iris.ipynb<\/a><\/p>\n\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1532521671907,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51517103",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72491505,
        "Question_title":"SageMaker Inference for a video input",
        "Question_body":"<p>I wonder if it's possible to run SageMaker Inference or Batch Transform job directly for a video input (.mp4 or another format)?<\/p>\n<p>If no could you please advice the best practice that might be used for pre-processing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654268345127,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":93,
        "Owner_creation_time":1442786553537,
        "Owner_last_access_time":1664039462827,
        "Owner_location":"Kyiv",
        "Owner_reputation":71,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":1654268469487,
        "Answer_body":"<p>Asynchronous inference could be a good option for this use case. There is a blog published by AWS that talks about how you can do this.<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-computer-vision-inference-on-large-videos-with-amazon-sagemaker-asynchronous-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-computer-vision-inference-on-large-videos-with-amazon-sagemaker-asynchronous-endpoints\/<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1654270724137,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72491505",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60816944,
        "Question_title":"AWS Sagemaker Notebook with multiple users",
        "Question_body":"<p>I am still new in AWS sagemaker. Working on a architecture where we would have an AWS sagemaker notebook. There would be multiple users, I want that students don`t see each other work. would I need to do that in terminal? or we can do that in notebook itself?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1584978635243,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":1907,
        "Owner_creation_time":1513883236660,
        "Owner_last_access_time":1663906475810,
        "Owner_location":"Noida, Uttar Pradesh, India",
        "Owner_reputation":465,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":46,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The simplest way is to create a small notebook instance for each student. This way you can have the needed isolation and also the responsibility of each student for their notebook to stop them when they are not in use.<\/p>\n\n<p>The smallest instance type <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">costs<\/a> $0.0464 per hour. If you have it running 24\/7 it costs about $30 per month. But if the students are responsible and stop their instances when they are not using them, it can be about $1 for 20 hours of work.<\/p>\n\n<p>If you want to enable isolation to the notebooks, you can use the ability to presign the URL that is used to open the Jupyter interface. See here on the way to use the CLI to create the URL: <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-presigned-notebook-instance-url.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-presigned-notebook-instance-url.html<\/a>. It is also supported in other SDK.<\/p>\n\n<pre><code>create-presigned-notebook-instance-url\n--notebook-instance-name &lt;student-instance-name&gt;\n--session-expiration-duration-in-seconds 3600\n<\/code><\/pre>\n\n<p>You can integrate it into the internal portal that you have in your institute. <\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1585041076743,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1585077695676,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60816944",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48507471,
        "Question_title":"AWS Sagemaker custom user algorithms: how to take advantage of extra instances",
        "Question_body":"<p>This is a fundamental AWS Sagemaker question. When I run training with one of Sagemaker's built in algorithms I am able to take advantage of the massive speedup from distributing the job to many instances by increasing the instance_count argument of the training algorithm. However, when I package my own custom algorithm then increasing the instance count seems to just duplicate the training on every instance, leading to no speedup. <\/p>\n\n<p>I suspect that when I am packaging my own algorithm there is something special I need to do to control how it handles the training differently for a particular instance inside of the my custom train() function (otherwise, how would it know how the job should be distributed?), but I have not been able to find any discussion of how to do this online. <\/p>\n\n<p>Does anyone know how to handle this? Thank you very much in advance.<\/p>\n\n<p>Specific examples:\n=> It works well in a standard algorithm: I verified that increasing train_instance_count in the first documented sagemaker example speeds things up here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a><\/p>\n\n<p>=> It does not work in my custom algorithm. I tried taking the standard sklearn build-your-own-model example and adding a few extra sklearn variants inside of the training and then printing out results to compare. When I increase the train_instance_count that is passed to the Estimator object, it runs the same training on every instance, so the output gets duplicated across each instance (the printouts of the results are duplicated) and there is no speedup.\nThis is the sklearn example base: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a> . The third argument of the Estimator object partway down in this notebook is what lets you control the number of training instances.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1517249392277,
        "Question_score":3,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1369,
        "Owner_creation_time":1368093601223,
        "Owner_last_access_time":1599850446873,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Distributed training requires having a way to sync the results of the training between the training workers. Most of the traditional libraries, such as scikit-learn are designed to work with a single worker, and can't just be used in a distributed environment. Amazon SageMaker is distributing the data across the workers, but it is up to you to make sure that the algorithm can benefit from the multiple workers. Some algorithms, such as Random Forest, are easier to take advantage of the distribution, as each worker can build a different part of the forest, but other algorithms need more help. <\/p>\n\n<p>Spark MLLib has distributed implementations of popular algorithms such as k-means, logistic regression, or PCA, but these implementations are not good enough for some cases. Most of them were too slow and some even crushed when a lot of data was used for the training. The Amazon SageMaker team reimplemented many of these algorithms from scratch to benefit from the scale and economics of the cloud (20 hours of one instance costs the same as 1 hour of 20 instances, just 20 times faster). Many of these algorithms are now more stable and much faster beyond the linear scalability. See more details here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html<\/a><\/p>\n\n<p>For the deep learning frameworks (TensorFlow and MXNet) SageMaker is using the built-in parameters server that each one is using, but it is taking the heavy lifting of the building the cluster and configuring the instances to communicate with it. <\/p>",
        "Answer_comment_count":5,
        "Answer_creation_time":1517706762273,
        "Answer_score":2,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48507471",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56799763,
        "Question_title":"Uploading a Dataframe to AWS S3 Bucket from SageMaker",
        "Question_body":"<p>I am new to AWS environment and trying to solve how the data flow works. After successfully uploading CSV files from S3 to SageMaker notebook instance, I am stuck on doing the reverse. <\/p>\n\n<p>I have a dataframe and want to upload that to S3 Bucket as CSV or JSON. The code that I have is below:<\/p>\n\n<pre><code>bucket='bucketname'\ndata_key = 'test.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\ndf.to_csv(data_location)\n<\/code><\/pre>\n\n<p>I assumed since I successfully used <code>pd.read_csv()<\/code> while loading, using <code>df.to_csv()<\/code> would also work but it didn't. Probably it is generating error because this way I cannot pick the privacy options while uploading a file manually to S3. Is there a way to upload the data to S3 from SageMaker?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1561682183347,
        "Question_score":9,
        "Question_tags":"python|pandas|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":16471,
        "Owner_creation_time":1541972092477,
        "Owner_last_access_time":1664035936107,
        "Owner_location":"Santa Clara, CA, USA",
        "Owner_reputation":731,
        "Owner_up_votes":65,
        "Owner_down_votes":0,
        "Owner_views":51,
        "Question_last_edit_time":1561688539710,
        "Answer_body":"<p>One way to solve this would be to save the CSV to the local storage on the SageMaker notebook instance, and then use the S3 API's via <code>boto3<\/code> to upload the file as an s3 object. \n<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file\" rel=\"noreferrer\">S3 docs for <code>upload_file()<\/code> available here.<\/a><\/p>\n\n<p>Note, you'll need to ensure that your SageMaker hosted notebook instance has proper <code>ReadWrite<\/code> permissions in its IAM role, otherwise you'll receive a permissions error.<\/p>\n\n<pre><code># code you already have, saving the file locally to whatever directory you wish\nfile_name = \"mydata.csv\" \ndf.to_csv(file_name)\n<\/code><\/pre>\n\n<pre><code># instantiate S3 client and upload to s3\nimport boto3\n\ns3 = boto3.resource('s3')\ns3.meta.client.upload_file(file_name, 'YOUR_S3_BUCKET_NAME', 'DESIRED_S3_OBJECT_NAME')\n<\/code><\/pre>\n\n<p>Alternatively, <code>upload_fileobj()<\/code> may help for parallelizing as a multi-part upload. <\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1561684844023,
        "Answer_score":10,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":1561686321643,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56799763",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64302986,
        "Question_title":"How to highlight custom extractions using a2i's crowd-textract-analyze-document?",
        "Question_body":"<p>I would like to create a human review loop for images that undergone OCR using Amazon Textract and Entity Extraction using Amazon Comprehend.<\/p>\n<p>My process is:<\/p>\n<ol>\n<li>send image to Textract to extract the text<\/li>\n<li>send text to Comprehend to extract entities<\/li>\n<li>find the Block IDs in Textract's output of the entities extracted by Comprehend<\/li>\n<li>add new Blocks of type <code>KEY_VALUE_SET<\/code> to textract's JSON output <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/a2i-crowd-textract-detection.html\" rel=\"nofollow noreferrer\">per the docs<\/a><\/li>\n<li>create a Human Task with <code>crowd-textract-analyze-document<\/code> element in the template and feed it the modified textract output<\/li>\n<\/ol>\n<p>What fails to work in this process is step 5. My custom entities are not rendered properly. By &quot;fails to work&quot; I mean that the entities are not highlighted on the image when I click them on the sidebar. There is no error in the browser's console.<\/p>\n<p>Has anyone tried such a thing?<\/p>\n<p><em>Sorry for not including examples. I will remove secrets\/PII from my files and attach them to the question<\/em><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1602412144293,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-textract|amazon-comprehend",
        "Question_view_count":252,
        "Owner_creation_time":1354111242423,
        "Owner_last_access_time":1664060126030,
        "Owner_location":"Israel",
        "Owner_reputation":2162,
        "Owner_up_votes":390,
        "Owner_down_votes":16,
        "Owner_views":307,
        "Question_last_edit_time":1624091301367,
        "Answer_body":"<p>I used the AWS documentation of the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/a2i-crowd-textract-detection.html\" rel=\"nofollow noreferrer\">a2i-crowd-textract-detection human task element<\/a> to generate the value of the <code>initialValue<\/code> attribute. It appears the doc for that attribute is incorrect. While the the doc shows that the value should be in the same format as the output of Textract, namely:<\/p>\n<pre><code>[\n        {\n            &quot;BlockType&quot;: &quot;KEY_VALUE_SET&quot;,\n            &quot;Confidence&quot;: 38.43309020996094,\n            &quot;Geometry&quot;: { ... }\n            &quot;Id&quot;: &quot;8c97b240-0969-4678-834a-646c95da9cf4&quot;,\n            &quot;Relationships&quot;: [\n                { &quot;Type&quot;: &quot;CHILD&quot;, &quot;Ids&quot;: [...]},\n                { &quot;Type&quot;: &quot;VALUE&quot;, &quot;Ids&quot;: [...]}\n            ],\n            &quot;EntityTypes&quot;: [&quot;KEY&quot;],\n            &quot;Text&quot;: &quot;Foo bar&quot;\n        },\n]\n<\/code><\/pre>\n<p>the <code>a2i-crowd-textract-detection<\/code> expects the input to have lowerCamelCase attribute names (rather than UpperCamelCase). For example:<\/p>\n<pre><code>[\n        {\n            &quot;blockType&quot;: &quot;KEY_VALUE_SET&quot;,\n            &quot;confidence&quot;: 38.43309020996094,\n            &quot;geometry&quot;: { ... }\n            &quot;id&quot;: &quot;8c97b240-0969-4678-834a-646c95da9cf4&quot;,\n            &quot;relationships&quot;: [\n                { &quot;Type&quot;: &quot;CHILD&quot;, &quot;ids&quot;: [...]},\n                { &quot;Type&quot;: &quot;VALUE&quot;, &quot;ids&quot;: [...]}\n            ],\n            &quot;entityTypes&quot;: [&quot;KEY&quot;],\n            &quot;text&quot;: &quot;Foo bar&quot;\n        },\n]\n<\/code><\/pre>\n<p>I opened a support case about this documentation error to AWS.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1603012533233,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64302986",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65770913,
        "Question_title":"Sagemaker Studio Pyspark example fails",
        "Question_body":"<p>When I try to run the Sagemaker provided examples with PySpark in Sagemaker Studio<\/p>\n<pre><code>import os\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nimport sagemaker\nfrom sagemaker import get_execution_role\nimport sagemaker_pyspark\n\nrole = get_execution_role()\n\n# Configure Spark to use the SageMaker Spark dependency jars\njars = sagemaker_pyspark.classpath_jars()\n\nclasspath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars())\n\n# See the SageMaker Spark Github repo under sagemaker-pyspark-sdk\n# to learn how to connect to a remote EMR cluster running Spark from a Notebook Instance.\nspark = SparkSession.builder.config(&quot;spark.driver.extraClassPath&quot;, classpath)\\\n    .master(&quot;local[*]&quot;).getOrCreate()\n<\/code><\/pre>\n<p>I get the following exception:<\/p>\n<pre><code>    ---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n&lt;ipython-input-6-c8f6fff0daaf&gt; in &lt;module&gt;\n     19 # to learn how to connect to a remote EMR cluster running Spark from a Notebook Instance.\n     20 spark = SparkSession.builder.config(&quot;spark.driver.extraClassPath&quot;, classpath)\\\n---&gt; 21     .master(&quot;local[*]&quot;).getOrCreate()\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/sql\/session.py in getOrCreate(self)\n    171                     for key, value in self._options.items():\n    172                         sparkConf.set(key, value)\n--&gt; 173                     sc = SparkContext.getOrCreate(sparkConf)\n    174                     # This SparkContext may be an existing one.\n    175                     for key, value in self._options.items():\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/context.py in getOrCreate(cls, conf)\n    361         with SparkContext._lock:\n    362             if SparkContext._active_spark_context is None:\n--&gt; 363                 SparkContext(conf=conf or SparkConf())\n    364             return SparkContext._active_spark_context\n    365 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/context.py in __init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\n    127                     &quot; note this option will be removed in Spark 3.0&quot;)\n    128 \n--&gt; 129         SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    130         try:\n    131             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/context.py in _ensure_initialized(cls, instance, gateway, conf)\n    310         with SparkContext._lock:\n    311             if not SparkContext._gateway:\n--&gt; 312                 SparkContext._gateway = gateway or launch_gateway(conf)\n    313                 SparkContext._jvm = SparkContext._gateway.jvm\n    314 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/java_gateway.py in launch_gateway(conf)\n     44     :return: a JVM gateway\n     45     &quot;&quot;&quot;\n---&gt; 46     return _launch_gateway(conf)\n     47 \n     48 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/java_gateway.py in _launch_gateway(conf, insecure)\n    106 \n    107             if not os.path.isfile(conn_info_file):\n--&gt; 108                 raise Exception(&quot;Java gateway process exited before sending its port number&quot;)\n    109 \n    110             with open(conn_info_file, &quot;rb&quot;) as info:\n\nException: Java gateway process exited before sending its port number\n<\/code><\/pre>\n<p>Before running the example I installed pyspark and sagemaker_pyspark with pip from the notebook. I am also using SparkMagic kernel from the kernels library of SageMaker.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1610957989217,
        "Question_score":2,
        "Question_tags":"amazon-web-services|pyspark|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":1827,
        "Owner_creation_time":1452190055593,
        "Owner_last_access_time":1660055701297,
        "Owner_location":"Germany",
        "Owner_reputation":247,
        "Owner_up_votes":40,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Maybe, you are having this issue because this notebook was designed to run when you have an EMR cluster. I suggest you start a notebook with conda_python3 kernel on Sagemaker instead of the SparkMagic kernel. You will need to install <code>pyspark<\/code> and <code>sagemaker_pyspark<\/code> using pip, but it should work with the code you posted.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1611005573523,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65770913",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72663991,
        "Question_title":"Using external libraries for model training in aws sagemaker",
        "Question_body":"<p>I am just getting started with aws sagemaker and realized it doesn't have a random forest classifier. I found this github tutorial on creating your own and deploying it in sagemaker: <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a>.<\/p>\n<p>I am using the python sdk and was more or less curious to see if anyone actually uses this or any external libraries for training with sagemaker. It seems that if you aren't using the built in algorithms then it is very involved to create your own and the functionality of the model and ability to interpret it is very limited once you do get it trained.<\/p>\n<p>For example after deploying the model to an aws endpoint and pulling down the artifacts I could only call the <code>predict<\/code> method (no <code>predict_probab<\/code> as is possible in the actual <code>sklearn randomforestclassifier<\/code>). I also haven't been able to find anything like what you get in <code>sklearn.metrics<\/code> such as <code>accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix<\/code> etc so I'm assuming one would need to build equivalents to these from scratch to be able to interpret their model.<\/p>\n<p>I have spent a week or so researching this and trying to get this rigged up and it just seems that importing external libraries for model training in sagemaker is not very popular or well-documented online. Interested to know if I'm just unaware of more functionality or if there are alternatives that people prefer or if I should just stick with the built in xgboost classifier if I am looking for a tree-based option. Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655494198873,
        "Question_score":0,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":50,
        "Owner_creation_time":1589338575433,
        "Owner_last_access_time":1664013922157,
        "Owner_location":null,
        "Owner_reputation":373,
        "Owner_up_votes":73,
        "Owner_down_votes":0,
        "Owner_views":110,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It is possible to add extra packages for training when using any of the Framework containers such as SKLearn. Kindly see this <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#using-third-party-libraries\" rel=\"nofollow noreferrer\">link<\/a> for more information.<\/p>\n<p>From a hosting perspective, you do have the ability to provide a custom entry_point \/ inference.py script that you can use to control model loading, pre and post processing. Please see this <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#load-a-model\" rel=\"nofollow noreferrer\">link<\/a> for more information<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1657063869133,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72663991",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59430560,
        "Question_title":"Reading Data from AWS S3",
        "Question_body":"<p>I have some data with very particular format (e.g., tdms files generated by NI systems) and I stored them in a S3 bucket. Typically, for reading this data in python if the data was stored in my local computer, I would use npTDMS package. But, how should is read this tdms files when they are stored in a S3 bucket? One solution is to download the data for instance to the EC2 instance and then use npTDMS package for reading the data into python. But it does not seem to be a perfect solution. Is there any way that I can read the data similar to reading CSV files from S3? <\/p>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_time":1576870865157,
        "Question_score":1,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":4393,
        "Owner_creation_time":1534965197293,
        "Owner_last_access_time":1663788259063,
        "Owner_location":"Seattle, WA",
        "Owner_reputation":320,
        "Owner_up_votes":91,
        "Owner_down_votes":2,
        "Owner_views":48,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Some Python packages (such as Pandas) support reading data directly from S3, as it is the most popular location for data. See <a href=\"https:\/\/stackoverflow.com\/questions\/37703634\/how-to-import-a-text-file-on-aws-s3-into-pandas-without-writing-to-disk\">this question<\/a> for example on the way to do that with Pandas.<\/p>\n\n<p>If the package (npTDMS) doesn't support reading directly from S3, you should copy the data to the local disk of the notebook instance.<\/p>\n\n<p>The simplest way to copy is to run the AWS CLI in a cell in your notebook<\/p>\n\n<pre><code>!aws s3 cp s3:\/\/bucket_name\/path_to_your_data\/ data\/\n<\/code><\/pre>\n\n<p>This command will copy all the files under the \"folder\" in S3 to the local folder <code>data<\/code><\/p>\n\n<p>You can use more fine-grained copy using the filtering of the files and other specific requirements using the boto3 rich capabilities. For example:<\/p>\n\n<pre><code>s3 = boto3.resource('s3')\nbucket = s3.Bucket('my-bucket')\nobjs = bucket.objects.filter(Prefix='myprefix')\nfor obj in objs:\n   obj.download_file(obj.key)\n<\/code><\/pre>",
        "Answer_comment_count":1,
        "Answer_creation_time":1577181091057,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1577202690472,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59430560",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59679192,
        "Question_title":"Hosting multiple models for multiple datasets in aws sagemaker",
        "Question_body":"<p>I read that there is a way to train and host multiple models using a single endpoint for a single dataset in AWS Sagemaker. But I have 2 different datasets in S3 and have to train a model for each dataset. Can these 2 different models be hosted using a single endpoint? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1578649761440,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":411,
        "Owner_creation_time":1550756471933,
        "Owner_last_access_time":1663939288837,
        "Owner_location":null,
        "Owner_reputation":67,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes, this is called a multi-model endpoint. You can use a large number of models on the same endpoint. They get loaded and unloaded dynamically as needed, and you simply have to pass the model name in your prediction request.<\/p>\n\n<p>Here are some resources:<\/p>\n\n<ul>\n<li><p>Blog post + example : <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/<\/a><\/p><\/li>\n<li><p>Video explaining model deployment scenarios on SageMaker: <a href=\"https:\/\/youtu.be\/dT8jmdF-ZWw\" rel=\"nofollow noreferrer\">https:\/\/youtu.be\/dT8jmdF-ZWw<\/a><\/p><\/li>\n<\/ul>",
        "Answer_comment_count":0,
        "Answer_creation_time":1578755333997,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59679192",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61181955,
        "Question_title":"Is there any limits of saving result on S3 from sagemaker Processing?",
        "Question_body":"<p>\u203b I used google translation, if you have any question, let me know!<\/p>\n\n<p>I am trying to run python script with huge 4 data, using sagemaker processing. And my current situation are as follows:<\/p>\n\n<ul>\n<li>can run this script with 3 data<\/li>\n<li>can't run the script with only 1 data (the biggest, the same structure with others)<\/li>\n<li>as for all of 4 data, the script has finished (so, I suspected this error in S3, ie. when copying sagemaker result to S3)<\/li>\n<\/ul>\n\n<p>The error I got is this InternalServerError.<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"sagemaker_train_and_predict.py\", line 56, in &lt;module&gt;\n    outputs=outputs\n  File \"{xxx}\/sagemaker_constructor.py\", line 39, in run\n    outputs=outputs\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 408, in run\n    self.latest_job.wait(logs=logs)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 723, in wait\n    self.sagemaker_session.logs_for_processing_job(self.job_name, wait=True)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 3111, in logs_for_processing_job\n    self._check_job_status(job_name, description, \"ProcessingJobStatus\")\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 2615, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Processing job sagemaker-vm-train-and-predict-2020-04-12-04-15-40-655: Failed. Reason: InternalServerError: We encountered an internal error.  Please try again.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586755348657,
        "Question_score":1,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":95,
        "Owner_creation_time":1586754432800,
        "Owner_last_access_time":1593512686190,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There may be some issue transferring the output data to S3 if the output is generated at a high rate and size is too large. <\/p>\n\n<p>You can 1) try to slow down writing the output a bit or 2) call S3 from your algorithm container to upload the output directly using boto client (<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html<\/a>).<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1586886613193,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61181955",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62453292,
        "Question_title":"How to find memory leak in Python MXNet?",
        "Question_body":"<p>I am afraid that my Neural Network in MXNet, written in Python, has a memory leak. I have tried the MXNet profiler and the tracemalloc module to get an understanding of memory profiling, but I want to get information on any potential memory leaks, just like I'd do with valgrind in C.<\/p>\n\n<p>I found <a href=\"https:\/\/cwiki.apache.org\/confluence\/display\/MXNET\/Detecting+Memory+Leaks+and+Buffer+Overflows+in+MXNet\" rel=\"nofollow noreferrer\">Detecting Memory Leaks and Buffer Overflows in MXNet<\/a>, and after managing to build like described in section \"Using ASAN builds with MXNet\", by replacing the \"ubuntu_cpu\" part in <code>docker\/Dockerfile.build.ubuntu_cpu -t mxnetci\/build.ubuntu_cpu<\/code> with \"ubuntu_cpu_python\", I tried executing in an AWS Sagemaker Notebook like this:<\/p>\n\n<pre><code>root@33e38e00f825:\/work\/mxnet# nosetests3 --verbose \/home\/ec2-user\/SageMaker\/run_predict.py\n<\/code><\/pre>\n\n<p>and I get this import error:<\/p>\n\n<blockquote>\n  <p>Failure: ImportError (No module named 'run_predict') ... ERROR<\/p>\n<\/blockquote>\n\n<p>My run_predict.py looks like this:<\/p>\n\n<pre><code>#!\/usr\/bin\/env python\ndef run_predict(n):\n  # calling MXNet inference method\n\nrun_predict(-1)  # tried it putting it under 'if __name__ == \"__main__\":'\n<\/code><\/pre>\n\n<p>What I am missing in my script, what should I change?<\/p>\n\n<p>The example script they use in the link is <a href=\"https:\/\/github.com\/apache\/incubator-mxnet\/blob\/faccd91071cc34ed0b3a192d3c7932441fe7e35e\/tests\/python\/unittest\/test_rnn.py\" rel=\"nofollow noreferrer\">rnn_test.py<\/a>, but even when I run this example, I still get an analogous Import Error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592493163510,
        "Question_score":0,
        "Question_tags":"python|machine-learning|memory-leaks|amazon-sagemaker|mxnet",
        "Question_view_count":263,
        "Owner_creation_time":1369257942213,
        "Owner_last_access_time":1663776093950,
        "Owner_location":"London, UK",
        "Owner_reputation":70285,
        "Owner_up_votes":7595,
        "Owner_down_votes":12100,
        "Owner_views":13121,
        "Question_last_edit_time":1592579151163,
        "Answer_body":"<p>In MXNet, we automatically test for this through examining the garbage collection records. You can find how it's implemented here: <a href=\"https:\/\/github.com\/apache\/incubator-mxnet\/blob\/c3aff732371d6177e5d522c052fb7258978d8ce4\/tests\/python\/conftest.py#L26-L79\" rel=\"nofollow noreferrer\">https:\/\/github.com\/apache\/incubator-mxnet\/blob\/c3aff732371d6177e5d522c052fb7258978d8ce4\/tests\/python\/conftest.py#L26-L79<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1598820674153,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62453292",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73232032,
        "Question_title":"Start execution of existing SageMaker pipeline using Python SDK",
        "Question_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/run-pipeline.html#run-pipeline-prereq\" rel=\"nofollow noreferrer\">SageMaker documentatin<\/a> explains how to run a pipeline, but it assumes I have just <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html\" rel=\"nofollow noreferrer\">defined it<\/a> and I have the object <code>pipeline<\/code> available.<\/p>\n<p>How can I run an <strong>existing<\/strong> pipeline with <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"nofollow noreferrer\">Python SDK<\/a>?<\/p>\n<p>I know how to read a pipeline with AWS CLI (i.e. <code>aws sagemaker describe-pipeline --pipeline-name foo<\/code>). Can the same be done with Python code? Then I would have <code>pipeline<\/code> object ready to use.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659598635513,
        "Question_score":0,
        "Question_tags":"aws-sdk|amazon-sagemaker",
        "Question_view_count":106,
        "Owner_creation_time":1217615304817,
        "Owner_last_access_time":1664033547990,
        "Owner_location":"Poland",
        "Owner_reputation":16694,
        "Owner_up_votes":3004,
        "Owner_down_votes":154,
        "Owner_views":3155,
        "Question_last_edit_time":1659598939403,
        "Answer_body":"<p>If the Pipeline has been created, you can use the Python Boto3 SDK to make the <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.start_pipeline_execution\" rel=\"nofollow noreferrer\"><code>StartPipelineExecution<\/code><\/a> API call.<\/p>\n<pre><code>response = client.start_pipeline_execution(\n    PipelineName='string',\n    PipelineExecutionDisplayName='string',\n    PipelineParameters=[\n        {\n            'Name': 'string',\n            'Value': 'string'\n        },\n    ],\n    PipelineExecutionDescription='string',\n    ClientRequestToken='string',\n    ParallelismConfiguration={\n        'MaxParallelExecutionSteps': 123\n    }\n)\n<\/code><\/pre>\n<p>If you prefer AWS CLI, the most basic call is:<\/p>\n<pre><code>aws sagemaker start-pipeline-execution --pipeline-name &lt;name-of-the-pipeline&gt;\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1659628256873,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1660212471007,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73232032",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51698373,
        "Question_title":"Amazon SageMaker: Invoke endpoint with file as multipart\/form-data",
        "Question_body":"<p>After setting up an endpoint for my model on Amazon SageMaker, I am trying to invoke it with a POST request which contains a file with a key <code>image<\/code> &amp; content type as <code>multipart\/form-data<\/code>.<\/p>\n\n<p>My AWS CLI command is like this:<\/p>\n\n<pre><code>aws sagemaker-runtime invoke-endpoint --endpoint-name &lt;endpoint-name&gt; --body image=@\/local\/file\/path\/dummy.jpg --content-type multipart\/form-data output.json --region us-east-1\n<\/code><\/pre>\n\n<p>which should be an equivalent of:<\/p>\n\n<pre><code>curl -X POST -F \"image=@\/local\/file\/path\/dummy.jpg\" http:\/\/&lt;endpoint&gt;\n<\/code><\/pre>\n\n<p>After running the <code>aws<\/code> command, the file is not transferred via the request, and my model is receiving the request without any file in it.<\/p>\n\n<p>Can someone please tell me what should be the correct format of the <code>aws<\/code> command in order to achieve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1533504341903,
        "Question_score":6,
        "Question_tags":"amazon-web-services|curl|aws-cli|amazon-sagemaker",
        "Question_view_count":2346,
        "Owner_creation_time":1472843515757,
        "Owner_last_access_time":1568228274983,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The first problem is that you're using 'http' for your CURL request. Virtually all AWS services strictly use 'https' as their protocol, SageMaker included. <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/rande.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/rande.html<\/a>. I'm going to assume this was a typo though.<\/p>\n\n<p>You can check the verbose output of the AWS CLI by passing the '--debug' argument to your call. I re-ran a similar experiment with my favorite duck.jpg image:<\/p>\n\n<pre><code>aws --debug sagemaker-runtime invoke-endpoint --endpoint-name MyEndpoint --body image=@\/duck.jpg --content-type multipart\/form-data  &gt;(cat)\n<\/code><\/pre>\n\n<p>Looking at the output, I see:<\/p>\n\n<pre><code>2018-08-10 08:42:20,870 - MainThread - botocore.endpoint - DEBUG - Making request for OperationModel(name=InvokeEndpoint) (verify_ssl=True) with params: {'body': 'image=@\/duck.jpg', 'url': u'https:\/\/sagemaker.us-west-2.amazonaws.com\/endpoints\/MyEndpoint\/invocations', 'headers': {u'Content-Type': 'multipart\/form-data', 'User-Agent': 'aws-cli\/1.15.14 Python\/2.7.10 Darwin\/16.7.0 botocore\/1.10.14'}, 'context': {'auth_type': None, 'client_region': 'us-west-2', 'has_streaming_input': True, 'client_config': &lt;botocore.config.Config object at 0x109a58ed0&gt;}, 'query_string': {}, 'url_path': u'\/endpoints\/MyEndpoint\/invocations', 'method': u'POST'}\n<\/code><\/pre>\n\n<p>It looks like the AWS CLI is using the string literal '@\/duck.jpg', not the file contents.<\/p>\n\n<p>Trying again with curl and the \"--verbose\" flag:<\/p>\n\n<pre><code>curl --verbose -X POST -F \"image=@\/duck.jpg\" https:\/\/sagemaker.us-west-2.amazonaws.com\/endpoints\/MyEndpoint\/invocations\n<\/code><\/pre>\n\n<p>I see the following:<\/p>\n\n<pre><code>Content-Length: 63097\n<\/code><\/pre>\n\n<p>Much better. The '@' operator is a CURL specific feature. The AWS CLI does have a way to pass files though: <\/p>\n\n<pre><code>--body fileb:\/\/\/duck.jpg\n<\/code><\/pre>\n\n<p>There is also a 'file' for non-binary files such as JSON. Unfortunately you cannot have the prefix. That is, you cannot say:<\/p>\n\n<pre><code> --body image=fileb:\/\/\/duck.jpg\n<\/code><\/pre>\n\n<p>You can prepend the string 'image=' to your file with a command such as the following. (You'll probably need to be more clever if your images are really big; this is really inefficient.)<\/p>\n\n<pre><code> echo -e \"image=$(cat \/duck.jpg)\" &gt; duck_with_prefix\n<\/code><\/pre>\n\n<p>Your final command would then be:<\/p>\n\n<pre><code> aws sagemaker-runtime invoke-endpoint --endpoint-name MyEndpoint --body fileb:\/\/\/duck_with_prefix --content-type multipart\/form-data  &gt;(cat)\n<\/code><\/pre>\n\n<p>Another note: Using raw curl with AWS services is extremely difficult due to the AWS Auth signing requirements - <a href=\"https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/API\/sig-v4-authenticating-requests.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/API\/sig-v4-authenticating-requests.html<\/a> <\/p>\n\n<p>It can be done, but you'll likely be more productive by using the AWS CLI or a pre-existing tool such as Postman - <a href=\"https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/how-to-use-postman-to-call-api.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/how-to-use-postman-to-call-api.html<\/a> <\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1533918119980,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51698373",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55854377,
        "Question_title":"comprehend.start_topics_detection_job Fails with Silent Error?",
        "Question_body":"<p>I have <a href=\"https:\/\/docs.aws.amazon.com\/code-samples\/latest\/catalog\/python-comprehend-TopicModeling.py.html\" rel=\"nofollow noreferrer\">Amazon sample code<\/a> for running <code>comprehend.start_topics_detection_job<\/code>. Here is the code with the variables filled in for my job:<\/p>\n\n<pre><code>import re\nimport csv\nimport pytz\nimport boto3\nimport json\n\n# https:\/\/docs.aws.amazon.com\/code-samples\/latest\/catalog\/python-comprehend-TopicModeling.py.html\n# https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/API_InputDataConfig.html\n\n# Set these values before running the program\ninput_s3_url = \"s3:\/\/comprehend-topic-modelling-bucket\/input_800_cleaned_articles\/\"\ninput_doc_format = \"ONE_DOC_PER_LINE\"\noutput_s3_url = \"s3:\/\/comprehend-topic-modelling-bucket\/output\"\ndata_access_role_arn = \"arn:aws:iam::372656143103:role\/access-aws-services-from-sagemaker\"\nnumber_of_topics = 30\n\n# Set up job configuration\ninput_data_config = {\"S3Uri\": input_s3_url, \"InputFormat\": input_doc_format}\noutput_data_config = {\"S3Uri\": output_s3_url}\n\n# Begin a job to detect the topics in the document collection\ncomprehend = boto3.client('comprehend')\nstart_result = comprehend.start_topics_detection_job(\n    NumberOfTopics=number_of_topics,\n    InputDataConfig=input_data_config,\n    OutputDataConfig=output_data_config,\n    DataAccessRoleArn=data_access_role_arn)\n\n# Output the results\nprint('Start Topic Detection Job: ' + json.dumps(start_result))\njob_id = start_result['JobId']\nprint(f'job_id: {job_id}')\n\n# Retrieve and output information about the job\ndescribe_result = comprehend.describe_topics_detection_job(JobId=job_id)\nprint('Describe Job: ' + json.dumps(describe_result)) . #&lt;===LINE 36\n\n# List and output information about current jobs\nlist_result = comprehend.list_topics_detection_jobs()\nprint('list_topics_detection_jobs_result: ' + json.dumps(list_result))\n<\/code><\/pre>\n\n<p>It's failing with the error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-8-840a7ee043d4&gt; in &lt;module&gt;()\n     34 # Retrieve and output information about the job\n     35 describe_result = comprehend.describe_topics_detection_job(JobId=job_id)\n---&gt; 36 print('Describe Job: ' + json.dumps(describe_result))\n     37 \n     38 # List and output information about current jobs\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\n    229         cls is None and indent is None and separators is None and\n    230         default is None and not sort_keys and not kw):\n--&gt; 231         return _default_encoder.encode(obj)\n    232     if cls is None:\n    233         cls = JSONEncoder\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in encode(self, o)\n    197         # exceptions aren't as detailed.  The list call should be roughly\n    198         # equivalent to the PySequence_Fast that ''.join() would do.\n--&gt; 199         chunks = self.iterencode(o, _one_shot=True)\n    200         if not isinstance(chunks, (list, tuple)):\n    201             chunks = list(chunks)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in iterencode(self, o, _one_shot)\n    255                 self.key_separator, self.item_separator, self.sort_keys,\n    256                 self.skipkeys, _one_shot)\n--&gt; 257         return _iterencode(o, 0)\n    258 \n    259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in default(self, o)\n    178         \"\"\"\n    179         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n--&gt; 180                         o.__class__.__name__)\n    181 \n    182     def encode(self, o):\n\nTypeError: Object of type 'datetime' is not JSON serializable\n<\/code><\/pre>\n\n<p>It fails instantly, the second I pus \"run\". It seems to me that the call to <code>comprehend.start_topics_detection_job<\/code> may be failing, leading to an error line 36, <code>print('Describe Job: ' + json.dumps(describe_result))<\/code>.<\/p>\n\n<p>What am I missing?<\/p>\n\n<p><strong>UPDATE<\/strong><\/p>\n\n<p>The same IAM role is being used for the notebook, as well as in the above code. Here are the permissions currently assigned to that IAM role:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/6ihIr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6ihIr.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1556211825007,
        "Question_score":0,
        "Question_tags":"python|django|machine-learning|amazon-sagemaker|amazon-comprehend",
        "Question_view_count":232,
        "Owner_creation_time":1276294622427,
        "Owner_last_access_time":1664061462980,
        "Owner_location":null,
        "Owner_reputation":4334,
        "Owner_up_votes":409,
        "Owner_down_votes":1,
        "Owner_views":496,
        "Question_last_edit_time":1586235824609,
        "Answer_body":"<p>It turns out that there was nothing wrong with the call to <code>comprehend.describe_topics_detection_job<\/code> -- it was just returning, in <code>describe_result<\/code>, something that could not be json serialized, so <code>json.dumps(describe_result))<\/code> was throwing an error. <\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1556698045340,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55854377",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56497428,
        "Question_title":"Use images in s3 with SageMaker without .lst files",
        "Question_body":"<p>I am trying to create (what I thought was) a simple image classification pipeline between s3 and SageMaker.<\/p>\n\n<p>Images are stored in an s3 bucket with their class labels in their file names currently, e.g.<\/p>\n\n<p><strong>My-s3-bucket-dir<\/strong><\/p>\n\n<pre><code>cat-1.jpg\ndog-1.jpg\ncat-2.jpg\n..\n<\/code><\/pre>\n\n<p>I've been trying to leverage several related example .py scripts, but most seem to be download data sets already in .rec format or containing special manifest or annotation files I don't have.<\/p>\n\n<p>All I want is to pass the images from s3 to the SageMaker image classification algorithm that's located in the same region, IAM account, etc. I suppose this means I need a <code>.lst<\/code> file<\/p>\n\n<p>When I try to manually create the <code>.lst<\/code> it doesn't seem to like it and it also takes too long doing manual work to be a good practice.<\/p>\n\n<p>How can I automatically generate the <code>.lst<\/code> file (or otherwise send the images\/classes for training)? <\/p>\n\n<p>Things I read made it sound like <code>im2rec.py<\/code> was a solution, but I don't see how. The example I'm working with now is <\/p>\n\n<p><code>Image-classification-fulltraining-highlevel.ipynb<\/code><\/p>\n\n<p>but it seems to download the data as <code>.rec<\/code>, <\/p>\n\n<pre><code>download('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-train.rec')\ndownload('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-val.rec')\n<\/code><\/pre>\n\n<p>which just skips working with the .jpeg files. I found another that converts them to <code>.rec<\/code> but again it has essentially the <code>.lst<\/code> already as <code>.json<\/code> and just converts it.<\/p>\n\n<p>I have mostly been working in a Python Jupyter notebook within the AWS console (in my browser) but I have also tried using their GUI. <\/p>\n\n<p>How can I simply and automatically generate the <code>.lst<\/code> or otherwise get the data\/class info into SageMaker without manually creating a <code>.lst<\/code> file?<\/p>\n\n<p><strong><em>Update<\/em><\/strong><\/p>\n\n<p>It looks like im2py can't be run against s3. You'd have to completely download everything from all s3 buckets into the notebook's storage...<\/p>\n\n<blockquote>\n  <p>Please note that [...] im2rec.py is running locally,\n  therefore cannot take input from the S3 bucket. To generate the list\n  file, you need to download the data and then use the im2rec tool. - AWS SageMaker Team<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559921851153,
        "Question_score":2,
        "Question_tags":"python-3.x|amazon-s3|computer-vision|amazon-sagemaker",
        "Question_view_count":937,
        "Owner_creation_time":1399301338467,
        "Owner_last_access_time":1664044371257,
        "Owner_location":"Columbia, MD, USA",
        "Owner_reputation":21413,
        "Owner_up_votes":2163,
        "Owner_down_votes":516,
        "Owner_views":6465,
        "Question_last_edit_time":1560181849816,
        "Answer_body":"<p>There are 3 options to provide annotated data to the Image Classification algo: (1) packing labels in recordIO files, (2) storing labels in a JSON manifest file (\"augmented manifest\" option), (3) storing labels in a list file. All options are documented here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html<\/a>.<\/p>\n\n<p>Augmented Manifest and .lst files option are quick to do since they just require you to create an annotation file with a usually quick <code>for<\/code> loop for example. RecordIO requires you to use <code>im2rec.py<\/code> tool, which is a little more work.<\/p>\n\n<p>Using .lst files is <strong>another option<\/strong> that is reasonably easy: you just need to create annotation them with a quick for loop, like this:<\/p>\n\n<pre><code># assuming train_index, train_class, train_pics store the pic index, class and path\n\nwith open('train.lst', 'a') as file:\n    for index, cl, pic in zip(train_index, train_class, train_pics):\n        file.write(str(index) + '\\t' + str(cl) + '\\t' + pic + '\\n')\n<\/code><\/pre>",
        "Answer_comment_count":1,
        "Answer_creation_time":1559948298333,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56497428",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68736614,
        "Question_title":"How to set \"Task title\" of a SageMaker GrountTruth labeling job",
        "Question_body":"<p>I'm creating a SageMaker GroundTruth Labeling Job using the console UI. I'm looking for a way to configure &quot;Task title&quot;, which is shown in the workers Job list.<\/p>\n<p>I think this is related to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-create-labeling-job-api.html\" rel=\"nofollow noreferrer\">TaskTitle<\/a> configuration of AWS CLI. However, I cannot configure it from the AWS console. Can we configure it from the console GUI?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1628661017973,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":55,
        "Owner_creation_time":1354705336800,
        "Owner_last_access_time":1663474776330,
        "Owner_location":"Pittsburgh",
        "Owner_reputation":2517,
        "Owner_up_votes":90,
        "Owner_down_votes":13,
        "Owner_views":78,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I was also doing the similar stuff and running a sagemaker GT labeling job.\nI am following the below git repo by amazon :<\/p>\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline<\/a><\/p>\n<p>If you look into the below file in the repo:<\/p>\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/main\/notebooks\/util\/smgt.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/main\/notebooks\/util\/smgt.py<\/a><\/p>\n<p>at line no:349 ,you will see how they do it and you can do the similar change in your code.<\/p>\n<p><strong>&quot;TaskTitle&quot;: &quot;Credit Card Agreement Entities&quot;<\/strong><\/p>\n<p>Hope this will help.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1656482770813,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68736614",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51968742,
        "Question_title":"AWS sagemaker invokeEndpoint model internal error",
        "Question_body":"<p>I am trying to send a request on a model on sagemaker using .NET. The code I am using is: <\/p>\n\n<pre><code>var data = File.ReadAllBytes(@\"C:\\path\\file.csv\");\nvar credentials = new Amazon.Runtime.BasicAWSCredentials(\"\",\"\");\nvar awsClient = new AmazonSageMakerRuntimeClient(credentials, RegionEndpoint.EUCentral1);\nvar request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest\n{\n    EndpointName = \"EndpointName\",\n    ContentType = \"text\/csv\",\n    Body = new MemoryStream(data),\n};\n\nvar response = awsClient.InvokeEndpoint(request);\nvar predictions = Encoding.UTF8.GetString(response.Body.ToArray());\n<\/code><\/pre>\n\n<p>the error that I am getting on <code>awsClient.InvokeEndpoint(request)<\/code><\/p>\n\n<p>is:<\/p>\n\n<blockquote>\n  <p>Amazon.SageMakerRuntime.Model.ModelErrorException: 'The service\n  returned an error with Error Code ModelError and HTTP Body:\n  {\"ErrorCode\":\"INTERNAL_FAILURE_FROM_MODEL\",\"LogStreamArn\":\"arn:aws:logs:eu-central-1:xxxxxxxx:log-group:\/aws\/sagemaker\/Endpoints\/myEndpoint\",\"Message\":\"Received\n  server error (500) from model with message \\\"\\\". See\n  \"https:\/\/ url_to_logs_on_amazon\"\n  in account xxxxxxxxxxx for more\n  information.\",\"OriginalMessage\":\"\",\"OriginalStatusCode\":500}'<\/p>\n<\/blockquote>\n\n<p>the url that the error message suggests for more information does not help at all.<\/p>\n\n<p>I believe that it is a data format issue but I was not able to find a solution.<\/p>\n\n<p>Does anyone has encountered this behavior before?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1534946906973,
        "Question_score":3,
        "Question_tags":"c#|.net|amazon-web-services|amazon-sagemaker",
        "Question_view_count":4385,
        "Owner_creation_time":1495715386880,
        "Owner_last_access_time":1592382740743,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The problem relied on the data format as suspected. In my case all I had to do is send the data as a json serialized string array and use <code>ContentType = application\/json<\/code> because the python function running on the endpoint which is responsible for sending the data to the predictor was only accepting json strings. <\/p>\n\n<p>Another way to solve this issues is to modify the python function which is responsible for the input handling to accept all content types and modify the data in a way that the predictor will understand.<\/p>\n\n<p>example of working code for my case:<\/p>\n\n<pre><code>        var data = new string[] { \"this movie was extremely good .\", \"the plot was very boring .\" };\n        var serializedData = JsonConvert.SerializeObject(data);\n\n        var credentials = new Amazon.Runtime.BasicAWSCredentials(\"\",\"\");\n        var awsClient = new AmazonSageMakerRuntimeClient(credentials, RegionEndpoint.EUCentral1);\n        var request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest\n        {\n            EndpointName = \"endpoint\",\n            ContentType = \"application\/json\",\n            Body = new MemoryStream(Encoding.ASCII.GetBytes(serializedData)),\n        };\n\n        var response = awsClient.InvokeEndpoint(request);\n        var predictions = Encoding.UTF8.GetString(response.Body.ToArray());\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1535104925600,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51968742",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55580232,
        "Question_title":"Update SageMaker Jupyterlab environment",
        "Question_body":"<p>How can I update my SageMaker notebook's jupyter environment to the latest alpha release and then restart the process?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1554750635030,
        "Question_score":4,
        "Question_tags":"amazon-sagemaker|jupyter-lab",
        "Question_view_count":1720,
        "Owner_creation_time":1361339272693,
        "Owner_last_access_time":1663965928400,
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Hi and thank you for using SageMaker!<\/p>\n\n<p>To restart Jupyter from within a SageMaker Notebook Instance, you can issue the following command: <code>sudo initctl restart jupyter-server --no-wait<\/code>.<\/p>\n\n<p>Best,\nKevin<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1555007316017,
        "Answer_score":8,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55580232",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63138835,
        "Question_title":"How to save models trained locally in Amazon SageMaker?",
        "Question_body":"<p>I'm trying to use a local training job in SageMaker.<\/p>\n<p>Following this AWS notebook (<a href=\"http:\/\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb<\/a>) I was able to train and predict locally.<\/p>\n<p>There is any way to train locally and save the trained model in the Amazon SageMaker Training Job section?\nOtherwise, how can I properly save trained models I trained using local mode?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1595954217667,
        "Question_score":7,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":3195,
        "Owner_creation_time":1464391892937,
        "Owner_last_access_time":1658153265653,
        "Owner_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Owner_reputation":2243,
        "Owner_up_votes":497,
        "Owner_down_votes":32,
        "Owner_views":148,
        "Question_last_edit_time":1595959142729,
        "Answer_body":"<p>There is no way to have your local mode training jobs appear in the AWS console. The intent of local mode is to allow for faster iteration\/debugging before using SageMaker for training your model.<\/p>\n<p>You can create SageMaker Models from local model artifacts. Compress your model artifacts into a <code>.tar.gz<\/code> file, upload that file to S3, and then create the Model (with the SDK or in the console).<\/p>\n<p>Documentation:<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0,
        "Answer_creation_time":1596039571220,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63138835",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63740792,
        "Question_title":"Provide additional input to docker container running inference model",
        "Question_body":"<p>We are using AWS Sagemaker feature, bring your own docker, where we have inference model written in R. As I understood, batch transform job runs container in a following way:<\/p>\n<pre><code>docker run image serve\n<\/code><\/pre>\n<p>Also, on docker we have a logic to determine which function to invoke:<\/p>\n<pre><code>args &lt;- commandArgs()\nif (any(grepl('train', args))) {\n    train()}\nif (any(grepl('serve', args))) {\n    serve()}\n<\/code><\/pre>\n<p>Is there a way, to override default container invocation so we can pass some additional parameters?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1599220820873,
        "Question_score":2,
        "Question_tags":"amazon-web-services|docker|amazon-sagemaker",
        "Question_view_count":702,
        "Owner_creation_time":1473837223713,
        "Owner_last_access_time":1658935159907,
        "Owner_location":"Belgrade",
        "Owner_reputation":353,
        "Owner_up_votes":23,
        "Owner_down_votes":1,
        "Owner_views":66,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As you said, and is indicated in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html\" rel=\"nofollow noreferrer\">AWS documentation<\/a>, Sagemaker will run your container with the following command:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>docker run image serve\n<\/code><\/pre>\n<p>By issuing this command Sagemaker will overwrite any <code>CMD<\/code> that you provide in your container Dockerfile, so you cannot use <code>CMD<\/code> to provide dynamic arguments to your program.<\/p>\n<p>We can think in use the Dockerfile <code>ENTRYPOINT<\/code> to consume some environment variables, but the documentation of AWS dictates that it is preferable use the <code>exec<\/code> form of the <code>ENTRYPOINT<\/code>. Somethink like:<\/p>\n<pre><code>ENTRYPOINT [&quot;\/usr\/bin\/Rscript&quot;, &quot;\/opt\/ml\/mars.R&quot;, &quot;--no-save&quot;]\n<\/code><\/pre>\n<p>I think that, for analogy with <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">model training<\/a>, they need this kind of container execution to enable the container to receive termination signals:<\/p>\n<blockquote>\n<p>The exec form of the <code>ENTRYPOINT<\/code> instruction starts the executable directly, not as a child of <code>\/bin\/sh<\/code>. This enables it to receive signals like <code>SIGTERM<\/code> and <code>SIGKILL<\/code> from SageMaker APIs.<\/p>\n<\/blockquote>\n<p>To allow variable expansion, we need to use the <code>ENTRYPOINT<\/code> <code>shell<\/code> form. Imagine:<\/p>\n<pre><code>ENTRYPOINT [&quot;sh&quot;, &quot;-c&quot;, &quot;\/usr\/bin\/Rscript&quot;, &quot;\/opt\/ml\/mars.R&quot;, &quot;--no-save&quot;, &quot;$ENV_VAR1&quot;]\n<\/code><\/pre>\n<p>If you try to do the same with the <code>exec<\/code> form the variables provided will be treated as a literal and will not be sustituited for their actual values.<\/p>\n<p>Please, see the approved answer of <a href=\"https:\/\/stackoverflow.com\/questions\/37904682\/how-do-i-use-docker-environment-variable-in-entrypoint-array\">this<\/a> stackoverflow question for a great explanation of this subject.<\/p>\n<p>But, one thing you can do is obtain the value of these variables in your R code, similar as when you process <code>commandArgs<\/code>:<\/p>\n<pre class=\"lang-r prettyprint-override\"><code>ENV_VAR1 &lt;- Sys.getenv(&quot;ENV_VAR1&quot;)\n<\/code><\/pre>\n<p>To pass environment variables to the container, as indicated in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html\" rel=\"nofollow noreferrer\">AWS documentation<\/a>, you can use the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\"><code>CreateModel<\/code><\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\"><code>CreateTransformJob<\/code><\/a> requests on your container.<\/p>\n<p>You probably will need to include in your Dockerfile <code>ENV<\/code> definitions for every required environment variable on your container, and provide for these definitions default values with <code>ARG<\/code>:<\/p>\n<pre><code>ARG ENV_VAR1_DEFAULT_VALUE=VAL1\nENV_VAR1=$ENV_VAR1_DEFAULT_VALUE\n<\/code><\/pre>",
        "Answer_comment_count":4,
        "Answer_creation_time":1599691026140,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1599720910636,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63740792",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69853177,
        "Question_title":"Docker-compose wouldn't start on Sagemaker's Notebook instance",
        "Question_body":"<p>Docker-compose seems to have stopped working on Sagemaker Notebook instances. When running <code>docker-compose up<\/code> I encounter the following error:<\/p>\n<pre><code>During handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/bin\/docker-compose&quot;, line 8, in &lt;module&gt;\n    sys.exit(main())\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/main.py&quot;, line 81, in main\n    command_func()\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/main.py&quot;, line 200, in perform_command\n    project = project_from_options('.', options)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/command.py&quot;, line 70, in project_from_options\n    enabled_profiles=get_profiles_from_options(options, environment)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/command.py&quot;, line 153, in get_project\n    verbose=verbose, version=api_version, context=context, environment=environment\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/docker_client.py&quot;, line 43, in get_client\n    environment=environment, tls_version=get_tls_version(environment)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/docker_client.py&quot;, line 170, in docker_client\n    client = APIClient(use_ssh_client=not use_paramiko_ssh, **kwargs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/docker\/api\/client.py&quot;, line 197, in __init__\n    self._version = self._retrieve_server_version()\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/docker\/api\/client.py&quot;, line 222, in _retrieve_server_version\n    'Error while fetching server API version: {0}'.format(e)\ndocker.errors.DockerException: Error while fetching server API version: Timeout value connect was Timeout(connect=60, read=60, total=None), but it must be an int, float or None\n<\/code><\/pre>\n<p>I can start Docker containers as usual.<\/p>\n<pre><code>sh-4.2$ docker version\nClient:\n Version:           20.10.7\n API version:       1.41\n Go version:        go1.15.14\n Git commit:        f0df350\n Built:             Tue Sep 28 19:55:40 2021\n OS\/Arch:           linux\/amd64\n Context:           default\n Experimental:      true\n\nServer:\n Engine:\n  Version:          20.10.7\n  API version:      1.41 (minimum version 1.12)\n  Go version:       go1.15.14\n  Git commit:       b0f5bc3\n  Built:            Tue Sep 28 19:57:35 2021\n  OS\/Arch:          linux\/amd64\n  Experimental:     false\n containerd:\n  Version:          1.4.6\n  GitCommit:        d71fcd7d8303cbf684402823e425e9dd2e99285d\n runc:\n  Version:          1.0.0\n  GitCommit:        %runc_commit\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n<\/code><\/pre>\n<p>But <code>docker-compose<\/code> wouldn't work...<\/p>\n<pre><code>sh-4.2$ docker-compose version\ndocker-compose version 1.29.2, build unknown\ndocker-py version: 5.0.0\nCPython version: 3.6.13\nOpenSSL version: OpenSSL 1.1.1l  24 Aug 2021\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636114943820,
        "Question_score":0,
        "Question_tags":"amazon-web-services|docker|docker-compose|amazon-sagemaker",
        "Question_view_count":312,
        "Owner_creation_time":1384530039387,
        "Owner_last_access_time":1664020852630,
        "Owner_location":"Ljubljana, Slovenia",
        "Owner_reputation":2470,
        "Owner_up_votes":910,
        "Owner_down_votes":11,
        "Owner_views":285,
        "Question_last_edit_time":1636115811803,
        "Answer_body":"<p>For those of you who (might) have encountered the same issue, here's the fix:<\/p>\n<p>1). Install the newest version of docker-compose:<\/p>\n<pre><code>sh-4.2$ sudo curl -L &quot;https:\/\/github.com\/docker\/compose\/releases\/download\/1.29.2\/docker-compose-$(uname -s)-$(uname -m)&quot; -o \/usr\/local\/bin\/docker-compose\nsh-4.2$ sudo chmod +x \/usr\/local\/bin\/docker-compose\n<\/code><\/pre>\n<p>2). Change your <code>PATH<\/code> accordingly (since docker-compose is installed using <code>conda<\/code> and is picked up first) or use <code>\/usr\/local\/bin\/docker-compose<\/code> onwards:<\/p>\n<pre><code>sh-4.2$ PATH=\/usr\/local\/bin:$PATH\nsh-4.2$ docker-compose version\ndocker-compose version 1.29.2, build 5becea4c\ndocker-py version: 5.0.0\nCPython version: 3.7.10\nOpenSSL version: OpenSSL 1.1.0l  10 Sep 2019\n<\/code><\/pre>\n<p>Perhaps, the issue is related to this:<\/p>\n<blockquote>\n<p>On August 9, 2021 the Jupyter Notebook and Jupyter Lab open source software projects announced 2 security concerns that could impact Amazon Sagemaker Notebook Instance customers.<\/p>\n<p>Sagemaker has deployed updates to address these concerns, and we recommend customers with existing notebook sessions to stop and restart their notebook instance(s) to benefit from these updates. Notebook instances launched after August 10, 2021, when updates were deployed, are not impacted by this issue and do not need to be restarted.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0,
        "Answer_creation_time":1636114943820,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1636183275063,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69853177",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71623732,
        "Question_title":"ipydatagrid widget does not display in SageMaker Studio",
        "Question_body":"<h2>The problem<\/h2>\n<p>I have a jupyter notebook with an <code>ipydatagrid<\/code> widget that displays a dataframe. This notebook works correctly when run locally, but not when run in AWS SageMaker Studio. When run in SageMaker Studio, instead of showing the widget it simply shows the text <code>Loading widget...<\/code><\/p>\n<p>How does one use a <code>ipydatagrid<\/code> widget in the SageMaker Studio environment?<\/p>\n<h2>Details<\/h2>\n<p>Python version:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>$ python --version\nPython 3.7.10\n<\/code><\/pre>\n<p>Run at start:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>$ pip install -r requirements.txt\n$ jupyter nbextension enable --py --sys-prefix widgetsnbextension\n$ jupyter nbextension install --py --symlink --sys-prefix ipydatagrid\n$ jupyter nbextension enable --py --sys-prefix ipydatagrid\n<\/code><\/pre>\n<p>File <code>requirements.txt<\/code>:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>ipydatagrid==1.1.11\npandas==1.0.1\n<\/code><\/pre>\n<p>Notebook contents:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># %%\nimport pandas as pd\nfrom ipydatagrid import DataGrid\nfrom IPython.display import display\nimport ipywidgets\n\n# %%\ndata = [\n    (&quot;potato&quot;, 1.2, True),\n    (&quot;sweet potato&quot;, 0.8, False),\n    (&quot;french fries&quot;, 4.5, True),\n    (&quot;waffle fries&quot;, 4.9, True)\n]\ndf = pd.DataFrame(\n    data,\n    columns=[&quot;food&quot;, &quot;stars&quot;, &quot;is_available&quot;]\n)\n\n# %%\ngrid = DataGrid(df)\n\n# %%\ndisplay(grid)\n\n# %%\n# SANITY CHECK:\nbutton = ipywidgets.Button(\n    description=&quot;Button&quot;,\n    disabled=False\n)\ndef on_click(b):\n    print(&quot;CLICK&quot;)\nbutton.on_click(on_click)\ndisplay(button)\n<\/code><\/pre>\n<h3>Error messages<\/h3>\n<p>If I use the Google Chrome developer tools, I can see more logs in the browser that give some error messages, most of which are repeated:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>manager.js:305 Uncaught (in promise) Error: Module ipydatagrid, semver range ^1.1.11 is not registered as a widget module\n    at C.loadClass (manager.js:305:19)\n    at C.&lt;anonymous&gt; (manager-base.js:263:46)\n    at l (manager-base.js:44:23)\n    at Object.next (manager-base.js:25:53)\n    at manager-base.js:19:71\n    at new Promise (&lt;anonymous&gt;)\n    at Rtm6.k (manager-base.js:15:12)\n    at C.e._make_model (manager-base.js:257:16)\n    at C.&lt;anonymous&gt; (manager-base.js:246:45)\n    at l (manager-base.js:44:23)\n<\/code><\/pre>\n<pre class=\"lang-none prettyprint-override\"><code>utils.js:119 Error: Could not create a model.\n    at n (utils.js:119:27)\n    at async C._handleCommOpen (manager.js:61:51)\n    at async v._handleCommOpen (default.js:994:100)\n    at async v._handleMessage (default.js:1100:43)\n<\/code><\/pre>\n<pre class=\"lang-none prettyprint-override\"><code>manager-base.js:273 Could not instantiate widget\n<\/code><\/pre>\n<p>However, there is no overt error message that's immediately obvious to the user, including in the log where <code>print<\/code> statements send their output.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648246951157,
        "Question_score":0,
        "Question_tags":"python|pandas|amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":312,
        "Owner_creation_time":1560606498250,
        "Owner_last_access_time":1663010976300,
        "Owner_location":"Provo, UT, USA",
        "Owner_reputation":528,
        "Owner_up_votes":351,
        "Owner_down_votes":61,
        "Owner_views":39,
        "Question_last_edit_time":null,
        "Answer_body":"<p>SageMaker Studio currently runs JupyterLab v1.2 (as confirmed by <em>Help &gt; About JupyterLab<\/em>), and per the <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid#Installation\" rel=\"nofollow noreferrer\">ipydatagrid installation instructions<\/a>, current\/recent versions of this widget require v3+... So I think this is most likely your problem - as there were breaking changes in the interfaces for extensions between these major versions.<\/p>\n<p>I had a quick look at the past releases of <code>ipydatagrid<\/code> to see if using an older version would be possible, and it seems like the documented JLv3 requirement gets added between <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid\/tree\/0.2.16\" rel=\"nofollow noreferrer\">v0.2.16<\/a> and <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid\/tree\/1.0.1\" rel=\"nofollow noreferrer\">v1.0.1<\/a> (which are adjacent on GitHub).<\/p>\n<p>However, the old install instructions documented on 0.2 don't seem to work anymore: I get <code>ValueError: &quot;jupyter-datagrid&quot; is not a valid npm package<\/code> and also note that versions &lt;1.0 don't seem to be present <a href=\"https:\/\/libraries.io\/pypi\/ipydatagrid\/versions\" rel=\"nofollow noreferrer\">on PyPI<\/a>.<\/p>\n<p>So unfortunately I think (unless\/until SM Studio gets a JupyterLab version upgrade), this widget's not likely to work unless you dive in to building it from an old source code version.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1648549758097,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71623732",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57243583,
        "Question_title":"Sagemaker KMeans Built-In - List of files csv as input",
        "Question_body":"<p>I Want to use <strong>Sagemaker KMeans BuilIn Algorithm<\/strong> in one of my applications. I have a large CSV file in S3 (raw data) that I split into several parts to be easy to clean. Before I had cleaned, I tried to use it as the input of Kmeans to perform the training job but It doesn't work.<\/p>\n\n<p>My manifest file:<\/p>\n\n<pre><code>[\n    {\"prefix\": \"s3:\/\/&lt;BUCKET_NAME&gt;\/kmeans_data\/KMeans-2019-28-07-13-40-00-001\/\"}, \n    \"file1.csv\", \n    \"file2.csv\"\n]\n<\/code><\/pre>\n\n<p>The error I've got:<\/p>\n\n<pre><code>Failure reason: ClientError: Unable to read data channel 'train'. Requested content-type is 'application\/x-recordio-protobuf'. Please verify the data matches the requested content-type. (caused by MXNetError) Caused by: [16:47:31] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsCppLibs\/AIAlgorithmsCppLibs-2.0.1620.0\/AL2012\/generic-flavor\/src\/src\/aialgs\/io\/iterator_base.cpp:100: (Input Error) The header of the MXNet RecordIO record at position 0 in the dataset does not start with a valid magic number. Stack trace returned 10 entries: [bt] (0) \/opt\/amazon\/lib\/libaialgs.so(+0xb1f0) [0x7fb5674c31f0] [bt] (1) \/opt\/amazon\/lib\/libaialgs.so(+0xb54a) [0x7fb5674c354a] [bt] (2) \/opt\/amazon\/lib\/libaialgs.so(aialgs::iterator_base::Next()+0x4a6) [0x7fb5674cc436] [bt] (3) \/opt\/amazon\/lib\/libmxnet.so(MXDataIterNext+0x21) [0x7fb54ecbcdb1] [bt] (4) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call_unix64+0x4c) [0x7fb567a1e858] [bt] (5) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call+0x15f) [0x7fb567a1d95f\n<\/code><\/pre>\n\n<p>My question is: It's possible to use multiple CSV files as input in Sagemaker KMeans BuilIn Algorithm only in GUI? If it's possible, How should I format my manifest?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1564336514917,
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":522,
        "Owner_creation_time":1464391892937,
        "Owner_last_access_time":1658153265653,
        "Owner_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Owner_reputation":2243,
        "Owner_up_votes":497,
        "Owner_down_votes":32,
        "Owner_views":148,
        "Question_last_edit_time":1564342604716,
        "Answer_body":"<p>the manifest looks fine, but based on the error message, it looks like you haven't set the right data format for you S3 data. It's expecting protobuf, which is the default format :)<\/p>\n\n<p>You have to set the CSV data format explicitly. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.s3_input\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.s3_input<\/a>. <\/p>\n\n<p>It should look something like this:<\/p>\n\n<pre><code>s3_input_train = sagemaker.s3_input(\n  s3_data='s3:\/\/{}\/{}\/train\/manifest_file'.format(bucket, prefix),    \n  s3_data_type='ManifestFile',\n  content_type='csv')\n\n...\n\nkmeans_estimator = sagemaker.estimator.Estimator(kmeans_image, ...)\nkmeans_estimator.set_hyperparameters(...)\n\ns3_data = {'train': s3_input_train}\nkmeans_estimator.fit(s3_data)\n<\/code><\/pre>\n\n<p>Please note the KMeans estimator in the SDK only supports protobuf, see <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/kmeans.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/kmeans.html<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1564340123623,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57243583",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57273357,
        "Question_title":"Empty dictionary on AnnotationConsolidation lambda event for aws Sagemaker",
        "Question_body":"<p>I am starting to use aws sagemaker on the development of my machine learning model and I'm trying to build a lambda function to process the responses of a sagemaker labeling job. I already created my own lambda function but when I try to read the event contents I can see that the event dict is completely empty, so I'm not getting any data to read.<\/p>\n\n<p>I have already given enough permissions to the role of the lambda function. Including:\n- AmazonS3FullAccess.\n- AmazonSagemakerFullAccess.\n- AWSLambdaBasicExecutionRole<\/p>\n\n<p>I've tried using this code for the Post-annotation Lambda (adapted for python 3.6):<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step2-demo1.html#sms-custom-templates-step2-demo1-post-annotation\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step2-demo1.html#sms-custom-templates-step2-demo1-post-annotation<\/a><\/p>\n\n<p>As well as this one in this git repository:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py<\/a><\/p>\n\n<p>But none of them seemed to work.<\/p>\n\n<p>For creating the labeling job I'm using boto3's functions for sagemaker:\n<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_labeling_job\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_labeling_job<\/a><\/p>\n\n<p>This is the code i have for creating the labeling job:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def create_labeling_job(client,bucket_name ,labeling_job_name, manifest_uri, output_path):\n\n    print(\"Creating labeling job with name: %s\"%(labeling_job_name))\n\n    response = client.create_labeling_job(\n        LabelingJobName=labeling_job_name,\n        LabelAttributeName='annotations',\n        InputConfig={\n            'DataSource': {\n                'S3DataSource': {\n                    'ManifestS3Uri': manifest_uri\n                }\n            },\n            'DataAttributes': {\n                'ContentClassifiers': [\n                    'FreeOfAdultContent',\n                ]\n            }\n        },\n        OutputConfig={\n            'S3OutputPath': output_path\n        },\n        RoleArn='arn:aws:myrolearn',\n        LabelCategoryConfigS3Uri='s3:\/\/'+bucket_name+'\/config.json',\n        StoppingConditions={\n            'MaxPercentageOfInputDatasetLabeled': 100,\n        },\n        LabelingJobAlgorithmsConfig={\n            'LabelingJobAlgorithmSpecificationArn': 'arn:image-classification'\n        },\n        HumanTaskConfig={\n            'WorkteamArn': 'arn:my-private-workforce-arn',\n            'UiConfig': {\n                'UiTemplateS3Uri':'s3:\/\/'+bucket_name+'\/templatefile'\n            },\n            'PreHumanTaskLambdaArn': 'arn:aws:lambda:us-east-1:432418664414:function:PRE-BoundingBox',\n            'TaskTitle': 'Title',\n            'TaskDescription': 'Description',\n            'NumberOfHumanWorkersPerDataObject': 1,\n            'TaskTimeLimitInSeconds': 600,\n            'AnnotationConsolidationConfig': {\n                'AnnotationConsolidationLambdaArn': 'arn:aws:my-custom-post-annotation-lambda'\n            }\n        }\n    )\n\n    return response\n<\/code><\/pre>\n\n<p>And this is the one i have for the lambda function:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>    print(\"Received event: \" + json.dumps(event, indent=2))\n    print(\"event: %s\"%(event))\n    print(\"context: %s\"%(context))\n    print(\"event headers: %s\"%(event[\"headers\"]))\n\n    parsed_url = urlparse(event['payload']['s3Uri']);\n    print(\"parsed_url: \",parsed_url)\n\n    labeling_job_arn = event[\"labelingJobArn\"]\n    label_attribute_name = event[\"labelAttributeName\"]\n\n    label_categories = None\n    if \"label_categories\" in event:\n        label_categories = event[\"labelCategories\"]\n        print(\" Label Categories are : \" + label_categories)\n\n    payload = event[\"payload\"]\n    role_arn = event[\"roleArn\"]\n\n    output_config = None # Output s3 location. You can choose to write your annotation to this location\n    if \"outputConfig\" in event:\n        output_config = event[\"outputConfig\"]\n\n    # If you specified a KMS key in your labeling job, you can use the key to write\n    # consolidated_output to s3 location specified in outputConfig.\n    kms_key_id = None\n    if \"kmsKeyId\" in event:\n        kms_key_id = event[\"kmsKeyId\"]\n\n    # Create s3 client object\n    s3_client = S3Client(role_arn, kms_key_id)\n\n    # Perform consolidation\n    return do_consolidation(labeling_job_arn, payload, label_attribute_name, s3_client)\n<\/code><\/pre>\n\n<p>I've tried debugging the event object with:<\/p>\n\n<pre><code>    print(\"Received event: \" + json.dumps(event, indent=2))\n<\/code><\/pre>\n\n<p>But it just prints an empty dictionary: <code>Received event: {}<\/code><\/p>\n\n<p>I expect the output to be something like:<\/p>\n\n<pre><code>    #Content of an example event:\n    {\n        \"version\": \"2018-10-16\",\n        \"labelingJobArn\": &lt;labelingJobArn&gt;,\n        \"labelCategories\": [&lt;string&gt;],  # If you created labeling job using aws console, labelCategories will be null\n        \"labelAttributeName\": &lt;string&gt;,\n        \"roleArn\" : \"string\",\n        \"payload\": {\n            \"s3Uri\": &lt;string&gt;\n        }\n        \"outputConfig\":\"s3:\/\/&lt;consolidated_output configured for labeling job&gt;\"\n    }\n<\/code><\/pre>\n\n<p>Lastly, when I try yo get the labeling job ARN with:<\/p>\n\n<pre><code>    labeling_job_arn = event[\"labelingJobArn\"]\n<\/code><\/pre>\n\n<p>I just get a KeyError (which makes sense because the dictionary is empty).<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1564494701657,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|aws-lambda|python-3.6|amazon-sagemaker",
        "Question_view_count":846,
        "Owner_creation_time":1525449880547,
        "Owner_last_access_time":1662375685780,
        "Owner_location":"Madrid, Spain",
        "Owner_reputation":81,
        "Owner_up_votes":54,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1575462189052,
        "Answer_body":"<p>I found the problem, I needed to add the ARN of the role used by my Lamda function as a Trusted Entity on the Role used for the Sagemaker Labeling Job.<\/p>\n\n<p>I just went to <code>Roles &gt; MySagemakerExecutionRole &gt; Trust Relationships<\/code> and added:<\/p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": [\n          \"arn:aws:iam::xxxxxxxxx:role\/My-Lambda-Role\",\n           ...\n        ],\n        \"Service\": [\n          \"lambda.amazonaws.com\",\n          \"sagemaker.amazonaws.com\",\n           ...\n        ]\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n<\/code><\/pre>\n\n<p>This made it work for me.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1564741689013,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57273357",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69693666,
        "Question_title":"How to Deploy ML Recommender System on AWS",
        "Question_body":"<p>I'm dabbling with ML and was able to take a tutorial and get it to work for my needs.  It's a simple recommender system using TfidfVectorizer and linear_kernel.  I run into a problem with how I go about deploying it through Sagemaker with an end point.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel \nimport json\nimport csv\n\nwith open('data\/big_data.json') as json_file:\n    data = json.load(json_file)\n\nds = pd.DataFrame(data)\n\ntf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\ntfidf_matrix = tf.fit_transform(ds['content'])\ncosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n\nresults = {}\n\nfor idx, row in ds.iterrows():\n    similar_indices = cosine_similarities[idx].argsort()[:-100:-1]\n    similar_items = [(cosine_similarities[idx][i], ds['id'][i]) for i in similar_indices]\n\n    results[row['id']] = similar_items[1:]\n\ndef item(id):\n    return ds.loc[ds['id'] == id]['id'].tolist()[0]\n\ndef recommend(item_id, num):\n    print(&quot;Recommending &quot; + str(num) + &quot; products similar to &quot; + item(item_id) + &quot;...&quot;)\n    print(&quot;-------&quot;)\n    recs = results[item_id][:num]\n    for rec in recs:\n        print(&quot;Recommended: &quot; + item(rec[1]) + &quot; (score:&quot; + str(rec[0]) + &quot;)&quot;)\n\nrecommend(item_id='129035', num=5)\n<\/code><\/pre>\n<p>As a starting point I'm not sure if the output from <code>tf.fit_transform(ds['content'])<\/code> is considered the model or the output from <code>linear_kernel(tfidf_matrix, tfidf_matrix)<\/code>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635046349497,
        "Question_score":0,
        "Question_tags":"machine-learning|amazon-sagemaker|tfidfvectorizer",
        "Question_view_count":63,
        "Owner_creation_time":1635045129020,
        "Owner_last_access_time":1650386337657,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1635173315200,
        "Answer_body":"<p>I came to the conclusion that I didn't need to deploy this through SageMaker.  Since the final linear_kernel output was a Dictionary I could do quick ID lookups to find correlations.<\/p>\n<p>I have it working on AWS with API Gateway\/Lambda, DynamoDB and an EC2 server to collect, process and plug the data into DynamoDB for fast lookups.  No expensive SageMaker endpoint needed.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1636075476147,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69693666",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57212696,
        "Question_title":"Gluonnlp installation not found on Sagemaker jupyter notebook",
        "Question_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/I8c93.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/I8c93.png\" alt=\"enter image description here\"><\/a>I am attempting to install Gluonnlp to a sagemaker jupyter notebook. Im using the command <code>!sudo pip3 install gluonnlp<\/code> to install.  Which is successful.  However on import I get <code>ModuleNotFoundError: No module named 'gluonnlp'<\/code><\/p>\n\n<p>I got the same issue when attempting to install mxnet with pip in the same notebook.  It was resolved when I conda installed mxnet instead.  However conda install has not been working for gluonnlp as it cannot find the package.  I can't seem to find a way to conda install gluonnlp.  Any suggestions would be highly appreciated.<\/p>\n\n<p>Here are some of the commands I have tried<\/p>\n\n<p><code>!sudo pip3 install gluonnlp<\/code><\/p>\n\n<p><code>!conda install gluonnlp<\/code> --> Anaconda cant find the package in any channels<\/p>\n\n<pre><code>!conda install pip --y\n!sudo pip3 install gluonnlp\n\n!sudo pip3 install gluonnlp\n\n!conda install -c conda-forge gluonnlp --y\n<\/code><\/pre>\n\n<p>All these commands on my import \nimport warnings<\/p>\n\n<pre><code>warnings.filterwarnings('ignore')\n\nimport io\nimport random\nimport numpy as np\nimport mxnet as mx\nimport gluonnlp as nlp\nfrom bert import data, model\n<\/code><\/pre>\n\n<p>result in the error<\/p>\n\n<pre><code>ModuleNotFoundError: No module named 'gluonnlp'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1564111030610,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|jupyter-notebook|amazon-sagemaker|gluon",
        "Question_view_count":2015,
        "Owner_creation_time":1531840489147,
        "Owner_last_access_time":1636067734717,
        "Owner_location":"Berkeley, CA, USA",
        "Owner_reputation":425,
        "Owner_up_votes":12,
        "Owner_down_votes":1,
        "Owner_views":92,
        "Question_last_edit_time":1564413127092,
        "Answer_body":"<p>this is as simple as creating a Jupyter notebook using the 'conda_mxnet_p36' kernel, and adding a cell containing:<\/p>\n\n<pre><code>!pip install gluonnlp\n<\/code><\/pre>",
        "Answer_comment_count":5,
        "Answer_creation_time":1564315813180,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57212696",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54485769,
        "Question_title":"SageMaker deploying to EIA from TF Script Mode Python3",
        "Question_body":"<p>I've fitted a Tensorflow Estimator in SageMaker using Script Mode with <code>framework_version='1.12.0'<\/code> and <code>python_version='py3'<\/code>, using a GPU instance. <\/p>\n\n<p>Calling deploy directly on this estimator works if I select deployment instance type as GPU as well. However, if I select a CPU instance type and\/or try to add an accelerator, it fails with an error that docker cannot find a corresponding image to pull. <\/p>\n\n<p>Anybody know how to train a py3 model on a GPU with Script Mode and then deploy to a CPU+EIA instance? <\/p>\n\n<hr>\n\n<p>I've found a partial workaround by taking the intermediate step of creating a TensorFlowModel from the estimator's training artifacts and then deploying from the model, but this does not seem to support python 3 (again, doesn't find a corresponding container). If I switch to python_version='py2', it will find the container, but fail to pass health checks because all my code is for python 3.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1549048113447,
        "Question_score":0,
        "Question_tags":"python-3.x|tensorflow|amazon-ec2|amazon-sagemaker|docker-pull",
        "Question_view_count":378,
        "Owner_creation_time":1361339272693,
        "Owner_last_access_time":1663965928400,
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Question_last_edit_time":1549048982467,
        "Answer_body":"<p>Unfortunately there are no TF + Python 3 + EI serving images at this time. If you would like to use TF + EI, you'll need to make sure your code is compatible with Python 2.<\/p>\n\n<p>Edit: after I originally wrote this, support for TF + Python 3 + EI has been released. At the time of this writing, I believe TF 1.12.0, 1.13.1, and 1.14.0 all have Python 3 + EI support. For the full list, see <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#tensorflow-sagemaker-estimators\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#tensorflow-sagemaker-estimators<\/a>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1549915825300,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1585784312460,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54485769",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62012264,
        "Question_title":"AWS SageMaker SparkML Schema Eroor: member.environment' failed to satisfy constraint",
        "Question_body":"<p>I am deploying a model onto AWS via Sagemaker:<\/p>\n\n<p>I set up my JSON schema as follow:<\/p>\n\n<pre><code>import json\nschema = {\n    \"input\": [\n        {\n            \"name\": \"V1\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V2\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V3\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V4\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V5\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V6\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V7\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V8\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V9\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V10\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V11\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V12\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V13\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V14\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V15\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V16\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V17\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V18\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V19\",\n            \"type\": \"double\"\n        }, \n                {\n            \"name\": \"V20\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V21\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V22\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V23\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V24\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V25\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V26\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V27\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V28\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"Amount\",\n            \"type\": \"double\"\n        },         \n    ],\n    \"output\": \n        {\n            \"name\": \"features\",\n            \"type\": \"double\",\n            \"struct\": \"vector\"\n        }\n}\nschema_json = json.dumps(schema)\nprint(schema_json)\n<\/code><\/pre>\n\n<p>And deployed as:<\/p>\n\n<pre><code>from sagemaker.model import Model\nfrom sagemaker.pipeline import PipelineModel\nfrom sagemaker.sparkml.model import SparkMLModel\n\nsparkml_data = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\n# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands\nsparkml_model = SparkMLModel(model_data=sparkml_data, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json})\nxgb_model = Model(model_data=xgb_model.model_data, image=training_image)\n\nmodel_name = 'inference-pipeline-' + timestamp_prefix\nsm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, xgb_model])\n\n    endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)\n<\/code><\/pre>\n\n<p>I got the error as below:<\/p>\n\n<p>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: 1 validation error detected: Value '{SAGEMAKER_SPARKML_SCHEMA={\"input\": [{\"type\": \"double\", \"name\": \"V1\"}, {\"type\": \"double\", \"name\": \"V2\"}, {\"type\": \"double\", \"name\": \"V3\"}, {\"type\": \"double\", \"name\": \"V4\"}, {\"type\": \"double\", \"name\": \"V5\"}, {\"type\": \"double\", \"name\": \"V6\"}, {\"type\": \"double\", \"name\": \"V7\"}, {\"type\": \"double\", \"name\": \"V8\"}, {\"type\": \"double\", \"name\": \"V9\"}, {\"type\": \"double\", \"name\": \"V10\"}, {\"type\": \"double\", \"name\": \"V11\"}, {\"type\": \"double\", \"name\": \"V12\"}, {\"type\": \"double\", \"name\": \"V13\"}, {\"type\": \"double\", \"name\": \"V14\"}, {\"type\": \"double\", \"name\": \"V15\"}, {\"type\": \"double\", \"name\": \"V16\"}, {\"type\": \"double\", \"name\": \"V17\"}, {\"type\": \"double\", \"name\": \"V18\"}, {\"type\": \"double\", \"name\": \"V19\"}, {\"type\": \"double\", \"name\": \"V20\"}, {\"type\": \"double\", \"name\": \"V21\"}, {\"type\": \"double\", \"name\": \"V22\"}, {\"type\": \"double\", \"name\": \"V23\"}, {\"type\": \"double\", \"name\": \"V24\"}, {\"type\": \"double\", \"name\": \"V25\"}, {\"type\": \"double\", \"name\": \"V26\"}, {\"type\": \"double\", \"name\": \"V27\"}, {\"type\": \"double\", \"name\": \"V28\"}, {\"type\": \"double\", \"name\": \"Amount\"}], \"output\": {\"type\": \"double\", \"name\": \"features\", \"struct\": \"vector\"}}}' at 'containers.1**.<strong>member.environment' failed to satisfy constraint: Map value must satisfy constraint: [Member must have length less than or equal to 1024<\/strong>,** Member must have length greater than or equal to 0, Member must satisfy regular expression pattern: [\\S\\s]*]<\/p>\n\n<p>I try to reduce my features to 20 and it able to deploy. Just wondering how can I Pass the schema with 29 attributes?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1590448983843,
        "Question_score":0,
        "Question_tags":"json|amazon-web-services|deployment|amazon-sagemaker",
        "Question_view_count":226,
        "Owner_creation_time":1359061977540,
        "Owner_last_access_time":1608951269537,
        "Owner_location":"New York, NY, USA",
        "Owner_reputation":427,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":62,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I do not think the environment length of 1024 limit will be increased in a short time. To work around this, you could try to rebuild the spark ml container with the <code>SAGEMAKER_SPARKML_SCHEMA<\/code> env var:<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1600448991413,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62012264",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69104302,
        "Question_title":"Terraform Error: error waiting for sagemaker notebook instance to create: unexpected state 'Failed', wanted target 'InService'. last error: %!s(<nil>)",
        "Question_body":"<p>The entire error message after executing <code>terraform apply<\/code> within the terraform-folder of <a href=\"https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance\" rel=\"nofollow noreferrer\">this source code in my GitHub-repo<\/a> (inspired by <a href=\"https:\/\/www.linkedin.com\/pulse\/terraform-sagemaker-part-2a-creating-custom-notebook-instance-david\" rel=\"nofollow noreferrer\">this tutorial<\/a> and <a href=\"https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\" rel=\"nofollow noreferrer\">its related GitHub-repo<\/a>):<\/p>\n<pre><code>aws_sagemaker_notebook_instance.notebook_instance: Creating...\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [10s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [20s elapsed]\n...\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [15m21s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [15m31s elapsed]\n\u2577\n\u2502 Error: error waiting for sagemaker notebook instance (aws-sm-notebook-instance) to create: unexpected state 'Failed', wanted target 'InService'. last error: %!s(&lt;nil&gt;)\n\u2502\n\u2502   with aws_sagemaker_notebook_instance.notebook_instance,\n\u2502   on notebook_instance.tf line 2, in resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot;:\n\u2502    2: resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot; {\n\u2502\n<\/code><\/pre>\n<p>Internet research seemed to provide the solution in <a href=\"https:\/\/yuyasugano.medium.com\/machine-learning-infrastructure-terraforming-sagemaker-part-2-f2460a9a4663\" rel=\"nofollow noreferrer\">this article<\/a>, which inspired be to increase the allowed <code>IDLE_TIME<\/code> in the <code>on-start.sh<\/code> - script to <code>IDLE_TIME=1800<\/code> (in seconds, which equals 30 minutes). This should've been sufficient for the deployment time of around 15 minutes; yet, it threw the same error again.<\/p>\n<p>Next, I found <a href=\"https:\/\/stackoverflow.com\/questions\/65884743\/resolving-broken-deleted-state-in-terraform\">this post on StackOverFlow<\/a> suggesting to<\/p>\n<blockquote>\n<p>run <code>terraform refresh<\/code>, which will cause Terraform to refresh its state\nfile against what actually exists with the cloud provider.<\/p>\n<\/blockquote>\n<p>Unfortunately, running <code>terraform apply<\/code> right after refreshing didn't resolve the issue either.\nI'm wondering why the aforementioned <code>IDLE_TIME=1800<\/code> - setting does not have any effect. This should be more than sufficient for a 15-minute apply-time.<\/p>\n<hr \/>\n<p><strong>EDIT: adding code specifics for enhanced understanding<\/strong><\/p>\n<p><strong>1. Creating the SageMaker notebook instance<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot; {\n  name                    = &quot;aws-sm-notebook-instance&quot;\n  role_arn                = aws_iam_role.notebook_iam_role.arn\n  instance_type           = &quot;ml.t2.medium&quot;\n  lifecycle_config_name   = aws_sagemaker_notebook_instance_lifecycle_configuration.notebook_config.name\n  default_code_repository = aws_sagemaker_code_repository.git_repo.code_repository_name\n}\n<\/code><\/pre>\n<p><strong>2. Defining the SageMaker notebook lifecycle configuration<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_notebook_instance_lifecycle_configuration&quot; &quot;notebook_config&quot; {\n  name      = &quot;dev-platform-al-sm-lifecycle-config&quot;\n  on_create = filebase64(&quot;..\/scripts\/on-create.sh&quot;)\n  on_start  = filebase64(&quot;..\/scripts\/on-start.sh&quot;)\n}\n<\/code><\/pre>\n<p><strong>3. Defining the Git repo to instantiate on the SageMaker notebook instance<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_code_repository&quot; &quot;git_repo&quot; {\n  code_repository_name = &quot;aws-sm-notebook-instance-repo&quot;\n\n  git_config {\n    repository_url = &quot;https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance.git&quot;\n  }\n}\n<\/code><\/pre>\n<p><strong>Contents of <code>on-start.sh<\/code> (including IDLE_TIME - parameter)<\/strong>\nNote that this script will be invoked by the <code>scripts\/autostop.py<\/code> - script, which you can find <a href=\"https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance\/blob\/main\/scripts\/autostop.py\" rel=\"nofollow noreferrer\">here<\/a> in the associated <a href=\"https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance\" rel=\"nofollow noreferrer\">public repo containing the source code<\/a>.<\/p>\n<pre><code>#!\/bin\/bash\n\nset -e\n\n## IDLE AUTOSTOP STEPS\n## ----------------------------------------------------------------\n\n## Setting the timeout (in seconds) for how long the SageMaker notebook can run idly before being auto-stopped\n# -&gt; e.g. 1800 s = 30 min since first deployment can take between 15 and 20 minutes which could then fail like so:\n# &quot;Error: error waiting for sagemaker notebook instance (aws-sm-notebook-instance) to create: unexpected state 'Failed', wanted target 'InService'. last error: %!s(&lt;nil&gt;)&quot;\n# Hint for solution under following link: https:\/\/yuyasugano.medium.com\/machine-learning-infrastructure-terraforming-sagemaker-part-2-f2460a9a4663\nIDLE_TIME=1800\n\n# Getting the autostop.py script from GitHub\necho &quot;Fetching the autostop script...&quot;\nwget https:\/\/raw.githubusercontent.com\/andreasluckert\/aws-sm-notebook-instance\/main\/scripts\/autostop.py\n\n# Using crontab to autostop the notebook when idle time is breached\necho &quot;Starting the SageMaker autostop script in cron.&quot;\n(crontab -l 2&gt;\/dev\/null; echo &quot;*\/5 * * * * \/usr\/bin\/python $PWD\/autostop.py --time $IDLE_TIME --ignore-connections&quot;) | crontab -\n\n\n\n## CUSTOM CONDA KERNEL USAGE STEPS\n## ----------------------------------------------------------------\n\n# Setting the proper user credentials\nsudo -u ec2-user -i &lt;&lt;'EOF'\nunset SUDO_UID\n\n# Setting the source for the custom conda kernel\nWORKING_DIR=\/home\/ec2-user\/SageMaker\/custom-miniconda\nsource &quot;$WORKING_DIR\/miniconda\/bin\/activate&quot;\n\n# Loading all the custom kernels\nfor env in $WORKING_DIR\/miniconda\/envs\/*; do\n    BASENAME=$(basename &quot;$env&quot;)\n    source activate &quot;$BASENAME&quot;\n    python -m ipykernel install --user --name &quot;$BASENAME&quot; --display-name &quot;Custom ($BASENAME)&quot;\ndone\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1631108848430,
        "Question_score":0,
        "Question_tags":"terraform|terraform-provider-aws|amazon-sagemaker",
        "Question_view_count":768,
        "Owner_creation_time":1572449042430,
        "Owner_last_access_time":1663946824940,
        "Owner_location":"Germany",
        "Owner_reputation":2082,
        "Owner_up_votes":342,
        "Owner_down_votes":3,
        "Owner_views":238,
        "Question_last_edit_time":1631177626360,
        "Answer_body":"<p>The solution to the problem was to check the CloudWatch Log events under <code>CloudWatch -&gt; Log groups -&gt; \/aws\/sagemaker\/NotebookInstances -&gt; aws-sm-notebook-instance\/LifecycleConfigOnCreate<\/code> to find the following error-message:<\/p>\n<pre><code>\/bin\/bash: \/tmp\/OnCreate_2021-09-08-12-24rw5al34g: \/bin\/bash^M: bad interpreter: No such file or directory\n<\/code><\/pre>\n<p>A bit of internet research brought me to <a href=\"https:\/\/askubuntu.com\/questions\/304999\/not-able-to-execute-a-sh-file-bin-bashm-bad-interpreter\/305001#305001\">this solution related to newline characters in shell-scripts<\/a>, which depend on whether you are on <code>Windows<\/code> or a <code>UNIX<\/code>-system.\nAs I'm working on Windows, the shell-scripts created in VS-Code comprised dos-specific <code>CRLF<\/code> newline-handling, which could be resolved via the button on the bottom-right in <code>VS-Code<\/code> to switch the <em>carriage return<\/em> (CRLF) character to the <em>line feed<\/em> (LF) character used by UNIX.<\/p>\n<p>As the compute instance employed by AWS Sagemaker is a Linux-system, it cannot handle the dos-style CRLF newline-characters in the shell-scripts and this &quot;adds&quot; a <code>^M<\/code> after <code>\/bin\/bash<\/code> which obviously leads to an error as such an interpreter does not exist.<\/p>\n<p>So, finally <code>terraform apply<\/code> worked out well:<\/p>\n<pre><code>$ terraform apply\n...\n...\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [7m30s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [7m40s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Creation complete after 7m43s [id=aws-sm-notebook-instance]\n\nApply complete! Resources: 1 added, 1 changed, 1 destroyed.\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1631187921613,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69104302",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63128111,
        "Question_title":"Sagemaker Object2Vec training samples per second",
        "Question_body":"<p>I am using Sagemaker Object2Vec to train on data of size 2GB.<\/p>\n<p>ml.p2.xlarge instance took 12 hours to train the data on 4 epochs going at the speed of 5000 samples\/sec.<\/p>\n<p>Now, I am using a higher level instance ml.p2.16xlarge and it only trains at 400 samples\/sec with this in the logs<\/p>\n<pre><code>[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:739: only 114 out of 240 GPU pairs are enabled direct access. It may affect the performance. You can set MXNET_ENABLE_GPU_P2P=0 to turn it off\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .vvvvvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: v.vvvvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vv.vvvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvv.vvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvv.vvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvv.vvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvvv.vv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvvvv.v.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvvvvv........\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: ..........vvvvvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........v.vvvvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vv.vvvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvv.vvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvvv.vv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvvvv.v\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvvvvv.\n<\/code><\/pre>\n<p>There are about 50 million samples.<\/p>\n<p>What can I do to correct this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1595917104483,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":59,
        "Owner_creation_time":1504123657673,
        "Owner_last_access_time":1622762763367,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>2 ideas:<\/p>\n<ol>\n<li>Before increasing the GPU count, grow batch size so that a single\nGPU is as busy as possible<\/li>\n<li>Use P3 instances instead of P2. P3 is more recent, has more memory, more CUDA cores, faster memory bandwidth and NVLink inter-GPU connections. Though it's more\nexpensive by hour, your total training cost may be much smaller if\nproperly tuned<\/li>\n<\/ol>\n<p>Also, if your problem involves sparse updates, meaning if just a small fraction of all tokens appear in a given mini-batch, you can try using <code>token_embedding_storage_type = 'row_sparse'<\/code>, which I think refers to using sparse gradient updates like described here <a href=\"https:\/\/medium.com\/apache-mxnet\/learning-embeddings-for-music-recommendation-with-mxnets-sparse-api-5698f4d7d8\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/apache-mxnet\/learning-embeddings-for-music-recommendation-with-mxnets-sparse-api-5698f4d7d8<\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1595937519410,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63128111",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59773503,
        "Question_title":"Using Sagemaker predictor in a Spark UDF function",
        "Question_body":"<p>I am trying to run inference on a Tensorflow model deployed on SageMaker from a Python Spark job.\nI am running a (Databricks) notebook which has the following cell:<\/p>\n\n<pre><code>def call_predict():\n        batch_size = 1\n        data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n        tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n        prediction = predictor.predict(tensor_proto)\n        print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n<\/code><\/pre>\n\n<p>If I just call call_predict() it works fine:<\/p>\n\n<pre><code>call_predict()\n<\/code><\/pre>\n\n<p>and I get the output:<\/p>\n\n<pre><code>Process time: 65.261396\nOut[61]: {'model_spec': {'name': u'generic_model',\n  'signature_name': u'serving_default',\n  'version': {'value': 1578909324L}},\n 'outputs': {u'ages': {'dtype': 1,\n   'float_val': [5.680944442749023],\n   'tensor_shape': {'dim': [{'size': 1L}]}}}}\n<\/code><\/pre>\n\n<p>but when I try to call from a Spark context (in a UDF) I get a serialization error.\nThe code I'm trying to run is:<\/p>\n\n<pre><code>dataRange = range(1, 10001)\nrangeRDD = sc.parallelize(dataRange, 8)\nnew_data = rangeRDD.map(lambda x : call_predict())\nnew_data.count()\n<\/code><\/pre>\n\n<p>and the error I get is:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nPicklingError                             Traceback (most recent call last)\n&lt;command-2282434&gt; in &lt;module&gt;()\n      2 rangeRDD = sc.parallelize(dataRange, 8)\n      3 new_data = rangeRDD.map(lambda x : call_predict())\n----&gt; 4 new_data.count()\n      5 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in count(self)\n   1094         3\n   1095         \"\"\"\n-&gt; 1096         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n   1097 \n   1098     def stats(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in sum(self)\n   1085         6.0\n   1086         \"\"\"\n-&gt; 1087         return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n   1088 \n   1089     def count(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in fold(self, zeroValue, op)\n    956         # zeroValue provided to each partition is unique from the one provided\n    957         # to the final reduce call\n--&gt; 958         vals = self.mapPartitions(func).collect()\n    959         return reduce(op, vals, zeroValue)\n    960 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in collect(self)\n    829         # Default path used in OSS Spark \/ for non-credential passthrough clusters:\n    830         with SCCallSiteSync(self.context) as css:\n--&gt; 831             sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n    832         return list(_load_from_socket(sock_info, self._jrdd_deserializer))\n    833 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _jrdd(self)\n   2573 \n   2574         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n-&gt; 2575                                       self._jrdd_deserializer, profiler)\n   2576         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n   2577                                              self.preservesPartitioning, self.is_barrier)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _wrap_function(sc, func, deserializer, serializer, profiler)\n   2475     assert serializer, \"serializer should not be empty\"\n   2476     command = (func, profiler, deserializer, serializer)\n-&gt; 2477     pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n   2478     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n   2479                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _prepare_for_python_RDD(sc, command)\n   2461     # the serialized command will be compressed by broadcast\n   2462     ser = CloudPickleSerializer()\n-&gt; 2463     pickled_command = ser.dumps(command)\n   2464     if len(pickled_command) &gt; sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  # Default 1M\n   2465         # The broadcast will have same life cycle as created PythonRDD\n\n\/databricks\/spark\/python\/pyspark\/serializers.pyc in dumps(self, obj)\n    709                 msg = \"Could not serialize object: %s: %s\" % (e.__class__.__name__, emsg)\n    710             cloudpickle.print_exec(sys.stderr)\n--&gt; 711             raise pickle.PicklingError(msg)\n    712 \n    713 \n\nPicklingError: Could not serialize object: TypeError: can't pickle _ssl._SSLSocket objects\n<\/code><\/pre>\n\n<p>Not sure what is this serialization error - does is complain about failing to deserialize the Predictor<\/p>\n\n<p>My notebook has a cell which was called prior to the above cells with the following imports:<\/p>\n\n<pre><code>import sagemaker\nimport boto3\nfrom sagemaker.tensorflow.model import TensorFlowPredictor\nimport tensorflow as tf\nimport numpy as np\nimport time\n<\/code><\/pre>\n\n<p>The Predictor was created with the following code:<\/p>\n\n<pre><code>sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY,\n                                aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\nsagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY,\n                                        aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\nboto_session = boto3.Session(region_name='us-east-1')\nsagemaker_session = sagemaker.Session(boto_session, sagemaker_client=sagemaker_client, sagemaker_runtime_client=sagemaker_runtime_client)\n\npredictor = TensorFlowPredictor('endpoint-poc', sagemaker_session)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579190415880,
        "Question_score":1,
        "Question_tags":"tensorflow|pyspark|amazon-sagemaker",
        "Question_view_count":322,
        "Owner_creation_time":1458550179920,
        "Owner_last_access_time":1658058794840,
        "Owner_location":null,
        "Owner_reputation":383,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The udf function will be executed by multiple spark tasks in parallel. Those tasks run in completely isolated python processes and they are scheduled to physically different machines. Hence each data, those functions reference, must be on the same node. This is the case for everything created within the udf.<\/p>\n\n<p>Whenever you reference any object outside of the udf from the function, this data structure needs to be serialised (pickled) to each executor. Some object state, like open connections to a socket, cannot be pickled.<\/p>\n\n<p>You need to make sure, that connections are lazily opened each executor. It must happen only on the first function call on that executor. The <a href=\"https:\/\/spark.apache.org\/docs\/latest\/streaming-programming-guide.html#design-patterns-for-using-foreachrdd\" rel=\"nofollow noreferrer\">connection pooling topic<\/a> is covered in the docs, however only in the spark streaming guide (though it also applies for normal batch jobs).<\/p>\n\n<p>Normally one can use the Singleton Pattern for this. But in python people use the Borgh pattern.<\/p>\n\n<pre><code>class Env:\n    _shared_state = {\n        \"sagemaker_client\": None\n        \"sagemaker_runtime_client\": None\n        \"boto_session\": None\n        \"sagemaker_session\": None\n        \"predictor\": None\n    }\n    def __init__(self):\n        self.__dict__ = self._shared_state\n        if not self.predictor:\n            self.sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n            self.sagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\n            self.boto_session = boto3.Session(region_name='us-east-1')\n            self.sagemaker_session = sagemaker.Session(self.boto_session, sagemaker_client=self.sagemaker_client, sagemaker_runtime_client=self.sagemaker_runtime_client)\n\n            self.predictor = TensorFlowPredictor('endpoint-poc', self.sagemaker_session)\n\n\n#....\ndef call_predict():\n   env = Env()\n   batch_size = 1\n   data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n   tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n   prediction = env.predictor.predict(tensor_proto)\n\n   print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n\nnew_data = rangeRDD.map(lambda x : call_predict())\n<\/code><\/pre>\n\n<p>The Env class is defined on the master node. Its <code>_shared_state<\/code> has empty entries. When then Env object is instantiated first time, it shares the state with all further instances of Env on any subsequent call to the udf. On each separate parallel running process this will happen exactly one time. This way the sessions are shared and do not need to pickled. <\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1579206604950,
        "Answer_score":1,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1579506845700,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59773503",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62602435,
        "Question_title":"How to train tensorflow on sagemaker in script mode when the data resides in multiple files on s3?",
        "Question_body":"<p>I have a <code>.npy<\/code> file for each one of the training instances. All of these files are available on S3 in <code>train_data<\/code> folder. I want to train a tensorflow model on these training instances. To do that, I wish to spin up separate aws training instance for each training job which could access the files from s3 and train the model on it. What changes in the training script are required for doing this?<\/p>\n<p>I have following config in the training script:<\/p>\n<pre><code>parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])\nparser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\nparser.add_argument('--train_channel', type=str, default=os.environ['SM_CHANNELS'])\n<\/code><\/pre>\n<p>I have created the training estimator in jupyter instance as:<\/p>\n<pre><code>tf_estimator = TensorFlow(entry_point = 'my_model.py', \n                          role = role, \n                          train_instance_count = 1, \n                          train_instance_type = 'local_gpu', \n                          framework_version = '1.15.2', \n                          py_version = 'py3', \n                          hyperparameters = {'epochs': 1})\n<\/code><\/pre>\n<p>I am calling the fit function of the estimator as:<\/p>\n<pre><code>tf_estimator.fit({'train_channel':'s3:\/\/sagemaker-ml\/train_data\/'})\n<\/code><\/pre>\n<p>where <code>train_data<\/code> folder on S3 contains the <code>.npy<\/code> files of training instances.<\/p>\n<p>But when I call the fit function, I get an error:<\/p>\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '[&quot;train_channel&quot;]\/train_data_12.npy'\n<\/code><\/pre>\n<p>Not sure what am I missing here, as I can see the file mentioned above on S3.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593203486503,
        "Question_score":0,
        "Question_tags":"amazon-s3|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":340,
        "Owner_creation_time":1378268842847,
        "Owner_last_access_time":1663808352557,
        "Owner_location":"Pune, India",
        "Owner_reputation":4616,
        "Owner_up_votes":751,
        "Owner_down_votes":5,
        "Owner_views":592,
        "Question_last_edit_time":1593203849583,
        "Answer_body":"<p><code>SM_CHANNELS<\/code> returns a list of channel names. What you're looking for is <code>SM_CHANNEL_TRAIN_CHANNEL<\/code> (&quot;SM_CHANNEL&quot; + your channel name), which provides the filesystem location for the channel:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>parser.add_argument('--train_channel', type=str, default=os.environ['SM_CHANNEL_TRAIN_CHANNEL'])\n<\/code><\/pre>\n<p>docs: <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_channel_channel_name\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_channel_channel_name<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1593530314240,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62602435",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72490682,
        "Question_title":"How to create an aws sagemaker project using terraform?",
        "Question_body":"<p>This is the terraform shown in the docs:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.example.id\n  }\n}\n<\/code><\/pre>\n<p>I created a service catalog product with id: &quot;prod-xxxxxxxxxxxxx&quot;.\nWhen I substitute the service catalog product id into the above template,\nto get the following:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.prod-xxxxxxxxxxxxx\n  }\n}\n<\/code><\/pre>\n<p>I run terraform plan, but the following error occurs:<\/p>\n<pre><code>A managed resource &quot;aws_servicecatalog_product&quot; &quot;prod-xxxxxxxxxxxxx&quot; has not been declared in the root module.\n\n<\/code><\/pre>\n<p>What do I need to do to fix this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1654264489180,
        "Question_score":1,
        "Question_tags":"amazon-web-services|terraform|amazon-sagemaker",
        "Question_view_count":298,
        "Owner_creation_time":1653511725307,
        "Owner_last_access_time":1663251784530,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":1654266014543,
        "Answer_body":"<p>Since the documentation is lacking a bit of clarity, in order to have this work as in the example, you would first have to create the Service Catalog product in Terraform as well, e.g.:<\/p>\n<pre><code>resource &quot;aws_servicecatalog_product&quot; &quot;example&quot; {\n  name  = &quot;example&quot;\n  owner = [aws_security_group.example.id] # &lt;---- This would need to be created first\n  type  = aws_subnet.main.id # &lt;---- This would need to be created first\n\n  provisioning_artifact_parameters {\n    template_url = &quot;https:\/\/s3.amazonaws.com\/cf-templates-ozkq9d3hgiq2-us-east-1\/temp1.json&quot;\n  }\n\n  tags = {\n    foo = &quot;bar&quot;\n  }\n}\n<\/code><\/pre>\n<p>You can reference it then in the SageMaker project the same way as in the example:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.example.id\n  }\n}\n<\/code><\/pre>\n<p>Each of the resources that gets created has a set of attributes that can be accessed as needed by other resources, data sources or outputs. In order to understand how this works, I strongly suggest reading the documentation about referencing values [1]. Since you already created the Service Catalog product, the only thing you need to do is provide the string value for the product ID:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = &quot;prod-xxxxxxxxxxxxx&quot;\n  }\n}\n<\/code><\/pre>\n<p>When I can't understand what value is expected by an argument (e.g., <code>product_id<\/code> in this case), I usually read the docs and look for examples like in [2]. Note: That example is CloudFormation, but it can help you understand what type of a value is expected (e.g., string, number, bool).<\/p>\n<p>You could also import the created Service Catalog product into Terraform so you can manage it with IaC [3]. You should understand all the implications of <code>terraform import<\/code> though before trying it [4].<\/p>\n<hr \/>\n<p>[1] <a href=\"https:\/\/www.terraform.io\/language\/expressions\/references\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/expressions\/references<\/a><\/p>\n<p>[2] <a href=\"https:\/\/docs.amazonaws.cn\/en_us\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-project.html#aws-resource-sagemaker-project--examples--SageMaker_Project_Example\" rel=\"nofollow noreferrer\">https:\/\/docs.amazonaws.cn\/en_us\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-project.html#aws-resource-sagemaker-project--examples--SageMaker_Project_Example<\/a><\/p>\n<p>[3] <a href=\"https:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/resources\/servicecatalog_product#import\" rel=\"nofollow noreferrer\">https:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/resources\/servicecatalog_product#import<\/a><\/p>\n<p>[4] <a href=\"https:\/\/www.terraform.io\/cli\/commands\/import\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/cli\/commands\/import<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1654269294487,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72490682",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56721821,
        "Question_title":"Can I copy or upload a Google Colab notebook onto a AWS Sagemaker instance?",
        "Question_body":"<p>Want to have a copy of my Google Colab python notebook in my AWS Sagemaker Jupyter notebook(newbie to AWS Sagemaker)<\/p>\n\n<p>I tried selecting all cells in my Colab notebook and pasting in my sagemaker Jupyter notebook using copy paste icons and via cmd+C and cmd+V<\/p>\n\n<p>Cannot copy paste all selected cells at once between Colab and Sagemaker Jupyter notebooks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1561274387650,
        "Question_score":1,
        "Question_tags":"python|jupyter-notebook|google-colaboratory|amazon-sagemaker",
        "Question_view_count":1621,
        "Owner_creation_time":1540309037063,
        "Owner_last_access_time":1646061363073,
        "Owner_location":null,
        "Owner_reputation":404,
        "Owner_up_votes":44,
        "Owner_down_votes":1,
        "Owner_views":24,
        "Question_last_edit_time":1561274899232,
        "Answer_body":"<p>While doing the drudgery work of copy pasting each cell between the notebooks(my bad), I realized that we could just <strong>download the notebook as .ipynb file on Colab<\/strong> and <strong>upload on the Sagemaker notebook instance using the <code>Upload button<\/code><\/strong>.<a href=\"https:\/\/i.stack.imgur.com\/z5wHt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/z5wHt.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1561274854930,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1561536601703,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56721821",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58755708,
        "Question_title":"Sagemaker Notebook Instance Type Recommendation",
        "Question_body":"<p>I will be running ml models on a pretty large dataset. It is about 15 gb, with 200 columns and 4.3 million rows. I'm wondering what the best Notebook instance type is for this kind of dataset in AWS Sagemaker.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1573155038577,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":2326,
        "Owner_creation_time":1568738384723,
        "Owner_last_access_time":1630159299740,
        "Owner_location":null,
        "Owner_reputation":137,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p><strong>For choosing a SageMaker hosted notebook type:<\/strong><\/p>\n\n<p>Do you plan to do all of your preprocessing of your data in-memory on the notebook, or do you plan to orchestrate ETL with external services? <\/p>\n\n<p>If you're planning to load the dataset into memory on the notebook instance for exploration\/preprocessing, the primary bottleneck here would be ensuring the instance has enough memory for your dataset. This would require at least the 16gb types (<em>.xlarge<\/em>) (full list of ML instance types <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/\" rel=\"noreferrer\">available here<\/a>). Further, depending on how compute intensive your pre-processing is, and your desired pre-processing completion time, you can opt for a compute optimized instance (<em>c4, c5<\/em>) to speed this up.<\/p>\n\n<hr>\n\n<p><strong>For the training job, specifically:<\/strong><\/p>\n\n<p>Using the Amazon SageMaker SDK, your training data will be loaded and distributed to the training cluster, allowing your training job to be completely separate from the instance your hosted notebook is running on.<\/p>\n\n<p>Figuring out the ideal instance type for training will depend on whether your algorithm of choice\/training job is memory, CPU, or IO bound. Since your dataset will likely be loaded onto your training cluster from S3, the instance you choose for your hosted notebook will have no bearing on the speed of your training job.<\/p>\n\n<hr>\n\n<p><strong>Broadly:<\/strong>\nWhen it comes to SageMaker notebooks, the best practice is to use your notebook as a \"puppeteer\" or orchestrator, that calls out to external services (AWS Glue or Amazon EMR for preprocessing, SageMaker for training, S3 for storage, etc). It is best to treat them as ephemeral forms of compute\/storage for building and kicking off your experiment pipeline.<\/p>\n\n<p>This will allow you to more closely pair compute, storage, and hosting resources\/services with the demands for your workload, ultimately resulting in the best bang for your buck by not having you pay for latent or unused resources.<\/p>\n\n<hr>",
        "Answer_comment_count":0,
        "Answer_creation_time":1573157762370,
        "Answer_score":6,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58755708",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71221741,
        "Question_title":"ValidationException in Sagemaker pipeline creation",
        "Question_body":"<p>I am new to Sagmaker. I am creating a pipeline in sagemaker where I initialize the number of epochs as a pipeline parameter. But when I upsert, it shows this error.\nCheck the following code for reference, please.<\/p>\n<pre><code>epoch_count = ParameterInteger(name=&quot;EpochCount&quot;, default_value=1)\npipeline = Pipeline(\nname=f&quot;a_name&quot;,\nparameters=[\n    training_instance_type,\n    training_instance_count,\n    epoch_count,\n    hugging_face_model_name,\n    endpoint_instance_type,\n    endpoint_instance_type_alternate,\n],\nsteps=[step_train, step_register, step_deploy_lambda],\nsagemaker_session=sagemaker_session,\n<\/code><\/pre>\n<p>)<\/p>\n<p>Error - ---<\/p>\n<pre><code>---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\n&lt;ipython-input-54-138a517611f0&gt; in &lt;module&gt;\n----&gt; 1 pipeline.upsert(role_arn=role)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in upsert(self, role_arn, description, tags, parallelism_config)\n    217         &quot;&quot;&quot;\n    218         try:\n--&gt; 219             response = self.create(role_arn, description, tags, parallelism_config)\n    220         except ClientError as e:\n    221             error = e.response[&quot;Error&quot;]\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in create(self, role_arn, description, tags, parallelism_config)\n    119             Tags=tags,\n    120         )\n--&gt; 121         return self.sagemaker_session.sagemaker_client.create_pipeline(**kwargs)\n    122 \n    123     def _create_args(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    389                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    390             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 391             return self._make_api_call(operation_name, kwargs)\n    392 \n    393         _api_call.__name__ = str(py_operation_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    717             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)\n    718             error_class = self.exceptions.from_code(error_code)\n--&gt; 719             raise error_class(parsed_response, operation_name)\n    720         else:\n    721             return parsed_response\n\nClientError: An error occurred (ValidationException) when calling the CreatePipeline operation: Cannot assign property reference [Parameters.EpochCount] to argument of type [String]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645534662633,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":608,
        "Owner_creation_time":1383025927423,
        "Owner_last_access_time":1663674890763,
        "Owner_location":"Dhaka, Bangladesh",
        "Owner_reputation":647,
        "Owner_up_votes":62,
        "Owner_down_votes":3,
        "Owner_views":105,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I replace<\/p>\n<pre><code>epoch_count = ParameterInteger(name=&quot;EpochCount&quot;, default_value=1)\n<\/code><\/pre>\n<p>with<\/p>\n<pre><code>epoch_count = ParameterString(name=&quot;EpochCount&quot;, default_value=&quot;1&quot;)\n<\/code><\/pre>\n<p>And it works. Maybe we can only use an integer in pipeline parameters from the sagemaker notebook. But epoch_count is being used in the docker container, which is not directly something of Sagemaker, and that's my understanding.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1645641627683,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1649157854310,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71221741",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53280902,
        "Question_title":"AWS SageMaker pd.read_pickle() doesn't work but read_csv() does?",
        "Question_body":"<p>I've recently been trying to train some models on an AWS SageMaker jupyter notebook instance.<\/p>\n\n<p>Everything is worked very well until I tried to load in some custom dataset (REDD) through files.<\/p>\n\n<p>I have the dataframes stored in Pickle (.pkl) files on an S3 bucket. I couldn't manage to read them into sagemaker so I decided to convert them to csv's as this seemed to work but I ran into a problem. This data has an index of type datetime64 and when using <code>.to_csv()<\/code> this index gets converted to pure text and it loses it's data structure (and I need to keep this specific index for correct plotting.)<\/p>\n\n<p>So I decided to try the Pickle files again but I can't get it to work and have no idea why.<\/p>\n\n<p>The following code for csv's works but I can't use it due to the index problem:<\/p>\n\n<pre><code>bucket = 'sagemaker-peno'\nhouses_dfs = {}\ndata_key = 'compressed_data\/'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\nfor file in range(6):\n    houses_dfs[file+1] = pd.read_csv(data_location+'house_'+str(file+1)+'.csv', index_col='Unnamed: 0')\n<\/code><\/pre>\n\n<p>But this code does NOT work even though it uses almost the exact same syntax:<\/p>\n\n<pre><code>bucket = 'sagemaker-peno'\nhouses_dfs = {}\ndata_key = 'compressed_data\/'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\nfor file in range(6):\n    houses_dfs[file+1] =  pd.read_pickle(data_location+'house_'+str(file+1)+'.pkl')\n<\/code><\/pre>\n\n<p>Yes it's 100% the correct path, because the csv and pkl files are stored in the same directory (compressed_data).<\/p>\n\n<p>It throws me this error while using the Pickle method:<\/p>\n\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/sagemaker-peno\/compressed_data\/house_1.pkl'\n<\/code><\/pre>\n\n<p>I hope to find someone who has dealt with this before and can solve the <code>read_pickle()<\/code> issue or as an alternative fix my datetime64 type issue with csv's.<\/p>\n\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1542111648037,
        "Question_score":1,
        "Question_tags":"pandas|csv|amazon-s3|pickle|amazon-sagemaker",
        "Question_view_count":788,
        "Owner_creation_time":1439124701797,
        "Owner_last_access_time":1657225764310,
        "Owner_location":"Leuven, Belgi\u00eb",
        "Owner_reputation":87,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Question_last_edit_time":null,
        "Answer_body":"<p>read_pickle() likes the full path more than a relative path from where it was run. This fixed my issue.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1543264373167,
        "Answer_score":-1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53280902",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54334462,
        "Question_title":"How can I quickly debug a SageMaker training script?",
        "Question_body":"<p>When running an ML training job in Amazon SageMaker, the training script is \"deployed\" and given an ML training instance, which takes about 10 minutes to spin up and get the data it needs. <\/p>\n\n<p>I can only get one error message from the training job, then it dies and the instance is killed along with it. <\/p>\n\n<p>After I make a change to the training script to fix it, I need to deploy and run it which takes another 10 minutes or so.<\/p>\n\n<p>How can I accomplish this faster, or keep the training instance running?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1548272148563,
        "Question_score":4,
        "Question_tags":"amazon-web-services|tensorflow|machine-learning|amazon-sagemaker",
        "Question_view_count":1902,
        "Owner_creation_time":1416193017423,
        "Owner_last_access_time":1663696463113,
        "Owner_location":"Gensokyo",
        "Owner_reputation":880,
        "Owner_up_votes":211,
        "Owner_down_votes":2,
        "Owner_views":111,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It seems that you are running a training job using one of the SageMaker frameworks. Given that, you can use the \"local mode\" feature of SageMaker, which will run your training job (specifically the container) locally in your notebook instance. That way, you can iterate on your script until it works. Then you can move on to the remote training cluster to train the model against the whole dataset if needed. To use local mode, you just set the instance type to \"local\". More details about local mode can be found at <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#sagemaker-python-sdk-overview\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#sagemaker-python-sdk-overview<\/a> and the blog post: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1548282026927,
        "Answer_score":5,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54334462",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62002183,
        "Question_title":"Use tensorboard with object detection API in sagemaker",
        "Question_body":"<p>with <a href=\"https:\/\/github.com\/svpino\/tensorflow-object-detection-sagemaker\" rel=\"nofollow noreferrer\">this<\/a> I successfully created a training job on sagemaker using the Tensorflow Object Detection API in a docker container. Now I'd like to monitor the training job using sagemaker, but cannot find anything explaining how to do it. I don't use a sagemaker notebook.\nI think I can do it by saving the logs into a S3 bucket and point there a local tensorboard instance .. but don't know how to tell the tensorflow object detection API where to save the logs (is there any command line argument for this ?).\nSomething like <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/keras_script_mode_pipe_mode_horovod\/tensorflow_keras_CIFAR10.ipynb\" rel=\"nofollow noreferrer\">this<\/a>, but the script <code>generate_tensorboard_command.py<\/code> fails because my training job don't have the <code>sagemaker_submit_directory<\/code> parameter..<\/p>\n<p>The fact is when I start the training job nothing is created on my s3 until the job finish and upload everything. There should be a way tell tensorflow where to save the logs (s3) during the training, hopefully without modifying the API source code..<\/p>\n<p><strong>Edit<\/strong><\/p>\n<p>I can finally make it works with the accepted solution (tensorflow natively supports read\/write to s3), there are however additional steps to do:<\/p>\n<ol>\n<li>Disable network isolation in the training job configuration<\/li>\n<li>Provide credentials to the docker image to write to S3 bucket<\/li>\n<\/ol>\n<p>The only thing is that Tensorflow continuously polls filesystem (i.e. looking for an updated model in serving mode) and this cause useless requests to S3, that you will have to pay (together with a buch of errors in the console). I opened a new question <a href=\"https:\/\/stackoverflow.com\/q\/64969198\/4267439\">here<\/a> for this. At least it works.<\/p>\n<p><strong>Edit 2<\/strong><\/p>\n<p>I was wrong, TF just write logs, is not polling so it's an expected behavior and the extra costs are minimal.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1590408759343,
        "Question_score":2,
        "Question_tags":"object-detection|tensorboard|amazon-sagemaker|object-detection-api|tensorflow-model-garden",
        "Question_view_count":311,
        "Owner_creation_time":1416346350293,
        "Owner_last_access_time":1664039219437,
        "Owner_location":"Jesi, Italy",
        "Owner_reputation":2302,
        "Owner_up_votes":51,
        "Owner_down_votes":4,
        "Owner_views":227,
        "Question_last_edit_time":1606323198012,
        "Answer_body":"<p>Looking through the example you posted, it looks as though the <code>model_dir<\/code> passed to the TensorFlow Object Detection package is configured to <code>\/opt\/ml\/model<\/code>:<\/p>\n<pre><code># These are the paths to where SageMaker mounts interesting things in your container.\nprefix = '\/opt\/ml\/'\ninput_path = os.path.join(prefix, 'input\/data')\noutput_path = os.path.join(prefix, 'output')\nmodel_path = os.path.join(prefix, 'model')\nparam_path = os.path.join(prefix, 'input\/config\/hyperparameters.json')\n<\/code><\/pre>\n<p>During the training process, tensorboard logs will be written to <code>\/opt\/ml\/model<\/code>, and then uploaded to s3 as a final model artifact AFTER training: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html<\/a>.<\/p>\n<p>You <em>might<\/em> be able to side-step the SageMaker artifact upload step and point the <code>model_dir<\/code> of TensorFlow Object Detection API directly at an s3 location during training:<\/p>\n<pre><code>model_path = &quot;s3:\/\/your-bucket\/path\/here\n<\/code><\/pre>\n<p>This means that the TensorFlow library within the SageMaker job is directly writing to S3 instead of the filesystem inside of it's container. Assuming the underlying TensorFlow Object Detection code can write directly to S3 (something you'll have to verify), you should be able to see the tensorboard logs and checkpoints there in realtime.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1594137982057,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62002183",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62309772,
        "Question_title":"Difference between Processor and ScriptProcessor in AWS Sagemaker SDK",
        "Question_body":"<p>From <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Sagemaker python SDK<\/a> I have seen two API, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.ScriptProcessor\" rel=\"nofollow noreferrer\">ScriptProcessor<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.Processor\" rel=\"nofollow noreferrer\">Processor<\/a>. It seems like we can achieve the same goals using either of them, the only difference I noticed ScriptProcessor support docker <code>command<\/code> parameter on the other hand Processor support docker <code>entrypoint<\/code> parameter. Is there any other difference amongst them? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591810831397,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":400,
        "Owner_creation_time":1370231111260,
        "Owner_last_access_time":1663504379830,
        "Owner_location":"Toronto, ON, Canada",
        "Owner_reputation":5295,
        "Owner_up_votes":351,
        "Owner_down_votes":4,
        "Owner_views":455,
        "Question_last_edit_time":null,
        "Answer_body":"<p><code>sagemaker.processing.ScriptProcessor<\/code> subclasses <code>sagemaker.processing.Processor<\/code>. <code>ScriptProcessor<\/code> can be used to write a custom processing script. <code>Processor<\/code> can be subclassed to create a <code>CustomProcessor<\/code> class for a more complex use case.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1604879360050,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62309772",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65753455,
        "Question_title":"How to get the version of gremlin python client on AWS SageMaker",
        "Question_body":"<p>What is the command to check the version of Gremlin Python client running on a AWS Sagemaker jupyter notebook? I would like to run the command on the jupyter notebook cell.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610823303577,
        "Question_score":0,
        "Question_tags":"gremlin|amazon-sagemaker",
        "Question_view_count":198,
        "Owner_creation_time":1475704783950,
        "Owner_last_access_time":1639340625583,
        "Owner_location":null,
        "Owner_reputation":473,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Question_last_edit_time":null,
        "Answer_body":"<p>From a notebook cell you should be able to just ask Pip which version is being used<\/p>\n<pre><code>!pip list | grep gremlinpython\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1610839775480,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65753455",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59801874,
        "Question_title":"When do you specify the Target variable in a SageMaker Training job?",
        "Question_body":"<p>I'm trying to create a Machine Learning algorithm following this tutorial : <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-console.html\" rel=\"nofollow noreferrer\">Get Started with Amazon SageMaker<\/a><\/p>\n\n<p>Unless I missed something in the tutorial, I didn't find any steps where we specify the target variable. Can someone explain where \/ when we specify our target variable when creating an ML model using SageMaker built-in algorithms? <\/p>\n\n<p>Thanks a lot! <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1579360480360,
        "Question_score":2,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":715,
        "Owner_creation_time":1562055808543,
        "Owner_last_access_time":1663141286633,
        "Owner_location":null,
        "Owner_reputation":895,
        "Owner_up_votes":29,
        "Owner_down_votes":1,
        "Owner_views":53,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It depends on the scientific paradigm you're using in SageMaker :)<\/p>\n\n<ul>\n<li>SageMaker Built-in algorithms all have their input specification,\ndescribed in their respective documentation. For example, for\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner.html#ll-input_output\" rel=\"nofollow noreferrer\">SageMaker Linear Learner<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#InputOutput-XGBoost\" rel=\"nofollow noreferrer\">SageMaker XGBoost<\/a> the target is assumed\nto be the first column.<\/li>\n<li>With custom code, such as Bring-Your-Own-Docker or SageMaker Framework containers (for Sklearn, TF, PyTorch, MXNet) since you are the one writing the code you can write any sort of logic, and the target can be any column of your dataset.<\/li>\n<\/ul>",
        "Answer_comment_count":0,
        "Answer_creation_time":1579512805583,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59801874",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65285203,
        "Question_title":"Why does AWS SageMaker create an S3 Bucket",
        "Question_body":"<p>Upon deploying a custom pytorch model with the boto3 client in python. I noticed that a new S3 bucket had been created with no visible objects. Is there a reason for this?<\/p>\n<p>The bucket that contained my model was named with the keyword &quot;sagemaker&quot; included, so I don't any issue there.<\/p>\n<p>Here is the code that I used for deployment:<\/p>\n<pre><code>remote_model = PyTorchModel(\n                     name = model_name, \n                     model_data=model_url,\n                     role=role,\n                     sagemaker_session = sess,\n                     entry_point=&quot;inference.py&quot;,\n                     # image=image, \n                     framework_version=&quot;1.5.0&quot;,\n                     py_version='py3'\n                    )\n\nremote_predictor = remote_model.deploy(\n                         instance_type='ml.g4dn.xlarge', \n                         initial_instance_count=1,\n                         #update_endpoint = True, # comment or False if endpoint doesns't exist\n                         endpoint_name=endpoint_name, # define a unique endpoint name; if ommited, Sagemaker will generate it based on used container\n                         wait=True\n                         )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1607931328823,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|amazon-s3|boto3|amazon-sagemaker",
        "Question_view_count":1047,
        "Owner_creation_time":1577873077020,
        "Owner_last_access_time":1663647732437,
        "Owner_location":"Perth WA, Australia",
        "Owner_reputation":438,
        "Owner_up_votes":88,
        "Owner_down_votes":3,
        "Owner_views":67,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It was likely created as a default bucket by the SageMaker Python SDK. Note that the code you wrote about is not <code>boto3<\/code> (AWS python SDK), but <code>sagemaker<\/code> (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">link<\/a>), the SageMaker-specific Python SDK, that is higher-level than boto3.<\/p>\n<p>The SageMaker Python SDK uses S3 at multiple places, for example to stage training code when using a Framework Estimator, and to stage inference code when deployment with a Framework Model (your case). It gives you control of the S3 location to use, but if you don't specify it, it may use an automatically generated bucket, if it has the permissions to do so.<\/p>\n<p>To control code staging S3 location, you can use the parameter <code>code_location<\/code> in either your <code>PyTorchEstimator<\/code> (training) or your <code>PyTorchModel<\/code> (serving)<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1608132873677,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65285203",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61550297,
        "Question_title":"Await returns too soon",
        "Question_body":"<p>I am retrieving a list of image filenames from DynamoDB and using those image filenames to replace the default <code>src=<\/code> image in a portion of a website.<\/p>\n\n<p>I'm a JS novice, so I'm certainly missing something, but my function is returning the list of filenames too late.<\/p>\n\n<p>My inline script is:<\/p>\n\n<pre><code>&lt;script&gt;\n        customElements.whenDefined( 'crowd-bounding-box' ).then( () =&gt; {  \n        var imgBox = document.getElementById('annotator');\n        newImg = imageslist();\n        console.log(\"Result of newImg is: \" + newImg);\n        imgBox.src = \"https:\/\/my-images-bucket.s3.amazonaws.com\/\" + newImg;\n    } )\n&lt;\/script&gt;\n<\/code><\/pre>\n\n<p>My JS function is:<\/p>\n\n<pre><code>async function imageslist() {\n    const username = \"sampleuser\";\n    const params = {\n        TableName: \"mytable\",\n        FilterExpression: \"attribute_not_exists(\" + username + \")\",\n        ReturnConsumedCapacity: \"NONE\"\n    };\n    try {\n        var data = await ddb.scan(params).promise()\n        var imglist = [];\n        for(let i = 0; i &lt; data.Items.length; i++) {\n            imglist.push(data.Items[i].img.S);\n        };\n        imglist.sort();\n        var firstimg = imglist[0];\n        console.log(firstimg);\n        return imglist\n    } catch(error) {\n        console.error(error);\n    }\n}\n<\/code><\/pre>\n\n<p>The console reports <code>Result of newImg is: [object Promise]<\/code> and shortly after that, it reports the expected filename.  After the page has been rendered, I can input <code>newImg<\/code> in the console and I receive the expected result.<\/p>\n\n<p>Am I using the <strong>await<\/strong> syntax improperly?<\/p>\n\n<p>Side note: This site uses the Crowd HTML Elements (for Ground Truth and Mechanical Turk), so I'm forced to have the <code>src=<\/code> attribute present in my <code>crowd-bounding-box<\/code> tag and it must be a non-zero value.  I'm loading a default image and replacing it with another image.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1588364900607,
        "Question_score":0,
        "Question_tags":"javascript|promise|async-await|amazon-sagemaker|mechanicalturk",
        "Question_view_count":63,
        "Owner_creation_time":1483444144907,
        "Owner_last_access_time":1643982821673,
        "Owner_location":"Hoth",
        "Owner_reputation":312,
        "Owner_up_votes":28,
        "Owner_down_votes":1,
        "Owner_views":65,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Anytime you use the <code>await<\/code> keyword, you must use the <code>async<\/code> keyword before the function definition (which you have done). The thing is, any time an async function is called, it will always return a <code>Promise<\/code> object since it expects that some asynchronous task will take place within the function.<\/p>\n\n<p>Therefore,you'll need to <code>await<\/code> the result of the imageslist function and make the surrounding function <code>async<\/code>.<\/p>\n\n<pre class=\"lang-js prettyprint-override\"><code>&lt;script&gt;\n    customElements.whenDefined( 'crowd-bounding-box' ).then( async () =&gt; {  \n        var imgBox = document.getElementById('annotator');\n        newImg = await imageslist();\n        console.log(\"Result of newImg is: \" + newImg);\n        imgBox.src = \"https:\/\/my-images-bucket.s3.amazonaws.com\/\" + newImg;\n    } )\n&lt;\/script&gt;\n<\/code><\/pre>",
        "Answer_comment_count":1,
        "Answer_creation_time":1588365818457,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61550297",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56308169,
        "Question_title":"Creating a model for use in a pipeline from a hyperparameter tuning job",
        "Question_body":"<p>I'm trying to implement the best estimator from a hyperparameter tuning job into a pipeline object to deploy an endpoint.<\/p>\n\n<p>I've read the docs in a best effort to include the results from the tuning job in the pipeline, but I'm having trouble creating the Model() class object.<\/p>\n\n<pre><code># This is the hyperparameter tuning job\ntuner.fit({'train': s3_train, 'validation': s3_val}, \ninclude_cls_metadata=False)\n\n\n#With a standard Model (Not from the tuner) the process was as follows:\nscikit_learn_inferencee_model_name = sklearn_preprocessor.create_model()\nxgb_model_name = Model(model_data=xgb_model.model_data, image=xgb_image)\n\n\nmodel_name = 'xgb-inference-pipeline-' + timestamp_prefix\nendpoint_name = 'xgb-inference-pipeline-ep-' + timestamp_prefix\nsm_model = PipelineModel(\n    name=model_name, \n    role=role, \n    models=[\n        scikit_learn_inferencee_model_name, \n        xgb_model_name])\n\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', \nendpoint_name=endpoint_name)\n<\/code><\/pre>\n\n<p>I would like to be able to cleanly instantiate a model object using my results from the tuning job and pass it into the PipelineModel object. Any guidance is appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1558813444520,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|pipeline|amazon-sagemaker",
        "Question_view_count":455,
        "Owner_creation_time":1558812981693,
        "Owner_last_access_time":1629605922633,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I think you are on the right track. Do you get any error? Refer this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/e9c295c8538d29cc9fea2f73a29649126628064a\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb\" rel=\"nofollow noreferrer\">notebook<\/a>  for instantiating the model from the tuner and use in inference pipeline.<\/p>\n\n<p>Editing previous response based on the comment. To create model from the best training job of the hyperparameter tuning job, you can use below snippet<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tuner import HyperparameterTuner\nfrom sagemaker.estimator import Estimator\nfrom sagemaker.model import Model\n\n# Attach to an existing hyperparameter tuning job.\nxgb_tuning_job_name = 'my_xgb_hpo_tuning_job_name'\nxgb_tuner = HyperparameterTuner.attach(xgb_tuning_job_name)\n\n# Get the best XGBoost training job name from the HPO job\nxgb_best_training_job = xgb_tuner.best_training_job()\nprint(xgb_best_training_job)\n\n# Attach estimator to the best training job name\nxgb_best_estimator = Estimator.attach(xgb_best_training_job)\n\n# Create model to be passed to the inference pipeline\nxgb_model = Model(model_data=xgb_best_estimator.model_data,\n                  role=sagemaker.get_execution_role(),\n                  image=xgb_best_estimator.image_name)\n<\/code><\/pre>",
        "Answer_comment_count":3,
        "Answer_creation_time":1558879357103,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1559176084156,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56308169",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50281188,
        "Question_title":"Sagemaker Java client generate IOrecord",
        "Question_body":"<p>I am trying to build a training set for Sagemaker using the Linear Learner algorithm. This algorithm supports recordIO wrapped protobuf and csv as format for the training data. As the training data is generated using spark I am having issues to generate a csv file from a dataframe (this seem broken for now), so I am trying to use protobuf. <\/p>\n\n<p>I managed to create a binary file for the training dataset using Protostuff which is a library that allows to generate protobuf messages from POJO objects. The problem is when triggering the training job I receive that message from SageMaker:\nClientError: No training data processed. Either the training channel is empty or the mini-batch size is too high. Verify that training data contains non-empty files and the mini-batch size is less than the number of records per training host.<\/p>\n\n<p>The training file is certainly not null. I suspect the way I generate the training data to be incorrect as I am able to train models using the libsvm format. Is there a way to generate IOrecord using the Sagemaker java client ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1525984750483,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":222,
        "Owner_creation_time":1428454496053,
        "Owner_last_access_time":1570063575877,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Answering my own question. It was an issue in the algorithm configuration. I reduced mini batch size and it worked fine.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1526653073370,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50281188",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69024005,
        "Question_title":"How to use SageMaker Estimator for model training and saving",
        "Question_body":"<p>The documentations of how to use SageMaker estimators are scattered around, sometimes obsolete, incorrect. Is there a one stop location which gives the comprehensive views of how to use SageMaker SDK Estimator to train and save models?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630555307170,
        "Question_score":27,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":6655,
        "Owner_creation_time":1416648155470,
        "Owner_last_access_time":1664057583237,
        "Owner_location":null,
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Question_last_edit_time":null,
        "Answer_body":"<h1>Answer<\/h1>\n<p>There is no one such resource from AWS that provides the comprehensive view of how to use SageMaker SDK Estimator to train and save models.<\/p>\n<h2>Alternative Overview Diagram<\/h2>\n<p>I put a diagram and brief explanation to get the overview on how SageMaker Estimator runs a training.<\/p>\n<ol>\n<li><p>SageMaker sets up a docker container for a training job where:<\/p>\n<ul>\n<li>Environment variables are set as in <a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container. Environment Variables<\/a>.<\/li>\n<li>Training data is setup under <code>\/opt\/ml\/input\/data<\/code>.<\/li>\n<li>Training script codes are setup under <code>\/opt\/ml\/code<\/code>.<\/li>\n<li><code>\/opt\/ml\/model<\/code> and <code>\/opt\/ml\/output<\/code> directories are setup to store training outputs.<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json  &lt;--- From Estimator hyperparameter arg\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;        &lt;--- From Estimator fit method inputs arg\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 code\n\u2502   \u2514\u2500\u2500 &lt;code files&gt;              &lt;--- From Estimator src_dir arg\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;             &lt;--- Location to save the trained model artifacts\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure                   &lt;--- Training job failure logs\n<\/code><\/pre>\n<ol start=\"2\">\n<li><p>SageMaker Estimator <code>fit(inputs)<\/code> method executes the training script. Estimator <code>hyperparameters<\/code> and <code>fit<\/code> method <code>inputs<\/code> are provided as its command line arguments.<\/p>\n<\/li>\n<li><p>The training script saves the model artifacts in the <code>\/opt\/ml\/model<\/code> once the training is completed.<\/p>\n<\/li>\n<li><p>SageMaker archives the artifacts under <code>\/opt\/ml\/model<\/code> into <code>model.tar.gz<\/code> and save it to the S3 location specified to <code>output_path<\/code> Estimator parameter.<\/p>\n<\/li>\n<li><p>You can set Estimator <code>metric_definitions<\/code> parameter to extract model metrics from the training logs. Then you can monitor the training progress in the SageMaker console metrics.<\/p>\n<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I believe AWS needs to stop mass-producing verbose, redundant, wordy, scattered, and obsolete documents. AWS needs to understand <strong>A picture is worth thousand words<\/strong>.<\/p>\n<p>Have diagrams and piece document parts together in a <strong>context<\/strong> with a clear objective to achieve.<\/p>\n<hr \/>\n<h1>Problem<\/h1>\n<p>AWS documentations need serious re-design and re-structuring. Just to understand <strong>how to train and save a model<\/strong> forces us going through dozens of scattered,  fragmented, verbose, redundant documentations, which are often obsolete, incomplete, and sometime incorrect.<\/p>\n<p>It is well-summarized in <a href=\"https:\/\/nandovillalba.medium.com\/why-i-think-gcp-is-better-than-aws-ea78f9975bda\" rel=\"noreferrer\">Why I think GCP is better than AWS<\/a>:<\/p>\n<blockquote>\n<p>It\u2019s not that AWS is harder to use than GCP, it\u2019s that <strong>it is needlessly hard<\/strong>; a disjointed, sprawl of infrastructure primitives with poor cohesion between them.  <br><br>\nA challenge is nice, a confusing mess is not, and <strong>the problem with AWS is that a large part of your working hours will be spent untangling their documentation and weeding through features and products to find what you want<\/strong>, rather than focusing on cool interesting challenges.<\/p>\n<\/blockquote>\n<p>Especially the SageMaker team keeps changing implementations without updating documents. Its roll-out was also inconsistent, e.g. SDK version 2 was rolled out in the SageMaker Studio making the AWS examples in Github incompatible without announcing it. Whereas SageMaker instance still had SDK 1, hence code worked in Instance but not in Studio.<\/p>\n<p>It is mind-boggling that we have to go through these many documents below to understand how to use the SageMaker SDK Estimator for training.<\/p>\n<h2>Documents for Model Training<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\" rel=\"noreferrer\">Train a Model with Amazon SageMaker<\/a><\/li>\n<\/ul>\n<p>This document gives 20,000 feet overview of how SageMaker training but does not give any clue what to do.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<p>This document gives an overview of how SageMaker training looks like. However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<blockquote>\n<p>WARNING: This package has been deprecated. Please use the SageMaker Training Toolkit for model training and the SageMaker Inference Toolkit for model serving.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model.html\" rel=\"noreferrer\">Step 4: Train a Model<\/a><\/li>\n<\/ul>\n<p>This document layouts the steps for training.<\/p>\n<blockquote>\n<p>The Amazon SageMaker Python SDK provides framework estimators and generic estimators to train your model while orchestrating the machine learning (ML) lifecycle accessing the SageMaker features for training and the AWS infrastructures<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#train-a-model-with-the-sagemaker-python-sdk\" rel=\"noreferrer\">Train a Model with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To train a model by using the SageMaker Python SDK, you:<\/p>\n<ul>\n<li>Prepare a training script<\/li>\n<li>Create an estimator<\/li>\n<li>Call the fit method of the estimator<\/li>\n<\/ul>\n<\/blockquote>\n<p>Finally this document gives concrete steps and ideas. However still missing comprehensiv details about Environment Variables, Directory structure in the SageMaker docker container**, S3 for uploading code, placing data, S3 where the trained model is saved, etc.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html\" rel=\"noreferrer\">Use TensorFlow with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<p>This documents is focused on TensorFlow Estimator implementation steps. Use <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/get_started_mnist_train.ipynb\" rel=\"noreferrer\">Training a Tensorflow Model on MNIST<\/a> Github example to accompany with to follow the actual implementation.<\/p>\n<h2>Documents for passing parameters and data locations<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig\" rel=\"noreferrer\">How Amazon SageMaker Provides Training Information<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>This section explains how SageMaker makes training information, such as training data, hyperparameters, and other configuration information, available to your Docker container.<\/p>\n<\/blockquote>\n<p>This document finally gives the idea of how parameters and data are passed around but again, not comprehensive.<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container Environment Variables<\/a><\/li>\n<\/ul>\n<p>This documentation is marked as <strong>deprecated<\/strong> but the only document which explains the SageMaker Environment Variables.<\/p>\n<blockquote>\n<h3>IMPORTANT ENVIRONMENT VARIABLES<\/h3>\n<ul>\n<li>SM_MODEL_DIR<\/li>\n<li>SM_CHANNELS<\/li>\n<li>SM_CHANNEL_{channel_name}<\/li>\n<li>SM_HPS<\/li>\n<li>SM_HP_{hyperparameter_name}<\/li>\n<li>SM_CURRENT_HOST<\/li>\n<li>SM_HOSTS<\/li>\n<li>SM_NUM_GPUS<\/li>\n<\/ul>\n<h3>List of provided environment variables by SageMaker Containers<\/h3>\n<ul>\n<li>SM_NUM_CPUS<\/li>\n<li>SM_LOG_LEVEL<\/li>\n<li>SM_NETWORK_INTERFACE_NAME<\/li>\n<li>SM_USER_ARGS<\/li>\n<li>SM_INPUT_DIR<\/li>\n<li>SM_INPUT_CONFIG_DIR<\/li>\n<li>SM_OUTPUT_DATA_DIR<\/li>\n<li>SM_RESOURCE_CONFIG<\/li>\n<li>SM_INPUT_DATA_CONFIG<\/li>\n<li>SM_TRAINING_ENV<\/li>\n<\/ul>\n<\/blockquote>\n<h2>Documents for SageMaker Docker Container Directory Structure<\/h2>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure\n<\/code><\/pre>\n<p>This document explains the directory structure and purpose of each directory.<\/p>\n<blockquote>\n<h3>The input<\/h3>\n<ul>\n<li>\/opt\/ml\/input\/config contains information to control how your program runs. hyperparameters.json is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. resourceConfig.json is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn\u2019t support distributed training, we\u2019ll ignore it here.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;\/ (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it\u2019s generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;_&lt;epoch_number&gt; (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch.<\/li>\n<\/ul>\n<h3>The output<\/h3>\n<ul>\n<li>\/opt\/ml\/model\/ is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the DescribeTrainingJob result.<\/li>\n<li>\/opt\/ml\/output is a directory where the algorithm can write a file failure that describes why the job failed. The contents of this file will be returned in the FailureReason field of the DescribeTrainingJob result. For jobs that succeed, there is no reason to write this file as it will be ignored.<\/li>\n<\/ul>\n<\/blockquote>\n<p>However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<h2>Documents for Model Saving<\/h2>\n<p>The information on where the trained model is saved and in what format are fundamentally missing. The training script needs to save the model under <code>\/opt\/ml\/model<\/code> and the format and sub-directory structure depend on the frameworks e,g TensorFlow, Pytorch. This is because SageMaker deployment uses the Framework dependent model-serving, e,g. TensorFlow Serving for TensorFlow framework.<\/p>\n<p>This is not clearly documented and causing confusions. The developer needs to specify which format to use and under which sub-directory to save.<\/p>\n<p>To use TensorFlow Estimator training and deployment:<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/aws_sagemaker_studio\/frameworks\/keras_pipe_mode_horovod\/keras_pipe_mode_horovod_cifar10.html#Deploy-the-trained-model\" rel=\"noreferrer\">Deploy the trained model<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>Because <strong>we\u2019re using TensorFlow Serving for deployment<\/strong>, our training script <strong>saves the model in TensorFlow\u2019s SavedModel format<\/strong>.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/code\/train.py#L159-L166\" rel=\"noreferrer\">amazon-sagemaker-examples\/frameworks\/tensorflow\/code\/train.py <\/a><\/li>\n<\/ul>\n<pre><code>    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    ckpt_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n    model.save(ckpt_dir)\n<\/code><\/pre>\n<p>The code is saving the model in <code>\/opt\/ml\/model\/00000000<\/code> because this is for TensorFlow serving.<\/p>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/guide\/saved_model\" rel=\"noreferrer\">Using the SavedModel format<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>The save-path follows a convention used by TensorFlow Serving where the last path component (1\/ here) is a version number for your model - it allows tools like Tensorflow Serving to reason about the relative freshness.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/tfx\/tutorials\/serving\/rest_simple#save_your_model\" rel=\"noreferrer\">Train and serve a TensorFlow model with TensorFlow Serving<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To load our trained model into TensorFlow Serving we first need to save it in SavedModel format. This will create a protobuf file in a well-defined directory hierarchy, and will include a version number. TensorFlow Serving allows us to select which version of a model, or &quot;servable&quot; we want to use when we make inference requests. Each version will be exported to a different sub-directory under the given path.<\/p>\n<\/blockquote>\n<h2>Documents for API<\/h2>\n<p>Basically the SageMaker SDK Estimator implements the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html\" rel=\"noreferrer\">CreateTrainingJob<\/a> API for training part. Hence, better to understand how it is designed and what parameters need to be defined. Otherwise working on Estimators are like walking in the dark.<\/p>\n<hr \/>\n<h1>Example<\/h1>\n<h2>Jupyter Notebook<\/h2>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\nbucket = sagemaker_session.default_bucket()\n\nmetric_definitions = [\n    {&quot;Name&quot;: &quot;train:loss&quot;, &quot;Regex&quot;: &quot;.*loss: ([0-9\\\\.]+) - accuracy: [0-9\\\\.]+.*&quot;},\n    {&quot;Name&quot;: &quot;train:accuracy&quot;, &quot;Regex&quot;: &quot;.*loss: [0-9\\\\.]+ - accuracy: ([0-9\\\\.]+).*&quot;},\n    {\n        &quot;Name&quot;: &quot;validation:accuracy&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: ([0-9\\\\.]+).*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;validation:loss&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: ([0-9\\\\.]+) - val_accuracy: [0-9\\\\.]+.*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;sec\/sample&quot;,\n        &quot;Regex&quot;: &quot;.* - \\d+s (\\d+)[mu]s\/sample - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: [0-9\\\\.]+&quot;,\n    },\n]\n\nimport uuid\n\ncheckpoint_s3_prefix = &quot;checkpoints\/{}&quot;.format(str(uuid.uuid4()))\ncheckpoint_s3_uri = &quot;s3:\/\/{}\/{}\/&quot;.format(bucket, checkpoint_s3_prefix)\n\nfrom sagemaker.tensorflow import TensorFlow\n\n# --------------------------------------------------------------------------------\n# 'trainingJobName' msut satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}\n# --------------------------------------------------------------------------------\nbase_job_name = &quot;fashion-mnist&quot;\nhyperparameters = {\n    &quot;epochs&quot;: 2, \n    &quot;batch-size&quot;: 64\n}\nestimator = TensorFlow(\n    entry_point=&quot;fashion_mnist.py&quot;,\n    source_dir=&quot;src&quot;,\n    metric_definitions=metric_definitions,\n    hyperparameters=hyperparameters,\n    role=role,\n    input_mode='File',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    base_job_name=base_job_name,\n    checkpoint_s3_uri=checkpoint_s3_uri,\n    model_dir=False\n)\nestimator.fit()\n<\/code><\/pre>\n<h2>fashion_mnist.py<\/h2>\n<pre><code>import os\nimport argparse\nimport json\nimport multiprocessing\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras import backend as K\n\nprint(&quot;TensorFlow version: {}&quot;.format(tf.__version__))\nprint(&quot;Eager execution is: {}&quot;.format(tf.executing_eagerly()))\nprint(&quot;Keras version: {}&quot;.format(tf.keras.__version__))\n\n\nimage_width = 28\nimage_height = 28\n\n\ndef load_data():\n    fashion_mnist = tf.keras.datasets.fashion_mnist\n    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n\n    number_of_classes = len(set(y_train))\n    print(&quot;number_of_classes&quot;, number_of_classes)\n\n    x_train = x_train \/ 255.0\n    x_test = x_test \/ 255.0\n    x_full = np.concatenate((x_train, x_test), axis=0)\n    print(x_full.shape)\n\n    print(type(x_train))\n    print(x_train.shape)\n    print(x_train.dtype)\n    print(y_train.shape)\n    print(y_train.dtype)\n\n    # ## Train\n    # * C: Convolution layer\n    # * P: Pooling layer\n    # * B: Batch normalization layer\n    # * F: Fully connected layer\n    # * O: Output fully connected softmax layer\n\n    # Reshape data based on channels first \/ channels last strategy.\n    # This is dependent on whether you use TF, Theano or CNTK as backend.\n    # Source: https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/mnist_cnn.py\n    if K.image_data_format() == 'channels_first':\n        x = x_train.reshape(x_train.shape[0], 1, image_width, image_height)\n        x_test = x_test.reshape(x_test.shape[0], 1, image_width, image_height)\n        input_shape = (1, image_width, image_height)\n    else:\n        x_train = x_train.reshape(x_train.shape[0], image_width, image_height, 1)\n        x_test = x_test.reshape(x_test.shape[0], image_width, image_height, 1)\n        input_shape = (image_width, image_height, 1)\n\n    return x_train, y_train, x_test, y_test, input_shape, number_of_classes\n\n# tensorboard --logdir=\/full_path_to_your_logs\n\nvalidation_split = 0.2\nverbosity = 1\nuse_multiprocessing = True\nworkers = multiprocessing.cpu_count()\n\n\ndef train(model, x, y, args):\n    # SavedModel Output\n    tensorflow_saved_model_path = os.path.join(args.model_dir, &quot;tensorflow\/saved_model\/0&quot;)\n    os.makedirs(tensorflow_saved_model_path, exist_ok=True)\n\n    # Tensorboard Logs\n    tensorboard_logs_path = os.path.join(args.model_dir, &quot;tensorboard\/&quot;)\n    os.makedirs(tensorboard_logs_path, exist_ok=True)\n\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=tensorboard_logs_path,\n        write_graph=True,\n        write_images=True,\n        histogram_freq=1,  # How often to log histogram visualizations\n        embeddings_freq=1,  # How often to log embedding visualizations\n        update_freq=&quot;epoch&quot;,\n    )  # How often to write logs (default: once per epoch)\n\n    model.compile(\n        optimizer='adam',\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\n        metrics=['accuracy']\n    )\n    history = model.fit(\n        x,\n        y,\n        shuffle=True,\n        batch_size=args.batch_size,\n        epochs=args.epochs,\n        validation_split=validation_split,\n        use_multiprocessing=use_multiprocessing,\n        workers=workers,\n        verbose=verbosity,\n        callbacks=[\n            tensorboard_callback\n        ]\n    )\n    return history\n\n\ndef create_model(input_shape, number_of_classes):\n    model = Sequential([\n        Conv2D(\n            name=&quot;conv01&quot;,\n            filters=32,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=&quot;same&quot;,\n            activation='relu',\n            input_shape=input_shape\n        ),\n        MaxPooling2D(\n            name=&quot;pool01&quot;,\n            pool_size=(2, 2)\n        ),\n        Flatten(),  # 3D shape to 1D.\n        BatchNormalization(\n            name=&quot;batch_before_full01&quot;\n        ),\n        Dense(\n            name=&quot;full01&quot;,\n            units=300,\n            activation=&quot;relu&quot;\n        ),  # Fully connected layer\n        Dense(\n            name=&quot;output_softmax&quot;,\n            units=number_of_classes,\n            activation=&quot;softmax&quot;\n        )\n    ])\n    return model\n\n\ndef save_model(model, args):\n    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    model_save_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(model_save_dir):\n        os.makedirs(model_save_dir)\n    print(f&quot;saving model at {model_save_dir}&quot;)\n    model.save(model_save_dir)\n\n\ndef parse_args():\n    # --------------------------------------------------------------------------------\n    # https:\/\/docs.python.org\/dev\/library\/argparse.html#dest\n    # --------------------------------------------------------------------------------\n    parser = argparse.ArgumentParser()\n\n    # --------------------------------------------------------------------------------\n    # hyperparameters Estimator argument are passed as command-line arguments to the script.\n    # --------------------------------------------------------------------------------\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch-size', type=int, default=64)\n\n    # \/opt\/ml\/model\n    # sagemaker.tensorflow.estimator.TensorFlow override 'model_dir'.\n    # See https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/\\\n    # sagemaker.tensorflow.html#sagemaker.tensorflow.estimator.TensorFlow\n    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n\n    # \/opt\/ml\/output\n    parser.add_argument(&quot;--output_dir&quot;, type=str, default=os.environ[&quot;SM_OUTPUT_DIR&quot;])\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == &quot;__main__&quot;:\n    args = parse_args()\n    print(&quot;---------- key\/value args&quot;)\n    for key, value in vars(args).items():\n        print(f&quot;{key}:{value}&quot;)\n\n    x_train, y_train, x_test, y_test, input_shape, number_of_classes = load_data()\n    model = create_model(input_shape, number_of_classes)\n\n    history = train(model=model, x=x_train, y=y_train, args=args)\n    print(history)\n    \n    save_model(model, args)\n    results = model.evaluate(x_test, y_test, batch_size=100)\n    print(&quot;test loss, test accuracy:&quot;, results)\n<\/code><\/pre>\n<h2>SageMaker Console<\/h2>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Notebook output<\/h2>\n<pre><code>2021-09-03 03:02:04 Starting - Starting the training job...\n2021-09-03 03:02:16 Starting - Launching requested ML instancesProfilerReport-1630638122: InProgress\n......\n2021-09-03 03:03:17 Starting - Preparing the instances for training.........\n2021-09-03 03:04:59 Downloading - Downloading input data\n2021-09-03 03:04:59 Training - Downloading the training image...\n2021-09-03 03:05:23 Training - Training image download completed. Training in progress.2021-09-03 03:05:23.966037: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:23.969704: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n2021-09-03 03:05:24.118054: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:26,842 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n2021-09-03 03:05:26,852 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:27,734 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n\/usr\/local\/bin\/python3.7 -m pip install -r requirements.txt\nWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\nYou should consider upgrading via the '\/usr\/local\/bin\/python3.7 -m pip install --upgrade pip' command.\n\n2021-09-03 03:05:29,028 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,045 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,062 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,072 sagemaker-training-toolkit INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {},\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;batch-size&quot;: 64,\n        &quot;epochs&quot;: 2\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {},\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;fashion_mnist&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 4,\n    &quot;num_gpus&quot;: 0,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;fashion_mnist.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;batch-size&quot;:64,&quot;epochs&quot;:2}\nSM_USER_ENTRY_POINT=fashion_mnist.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=fashion_mnist\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=4\nSM_NUM_GPUS=0\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;batch-size&quot;:64,&quot;epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;fashion_mnist&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:4,&quot;num_gpus&quot;:0,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;fashion_mnist.py&quot;}\nSM_USER_ARGS=[&quot;--batch-size&quot;,&quot;64&quot;,&quot;--epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_HP_BATCH-SIZE=64\nSM_HP_EPOCHS=2\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/local\/lib\/python37.zip:\/usr\/local\/lib\/python3.7:\/usr\/local\/lib\/python3.7\/lib-dynload:\/usr\/local\/lib\/python3.7\/site-packages\n\nInvoking script with the following command:\n\n\/usr\/local\/bin\/python3.7 fashion_mnist.py --batch-size 64 --epochs 2\n\n\nTensorFlow version: 2.3.1\nEager execution is: True\nKeras version: 2.4.0\n---------- key\/value args\nepochs:2\nbatch_size:64\nmodel_dir:\/opt\/ml\/model\noutput_dir:\/opt\/ml\/output\n<\/code><\/pre>",
        "Answer_comment_count":4,
        "Answer_creation_time":1630555307170,
        "Answer_score":65,
        "Question_favorite_count":14.0,
        "Answer_last_edit_time":1656913410063,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69024005",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73829280,
        "Question_title":"NVIDIA Triton vs TorchServe for SageMaker Inference",
        "Question_body":"<p><a href=\"https:\/\/developer.nvidia.com\/nvidia-triton-inference-server\" rel=\"nofollow noreferrer\">NVIDIA Triton<\/a>\u00a0vs\u00a0<a href=\"https:\/\/pytorch.org\/serve\/\" rel=\"nofollow noreferrer\">TorchServe<\/a>\u00a0for SageMaker inference? When to recommend each?<\/p>\n<p>Both are modern, production grade inference servers. TorchServe is the DLC default inference server for PyTorch models. Triton is also supported for PyTorch inference on SageMaker.<\/p>\n<p>Anyone has a good comparison matrix for both?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663943338403,
        "Question_score":0,
        "Question_tags":"pytorch|amazon-sagemaker|inference|torchserve|tritonserver",
        "Question_view_count":12,
        "Owner_creation_time":1389887039673,
        "Owner_last_access_time":1664076128463,
        "Owner_location":"Singapore",
        "Owner_reputation":5854,
        "Owner_up_votes":155,
        "Owner_down_votes":70,
        "Owner_views":794,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Important notes to add here where both serving stacks differ:<\/p>\n<p>TorchServe does not provide the Instance Groups feature that Triton does (that is, stacking many copies of the same model or even different models onto the same GPU). This is a major advantage for both realtime and batch use-cases, as the performance increase is almost proportional to the model replication count (i.e. 2 copies of the model get you almost twice the throughput and half the latency; check out a BERT benchmark of this here). Hard to match a feature that is almost like having 2+ GPU's for the price of one.\nif you are deploying PyTorch DL models, odds are you often want to accelerate them with GPU's. TensorRT (TRT) is a compiler developed by NVIDIA that automatically quantizes and optimizes your model graph, which represents another huge speed up, depending on GPU architecture and model. It is understandably so probably the best way of automatically optimizing your model to run efficiently on GPU's and make good use of TensorCores. Triton has native integration to run TensorRT engines as they're called (even automatically converting your model to a TRT engine via config file), while TorchServe does not (even though you can use TRT engines with it).\nThere is more parity between both when it comes to other important serving features: both have dynamic batching support, you can define inference DAG's with both (not sure if the latter works with TorchServe on SageMaker without a big hassle), and both support custom code\/handlers instead of just being able to serve a model's forward function.<\/p>\n<p>Finally, MME on GPU (coming shortly) will be based on Triton, which is a valid argument for customers to get familiar with it so that they can quickly leverage this new feature for cost-optimization.<\/p>\n<p>Bottom line I think that Triton is just as easy (if not easier) ot use, a lot more optimized\/integrated for taking full advantage of the underlying hardware (and will be updated to keep being that way as newer GPU architectures are released, enabling an easy move to them), and in general blows TorchServe out of the water performance-wise when its optimization features are used in combination.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1663971240670,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73829280",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73645084,
        "Question_title":"Create Hugging Face Transformers Tokenizer using Amazon SageMaker in a distributed way",
        "Question_body":"<p>I am using the SageMaker HuggingFace Processor to create a custom tokenizer on a large volume of text data.\nIs there a way to make this job data distributed - meaning read partitions of data across nodes and train the tokenizer leveraging multiple CPUs\/GPUs.<\/p>\n<p>At the moment, providing more nodes to the processing cluster merely replicates the tokenization process (basically duplicates the process of creation), which is redundant. You can primarily only scale vertically.<\/p>\n<p>Any insights into this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662621424647,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|huggingface-transformers|huggingface-tokenizers",
        "Question_view_count":27,
        "Owner_creation_time":1662621266503,
        "Owner_last_access_time":1663966999637,
        "Owner_location":null,
        "Owner_reputation":48,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1662627758727,
        "Answer_body":"<p>Considering the following example code for\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/processing-job-frameworks-hugging-face.html\" rel=\"nofollow noreferrer\">HuggingFaceProcessor<\/a>:<\/p>\n<p>If you have 100 large files in S3 and use a ProcessingInput with\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ProcessingS3Input.html#:%7E:text=S3DataDistributionType\" rel=\"nofollow noreferrer\">s3_data_distribution_type<\/a>=&quot;ShardedByS3Key&quot; (instead of FullyReplicated), the objects in your S3 prefix will be sharded and distributed to your instances.<\/p>\n<p>For example, if you have 100 large files and want to filter records from them using HuggingFace on 5 instances, the s3_data_distribution_type=&quot;ShardedByS3Key&quot; will put 20 objects on each instance, and each instance can read the files from its own path, filter out records, and write (uniquely named) files to the output paths, and SageMaker Processing will put the filtered files in S3.<\/p>\n<p>However, if your filtering criteria is stateful or depends on doing a full pass over the dataset first (such as: filtering outliers based on mean and standard deviation on a feature - in case of using SKLean Processor for example): you'll need to pass that information in to the job so each instance can know how to filter. To send information to the instances launched, you have to use the\u00a0<code>\/opt\/ml\/config\/resourceconfig.json<\/code>\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-your-own-processing-container.html#byoc-config\" rel=\"nofollow noreferrer\">file<\/a>:<\/p>\n<p><code>{ &quot;current_host&quot;: &quot;algo-1&quot;, &quot;hosts&quot;: [&quot;algo-1&quot;,&quot;algo-2&quot;,&quot;algo-3&quot;] }<\/code><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1662653309533,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73645084",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59446807,
        "Question_title":"sagemaker horizontally scaling tensorflow (keras) model",
        "Question_body":"<p>I am roughly following this script <a href=\"https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/blob\/master\/keras\/05-keras-blog-post\/Fashion%20MNIST-SageMaker.ipynb\" rel=\"nofollow noreferrer\">fashion-MNIST-sagemaker<\/a>.<\/p>\n\n<p>I see that in the notebook <\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='mnist_keras_tf.py', \n                          role=role,\n                          train_instance_count=1, \n                          train_instance_type='local',\n                          framework_version='1.12', \n                          py_version='py3',\n                          script_mode=True,\n                          hyperparameters={'epochs': 1}\n                         )\n<\/code><\/pre>\n\n<p>I am wondering to what extent I can and should use the <code>train_instance_count<\/code> parameter. Will it distribute training along some dimension automatically, if yes - what is the dimension?<\/p>\n\n<p>Further, does it generally make sense to distribute training horizontally in a keras (with tensorflow) based setting?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1577037857400,
        "Question_score":0,
        "Question_tags":"python|tensorflow|keras|amazon-sagemaker|horizontal-scaling",
        "Question_view_count":295,
        "Owner_creation_time":1456487654210,
        "Owner_last_access_time":1664019268877,
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":1464,
        "Owner_up_votes":81,
        "Owner_down_votes":9,
        "Owner_views":62,
        "Question_last_edit_time":null,
        "Answer_body":"<p>distributed training is model and framework specific. Not all models are easy to distribute, and from ML framework to ML framework things are not equally easy. <strong>It is rarely automatic, even less so with TensorFlow and Keras.<\/strong><\/p>\n\n<p>Neural nets are conceptually easy to distribute under the data-parallel paradigm, whereby the gradient computation of a given mini-batch is split among workers, which could be multiple devices in the same host (multi-device) or multiple hosts with each multiple devices (multi-device multi-host). The D2L.ai course provides an in-depth view of how neural nets are distributed <a href=\"http:\/\/d2l.ai\/chapter_computational-performance\/multiple-gpus.html\" rel=\"nofollow noreferrer\">here<\/a> and <a href=\"http:\/\/d2l.ai\/chapter_computational-performance\/parameterserver.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Keras used to be trivial to distribute in multi-device, single host fashion with the <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/utils\/multi_gpu_model\" rel=\"nofollow noreferrer\"><code>multi_gpu_model<\/code>, which will sadly get deprecated in 4 months<\/a>. In your case, you seem to refer to multi-host model (more than one machine), and that requires writing ad-hoc synchronization code such as the one seen in <a href=\"https:\/\/www.tensorflow.org\/tutorials\/distribute\/multi_worker_with_keras\" rel=\"nofollow noreferrer\">this official tutorial<\/a>. <\/p>\n\n<p>Now let's look at how does this relate to SageMaker.<\/p>\n\n<p>SageMaker comes with 3 options for algorithm development. Using distributed training may require a varying amount of custom work depending on the option you choose:<\/p>\n\n<ol>\n<li><p>The <strong>built-in algorithms<\/strong> is a library of 18 pre-written algorithms. Many of them are written to be distributed in single-host multi-GPU or multi-GPU multi-host. With that first option, you don't have anything to do apart from setting <code>train_instance_count<\/code> > 1 to distribute over multiple instances<\/p><\/li>\n<li><p>The <strong>Framework containers<\/strong> (the option you are using) are containers developed for popular frameworks (TensorFlow, PyTorch, Sklearn, MXNet) and provide pre-written docker environment in which you can write arbitrary code. In this options, some container will support one-click creation of ephemeral training clusters to do distributed training, <strong>however using <code>train_instance_count<\/code> greater than one is not enough to distribute the training of your model. It will just run your script on multiple machines. In order to distribute your training, you must write appropriate distribution and synchronization code in your <code>mnist_keras_tf.py<\/code> script.<\/strong> For some frameworks such code modification will be very simple, for example for TensorFlow and Keras, SageMaker comes with Horovod pre-installed. Horovod is a peer-to-peer ring-style communication mechanism that requires very little code modification and is highly scalable (<a href=\"https:\/\/eng.uber.com\/horovod\/\" rel=\"nofollow noreferrer\">initial annoucement from Uber<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html#training-with-horovod\" rel=\"nofollow noreferrer\">SageMaker doc<\/a>, <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_horovod\/tensorflow_script_mode_horovod.ipynb\" rel=\"nofollow noreferrer\">SageMaker example<\/a>, <a href=\"https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/launching-tensorflow-distributed-training-easily-with-horovod-or-parameter-servers-in-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">SageMaker blog post<\/a>). My recommendation would be to try using Horovod to distribute your code. Similarly, in Apache MXNet you can easily create Parameter Stores to host model parameters in a distributed fashion and sync with them from multiple nodes. MXNet scalability and ease of distribution is one of the reason Amazon loves it.<\/p><\/li>\n<li><p>The <strong>Bring-Your-Own Container<\/strong> requires you to write both docker container and algorithm code. In this situation, you can of course distribute your training over multiple machines but you also have to write machine-to-machine communication code<\/p><\/li>\n<\/ol>\n\n<p>For your specific situation my recommendation would be to scale horizontally first in a single node with multiple GPUs over bigger and bigger machine types, because latency and complexity increase drastically as you switch from single-host to multi-host context. If truly necessary, use multi-node context and things maybe easier if that's done with Horovod.\nIn any case, things are still much easier to do with SageMaker since it manages creation of ephemeral, billed-per-second clusters with built-in, logging and metadata and artifacts persistence and also handles fast training data loading from s3, sharded over training nodes. <\/p>\n\n<p><strong>Note on the relevancy of distributed training<\/strong>: Keep in mind that when you distribute over N devices a model that was running fine on one device, you usually grow the batch size by N so that the per-device batch size stays constant and each device keeps being busy. This will disturb your model convergence, because bigger batches means a less noisy SGD. A common heuristic is to grow the learning rate by N (more info in <a href=\"https:\/\/arxiv.org\/abs\/1706.02677\" rel=\"nofollow noreferrer\">this great paper from Priya Goyal et al<\/a>), but this on the other hand induces instability at the first couple epochs, so it is sometimes associated with a learning rate warmup. Scaling SGD to work well with very large batches is still an active research problem, with new ideas coming up frequently. Reaching good model performance with very large batches sometimes require ad-hoc research and a fair amount of parameter tuning, occasionally to the extent where the extra money spent on finding how to distribute well overcome the benefits of the faster training you eventually manage to run. A situation where distributed training makes sense is when an individual record represent too much compute to form a big enough physical batch on a device, a situation seen on big input sizes (eg vision over HD pictures) or big parameter counts (eg BERT). That being said, for those models requiring very big logical batch you don't necessarily have to distribute things physically: you can run sequentially N batches through your single GPU and wait N per-device batches before doing the gradient averaging and parameter update to simulate having an N times bigger GPU. (a clever hack sometimes called <em>gradient accumulation<\/em>)<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1577100115740,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1577101498307,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59446807",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64925353,
        "Question_title":"How to run and deploy AWS's XGBoost MNIST sample notebook on SageMaker?",
        "Question_body":"<p>I am trying to use <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/simplify-machine-learning-inference-on-kubernetes-with-amazon-sagemaker-operators\/\" rel=\"nofollow noreferrer\">Kubernetes SageMaker Operations<\/a> with the XGBoost MNIST AWS's <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_mnist\/xgboost_mnist.ipynb\" rel=\"nofollow noreferrer\">example<\/a>.<\/p>\n<p>Before I am enabling Kubernetes SageMaker Ops, I have deployed the XGBoost MNIST example via SageMaker WebUI itself and tried to access the endpoint via awscli:<\/p>\n<pre><code>$ aws sagemaker-runtime invoke-endpoint \\\n    --region eu-west-1 \\\n    --endpoint-name DEMO-XGBoostEndpoint-2020-11-20-06-26-30 \\\n    --body $(seq 784 | xargs echo | sed 's\/ \/,\/g') \\\n    &gt;(cat) \\\n    --content-type text\/csv &gt; \/dev\/null\n<\/code><\/pre>\n<p>However, I am encountering the following decoding error:<\/p>\n<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message &quot;Loading csv data failed with Exception, please ensure data is in csv format:\n &lt;class 'UnicodeDecodeError'&gt;\n 'utf-8' codec can't decode byte 0xd7 in position 0: invalid continuation byte&quot;. See https:\/\/xxxx.console.aws.amazon.com\/cloudwatch\/home?region=xxxxx#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/DEMO-XGBoostEndpoint-2020-11-20-06-26-30 in account XXX for more information.\n<\/code><\/pre>\n<p>And in the log I can see:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/algorithm_mode\/serve_utils.py&quot;, line 102, in parse_content_data\n    decoded_payload = payload.strip().decode(&quot;utf-8&quot;)\nTraceback (most recent call last): File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/algorithm_mode\/serve_utils.py&quot;, line 102, in parse_content_data decoded_payload = payload.strip().decode(&quot;utf-8&quot;)\n<\/code><\/pre>\n<p>When I go to the source code of <a href=\"https:\/\/github.com\/aws\/sagemaker-xgboost-container\/blob\/master\/src\/sagemaker_xgboost_container\/algorithm_mode\/serve_utils.py#L102\" rel=\"nofollow noreferrer\">sagemaker_xgboost_container<\/a> I can see that they expect UTF-8 format:<\/p>\n<pre><code>        decoded_payload = payload.strip().decode(&quot;utf-8&quot;)\n<\/code><\/pre>\n<p>My <code>locale<\/code> seems fine and I am really not sure what else could go wrong:<\/p>\n<pre><code>$ locale\nLANG=C.UTF-8\nLANGUAGE=\nLC_CTYPE=&quot;C.UTF-8&quot;\nLC_NUMERIC=&quot;C.UTF-8&quot;\nLC_TIME=&quot;C.UTF-8&quot;\nLC_COLLATE=&quot;C.UTF-8&quot;\nLC_MONETARY=&quot;C.UTF-8&quot;\nLC_MESSAGES=&quot;C.UTF-8&quot;\nLC_PAPER=&quot;C.UTF-8&quot;\nLC_NAME=&quot;C.UTF-8&quot;\nLC_ADDRESS=&quot;C.UTF-8&quot;\nLC_TELEPHONE=&quot;C.UTF-8&quot;\nLC_MEASUREMENT=&quot;C.UTF-8&quot;\nLC_IDENTIFICATION=&quot;C.UTF-8&quot;\nLC_ALL=\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1605856943127,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|encoding|amazon-sagemaker",
        "Question_view_count":183,
        "Owner_creation_time":1320517748837,
        "Owner_last_access_time":1664043736817,
        "Owner_location":null,
        "Owner_reputation":37777,
        "Owner_up_votes":1579,
        "Owner_down_votes":282,
        "Owner_views":5307,
        "Question_last_edit_time":1606193820232,
        "Answer_body":"<p>I have contacted AWS support and apparently this is a bug in <code>awscli<\/code>. Here is a revised excerpt from their long and detailed answer.<\/p>\n<blockquote>\n<p>This is an encoding issue with AWSCLI v2. For now, you can proceed\nwith AWSCLI v1.18 as a temporary solution.<\/p>\n<\/blockquote>\n<p>I also verified it works with aws-cli\/1.18.185:<\/p>\n<pre><code>$ aws --version\naws-cli\/1.18.185 Python\/3.8.3 Linux\/4.19.104-microsoft-standard botocore\/1.19.25\n\n$ aws sagemaker-runtime invoke-endpoint \\\n&gt;     --region eu-west-1 \\\n&gt;     --endpoint-name DEMO-XGBoostEndpoint-2020-11-20-06-26-30 \\\n&gt;     --body $(seq 784 | xargs echo | sed 's\/ \/,\/g') \\\n&gt;     &gt;(cat) \\\n&gt;     --content-type text\/csv &gt; \/dev\/null\n8.0%\n<\/code><\/pre>\n<p>In AWS cli v2.1.21 amazon added the <code>--cli-binary-format raw-in-base64-out<\/code> option and this should work as it worked with AWS cli v1.18:<\/p>\n<pre><code>aws sagemaker-runtime invoke-endpoint \\\n&gt;     --region &lt;aws-region&gt; \\\n&gt;     --endpoint-name &lt;you-endpoint-name&gt; \\\n&gt;     --cli-binary-format raw-in-base64-out \\\n&gt;     --body $(seq 784 | xargs echo | sed 's\/ \/,\/g') \\\n&gt;     &gt;(cat) \\\n&gt;     --content-type text\/csv &gt; \/dev\/null\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1606454756257,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1612457107052,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64925353",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53809556,
        "Question_title":"Load csv into S3 from local",
        "Question_body":"<p>I am exploring AWS sagemaker for ML. I have created a bucket:<\/p>\n\n<pre><code>bucket_name = 'test-bucket' \ns3 = boto3.resource('s3')\ntry:\n   if  my_region == 'us-east-1':\n      s3.create_bucket(Bucket=bucket_name)\n   else: \n      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\nprint('S3 bucket created successfully')\nexcept Exception as e:\n    print('S3 error: ',e)\n<\/code><\/pre>\n\n<p>I have a csv in my local and I want to load that into the bucket I created.<\/p>\n\n<p>All the links I have referred  have directions to load it from a link and unzip it. Is there a way to load the data into the bucket from the local.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1545025564273,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":1703,
        "Owner_creation_time":1455496483357,
        "Owner_last_access_time":1642546968190,
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":3912,
        "Owner_up_votes":135,
        "Owner_down_votes":47,
        "Owner_views":311,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If you are using Amazon SageMaker you can use the SageMaker python library that is implementing the most useful commands for data scientists, including the upload of files to S3. It is already installed on your SageMaker notebook instance by default. <\/p>\n\n<pre><code>import sagemaker\nsess = sagemaker.Session()\n\n# Uploading the local file to S3\nsess.upload_data(path='local-file.txt', bucket=bucket_name, key_prefix='input')    \n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1545428602017,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53809556",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66915920,
        "Question_title":"Using pytorch cuda in AWS sagemaker notebook instance",
        "Question_body":"<p>In colab, whenever we need GPU, we simply click <code>change runtime type<\/code> and change hardware accelarator to <code>GPU<\/code><\/p>\n<p>and cuda becomes available, <code>torch.cuda.is_available()<\/code> is <code>True<\/code><\/p>\n<p>How to do this is AWS sagemaker, i.e. turning on cuda.\nI am new to AWS and trying to train model using pytorch in aws sagemaker, where Pytorch code is first tested in colab environment.<\/p>\n<p>my sagemaker notebook insatnce is <code>ml.t2.medium<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617348836210,
        "Question_score":0,
        "Question_tags":"amazon-ec2|pytorch|amazon-sagemaker",
        "Question_view_count":1608,
        "Owner_creation_time":1567880532003,
        "Owner_last_access_time":1661706476487,
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":137,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":100,
        "Question_last_edit_time":1617349698827,
        "Answer_body":"<p>Using AWS Sagemaker you don't need to worry about the GPU, you simply select an instance type with GPU ans Sagemaker will use it. Specifically <code>ml.t2.medium<\/code> doesn't have a GPU but it's anyway not the right way to train a model.\nBasically you have 2 canonical ways to use Sagemaker (look at the documentation and examples please), the first is to use a notebook with a limited computing resource to spin up a training job using a prebuilt image, in that case when you call the estimator you simply specify what <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/\" rel=\"nofollow noreferrer\">instance type<\/a> you want (you'll choose one with GPU, looking at the costs). The second way is to use your own container, push it to ECR and launch a training job from the console, where you specify the instance type.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1617464162903,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66915920",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70565147,
        "Question_title":"AWS sagemaker datawrangler continues to be used after closing everything",
        "Question_body":"<p>I used data wrangler for maybe 3h a week ago, and I open my account today to see that Ive been charged for 6 days worth of data wrangler usage. Basically it was running in the background the whole time. The first 25h were part of free tier then I got charged for the rest of the time. I dont have any endpoints to close so whats the issue? I dont care about the costs, I know I can talk to support to get the charges reversed but they dont seem to know whats going on because they havent helped me at all.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641209554070,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":36,
        "Owner_creation_time":1535382420717,
        "Owner_last_access_time":1647779585177,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":"<p>After going over <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-wrangler-shut-down.html\" rel=\"nofollow noreferrer\">the docs<\/a>, I found that I needed to shut down the wrangler instance under Running Instances and Kernels button.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1641216477460,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1641216587632,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70565147",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55964972,
        "Question_title":"Can I pass arguments to the entrypoint of a SageMaker estimator?",
        "Question_body":"<p>I'm using the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html#sagemaker.estimator.Framework\" rel=\"noreferrer\">SageMaker python sdk<\/a> and was hoping to pass in some arguments to be used by my entrypoint, I'm not seeing how to do this.<\/p>\n\n<pre><code>from sagemaker.sklearn.estimator import SKLearn  # sagemaker python sdk\n\nentrypoint = 'entrypoint_script.py'\n\nsklearn = SKLearn(entry_point=entrypoint,  # &lt;-- need to pass args to this\n                  train_instance_type=instance_class,\n                  role=role,\n                  sagemaker_session=sm)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1556867880610,
        "Question_score":5,
        "Question_tags":"python|scikit-learn|data-science|amazon-sagemaker",
        "Question_view_count":2780,
        "Owner_creation_time":1343237570417,
        "Owner_last_access_time":1661366012027,
        "Owner_location":"California",
        "Owner_reputation":1325,
        "Owner_up_votes":303,
        "Owner_down_votes":10,
        "Owner_views":131,
        "Question_last_edit_time":1556899526172,
        "Answer_body":"<p>The answer is no as there is no parameter on the Estimator base class, or the fit method, that accepts arguments to pass to the entrypoint.<\/p>\n\n<p>I resolved this by passing the parameter as part of the hyperparameter dictionary. This gets passed to the entrypoint as arguments.<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1556949846450,
        "Answer_score":6,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1578346552883,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55964972",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65794703,
        "Question_title":"Amazon Forecast - extra attributes in the TARGET_TIME_SERIES?",
        "Question_body":"<p>On Amazon Forecast, let\u2019s say I have a variable (Z) which is supposed to help me predict a set of target variables (Y1, Y2, Y3).<\/p>\n<p>First question is, what is the difference between:<\/p>\n<ol>\n<li>put Z as an extra attribute in the TARGET_TIME_SERIES, that is, as an extra column<\/li>\n<li>put Z as an attribute in the RELATED_TIME_SERIES<\/li>\n<\/ol>\n<p>Second question is, given that Z has just one value per day (let\u2019s say this is a stock price), how should I deal with the fact that I have 3x-repeated timestamps? Should I just repeat Z for each repeated date?<\/p>\n<p>I understand that, if I'm not training my model to predict Z, I need to provide future values for it. But this makes option 1) even it more confusing to me. In which cases should one add an extra attribute in TARGET_TIME_SERIES?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611070761497,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-forecast",
        "Question_view_count":36,
        "Owner_creation_time":1557535777467,
        "Owner_last_access_time":1663950830700,
        "Owner_location":"Stockholm, Sweden",
        "Owner_reputation":373,
        "Owner_up_votes":349,
        "Owner_down_votes":1,
        "Owner_views":41,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I got a very nice explanation of it in here, so I'll leave it as the answer:\n<a href=\"https:\/\/github.com\/aws-samples\/amazon-forecast-samples\/issues\/104#issuecomment-764896502\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-forecast-samples\/issues\/104#issuecomment-764896502<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1611263092840,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65794703",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57908395,
        "Question_title":"How can I specify content_type in a training job of XGBoost from Sagemaker in Python?",
        "Question_body":"<p>I am trying to train a model using the sagemaker library. So far, my code is the following:<\/p>\n\n<pre><code>container = get_image_uri(boto3.Session().region_name,\n                      'xgboost', \n                      repo_version='0.90-1')\n\nestimator = sagemaker.estimator.Estimator(container, \n                                          role = 'AmazonSageMaker-ExecutionRole-20190305TXXX',\n                                          train_instance_count = 1,\n                                          train_instance_type = 'ml.m4.2xlarge',\n                                          output_path = 's3:\/\/antifraud\/production\/',\n                                          hyperparameters = {'num_rounds':'400',\n                                                             'objective':'binary:logistic',\n                                                             'eval_metric':'error@0.1'})\n\ntrain_config = training_config(estimator=estimator,\n                               inputs = {'train':'s3:\/\/antifraud\/production\/train',\n                                         'validation':'s3:\/\/-antifraud\/production\/validation'})\n<\/code><\/pre>\n\n<p>And I get an error parsing the hyperparameters. This commands gives me a configuration JSON output in the console. I have been able to run a training job using boto3 with the configuration as Json, so I have figured out that the thing I am missing in my json configuration generated by my code is the content_type parameter, which should be there as follow:<\/p>\n\n<pre><code>\"InputDataConfig\": [\n    {\n        \"ChannelName\": \"train\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"S3Prefix\",\n                \"S3Uri\": \"s3:\/\/antifraud\/production\/data\/train\",\n                \"S3DataDistributionType\": \"FullyReplicated\" \n            }\n        },\n        \"ContentType\": \"text\/csv\",\n        \"CompressionType\": \"None\"\n    },\n    {\n        \"ChannelName\": \"validation\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"S3Prefix\",\n                \"S3Uri\": \"s3:\/\/antifraud\/production\/validation\",\n                \"S3DataDistributionType\": \"FullyReplicated\"\n            }\n        },\n        \"ContentType\": \"text\/csv\",\n        \"CompressionType\": \"None\"\n    }\n]\n<\/code><\/pre>\n\n<p>I have tried coding content_type = 'text\/csv' in container, estimator and train_config as parameter and also inside inputs as another key of the dictionary, with no success. How could I make this work?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1568296785460,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":644,
        "Owner_creation_time":1523298968403,
        "Owner_last_access_time":1663934452963,
        "Owner_location":null,
        "Owner_reputation":1754,
        "Owner_up_votes":396,
        "Owner_down_votes":76,
        "Owner_views":197,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I have solved it using s3_input objects:<\/p>\n\n<pre><code>s3_input_train = sagemaker.s3_input(s3_data='s3:\/\/antifraud\/production\/data\/{domain}-{product}-{today}\/train_data.csv',\ncontent_type='text\/csv')\ns3_input_validation = sagemaker.s3_input(s3_data='s3:\/\/antifraud\/production\/data\/{domain}-{product}-{today}\/validation_data.csv',\ncontent_type='text\/csv')\n\ntrain_config = training_config(estimator=estimator,\ninputs = {'train':s3_input_train,\n          'validation':s3_input_validation})\n<\/code><\/pre>",
        "Answer_comment_count":1,
        "Answer_creation_time":1568804066440,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57908395",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56773989,
        "Question_title":"In Amazon SageMaker, what (if any) is the difference between an inference and prediction?",
        "Question_body":"<p>Amazon SageMaker has <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">inference pipelines<\/a> that process requests for inferences on data. It sounds as though inferences are similar (or perhaps identical) to predictions. Are there any differences between inferences and predictions? If so, what? If not, why not just call it a prediction pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1561555658777,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":42,
        "Owner_creation_time":1336973807643,
        "Owner_last_access_time":1655749162853,
        "Owner_location":"Minneapolis, MN, United States",
        "Owner_reputation":1907,
        "Owner_up_votes":692,
        "Owner_down_votes":6,
        "Owner_views":174,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Inference usually refers to applying a learned transformation to input data. That learned transformation could be something else than a prediction (eg dim reduction, clustering, entity extraction etc). So calling that process a prediction would be a bit too restrictive in my opinion<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1561569885853,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56773989",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68193708,
        "Question_title":"Unable to Define Auto Scaling for SageMaker Endpoint",
        "Question_body":"<p>I have deployed an AWS endpoint using a Docker container (I followed <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/docker-containers.html\" rel=\"nofollow noreferrer\">this<\/a>).<\/p>\n<p>Everything is working perfectly but now I need to put it in production and define an auto scaling strategy.<\/p>\n<p>I tried 2 things:<\/p>\n<ol>\n<li><p>AWS console but the auto scaling button is greyed\nout.<\/p>\n<\/li>\n<li><p>The method described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-add-code-apply.html\" rel=\"nofollow noreferrer\">here<\/a>. My endpoint name\nis <code>EmbeddingEndpoint<\/code> and my variant name is <code>SimpleVariant<\/code>. So my\nfinal command is<\/p>\n<\/li>\n<\/ol>\n<pre><code>aws application-autoscaling put-scaling-policy \\\n--policy-name scalable_policy_for_embedding \\\n--policy-type TargetTrackingScaling \\\n--resource-id endpoint\/EmbeddingEndpoint\/variant\/SimpleVariant \\\n--service-namespace sagemaker \\\n--scalable-dimension sagemaker:variant:DesiredInstanceCount \\\n--target-tracking-scaling-policy-configuration file:\/\/policy_config.json\n<\/code><\/pre>\n<p>but I get this result :<\/p>\n<pre><code>An error occurred (ObjectNotFoundException) when calling the PutScalingPolicy operation: \nNo scalable target registered for service namespace: sagemaker, resource ID: \nendpoint\/EmbeddingEndpoint\/variant\/SimpleVariant, scalable dimension: \nsagemaker:variant:DesiredInstanceCount\n<\/code><\/pre>\n<p>Does someone have another solution, or is it that I didn't set the variable well ?\nThank you in advance !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625051653240,
        "Question_score":1,
        "Question_tags":"amazon-web-services|docker|scaling|endpoint|amazon-sagemaker",
        "Question_view_count":336,
        "Owner_creation_time":1570620624977,
        "Owner_last_access_time":1663790606097,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1630503140507,
        "Answer_body":"<p>Your <code>sagemaker<\/code> service-namespace does not have any registered scaling targets. You need to first run <code>register-scalable-target<\/code> before running <code>put-scaling-policy<\/code>.<\/p>\n<pre><code>aws application-autoscaling register-scalable-target \\\n    --service-namespace sagemaker \\\n    --scalable-dimension sagemaker:variant:DesiredInstanceCount \\\n    --resource-id endpoint\/EmbeddingEndpoint\/variant\/SimpleVariant\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1630466729773,
        "Answer_score":1,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68193708",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67380942,
        "Question_title":"Pushing docker image in AWS ECR from SageMaker Studio using AWS CLI",
        "Question_body":"<p>We can now publish Docker images to AWS ECR directly from <strong>SageMaker Studio<\/strong> using this code <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli<\/a>\nI did follow the easy installation instructions:<\/p>\n<pre><code>!pip install sagemaker-studio-image-build\nsm-docker build .\n<\/code><\/pre>\n<p>Also Trust policy and permissions have been set as described in the instructions.\nBut I'm getting the error &quot;<strong>Command did not exit successfully docker push<\/strong>&quot; at the stage where it is pushing the Docker image to AWS ECR. Any idea why? Here are the details print as output:<\/p>\n<pre><code>[Container] 2021\/05\/04 06:57:20 Running command echo Pushing the Docker image...\nPushing the Docker image...\n\n[Container] 2021\/05\/04 06:57:20 Running command docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\/$IMAGE_REPO_NAME:$IMAGE_TAG\nThe push refers to repository [752731038471.dkr.ecr.eu-central-1.amazonaws.com\/sagemaker-studio-d-tfbogtriaiml]\nAn image does not exist locally with the tag: 752731038471.dkr.ecr.eu-central-1.amazonaws.com\/sagemaker-studio-d-tfbogtriaiml\n\n[Container] 2021\/05\/04 06:57:20 Command did not exit successfully docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\/$IMAGE_REPO_NAME:$IMAGE_TAG exit status 1\n[Container] 2021\/05\/04 06:57:20 Phase complete: POST_BUILD State: FAILED\n[Container] 2021\/05\/04 06:57:20 Phase context status code: COMMAND_EXECUTION_ERROR Message: Error while executing command: docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\/$IMAGE_REPO_NAME:$IMAGE_TAG. Reason: exit status 1\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620115524227,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|amazon-ecr",
        "Question_view_count":1046,
        "Owner_creation_time":1469978721363,
        "Owner_last_access_time":1664014644413,
        "Owner_location":null,
        "Owner_reputation":415,
        "Owner_up_votes":102,
        "Owner_down_votes":1,
        "Owner_views":48,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In the Dockerfile, there was a reference to another file that was not present in the directory from where the command <code>sm-docker build .<\/code> was launched.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1620378926343,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67380942",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59882941,
        "Question_title":"ValueError: no SavedModel bundles found! when trying to deploy a TF2.0 model to SageMaker",
        "Question_body":"<p>I'm trying to deploy a TF2.0 model to SageMaker. So far, I managed to train the model and save it into an S3 bucket but when I'm calling the <code>.deploy()<\/code> method, I get the following error from cloud Watch <\/p>\n\n<p><code>ValueError: no SavedModel bundles found!<\/code><\/p>\n\n<p>Here is my training script: <\/p>\n\n<pre><code>### Code to add in a tensorflow_estimator.py file\n\nimport argparse\nimport os\nimport pathlib\nimport tensorflow as tf\n\n\nif __name__ == '__main__':\n\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--learning_rate', type=float, default=0.1)\n\n    # Data, model, and output directories\n    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n    args, _ = parser.parse_known_args()\n\n    print(\"##### ARGS ##### \\n{}\".format(args))\n\n    # Get files \n    path = pathlib.Path(args.train)\n\n    # Print out folder content \n    for item in path.iterdir():\n        print(\"##### DIRECTORIES ##### \\n{}\".format(item))\n\n    # Get all images \n    all_images = list(path.glob(\"*\/*\"))\n    all_image_paths = [str(path) for path in list(path.glob(\"*\/*\"))]\n\n    # Transform images into tensors\n    def preprocess_and_load_images(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, [192, 192])\n        return image\n\n    # Apply preprocessing function\n    ds_paths = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    ds_images = ds_paths.map(preprocess_and_load_images)\n\n    # Map Labels\n    labels = []\n    for data in path.iterdir():  \n        if data.is_dir():               \n            labels += [data.name]    \n\n    labels_index = {}\n    for i,label in enumerate(labels):\n        labels_index[label]=i\n\n    print(\"##### Label Index ##### \\n{}\".format(labels_index))\n\n    all_image_labels = [labels_index[path.parent.name] for path in list(path.glob(\"*\/*\"))]\n\n    # Create a tf Dataset\n    labels_ds = tf.data.Dataset.from_tensor_slices(all_image_labels)\n\n    # Zip train and labeled dataset\n    full_ds = tf.data.Dataset.zip((ds_images, labels_ds))\n\n    # Shuffle Dataset and batch it \n    full_ds = full_ds.shuffle(len(all_images)).batch(args.batch_size)\n\n    # Create a pre-trained model \n    base_model = tf.keras.applications.InceptionV3(input_shape=(192,192,3), \n                                               include_top=False,\n                                               weights = \"imagenet\"\n                                               )\n\n    base_model.trainable = False\n    model = tf.keras.Sequential([\n                base_model,\n                tf.keras.layers.GlobalAveragePooling2D(),\n                tf.keras.layers.Dense(len(labels), activation=\"softmax\")\n            ])\n\n    initial_learning_rate = args.learning_rate\n\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=1000,\n        decay_rate=0.96,\n        staircase=True) \n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n\n    # Train the model\n    model.fit(full_ds, epochs=args.epochs)\n\n    # Save the model \n    model.save(os.path.join(args.model_dir, \"tf_model\"), save_format=\"tf\")\n\ndef model_fn(model_dir):\n    classifier = tf.keras.models.load_model(os.path.join(model_dir, \"tf_model\"))\n    return classifier\n<\/code><\/pre>\n\n<p>And here is the code that I wrote into Colab <\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='tensorflow_estimator.py', \n                          role=role,\n                          train_instance_count=1, \n                          train_instance_type='ml.m5.large',\n                          framework_version='2.0.0', \n                          sagemaker_session=sagemaker_session,\n                          output_path=s3_output_location,\n                          hyperparameters={'epochs': 1,\n                                           'batch_size': 30,\n                                           'learning_rate': 0.001},\n                          py_version='py3')\n\n\ntf_estimator.fit({\"train\":train_data})\n\nfrom sagemaker.tensorflow.serving import Model\n\nmodel = Model(model_data='s3:\/\/path\/to\/model.tar.gz', \n              role=role,\n              framework_version=\"2.0.0\",\n              sagemaker_session=sagemaker_session)\n\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m5.large')\n<\/code><\/pre>\n\n<p>I already tried to look at <a href=\"https:\/\/stackoverflow.com\/questions\/57172147\/no-savedmodel-bundles-found-on-tensorflow-hub-model-deployment-to-aws-sagemak\">this thread<\/a> but I actually don't have the problem of versions in my tar.gz file as the structure is the following : <\/p>\n\n<pre><code>\u251c\u2500\u2500 assets\n\u251c\u2500\u2500 saved_model.pb\n\u2514\u2500\u2500 variables\n    \u251c\u2500\u2500 variables.data-00000-of-00001\n    \u2514\u2500\u2500 variables.index\n<\/code><\/pre>\n\n<p>I feel I might be wrong when defining <code>model_fn()<\/code> in my training script but definitely don't what to replace that with. Would you guys have an idea? <\/p>\n\n<p>Thanks a lot for your help!  <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1579796394240,
        "Question_score":4,
        "Question_tags":"python|deployment|tensorflow2.0|amazon-sagemaker",
        "Question_view_count":943,
        "Owner_creation_time":1562055808543,
        "Owner_last_access_time":1663141286633,
        "Owner_location":null,
        "Owner_reputation":895,
        "Owner_up_votes":29,
        "Owner_down_votes":1,
        "Owner_views":53,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I actually tried to modify my training script to the following : <\/p>\n\n<pre><code>### Code to add in a tensorflow_estimator.py file\n\nimport argparse\nimport os\nimport pathlib\nimport tensorflow as tf\n\n\nif __name__ == '__main__':\n\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--learning_rate', type=float, default=0.1)\n\n    # Data, model, and output directories\n    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n    args, _ = parser.parse_known_args()\n\n    print(\"##### ARGS ##### \\n{}\".format(args))\n\n    # Get files \n    path = pathlib.Path(args.train)\n\n    # Print out folder content \n    for item in path.iterdir():\n        print(\"##### DIRECTORIES ##### \\n{}\".format(item))\n\n    # Get all images \n    all_images = list(path.glob(\"*\/*\"))\n    all_image_paths = [str(path) for path in list(path.glob(\"*\/*\"))]\n\n    # Transform images into tensors\n    def preprocess_and_load_images(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, [192, 192])\n        return image\n\n    # Apply preprocessing function\n    ds_paths = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    ds_images = ds_paths.map(preprocess_and_load_images)\n\n    # Map Labels\n    labels = []\n    for data in path.iterdir():  \n        if data.is_dir():               \n            labels += [data.name]    \n\n    labels_index = {}\n    for i,label in enumerate(labels):\n        labels_index[label]=i\n\n    print(\"##### Label Index ##### \\n{}\".format(labels_index))\n\n    all_image_labels = [labels_index[path.parent.name] for path in list(path.glob(\"*\/*\"))]\n\n    # Create a tf Dataset\n    labels_ds = tf.data.Dataset.from_tensor_slices(all_image_labels)\n\n    # Zip train and labeled dataset\n    full_ds = tf.data.Dataset.zip((ds_images, labels_ds))\n\n    # Shuffle Dataset and batch it \n    full_ds = full_ds.shuffle(len(all_images)).batch(args.batch_size)\n\n    # Create a pre-trained model \n    base_model = tf.keras.applications.InceptionV3(input_shape=(192,192,3), \n                                               include_top=False,\n                                               weights = \"imagenet\"\n                                               )\n\n    base_model.trainable = False\n    model = tf.keras.Sequential([\n                base_model,\n                tf.keras.layers.GlobalAveragePooling2D(),\n                tf.keras.layers.Dense(len(labels), activation=\"softmax\")\n            ])\n\n    initial_learning_rate = args.learning_rate\n\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=1000,\n        decay_rate=0.96,\n        staircase=True) \n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n\n    # Train the model\n    model.fit(full_ds, epochs=args.epochs)\n\n    # Save the model \n    model.save(os.path.join(args.model_dir, \"tensorflow_model\/1\"), save_format=\"tf\")\n\n<\/code><\/pre>\n\n<p>It seems that it's important to have a numerical name for your folder:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Save the model\nmodel.save(os.path.join(args.model_dir, \"tensorflow_model\/1\"), save_format=\"tf\")\n<\/code><\/pre>",
        "Answer_comment_count":2,
        "Answer_creation_time":1579799847967,
        "Answer_score":4,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1590696546172,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59882941",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54629890,
        "Question_title":"Invoke aws sagemaker endpoint",
        "Question_body":"<p>I have some data in S3 and I want to create a lambda function to predict the output with my deployed aws sagemaker endpoint then I put the outputs in S3 again. Is it necessary in this case to create an api gateway like decribed in this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"noreferrer\">link<\/a> ? and in the lambda function what I have to put. I expect to put (where to find the data, how to invoke the endpoint, where to put the data) <\/p>\n\n<pre><code>import boto3\nimport io\nimport json\nimport csv\nimport os\n\n\nclient = boto3.client('s3') #low-level functional API\n\nresource = boto3.resource('s3') #high-level object-oriented API\nmy_bucket = resource.Bucket('demo-scikit-byo-iris') #subsitute this for your s3 bucket name. \n\nobj = client.get_object(Bucket='demo-scikit-byo-iris', Key='foo.csv')\nlines= obj['Body'].read().decode('utf-8').splitlines()\nreader = csv.reader(lines)\n\nimport io\nfile = io.StringIO(lines)\n\n# grab environment variables\nruntime= boto3.client('runtime.sagemaker')\n\nresponse = runtime.invoke_endpoint(\n    EndpointName= 'nilm2',\n    Body = file.getvalue(),\n    ContentType='*\/*',\n    Accept = 'Accept')\n\noutput = response['Body'].read().decode('utf-8')\n<\/code><\/pre>\n\n<p>my data is a csv file of 2 columns of floats with no headers, the problem is that lines return a list of strings(each row is an element of this list:['11.55,65.23', '55.68,69.56'...]) the invoke work well but the response is also a string: output = '65.23\\n,65.23\\n,22.56\\n,...' <\/p>\n\n<p>So how to save this output to S3 as a csv file <\/p>\n\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1549885499183,
        "Question_score":5,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":3146,
        "Owner_creation_time":1502010899810,
        "Owner_last_access_time":1573819157507,
        "Owner_location":"Tunis, Tunisia",
        "Owner_reputation":109,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If your Lambda function is scheduled, then you won't need an API Gateway. But if the predict action will be triggered by a user, by an application, for example, you will need.<\/p>\n\n<p>When you call the invoke endpoint, actually you are calling a SageMaker endpoint, which is not the same as an API Gateway endpoint. <\/p>\n\n<p>A common architecture with SageMaker is:<\/p>\n\n<ol>\n<li>API Gateway with receives a request then calls an authorizer, then\ninvoke your Lambda; <\/li>\n<li>A Lambda with does some parsing in your input data, then calls your SageMaker prediction endpoint, then, handles the result and returns to your application.<\/li>\n<\/ol>\n\n<p>By the situation you describe, I can't say if your task is some academic stuff or a production one.<\/p>\n\n<p>So, how you can save the data as a CSV file from your Lambda? <\/p>\n\n<p>I believe you can just parse the output, then just upload the file to S3. Here you will parse manually or with a lib, with boto3 you can upload the file. The output of your model depends on your implementation on SageMaker image. So, if you need the response data in another format, maybe you will need to use a <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">custom image<\/a>. I normally use a custom image, which I can define how I want to handle my data on requests\/responses.<\/p>\n\n<p>In terms of a production task, I certainly recommend you check Batch transform jobs from SageMaker. You can provide an input file (the S3 path) and also a destination file (another S3 path). The SageMaker will run the batch predictions and will persist a file with the results. Also, you won't need to deploy your model to an endpoint, when this job run, will create an instance of your endpoint, download your data to predict, do the predictions, upload the output, and shut down the instance. You only need a trained model.<\/p>\n\n<p>Here some info about Batch transform jobs:<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html<\/a><\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-batch-transform.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-batch-transform.html<\/a><\/p>\n\n<p>I hope it helps, let me know if need more info.<\/p>\n\n<p>Regards.<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1550022296023,
        "Answer_score":4,
        "Question_favorite_count":4.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54629890",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68193944,
        "Question_title":"Set custom environment variables in AWS",
        "Question_body":"<p>I am using AWS sagemaker, I have some secret keys and access keys to access some APIs that I don't want to expose directly in code.<\/p>\n<p>What are the ways like environment variables etc., that can be used to hide these keys and I can use them securely, and how to set them.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625052591003,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":277,
        "Owner_creation_time":1567880532003,
        "Owner_last_access_time":1661706476487,
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":137,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":100,
        "Question_last_edit_time":1625058756809,
        "Answer_body":"<p>AWS System Manager (SSM) is designed to store keys and tokens securely.<\/p>\n<p>Depending on how your notebook is defined, you could <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html\" rel=\"nofollow noreferrer\">use the 'env' property<\/a> directly or in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-environment-variables\" rel=\"nofollow noreferrer\">training data<\/a>, or you could access SSM directly from sagemaker. For example this Snowflake KB article explains how to fetch auth info from ssm: <a href=\"https:\/\/community.snowflake.com\/s\/article\/Connecting-a-Jupyter-Notebook-Part-3\" rel=\"nofollow noreferrer\">https:\/\/community.snowflake.com\/s\/article\/Connecting-a-Jupyter-Notebook-Part-3<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1625064328873,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68193944",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63457857,
        "Question_title":"What is called within a sagemaker custom (training) container?",
        "Question_body":"<p>Somewhere this spring the behaviour of the sagemaker docker image changed and I cannot find the way I need to construct it now.<\/p>\n<p><strong>Directory structure<\/strong><\/p>\n<pre><code>\/src\/some\/package\n\/project1\n    \/some_entrypoint.py\n    \/some_notebook.ipynb\n\/project2\n    \/another_entrypoint.py\n    \/another_notebook.ipynb\nsetup.py\n<\/code><\/pre>\n<p><strong>Docker file<\/strong><\/p>\n<p>Note that I want to shift tensorflow version, so I changed the <code>FROM<\/code> to the latest version. This was the\nbreaking change.<\/p>\n<pre><code># Core\nFROM 763104351884.dkr.ecr.eu-west-1.amazonaws.com\/tensorflow-training:2.3.0-cpu-py37-ubuntu18.04\n\nCOPY . \/opt\/ml\/code\/all\/\nRUN pip install \/opt\/ml\/code\/all\/\n\nWORKDIR &quot;\/opt\/ml\/code&quot;\n<\/code><\/pre>\n<p><strong>Python code<\/strong><\/p>\n<p>This code should start the entrypoints, for example here we have the code of some_notebook.ipynb. I tried all possible combinations of working directory + source_dir (None, '.', or '..'), entry_point (with or without \/), dependencies ('src')...<\/p>\n<ul>\n<li>if setup is present it tries to call my project as a module (python -m some_entrypoint)<\/li>\n<li>if not, it often is not able to find my entrypoint. Which I don't understand because the TensorFlow is supposed to add it to the container, isn't it?<\/li>\n<\/ul>\n<pre><code>estimator = TensorFlow(\n   entry_point='some_entrypoint.py', \n   image_name='ECR.dkr.ecr.eu-west-1.amazonaws.com\/overall-project\/sagemaker-training:latest',\n   source_dir='.',\n#    dependencies=['..\/src\/'],\n   script_mode=True,\n\n   train_instance_type='ml.m5.4xlarge',\n   train_instance_count=1,\n   train_max_run=60*60,  # seconds * minutes\n   train_max_wait=60*60,  # seconds * minutes. Must be &gt;= train_max_run\n   hyperparameters=hyperparameters,\n   metric_definitions=metrics,\n   role=role,\n   framework_version='2.0.0',\n   py_version='py3',\n  )\nestimator.fit({\n    'training': f&quot;s3:\/\/some-data\/&quot;}\n#   , wait=False\n)\n<\/code><\/pre>\n<p>Ideally I would want to understand the logic within: what is called given what settings?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597694559337,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":61,
        "Owner_creation_time":1484838464573,
        "Owner_last_access_time":1663858511743,
        "Owner_location":"Amsterdam, Nederland",
        "Owner_reputation":3937,
        "Owner_up_votes":672,
        "Owner_down_votes":27,
        "Owner_views":387,
        "Question_last_edit_time":1597697877660,
        "Answer_body":"<p>when the training container runs, your entry_point script will be executed.<\/p>\n<p>Since your notebook file and entry_point script are under the same directory, your <code>source_dir<\/code> should just be &quot;.&quot;<\/p>\n<p>Does your entry_point script import any modules that are not installed by the tensorflow training container by default? Also could you share your stacktrace of the error?<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1600272249597,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63457857",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72712449,
        "Question_title":"SageMaker Pipeline - Processing step for ImageClassification model",
        "Question_body":"<p>I'm trying to solve ImageClassification task. I have prepared a code to train, evaluate and deploy tensorflow model in SageMaker Notebook. I'm new with SageMaker and SageMaker Pipeline too. Currently, I'm trying to split my code and create SageMaker pipeline to solve Image Classification task.\nIn reference to AWS documentation there is <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-and-manage-steps.html#step-type-processing\" rel=\"nofollow noreferrer\">Processing steps<\/a>. I have a code which read data from S3 and use <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator\" rel=\"nofollow noreferrer\">ImageGenerator<\/a> to generate augmented images on the fly while tensorflow model is still in the training stage.<\/p>\n<p>I don't find anything of how I can use <code>ImageGenerator<\/code> inside of Processing step in SageMaker Pipeline.<\/p>\n<p>My Code of <code>ImageGenerator<\/code>:<\/p>\n<pre><code>def load_data(mode):\n    if mode == 'TRAIN':\n        datagen = ImageDataGenerator(\n            rescale=1. \/ 255,\n            rotation_range = 0.5,\n            shear_range=0.2,\n            zoom_range=0.2,\n            width_shift_range = 0.2,\n            height_shift_range = 0.2,\n            fill_mode = 'nearest',\n            horizontal_flip=True)\n    else:\n        datagen = ImageDataGenerator(rescale=1. \/ 255)\n    return datagen\n\n\ndef get_flow_from_directory(datagen,\n                            data_dir,\n                            batch_size,\n                            shuffle=True):\n    assert os.path.exists(data_dir), (&quot;Unable to find images resources for input&quot;)\n    generator = datagen.flow_from_directory(data_dir,\n                                            class_mode = &quot;categorical&quot;,\n                                            target_size=(HEIGHT, WIDTH),\n                                            batch_size=batch_size,\n                                            shuffle=shuffle\n                                            )\n    print('Labels are: ', generator.class_indices)\n    return generator\n<\/code><\/pre>\n<p>Question is - does it possible to use <code>ImageGenerator<\/code> inside of <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-and-manage-steps.html#step-type-processing\" rel=\"nofollow noreferrer\">Processing step<\/a> of SageMaker Pipeline?\nI'd appreciate for any ideas, Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655888089677,
        "Question_score":0,
        "Question_tags":"tensorflow|keras|amazon-sagemaker|image-preprocessing",
        "Question_view_count":68,
        "Owner_creation_time":1470228490790,
        "Owner_last_access_time":1663345526560,
        "Owner_location":null,
        "Owner_reputation":504,
        "Owner_up_votes":758,
        "Owner_down_votes":3,
        "Owner_views":82,
        "Question_last_edit_time":null,
        "Answer_body":"<p>So, <code>ImageGenerator<\/code> and <code>flow_from_directory<\/code> I continue use inside of Training step. Processing step I skip at all, just use Training, Evaluating and Register model.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1656099367803,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72712449",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53533434,
        "Question_title":"AWS Sagemaker - Blazingtext BatchTransform no output",
        "Question_body":"<p>I have trained a blazingText model and followed this guide.\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html<\/a><\/p>\n\n<p>\"Sample JSON request\" The Invoke end point is working perfectly. So I switched to,\nBatch Transform Job with \"content-type: application\/jsonlines\" and created a file in S3 with the following format data:<\/p>\n\n<pre><code>{\"source\": \"source_0\"}\n<\/code><\/pre>\n\n<p>The job ran success. But the output did not sent to S3. Also In the cloud logs,<\/p>\n\n<pre><code>\" [79] [INFO] Booting worker with pid: 79\"\n<\/code><\/pre>\n\n<p>This is is the last response. Did anyone know what went wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1543474625430,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":317,
        "Owner_creation_time":1433761344910,
        "Owner_last_access_time":1664023707623,
        "Owner_location":"Aruppukkottai, India",
        "Owner_reputation":802,
        "Owner_up_votes":556,
        "Owner_down_votes":6,
        "Owner_views":151,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I have found the issue. The batchtransform select the folder as input and the s3 source should be S3Prefix instead of manifest.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1543477871070,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53533434",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54671841,
        "Question_title":"Sagemaker endpoint(with VPC) url accessible from internet",
        "Question_body":"<p>I have created a model with VPC, Private subnet, and appropriate security group. The endpoint URL can, however, be reached through the internet though failing due to the lack of security token<\/p>\n\n<p>Things I need clarification on now are<\/p>\n\n<ol>\n<li>Is there a way to avoid the URL being accessible from the internet<\/li>\n<li>Are we not charged for requests failed on AUTH(like for API Gateway)<\/li>\n<li>Does that make our deployment vulnerable to any attacks<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1550066008687,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":679,
        "Owner_creation_time":1320419229500,
        "Owner_last_access_time":1655464612793,
        "Owner_location":null,
        "Owner_reputation":4924,
        "Owner_up_votes":236,
        "Owner_down_votes":14,
        "Owner_views":358,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You are not hitting your endpoint, but the endpoint of AWS SageMaker runtime. This endpoint is checking all the permissions to access your hosted model, and only if the credentials and requirements are met, the request is forwarded to your instances and models. <\/p>\n\n<p>Therefore, you can't prevent this URL from being accessible from the Internet, but at the same time, you don't need to protect it or pay for it. AWS has a high level of security on these endpoints, and I don't think that you have a more secure way to protect these endpoints. <\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1550250221603,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54671841",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60036916,
        "Question_title":"Sagemaker lifecycle configuration for installing pandas not working",
        "Question_body":"<p>I am trying to update pandas within a lifecycle configuration, and following the example of AWS I have the next code:<\/p>\n\n<pre><code>#!\/bin\/bash\n\nset -e\n\n# OVERVIEW\n# This script installs a single pip package in a single SageMaker conda environments.\n\nsudo -u ec2-user -i &lt;&lt;EOF\n# PARAMETERS\nPACKAGE=pandas\nENVIRONMENT=python3\nsource \/home\/ec2-user\/anaconda3\/bin\/activate \"$ENVIRONMENT\"\npip install --upgrade \"$PACKAGE\"==0.25.3\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\nEOF\n<\/code><\/pre>\n\n<p>Then I attach it to a notebook and when I enter the notebook and open a notebook file, I see that pandas have not been updated. Using <code>!pip show pandas<\/code> I get:<\/p>\n\n<pre><code>Name: pandas\nVersion: 0.24.2\nSummary: Powerful data structures for data analysis, time series, and statistics\nHome-page: http:\/\/pandas.pydata.org\nAuthor: None\nAuthor-email: None\nLicense: BSD\nLocation: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\nRequires: pytz, python-dateutil, numpy\nRequired-by: sparkmagic, seaborn, odo, hdijupyterutils, autovizwidget\n<\/code><\/pre>\n\n<p>So we can see that I am indeed in the python3 env although the version is 0.24. <\/p>\n\n<p>However, the log in cloudwatch shows that it has been installed:<\/p>\n\n<pre><code>Collecting pandas==0.25.3 Downloading https:\/\/files.pythonhosted.org\/packages\/52\/3f\/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d\/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in .\/anaconda3\/lib\/python3.6\/site-packages (from pandas==0.25.3) (2018.4)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: python-dateutil&gt;=2.6.1 in .\/anaconda3\/lib\/python3.6\/site-packages (from pandas==0.25.3) (2.7.3)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: numpy&gt;=1.13.3 in .\/anaconda3\/lib\/python3.6\/site-packages (from pandas==0.25.3) (1.16.4)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: six&gt;=1.5 in .\/anaconda3\/lib\/python3.6\/site-packages (from python-dateutil&gt;=2.6.1-&gt;pandas==0.25.3) (1.13.0)\n2020-02-03T12:33:09.065+01:00\nInstalling collected packages: pandas Found existing installation: pandas 0.24.2 Uninstalling pandas-0.24.2: Successfully uninstalled pandas-0.24.2\n2020-02-03T12:33:12.066+01:00\nSuccessfully installed pandas-0.25.3\n<\/code><\/pre>\n\n<p>What could be the problem? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1580724156233,
        "Question_score":2,
        "Question_tags":"python|pandas|amazon-web-services|pip|amazon-sagemaker",
        "Question_view_count":1493,
        "Owner_creation_time":1523298968403,
        "Owner_last_access_time":1663934452963,
        "Owner_location":null,
        "Owner_reputation":1754,
        "Owner_up_votes":396,
        "Owner_down_votes":76,
        "Owner_views":197,
        "Question_last_edit_time":1580746498476,
        "Answer_body":"<p>if you want to install the  packages only in for the python3 environment, use the following script in your <strong>Create Sagemaker Lifecycle<\/strong> configurations. <\/p>\n\n<pre><code>#!\/bin\/bash\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\n# This will affect only the Jupyter kernel called \"conda_python3\".\nsource activate python3\n\n# Replace myPackage with the name of the package you want to install.\npip install pandas==0.25.3\n# You can also perform \"conda install\" here as well.\nsource deactivate\nEOF\n<\/code><\/pre>\n\n<p>Reference : \"<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">Lifecycle Configuration Best Practices<\/a>\" <\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1581352236820,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60036916",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73611956,
        "Question_title":"Remove JSON object via AWS Update* API to prevent Terraform from recreating the resource",
        "Question_body":"<p>I have an AWS SageMaker domain in my account created via Terraform. The resource was modified outside of Terraform. The modification was the equivalent of the following:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>aws sagemaker update-domain --domain-id d-domainid123 --default-user-settings '{&quot;KernelGatewayAppSettings&quot;: { &quot;CustomImages&quot;: [ { ... } ] } }'\n<\/code><\/pre>\n<p>Ever since, all <code>terraform plan<\/code> operations want to replace the AWS SageMaker domain:<\/p>\n<pre><code>  # module.main.aws_sagemaker_domain.default must be replaced\n-\/+ resource &quot;aws_sagemaker_domain&quot; &quot;default&quot; {\n      ~ arn                                            = &quot;arn:aws:sagemaker:eu-central-1:000111222333:domain\/d-domainid123&quot; -&gt; (known after apply)\n      ...\n        # (6 unchanged attributes hidden)\n      ~ default_user_settings {\n            # (2 unchanged attributes hidden)\n          - kernel_gateway_app_settings { # forces replacement\n               - custom_images = [ ... ]\n            }\n        }\n    }\n<\/code><\/pre>\n<p>My goal is to reconcile the situation without Terraform or me needing to create a new domain. I can't modify the Terraform sources to match the state of the SageMaker domain because that would force the recreation of domains in other accounts provisioned from the same Terraform source code.<\/p>\n<p><strong>I want to issue an <code>aws<\/code> CLI command that updates the domain and removes the <code>&quot;KernelGatewayAppSettings&quot;: { ... }<\/code> key completely from the <code>&quot;DefaultUserSettings&quot;<\/code> of the SageMaker domain. Is there a way to do this?<\/strong><\/p>\n<p>I tried the following, but the empty object is still there, so they did not work.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>aws sagemaker update-domain --domain-id d-domainid123 --default-user-settings '{&quot;KernelGatewayAppSettings&quot;: {} }'\naws sagemaker update-domain --domain-id d-domainid123 --default-user-settings '{&quot;KernelGatewayAppSettings&quot;: null }'\n\n# Still:\naws sagemaker describe-domain --domain-id d-domainid123\n{\n    &quot;DomainArn&quot;: ...,\n    &quot;DomainId&quot;: ...,\n    ...\n    &quot;DefaultUserSettings&quot;: {\n        &quot;ExecutionRole&quot;: &quot;arn:aws:iam::0001112233444:role\/SageMakerStudioExecutionRole&quot;,\n        &quot;SecurityGroups&quot;: [\n            &quot;...&quot;\n        ],\n        &quot;KernelGatewayAppSettings&quot;: {\n            &quot;CustomImages&quot;: []\n        }\n    },\n    ...\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662393203630,
        "Question_score":0,
        "Question_tags":"amazon-web-services|terraform|aws-cli|amazon-sagemaker",
        "Question_view_count":32,
        "Owner_creation_time":1219760368600,
        "Owner_last_access_time":1663933250613,
        "Owner_location":"Miskolc, Hungary",
        "Owner_reputation":3743,
        "Owner_up_votes":12,
        "Owner_down_votes":5,
        "Owner_views":178,
        "Question_last_edit_time":1662455073636,
        "Answer_body":"<p>One option you have is to use the <a href=\"https:\/\/www.terraform.io\/language\/meta-arguments\/lifecycle\" rel=\"nofollow noreferrer\">lifecycle meta argument<\/a> to ignore out-of-band changes to the resource.<\/p>\n<pre><code>  lifecycle {\n    ignore_changes = [\n      default_user_settings\n    ]\n  }\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1662394449850,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73611956",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73646830,
        "Question_title":"What inference can be made out of Baseline drift distance in data quality monitoring sagemaker?",
        "Question_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/GU4v0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/GU4v0.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>How is the above distance calculated and can I set my own threshold?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662629736510,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":55,
        "Owner_creation_time":1660119311500,
        "Owner_last_access_time":1664029159210,
        "Owner_location":null,
        "Owner_reputation":15,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1663220877289,
        "Answer_body":"<p>You can find information on how the distributions are compared here (see <code>distribution_constraints<\/code> in the table):<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-byoc-constraints.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-byoc-constraints.html<\/a><\/p>\n<p>You can change the threshold in the constraint file to what you would like.<\/p>\n<p>The baseline computes baseline schema constraints and statistics for each feature using Deequ. If youwould like more details you can take a look at the implementation here:<\/p>\n<p><a href=\"https:\/\/github.com\/awslabs\/deequ\/blob\/master\/src\/main\/scala\/com\/amazon\/deequ\/analyzers\/Distance.scala\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/deequ\/blob\/master\/src\/main\/scala\/com\/amazon\/deequ\/analyzers\/Distance.scala<\/a><\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1662747458550,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73646830",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63500377,
        "Question_title":"memory issues for sparse one hot encoded features",
        "Question_body":"<p>I want to create sparse matrix for one hot encoded features from data frame <code>df<\/code>. But I am getting memory issue for code given below. Shape of <code>sparse_onehot<\/code> is  (450138, 1508)<\/p>\n<pre><code>sp_features = ['id', 'video_id', 'genre']\nsparse_onehot = pd.get_dummies(df[sp_features], columns = sp_features)\nimport scipy\nX = scipy.sparse.csr_matrix(sparse_onehot.values)\n<\/code><\/pre>\n<p>I get memory error as shown below.<\/p>\n<pre><code>MemoryError: Unable to allocate 647. MiB for an array with shape (1508, 450138) and data type uint8\n<\/code><\/pre>\n<p>I have tried <code>scipy.sparse.lil_matrix<\/code> and get same error as above.<\/p>\n<p>Is there any efficient way of handling this?\nThanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1597908911857,
        "Question_score":2,
        "Question_tags":"python-3.x|pandas|scipy|sparse-matrix|amazon-sagemaker",
        "Question_view_count":97,
        "Owner_creation_time":1487135761367,
        "Owner_last_access_time":1663945128510,
        "Owner_location":null,
        "Owner_reputation":911,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":91,
        "Question_last_edit_time":1597911320856,
        "Answer_body":"<p>Try setting to <code>True<\/code> the <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html\" rel=\"nofollow noreferrer\"><code>sparse<\/code> parameter<\/a>:<\/p>\n<blockquote>\n<p>sparsebool, default False\nWhether the dummy-encoded columns should be backed by a SparseArray (True) or a regular NumPy array (False).<\/p>\n<\/blockquote>\n<pre><code>sparse_onehot = pd.get_dummies(df[sp_features], columns = sp_features, sparse = True)\n<\/code><\/pre>\n<p>This will use a much more memory efficient (but somewhat slower) representation than the default one.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1597911539563,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63500377",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51110274,
        "Question_title":"Can I use AWS Sagemaker without S3",
        "Question_body":"<p>If I am not using the notebook on AWS but instead just the Sagemaker CLI and want to train a model, can I specify a local path to read from and write to?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1530312595250,
        "Question_score":1,
        "Question_tags":"amazon-s3|amazon-sagemaker",
        "Question_view_count":826,
        "Owner_creation_time":1528500562963,
        "Owner_last_access_time":1635459100793,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If you use local mode with the SageMaker Python SDK, you can train using local data:<\/p>\n\n<pre><code>from sagemaker.mxnet import MXNet\n\nmxnet_estimator = MXNet('train.py',\n                        train_instance_type='local',\n                        train_instance_count=1)\n\nmxnet_estimator.fit('file:\/\/\/tmp\/my_training_data')\n<\/code><\/pre>\n\n<p>However, this only works if you are training a model locally, not on SageMaker. If you want to train on SageMaker, then yes, you do need to use S3.<\/p>\n\n<p>For more about local mode: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#local-mode\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#local-mode<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1530570724340,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51110274",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67186515,
        "Question_title":"Installing modules inside python .py file",
        "Question_body":"<p>I am deploying a custom pytorch model on AWS sagemaker, Following <a href=\"https:\/\/github.com\/abiodunjames\/MachineLearning\/blob\/master\/DeployYourModelToSageMaker\/inference.py\" rel=\"nofollow noreferrer\">this<\/a> tutorial.\nIn my case I have few dependencies to install some modules.<\/p>\n<p>I need pycocotools in my inference.py script. I can easily install pycocotool inside a separate notebook using this bash command,<\/p>\n<p><code>%%bash<\/code><\/p>\n<p><code>pip -g install pycocotools<\/code><\/p>\n<p>But when I create my endpoint for deployment, I get this error that pycocotools in not defined.\nI need pycocotools inside my inference.py script. How I can install this inside a .py file<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1618953881933,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":167,
        "Owner_creation_time":1567880532003,
        "Owner_last_access_time":1661706476487,
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":137,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":100,
        "Question_last_edit_time":1618961452447,
        "Answer_body":"<p>At the beginning of inference.py add these lines:<\/p>\n<pre><code>from subprocess import check_call, run, CalledProcessError\nimport sys\nimport os\n\n# Since it is likely that you're going to run inference.py multiple times, this avoids reinstalling the same package:\nif not os.environ.get(&quot;INSTALL_SUCCESS&quot;):\n    \n    try:\n        check_call(\n        [ sys.executable, &quot;pip&quot;, &quot;install&quot;, &quot;pycocotools&quot;,]\n        )\n    except CalledProcessError:\n        run(\n        [&quot;pip&quot;, &quot;install&quot;, &quot;pycocotools&quot;,]\n        )\n    os.environ[&quot;INSTALL_SUCCESS&quot;] = &quot;True&quot;\n<\/code><\/pre>",
        "Answer_comment_count":2,
        "Answer_creation_time":1618965088403,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1619882971870,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67186515",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54224934,
        "Question_title":"SageMaker TensorFlow serving stack comparisons",
        "Question_body":"<p>SageMaker seems to give examples of using two different serving stacks for serving custom docker images:<\/p>\n\n<ol>\n<li>NGINX + Gunicorn + Flask<\/li>\n<li>NGINX + TensorFlow Serving<\/li>\n<\/ol>\n\n<p>Could someone explain to me at a very high level (I have very little knowledge of network engineering) what responsibilities these different components have? And since the second stack has only two components instead of one, can I rightly assume that TensorFlow Serving does the job (whatever that may be) of both Gunicorn and Flask? <\/p>\n\n<p>Lastly, I've read that it's possible to use Flask and TensorFlow serving at the same time. Would this then be NGINX -> Gunicorn -> Flask -> TensorFlow Serving? And what are there advantages of this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1547670814743,
        "Question_score":2,
        "Question_tags":"nginx|networking|tensorflow-serving|amazon-sagemaker|serving",
        "Question_view_count":741,
        "Owner_creation_time":1361339272693,
        "Owner_last_access_time":1663965928400,
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Question_last_edit_time":1547802223467,
        "Answer_body":"<p>I'll try to answer your question on a high level. Disclaimer: I'm not at an expert across the full stack of what you describe, and I would welcome corrections or additions from people who are. <\/p>\n\n<p>I'll go over the different components from bottom to top:<\/p>\n\n<p><strong>TensorFlow Serving<\/strong> is a library for deploying and hosting TensorFlow models as model servers that accept requests with input data and return model predictions. The idea is to train models with TensorFlow, export them to the SavedModel format and serve them with TF Serving. You can set up a TF Server to accept requests via HTTP and\/or RPC. One advantage of RPC is that the request message is compressed, which can be useful when sending large payloads, for instance with image data.<\/p>\n\n<p><strong>Flask<\/strong> is a python framework for writing web applications. It's much more general-purpose than TF Serving and is widely used to build web services, for instance in microservice architectures. <\/p>\n\n<p>Now, the combination of Flask and TensorFlow serving should make sense. You could write a Flask web application that exposes an API to the user and calls a TF model hosted with TF Serving under the hood. The user uses the API to transmit some data (<strong>1<\/strong>), the Flask app perhaps transform the data (for example, wrap it in numpy arrays), calls the TF Server to get a model prediction (<strong>2<\/strong>)(<strong>3<\/strong>), perhaps transforms the prediction (for example convert a predicted probability that is larger than 0.5 to a class label of 1), and returns the prediction to the user (<strong>4<\/strong>). You could visualize this as follows:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/67EXW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/67EXW.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Gunicorn<\/strong> is a Web Server Gateway Interface (WSGI) that is commonly used to host Flask applications in production systems. As the name says, it's the interface between a web server and a web application. When you are developing a Flask app, you can run it locally to test it. In production, gunicorn will run the app for you.<\/p>\n\n<p>TF Serving will host your model as a functional application. Therefore, you do not need gunicorn to run the TF Server application for you. <\/p>\n\n<p><strong>Nginx<\/strong> is the actual web server, which will host your application, handle requests from the outside and pass them to the application server (gunicorn). Nginx cannot talk directly to Flask applications, which is why gunicorn is there. <\/p>\n\n<p><a href=\"https:\/\/serverfault.com\/questions\/331256\/why-do-i-need-nginx-and-something-like-gunicorn\">This answer<\/a> might be helpful as well. <\/p>\n\n<p>Finally, if you are working on a cloud platform, the web server part will probably be handled for you, so you will either need to write the Flask app and host it with gunicorn, or setup the TF Serving server. <\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1547801639500,
        "Answer_score":2,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":1547816279183,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54224934",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55645119,
        "Question_title":"How to remotely connect to GCP ML Engine\/AWS Sagemaker managed notebooks?",
        "Question_body":"<p>GCP has finally released managed Jupyter notebooks.  I would like to be able to interact with the notebook locally by connecting to it.  Ie. i use PyCharm to connect to the externaly configured jupyter notebbok server by passing its URL &amp; token param.<\/p>\n\n<p>Question also applies to AWS Sagemaker notebooks.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1555047546513,
        "Question_score":8,
        "Question_tags":"amazon-web-services|google-cloud-platform|google-cloud-ml|amazon-sagemaker|gcp-ai-platform-notebook",
        "Question_view_count":3701,
        "Owner_creation_time":1298837928100,
        "Owner_last_access_time":1663978463180,
        "Owner_location":"Los Angeles, CA",
        "Owner_reputation":1367,
        "Owner_up_votes":655,
        "Owner_down_votes":16,
        "Owner_views":243,
        "Question_last_edit_time":1561940059343,
        "Answer_body":"<p>On AWS, you can use AWS Glue to create a <a href=\"https:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/dev-endpoint.html\" rel=\"nofollow noreferrer\">developer endpoint<\/a>, and then you create the Sagemaker notebook from there. A developer endpoint gives you access to connect to your python or Scala spark REPL via ssh, and it also allows you to tunnel the connection and access from any other tool, including PyCharm.<\/p>\n\n<p>For PyCharm professional we have even <a href=\"https:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/dev-endpoint-tutorial-pycharm.html\" rel=\"nofollow noreferrer\">tighter integration<\/a>, allowing you to SFTP files and debug remotely.<\/p>\n\n<p>And if you need to install any dependencies on the notebook, apart from doing it directly on the notebook, you can always choose <code>new&gt;terminal<\/code> and you will have a connection to that machine directly from your jupyter environment where you can install <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\" rel=\"nofollow noreferrer\">anything you want<\/a>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1555057761750,
        "Answer_score":3,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55645119",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73133746,
        "Question_title":"Fb-Prophet, Apache Spark in Colab and AWS SageMaker\/ Lambda",
        "Question_body":"<p>I am using <code>Google-Colab<\/code> for creating a model by using FbProphet and i am try to use Apache Spark in the <code>Google-Colab<\/code> itself. Now can i upload this <code>Google-colab<\/code> notebook in <code>aws Sagemaker\/Lambda<\/code> for free <code>(without charge for Apache Spark and only charge for AWS SageMaker)<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658906440657,
        "Question_score":1,
        "Question_tags":"apache-spark|google-colaboratory|amazon-sagemaker|facebook-prophet",
        "Question_view_count":51,
        "Owner_creation_time":1658906023853,
        "Owner_last_access_time":1663921457750,
        "Owner_location":null,
        "Owner_reputation":152,
        "Owner_up_votes":15,
        "Owner_down_votes":1,
        "Owner_views":11,
        "Question_last_edit_time":1660220920907,
        "Answer_body":"<p>In short, You can upload the notebook without any issue into SageMaker. Few things to keep in mind<\/p>\n<ol>\n<li>If you are using the pyspark library in colab and running spark locally,  you should be able to do the same by installing necessary pyspark libs in Sagemaker studio kernels. Here you will only pay for the underlying compute for the notebook instance. If you are experimenting then I would recommend you to use <a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a> to create a free account and try things out.<\/li>\n<li>If you had a separate spark cluster setup then you may need a similar setup in AWS using EMR so that you can connect to the cluster to execute the job.<\/li>\n<\/ol>",
        "Answer_comment_count":0,
        "Answer_creation_time":1658964743500,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73133746",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56046428,
        "Question_title":"What is \"[0]#011train-merror:0.17074#011validation-merror:0.1664\" error when running xgb_model.fit() in AWS Sagemaker?",
        "Question_body":"<p>I'm running through the official sagemaker tutorial <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>And although training completes, I'm getting errors like below periodically during training, <code>xgb_model.fit(inputs=data_channels,  logs=True)<\/code>.<\/p>\n\n<p>I have no experience with xgboost or sagemaker at this point.<\/p>\n\n<pre><code>[0]#011train-merror:0.17074#011validation-merror:0.1664\n<\/code><\/pre>\n\n<p><strong>Full logs:<\/strong><\/p>\n\n<pre><code>2019-05-08 17:04:32 Starting - Starting the training job...\n2019-05-08 17:04:33 Starting - Launching requested ML instances.........\n2019-05-08 17:06:10 Starting - Preparing the instances for training......\n2019-05-08 17:07:06 Downloading - Downloading input data...\n2019-05-08 17:07:50 Training - Training image download completed. Training in progress.\nArguments: train\n[2019-05-08:17:07:51:INFO] Running standalone xgboost training.\n[2019-05-08:17:07:51:INFO] File size need to be processed in the node: 1122.95mb. Available memory size in the node: 152390.7mb\n[2019-05-08:17:07:51:INFO] Determined delimiter of CSV input is ','\n[17:07:51] S3DistributionType set as FullyReplicated\n[17:07:55] 50000x784 matrix with 39200000 entries loaded from \/opt\/ml\/input\/data\/train?format=csv&amp;label_column=0&amp;delimiter=,\n[2019-05-08:17:07:55:INFO] Determined delimiter of CSV input is ','\n[17:07:55] S3DistributionType set as FullyReplicated\n[17:07:56] 10000x784 matrix with 7840000 entries loaded from \/opt\/ml\/input\/data\/validation?format=csv&amp;label_column=0&amp;delimiter=,\n[17:07:56] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 8 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 8 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n[0]#011train-merror:0.17074#011validation-merror:0.1664\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 14 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 8 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 12 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 12 pruned nodes, max_depth=5\n[1]#011train-merror:0.12624#011validation-merror:0.1273\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 10 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 20 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 2 pruned nodes, max_depth=5\n[2]#011train-merror:0.11272#011validation-merror:0.1143\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 8 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 20 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 8 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 12 pruned nodes, max_depth=5\n[3]#011train-merror:0.10072#011validation-merror:0.1052\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 12 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 22 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 4 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 8 pruned nodes, max_depth=5\n[4]#011train-merror:0.09216#011validation-merror:0.097\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 12 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 22 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 4 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 10 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 10 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 14 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 8 pruned nodes, max_depth=5\n[5]#011train-merror:0.08544#011validation-merror:0.0904\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 12 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 14 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 8 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 2 pruned nodes, max_depth=5\n\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 4 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 16 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 12 pruned nodes, max_depth=5\n[6]#011train-merror:0.08064#011validation-merror:0.0864\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 8 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 14 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 4 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 8 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 14 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 4 pruned nodes, max_depth=5\n[7]#011train-merror:0.0769#011validation-merror:0.0821\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 16 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 20 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 4 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 10 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 4 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 8 pruned nodes, max_depth=5\n[8]#011train-merror:0.0731#011validation-merror:0.0809\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 12 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 24 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 10 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 8 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 8 pruned nodes, max_depth=5\n[17:08:04] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:04] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 14 pruned nodes, max_depth=5\n[17:08:04] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:04] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 6 pruned nodes, max_depth=5\n[9]#011train-merror:0.06942#011validation-merror:0.0773\n\n2019-05-08 17:08:12 Uploading - Uploading generated training model\n2019-05-08 17:08:12 Completed - Training job completed\nBillable seconds: 66\n\n<\/code><\/pre>\n\n<p>Is this something I should be concerned about?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1557337671520,
        "Question_score":0,
        "Question_tags":"python|xgboost|amazon-sagemaker",
        "Question_view_count":302,
        "Owner_creation_time":1431872955547,
        "Owner_last_access_time":1663095099033,
        "Owner_location":"Unknown",
        "Owner_reputation":9923,
        "Owner_up_votes":288,
        "Owner_down_votes":0,
        "Owner_views":530,
        "Question_last_edit_time":null,
        "Answer_body":"<p>No need to worry. These are not errors in your code. These are info messages that are calculating the error of the model on the training data (train-error) and on the validation data (validation-error), and these values should get smaller as the training progress. <\/p>\n\n<p>In time, these values will be more meaningful for you. You will be able to compare different algorithms and hyper-parameters based on which is the smaller error, or you will be able to see that your model is overfitting, when the error values of the training is very different from the validation error. <\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1557581566207,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56046428",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56666667,
        "Question_title":"Clienterror: An error occured when calling the CreateModel operation",
        "Question_body":"<p>I want to deploy sklearn model in sagemaker. I created a training script.<\/p>\n\n<p>scripPath=' sklearn.py'<\/p>\n\n<p><code>sklearn=SKLearn(entry_point=scripPath,\n                                 train_instance_type='ml.m5.xlarge',\n                                   role=role,                  output_path='s3:\/\/{}\/{}\/output'.format(bucket,prefix), sagemaker_session=session)\nsklearn.fit({\"train-dir' : train_input})<\/code><\/p>\n\n<p>When I deploy it\n<code>predictor=sklearn.deploy(initial_count=1,instance_type='ml.m5.xlarge')<\/code><\/p>\n\n<p>It throws,\n<code>Clienterror: An error occured when calling the CreateModel operation:Could not find model data at s3:\/\/tree\/sklearn\/output\/model.tar.gz<\/code><\/p>\n\n<p>Can anyone say how to solve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1560943786943,
        "Question_score":0,
        "Question_tags":"aws-sdk|amazon-sagemaker",
        "Question_view_count":1033,
        "Owner_creation_time":1560085651597,
        "Owner_last_access_time":1561731351963,
        "Owner_location":null,
        "Owner_reputation":155,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When deploying models, SageMaker looks up S3 to find your trained model artifact. It seems that there is no trained model artifact at <code>s3:\/\/tree\/sklearn\/output\/model.tar.gz<\/code>. Make sure to persist your model artifact in your training script at the appropriate local location in docker which is <code>\/opt\/ml\/model<\/code>.\nfor example, in your training script this could look like:<\/p>\n\n<pre><code>joblib.dump(model, \/opt\/ml\/model\/mymodel.joblib)\n<\/code><\/pre>\n\n<p>After training, SageMaker will copy the content of <code>\/opt\/ml\/model<\/code> to s3 at the <code>output_path<\/code> location.<\/p>\n\n<p>If you deploy in the same session a <code>model.deploy()<\/code> will map automatically to the artifact path. If you want to deploy a model that you trained elsewhere, possibly during a different session or in a different hardware, you need to explicitly instantiate a model before deploying<\/p>\n\n<pre><code>from sagemaker.sklearn.model import SKLearnModel\n\nmodel = SKLearnModel(\n    model_data='s3:\/\/...model.tar.gz',  # your artifact\n    role=get_execution_role(),\n    entry_point='script.py')  # script containing inference functions\n\nmodel.deploy(\n    instance_type='ml.m5.xlarge',\n    initial_instance_count=1,\n    endpoint_name='your_endpoint_name')\n<\/code><\/pre>\n\n<p>See more about Sklearn in SageMaker here <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html<\/a><\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1560972233450,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56666667",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61191412,
        "Question_title":"Unable to install toc2 notebook extension for AWS Sagemaker Instance (Lifecycle Configurations)",
        "Question_body":"<p>There's probably something very obvious I'm missing or Sagemaker just doesn't support these kinds of extensions, but I've been trying to enable toc2 (Table of Contents) jupyter extension for my Sagemaker notebook via lifecycle configurations, but for whatever reason it still isn't showing up.<\/p>\n\n<p>I built my script out combining a sample AWS script and a quick article on the usual ways of enabling extensions:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-nb-extension\/on-start.sh\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-nb-extension\/on-start.sh<\/a><\/p>\n\n<p><a href=\"https:\/\/towardsdatascience.com\/jupyter-notebook-extensions-517fa69d2231\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/jupyter-notebook-extensions-517fa69d2231<\/a><\/p>\n\n<pre><code>#!\/bin\/bash\n\nset -e\nsudo -u ec2-user -i &lt;&lt;EOF\n\n--Activate notebook environment\nsource activate JupyterSystemEnv\n\n--Install extensions\npip install jupyter_contrib_nbextensions &amp;&amp; jupyter contrib\nnbextension install\njupyter nbextension enable toc2 --py --sys-prefix\n\nsource deactivate\n\n\nEOF\n<\/code><\/pre>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_time":1586792396013,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|jupyter-notebook|amazon-sagemaker|tableofcontents",
        "Question_view_count":1027,
        "Owner_creation_time":1456986606313,
        "Owner_last_access_time":1663952857883,
        "Owner_location":null,
        "Owner_reputation":757,
        "Owner_up_votes":83,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Answering my question, looks like I was just missing the line <code>jupyter contrib nbextension install --user<\/code> to copy the JS\/CSS files into Jupyter's search directory and some config updates (<a href=\"https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions\" rel=\"nofollow noreferrer\">https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions<\/a>).<\/p>\n<p>Corrected statement<\/p>\n<pre><code>#!\/bin\/bash\n\nset -e\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\nsource \/home\/ec2-user\/anaconda3\/bin\/activate JupyterSystemEnv\n\npip install jupyter_contrib_nbextensions\njupyter contrib nbextension install --user\njupyter nbextension enable toc2\/main\n\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\n\n\nEOF\n\n##Below may be unnecessary, but other user needed to run to see success\ninitctl restart jupyter-server --no-wait\n<\/code><\/pre>",
        "Answer_comment_count":2,
        "Answer_creation_time":1610748471693,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1615352840849,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61191412",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60405600,
        "Question_title":"How to make inference on local PC with the model trained on AWS SageMaker by using the built-in algorithm Semantic Segmentation?",
        "Question_body":"<p>Similar to the issue of <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/200\" rel=\"nofollow noreferrer\">The trained model can be deployed on the other platform without dependency of sagemaker or aws service?<\/a>.<\/p>\n\n<p>I have trained a model on AWS SageMaker by using the built-in algorithm <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/semantic-segmentation.html\" rel=\"nofollow noreferrer\">Semantic Segmentation<\/a>. This trained model named as <code>model.tar.gz<\/code> is stored on S3. So I want to download this file from S3 and then use it to make inference on my local PC without using AWS SageMaker anymore. Since the built-in algorithm Semantic Segmentation is built using the <a href=\"https:\/\/github.com\/dmlc\/gluon-cv\" rel=\"nofollow noreferrer\">MXNet Gluon framework and the Gluon CV toolkit<\/a>, so I try to refer the documentation of <a href=\"https:\/\/mxnet.apache.org\/\" rel=\"nofollow noreferrer\">mxnet<\/a> and <a href=\"https:\/\/gluon-cv.mxnet.io\/\" rel=\"nofollow noreferrer\">gluon-cv<\/a> to make inference on local PC.<\/p>\n\n<p>It's easy to download this file from S3, and then I unzip this file to get three files:<\/p>\n\n<ol>\n<li><strong>hyperparams.json<\/strong>: includes the parameters for network architecture, data inputs, and training. Refer to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/segmentation-hyperparameters.html\" rel=\"nofollow noreferrer\">Semantic Segmentation Hyperparameters<\/a>.<\/li>\n<li><strong>model_algo-1<\/strong><\/li>\n<li><strong>model_best.params<\/strong><\/li>\n<\/ol>\n\n<p>Both <strong>model_algo-1<\/strong> and <strong>model_best.params<\/strong> are the trained models, and I think it's the output from <code>net.save_parameters<\/code> (Refer to <a href=\"https:\/\/beta.mxnet.io\/guide\/getting-started\/crash-course\/4-train.html\" rel=\"nofollow noreferrer\">Train the neural network<\/a>). I can also load them with the function <code>mxnet.ndarray.load<\/code>.<\/p>\n\n<p>Refer to <a href=\"https:\/\/beta.mxnet.io\/guide\/getting-started\/crash-course\/5-predict.html\" rel=\"nofollow noreferrer\">Predict with a pre-trained model<\/a>. I found there are two necessary things:<\/p>\n\n<ol>\n<li>Reconstruct the network for making inference.<\/li>\n<li>Load the trained parameters.<\/li>\n<\/ol>\n\n<p>As for reconstructing the network for making inference, since I have used PSPNet from training, so I can use the class <code>gluoncv.model_zoo.PSPNet<\/code> to reconstruct the network. And I know how to use some services of AWS SageMaker, for example batch transform jobs, to make inference. I want to reproduce it on my local PC. If I use the class <code>gluoncv.model_zoo.PSPNet<\/code> to reconstruct the network, I can't make sure whether the parameters for this network are same those used on AWS SageMaker while making inference. Because I can't see the image <code>501404015308.dkr.ecr.ap-northeast-1.amazonaws.com\/semantic-segmentation:latest<\/code> in detail. <\/p>\n\n<p>As for loading the trained parameters, I can use the <code>load_parameters<\/code>. But as for <strong>model_algo-1<\/strong> and <strong>model_best.params<\/strong>, I don't know which one I should use.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1582681249783,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|mxnet|gluon",
        "Question_view_count":351,
        "Owner_creation_time":1472967821507,
        "Owner_last_access_time":1663314033960,
        "Owner_location":"Tokyo, Japan",
        "Owner_reputation":61,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1582681944470,
        "Answer_body":"<p>The following code works well for me.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import mxnet as mx\nfrom mxnet import image\nfrom gluoncv.data.transforms.presets.segmentation import test_transform\nimport gluoncv\n\n# use cpu\nctx = mx.cpu(0)\n\n# load test image\nimg = image.imread('.\/img\/IMG_4015.jpg')\nimg = test_transform(img, ctx)\nimg = img.astype('float32')\n\n# reconstruct the PSP network model\nmodel = gluoncv.model_zoo.PSPNet(2)\n\n# load the trained model\nmodel.load_parameters('.\/model\/model_algo-1')\n\n# make inference\noutput = model.predict(img)\npredict = mx.nd.squeeze(mx.nd.argmax(output, 1)).asnumpy()\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1583126137777,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60405600",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65719292,
        "Question_title":"How to run tensorboard for tensorflow in AWS Sagemaker?",
        "Question_body":"<p>I need to visualize real-time losses and metrics for a tensorflow model on AWS Sagemaker instance.\nIn a Jupyter notebook, I tried running<\/p>\n<pre><code>%load_ext tensorboard\n%tensorboard --logdir &lt;path&gt;\n<\/code><\/pre>\n<p>But nothing really happened. How can I get this working?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1610628627223,
        "Question_score":1,
        "Question_tags":"amazon-web-services|tensorflow|tensorflow2.0|tensorboard|amazon-sagemaker",
        "Question_view_count":715,
        "Owner_creation_time":1512023194593,
        "Owner_last_access_time":1663919015323,
        "Owner_location":null,
        "Owner_reputation":547,
        "Owner_up_votes":71,
        "Owner_down_votes":0,
        "Owner_views":61,
        "Question_last_edit_time":1610628935100,
        "Answer_body":"<p>You need to use the conda_pytorch_36 kernel (this is the one I used) and tensorboard is not installed by default so you need to run<\/p>\n<pre><code>!pip install tensorboard\n<\/code><\/pre>\n<p>Then you will get a blank screen when you run.<\/p>\n<pre><code>%load_ext tensorboard\n%tensorboard --logdir &quot;.\/runs&quot;\n<\/code><\/pre>\n<p>You can connect to tensorboard using your URL with notebook or lab replaced with proxy\/6006<\/p>\n<pre><code>https:\/\/YOUR_NOTEBOOK_INSTANCE_NAME.notebook.ap-northeast-1.sagemaker.aws\/proxy\/6006\/\n<\/code><\/pre>",
        "Answer_comment_count":3,
        "Answer_creation_time":1610631020420,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65719292",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72097417,
        "Question_title":"Segfault using htop on AWS Sagemaker pytorch-1.10-cpu-py38 app",
        "Question_body":"<p>I am trying to launch the htop command in the Pytorch 1.10 - Python 3.8 CPU optimized AWS Sagemaker container. This works fine in other images I have used till now, but in this one, the command fails with a segfault:<\/p>\n<pre><code>htop \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nSegmentation fault (core dumped)\n<\/code><\/pre>\n<p>More info :<\/p>\n<pre><code>htop --version\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop 2.2.0 - (C) 2004-2019 Hisham Muhammad\nReleased under the GNU GPL.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651570402183,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|conda|amazon-sagemaker|htop",
        "Question_view_count":147,
        "Owner_creation_time":1551431163127,
        "Owner_last_access_time":1660223920803,
        "Owner_location":"Paris, France",
        "Owner_reputation":494,
        "Owner_up_votes":133,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I fixed this with<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code># Note: add sudo if needed:\nln -fs \/lib\/x86_64-linux-gnu\/libncursesw.so.6 \/opt\/conda\/lib\/libncursesw.so.6\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1655459795693,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72097417",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49582307,
        "Question_title":"How to schedule tasks on SageMaker",
        "Question_body":"<p>I have a notebook on SageMaker I would like to run every night. What's the best way to schedule this task. Is there a way to run a bash script and schedule Cron job from SageMaker?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1522449441927,
        "Question_score":14,
        "Question_tags":"amazon-web-services|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":17339,
        "Owner_creation_time":1420001102893,
        "Owner_last_access_time":1638226427930,
        "Owner_location":null,
        "Owner_reputation":173,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Amazon SageMaker is a set of API that can help various machine learning and data science tasks. These API can be invoked from various sources, such as CLI, <a href=\"https:\/\/aws.amazon.com\/tools\/\" rel=\"noreferrer\">SDK<\/a> or specifically from schedule AWS Lambda functions (see here for documentation: <a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-scheduled-events.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-scheduled-events.html<\/a> )<\/p>\n\n<p>The main parts of Amazon SageMaker are notebook instances, training and tuning jobs, and model hosting for real-time predictions. Each one has different types of schedules that you might want to have. The most popular are:<\/p>\n\n<ul>\n<li><strong>Stopping and Starting Notebook Instances<\/strong> - Since the notebook instances are used for interactive ML models development, you don't really need them running during the nights or weekends. You can schedule a Lambda function to call the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StopNotebookInstance.html\" rel=\"noreferrer\">stop-notebook-instance<\/a> API at the end of the working day (8PM, for example), and the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StartNotebookInstance.html\" rel=\"noreferrer\">start-notebook-instance<\/a> API in the morning. Please note that you can also run crontab on the notebook instances (after opening the local terminal from the Jupyter interface).<\/li>\n<li><strong>Refreshing an ML Model<\/strong> - Automating the re-training of models, on new data that is flowing into the system all the time, is a common issue that with SageMaker is easier to solve. Calling <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html\" rel=\"noreferrer\">create-training-job<\/a> API from a scheduled Lambda function (or even from a <a href=\"https:\/\/docs.aws.amazon.com\/AmazonCloudWatch\/latest\/events\/WhatIsCloudWatchEvents.html\" rel=\"noreferrer\">CloudWatch Event<\/a> that is monitoring the performance of the existing models), pointing to the S3 bucket where the old and new data resides, can <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateModel.html\" rel=\"noreferrer\">create a refreshed model<\/a> that you can now deploy into an <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_UpdateEndpointWeightsAndCapacities.html\" rel=\"noreferrer\">A\/B testing environment<\/a> .<\/li>\n<\/ul>\n\n<p>----- UPDATE (thanks to @snat2100 comment) -----<\/p>\n\n<ul>\n<li><strong>Creating and Deleting Real-time Endpoints<\/strong> - If your realtime endpoints are not needed 24\/7 (for example, serving internal company users working during workdays and hours), you can also <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpoint.html\" rel=\"noreferrer\">create the endpoints<\/a> in the morning and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_DeleteEndpoint.html\" rel=\"noreferrer\">delete them<\/a> at night. <\/li>\n<\/ul>",
        "Answer_comment_count":2,
        "Answer_creation_time":1523213115337,
        "Answer_score":19,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":1570200701000,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49582307",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50983316,
        "Question_title":"Continuous Training in Sagemaker",
        "Question_body":"<p>I am trying out <strong>Amazon Sagemaker<\/strong>, I haven't figured out how we can have Continuous training.\n<br>\nFor example if i have a CSV file in s3 and I want to train each time the CSV file is updated.<\/p>\n\n<p>I know we can go again to the notebook and re-run the whole notebook to make this happen.\n<br>\nBut i am looking for an automated way, with some python scripts or using a lambda function with s3 events etc<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1529654311630,
        "Question_score":4,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1191,
        "Owner_creation_time":1440734188430,
        "Owner_last_access_time":1655569862827,
        "Owner_location":"India",
        "Owner_reputation":1491,
        "Owner_up_votes":198,
        "Owner_down_votes":32,
        "Owner_views":112,
        "Question_last_edit_time":1529654668140,
        "Answer_body":"<p>You can use boto3 sdk for python to start training on lambda then you need to trigger the lambda when csv is update.<\/p>\n\n<blockquote>\n  <p><a href=\"http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/sagemaker.html\" rel=\"nofollow noreferrer\">http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/sagemaker.html<\/a><\/p>\n<\/blockquote>\n\n<p>Example python code<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a><\/p>\n<\/blockquote>\n\n<p>Addition: You dont need to use lambda you just start\/cronjob the python script any kind of instance which has python and aws sdk in it.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1530199631933,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50983316",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54462105,
        "Question_title":"SageMaker Ground Truth with TensorFlow",
        "Question_body":"<p>I've seen examples of labeling data using SageMaker Ground Truth and then using that data to train off-the-shelf SageMaker models. However, am I able to use this same annotation format with TensorFlow Script Mode? <\/p>\n\n<p>More specifically, I have a tensorflow.keras model I'm training using TF Script Mode, and I'd like to take data labeled with Ground Truth and convert my script from File mode to Pipe mode.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548942784467,
        "Question_score":2,
        "Question_tags":"python|tensorflow|amazon-sagemaker|labeling",
        "Question_view_count":454,
        "Owner_creation_time":1361339272693,
        "Owner_last_access_time":1663965928400,
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I am from Amazon SageMaker Ground Truth team and happy to assist you in your experiment. Just to be clear our understanding, are you running TF model in SageMaker using TF estimator in your own container (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst<\/a>)? <\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1549915976767,
        "Answer_score":3,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54462105",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60953289,
        "Question_title":"SageMaker gives CannotStartContainerError although I specified an entrypoint",
        "Question_body":"<p>I want to train a custom ML model with SageMaker. The model is written in Python and should be shipped to SageMaker in a Docker image. Here is a simplified version of my Dockerfile (the model sits in the train.py file):<\/p>\n\n<pre><code>FROM amazonlinux:latest\n\n# Install Python 3\nRUN yum -y update &amp;&amp; yum install -y python3-pip python3-devel gcc &amp;&amp; yum clean all\n\n# Install sagemaker-containers (the official SageMaker utils package)\nRUN pip3 install --target=\/usr\/local\/lib\/python3.7\/site-packages sagemaker-containers &amp;&amp; rm -rf \/root\/.cache\n\n# Bring the script with the model to the image \nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n\n<p>Now, if I initialize this image as a SageMaker estimator and then run the <code>fit<\/code> method on this estimator I get the following error:<\/p>\n\n<p>\"AlgorithmError: CannotStartContainerError. Please make sure the container can be run with 'docker run  train'.\"<\/p>\n\n<p>In other words: SageMaker is not able to get into the container and run the train.py file. But why? The way I am specifying the entrypoint with <code>ENV SAGEMAKER_PROGRAM train.py<\/code> is recommended in the <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\/blob\/master\/README.rst\" rel=\"nofollow noreferrer\">docs of the sagemaker-containers package<\/a> (see 'How a script is executed inside the container').<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1585665709967,
        "Question_score":1,
        "Question_tags":"python|image|docker|amazon-sagemaker|amazon-linux-2",
        "Question_view_count":1133,
        "Owner_creation_time":1448655975827,
        "Owner_last_access_time":1663931632060,
        "Owner_location":null,
        "Owner_reputation":1478,
        "Owner_up_votes":653,
        "Owner_down_votes":1,
        "Owner_views":135,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I found a hint in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">the AWS docs<\/a> and came up with this solution:<\/p>\n\n<pre><code>ENTRYPOINT [\"python3.7\", \"\/opt\/ml\/code\/train.py\"]\n<\/code><\/pre>\n\n<p>With this the container <a href=\"https:\/\/docs.docker.com\/engine\/reference\/builder\/#entrypoint\" rel=\"nofollow noreferrer\">will run as an executable<\/a>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1585671760393,
        "Answer_score":1,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60953289",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61059996,
        "Question_title":"Sagemaker Processing doesn't upload",
        "Question_body":"<p>I'm trying to use the sagemaker processor to replace some processes we run on Amazon batch.<\/p>\n<pre><code>from sagemaker.processor import ScriptProcessor \nproc = ScriptProcessor(\n    image_uri='your-image-uri', \n    command=['python3'], \n    role=role, \n    instance_count=1, \n    instance_type='m4.4x.large',  \n    volume_size_in_gb=500,\n    base_job_name='preprocessing-test',\n)\nproc.run(\n    code='test.py',\n)\n<\/code><\/pre>\n<p>First of all, is it true that the <code>ScriptProcessing<\/code> syntax is more complicated than the <code>TrainingJob<\/code> version where you can specify the <code>source_dir<\/code> and <code>entrypoint<\/code> to upload your code to a default container?<\/p>\n<p>Secondly, this code above gives me this error<\/p>\n<pre><code>ParamValidationError: Parameter validation failed:\nInvalid bucket name &quot;sagemaker-eu-west-1-&lt;account-id&gt;\\preprocessing-test-&lt;timestamp&gt;\\input\\code&quot;: Bucket name must match the regex &quot;^[a-zA-Z0-9.\\-_]{1,255}$&quot; or be an ARN matching the regex &quot;^arn:(aws).*:s3:[a-z\\-0-9]+:[0-9]{12}:accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$&quot;\n<\/code><\/pre>\n<p>I guess this key is created internally when trying to upload my <code>test.py<\/code>, but why does it not work? :) The documentation says you can use both local and s3 paths.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1586176511533,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":314,
        "Owner_creation_time":1484838464573,
        "Owner_last_access_time":1663858511743,
        "Owner_location":"Amsterdam, Nederland",
        "Owner_reputation":3937,
        "Owner_up_votes":672,
        "Owner_down_votes":27,
        "Owner_views":387,
        "Question_last_edit_time":1616657799110,
        "Answer_body":"<p>The bucket name `sagemaker-eu-west-1-\\preprocessing-test-\\input\\code looks like a hardcoded string. In SageMaker Python SDK, the code upload function is <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/3bf569ece9e46a097d1ab69286ee89f762931e6c\/src\/sagemaker\/processing.py#L463\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<p>Are you using a Windows environment? As Lauren noted in the comments, there have been some bug fixes there, so make sure to use the last version<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1587103591427,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1616657708820,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61059996",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57164290,
        "Question_title":"Training with TensorFlow on Sagemaker No module named 'tf_container'",
        "Question_body":"<p>I'm trying to training TensorFlow model on AWS Sagemaker.\nI created container with external lib for that (Use Your Own Algorithms or Models with Amazon SageMaker).<\/p>\n\n<p>we run a training job with TensorFlow API<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tensorflow import TensorFlow\nestimator = TensorFlow(\n  entry_point=\"entry.py\",             # entry script\n  role=role,\n  framework_version=\"1.13.0\",   \n  py_version='py3',\n  hyperparameters=hyperparameters,\n  train_instance_count=1,                   # \"The number of GPUs instances to use\"\n  train_instance_type=train_instance_type,\n  image_name=my_image\n\n)\nestimator.fit({'train': train_s3, 'eval': eval_s3})\n<\/code><\/pre>\n\n<p>and got an error:<\/p>\n\n<pre><code>09:06:46\n2019-07-23 09:06:45,463 INFO - root - running container entrypoint \uf141\n09:06:46\n2019-07-23 09:06:45,463 INFO - root - starting train task \uf141\n09:06:46\n2019-07-23 09:06:45,476 INFO - container_support.training - Training starting \uf141\n09:06:46\n2019-07-23 09:06:45,479 ERROR - container_support.training - uncaught exception during training: No module named 'tf_container'\n\uf141\n09:06:46\nTraceback (most recent call last): File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/environment.py\", line 136, in load_framework return importlib.import_module('mxnet_container') File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File \"&lt;frozen importlib._bootstrap&gt;\", line 994, in _gcd_i \uf141\n09:06:46\nModuleNotFoundError: No module named 'mxnet_container'\n\uf141\n09:06:46\nDuring handling of the above exception, another exception occurred:\n\uf141\n09:06:46\nTraceback (most recent call last): File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/training.py\", line 35, in start fw = TrainingEnvironment.load_framework() File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/environment.py\", line 138, in load_framework return importlib.import_module('tf_container') File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, \uf141\n09:06:46\nModuleNotFoundError: No module named 'tf_container'\n<\/code><\/pre>\n\n<p>What can I do to solve this issue? how can I debug this case?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1563885123080,
        "Question_score":1,
        "Question_tags":"python|tensorflow|amazon-sagemaker",
        "Question_view_count":1191,
        "Owner_creation_time":1471952551663,
        "Owner_last_access_time":1662451190670,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I'm guessing that you used your own TF container, not the SageMaker one at <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a><\/p>\n\n<p>If that's the case, your container is missing the support code needed to use the TensorFlow estimator ('tf_container' package).<\/p>\n\n<p>The solution is to start from the SageMaker container, customize it, push it back to ECR and pass the image name to the SageMaker estimator with the 'image_name' parameter.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1563901268600,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57164290",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51533650,
        "Question_title":"No space left on device in Sagemaker model training",
        "Question_body":"<p>I'm using custom algorithm running shipped with Docker image on p2 instance with AWS Sagemaker (a bit similar to <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a>)<\/p>\n\n<p>At the end of training process, I try to write down my model to output directory, that is mounted via Sagemaker (like in tutorial), like this:<\/p>\n\n<pre><code>model_path = \"\/opt\/ml\/model\"\nmodel.save(os.path.join(model_path, 'model.h5'))\n<\/code><\/pre>\n\n<p>Unluckily, apparently the model gets too big with time and I get the\nfollowing error:<\/p>\n\n<blockquote>\n  <p>RuntimeError: Problems closing file (file write failed: time = Thu Jul\n  26 00:24:48 2018<\/p>\n  \n  <p>00:24:49 , filename = 'model.h5', file descriptor = 22, errno = 28,\n  error message = 'No space left on device', buf = 0x1a41d7d0, total\n  write[...]<\/p>\n<\/blockquote>\n\n<p>So all my hours of GPU time are wasted. How can I prevent this from happening again? Does anyone know what is the size limit for model that I store on Sagemaker\/mounted directories?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1532591463817,
        "Question_score":2,
        "Question_tags":"amazon-web-services|keras|amazon-sagemaker",
        "Question_view_count":2721,
        "Owner_creation_time":1366408339740,
        "Owner_last_access_time":1663750023060,
        "Owner_location":null,
        "Owner_reputation":8057,
        "Owner_up_votes":1123,
        "Owner_down_votes":72,
        "Owner_views":491,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When you train a model with <code>Estimators<\/code>, it <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/estimators.html\" rel=\"nofollow noreferrer\">defaults to 30 GB of storage<\/a>, which may not be enough. You can use the <code>train_volume_size<\/code> param on the constructor to increase this value. Try with a large-ish number (like 100GB) and see how big your model is. In subsequent jobs, you can tune down the value to something closer to what you actually need.<\/p>\n\n<p>Storage costs <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">$0.14 per GB-month of provisioned storage<\/a>. Partial usage is prorated, so giving yourself some extra room is a cheap insurance policy against running out of storage.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1539631654853,
        "Answer_score":1,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51533650",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58292566,
        "Question_title":"How can I use a list of files as the training set on Sagemaker with Tensorflow?",
        "Question_body":"<p>I have several million images in my training folder and want to specify a subset of them for training - the way to do this seems to be with a manifest file as described here.<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/augmented-manifest.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/augmented-manifest.html<\/a><\/p>\n\n<p>But this seems to be geared towards labelled data. How can I start a sagemaker training job using sagemaker's Tensorflow <code>estimator.fit<\/code> with a list of files instead of the entire directory as input?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1570561431850,
        "Question_score":1,
        "Question_tags":"python|tensorflow|computer-vision|amazon-sagemaker",
        "Question_view_count":809,
        "Owner_creation_time":1416193017423,
        "Owner_last_access_time":1663696463113,
        "Owner_location":"Gensokyo",
        "Owner_reputation":880,
        "Owner_up_votes":211,
        "Owner_down_votes":2,
        "Owner_views":111,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use an input type pipe parameter like so: <\/p>\n\n<pre><code>hyperparameters = {'save_checkpoints_secs':None,\n                   'save_checkpoints_steps':1000}\n\ntf_estimator = TensorFlow(entry_point='.\/my-training-file', role=role,\n                          training_steps=5100, evaluation_steps=100,\n                          train_instance_count=1, train_instance_type='ml.p3.2xlarge',\n                          input_mode = 'Pipe',\n                          train_volume_size=300, output_path = 's3:\/\/sagemaker-pocs\/test-carlsoa\/kepler\/model',\n                          framework_version = '1.12.0', hyperparameters=hyperparameters, checkpoint_path = None)\n<\/code><\/pre>\n\n<p>And create the manifest file pipe as an input:<\/p>\n\n<pre><code>train_data = sagemaker.session.s3_input('s3:\/\/sagemaker-pocs\/test-carlsoa\/manifest.json',\n                                        distribution='FullyReplicated',\n                                        content_type='image\/jpeg',\n                                        s3_data_type='ManifestFile',\n                                        attribute_names=['source-ref']) \n                                        #attribute_names=['source-ref', 'annotations']) \ndata_channels = {'train': train_data}\n<\/code><\/pre>\n\n<p>Note that you can use ManifestFile or AugmentedManifestFile depending on whether you have extra data or labels to provide. Now you can use data_channels as the input to the tf estimator:<\/p>\n\n<p><code>tf_estimator.fit(inputs=data_channels, logs=True)<\/code><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1570649242910,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58292566",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56467434,
        "Question_title":"Making a Prediction Sagemaker Pytorch",
        "Question_body":"<p>I have trained and deployed a model in Pytorch with Sagemaker. I am able to call the endpoint and get a prediction. I am using the default input_fn() function (i.e. not defined in my serve.py).<\/p>\n\n<pre><code>model = PyTorchModel(model_data=trained_model_location,\n                     role=role,\n                     framework_version='1.0.0',\n                     entry_point='serve.py',\n                     source_dir='source')\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>A prediction can be made as follows:<\/p>\n\n<pre><code>input =\"0.12787057,  1.0612601,  -1.1081504\"\npredictor.predict(np.genfromtxt(StringIO(input), delimiter=\",\").reshape(1,3) )\n<\/code><\/pre>\n\n<p>I want to be able to serve the model with REST API and am HTTP POST using lambda and API gateway. I was able to use invoke_endpoint() for this with an XGBOOST model in Sagemaker this way. I am not sure what to send into the body for Pytorch.<\/p>\n\n<pre><code>client = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(EndpointName=ENDPOINT  ,\nContentType='text\/csv',\nBody=???)\n<\/code><\/pre>\n\n<p>I believe I need to understand how to write the customer input_fn to accept and process the type of data I am able to send through invoke_client. Am I on the right track and if so, how could the input_fn be written to accept a csv from invoke_endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559765963530,
        "Question_score":1,
        "Question_tags":"python|pytorch|amazon-sagemaker",
        "Question_view_count":2346,
        "Owner_creation_time":1294628108597,
        "Owner_last_access_time":1663884676413,
        "Owner_location":null,
        "Owner_reputation":1748,
        "Owner_up_votes":80,
        "Owner_down_votes":5,
        "Owner_views":393,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes you are on the right track. You can send csv-serialized input to the endpoint without using the <code>predictor<\/code> from the SageMaker SDK, and using other SDKs such as <code>boto3<\/code> which is installed in lambda:<\/p>\n\n<pre><code>import boto3\nruntime = boto3.client('sagemaker-runtime')\n\npayload = '0.12787057,  1.0612601,  -1.1081504'\n\nresponse = runtime.invoke_endpoint(\n    EndpointName=ENDPOINT_NAME,\n    ContentType='text\/csv',\n    Body=payload.encode('utf-8'))\n\nresult = json.loads(response['Body'].read().decode()) \n<\/code><\/pre>\n\n<p>This will pass to the endpoint a csv-formatted input, that you may need to reshape back in the <code>input_fn<\/code> to put in the appropriate dimension expected by the model.<\/p>\n\n<p>for example:<\/p>\n\n<pre><code>def input_fn(request_body, request_content_type):\n    if request_content_type == 'text\/csv':\n        return torch.from_numpy(\n            np.genfromtxt(StringIO(request_body), delimiter=',').reshape(1,3))\n<\/code><\/pre>\n\n<p><strong>Note<\/strong>: I wasn't able to test the specific <code>input_fn<\/code> above with your input content and shape but I used the approach on Sklearn RandomForest couple times, and looking at the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html#model-serving\" rel=\"nofollow noreferrer\">Pytorch SageMaker serving doc<\/a> the above rationale should work.<\/p>\n\n<p>Don't hesitate to use endpoint logs in Cloudwatch to diagnose any inference error (available from the endpoint UI in the console), those logs are usually <strong>much more verbose<\/strong> that the high-level logs returned by the inference SDKs<\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1559781766690,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56467434",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65091602,
        "Question_title":"Is it possible to use more than 50 Labels in AWS Ground Truth",
        "Question_body":"<p>Is it possible to use more than 50 labels with AWS Ground Truth?<\/p>\n<p>For example <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-bounding-box.html\" rel=\"nofollow noreferrer\">here<\/a> are 3 labels:<\/p>\n<ul>\n<li>bird<\/li>\n<li>plane<\/li>\n<li>kite<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/GII4X.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/GII4X.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It shows that only 50 labels can be created. Is it possible to create more than 50 labels via AWS-CLI or any other API?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1606830024587,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|labeling|amazon-ground-truth",
        "Question_view_count":149,
        "Owner_creation_time":1510064331503,
        "Owner_last_access_time":1664041948290,
        "Owner_location":"M\u00fcnchen, Deutschland",
        "Owner_reputation":5537,
        "Owner_up_votes":1253,
        "Owner_down_votes":7,
        "Owner_views":215,
        "Question_last_edit_time":null,
        "Answer_body":"<p>No, according to the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-text-classification-multilabel.html\" rel=\"nofollow noreferrer\">documentation<\/a> the maximum is 50.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1606832097010,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65091602",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54184145,
        "Question_title":"AWS Sagemaker does not update the package",
        "Question_body":"<p>AWS Sagemaker's notebook comes with Scikit-Learn version 0.19.1<\/p>\n\n<p>I would like to use version 0.20.2. To avoid updating it every time in the notebook code, I tried using the lifecycle configurations. I created one with the following code :<\/p>\n\n<pre><code>#!\/bin\/bash\nset -e\n\/home\/ec2-user\/anaconda3\/bin\/conda install scikit-learn -y\n<\/code><\/pre>\n\n<p>When I run the attached notebook instance and go to the terminal, the version of scikit-learn found with <code>conda list<\/code> is correct (0.20.2). But when I run a notebook and import sklearn, the version is still 0.19.2.<\/p>\n\n<pre><code>import sklearn\nprint(sklearn.__version__)\n<\/code><\/pre>\n\n<p>Is there any virtual environment on the SageMaker instances where I should install the package ? How can I fix my notebook lifecycle configuration ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1547478776530,
        "Question_score":5,
        "Question_tags":"python|conda|amazon-sagemaker",
        "Question_view_count":1546,
        "Owner_creation_time":1527781503483,
        "Owner_last_access_time":1625555943023,
        "Owner_location":"Metz, France",
        "Owner_reputation":352,
        "Owner_up_votes":81,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Your conda update does not refer to a specific virtualenv, while your notebook probably does. Therefore you dont see an update on the notebook virtualenv.<\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1547708377157,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54184145",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51549048,
        "Question_title":"How to convert jupyter notebook (ipython) to slideshow NOT using command line",
        "Question_body":"<p>I am new with <code>Jupyter<\/code>, and I use Amazon SageMaker so that everything is cloud based and not local.  I cannot use any resources locally, nor can I install <code>Jupyter<\/code> on this local computer that I want to do this on, so I cannot use the command line to put :<\/p>\n\n<pre><code>jupyter nbconvert Jupyter\\ Slides.ipynb --to slides --post serve\n<\/code><\/pre>\n\n<p>So, I am struggling to find a way to convert my notebook to a slideshow NOT using command line. Thanks in advance!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1532649972710,
        "Question_score":1,
        "Question_tags":"ipython|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":4610,
        "Owner_creation_time":1528567336707,
        "Owner_last_access_time":1652301904530,
        "Owner_location":"Kelowna, BC, Canada",
        "Owner_reputation":165,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":1532675039560,
        "Answer_body":"<p>You can follow below steps to convert your notebook to slides on AWS Sagemaker (tried on sagemaker notebook instance) without installing any extensions.<\/p>\n\n<p><strong>Step 1:<\/strong> Follow this <a href=\"https:\/\/medium.com\/@mjspeck\/presenting-code-using-jupyter-notebook-slides-a8a3c3b59d67\" rel=\"nofollow noreferrer\">article<\/a> to chose which cells in your notebook can be presented or skipped.\n  - Go to View \u2192 Cell Toolbar \u2192 Slideshow\n  - A light gray bar will appear above each cell with a scroll down window on the top right\n  - Select type of slide each cell should be - regular slide, sub-slide, skip, notes<\/p>\n\n<p><strong>Step 2:<\/strong> Go to Sagemaker notebook home page and open terminal<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/kDl3d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kDl3d.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 3:<\/strong> Change directory in the instance where your notebook exists<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/rA1lZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rA1lZ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 4:<\/strong> Clone <code>reveal.js<\/code> in the directory where notebook exists from <a href=\"https:\/\/github.com\/hakimel\/reveal.js\" rel=\"nofollow noreferrer\">github<\/a>. <code>reveal.js<\/code> is used for rendering HTML file as presentation.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/dillF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/dillF.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 5:<\/strong> Run the below command (same as in your question) to convert the notebook to slides without serving them (since there is no browser on the Sagemaker instance). This will just convert notebook to slides html.<\/p>\n\n<pre><code>jupyter nbconvert Image-classification-fulltraining.ipynb --to slides\n[NbConvertApp] Converting notebook Image-classification-fulltraining.ipynb to slides\n[NbConvertApp] Writing 346423 bytes to Image-classification-fulltraining.slides.html\n<\/code><\/pre>\n\n<p><strong>Step 6:<\/strong> Now open the html file from Sagemaker notebook file browser <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/fykyl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fykyl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Now you can see the notebook rendered as slides based on how setup each cell in your notebook in Step 1<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9PLUA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9PLUA.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Hope it helps.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1532732192903,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51549048",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71161777,
        "Question_title":"Sagemaker Batch Transform entry point",
        "Question_body":"<p>Before the AWS Sagemaker batch transform I need to do some transform. is it possible to have an custom script and associate as entry point to BatchTransformer?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1645114921967,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":230,
        "Owner_creation_time":1554425457573,
        "Owner_last_access_time":1659094784667,
        "Owner_location":null,
        "Owner_reputation":63,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The inference code and requirement.txt should be stored as part of model.gz while training.  They will be used in the batch transform!!<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1646173336163,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71161777",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69076270,
        "Question_title":".txt altered after save leads to CSV reader seeing too many fields",
        "Question_body":"<p>I am running a <code>JupyterLab<\/code> on <code>AWS SageMaker<\/code>. Kernel: <code>conda_amazonei_mxnet_p27<\/code><\/p>\n<p>The number of fields found: <code>saw 9<\/code> increments by 1, each run.<\/p>\n<p><strong>Error:<\/strong> <code>ParserError: Error tokenizing data. C error: Expected 2 fields in line 50, saw 9<\/code><\/p>\n<hr \/>\n<h3>Code:<\/h3>\n<p>Invocation (Error doesn't appear when running all cells before this but does when this is ran):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>train = open('train_textcorrupted.csv', 'a')\nval = open('val.csv', 'a')\nclasses = open('classes.txt', 'a')\nuni_label = 'Organisation\\tUniversity'\nn_pad = 4\nfor i in range(len(unis)-n_pad):\n    record = ' '.join(unis[i:(i+n_pad)])\n    full_record = f'{uni_label}\\t{record}\\n'\n    if random.random() &gt; 0.9:\n        val.write(full_record)\n    else:\n        train.write(full_record) \n\nclasses.write(uni_label)\nclasses.close() \nval.close()\ntrain.close()                      \n<\/code><\/pre>\n<h3>Traceback:<\/h3>\n<pre class=\"lang-sh prettyprint-override\"><code>---------------------------------------------------------------------------\nParserError                               Traceback (most recent call last)\n&lt;ipython-input-8-89b1728bd5a6&gt; in &lt;module&gt;\n      7       --gpus 1\n      8     &quot;&quot;&quot;.split()\n----&gt; 9 run_training(args)\n&lt;ipython-input-5-091daf2638a1&gt; in run_training(input)\n     55     csv_logger = pl.loggers.CSVLogger(save_dir=f'{args.modeldir}\/csv_logs')\n     56     loggers = [logger, csv_logger]\n---&gt; 57     dm = OntologyTaggerDataModule.from_argparse_args(args)\n     58     if args.model_uri:\n     59         local_model_uri = os.environ.get('SM_CHANNEL_MODEL', '.')\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pytorch_lightning\/core\/datamodule.py in from_argparse_args(cls, args, **kwargs)\n    324         datamodule_kwargs.update(**kwargs)\n    325 \n--&gt; 326         return cls(**datamodule_kwargs)\n    327 \n    328     @classmethod\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pytorch_lightning\/core\/datamodule.py in __call__(cls, *args, **kwargs)\n     47 \n     48         # Get instance of LightningDataModule by mocking its __init__ via __call__\n---&gt; 49         obj = type.__call__(cls, *args, **kwargs)\n     50 \n     51         return obj\n&lt;ipython-input-3-66ee2be72e78&gt; in __init__(self, traindir, train_file, validate_file, model_name, labels, batch_size)\n     30         print('tokenizer', tokenizer)\n     31         print('labels_file', labels_file)\n---&gt; 32         label_mapper = LabelMapper(labels_file)\n     33         self.batch_size = batch_size\n     34         self.num_classes = label_mapper.num_classes\n&lt;ipython-input-3-66ee2be72e78&gt; in __init__(self, classes_file)\n    102 \n    103     def __init__(self, classes_file):\n--&gt; 104         self._raw_labels = pd.read_csv(classes_file, header=None, sep='\\t')\n    105 \n    106         self._map = []\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\n    686     )\n    687 \n--&gt; 688     return _read(filepath_or_buffer, kwds)\n    689 \n    690 \n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in _read(filepath_or_buffer, kwds)\n    458 \n    459     try:\n--&gt; 460         data = parser.read(nrows)\n    461     finally:\n    462         parser.close()\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in read(self, nrows)\n   1196     def read(self, nrows=None):\n   1197         nrows = _validate_integer(&quot;nrows&quot;, nrows)\n-&gt; 1198         ret = self._engine.read(nrows)\n   1199 \n   1200         # May alter columns \/ col_dict\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in read(self, nrows)\n   2155     def read(self, nrows=None):\n   2156         try:\n-&gt; 2157             data = self._reader.read(nrows)\n   2158         except StopIteration:\n   2159             if self._first_chunk:\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader.read()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._read_low_memory()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._read_rows()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._tokenize_rows()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.raise_parser_error()\nParserError: Error tokenizing data. C error: Expected 2 fields in line 50, saw 9\n<\/code><\/pre>\n<hr \/>\n<p><code>classes.txt<\/code> (tab-separated) Before runtime<\/p>\n<pre><code>Activity    Event\nActor   Person\nAgent   Person\nAlbum   Product\nAnimal  Object\nArchitecturalStructure  Location\nArtist  Person\nAthlete Person\nAutomobileEngine    Product\nAward   Object\nBiomolecule Object\nBird    Object\nBodyOfWater Location\nBuilding    Location\nChemicalSubstance   Object\nCompany Organisation\nCompetition Event\nDevice  Product\nDisease Object\nDistrict    Location\nEukaryote   Object\nEvent   Event\nFilm    Object\nFood    Object\nLanguage    Object\nLocation    Location\nMeanOfTransportation    Product\nMotorsportSeason    Event\nMunicipality    Location\nMusicalWork Product\nOrganisation    Organisation\nPainter Person\nPeriodicalLiterature    Product\nPerson  Person\nPersonFunction  Person\nPlant   Object\nPoet    Person\nPolitician  Person\nRiver   Location\nSchool  Organisation\nSettlement  Location\nSoftware    Product\nSong    Product\nSpecies Object\nSportsSeason    Event\nStation Location\nTown    Location\nVillage Location\nWriter  Person\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1630938895957,
        "Question_score":0,
        "Question_tags":"python|python-3.x|amazon-web-services|jupyter-lab|amazon-sagemaker",
        "Question_view_count":87,
        "Owner_creation_time":1622632545867,
        "Owner_last_access_time":1646839538183,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":160,
        "Owner_down_votes":15,
        "Owner_views":111,
        "Question_last_edit_time":1631000955160,
        "Answer_body":"<p>Problem Found:<\/p>\n<p>So no fault of my own, I keep ensuring these fields are on their own lines in <code>classes.txt<\/code> and <code>Ctrl+S<\/code>. Then when I reopen the file, <strong>after runtime<\/strong>, it'll have fields be on the same line again.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/UbRTm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/UbRTm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>To fix this, on line <code>classes.write(uni_label)<\/code>.<\/p>\n<p>I replaced it with <code>classes.write('\\n'+uni_label)<\/code>.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1631000840377,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1631001042847,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69076270",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53125108,
        "Question_title":"How do I download files within a Sagemaker notebook instance programatically?",
        "Question_body":"<p>We have a notebook instance within Sagemaker which contains many Jupyter Python scripts. I'd like to write a program which downloads these various scripts each day (i.e. so that I could back them up). Unfortunately I don't see any reference to this in the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/index.html\" rel=\"nofollow noreferrer\">AWS CLI API<\/a>.<\/p>\n\n<p>Is this achievable? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1541188759393,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|aws-lambda|aws-cli|amazon-sagemaker",
        "Question_view_count":8552,
        "Owner_creation_time":1299752760990,
        "Owner_last_access_time":1664061914187,
        "Owner_location":null,
        "Owner_reputation":2563,
        "Owner_up_votes":121,
        "Owner_down_votes":0,
        "Owner_views":167,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It's not exactly that you want, but looks like <a href=\"https:\/\/en.wikipedia.org\/wiki\/Version_control\" rel=\"nofollow noreferrer\">VCS<\/a> can fit your needs. You can use Github(if you already use it) or CodeCommit(free privat repos) Details and additional ways like <code>sync<\/code> target <code>dir<\/code> with <code>S3<\/code> bucket - <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/how-to-use-common-workflows-on-amazon-sagemaker-notebook-instances\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/how-to-use-common-workflows-on-amazon-sagemaker-notebook-instances\/<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1542042145470,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1542125550812,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53125108",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64537150,
        "Question_title":"AWS Sagemaker Multiple Training Jobs",
        "Question_body":"<p>We currently have a system running on AWS Sagemaker whereby several units have their own trained machine learning model artifact (using an SKLearn training script with the Sagemaker SKLearn estimator).<\/p>\n<p>Through the use of Sagemaker's multi-model endpoints, we are able to host all of these units on a single instance.<\/p>\n<p>The problem we have is that we need to scale this system up such that we can train individual models for hundreds of thousand of units and then host the resulting model artifacts on a multi-model endpoint. But, Sagemaker has a limit to the number of models you can train in parallel (our limit is 30).<\/p>\n<p>Aside from training our models in batches, does anyone have any ideas how to go about implementing a system in AWS Sagemaker whereby for hundreds of thousands of units, we can have a separate trained model artifact for each unit?<\/p>\n<p>Is there a way to output multiple model artifacts for 1 sagemaker training job with the use of an SKLearn estimator?<\/p>\n<p>Furthermore, how does Sagemaker make use of multiple CPUs when a training script is submitted? Does this have to be specified in the training script\/estimator object or is this handled automatically?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603715439387,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|machine-learning|scikit-learn|amazon-sagemaker",
        "Question_view_count":1053,
        "Owner_creation_time":1592311727163,
        "Owner_last_access_time":1647692327233,
        "Owner_location":null,
        "Owner_reputation":153,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":1603716498123,
        "Answer_body":"<p>Here are some ideas:<\/p>\n<p><em><strong>1. does anyone have any ideas how to go about implementing a system in AWS Sagemaker whereby for hundreds of thousands of units, we can have a separate trained model artifact for each unit? Is there a way to output multiple model artifacts for 1 sagemaker training job with the use of an SKLearn estimator?<\/strong><\/em><\/p>\n<p>I don't know if the 30-training job concurrency is a hard limit, if it is a blocker you should try and open a support ticket to ask if it is and try and get it raised. Otherwise as you can point out, you can try and train multiple models in one job, and produce multiple artifacts that you can either (a) send to S3 manually, or (b) save to <code>opt\/ml\/model<\/code> so that they all get sent to the model.tar.gz artifact in S3. Note that if this artifact gets too big this could get impractical though<\/p>\n<p><em><strong>2. how does Sagemaker make use of multiple CPUs when a training script is submitted? Does this have to be specified in the training script\/estimator object or is this handled automatically?<\/strong><\/em><\/p>\n<p>This depends on the type of training container you are using. SageMaker built-in containers are developed by Amazon teams and designed to efficiently use available resources. If you use your own code such as custom python in the Sklearn container, you are responsible for making sure that your code is efficiently written and uses available hardware. Hence framework choice is quite important :) for example, some sklearn models support explicitly using multiple CPUs (eg the <code>n_jobs<\/code> parameter in the <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\" rel=\"nofollow noreferrer\">random forest<\/a>), but I don't think that Sklearn natively supports GPU, multi-GPU or multi-node training.<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1603790179687,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64537150",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73663585,
        "Question_title":"Add Security groups in Amazon SageMaker for distributed training jobs",
        "Question_body":"<p>We would like to enforce specific security groups to be set on the SageMaker training jobs (XGBoost in script mode).\nHowever, distributed training, in this case, won\u2019t work out of the box, since the containers need to communicate with each other. What are the minimum inbound\/outbound rules (ports) that we need to specify for training jobs so that they can communicate?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662733216593,
        "Question_score":1,
        "Question_tags":"amazon-web-services|xgboost|amazon-sagemaker|distributed-training",
        "Question_view_count":19,
        "Owner_creation_time":1662621266503,
        "Owner_last_access_time":1663966999637,
        "Owner_location":null,
        "Owner_reputation":48,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>setting up training in VPC including specifying security groups is documented here:\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-vpc.html#train-vpc-groups\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-vpc.html#train-vpc-groups<\/a><\/p>\n<p>Normally you would allow all communication between the training nodes. To do this you specify the security group source and destination to the name of the security group itself, and allow all IPv4 traffic. If you want to figure out what ports are used, you could: 1\/ define the permissive security group. 2\/ Turn on VPC flow logs 3\/ run training. 4\/ examine VPC Flow logs 5\/ update the security group only to the required ports.<\/p>\n<p>I must say restricting communication between the training nodes might be an extreme, so I would challenge the customer why it's really needed, as all nodes carry the same job, have the same IAM role, and are transiate by nature.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1662835019253,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73663585",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60341782,
        "Question_title":"Reading a file from s3 to sagemaker on AWS gives 403 forbidden error, but other operations work on the file",
        "Question_body":"<p>This command:<\/p>\n\n<pre><code>BUCKET_TO_READ='my-bucket'\nFILE_TO_READ='myFile'\ndata_location = 's3:\/\/{}\/{}'.format(BUCKET_TO_READ, FILE_TO_READ)\ndf=pd.read_csv(data_location)\n<\/code><\/pre>\n\n<p>is failing with a <\/p>\n\n<pre><code>ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n\n<p>Error and I'm unable to figure out why.  That should work according to <a href=\"https:\/\/stackoverflow.com\/a\/50244897\/3763782\">https:\/\/stackoverflow.com\/a\/50244897\/3763782<\/a> <\/p>\n\n<p>Here are my permissions on the bucket:<\/p>\n\n<pre><code>            \"Action\": [\n                \"s3:ListMultipartUploadParts\",\n                \"s3:ListBucket\",\n                \"s3:GetObjectVersionTorrent\",\n                \"s3:GetObjectVersionTagging\",\n                \"s3:GetObjectVersionAcl\",\n                \"s3:GetObjectVersion\",\n                \"s3:GetObjectTorrent\",\n                \"s3:GetObjectTagging\",\n                \"s3:GetObjectAcl\",\n                \"s3:GetObject\"\n<\/code><\/pre>\n\n<p>And these commands work as expected: <\/p>\n\n<pre><code>role = get_execution_role()\nregion = boto3.Session().region_name\nprint(role)\nprint(region)\n\ns3 = boto3.resource('s3')\nbucket = s3.Bucket(BUCKET_TO_READ)\nprint(bucket.creation_date)\n\nfor my_bucket_object in bucket.objects.all():\n    print(my_bucket_object)\n    FILE_TO_READ = my_bucket_object.key\n    break\n\nobj = s3.Object(BUCKET_TO_READ, FILE_TO_READ)\nprint(obj)\n\n<\/code><\/pre>\n\n<p>All of those print statements worked just fine.  <\/p>\n\n<p>I'm not sure if it matters, but each file is within a folder, so my FILE_TO_READ looks like <code>folder\/file<\/code>.<\/p>\n\n<p>This command which should download the file to sagemaker also falied with a 403:<\/p>\n\n<pre><code>import boto3\ns3 = boto3.resource('s3')\ns3.Object(BUCKET_TO_READ, FILE_TO_READ).download_file(FILE_TO_READ)\n<\/code><\/pre>\n\n<p>This is also happening when I open a terminal and use <\/p>\n\n<pre><code>aws s3 cp AWSURI local_file_name\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1582298932137,
        "Question_score":4,
        "Question_tags":"python|pandas|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":2322,
        "Owner_creation_time":1403392071733,
        "Owner_last_access_time":1634681182993,
        "Owner_location":null,
        "Owner_reputation":91,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":1582306425572,
        "Answer_body":"<p>The reason was that we granted permission to the bucket not the objects.  That would be granting <code>\"Resource\": \"arn:aws:s3:::bucket-name\/\"<\/code> but not <code>\"Resource\": \"arn:aws:s3:::bucket-name\/*\"<\/code><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1582382844413,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60341782",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70873792,
        "Question_title":"How to handle Sagemaker Batch Transform discarding a file with a failed model request",
        "Question_body":"<p>I have a large number of JSON requests for a model split across multiple files in an S3 bucket. I would like to use Sagemaker's Batch Transform feature to process all of these requests (I have done a couple of test runs using small amounts of data and the transform job succeeds). My main issue is here (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors<\/a>), specifically:<\/p>\n<blockquote>\n<p>If a batch transform job fails to process an input file because of a problem with the dataset, SageMaker marks the job as failed. If an input file contains a bad record, the transform job doesn't create an output file for that input file because doing so prevents it from maintaining the same order in the transformed data as in the input file. When your dataset has multiple input files, a transform job continues to process input files even if it fails to process one. The processed files still generate useable results.<\/p>\n<\/blockquote>\n<p>This is not preferable mainly because if 1 request fails (whether its a transient error, a malformmated request, or something wrong with the model container) in a file with a large number of requests, all of those requests will get discarded (even if all of them succeeded and the last one failed). I would ideally prefer Sagemaker to just write the output of the failed response to the file and keep going, rather than discarding the entire file.<\/p>\n<p>My question is, are there any suggestions to mitigating this issue? I was thinking about storing 1 request per file in S3, but this seems somewhat ridiculous? Even if I did this, is there a good way of seeing which requests specifically failed after the transform job finishes?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643261540133,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":357,
        "Owner_creation_time":1597858315077,
        "Owner_last_access_time":1663976401100,
        "Owner_location":null,
        "Owner_reputation":84,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You've got the right idea: the fewer datapoints are in each file, the less likely a given file is to fail. The issue is that while you can pass a prefix with many files to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB\" rel=\"nofollow noreferrer\">CreateTransformJob<\/a>, partitioning one datapoint per file at least requires an S3 read per datapoint, plus a model invocation per datapoint, which is probably not great. Be aware also that <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=1000415&amp;tstart=0\" rel=\"nofollow noreferrer\">apparently there are hidden rate limits<\/a>.<\/p>\n<p>Here are a couple options:<\/p>\n<ol>\n<li><p>Partition into small-ish files, and plan on failures being rare. Hopefully, not many of your datapoints would actually fail. If you partition your dataset into e.g. 100 files, then a single failure only requires reprocessing 1% of your data. Note that Sagemaker has built-in retries, too, so most of the time failures should be caused by your data\/logic, not randomness on Sagemaker's side.<\/p>\n<\/li>\n<li><p>Deal with failures directly in your model. The same doc you quoted in your question also says:<\/p>\n<\/li>\n<\/ol>\n<blockquote>\n<p>If you are using your own algorithms, you can use placeholder text, such as ERROR, when the algorithm finds a bad record in an input file. For example, if the last record in a dataset is bad, the algorithm places the placeholder text for that record in the output file.<\/p>\n<\/blockquote>\n<p>Note that the reason Batch Transform does this whole-file failure is to maintain a 1-1 mapping between rows in the input and the output. If you can substitute the output for failed datapoints with an error message from inside your model, without actually causing the model itself to fail processing, Batch Transform will be happy.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1644424499010,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1644528409280,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70873792",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50271174,
        "Question_title":"How to make a Python Visualization as service | Integrate with website | specially sagemaker",
        "Question_body":"<p>I am from R background where we can use Plumber kind tool which provide visualization\/graph as Image via end points so we can integrate in our Java application.<\/p>\n\n<p>Now I want to integrate my Python\/Juypter visualization graph with my Java application but not sure how to host it and make it as endpoint. Right now I using AWS sagemaker to host Juypter notebook<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1525949293883,
        "Question_score":0,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":76,
        "Owner_creation_time":1501403168107,
        "Owner_last_access_time":1663685130870,
        "Owner_location":"Delhi, India",
        "Owner_reputation":1370,
        "Owner_up_votes":94,
        "Owner_down_votes":1,
        "Owner_views":125,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Amazon SageMaker is a set of different services for data scientists. You are using the notebook service that is used for developing ML models in an interactive way. The hosting service in SageMaker is creating an endpoint based on a trained model. You can call this endpoint with invoke-endpoint API call for real time inference. <\/p>\n\n<p>It seems that you are looking for a different type of hosting that is more suitable for serving HTML media rich pages, and doesn\u2019t fit into the hosting model of SageMaker. A combination of EC2 instances, with pre-built AMI or installation scripts, Congnito for authentication, S3 and EBS for object and block storage, and similar building blocks should give you a scalable and cost effective solution. <\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1526073355693,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50271174",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72760982,
        "Question_title":"How to use Tensorboard within a notebook running on Amazon SageMaker Studio Lab?",
        "Question_body":"<p>I have a Jupyter notebook running within an <code>Amazon SageMaker Studio Lab<\/code> (<a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a>) environment, and I want to use Tensordboard to monitor my model's performance inside the notebook.<\/p>\n<p>I have used the following commands to set up the Tensorboard:<\/p>\n<pre><code>%load_ext tensorboard\n# tb_log_dir variable holds the path to the log directory\n%tensorboard --logdir tb_log_dir\n<\/code><\/pre>\n<p>But nothing shows up in the output of the cell where I execute the commands. See:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TwbTk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TwbTk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The two buttons shown in the picture are not responding, BTW.<\/p>\n<p>How to solve this problem? Any suggestions would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656241190023,
        "Question_score":1,
        "Question_tags":"jupyter-notebook|amazon-sagemaker|tensorboard",
        "Question_view_count":121,
        "Owner_creation_time":1420286650807,
        "Owner_last_access_time":1663841459487,
        "Owner_location":"Trondheim, Norway",
        "Owner_reputation":720,
        "Owner_up_votes":129,
        "Owner_down_votes":0,
        "Owner_views":126,
        "Question_last_edit_time":1656243378470,
        "Answer_body":"<p>I would try the canonical way to use tensorboard in AWS Sagemaker, it should be supported also by Studio Lab, it is described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tensorboard.html\" rel=\"nofollow noreferrer\">here<\/a>. Basically install tensorboard and using the <code>EFS_PATH_LOG_DIR<\/code> launch tensorboard using the embedded console (you can do the following also from a cell):<\/p>\n<pre><code>pip install tensorboard\ntensorboard --logdir &lt;EFS_PATH_LOG_DIR&gt;\n<\/code><\/pre>\n<p>Be careful with the EFS_PATH_LOG_DIR, be sure this folder is valida path from the location you are, for example by default you are located in <code>studio-lab-user\/sagemaker-studiolab-notebooks\/<\/code> so the proper command would be <code>!tensorboard --logdir logs\/fit<\/code>.<\/p>\n<p>Then open a browser to:<\/p>\n<pre><code>https:\/\/&lt;YOUR URL&gt;\/studiolab\/default\/jupyter\/proxy\/6006\/\n<\/code><\/pre>",
        "Answer_comment_count":5,
        "Answer_creation_time":1656410088380,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1657181121616,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72760982",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55158307,
        "Question_title":"Can I make Amazon SageMaker deliver a recommendation based on historic data instead of a probability score?",
        "Question_body":"<p>We have a huge set of data in CSV format, containing a few numeric elements, like this:<\/p>\n\n<pre><code>Year,BinaryDigit,NumberToPredict,JustANumber, ...other stuff\n1954,1,762,16, ...other stuff\n1965,0,142,16, ...other stuff\n1977,1,172,16, ...other stuff\n<\/code><\/pre>\n\n<p>The thing here is that there is a strong correlation between the third column and the columns before that. So I have pre-processed the data and it's now available in a format I think is perfect:<\/p>\n\n<pre><code>1954,1,762\n1965,0,142\n1977,1,172\n<\/code><\/pre>\n\n<p>What I want is a predicition on the value in the third column, using the first two as input. So in the case above, I want the input 1965,0 to return 142. In real life this file is thousands of rows, but since there's a pattern, I'd like to retrieve the most possible value.<\/p>\n\n<p>So far I've setup a train job on the CSV file using the L<em>inear Learner<\/em> algorithm, with the following settings:<\/p>\n\n<pre><code>label_size = 1\nfeature_dim = 2\npredictor_type = regression\n<\/code><\/pre>\n\n<p>I've also created a model from it, and setup an endpoint. When I invoke it, I get a score in return.<\/p>\n\n<pre><code>    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                   ContentType='text\/csv',\n                                   Body=payload)\n<\/code><\/pre>\n\n<p>My goal here is to get the third column prediction instead. How can I achieve that? I have read a lot of the documentation regarding this, but since I'm not very familiar with AWS, I might as well have used the wrong algorithms for what I am trying to do.<\/p>\n\n<p>(Please feel free to edit this question to better suit AWS terminology)<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1552553455710,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":271,
        "Owner_creation_time":1411464641600,
        "Owner_last_access_time":1663665272227,
        "Owner_location":"\u00d6rebro, Sverige",
        "Owner_reputation":205,
        "Owner_up_votes":49,
        "Owner_down_votes":0,
        "Owner_views":38,
        "Question_last_edit_time":1552895653107,
        "Answer_body":"<p>For csv input, the label should be in the first column, as mentioned <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/cdf-training.html\" rel=\"nofollow noreferrer\">here<\/a>:  So you should preprocess your data to put the label (the column you want to predict) on the left.<\/p>\n\n<p>Next, you need to decide whether this is a regression problem or a classification problem. <\/p>\n\n<p>If you want to predict a number that's as close as possible to the true number, that's regression. For example, the truth might be 4, and the model might predict 4.15. If you need an integer prediction, you could round the model's output.<\/p>\n\n<p>If you want the prediction to be one of a few categories, then you have a classification problem. For example, we might encode 'North America' = 0, 'Europe' = 1, 'Africa' = 2, and so on. In this case, a fractional prediction wouldn't make sense. <\/p>\n\n<p>For regression, use <code>'predictor_type' = 'regressor'<\/code> and for classification with more than 2 classes, use <code>'predictor_type' = 'multiclass_classifier'<\/code> as documented <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>The output of regression will contain only a <code>'score'<\/code> field, which is the model's prediction. The output of multiclass classification will contain a <code>'predicted_label'<\/code> field, which is the model's prediction, as well as a <code>'score'<\/code> field, which is a vector of probabilities representing the model's confidence. The index with the highest probability will be the one that's predicted as the <code>'predicted_label'<\/code>. The output formats are documented <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/LL-in-formats.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1553880598533,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55158307",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48438202,
        "Question_title":"Errors while using sagemaker api to invoke endpoints",
        "Question_body":"<p>I've deployed an endpoint in sagemaker and was trying to invoke it through my python program. I had tested it using postman and it worked perfectly ok. Then I wrote the invocation code as follows<\/p>\n\n<pre><code>import boto3\nimport pandas as pd\nimport io\nimport numpy as np\n\ndef np2csv(arr):\n    csv = io.BytesIO()\n    np.savetxt(csv, arr, delimiter=',', fmt='%g')\n    return csv.getvalue().decode().rstrip()\n\n\nruntime= boto3.client('runtime.sagemaker')\npayload = np2csv(test_X)\n\nruntime.invoke_endpoint(\n    EndpointName='&lt;my-endpoint-name&gt;',\n    Body=payload,\n    ContentType='text\/csv',\n    Accept='Accept'\n)\n<\/code><\/pre>\n\n<p>Now whe I run this I get a validation error<\/p>\n\n<pre><code>ValidationError: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint &lt;my-endpoint-name&gt; of account &lt;some-unknown-account-number&gt; not found.\n<\/code><\/pre>\n\n<p>While using postman i had given my access key and secret key but I'm not sure how to pass it when using sagemaker apis. I'm not able to find it in the documentation also. <\/p>\n\n<p>So my question is, how can I use sagemaker api from my local machine to invoke my endpoint?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1516867519790,
        "Question_score":5,
        "Question_tags":"python|python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":4467,
        "Owner_creation_time":1410972175307,
        "Owner_last_access_time":1663832955917,
        "Owner_location":null,
        "Owner_reputation":1124,
        "Owner_up_votes":156,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":1516875209620,
        "Answer_body":"<p>When you are using any of the AWS SDK (including the one for Amazon SageMaker), you need to configure the credentials of your AWS account on the machine that you are using to run your code. If you are using your local machine, you can use the AWS CLI flow. You can find detailed instructions on the Python SDK page: <a href=\"https:\/\/aws.amazon.com\/developers\/getting-started\/python\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/developers\/getting-started\/python\/<\/a> <\/p>\n\n<p>Please note that when you are deploying the code to a different machine, you will have to make sure that you are giving the EC2, ECS, Lambda or any other target a role that will allow the call to this specific endpoint. While in your local machine it can be OK to give you admin rights or other permissive permissions, when you are deploying to a remote instance, you should restrict the permissions as much as possible. <\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"sagemaker:InvokeEndpoint\",\n            \"Resource\": \"arn:aws:sagemaker:*:1234567890:endpoint\/&lt;my-endpoint-name&gt;\"\n        }\n    ]\n}\n<\/code><\/pre>",
        "Answer_comment_count":2,
        "Answer_creation_time":1517113913470,
        "Answer_score":4,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48438202",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57032981,
        "Question_title":"how long does converting to amazon protobuf record using .record_set() take to complete",
        "Question_body":"<p>I am trying to convert a numpy array to an amazon protobuf record using <code>sagemaker.amazon.amazon_estimator.AmazonAlgorithmEstimatorBase.record_set()<\/code> However, this is taking a really long time. <\/p>\n\n<p>I'm wondering how the function actually performs and how long it should take<\/p>\n\n<pre><code>from sagemaker import LinearLearner\nimport numpy as np\n\nmodel=LinearLearner(role=get_execution_role(),\n                             train_instance_count=len(train_features),\n                             train_instance_type='ml.t2.medium',\n                             predictor_type='binary_classifier',\n                                )\n<\/code><\/pre>\n\n<pre><code>numpy_array = np.array([[7.4727994e-01 9.5506465e-01 7.6940370e-01 8.2015032e-01 1.8113719e-01\n  7.8720862e-01 2.9677063e-01 2.6711187e-01 7.9498607e-01 4.4924998e-01\n  4.9533784e-01 2.6846960e-01 7.0506859e-01 4.1573554e-01 6.5843487e-01\n  3.2448095e-01 4.3870610e-01 7.2739214e-01 6.0914969e-01 5.5108833e-01\n  5.8835250e-01 5.5872935e-01 4.4392920e-01 6.8353373e-01 4.7664520e-01\n  5.6887656e-01 4.7034043e-01 4.1631639e-01 3.1357434e-01 5.5933639e-04]\n [5.7815754e-01 9.5828843e-01 7.7824914e-01 8.3188844e-01 2.3287645e-01\n  7.7196079e-01 2.5512937e-01 2.7032304e-01 7.8349811e-01 5.0130588e-01\n  4.8345023e-01 3.8397798e-01 5.9922373e-01 4.7720599e-01 6.7832541e-01\n  2.7788603e-01 4.6435007e-01 7.6100332e-01 7.7771670e-01 5.1536995e-01\n  5.8536130e-01 5.6407303e-01 5.0898582e-01 6.7815554e-01 3.0614817e-01\n  5.7353836e-01 3.8981739e-01 4.1474316e-01 3.1389123e-01 3.5031504e-04]]) \n<\/code><\/pre>\n\n<pre><code>record=model.record_set(numpy_array)\n<\/code><\/pre>\n\n<h2>Expected output<\/h2>\n\n<p>I expect the variable record to container a record ready for training with linearlearning model<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1563161763097,
        "Question_score":0,
        "Question_tags":"python|linear-regression|amazon-sagemaker",
        "Question_view_count":119,
        "Owner_creation_time":1515320453680,
        "Owner_last_access_time":1664019125340,
        "Owner_location":"Bangkok",
        "Owner_reputation":1848,
        "Owner_up_votes":1616,
        "Owner_down_votes":1,
        "Owner_views":206,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I believe this is the problem:<\/p>\n\n<pre><code>train_instance_count=len(train_features)\n<\/code><\/pre>\n\n<p>This parameter is about infrastructure (how many SageMaker instances you want to train on), not about features. You should set it to 1.<\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker import LinearLearner\nimport numpy as np\n\nmodel=LinearLearner(role=sagemaker.get_execution_role(),\n                             train_instance_count=1,\n                             train_instance_type='ml.t2.medium',\n                             predictor_type='binary_classifier')\n\nnumpy_array = np.array(...)\n\nrecord=model.record_set(numpy_array)\n# This takes &lt;100 ms on my t3 notebook instance\n\nprint(record)\n\n(&lt;class 'sagemaker.amazon.amazon_estimator.RecordSet'&gt;, {'s3_data':\n's3:\/\/sagemaker-eu-west-1-123456789012\/sagemaker-record-sets\/LinearLearner-\n2019-07-18-09-48-21-639\/.amazon.manifest', 'feature_dim': 30, 'num_records': 2,\n's3_data_type': 'ManifestFile', 'channel': 'train'})\n<\/code><\/pre>\n\n<p>The manifest file lists the protobuf-encoded file(s):<\/p>\n\n<pre><code>[{\"prefix\": \"s3:\/\/sagemaker-eu-west-1-123456789012\/sagemaker-record-sets\/LinearLearner-2019-07-18-09-48-21-639\/\"}, \"matrix_0.pbr\"]\n<\/code><\/pre>\n\n<p>You can now use it for the training channel when you call fit(), re: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html<\/a><\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1563444116137,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57032981",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70163094,
        "Question_title":"SageMaker Model Registry Sharing",
        "Question_body":"<p>Is it possible to share a model registry completely between Dev and Prod environment? So my idea is to create 10000 models in dev and maybe select 2000 from there to work in prod. I am planning to use AWS model registry. So if I do the training and testing and hyperparameter tuning in my AWS dev environment, is it possible to then share the registry in prod? The obvious reason is that it does not make sense to use the prod to do the training and testing again.<\/p>\n<p>Please advise!<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638234190620,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":229,
        "Owner_creation_time":1557333597230,
        "Owner_last_access_time":1663953686617,
        "Owner_location":null,
        "Owner_reputation":99,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It depends how you define Dev and Prod.<\/p>\n<ul>\n<li><p>If by Dev and Prod you mean different AWS account (which is a good practice - see <a href=\"https:\/\/docs.aws.amazon.com\/whitepapers\/latest\/organizing-your-aws-environment\/benefits-of-using-multiple-aws-accounts.html\" rel=\"nofollow noreferrer\">doc<\/a> and <a href=\"https:\/\/aws.amazon.com\/blogs\/devops\/aws-building-a-secure-cross-account-continuous-delivery-pipeline\/\" rel=\"nofollow noreferrer\">blog<\/a>), you cannot share fractions of a model registry from a given account to another account, but you can create triggers to export models from one model registry to another, as documented in this blog post <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/patterns-for-multi-account-hub-and-spoke-amazon-sagemaker-model-registry\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/patterns-for-multi-account-hub-and-spoke-amazon-sagemaker-model-registry\/<\/a><\/p>\n<\/li>\n<li><p>If your Dev and Prod live in the same AWS account and you are just looking for ways to differentiate them, you can use:<\/p>\n<ul>\n<li>Model Registry Status information<\/li>\n<li>Tags<\/li>\n<\/ul>\n<\/li>\n<\/ul>",
        "Answer_comment_count":7,
        "Answer_creation_time":1638378803110,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70163094",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72419908,
        "Question_title":"How to match input\/output with sagemaker batch transform?",
        "Question_body":"<p>I'm using sagemaker batch transform, with json input files. see below for sample input\/output files. i have custom inference code below, and i'm using json.dumps to return prediction, but it's not returning json. I tried to use =&gt;    &quot;DataProcessing&quot;: {&quot;JoinSource&quot;: &quot;string&quot;,  }, to match input and output. but i'm getting error that &quot;unable to marshall ...&quot; . I think because , the output_fn is returning array of list or just list and not json , that is why it is unable to match input with output.any suggestions on how should i return the data?<\/p>\n<p>infernce code<\/p>\n<pre><code>def model_fn(model_dir):\n...\ndef input_fn(data, content_type):\n...\ndef predict_fn(data, model):\n...\ndef output_fn(prediction, accept):\n    if accept == &quot;application\/json&quot;:\n        return json.dumps(prediction), mimetype=accept)\n    raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))\n<\/code><\/pre>\n<p>input file<\/p>\n<pre><code>{&quot;data&quot; : &quot;input line  one&quot; }\n{&quot;data&quot; : &quot;input line  two&quot; }\n....\n<\/code><\/pre>\n<p>output file<\/p>\n<pre><code>[&quot;output line  one&quot; ]\n[&quot;output line  two&quot; ]\n<\/code><\/pre>\n<pre><code>{\n   &quot;BatchStrategy&quot;: SingleRecord,\n   &quot;DataProcessing&quot;: { \n      &quot;JoinSource&quot;: &quot;string&quot;,\n   },\n   &quot;MaxConcurrentTransforms&quot;: 3,\n   &quot;MaxPayloadInMB&quot;: 6,\n   &quot;ModelClientConfig&quot;: { \n      &quot;InvocationsMaxRetries&quot;: 1,\n      &quot;InvocationsTimeoutInSeconds&quot;: 3600\n   },\n   &quot;ModelName&quot;: &quot;some-model&quot;,\n   &quot;TransformInput&quot;: { \n      &quot;ContentType&quot;: &quot;string&quot;,\n      &quot;DataSource&quot;: { \n         &quot;S3DataSource&quot;: { \n            &quot;S3DataType&quot;: &quot;string&quot;,\n            &quot;S3Uri&quot;: &quot;s3:\/\/bucket-sample&quot;\n         }\n      },\n      &quot;SplitType&quot;: &quot;Line&quot;\n   },\n   &quot;TransformJobName&quot;: &quot;transform-job&quot;\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653780407103,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":358,
        "Owner_creation_time":1590797441983,
        "Owner_last_access_time":1664049080543,
        "Owner_location":null,
        "Owner_reputation":525,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":98,
        "Question_last_edit_time":null,
        "Answer_body":"<p><code>json.dumps<\/code> will not convert your array to a dict structure and serialize it to a JSON String.<\/p>\n<p>What data type is <code>prediction<\/code> ? Have you tested making sure <code>prediction<\/code> is a dict?<\/p>\n<p>You can confirm the data type by adding <code>print(type(prediction))<\/code> to see the data type in the CloudWatch Logs.<\/p>\n<p>If prediction is a <code>list<\/code> you can test the following:<\/p>\n<pre><code>def output_fn(prediction, accept):\n    if accept == &quot;application\/json&quot;:\n\n        my_dict = {'output': prediction}\n        return json.dumps(my_dict), mimetype=accept)\n\n    raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))\n<\/code><\/pre>\n<p><code>DataProcessing<\/code> and <code>JoinSource<\/code> are used to associate the data that is relevant to the prediction results in the output. It is not meant to be used to match the input and output format.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1654129068707,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72419908",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63813624,
        "Question_title":"Load a Picked or Joblib Pre trained ML Model to Sagemaker and host as endpoint",
        "Question_body":"<p>If I have a trained model in Using pickle, or Joblib.\nLets say its Logistic regression or XGBoost.<\/p>\n<p>I would like to host that model in AWS Sagemaker as endpoint without running a training job.\nHow to achieve that.<\/p>\n<pre><code>#Lets Say myBucketName contains model.pkl\nmodel = joblib.load('filename.pkl')  \n# X_test = Numpy Array \nmodel.predict(X_test)  \n<\/code><\/pre>\n<p>I am not interested to <code>sklearn_estimator.fit('S3 Train, S3 Validate' )<\/code> , I have the trained model<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1599661148083,
        "Question_score":3,
        "Question_tags":"python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":1622,
        "Owner_creation_time":1370509408500,
        "Owner_last_access_time":1664063534747,
        "Owner_location":null,
        "Owner_reputation":1573,
        "Owner_up_votes":97,
        "Owner_down_votes":12,
        "Owner_views":194,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For Scikit Learn for example, you can get inspiration from this public demo <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>\n<p>Step 1: Save your artifact (eg the joblib) compressed in S3 at <code>s3:\/\/&lt;your path&gt;\/model.tar.gz<\/code><\/p>\n<p>Step 2: Create an inference script with the deserialization function <code>model_fn<\/code>. (Note that you could also add custom inference functions <code>input_fn<\/code>, <code>predict_fn<\/code>, <code>output_fn<\/code> but for scikit the defaults function work fine)<\/p>\n<pre><code>%%writefile inference_script.py. # Jupiter command to create file in case you're in Jupiter\n\nimport joblib\nimport os\n\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return clf\n<\/code><\/pre>\n<p>Step 3: Create a model associating the artifact with the right container<\/p>\n<pre><code>from sagemaker.sklearn.model import SKLearnModel\n\nmodel = SKLearnModel(\n    model_data='s3:\/\/&lt;your path&gt;\/model.tar.gz',\n    role='&lt;your role&gt;',\n    entry_point='inference_script.py',\n    framework_version='0.23-1')\n<\/code><\/pre>\n<p>Step 4: Deploy!<\/p>\n<pre><code>model.deploy(\n    instance_type='ml.c5.large',  # choose the right instance type\n    initial_instance_count=1)\n<\/code><\/pre>",
        "Answer_comment_count":2,
        "Answer_creation_time":1599722577177,
        "Answer_score":5,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63813624",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70455676,
        "Question_title":"AWS Sagemaker Pipelines throws a \"No finished training job found associated with this estimator\" after introducing a register step",
        "Question_body":"<p>I am currently working on creating a Sagemaker Pipeline to train a Tensorflow model. I'm new to this area and I have been following <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-pipelines\/tabular\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint.ipynb\" rel=\"nofollow noreferrer\">this guide<\/a> created by AWS as well as the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html\" rel=\"nofollow noreferrer\">standard pipeline workflow<\/a> listed in the Sagemaker developer guide.<\/p>\n<p>I have a pipeline that runs without error when I only include the preprocessing, training, evaluation, and condition steps. When I add the register step:<\/p>\n<pre><code># Package evaluation metrics into an evaluation report `PropertyFile`\nevaluation_report = PropertyFile(\n        name=&quot;EvaluationReport&quot;, output_name=&quot;evaluation&quot;, path=&quot;evaluation_report.json&quot;\n)\n\n# Create ModelMetrics object using the evaluation report from the evaluation step\n# A ModelMetrics object contains metrics captured from a model.\nmodel_metrics = ModelMetrics(model_statistics=evaluation_report)\n\n# Create a RegisterModel step, which registers the model with Sagemaker Model Registry.\nregister_step = RegisterModel(\n    name=&quot;Foo&quot;,\n    estimator=estimator,\n    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n    content_types=[&quot;text\/csv&quot;],\n    response_types=[&quot;text\/csv&quot;],\n    inference_instances=config[&quot;instance&quot;][&quot;inference&quot;],\n    transform_instances=config[&quot;instance&quot;][&quot;transform&quot;],\n    model_package_group_name=&quot;Bar&quot;,\n    model_metrics=model_metrics,\n    approval_status=&quot;approved&quot;,\n)\n<\/code><\/pre>\n<p>to the condition step's <code>if_steps<\/code>:<\/p>\n<pre><code># Create a Sagemaker Pipelines ConditionStep, using the condition above.\n# Enter the steps to perform if the condition returns True \/ False.\ncond_step = ConditionStep(\n    name=&quot;MSE-Lower-Than-Threshold-Condition&quot;,\n    conditions=[cond_lte],\n    if_steps=[register_step],\n    else_steps=[],\n)\n<\/code><\/pre>\n<p>I get the following trace:<\/p>\n<pre><code>PropertyFile(name='EvaluationReport', output_name='evaluation', path='evaluation_report.json')\nNo finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\nTraceback (most recent call last):\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/pipeline_definition.py&quot;, line 474, in &lt;module&gt;\n    main()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/pipeline_definition.py&quot;, line 466, in main\n    pipeline = define_pipeline()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/pipeline_definition.py&quot;, line 457, in define_pipeline\n    print(json.loads(pipeline.definition()))\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/pipeline.py&quot;, line 257, in definition\n    request_dict = self.to_request()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/pipeline.py&quot;, line 89, in to_request\n    &quot;Steps&quot;: list_to_request(self.steps),\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/utilities.py&quot;, line 37, in list_to_request\n    request_dicts.append(entity.to_request())\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/steps.py&quot;, line 99, in to_request\n    &quot;Arguments&quot;: self.arguments,\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/condition_step.py&quot;, line 87, in arguments\n    IfSteps=list_to_request(self.if_steps),\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/utilities.py&quot;, line 39, in list_to_request\n    request_dicts.extend(entity.request_dicts())\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/step_collections.py&quot;, line 50, in request_dicts\n    return [step.to_request() for step in self.steps]\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/step_collections.py&quot;, line 50, in &lt;listcomp&gt;\n    return [step.to_request() for step in self.steps]\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/steps.py&quot;, line 209, in to_request\n    step_dict = super().to_request()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/steps.py&quot;, line 99, in to_request\n    &quot;Arguments&quot;: self.arguments,\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/_utils.py&quot;, line 423, in arguments\n    model_package_args = get_model_package_args(\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/session.py&quot;, line 4217, in get_model_package_args\n    model_package_args[&quot;model_metrics&quot;] = model_metrics._to_request_dict()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/model_metrics.py&quot;, line 66, in _to_request_dict\n    model_quality[&quot;Statistics&quot;] = self.model_statistics._to_request_dict()\nAttributeError: 'PropertyFile' object has no attribute '_to_request_dict'\n<\/code><\/pre>\n<p>From this trace I see two, potentially related, issues. The immediate issue is the <code>AttributeError: 'PropertyFile' object has no attribute '_to_request_dict'<\/code>. I haven't been able to find any information on why we might be receiving it between forums and Sagemaker documentation.<\/p>\n<p>I also see a sneaky issue towards the top of the trace that has plagued me all day. The line <code>No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config<\/code> tells me that the register step is using our estimator when it should be waiting until after the training step has run. I can't seem to find any reference to this error, besides a somewhat-similar <a href=\"https:\/\/datascience.stackexchange.com\/questions\/100113\/how-to-fix-sagemakers-no-finished-training-job-found-associated-with-this-esti\">stack exchange post<\/a>.<\/p>\n<p>I've compared my code to the AWS-published examples many times and I'm confident that I'm not doing anything taboo. Would anyone be able to shine some light on what these errors are suggesting? Is there any more information or code that would be relevant?<\/p>\n<p>Thanks so much!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1640211128263,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":1006,
        "Owner_creation_time":1619402503747,
        "Owner_last_access_time":1655994422190,
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":36,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I was able to work through the issue! Here is the code for the register step that ended up working with my Tensorflow model:<\/p>\n<pre><code># Package the model\npipeline_model = PipelineModel(models=[model], role=params[&quot;role&quot;].default_value, sagemaker_session=session)\n\n# Create a RegisterModel step, which registers the model with Sagemaker Model Registry.\nregister_step = RegisterModel(\n    name=&quot;Bar&quot;,\n    model=pipeline_model,\n    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n    content_types=[&quot;application\/json&quot;],\n    response_types=[&quot;application\/json&quot;],\n    inference_instances=[config[&quot;instance&quot;][&quot;inference&quot;]],\n    transform_instances=[config[&quot;instance&quot;][&quot;transform&quot;]],\n    model_package_group_name=&quot;Foo&quot;,\n    approval_status=&quot;Approved&quot;,\n)\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1642620406233,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70455676",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60892850,
        "Question_title":"Deepracer log_analysis tool - sagemaker role errors",
        "Question_body":"<p>I'm trying to run the Deepracer log analysis tool from <a href=\"https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb<\/a> on my local laptop. However I get below error while trying to run step [5] \"Create an IAM role\". <\/p>\n\n<pre><code>try:\n    sagemaker_role = sagemaker.get_execution_role()\nexcept:\n    sagemaker_role = get_execution_role('sagemaker')\n\nprint(\"Using Sagemaker IAM role arn: \\n{}\".format(sagemaker_role))\n\nCouldn't call 'get_role' to get Role ARN from role name arn:aws:iam::26********:root to get Role path.\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-5-3bea8175b8c7&gt; in &lt;module&gt;\n      1 try:\n----&gt; 2     sagemaker_role = sagemaker.get_execution_role()\n      3 except:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in get_execution_role(sagemaker_session)\n   3302     )\n-&gt; 3303     raise ValueError(message.format(arn))\n   3304 \n\nValueError: The current AWS identity is not a role: arn:aws:iam::26********:root, therefore it cannot be used as a SageMaker execution role\n\nDuring handling of the above exception, another exception occurred:\n\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-5-3bea8175b8c7&gt; in &lt;module&gt;\n      2     sagemaker_role = sagemaker.get_execution_role()\n      3 except:\n----&gt; 4     sagemaker_role = get_execution_role('sagemaker')\n      5 \n      6 print(\"Using Sagemaker IAM role arn: \\n{}\".format(sagemaker_role))\n\nNameError: name 'get_execution_role' is not defined\n<\/code><\/pre>\n\n<p>Does anybody know what needs to be done to execute above code without errors? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1585337019477,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|log-analysis",
        "Question_view_count":491,
        "Owner_creation_time":1404227763680,
        "Owner_last_access_time":1659532526860,
        "Owner_location":"New York, NY, USA",
        "Owner_reputation":1807,
        "Owner_up_votes":303,
        "Owner_down_votes":2,
        "Owner_views":207,
        "Question_last_edit_time":null,
        "Answer_body":"<p>AWS support recommended below solution:<\/p>\n\n<p>This seems to be a known issue when executing the code locally, as mentioned in the following Github issue [3]. A work-around to fix the issue is also defined in that issue [3] and can be referred to using the following link: aws\/sagemaker-python-sdk#300 (comment)<\/p>\n\n<p>The steps in the work-around given in the above link are:<\/p>\n\n<ol>\n<li><p>Login to the AWS console -> IAM -> Roles -> Create Role<\/p><\/li>\n<li><p>Create an IAM role and select the \"SageMaker\" service<\/p><\/li>\n<li><p>Give the role \"AmazonSageMakerFullAccess\" permission<\/p><\/li>\n<li><p>Review and create the role<\/p><\/li>\n<li><p>Next, also attach the \"AWSRoboMakerFullAccess\" permission policy to the above created role (as required in the Github notebook [1]).<\/p><\/li>\n<li><p>The original code would then need to be modified to fetch the IAM role directly when the code is executed on a local machine. The code snippet to be used is given below:<\/p><\/li>\n<\/ol>\n\n<pre><code>try:\n   sagemaker_role = sagemaker.get_execution_role()\n except ValueError:\n   iam = boto3.client('iam')\n   sagemaker_role = iam.get_role(RoleName='&lt;sagemaker-IAM-role-name&gt;')['Role']['Arn']\n<\/code><\/pre>\n\n<p>In the above snippet, replace the \"\" text with the IAM role name created in Step 4.<\/p>\n\n<p>References:<\/p>\n\n<p>[1] <a href=\"https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/aws-deepracer-workshops\/blob\/master\/log-analysis\/DeepRacer%20Log%20Analysis.ipynb<\/a><\/p>\n\n<p>[2] <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-role.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-role.html<\/a><\/p>\n\n<p>[3] aws\/sagemaker-python-sdk#300<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1585760437230,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60892850",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69338516,
        "Question_title":"how to train and deploy YOLOv5 on aws sagemaker",
        "Question_body":"<p>I want to train YOLOv5 on aws sagemaker also deploy the model on sagemaker itself,need to know about entrypoint python script as well. how can I build a pipeline for this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1632686467053,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|yolov5",
        "Question_view_count":2510,
        "Owner_creation_time":1556022524713,
        "Owner_last_access_time":1653309846467,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1632754422356,
        "Answer_body":"<p>This official AWS <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/speed-up-yolov4-inference-to-twice-as-fast-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">blog post<\/a> has information on how to deploy YOLOv4. I wonder if you can use it as a guide and change the model to v5.<\/p>\n<p>If not, there is a 3rd party implementation of YOLOv5 <a href=\"https:\/\/github.com\/HKT-SSA\/yolov5-on-sagemaker\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1632853607047,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69338516",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73751712,
        "Question_title":"Where can I find the Performance Metrics generated by SageMaker Debugger\/Profiler?",
        "Question_body":"<p>Where can I look for the <strong>performance metrics<\/strong> generated by <strong>Amazon SageMaker Debugger\/Profiler<\/strong>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663381119547,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|performancecounter|amazon-sagemaker-debugger",
        "Question_view_count":23,
        "Owner_creation_time":1389887039673,
        "Owner_last_access_time":1664076128463,
        "Owner_location":"Singapore",
        "Owner_reputation":5854,
        "Owner_up_votes":155,
        "Owner_down_votes":70,
        "Owner_views":794,
        "Question_last_edit_time":null,
        "Answer_body":"<p>1.<code>from sagemaker.debugger import ProfilerConfig<\/code><\/p>\n<p><code>profiler_config = ProfilerConfig( framework_profile_params=FrameworkProfile(start_step=1, num_steps=2) )<\/code><\/p>\n<p>2.\n<code>from sagemaker.debugger import TensorBoardOutputConfig<\/code><\/p>\n<p><code>tensorboard_output_config = TensorBoardOutputConfig(s3_output_path= &lt;&lt; add your bucket name an folder &gt;&gt; )<\/code><\/p>\n<ol start=\"3\">\n<li><p>In your estimator - specify :  <code>profiler_config= profiler_config<\/code> and <code>tensorboard_output_config=tensorboard_output_config<\/code><\/p>\n<\/li>\n<li><p>Train your model<\/p>\n<\/li>\n<li><p>Go to the s3 bucket specified  for your training job name that is assigned in Sagemaker . You should see a report under <strong>rule-output<\/strong> &gt; <strong>ProfilerReport<\/strong> *** &gt; <strong>profiler-output\/<\/strong> &gt; <strong>profiler-report.html<\/strong><\/p>\n<\/li>\n<\/ol>",
        "Answer_comment_count":0,
        "Answer_creation_time":1663457554493,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1663469070623,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73751712",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50508217,
        "Question_title":"Can sagemaker's linear learner be used for multiclass classification?",
        "Question_body":"<p>I am building a multiclass classifier on aws Sagemaker, and would love to use the predefined linearlearner algorithm for classification. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1527161288830,
        "Question_score":1,
        "Question_tags":"algorithm|amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":410,
        "Owner_creation_time":1527161034077,
        "Owner_last_access_time":1555067138630,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes, it is possible now.<\/p>\n\n<p>You can set the predictor_type hyper-parameter to <code>multiclass_classifier<\/code>.<\/p>\n\n<p>See the documentation here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html<\/a>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1531423415907,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50508217",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73415182,
        "Question_title":"When do I use a glue job or a Sagemaker Processing job for an etl?",
        "Question_body":"<p>I am currently struggling to decide on what situations in which a glue job is preferable over a sagemaker processing job and vice versa? Some advice on this topic would be greatly appreciated.<\/p>\n<p>I can do the same on both, so why should I bother with the difference?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660903988283,
        "Question_score":2,
        "Question_tags":"amazon-web-services|aws-glue|amazon-sagemaker",
        "Question_view_count":34,
        "Owner_creation_time":1644981356940,
        "Owner_last_access_time":1663945577550,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<ul>\n<li>if you want to use a specific EC2 instance, use SageMaker<\/li>\n<li>Pricing: SageMaker is pro-rated per-second while Glue has minimum charge amount (1min or 10min depending on versions). You should measure how much would a workload cost you on each platform<\/li>\n<li>customization: in SageMaker Processing you can customize the execution environment, as you provide a Docker image (you could run more than Spark\/Python, such as C++ or R)<\/li>\n<\/ul>",
        "Answer_comment_count":0,
        "Answer_creation_time":1660951371720,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73415182",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68143997,
        "Question_title":"Automate Docker Run command on Sagemaker's Notebook Instance",
        "Question_body":"<p>I have a Docker image in AWS ECR and I open my Sagemaker Notebook instance---&gt;go to terminal--&gt;docker run....\nThis is how I start my Docker container.<\/p>\n<p>Now, I want to automate this process(running my docker image on Sagemaker Notebook Instance) instead of typing the docker run commands.<\/p>\n<p>Can I create a cron job on Sagemaker? or Is there any other approach?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1624722520977,
        "Question_score":0,
        "Question_tags":"amazon-web-services|docker|containers|amazon-sagemaker",
        "Question_view_count":393,
        "Owner_creation_time":1497621837833,
        "Owner_last_access_time":1663656157060,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":230,
        "Owner_up_votes":11,
        "Owner_down_votes":4,
        "Owner_views":27,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For this you can create an inline Bash shell in your SageMaker notebook as follows. This will take your Docker container, create the image, ECR repo if it does not exist and push the image.<\/p>\n<pre><code>%%sh\n\n# Name of algo -&gt; ECR\nalgorithm_name=your-algo-name\n\ncd container #your directory with dockerfile and other sm components\n\nchmod +x randomForest-Petrol\/train #train file for container\nchmod +x randomForest-Petrol\/serve #serve file for container\n\naccount=$(aws sts get-caller-identity --query Account --output text)\n\n# Region, defaults to us-west-2\nregion=$(aws configure get region)\nregion=${region:-us-west-2}\n\nfullname=&quot;${account}.dkr.ecr.${region}.amazonaws.com\/${algorithm_name}:latest&quot;\n\n# If the repository doesn't exist in ECR, create it.\naws ecr describe-repositories --repository-names &quot;${algorithm_name}&quot; &gt; \/dev\/null 2&gt;&amp;1\n\nif [ $? -ne 0 ]\nthen\n    aws ecr create-repository --repository-name &quot;${algorithm_name}&quot; &gt; \/dev\/null\nfi\n\n# Get the login command from ECR and execute it directly\naws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n\n# Build the docker image locally with the image name and then push it to ECR\n# with the full name.\n\ndocker build  -t ${algorithm_name} .\ndocker tag ${algorithm_name} ${fullname}\n\ndocker push ${fullname}\n<\/code><\/pre>\n<p>I am contributing this on behalf of my employer, AWS. My contribution is licensed under the MIT license. See here for a more detailed explanation\n<a href=\"https:\/\/aws-preview.aka.amazon.com\/tools\/stackoverflow-samples-license\/\" rel=\"nofollow noreferrer\">https:\/\/aws-preview.aka.amazon.com\/tools\/stackoverflow-samples-license\/<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1626977403510,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1626979264576,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68143997",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63145277,
        "Question_title":"Sagemaker Training Job Not Uploading\/Saving Training Model to S3 Output Path",
        "Question_body":"<p>Ok I've been dealing with this issue in Sagemaker for almost a week and I'm ready to pull my hair out. I've got a custom training script paired with a data processing script in a BYO algorithm Docker deployment type scenario. It's a Pytorch model built with Python 3.x, and the BYO Docker file was originally built for Python 2, but I can't see an issue with the problem that I am having.....which is that after a successful training run Sagemaker doesn't save the model to the target S3 bucket.<\/p>\n<p>I've searched far and wide and can't seem to find an applicable answer anywhere. This is all done inside a Notebook instance. Note: I am using this as a contractor and don't have full permissions to the rest of AWS, including downloading the Docker image.<\/p>\n<p>Dockerfile:<\/p>\n<pre><code>FROM ubuntu:18.04\n\nMAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python-pip \\\n         python3-pip3\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\nRUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python3 get-pip.py &amp;&amp; \\\n    pip3 install future numpy torch scipy scikit-learn pandas flask gevent gunicorn &amp;&amp; \\\n        rm -rf \/root\/.cache\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=&quot;\/opt\/program:${PATH}&quot;\n\nCOPY decision_trees \/opt\/program\nWORKDIR \/opt\/program\n<\/code><\/pre>\n<p>Docker Image Build:<\/p>\n<pre><code>%%sh\n\nalgorithm_name=&quot;name-this-algo&quot;\n\ncd container\n\nchmod +x decision_trees\/train\nchmod +x decision_trees\/serve\n\naccount=$(aws sts get-caller-identity --query Account --output text)\n\nregion=$(aws configure get region)\nregion=${region:-us-east-2}\n\nfullname=&quot;${account}.dkr.ecr.${region}.amazonaws.com\/${algorithm_name}:latest&quot;\n\naws ecr describe-repositories --repository-names &quot;${algorithm_name}&quot; &gt; \/dev\/null 2&gt;&amp;1\n\nif [ $? -ne 0 ]\nthen\n    aws ecr create-repository --repository-name &quot;${algorithm_name}&quot; &gt; \/dev\/null\nfi\n\n# Get the login command from ECR and execute it directly\n$(aws ecr get-login --region ${region} --no-include-email)\n\n# Build the docker image locally with the image name and then push it to ECR\n# with the full name.\n\ndocker build  -t ${algorithm_name} .\ndocker tag ${algorithm_name} ${fullname}\n\ndocker push ${fullname}\n<\/code><\/pre>\n<p>Env setup and session start:<\/p>\n<pre><code>common_prefix = &quot;pytorch-lstm&quot;\ntraining_input_prefix = common_prefix + &quot;\/training-input-data&quot;\nbatch_inference_input_prefix = common_prefix + &quot;\/batch-inference-input-data&quot;\n\nimport os\nfrom sagemaker import get_execution_role\nimport sagemaker as sage\n\nsess = sage.Session()\n\nrole = get_execution_role()\nprint(role)\n<\/code><\/pre>\n<p>Training Directory, Image, and Estimator Setup, then a <code>fit<\/code> call:<\/p>\n<pre><code>TRAINING_WORKDIR = &quot;a\/local\/directory&quot;\n\ntraining_input = sess.upload_data(TRAINING_WORKDIR, key_prefix=training_input_prefix)\nprint (&quot;Training Data Location &quot; + training_input)\n\naccount = sess.boto_session.client('sts').get_caller_identity()['Account']\nregion = sess.boto_session.region_name\nimage = '{}.dkr.ecr.{}.amazonaws.com\/image-that-works:working'.format(account, region)\n\ntree = sage.estimator.Estimator(image,\n                       role, 1, 'ml.p2.xlarge',\n                       output_path=&quot;s3:\/\/sagemaker-directory-that-definitely\/exists&quot;,\n                       sagemaker_session=sess)\n\ntree.fit(training_input)\n<\/code><\/pre>\n<p>The above script is working, for sure. I have print statements in my script and they are printing the expected results to the console. This runs as it's supposed to, finishes up, and says that it's deploying model artifacts when IT DEFINITELY DOES NOT.<\/p>\n<p>Model Deployment:<\/p>\n<pre><code>model = tree.create_model()\npredictor = tree.deploy(1, 'ml.m4.xlarge')\n<\/code><\/pre>\n<p>This throws an error that the model can't be found. A call to <code>aws sagemaker describe-training-job<\/code> shows that the training was completed but I found that the time it took to upload the model was super fast, so obviously there's an error somewhere and it's not telling me. Thankfully it's not just uploading it to the aether.<\/p>\n<pre><code>{\n            &quot;Status&quot;: &quot;Uploading&quot;,\n            &quot;StartTime&quot;: 1595982984.068,\n            &quot;EndTime&quot;: 1595982989.994,\n            &quot;StatusMessage&quot;: &quot;Uploading generated training model&quot;\n        },\n<\/code><\/pre>\n<p>Here's what I've tried so far:<\/p>\n<ol>\n<li>I've tried uploading it to a different bucket. I figured my permissions were the problem so I pointed it to one that I new allowed me to upload as I had done it before to that bucket. No dice.<\/li>\n<li>I tried backporting the script to Python 2.x, but that caused more problems than it probably would have solved, and I don't really see how that would be the problem anyways.<\/li>\n<li>I made sure the Notebook's IAM role has sufficient permissions, and it does have a SagemakerFullAccess policy<\/li>\n<\/ol>\n<p>What bothers me is that there's no error log I can see. If I could be directed to that I would be happy too, but if there's some hidden Sagemaker kungfu that I don't know about I would be forever grateful.<\/p>\n<hr \/>\n<p>EDIT<\/p>\n<p>The training job runs and prints to both the Jupyter cell and CloudWatch as expected. I've since lost the cell output in the notebook but below is the last few lines in CloudWatch. The first number is the epoch and the rest are various custom model metrics.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/y3I9L.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/y3I9L.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1595988324557,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|pytorch|amazon-sagemaker",
        "Question_view_count":2438,
        "Owner_creation_time":1360536048187,
        "Owner_last_access_time":1663951121943,
        "Owner_location":"Miami, FL, USA",
        "Owner_reputation":45,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Question_last_edit_time":1596039837756,
        "Answer_body":"<p>Can you verify from the training job logs that your training script is running? It doesn't look like your Docker image would respond to the command <code>train<\/code>, which is what SageMaker requires, and so I suspect that your model isn't actually getting trained\/saved to <code>\/opt\/ml\/model<\/code>.<\/p>\n<p>AWS documentation about how SageMaker runs the Docker container: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html<\/a><\/p>\n<p>edit: summarizing from the comments below - the training script must also save the model to <code>\/opt\/ml\/model<\/code> (the model isn't saved automatically).<\/p>",
        "Answer_comment_count":6,
        "Answer_creation_time":1596039270563,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1619730108172,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63145277",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63960011,
        "Question_title":"Using AWS Sagemaker for model performance without creating endpoint",
        "Question_body":"<p>I've been using Amazon Sagemaker Notebooks to build a pytorch model for an NLP task.\nI know you can use Sagemaker to train, deploy, hyper parameter tuning, and model monitoring.<\/p>\n<p>However, it looks like you have to create an inference endpoint in order to monitor the model's inference performance.<\/p>\n<p>I already have a EC2 instance setup to perform inference tasks on our model, which is currently on a development box and rather not use an endpoint to make<\/p>\n<p>Is it possible to use Sagemaker to train, run hyperparam tuning and model eval without creating an endpoint.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1600448783673,
        "Question_score":1,
        "Question_tags":"pytorch|amazon-sagemaker",
        "Question_view_count":494,
        "Owner_creation_time":1285379271580,
        "Owner_last_access_time":1649074179223,
        "Owner_location":null,
        "Owner_reputation":625,
        "Owner_up_votes":174,
        "Owner_down_votes":11,
        "Owner_views":110,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If you don't want to keep an inference endpoint up, one option is to use SageMaker Processing to run a job that takes your trained model and test dataset as input, performs inference and computes evaluation metrics, and saves them to S3 in a JSON file.<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">This Jupyter notebook example<\/a> steps through (1) preprocessing training and test data, (2) training a model, then (3) evaluating the model<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1604012509390,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63960011",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60163614,
        "Question_title":"What is correct input for mxnet's linear learner in AWS SageMaker?",
        "Question_body":"<p>I am trying to create a simple linear learner in AWS SageMaker with MXNet. I have never worked with SageMaker or MXNet previously. Fitting the model gives runtime error as follows and shuts the instance:<\/p>\n\n<blockquote>\n  <p>UnexpectedStatusException: Error for Training job\n  linear-learner-2020-02-11-06-13-22-712: Failed. Reason: ClientError:\n  Unable to read data channel 'train'. Requested content-type is\n  'application\/x-recordio-protobuf'. Please verify the data matches the\n  requested content-type. (caused by MXNetError)<\/p>\n<\/blockquote>\n\n<p>I think that the data should be converted to protobuf format before passing as training data. Could someone please explain to me what is the correct format for MXNet models? What is the best way to convert a simple data frame into protobuf?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1581404906080,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker|mxnet",
        "Question_view_count":617,
        "Owner_creation_time":1490866954400,
        "Owner_last_access_time":1664018123747,
        "Owner_location":"Ahmedabad, Gujarat, India",
        "Owner_reputation":834,
        "Owner_up_votes":118,
        "Owner_down_votes":11,
        "Owner_views":91,
        "Question_last_edit_time":null,
        "Answer_body":"<p><a href=\"https:\/\/github.com\/awslabs\/fraud-detection-using-machine-learning\/blob\/master\/source\/notebooks\/sagemaker_fraud_detection.ipynb\" rel=\"nofollow noreferrer\">This end-to-end demo<\/a> shows usage of Linear Learner from input data pre-processed in <code>pandas<\/code> dataframes and then converted to protobuf using the SDK. But note that:<\/p>\n\n<ul>\n<li>There is no need to use protobuf, you can also pass csv data with the target variable on the first column of the files, as <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner.html#ll-input_output\" rel=\"nofollow noreferrer\">indicated here<\/a>.<\/li>\n<li>There is no need to know MXNet in order to use the SageMaker Linear Learner, just use the SDK of your choice, bring data to S3, and orchestrate training and inference :)<\/li>\n<\/ul>",
        "Answer_comment_count":2,
        "Answer_creation_time":1581411605430,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60163614",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52359397,
        "Question_title":"AWS-ML: How to deploy\/setup my own ML algorithms on AWS platform as pay-to-use API?",
        "Question_body":"<p>The title sums it up. Essentially, I'd like to offer my own closed-source proprietary ML algorithms to Amazon AWS customers on a pay-to-use basis API - e.g., sales volumes prediction algorithm service licensed monthly or annually or per call. Most information found talks about how to build and give it away, or use it internally within one's company, but I'm looking to offer it to the public as a commercial offering on AWS.<\/p>\n\n<p>Thank you in advance for your help - links to articles, help pages, or direct steps on how to do this.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1537142950110,
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-machine-learning",
        "Question_view_count":167,
        "Owner_creation_time":1537030909473,
        "Owner_last_access_time":1554190731110,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":1537210523387,
        "Answer_body":"<p>Allow me please to answer my own question. Although not a 100% what I was hoping for, there's certainly support for this in the platform which is great to see: <a href=\"https:\/\/docs.aws.amazon.com\/marketplace\/latest\/userguide\/saas-products.html\" rel=\"nofollow noreferrer\">Software-as-a-Service-Based Products<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1537318983690,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52359397",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67361483,
        "Question_title":"AWS Sagemaker Studio, cannot load pickle files",
        "Question_body":"<p>I'm a newbie in Sagemaker and i'm trying to load a pickle dataset into sagemaker notebook.\nI'm using the Python 3 (Data Science) kernel and ml.t3.medium instance.\nEither i load the pickle from S3 or I upload it directly from the studio like this:<\/p>\n<pre><code>import pickle5\nwith open('filename', 'rb') as f:\n    x = pickle.load(f)\n<\/code><\/pre>\n<p><strong>I get this Error:<\/strong><\/p>\n<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n\/opt\/conda\/lib\/python3.7\/site-packages\/IPython\/core\/formatters.py in __call__(self, obj)\n    700                 type_pprinters=self.type_printers,\n    701                 deferred_pprinters=self.deferred_printers)\n--&gt; 702             printer.pretty(obj)\n    703             printer.flush()\n    704             return stream.getvalue()\n\n..................... more errors here\n\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pandas\/core\/generic.py in __getattr__(self, name)\n   5268             or name in self._accessors\n   5269         ):\n-&gt; 5270             return object.__getattribute__(self, name)\n   5271         else:\n   5272             if self._info_axis._can_hold_identifiers_and_holds_name(name):\n\npandas\/_libs\/properties.pyx in pandas._libs.properties.AxisProperty.__get__()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pandas\/core\/generic.py in __getattr__(self, name)\n   5268             or name in self._accessors\n   5269         ):\n-&gt; 5270             return object.__getattribute__(self, name)\n   5271         else:\n   5272             if self._info_axis._can_hold_identifiers_and_holds_name(name):\n\nAttributeError: 'DataFrame' object has no attribute '_data'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619992853797,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|pickle|amazon-sagemaker",
        "Question_view_count":237,
        "Owner_creation_time":1553704286213,
        "Owner_last_access_time":1663944110353,
        "Owner_location":null,
        "Owner_reputation":27,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Can you check your Pandas versions? This error typically occurs when the pickled file was written in an old Pandas version. Your Sagemaker notebook probably runs Pandas &gt; 1.1 where as the Pandas in which the dataframe was pickled is probably &lt; 1.1<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1620025682633,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67361483",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52317237,
        "Question_title":"Machine Learning (NLP) on AWS. Cloud9? SageMaker? EC2-AMI?",
        "Question_body":"<p>I have finally arrived in the cloud to put my NLP work to the next level, but I am a bit overwhelmed with all the possibilities I have. So I am coming to you for advice.<\/p>\n\n<p>Currently I see three possibilities:<\/p>\n\n<ul>\n<li><strong>SageMaker<\/strong>\n\n<ul>\n<li>Jupyter Notebooks are great<\/li>\n<li>It's quick and simple<\/li>\n<li>saves a lot of time spent on managing everything, you can very easily get the model into production<\/li>\n<li>costs more<\/li>\n<li>no version control<\/li>\n<\/ul><\/li>\n<li><strong>Cloud9<\/strong><\/li>\n<li><strong>EC2(-AMI)<\/strong><\/li>\n<\/ul>\n\n<p>Well, that's where I am for now. I really like SageMaker, although I don't like the lack of version control (at least I haven't found anything for now).<\/p>\n\n<p>Cloud9 seems just to be an IDE to an EC2 instance.. I haven't found any comparisons of Cloud9 vs SageMaker for Machine Learning. Maybe because Cloud9 is not advertised as an ML solution. But it seems to be an option.<\/p>\n\n<p>What is your take on that question? What have I missed? What would you advise me to go for? What is your workflow and why? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1536853194713,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-ec2|nlp|amazon-sagemaker|aws-cloud9",
        "Question_view_count":1442,
        "Owner_creation_time":1473262595243,
        "Owner_last_access_time":1546872126263,
        "Owner_location":"Amsterdam, Netherlands",
        "Owner_reputation":23,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1540233603016,
        "Answer_body":"<blockquote>\n  <p>I am looking for an easy work environment where I can quickly test my models, exactly. And it won't be only me working on it, it's a team effort. <\/p>\n<\/blockquote>\n\n<p>Since you are working as a team I would recommend to use sagemaker with custom docker images. That way you have complete freedom over your algorithm. The docker images are stored in ecr. Here you can upload many versions of the same image and tag them to keep control of the different versions(which you build from a git repo).<\/p>\n\n<p>Sagemaker also gives the execution role to inside the docker image. So you still have full access to other aws resources (if the execution role has the right permissions)<\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a>\nIn my opinion this is a good example to start because it shows how sagemaker is interacting with your image.<\/p>\n\n<p><strong>Some notes on other solutions:<\/strong><\/p>\n\n<p>The problem of every other solution you posted is you want to build and execute on the same machine. Sure you can do this but keep in mind, that gpu instances are expensive and therefore you might only switch to the cloud when the code is ready to run.<\/p>\n\n<p><strong>Some other notes<\/strong><\/p>\n\n<ul>\n<li><p>Jupyter Notebooks in general are not made for collaborative programming. I think they want to change this with jupyter lab but this is still in development and sagemaker only use the notebook at the moment.<\/p><\/li>\n<li><p>EC2 is cheaper as sagemaker but you have  to do more work. Especially if you want to run your model as docker images. Also with sagemaker you can easily  build an endpoint for model inference which would be even more complex to realize with ec2.<\/p><\/li>\n<li><p>Cloud 9 I never used this service and but on first glance it seems good to develop on, but the question remains if you want to do this on a gpu machine. Because you're using ec2 as instance you have the same advantage\/disadvantage.<\/p><\/li>\n<\/ul>",
        "Answer_comment_count":0,
        "Answer_creation_time":1536933417933,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52317237",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65884046,
        "Question_title":"AWS Eventbridge Events (Sagemaker training job status change) fired multiple times with the same payload",
        "Question_body":"<p>I created an event rule for the Sagemaker training job state change in cloudwatch to monitor my training jobs. Then I use this events to trigger a lambda function that send messages in a telegram group as a bot. In this way I receive a message every time one of the training job change its status. It works but there is a problem with the events, they are fired multiple times with the same exact payload, so I receive tons of duplicate messages.\nSince all the payploads are identical (except the field <code>LastModifiedTime<\/code>) I cannot filter them in the lambda. Unfortunately I don't have the AWS Developer plan so I cannot receive support from Amazon. Any idea?<\/p>\n<p><strong>EDIT<\/strong><\/p>\n<p>There are no duplicate rules\/events. I also noticed that enabling the Sagemaker profiler (which is now by default) cause the number of identical rule invocations literally explode. All of them have the same payload except for the <code>LastModifiedTime<\/code> so I suspect that there is a bug in AWS for that. One solution could be to implement some sort of data retention on the lambda and check if an invocation has already been processed, but I don't want complicate a thing that should be very simple. Just tried to launch a new training job and got this sequence (I only report the fields I parse):<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Launching requested ML instances<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Starting the training job<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Starting the training job<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Starting the training job<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Preparing the instances for training<\/p>\n<p>Status: InProgress\nSecondary Status: Downloading\nStatus Message: Downloading input data<\/p>\n<p>Status: InProgress\nSecondary Status: Training\nStatus Message: Downloading the training image<\/p>\n<p>Status: InProgress\nSecondary Status: Training\nStatus Message: Training in-progres<\/p>\n<p>Status: InProgress\nSecondary Status: Training\nStatus Message: Training image download completed. Training in progress<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1611574816563,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|aws-event-bridge",
        "Question_view_count":780,
        "Owner_creation_time":1416346350293,
        "Owner_last_access_time":1664039219437,
        "Owner_location":"Jesi, Italy",
        "Owner_reputation":2302,
        "Owner_up_votes":51,
        "Owner_down_votes":4,
        "Owner_views":227,
        "Question_last_edit_time":1613131797383,
        "Answer_body":"<p>After a lot of experiments I can answer myself that Sagemaker generates multiple events with the same payload, except for the field <code>LastModifiedTime<\/code>. I don't know is this is a bug, but should not happen in my opinion. These are rules defined by AWS itself, so nothing I can customize. The situation is even worse if you enable the profiler.\nThere is nothing I can do, since I already posted on the official AWS forum multiple times without any luck.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1617006916700,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65884046",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63361229,
        "Question_title":"How do you write lifecycle configurations for SageMaker on windows?",
        "Question_body":"<p>I'm trying to set up a startup lifecycle configuration for a SageMaker sketchbook (which just ends up being a .sh file), and it seems like, regardless of what I do, my notebooks timeout on startup. I simplified everything as much as possible, to the point of commenting out all but <code>#!\/bin\/bash<\/code>, and I still get a timeout. Checking cloudwatch this shows up in the log:<\/p>\n<pre><code>\/bin\/bash: \/tmp\/OnStart_2020-08-11-07-01jgfhhkwa: \/bin\/bash^M: bad interpreter: No such file or directory\n<\/code><\/pre>\n<p>through testing, I also found that if I add a carriage return before <code>#!\/bin\/bash<\/code> I get this in the log:<\/p>\n<pre><code>\/tmp\/OnStart_2020-08-11-06-444y3fobzp: line 1: $'\\r': command not found\n<\/code><\/pre>\n<p>based on <a href=\"https:\/\/askubuntu.com\/questions\/966488\/how-do-i-fix-r-command-not-found-errors-running-bash-scripts-in-wsl\">this on the \\r error<\/a>, and <a href=\"https:\/\/stackoverflow.com\/questions\/14219092\/bash-script-and-bin-bashm-bad-interpreter-no-such-file-or-directory\">this on the ^M error<\/a>, this seems to be an incompatibility between windows and unix formatted text. However, I'm editing the lifecycle configuration through aws on my windows machine:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/A5oiU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/A5oiU.png\" alt=\"screenshot of editing the lifecycle config\" \/><\/a><\/p>\n<p>is there some way that I can edit this field on my windows machine through AWS, but it be properly written in unix on the other end?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597159003890,
        "Question_score":0,
        "Question_tags":"linux|windows|bash|amazon-web-services|amazon-sagemaker",
        "Question_view_count":608,
        "Owner_creation_time":1545360696800,
        "Owner_last_access_time":1664070875743,
        "Owner_location":"Earth",
        "Owner_reputation":1011,
        "Owner_up_votes":218,
        "Owner_down_votes":5,
        "Owner_views":93,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This is, indeed, to do with special character representation in different os' based on <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/issues\/8\" rel=\"nofollow noreferrer\">this<\/a> you can use notepad++ to easily convert the dos representation a unix representation, then just &quot;paste as plain text&quot;, and it works fine<\/p>\n<ul>\n<li>copy to notepad++ view<\/li>\n<li>show symbol<\/li>\n<li>show all symbols<\/li>\n<li>replace &quot;\/r&quot; with nothing CRLF should become LF which is valid in unix<\/li>\n<li>copy and paste as plain text<\/li>\n<\/ul>\n<p>Doing this fixed the problem<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1597164941183,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63361229",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57396212,
        "Question_title":"SageMaker and TensorFlow 2.0",
        "Question_body":"<p>What is the best way to run TensorFlow 2.0 with AWS Sagemeker?<\/p>\n\n<p>As of today (Aug 7th, 2019) AWS does not provide TensorFlow 2.0 <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"noreferrer\">SageMaker containers<\/a>, so my understanding is that I need to build my own.<\/p>\n\n<p>What is the best Base image to use? Example Dockerfile?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":1,
        "Question_creation_time":1565186451297,
        "Question_score":13,
        "Question_tags":"tensorflow|amazon-sagemaker|tensorflow2.0",
        "Question_view_count":4136,
        "Owner_creation_time":1446859510543,
        "Owner_last_access_time":1664052243983,
        "Owner_location":"Toronto, Canada",
        "Owner_reputation":3259,
        "Owner_up_votes":104,
        "Owner_down_votes":7,
        "Owner_views":233,
        "Question_last_edit_time":1565186790803,
        "Answer_body":"<p>EDIT: <strong>Amazon SageMaker does now support TF 2.0 and higher.<\/strong><\/p>\n<ul>\n<li>SageMaker + TensorFlow docs: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html<\/a><\/li>\n<li>Supported Tensorflow versions (and Docker URIs): <a href=\"https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images<\/a><\/li>\n<\/ul>\n<hr \/>\n<p><em>Original answer<\/em><\/p>\n<p>Here is an example Dockerfile that uses <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"nofollow noreferrer\">the underlying SageMaker Containers library<\/a> (this is what is used in the official pre-built Docker images):<\/p>\n<pre><code>FROM tensorflow\/tensorflow:2.0.0b1\n\nRUN pip install sagemaker-containers\n\n# Copies the training code inside the container\nCOPY train.py \/opt\/ml\/code\/train.py\n\n# Defines train.py as script entrypoint\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>For more information on this approach, see <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-container-to-train-script-get-started.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-container-to-train-script-get-started.html<\/a><\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1567635751827,
        "Answer_score":10,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1595008443227,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57396212",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60037376,
        "Question_title":"Can't use Keras CSVLogger callbacks in Sagemaker script mode. It fails to write the log file on S3 ( error - No such file or directory )",
        "Question_body":"<p>I have this script where I want to get the callbacks to a separate CSV file in sagemaker custom script docker container. But when I try to run in local mode, it fails giving the following error. I have a hyper-parameter tuning job(HPO) to run and this keeps giving me errors. I need to get this local mode run correctly before doing the HPO. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/de522.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/de522.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>In the notebook I use the following code.<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='lstm_model.py', \n                          role=role,\n                          code_location=custom_code_upload_location,\n                          output_path=model_artifact_location+'\/',\n                          train_instance_count=1, \n                          train_instance_type='local',\n                          framework_version='1.12', \n                          py_version='py3',\n                          script_mode=True,\n                          hyperparameters={'epochs': 1},\n                          base_job_name='hpo-lstm-local-test'\n                         )\n\ntf_estimator.fit({'training': training_input_path, 'validation': validation_input_path})\n<\/code><\/pre>\n\n<p>In my <strong>lstm_model.py<\/strong> script the following code is used.<\/p>\n\n<pre><code>lgdir = os.path.join(model_dir, 'callbacks_log.csv')\ncsv_logger = CSVLogger(lgdir, append=True)\n\nregressor.fit(x_train, y_train, batch_size=batch_size,\n              validation_data=(x_val, y_val), \n              epochs=epochs,\n              verbose=2,\n              callbacks=[csv_logger]\n              )\n<\/code><\/pre>\n\n<p>I tried creating a file before hand like shown below using tensorflow backend. But it doesn't create a file. ( K : tensorflow Backend, tf: tensorflow )<\/p>\n\n<pre><code>filename = tf.Variable(lgdir , tf.string)\ncontent = tf.Variable(\"\", tf.string)\nsess = K.get_session()\ntf.io.write_file(filename, content)\n<\/code><\/pre>\n\n<p>I can't use any other packages like pandas to create the file as the TensorFlow docker container in SageMaker for custom scripts doesn't provide them. They give only a limited amount of packages. <\/p>\n\n<p>Is there a way I can write the csv file to the S3 bucket location, before the fit method try to write the callback. Or is that the solution to the problem? I am not sure. <\/p>\n\n<p>If you can even suggest other suggestions to get callbacks, I would even accept that answer. But it should be worth the effort. <\/p>\n\n<p>This docker image is really narrowing the scope. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1580725861867,
        "Question_score":0,
        "Question_tags":"tensorflow|keras|amazon-sagemaker",
        "Question_view_count":412,
        "Owner_creation_time":1517147266417,
        "Owner_last_access_time":1663959762520,
        "Owner_location":null,
        "Owner_reputation":65,
        "Owner_up_votes":89,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Well for starters, you can always make your own docker image using the Tensorflow image as a base. I work in Tensorflow 2.0 so this will be slightly different for you but here is an example of my image pattern:<\/p>\n\n<pre><code># Downloads the TensorFlow library used to run the Python script\nFROM tensorflow\/tensorflow:2.0.0a0 # you would use the equivalent for your TF version\n\n# Contains the common functionality necessary to create a container compatible with Amazon SageMaker\nRUN pip install sagemaker-containers -q \n\n# Wandb allows us to customize and centralize logging while maintaining open-source agility\nRUN pip install wandb -q # here you would install pandas\n\n# Copies the training code inside the container to the design pattern created by the Tensorflow estimator\n# here you could copy over a callbacks csv\nCOPY mnist-2.py \/opt\/ml\/code\/mnist-2.py \nCOPY callbacks.py \/opt\/ml\/code\/callbacks.py \nCOPY wandb_setup.sh \/opt\/ml\/code\/wandb_setup.sh\n\n# Set the login script as the entry point\nENV SAGEMAKER_PROGRAM wandb_setup.sh # here you would instead launch lstm_model.py\n<\/code><\/pre>\n\n<p>I believe you are looking for a pattern similar to this, but I prefer to log all of my model data using <a href=\"https:\/\/www.wandb.com\/\" rel=\"nofollow noreferrer\">Weights and Biases<\/a>. They're a little out of data on their SageMaker integration but I'm actually in the midst of writing an updated tutorial for them. It should certainly be finished this month and include logging and comparing runs from hyperparameter tuning jobs<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1583860547413,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60037376",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62462790,
        "Question_title":"How can I get current job-name in SageMaker training job script?",
        "Question_body":"<p>I write some training job on AWS-SageMaker framework.<\/p>\n\n<p>For some it's requirements, it needs know the job-name of which current running on.<\/p>\n\n<p>I know this code works for it ...<\/p>\n\n<pre><code>import sagemaker_containers\nenv = sagemaker_containers.training_env()\njob_name = env['job_name']\n<\/code><\/pre>\n\n<p>But <code>sagemaker_containers<\/code> package has been deprecated. (I read that on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"nofollow noreferrer\">it's GitHub<\/a>)<\/p>\n\n<p>What should i do?<\/p>\n\n<p>I just started learning about this platform last month. I would appreciate any advice. Thank you.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592537326547,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":753,
        "Owner_creation_time":1401925028590,
        "Owner_last_access_time":1646876872843,
        "Owner_location":null,
        "Owner_reputation":113,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For older containers using the deprecated <code>sagemaker_containers<\/code>, the approach you described is correct.<\/p>\n<p>For newer containers that use <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\" rel=\"nofollow noreferrer\"><code>sagemaker-training-toolkit<\/code><\/a>, this is how you retrieve information about the environment: <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit#get-information-about-the-container-environment\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit#get-information-about-the-container-environment<\/a><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker_training import environment\n\nenv = environment.Environment()\n\njob_name = env[&quot;job_name&quot;]\n<\/code><\/pre>\n<p>You can check the <a href=\"https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/dlc-release-notes.html\" rel=\"nofollow noreferrer\">DLC Release Notes<\/a> to see what's installed in each version.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1593541151550,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62462790",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68251533,
        "Question_title":"Why is AWS Sagemaker notebook instance designed to only persist data under ~\/Sagemaker?",
        "Question_body":"<p>In my current job we use AWS managed notebooks on Sagemaker EC2. I am largely okay with the user experience but the lack of data persistency outside <code>~\/Sagemaker<\/code> has been quite inconvenient. Every time should the instance need restarting, I'd lose all the settings and python packages. Wonder why AWS would make this particular decision for Sagemaker. Have used Google Cloud's AI platform before and it does not have such settings and my configurations would always persist.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625466236780,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":266,
        "Owner_creation_time":1408744280997,
        "Owner_last_access_time":1664070721297,
        "Owner_location":"Vancouver, BC, Canada",
        "Owner_reputation":2758,
        "Owner_up_votes":603,
        "Owner_down_votes":2,
        "Owner_views":122,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I faced a similar issue on other AWS services. Usually for managed services AWS uses read-only containers approach and leave just one folder of the filesystem for read\/write that persist across the stop\/restart cycle.\nReguarding the packages installation the seems to be to install your custom environment on the notebook instance's Amazon EBS volume, as described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1625487892607,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68251533",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56422325,
        "Question_title":"AWS Sagemaker - Either the training channel is empty or the mini-batch size is too high",
        "Question_body":"<p>I am trying to train a linear learner model in Sagemaker. My training set is 422 rows split into 4 files on AWS S3. The mini-batch size that I set is 50. <\/p>\n\n<p>I keep on getting this error in Sagemaker.<\/p>\n\n<blockquote>\n  <p>Customer Error: No training data processed. Either the training\n  channel is empty or the mini-batch size is too high. Verify that\n  training data contains non-empty files and the mini-batch size is less\n  than the number of records per training host.<\/p>\n<\/blockquote>\n\n<p>I am using this InputDataConfig<\/p>\n\n<pre><code>InputDataConfig=[\n            {\n                'ChannelName': 'train',\n                'DataSource': {\n                    'S3DataSource': {\n                        'S3DataType': 'S3Prefix',\n                        'S3Uri': 's3:\/\/MY_S3_BUCKET\/REST_OF_PREFIX\/exported\/',\n                        'S3DataDistributionType': 'FullyReplicated'\n                    }\n                },\n                'ContentType': 'text\/csv',\n                'CompressionType': 'Gzip'\n            }\n        ],\n<\/code><\/pre>\n\n<p>I am not sure what I am doing wrong here. I tried increasing the number of records to 5547495 split across 6 files. The same error. That makes me think that somehow the config itself has something missing. Due to which it seems to think training channel is just not present. I tried changing 'train' to 'training' as that is what the erorr message is saying. But then I got <\/p>\n\n<blockquote>\n  <p>Customer Error: Unable to initialize the algorithm. Failed to validate\n  input data configuration. (caused by ValidationError)<\/p>\n  \n  <p>Caused by: {u'training': {u'TrainingInputMode': u'Pipe',\n  u'ContentType': u'text\/csv', u'RecordWrapperType': u'None',\n  u'S3DistributionType': u'FullyReplicated'}} is not valid under any of\n  the given schemas<\/p>\n<\/blockquote>\n\n<p>I went back to train as that seems to be what is needed. But what am I doing wrong with that? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559544607823,
        "Question_score":0,
        "Question_tags":"boto3|amazon-sagemaker",
        "Question_view_count":599,
        "Owner_creation_time":1364896706347,
        "Owner_last_access_time":1664082840050,
        "Owner_location":"Noida, India",
        "Owner_reputation":6584,
        "Owner_up_votes":477,
        "Owner_down_votes":15,
        "Owner_views":962,
        "Question_last_edit_time":1559545639143,
        "Answer_body":"<p>Found the problem. The CompressionType was mentioned as 'Gzip' but I had changed the actual file to be not compressed when doing the exports. As soon as I changed it to be 'None' the training went smoothly.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1559566976623,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56422325",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66692579,
        "Question_title":"AWS SageMaker: PermissionError: Access Denied - Reading data from S3 bucket",
        "Question_body":"<p>I am using AWS SageMaker. I already used it before and I had no problems reading data from an S3 bucket.\nSo, I set up a new notebook instance and id this:<\/p>\n<pre><code>from sagemaker import get_execution_role\nrole = get_execution_role()\n\nbucket='my-bucket'\n\ndata_key = 'myfile.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n\ndf = pd.read_csv(data_location)\n<\/code><\/pre>\n<p>What I got is this:<\/p>\n<pre><code>PermissionError: Access Denied\n<\/code><\/pre>\n<p>Note: I checked the IAM Roles and also the policies and it seems to me that I have all the necessary rights to access the S3 bucket (AmazonS3FullAccess etc. are granted). What is different from the situation before is that my data is encrypted. Is there something I have to set up besides the roles?<\/p>\n<p>Edit:<\/p>\n<p>The role I use consist of three policies. These are<\/p>\n<ul>\n<li>AmazonS3FullAccess<\/li>\n<li>AmazonSageMakerFullAccess<\/li>\n<\/ul>\n<p>and an Execution Role where I added kms:encrypt and kms:decrypt. It looks like this one:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;xyz&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:PutObject&quot;,\n                &quot;s3:GetObject&quot;,\n                &quot;s3:ListBucket&quot;,\n                &quot;s3:DeleteObject&quot;,\n                &quot;kms:Encrypt&quot;,\n                &quot;kms:Decrypt&quot;\n            ],\n            &quot;Resource&quot;: &quot;arn:aws:s3:::*&quot;\n        }\n    ]\n}\n<\/code><\/pre>\n<p>Is there something missing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1616075760093,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|error-handling|amazon-sagemaker",
        "Question_view_count":1327,
        "Owner_creation_time":1559131080073,
        "Owner_last_access_time":1663924217557,
        "Owner_location":null,
        "Owner_reputation":1166,
        "Owner_up_votes":555,
        "Owner_down_votes":1,
        "Owner_views":248,
        "Question_last_edit_time":1616077230863,
        "Answer_body":"<p>You need to add (or modify) an IAM policy to grant access to the key the bucket uses for its encryption:<\/p>\n<pre><code>{\n  &quot;Sid&quot;: &quot;KMSAccess&quot;,\n  &quot;Action&quot;: [\n    &quot;kms:Decrypt&quot;\n  ],\n  &quot;Effect&quot;: &quot;Allow&quot;,\n  &quot;Resource&quot;: &quot;arn:aws:kms:example-region-1:123456789098:key\/111aa2bb-333c-4d44-5555-a111bb2c33dd&quot;\n}\n<\/code><\/pre>\n<p>Alternatively you can change the key policy of the KMS key directly to grant the sagemaker role access directly. <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/s3-bucket-access-default-encryption\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/s3-bucket-access-default-encryption\/<\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1616081046907,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66692579",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58325923,
        "Question_title":"Deepar Prediction Quantiles Explained",
        "Question_body":"<p>I am working with Deepar and trying to get a better understanding of the quantile values returned. From the documentation, the likelihood hyperparameter explains that: <code>...provide quantiles of the distribution and return samples<\/code>. <\/p>\n\n<p>If I look at a single data point the quantiles returned are linear. E.g. the 0.1 quantile has the lowest predicted value and 0.9 quantile has the highest predicted value. I am having trouble understanding this. If these are samples from the distribution, shouldn't they look similar to the distribution selected with the likelihood hyperparameter (negative-binomial in my case)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1570719704473,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":568,
        "Owner_creation_time":1457123770467,
        "Owner_last_access_time":1663868088727,
        "Owner_location":"Denver",
        "Owner_reputation":246,
        "Owner_up_votes":39,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":"<p>DeepAR returns probabilistic forecasts in terms of quantiles: by default, the 0.1, 0.2, 0.3, ..., 0.9 quantiles are returned. This means that, according to the model, in each future time step you have 10% chance of observing something lower than the 0.1 quantile, 20% chance of observing something lower than the 0.2 quantile, and so on. Quantiles are in fact in order, and they must be by definition of quantile. Hope this clarifies is a bit!<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1573131591227,
        "Answer_score":4,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58325923",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":70248817,
        "Question_title":"Shared input in Sagemaker inference pipeline models",
        "Question_body":"<p>I'm deploying a SageMaker inference pipeline composed of two PyTorch models (<code>model_1<\/code> and <code>model_2<\/code>), and I am wondering if it's possible to pass the same input to both the models composing the pipeline.<\/p>\n<p>What I have in mind would work more or less as follows<\/p>\n<ol>\n<li><p>Invoke the endpoint sending a binary encoded payload (namely <code>payload_ser<\/code>), for example:<\/p>\n<pre><code>client.invoke_endpoint(EndpointName=ENDPOINT,\n                       ContentType='application\/x-npy',\n                       Body=payload_ser)\n<\/code><\/pre>\n<\/li>\n<li><p>The first model parses the payload with <code>inut_fn<\/code> function, runs the predictor on it, and returns the output of the predictor. As a simplified example:<\/p>\n<pre><code>def input_fn(request_body, request_content_type):\n    if request_content_type == &quot;application\/x-npy&quot;:\n        input = some_function_to_parse_input(request_body)\n    return input\n\ndef predict_fn(input_object, predictor):\n    outputs = predictor(input_object)\n    return outputs\n\ndef output_fn(predictions, response_content_type):\n    return json.dumps(predictions)\n<\/code><\/pre>\n<\/li>\n<li><p>The second model gets as payload both the original payload (<code>payload_ser<\/code>) and the output of the previous model (predictions). Possibly, the <code>input_fn<\/code> function would be used to parse the output of model_1 (as in the &quot;standard case&quot;), but I'd need some way to also make the original payload available to model_2.  In this way, model_2 will use both the original payload and the output of model_1 to make the final prediction and return it to whoever invoked the endpoint.<\/p>\n<\/li>\n<\/ol>\n<p>Any idea if this is achievable?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638808672663,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":210,
        "Owner_creation_time":1508881117760,
        "Owner_last_access_time":1663930396547,
        "Owner_location":null,
        "Owner_reputation":157,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Sounds like you need an inference DAG. Amazon SageMaker Inference pipelines currently supports only a chain of handlers, where the output of handler N is the input for handler N+1.<\/p>\n<p>You could change model1's predict_fn() to return both (input_object, outputs), and output_fn(). output_fn() will receive these two objects as the predictions, and will handle serializing both as json. model2's input_fn() will need to know how to parse this pair input.<\/p>\n<p>Consider implementing this as a generic pipeline handling mechanism that adds the input to the model's output. This way you could reuse it for all models and pipelines.<\/p>\n<p>You could allow the model to be deployed as a standalone model, and as a part of a pipeline, and apply the relevant input\/output handling behavior that will be triggered by the presence of an environment variable (<code>Environment<\/code> dict), which you can specify when <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model\" rel=\"nofollow noreferrer\">creating<\/a> the inference pipelines model.<\/p>",
        "Answer_comment_count":5,
        "Answer_creation_time":1638812806037,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70248817",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54408673,
        "Question_title":"Getting sagemaker container locally",
        "Question_body":"<p>When I try to run sagemaker locally for tensorflow in script mode. It seems like I cannot pull the docker container. I have ran the code below from a sagemaker notebook instance and everything ran fine. But when running it on my machine it doesn't work.<\/p>\n\n<p>How can I download the container, so I can debug things locally?<\/p>\n\n<pre><code>import os\n\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\n\n\nhyperparameters = {}\nrole = 'arn:aws:iam::xxxxxxxx:role\/yyyyyyy'\nestimator = TensorFlow(\n    entry_point='train.py',\n    source_dir='.',\n    train_instance_type='local',\n    train_instance_count=1,\n    hyperparameters=hyperparameters,\n    role=role,\n    py_version='py3',\n    framework_version='1.12.0',\n    script_mode=True)\n\nestimator.fit()\n<\/code><\/pre>\n\n<p>I get this output<\/p>\n\n<pre><code>INFO:sagemaker:Creating training-job with name: sagemaker-tensorflow-\nscriptmode-2019-01-28-18-51-57-787\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nError response from daemon: pull access denied for 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode, repository does not exist or may require 'docker login'\n\nsubprocess.CalledProcessError: Command 'docker pull 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.12.0-cpu-py3' returned non-zero exit status 1.\n<\/code><\/pre>\n\n<p>The warning looks like the output you get when using the docker login stuff <a href=\"https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/Registries.html\" rel=\"nofollow noreferrer\">here<\/a>. If I follow these steps to register to the directory with tensorflow container it says login success<\/p>\n\n<pre><code>Invoke-Expression -Command (aws ecr get-login --no-include-email --registry-ids 520713654638 --region eu-west-2)\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\n<\/code><\/pre>\n\n<p>But then I still cannot pull it<\/p>\n\n<pre><code>docker pull 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.11.0-cpu-py3\nError response from daemon: pull access denied for 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode, repository does not exist or may require 'docker login'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548702401737,
        "Question_score":2,
        "Question_tags":"amazon-web-services|docker|amazon-sagemaker",
        "Question_view_count":2235,
        "Owner_creation_time":1352206833663,
        "Owner_last_access_time":1664011820603,
        "Owner_location":null,
        "Owner_reputation":893,
        "Owner_up_votes":112,
        "Owner_down_votes":2,
        "Owner_views":185,
        "Question_last_edit_time":null,
        "Answer_body":"<p>the same sequence works for me locally : 'aws ecr get-login', 'docker login', 'docker pull'. <\/p>\n\n<p>Does your local IAM user have sufficient credentials to pull from ECR? The 'AmazonEC2ContainerRegistryReadOnly' policy should be enough: <a href=\"https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/ecr_managed_policies.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/ecr_managed_policies.html<\/a><\/p>\n\n<p>Alternatively, you can grab the container from Github and build it: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a><\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1548750603277,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54408673",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68479297,
        "Question_title":"AWS send image to Sagemaker from Lambda: how to set content handling?",
        "Question_body":"<p>similar question to\n<a href=\"https:\/\/stackoverflow.com\/a\/66683538\/6896705\">AWS Lambda send image file to Amazon Sagemaker<\/a><\/p>\n<p>I try to make simple-mnist work (the model was built by referring to <a href=\"https:\/\/sagemaker-immersionday.workshop.aws\/en\/lab3\/option1.html\" rel=\"nofollow noreferrer\">aws tutorial<\/a>)<\/p>\n<p>Then I am using API gateway (REST API w\/ proxy integration) to post image data to lambda, and would like to send it to sagemaker endpoint and make an inference.<\/p>\n<p>In lambda function, I wrote the code(.py) like this.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>runtime = boto3.Session().client('sagemaker-runtime')\n\nendpoint_name = 'tensorflow-training-YYYY-mm-dd-...'\nres = runtime.invoke_endpoint(EndpointName=endpoint_name,\n                              Body=Image,\n                              ContentType='image\/jpeg',\n                              Accept='image\/jpeg')\n<\/code><\/pre>\n<p>However, when I send image to lambda via API gateway, this error occurs.<\/p>\n<blockquote>\n<p>[ERROR] ModelError: An error occurred (ModelError) when calling the\nInvokeEndpoint operation: Received client error (415) from model with\nmessage &quot; {\n&quot;error&quot;: &quot;Unsupported Media Type: image\/jpeg&quot; }<\/p>\n<\/blockquote>\n<p>I think I need to do something referring to <a href=\"https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/api-gateway-payload-encodings.html\" rel=\"nofollow noreferrer\">Working with binary media types for REST APIs\n<\/a><\/p>\n<p>But since I am very new, I have no idea about the appropriate thing to do, on which page (maybe API Gateway page?) or how...<\/p>\n<p>I need some clues to solve this problem. Thank you in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626929018517,
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":390,
        "Owner_creation_time":1475109151950,
        "Owner_last_access_time":1652793680043,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":1626941375923,
        "Answer_body":"<p>Looking <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/adapt-inference-container.html\" rel=\"nofollow noreferrer\">here<\/a> you can see that only some specific content types are supported by default, and images are not in this list. I think you have to either implement your <code>input_fn<\/code> function or adapt your data to one of the supported content types.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1626967022753,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68479297",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54295445,
        "Question_title":"TensorFlow Serving send data as b64 instead of Numpy Array",
        "Question_body":"<p>I have a TensorFlow Serving container in a SageMaker endpoint. I'm able to take a batch of images as a Numpy array and get back predictions like this:<\/p>\n\n<pre><code>import numpy as np\nimport sagemaker\nfrom sagemaker.predictor import json_serializer, json_deserializer\n\nimage = np.random.uniform(low=-1.0, high=1.0, size=(1,128,128,3)).astype(np.float32)    \nimage = {'instances': image}\nimage = json_serializer(image)\n\nrequest_args = {}\nrequest_args['Body'] = image\nrequest_args['EndpointName'] = endpoint_name\nrequest_args['ContentType'] = 'application\/json'\nrequest_args['Accept'] = 'application\/json'\n\n# works successfully\nresponse = sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\nresponse_body = response['Body']\npredictions = json_deserializer(response_body, response['ContentType'])\n<\/code><\/pre>\n\n<p>The size of the <code>request_args<\/code> payload is large doing it this way. I'm wondering, is there a way to send this in a more compressed format? <\/p>\n\n<p>I've tried experimenting with <code>base64<\/code> and <code>json.dumps<\/code>, but can't get past <code>Invalid argument: JSON Value: ...<\/code> errors. Not sure if this isn't supported or if I'm just doing it incorrectly.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548093365813,
        "Question_score":1,
        "Question_tags":"python|image|base64|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":796,
        "Owner_creation_time":1361339272693,
        "Owner_last_access_time":1663965928400,
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I've talked to AWS support about this (see <a href=\"https:\/\/stackoverflow.com\/questions\/54090270\/more-efficient-way-to-send-a-request-than-json-to-deployed-tensorflow-model-in-s\">More efficient way to send a request than JSON to deployed tensorflow model in Sagemaker?<\/a>).<\/p>\n\n<p>They suggest that it is possible to pass in a custom input_fn that will be used by the serving container where one can unpack a compressed format (such as protobuf).<\/p>\n\n<p>I'll be testing this soon and hopefully this stuff works since it would add a lot of flexibility to the input processing.<\/p>",
        "Answer_comment_count":5,
        "Answer_creation_time":1548251290417,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54295445",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":53154542,
        "Question_title":"AWS SageMaker - Realtime Data Processing",
        "Question_body":"<p>My company does online consumer behavior analysis and we do realtime predictions using the data we collected from various websites (with our java script embedded). <\/p>\n\n<p>We have been using AWS ML for real time prediction but now we are experimenting with AWS SageMaker it occurred to us that the realtime data processing is a problem compared to AWS ML. For example we have some string variables that AWS ML can convert to numerics and use them for real time prediction in AWS ML automatically. But it does not look like SageMaker can do it. <\/p>\n\n<p>Does anyone have any experience with real time data processing and prediction in AWS SageMaker?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1541421249963,
        "Question_score":4,
        "Question_tags":"prediction|amazon-sagemaker",
        "Question_view_count":1508,
        "Owner_creation_time":1481650459980,
        "Owner_last_access_time":1661430875243,
        "Owner_location":null,
        "Owner_reputation":1446,
        "Owner_up_votes":102,
        "Owner_down_votes":1,
        "Owner_views":107,
        "Question_last_edit_time":null,
        "Answer_body":"<p>AWS SageMaker is a robust machine learning service in AWS that manages every major aspect of machine learning implementation, including data preparation, model construction, training and fine-tuning, and deployment.<\/p>\n<p><strong>Preparation<\/strong><\/p>\n<p>SageMaker uses a range of resources to make it simple to prepare data for machine learning models, even though it comes from many sources or is in a variety of formats.<\/p>\n<p>It's simple to mark data, including video, images, and text, that's automatically processed into usable data, with SageMaker Ground Truth. GroundWork will process and merge this data using auto-segmentation and a suite of tools to create a single data label that can be used in machine learning models. AWS, in conjunction with SageMaker Data Wrangler and SageMaker Processing, reduces a data preparation phase that may take weeks or months to a matter of days, if not hours.<\/p>\n<p><strong>Build<\/strong><\/p>\n<p>SageMaker Studio Notebooks centralize everything relevant to your machine learning models, allowing them to be conveniently shared along with their associated data. You can choose from a variety of built-in, open-source algorithms to start processing your data with SageMaker JumpStart, or you can build custom parameters for your machine learning model.<\/p>\n<p>Once you've chosen a model, SageMaker starts processing data automatically and offers a simple, easy-to-understand interface for tracking your model's progress and performance.<\/p>\n<p><strong>Training<\/strong><\/p>\n<p>SageMaker provides a range of tools for training your model from the data you've prepared, including a built-in debugger for detecting possible errors.<\/p>\n<p>Machine Learning\nThe training job's results are saved in an Amazon S3 bucket, where they can be viewed using other AWS services including AWS Quicksight.<\/p>\n<p><strong>Deployment<\/strong><\/p>\n<p>It's pointless to have strong machine learning models if they can't be easily deployed to your hosting infrastructure. Fortunately, SageMaker allows deploying machine learning models to your current services and applications as easy as a single click.<\/p>\n<p>SageMaker allows for real-time data processing and prediction after installation. This has far-reaching consequences in a variety of areas, including finance and health. Businesses operating in the stock market, for example, may make real-time financial decisions about stock and make more attractive acquisitions by pinpointing the best time to buy.<\/p>\n<p>Incorporation with Amazon Comprehend, allows for natural language processing, transforming human speech into usable data to train better models, or provide a chatbot to customers through Amazon Lex.<\/p>\n<p><strong>In conclusion\u2026<\/strong><\/p>\n<p>Machine Learning is no longer a niche technological curiosity; it now plays a critical role in the decision-making processes of thousands of companies around the world. There has never been a better time to start your Machine Learning journey than now, with virtually unlimited frameworks and simple integration into the AWS system.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1620131934800,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53154542",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65609804,
        "Question_title":"How to append stepfunction execution id to SageMaker job names?",
        "Question_body":"<p>I have a step function statemachine which creates SageMaker batch transform job, the definition is written in Terraform, I wanted to add the stepfunction execution id to the batch transform job names:<\/p>\n<p>in stepfunction terraform file:<\/p>\n<pre><code>  definition = templatefile(&quot;stepfuntion.json&quot;,\n    {\n      xxxx\n)\n<\/code><\/pre>\n<p>in the &quot;stepfuntion.json&quot;:<\/p>\n<pre><code>{...\n          &quot;TransformJobName&quot;: &quot;jobname-$$.Execution.Id&quot;,\n  \n          }\n      },\n        &quot;End&quot;: true\n      }\n    }\n  }\n<\/code><\/pre>\n<p>But after terraform apply, it didn't generate the actual id, it gave me <code>jobname-$$.Execution.Id<\/code>, can anyone help with this please?<\/p>\n<p>Resources: <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/input-output-contextobject.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/input-output-contextobject.html<\/a>\n&quot;To access the context object, first specify the parameter name by appending .$ to the end, as you do when selecting state input with a path. Then, to access context object data instead of the input, prepend the path with $$.. This tells AWS Step Functions to use the path to select a node in the context object.&quot;<\/p>\n<p>Can someone tell me what I'm missing please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610012100150,
        "Question_score":0,
        "Question_tags":"amazon-web-services|terraform|terraform-provider-aws|amazon-sagemaker|aws-step-functions",
        "Question_view_count":488,
        "Owner_creation_time":1540920956270,
        "Owner_last_access_time":1663875036883,
        "Owner_location":"United Kingdom",
        "Owner_reputation":2385,
        "Owner_up_votes":1007,
        "Owner_down_votes":16,
        "Owner_views":585,
        "Question_last_edit_time":1610014278923,
        "Answer_body":"<p>The var you are trying to use terraform doesn't know about it<\/p>\n<blockquote>\n<p>jobname-$$.Execution.Id.<\/p>\n<\/blockquote>\n<p>That's something specific to the Step function and available within state machine not available for terraform.<\/p>",
        "Answer_comment_count":17,
        "Answer_creation_time":1610017101707,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65609804",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61464960,
        "Question_title":"SageMaker multimodel and RandomCutForest",
        "Question_body":"<p>I am trying to invoke a MultiModel Endpoint with a RandomCutForest Model. I receive error though, 'Error loading model'. I can invoke the endpoint with models given from the examples.\nAm I missing something e.g. limitations on what models I can use? <\/p>\n\n<p>For MultiModel inspiration I am using below:<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb<\/a><\/p>\n  \n  <p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/<\/a><\/p>\n<\/blockquote>\n\n<p>I am trying to deploy the outputted 'model.tar.gz' from below RCF example in the MultiModel endpoint:<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb<\/a><\/p>\n<\/blockquote>\n\n<pre><code>model_name = 'model'\nfull_model_name = '{}.tar.gz'.format(model_name)\nfeatures = data\n\nbody = ','.join(map(str, features)) + '\\n'\nresponse = runtime_sm_client.invoke_endpoint(\n                    EndpointName=endpoint_name,\n                    ContentType='text\/csv',\n                    TargetModel=full_model_name,\n                    Body=body)\nprint(response)\n<\/code><\/pre>\n\n<p><strong>Cloudwatch log Error:<\/strong><\/p>\n\n<pre><code>&gt; 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Error loading model: Unable\n&gt; to load model: invalid load key, '{'. [17:28:59]\n&gt; \/workspace\/src\/learner.cc:334: Check failed: fi-&gt;Read(&amp;mparam_,\n&gt; sizeof(mparam_)) == sizeof(mparam_) (25 vs. 136) : BoostLearner: wrong\n&gt; model format 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Stack trace: 2020-04-27\n&gt; 17:28:59,005 [INFO ] W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (0)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x24)\n&gt; [0x7f37ce1cacb4] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9 com.amazonaws.ml.mms.wlm.WorkerThread\n&gt; - Backend response time: 0 2020-04-27 17:28:59,005 [INFO ] W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (1)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(xgboost::LearnerImpl::Load(dmlc::Stream*)+0x4b5)\n&gt; [0x7f37ce266985] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (2)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(XGBoosterLoadModel+0x37)\n&gt; [0x7f37ce1bf417] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (3)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c)\n&gt; [0x7f37ee993ec0] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (4)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d)\n&gt; [0x7f37ee99387d] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (5)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce)\n&gt; [0x7f37eeba91de] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (6)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12c14)\n&gt; [0x7f37eeba9c14] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (7)\n&gt; \/miniconda3\/bin\/python(_PyObject_FastCallKeywords+0x48b)\n&gt; [0x563d71b4218b] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (8)\n&gt; \/miniconda3\/bin\/python(_PyEval_EvalFrameDefault+0x52cf)\n&gt; [0x563d71b91e8f] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  2020-04-27 17:28:59,005\n&gt; [WARN ] W-9003-b39b888fb4a3fa6cf83bb34a9\n&gt; com.amazonaws.ml.mms.wlm.WorkerThread - Backend worker thread\n&gt; exception. java.lang.IllegalArgumentException: reasonPhrase contains\n&gt; one of the following prohibited characters: \\r\\n: Unable to load\n&gt; model: Unable to load model: invalid load key, '{'. [17:28:59]\n&gt; \/workspace\/src\/learner.cc:334: Check failed: fi-&gt;Read(&amp;mparam_,\n&gt; sizeof(mparam_)) == sizeof(mparam_) (25 vs. 136) : BoostLearner: wrong\n&gt; model format Stack trace:   [bt] (0)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x24)\n&gt; [0x7f37ce1cacb4]   [bt] (1)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(xgboost::LearnerImpl::Load(dmlc::Stream*)+0x4b5)\n&gt; [0x7f37ce266985]   [bt] (2)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(XGBoosterLoadModel+0x37)\n&gt; [0x7f37ce1bf417]   [bt] (3)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c)\n&gt; [0x7f37ee993ec0]   [bt] (4)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d)\n&gt; [0x7f37ee99387d]   [bt] (5)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce)\n&gt; [0x7f37eeba91de]   [bt] (6)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12c14)\n&gt; [0x7f37eeba9c14]   [bt] (7)\n&gt; \/miniconda3\/bin\/python(_PyObject_FastCallKeywords+0x48b)\n&gt; [0x563d71b4218b]   [bt] (8)\n&gt; \/miniconda3\/bin\/python(_PyEval_EvalFrameDefault+0x52cf)\n&gt; [0x563d71b91e8f]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1588009185817,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|xgboost|amazon-sagemaker",
        "Question_view_count":324,
        "Owner_creation_time":1426492930157,
        "Owner_last_access_time":1663146061523,
        "Owner_location":null,
        "Owner_reputation":398,
        "Owner_up_votes":37,
        "Owner_down_votes":1,
        "Owner_views":84,
        "Question_last_edit_time":null,
        "Answer_body":"<p>SageMaker Random Cut Forest is part of the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">built-in algorithm library<\/a> and cannot be deployed in multi-model endpoint (MME). Built-in algorithms currently cannot be deployed to MME. XGboost is an exception, since it has an open-source container <a href=\"https:\/\/github.com\/aws\/sagemaker-xgboost-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-xgboost-container<\/a>.<\/p>\n\n<p>If you really need to deploy a RCF to a multi-model endpoint, one option is to find a reasonably similar open-source implementation (for example <a href=\"https:\/\/github.com\/kLabUM\/rrcf\" rel=\"nofollow noreferrer\"><code>rrcf<\/code><\/a> looks reasonably serious: based <a href=\"http:\/\/proceedings.mlr.press\/v48\/guha16.pdf\" rel=\"nofollow noreferrer\">on the same paper from Guha et al<\/a> and with 170+ github stars) and create a custom MME docker container. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">documentation is here<\/a> and there is <a href=\"https:\/\/github.com\/giuseppeporcelli\/sagemaker-custom-serving-containers\/blob\/master\/multi-model-server-container\/notebook\/multi-model-server-container.ipynb\" rel=\"nofollow noreferrer\">an excellent tuto here<\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1588061581360,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61464960",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63568274,
        "Question_title":"How to use Serializer and Deserializer in Sagemaker 2",
        "Question_body":"<p>I spin up a Sagemaker notebook using the <code>conda_python3<\/code> kernel, and follow the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"noreferrer\">example<\/a> Notebook for <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/randomcutforest.html\" rel=\"noreferrer\">Random Cut Forest<\/a>.<\/p>\n<p>As of this writing, the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"noreferrer\">Sagemaker SDK<\/a> that comes with <code>conda_python3<\/code> is version 1.72.0, but I want to use new features, so I update my notebook to use the latest<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>%%bash\npip install -U sagemaker\n<\/code><\/pre>\n<p>And I see it updates.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>print(sagemaker.__version__)\n\n# 2.4.1\n<\/code><\/pre>\n<p>A change from version 1.x to 2.x was the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html#serializer-and-deserializer-classes\" rel=\"noreferrer\">serializer\/deserializer classes<\/a><\/p>\n<p>Previously (in version 1.72.0) I'd update my predictor to use the proper serializer\/deserializer, and could run inference on my model<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.predictor import csv_serializer, json_deserializer\n\n\nrcf_inference = rcf.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m4.xlarge',\n)\n\nrcf_inference.content_type = 'text\/csv'\nrcf_inference.serializer = csv_serializer\nrcf_inference.accept = 'application\/json'\nrcf_inference.deserializer = json_deserializer\n\nresults = rcf_inference.predict(some_numpy_array)\n<\/code><\/pre>\n<p>(Note this all comes from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"noreferrer\">example<\/a><\/p>\n<p>I try and replicate this using sagemaker 2.4.1 like so<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.deserializers import JSONDeserializer\nfrom sagemaker.serializers import CSVSerializer\n\nrcf_inference = rcf.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.xlarge',\n    serializer=CSVSerializer,\n    deserializer=JSONDeserializer\n)\n\nresults = rcf_inference.predict(some_numpy_array)\n<\/code><\/pre>\n<p>And I receive an error of<\/p>\n<pre><code>TypeError: serialize() missing 1 required positional argument: 'data'\n<\/code><\/pre>\n<p>I know I'm using the serliaizer\/deserializer incorrectly, but can't find good documentation on how this should be used<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1598300762780,
        "Question_score":8,
        "Question_tags":"python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":5910,
        "Owner_creation_time":1348841548500,
        "Owner_last_access_time":1663964727910,
        "Owner_location":null,
        "Owner_reputation":5182,
        "Owner_up_votes":1902,
        "Owner_down_votes":7,
        "Owner_views":315,
        "Question_last_edit_time":null,
        "Answer_body":"<p>in order to use the new serializers\/deserializers, you will need to init them, for example:<\/p>\n<pre><code>from sagemaker.deserializers import JSONDeserializer\nfrom sagemaker.serializers import CSVSerializer\n\nrcf_inference = rcf.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.xlarge',\n    serializer=CSVSerializer(),\n    deserializer=JSONDeserializer()\n)\n<\/code><\/pre>",
        "Answer_comment_count":2,
        "Answer_creation_time":1598503719773,
        "Answer_score":16,
        "Question_favorite_count":4.0,
        "Answer_last_edit_time":1598615993680,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63568274",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":59150100,
        "Question_title":"Sagemaker and Tensorflow model not saved",
        "Question_body":"<p>I am learning Sagemaker and I have this entry point:<\/p>\n\n<pre><code>import os\nimport tensorflow as tf\nfrom tensorflow.python.estimator.model_fn import ModeKeys as Modes\n\nINPUT_TENSOR_NAME = 'inputs'\nSIGNATURE_NAME = 'predictions'\n\nLEARNING_RATE = 0.001\n\n\ndef model_fn(features, labels, mode, params):\n    # Input Layer\n    input_layer = tf.reshape(features[INPUT_TENSOR_NAME], [-1, 28, 28, 1])\n\n    # Convolutional Layer #1\n    conv1 = tf.layers.conv2d(\n        inputs=input_layer,\n        filters=32,\n        kernel_size=[5, 5],\n        padding='same',\n        activation=tf.nn.relu)\n\n    # Pooling Layer #1\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n    # Convolutional Layer #2 and Pooling Layer #2\n    conv2 = tf.layers.conv2d(\n        inputs=pool1,\n        filters=64,\n        kernel_size=[5, 5],\n        padding='same',\n        activation=tf.nn.relu)\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n    # Dense Layer\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n    dropout = tf.layers.dropout(\n        inputs=dense, rate=0.4, training=(mode == Modes.TRAIN))\n\n    # Logits Layer\n    logits = tf.layers.dense(inputs=dropout, units=10)\n\n    # Define operations\n    if mode in (Modes.PREDICT, Modes.EVAL):\n        predicted_indices = tf.argmax(input=logits, axis=1)\n        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\n\n    if mode in (Modes.TRAIN, Modes.EVAL):\n        global_step = tf.train.get_or_create_global_step()\n        label_indices = tf.cast(labels, tf.int32)\n        loss = tf.losses.softmax_cross_entropy(\n            onehot_labels=tf.one_hot(label_indices, depth=10), logits=logits)\n        tf.summary.scalar('OptimizeLoss', loss)\n\n    if mode == Modes.PREDICT:\n        predictions = {\n            'classes': predicted_indices,\n            'probabilities': probabilities\n        }\n        export_outputs = {\n            SIGNATURE_NAME: tf.estimator.export.PredictOutput(predictions)\n        }\n        return tf.estimator.EstimatorSpec(\n            mode, predictions=predictions, export_outputs=export_outputs)\n\n    if mode == Modes.TRAIN:\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n        train_op = optimizer.minimize(loss, global_step=global_step)\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n\n    if mode == Modes.EVAL:\n        eval_metric_ops = {\n            'accuracy': tf.metrics.accuracy(label_indices, predicted_indices)\n        }\n        return tf.estimator.EstimatorSpec(\n            mode, loss=loss, eval_metric_ops=eval_metric_ops)\n\n\ndef serving_input_fn(params):\n    inputs = {INPUT_TENSOR_NAME: tf.placeholder(tf.float32, [None, 784])}\n    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n\n\ndef read_and_decode(filename_queue):\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\n            'image_raw': tf.FixedLenFeature([], tf.string),\n            'label': tf.FixedLenFeature([], tf.int64),\n        })\n\n    image = tf.decode_raw(features['image_raw'], tf.uint8)\n    image.set_shape([784])\n    image = tf.cast(image, tf.float32) * (1. \/ 255)\n    label = tf.cast(features['label'], tf.int32)\n\n    return image, label\n\n\ndef train_input_fn(training_dir, params):\n    return _input_fn(training_dir, 'train.tfrecords', batch_size=100)\n\n\ndef eval_input_fn(training_dir, params):\n    return _input_fn(training_dir, 'test.tfrecords', batch_size=100)\n\n\ndef _input_fn(training_dir, training_filename, batch_size=100):\n    test_file = os.path.join(training_dir, training_filename)\n    filename_queue = tf.train.string_input_producer([test_file])\n\n    image, label = read_and_decode(filename_queue)\n    images, labels = tf.train.batch(\n        [image, label], batch_size=batch_size,\n        capacity=1000 + 3 * batch_size)\n\n    return {INPUT_TENSOR_NAME: images}, labels\n\ndef neo_preprocess(payload, content_type):\n    import logging\n    import numpy as np\n    import io\n\n    logging.info('Invoking user-defined pre-processing function')\n\n    if content_type != 'application\/x-image' and content_type != 'application\/vnd+python.numpy+binary':\n        raise RuntimeError('Content type must be application\/x-image or application\/vnd+python.numpy+binary')\n\n    f = io.BytesIO(payload)\n    image = np.load(f)*255\n\n    return image\n\n### NOTE: this function cannot use MXNet\ndef neo_postprocess(result):\n    import logging\n    import numpy as np\n    import json\n\n    logging.info('Invoking user-defined post-processing function')\n\n    # Softmax (assumes batch size 1)\n    result = np.squeeze(result)\n    result_exp = np.exp(result - np.max(result))\n    result = result_exp \/ np.sum(result_exp)\n\n    response_body = json.dumps(result.tolist())\n    content_type = 'application\/json'\n\n    return response_body, content_type\n<\/code><\/pre>\n\n<p>And I am training it <\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='cnn_fashion_mnist.py',\n                       role=role,\n                       input_mode='Pipe',\n                       training_steps=1, \n                       evaluation_steps=1,\n                       train_instance_count=1,\n                       output_path=output_path,\n                       train_instance_type='ml.c5.2xlarge',\n                       base_job_name='mnist')\n<\/code><\/pre>\n\n<p>so far it is trying correctly and it tells me that everything when well, but when I check the output there is nothing there or if I try to deploy it I get the error saying it couldn't find the model because there is nothing in the bucker, any ideas or extra configurations? Thank you<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1575346450973,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":658,
        "Owner_creation_time":1462770189773,
        "Owner_last_access_time":1626440092163,
        "Owner_location":null,
        "Owner_reputation":45,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Looks like you are using one of the older Tensorflow versions.\nWe would recommend switching to a newer more straight-forward way of running Tensorflow in SageMaker (script mode) by switching to a more recent Tensorflow version.<\/p>\n\n<p>You can read more about it in our documentation:\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html<\/a><\/p>\n\n<p>Here is an example that might help:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/tensorflow_script_mode_training_and_serving.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/tensorflow_script_mode_training_and_serving.ipynb<\/a> <\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1576626745670,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59150100",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61212875,
        "Question_title":"Access Crowd HTML output results",
        "Question_body":"<p>I'm creating a website using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-ui-template-reference.html\" rel=\"nofollow noreferrer\">Crowd HTML Elements<\/a> that let users\/workers annotate images with the bounding box format.  The form looks like this:<\/p>\n\n<pre><code>&lt;crowd-form&gt;\n  &lt;crowd-bounding-box\n    name=\"annotatedResult\"\n    labels=\"['Referee', 'Player']\"\n    src=\"https:\/\/s3.amazonaws.com\/cv-demo-images\/basketball-outdoor.jpg\"\n    header=\"Draw boxes around each basketball player and referee in this image\"\n  &gt;\n    &lt;full-instructions header=\"Bounding Box Instructions\" &gt;\n      &lt;p&gt;Use the bounding box tool to draw boxes around the requested target of interest:&lt;\/p&gt;\n      &lt;ol&gt;\n        &lt;li&gt;Draw a rectangle using your mouse over each instance of the target.&lt;\/li&gt;\n        &lt;li&gt;Make sure the box does not cut into the target, leave a 2 - 3 pixel margin&lt;\/li&gt;\n        &lt;li&gt;\n          When targets are overlapping, draw a box around each object,\n          include all contiguous parts of the target in the box.\n          Do not include parts that are completely overlapped by another object.\n        &lt;\/li&gt;\n        &lt;li&gt;\n          Do not include parts of the target that cannot be seen,\n          even though you think you can interpolate the whole shape of the target.\n        &lt;\/li&gt;\n        &lt;li&gt;Avoid shadows, they're not considered as a part of the target.&lt;\/li&gt;\n        &lt;li&gt;If the target goes off the screen, label up to the edge of the image.&lt;\/li&gt;\n      &lt;\/ol&gt;\n    &lt;\/full-instructions&gt;\n\n    &lt;short-instructions&gt;\n      Draw boxes around each basketball player and referee in this image.\n    &lt;\/short-instructions&gt;\n  &lt;\/crowd-bounding-box&gt;\n&lt;\/crowd-form&gt;\n<\/code><\/pre>\n\n<p>The results of a worker's submission looks like the following:<\/p>\n\n<pre><code>  {\n    \"annotatedResult\": {\n      \"boundingBoxes\": [\n        {\n          \"height\": 3300,\n          \"label\": \"Dog\",\n          \"left\": 536,\n          \"top\": 154,\n          \"width\": 4361\n        }\n      ],\n      \"inputImageProperties\": {\n        \"height\": 3456,\n        \"width\": 5184\n      }\n    }\n  }\n]\n<\/code><\/pre>\n\n<p>I'd like to take this output and write it to a database, pass it to AWS Lambda, store it as metadata, etc. but I don't know how to access the results.  Is the JSON output a property of some HTML DOM property I can grab?<\/p>\n\n<p>I can attach a javascript function to the submit action of the crowd-form portion...<\/p>\n\n<pre><code>&lt;script&gt;\n  document.querySelector('crowd-form').onsubmit = function() {\n      ???\n  };\n&lt;\/script&gt;\n<\/code><\/pre>\n\n<p>...but I'm not sure what object I need to grab to get the results.  Thanks for your help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586882719607,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|mechanicalturk",
        "Question_view_count":371,
        "Owner_creation_time":1483444144907,
        "Owner_last_access_time":1643982821673,
        "Owner_location":"Hoth",
        "Owner_reputation":312,
        "Owner_up_votes":28,
        "Owner_down_votes":1,
        "Owner_views":65,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can access the bounding boxes during the onsubmit event like this:<\/p>\n\n<pre><code>&lt;script&gt;\n    document.querySelector('crowd-form').onsubmit = function(e) {\n      const boundingBoxes = document.querySelector('crowd-bounding-box').value.boundingBoxes || document.querySelector('crowd-bounding-box')._submittableValue.boundingBoxes;\n    }\n&lt;\/script&gt;\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/jsfiddle.net\/ap56djgq\/\" rel=\"nofollow noreferrer\">Here's<\/a> a working jsfiddle.<\/p>\n\n<p>Your use case sounds interesting. If you don't mind sharing, please email me at samhenry@amazon.com and I may be able to help further.<\/p>\n\n<p>Thank you,<\/p>\n\n<p>Amazon Mechanical Turk <\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1586884865260,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61212875",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64215998,
        "Question_title":"Why is Crowd HTML breaking this image?",
        "Question_body":"<p>I'm using Crowd HTML Elements to perform bounding box annotation, but when I attempt to load some of my images, I get this error in the dev tools console:<\/p>\n<pre><code>crowd-html-elements.js:1 window.onError received an event without an error:  {event: ErrorEvent}\n(anonymous) @ crowd-html-elements.js:1\nerror (async)\ne @ crowd-html-elements.js:1\ne @ crowd-html-elements.js:1\n.\/src\/crowd-html-elements-loader.ts @ crowd-html-elements.js:1\ns @ crowd-html-elements.js:1\n(anonymous) @ crowd-html-elements.js:1\n(anonymous) @ crowd-html-elements.js:1\ncrowd-html-elements-without-ce-polyfill.js:6282 window.onError received an event without an error:  {event: ErrorEvent}\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:6282\nerror (async)\ne @ crowd-html-elements-without-ce-polyfill.js:6282\ne @ crowd-html-elements-without-ce-polyfill.js:6282\n.\/src\/index.ts @ crowd-html-elements-without-ce-polyfill.js:6282\nr @ crowd-html-elements-without-ce-polyfill.js:1\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:1\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:1\ncrowd-html-elements-without-ce-polyfill.js:6282 Uncaught Error: Unexpected image dimensions during normalization\n    at Function.normalizeHeight (crowd-html-elements-without-ce-polyfill.js:6282)\n    at Function.normalizeDimensions (crowd-html-elements-without-ce-polyfill.js:6282)\n    at new a (crowd-html-elements-without-ce-polyfill.js:6282)\n    at ie.handleTargetImageLoaded (crowd-html-elements-without-ce-polyfill.js:6282)\n    at Image.r.onload (crowd-html-elements-without-ce-polyfill.js:6282)\nnormalizeHeight @ crowd-html-elements-without-ce-polyfill.js:6282\nnormalizeDimensions @ crowd-html-elements-without-ce-polyfill.js:6282\na @ crowd-html-elements-without-ce-polyfill.js:6282\nhandleTargetImageLoaded @ crowd-html-elements-without-ce-polyfill.js:6282\nr.onload @ crowd-html-elements-without-ce-polyfill.js:6282\nload (async)\nsetBackgroundImage @ crowd-html-elements-without-ce-polyfill.js:6282\nrenderImageSrcChange @ crowd-html-elements-without-ce-polyfill.js:6282\nshouldComponentUpdate @ crowd-html-elements-without-ce-polyfill.js:6282\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nG @ crowd-html-elements-without-ce-polyfill.js:6278\nw @ crowd-html-elements-without-ce-polyfill.js:6278\nS @ crowd-html-elements-without-ce-polyfill.js:6278\ne.reactMount @ crowd-html-elements-without-ce-polyfill.js:3\ne.updateRegion @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\ne.reactBatchUpdate @ crowd-html-elements-without-ce-polyfill.js:3\ni @ crowd-html-elements-without-ce-polyfill.js:3\nf.componentDidUpdate @ crowd-html-elements-without-ce-polyfill.js:3\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nG @ crowd-html-elements-without-ce-polyfill.js:6278\nw @ crowd-html-elements-without-ce-polyfill.js:6278\n_renderReactComponent @ crowd-html-elements-without-ce-polyfill.js:6282\n_updateReactComponent @ crowd-html-elements-without-ce-polyfill.js:6282\nY @ crowd-html-elements-without-ce-polyfill.js:5984\nC @ crowd-html-elements-without-ce-polyfill.js:5984\nk @ crowd-html-elements-without-ce-polyfill.js:5984\n_propertiesChanged @ crowd-html-elements-without-ce-polyfill.js:5984\n_flushProperties @ crowd-html-elements-without-ce-polyfill.js:5954\n_flushProperties @ crowd-html-elements-without-ce-polyfill.js:5984\n_invalidateProperties @ crowd-html-elements-without-ce-polyfill.js:5984\n_setProperty @ crowd-html-elements-without-ce-polyfill.js:5984\nObject.defineProperty.set @ crowd-html-elements-without-ce-polyfill.js:5954\n(anonymous) @ labeling.html:199\nasync function (async)\n(anonymous) @ labeling.html:198\nPromise.then (async)\n(anonymous) @ labeling.html:196\n<\/code><\/pre>\n<p>The <strong>Unexpected image dimensions during normalization<\/strong> portion seems like the issue, but I've found nothing with regard to troubleshooting.  Can someone explain what expected image dimensions are and why some are failing?<\/p>\n<p>Here's a snippet of the code that's throwing the error.<\/p>\n<pre><code>            static normalizeHeight(e) {\n                if (e.height === e.naturalHeight)\n                    return e.height;\n                if (e.height === e.naturalWidth)\n                    return e.height;\n                if (Math.abs(e.height - e.naturalHeight) &lt; 2)\n                    return e.naturalHeight;\n                if (Math.abs(e.height - e.naturalWidth) &lt; 2)\n                    return e.naturalWidth;\n                throw new Error(&quot;Unexpected image dimensions during normalization&quot;)\n            }\n            static normalizeWidth(e) {\n                if (e.width === e.naturalWidth)\n                    return e.width;\n                if (e.width === e.naturalHeight)\n                    return e.width;\n                if (Math.abs(e.width - e.naturalWidth) &lt; 2)\n                    return e.naturalWidth;\n                if (Math.abs(e.width - e.naturalHeight) &lt; 2)\n                    return e.naturalHeight;\n                throw new Error(&quot;Unexpected image dimensions during normalization&quot;)\n<\/code><\/pre>\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1601930941820,
        "Question_score":1,
        "Question_tags":"javascript|amazon-sagemaker|mechanicalturk",
        "Question_view_count":168,
        "Owner_creation_time":1483444144907,
        "Owner_last_access_time":1643982821673,
        "Owner_location":"Hoth",
        "Owner_reputation":312,
        "Owner_up_votes":28,
        "Owner_down_votes":1,
        "Owner_views":65,
        "Question_last_edit_time":1602868269100,
        "Answer_body":"<p>The issue turned out to be related to the css styling that was being applied to the canvas portion of my site that was loading the labeling tools.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1603367517400,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64215998",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62320331,
        "Question_title":"Sagemaker API to list Hyperparameters",
        "Question_body":"<p>I'm currently trying to implement MLFlow Tracking into my training pipeline and would like to log the hyperparameters of my hyperparameter Tuning of each training job.<\/p>\n\n<p>Does anyone know, how to pull the list of hyperparameters that can be seen on the sagemaker training job interface (on the AWS console)? Is there any other smarter way to list how models perform in comparison in Sagemaker (and displayed)?<\/p>\n\n<p>I would assume there must be an easy and Pythonic way to do this (either boto3 or the sagemaker api) to get this data. I wasn't able to find it in Cloudwatch.<\/p>\n\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1591864702897,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|mlflow",
        "Question_view_count":484,
        "Owner_creation_time":1469720117217,
        "Owner_last_access_time":1648482744487,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":"<p>there is indeed a rather pythonic way in the SageMaker python SDK:<\/p>\n\n<pre><code>tuner = sagemaker.tuner.HyperparameterTuner.attach('&lt; your tuning jobname&gt;')\n\nresults = tuner.analytics().dataframe()  # all your tuning metadata, in pandas!\n<\/code><\/pre>\n\n<p>See full example here <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1592173650467,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62320331",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61052173,
        "Question_title":"Is there a limit on input json string for aws sagemaker endpoint?",
        "Question_body":"<p>I have ~5MB json string that I want to send to my endpoint. I am using boto3.client to invoke the endpoint from my python client. It throws ConnectionResetError. <\/p>\n\n<pre><code>    File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\urllib3\\connectionpool.py\", line 600, in urlopen\n    chunked=chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\urllib3\\connectionpool.py\", line 354, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 1229, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\botocore\\awsrequest.py\", line 92, in _send_request\n    method, url, body, headers, *args, **kwargs)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 1275, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 1224, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\botocore\\awsrequest.py\", line 119, in _send_output\n    self.send(msg)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\botocore\\awsrequest.py\", line 203, in send\n    return super(AWSConnection, self).send(str)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 977, in send\n    self.sock.sendall(data)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\ssl.py\", line 1012, in sendall\n    v = self.send(byte_view[count:])\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\ssl.py\", line 981, in send\n    return self._sslobj.write(data)\nConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n<\/code><\/pre>\n\n<p>Looking at the trace, I am guessing it is due to json string size. Could someone please help me how to get around this? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586140541570,
        "Question_score":1,
        "Question_tags":"json|python-3.x|http-post|amazon-sagemaker|urllib3",
        "Question_view_count":1281,
        "Owner_creation_time":1584461369093,
        "Owner_last_access_time":1663592296570,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1586146490856,
        "Answer_body":"<p>Exceeding the payload size limit does result in a connection reset from the SageMaker Runtime service.<\/p>\n\n<p>From the SageMaker <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>Maximum payload size for endpoint invocation |    5 MB<\/p>\n<\/blockquote>\n\n<p>There are likely more space-efficient data formats than JSON that you could use to transmit the payload, but the available options will depend on the type of data and what model image you are using (i.e. whether Amazon-provided or a custom implementation).<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1586421549190,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61052173",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54026623,
        "Question_title":"AWS Sagemaker - ClientError: Data download failed",
        "Question_body":"<p><strong>Problem:<\/strong>\nI am trying to setup a model in Sagemaker, however it fails when it comes to downloading the data.\nDoes anyone know what I am doing wrong?<\/p>\n\n<p><strong>What I did so far<\/strong>:\nIn order to avoid any mistakes on my side I decided to use the AWS tutorial:\ntensorflow_iris_dnn_classifier_using_estimators<\/p>\n\n<p>And I made only two changes:<\/p>\n\n<ol>\n<li>I copied the dataset to my own S3 instance. --> I tested if I could access \/ show the data and it worked.<\/li>\n<li>I edited the path to point to the new folder.<\/li>\n<\/ol>\n\n<p>This is the AWS source code:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators<\/a><\/p>\n\n<pre><code>%%time\nimport boto3\n\n# use the region-specific sample data bucket\nregion = boto3.Session().region_name\n#train_data_location = 's3:\/\/sagemaker-sample-data-{}\/tensorflow\/iris'.format(region)\ntrain_data_location = 's3:\/\/my-s3-bucket'\n\niris_estimator.fit(train_data_location)\n<\/code><\/pre>\n\n<p>And this is the error I get:<\/p>\n\n<pre><code>\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/interactiveshell.pyc in run_cell_magic(self, magic_name, line, cell)\n   2115             magic_arg_s = self.var_expand(line, stack_depth)\n   2116             with self.builtin_trap:\n-&gt; 2117                 result = fn(magic_arg_s, cell)\n   2118             return result\n   2119 \n\n&lt;decorator-gen-60&gt; in time(self, line, cell, local_ns)\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/magic.pyc in &lt;lambda&gt;(f, *a, **k)\n    186     # but it's overkill for just that one bit of state.\n    187     def magic_deco(arg):\n--&gt; 188         call = lambda f, *a, **k: f(*a, **k)\n    189 \n    190         if callable(arg):\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/magics\/execution.pyc in time(self, line, cell, local_ns)\n   1191         else:\n   1192             st = clock2()\n-&gt; 1193             exec(code, glob, local_ns)\n   1194             end = clock2()\n   1195             out = None\n\n&lt;timed exec&gt; in &lt;module&gt;()\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/tensorflow\/estimator.pyc in fit(self, inputs, wait, logs, job_name, run_tensorboard_locally)\n    314                 tensorboard.join()\n    315         else:\n--&gt; 316             fit_super()\n    317 \n    318     @classmethod\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/tensorflow\/estimator.pyc in fit_super()\n    293 \n    294         def fit_super():\n--&gt; 295             super(TensorFlow, self).fit(inputs, wait, logs, job_name)\n    296 \n    297         if run_tensorboard_locally and wait is False:\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/estimator.pyc in fit(self, inputs, wait, logs, job_name)\n    232         self.latest_training_job = _TrainingJob.start_new(self, inputs)\n    233         if wait:\n--&gt; 234             self.latest_training_job.wait(logs=logs)\n    235 \n    236     def _compilation_job_name(self):\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/estimator.pyc in wait(self, logs)\n    571     def wait(self, logs=True):\n    572         if logs:\n--&gt; 573             self.sagemaker_session.logs_for_job(self.job_name, wait=True)\n    574         else:\n    575             self.sagemaker_session.wait_for_job(self.job_name)\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/session.pyc in logs_for_job(self, job_name, wait, poll)\n   1126 \n   1127         if wait:\n-&gt; 1128             self._check_job_status(job_name, description, 'TrainingJobStatus')\n   1129             if dot:\n   1130                 print()\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/session.pyc in _check_job_status(self, job, desc, status_key_name)\n    826             reason = desc.get('FailureReason', '(No reason provided)')\n    827             job_type = status_key_name.replace('JobStatus', ' job')\n--&gt; 828             raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))\n    829 \n    830     def wait_for_endpoint(self, endpoint, poll=5):\n\nValueError: Error for Training job sagemaker-tensorflow-2019-01-03-16-32-16-435: Failed Reason: ClientError: Data download failed:S3 key: s3:\/\/my-s3-bucket\/\/sagemaker-tensorflow-2019-01-03-14-02-39-959\/source\/sourcedir.tar.gz has an illegal char sub-sequence '\/\/' in it\n<\/code><\/pre>",
        "Question_answer_count":4,
        "Question_comment_count":2,
        "Question_creation_time":1546534753933,
        "Question_score":9,
        "Question_tags":"python|amazon-web-services|tensorflow|jupyter|amazon-sagemaker",
        "Question_view_count":5145,
        "Owner_creation_time":1546261116430,
        "Owner_last_access_time":1620827806300,
        "Owner_location":null,
        "Owner_reputation":115,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The script is expecting 'bucket' to be bucket = Session().default_bucket() or your own. Have you tried setting bucket equal to your personal bucket?<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1546594981603,
        "Answer_score":2,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54026623",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72096297,
        "Question_title":"Hyperparameter tuning job In Sagemaker with cross valdiation",
        "Question_body":"<p>I managed to get something <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-tuning-job.html\" rel=\"nofollow noreferrer\">along those lines<\/a> to work. This is great but to be more on the save side (i.e. not rely too much on the train validation split) one should really use cross validation. I am curious, if this can also be achieved via Sagemaker hyperparameter tuning jobs? I googled extensively ...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1651564321897,
        "Question_score":0,
        "Question_tags":"python|cross-validation|amazon-sagemaker|hyperparameters",
        "Question_view_count":70,
        "Owner_creation_time":1267440784443,
        "Owner_last_access_time":1664045779313,
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It is not possible through HPO.<\/p>\n<p>You need to add additional step in your workflow to achieve cross-validation.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1651629955790,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72096297",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":60006106,
        "Question_title":"NotImplementedError: Text mode not supported, use mode='wb' and manage bytes in s3fs",
        "Question_body":"<p>I know that there are a similar question but it is more general and not specific of this package. I am saving a pandas dataframe within a Sagemaker Jupyter notebook into a csv in S3 as follow:<\/p>\n\n<pre><code>df.to_csv('s3:\/\/bucket\/key\/file.csv', index=False)\n<\/code><\/pre>\n\n<p>However I am getting the following error:<\/p>\n\n<pre><code>NotImplementedError: Text mode not supported, use mode='wb' and manage bytes\n<\/code><\/pre>\n\n<p>The code more or less is that I read a csv from S3, make some preprocessing on it and then saves it to S3. I can read csv from S3 successfully with:<\/p>\n\n<pre><code>df.read_csv('s3:\/\/bucket\/key\/file.csv')\n<\/code><\/pre>\n\n<p>The object that I am trying to save to S3 is indeed a <em>pandas.core.frame.DataFrame<\/em><\/p>\n\n<p>In the notebook I can see using <code>!pip show package<\/code> that I have pandas 0.24.2 and s3fs 0.1.5. <\/p>\n\n<p>What could be the problem? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":5,
        "Question_creation_time":1580482050793,
        "Question_score":2,
        "Question_tags":"python|pandas|jupyter-notebook|amazon-sagemaker|python-s3fs",
        "Question_view_count":1738,
        "Owner_creation_time":1523298968403,
        "Owner_last_access_time":1663934452963,
        "Owner_location":null,
        "Owner_reputation":1754,
        "Owner_up_votes":396,
        "Owner_down_votes":76,
        "Owner_views":197,
        "Question_last_edit_time":1581088182972,
        "Answer_body":"<p>Can you Please try<\/p>\n\n<pre><code>df.to_csv(\"s3:\/\/bucket\/key\/file.csv\", index=False, mode='wb')\n<\/code><\/pre>\n\n<p>It should fix your error. The default mode is <strong>w<\/strong> which writes in the file system as text and not bytes. Where as s3 expects the data to be bytes. hence you have to specify mode as <strong>wb<\/strong>(write bytes) while writing the dataframe as csv to the filesystem.<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1580493486723,
        "Answer_score":3,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60006106",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":49374476,
        "Question_title":"How do I call a SageMaker Endpoint using the AWS CLI (",
        "Question_body":"<p>I'm trying to invoke the iris endpointfrom the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\/tensorflow_iris_dnn_classifier_using_estimators.ipynb\" rel=\"nofollow noreferrer\">SageMaker example notebooks<\/a> using the aws cli. I've tried using the following command:<\/p>\n\n<pre><code>!aws sagemaker-runtime invoke-endpoint \\\n--endpoint-name sagemaker-tensorflow-py2-cpu-2018-03-19-21-27-52-956 \\\n--body \"[6.4, 3.2, 4.5, 1.5]\" \\\n--content-type \"application\/json\" \\\noutput.json\n<\/code><\/pre>\n\n<p>I get the following response:<\/p>\n\n<pre><code>{\n    \"InvokedProductionVariant\": \"AllTraffic\", \n    \"ContentType\": \"*\/*\"\n}\n<\/code><\/pre>\n\n<p>What am I doing wrong?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1521505092823,
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":4027,
        "Owner_creation_time":1443201378360,
        "Owner_last_access_time":1653587950970,
        "Owner_location":null,
        "Owner_reputation":749,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If you've gotten that response, your request is successful. The output should be in the output file you specified - output.json :)<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1521567781630,
        "Answer_score":5,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49374476",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":61520346,
        "Question_title":"No such file or directory: 'docker': 'docker' when running sagemaker studio in local mode",
        "Question_body":"<p>I try to train a pytorch model on amazon sagemaker studio.<\/p>\n\n<p>It's working when I use an EC2 for training with:<\/p>\n\n<pre><code>estimator = PyTorch(entry_point='train_script.py',\n                role=role,\n                sagemaker_session = sess,\n                train_instance_count=1,\n                train_instance_type='ml.c5.xlarge',\n                framework_version='1.4.0', \n                source_dir='.',\n                git_config=git_config, \n               )\nestimator.fit({'stockdata': data_path})\n<\/code><\/pre>\n\n<p>and it's work on local mode in classic sagemaker notebook (non studio) with:<\/p>\n\n<pre><code> estimator = PyTorch(entry_point='train_script.py',\n                role=role,\n                train_instance_count=1,\n                train_instance_type='local',\n                framework_version='1.4.0', \n                source_dir='.',\n                git_config=git_config, \n               )\nestimator.fit({'stockdata': data_path})\n<\/code><\/pre>\n\n<p>But when I use it the same code (with train_instance_type='local') on sagemaker studio it doesn't work and I have the following error: No such file or directory: 'docker': 'docker'<\/p>\n\n<p>I tried to install docker with pip install but the docker command is not found if use it in terminal<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1588239469173,
        "Question_score":5,
        "Question_tags":"docker|pytorch|amazon-sagemaker",
        "Question_view_count":1884,
        "Owner_creation_time":1576136255053,
        "Owner_last_access_time":1663858972457,
        "Owner_location":null,
        "Owner_reputation":795,
        "Owner_up_votes":37,
        "Owner_down_votes":1,
        "Owner_views":37,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This indicates that there is a problem finding the Docker service.<\/p>\n<p>By default, the Docker is not installed in the SageMaker Studio  (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/656#issuecomment-632170943\" rel=\"nofollow noreferrer\">confirming github ticket response<\/a>).<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1596561017227,
        "Answer_score":7,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1620410809823,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61520346",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67123040,
        "Question_title":"How to tell programmatically that an AWS Step Function execution has been completed?",
        "Question_body":"<p>I am triggering a Step Function execution via a Python cell in a SageMaker Notebook, like this:<\/p>\n<pre><code>state_machine_arn = 'arn:aws:states:us-west-1:1234567891:stateMachine:alexanderMyPackageStateMachineE3411O13-A1vQWERTP9q9'\nsfn = boto3.client('stepfunctions')\n..\nsfn.start_execution(**kwargs)  # Non Blocking Call\nrun_arn = response['executionArn']\nprint(f&quot;Started run {run_name}. ARN is {run_arn}.&quot;)\n<\/code><\/pre>\n<p>and then in order to check that the execution (which might take hours to complete depending on the input) has been completed, before I start doing some custom post-analysis on the results, I manually execute a cell with:<\/p>\n<pre><code>response = sfn.list_executions(\n    stateMachineArn=state_machine_arn,\n    maxResults=1\n)\nprint(response)\n<\/code><\/pre>\n<p>where I can see from the output the status of the execution, e.g. <code>'status': 'RUNNING'<\/code>.<\/p>\n<p>How can I automate this, i.e. trigger the Step Function and continue the execution on my post-analysis custom logic only after the execution has finished? Is there for example a blocking call to start the execution, or a callback method I could use?<\/p>\n<p>I can think of putting a sleep method, so that the Python Notebook cell would periodically call <code>list_executions()<\/code> and check the status, and only when the execution is completed, continue to rest of the code. I can statistically determine the sleep period, but I was wondering if there is a simpler\/more accurate way.<\/p>\n<hr \/>\n<p>PS: Related: <a href=\"https:\/\/stackoverflow.com\/questions\/46878423\/how-to-avoid-simultaneous-execution-in-aws-step-function\">How to avoid simultaneous execution in aws step function<\/a>, however I would like to avoid creating any new AWS resource, just for this, I would like to do everything from within the Notebook.<\/p>\n<p>PPS: I cannot make any change to <code>MyPackage<\/code> and the Step Function definition.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1618566783027,
        "Question_score":4,
        "Question_tags":"python|amazon-web-services|asynchronous|amazon-sagemaker|aws-step-functions",
        "Question_view_count":1216,
        "Owner_creation_time":1369257942213,
        "Owner_last_access_time":1663776093950,
        "Owner_location":"London, UK",
        "Owner_reputation":70285,
        "Owner_up_votes":7595,
        "Owner_down_votes":12100,
        "Owner_views":13121,
        "Question_last_edit_time":1618585069787,
        "Answer_body":"<p>Based on the comments.<\/p>\n<p>If no new resources are to be created (no CloudWatch Event rules, lambda functions) nor any changes to existing Step Function are allowed, then <strong>pooling iteratively<\/strong> <code>list_executions<\/code> would be the best solution.<\/p>\n<p>AWS CLI and boto3 have implemented similar solutions (not for Step Functions), but for some other services. They are called <code>waiters<\/code> (e.g. <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/ec2.html#waiters\" rel=\"nofollow noreferrer\">ec2 waiters<\/a>). So basically you would have to create your own <strong>waiter for Step Function<\/strong>, as AWS does not provide one for that. AWS uses <strong>15 seconds<\/strong> sleep time from what I recall for its waiters.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1618568305233,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67123040",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69046990,
        "Question_title":"How to pass dependency files to sagemaker SKLearnProcessor and use it in Pipeline?",
        "Question_body":"<p>I need to import function from different python scripts, which will used inside <code>preprocessing.py<\/code> file. I was not able to find a way to pass the dependent files to <code>SKLearnProcessor<\/code> Object, due to which I am getting <code>ModuleNotFoundError<\/code>.<\/p>\n<p><strong>Code:<\/strong><\/p>\n<pre><code>from sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     instance_type='ml.m5.xlarge',\n                                     instance_count=1)\n\n\nsklearn_processor.run(code='preprocessing.py',\n                      inputs=[ProcessingInput(\n                        source=input_data,\n                        destination='\/opt\/ml\/processing\/input')],\n                      outputs=[ProcessingOutput(output_name='train_data',\n                                                source='\/opt\/ml\/processing\/train'),\n                               ProcessingOutput(output_name='test_data',\n                                                source='\/opt\/ml\/processing\/test')],\n                      arguments=['--train-test-split-ratio', '0.2']\n                     )\n<\/code><\/pre>\n<p>I would like to pass,\n<code>dependent_files = ['file1.py', 'file2.py', 'requirements.txt']<\/code>. So, that <code>preprocessing.py<\/code> have access to all the dependent modules.<\/p>\n<p>And also need to install libraries from <code>requirements.txt<\/code> file.<\/p>\n<p>Can you share any work around or a right way to do this?<\/p>\n<p><strong>Update-25-11-2021:<\/strong><\/p>\n<p><strong>Q1.<\/strong>(Answered but looking to solve using <code>FrameworkProcessor<\/code>)<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/99f023e76a5db060907a796d4d8fee550f005844\/src\/sagemaker\/processing.py#L1426\" rel=\"noreferrer\">Here<\/a>, the <code>get_run_args<\/code> function, is handling <code>dependencies<\/code>, <code>source_dir<\/code> and <code>code<\/code> parameters by using <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/99f023e76a5db060907a796d4d8fee550f005844\/src\/sagemaker\/processing.py#L1265\" rel=\"noreferrer\">FrameworkProcessor<\/a>. Is there any way that we can set this parameters from <code>ScriptProcessor<\/code> or <code>SKLearnProcessor<\/code> or any other <code>Processor<\/code> to set them?<\/p>\n<p><strong>Q2.<\/strong><\/p>\n<p>Can you also please show some reference to use our <code>Processor<\/code> as <code>sagemaker.workflow.steps.ProcessingStep<\/code> and then use in <code>sagemaker.workflow.pipeline.Pipeline<\/code>?<\/p>\n<p>For having <code>Pipeline<\/code>, do we need <code>sagemaker-project<\/code> as mandatory or can we create <code>Pipeline<\/code> directly without any <code>Sagemaker-Project<\/code>?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":5,
        "Question_creation_time":1630681185260,
        "Question_score":11,
        "Question_tags":"python|amazon-web-services|scikit-learn|amazon-sagemaker",
        "Question_view_count":2139,
        "Owner_creation_time":1500824148410,
        "Owner_last_access_time":1664025062210,
        "Owner_location":"India",
        "Owner_reputation":4419,
        "Owner_up_votes":434,
        "Owner_down_votes":324,
        "Owner_views":962,
        "Question_last_edit_time":1637936310430,
        "Answer_body":"<p>There are a couple of options for you to accomplish that.<\/p>\n<p>One that is really simple is adding all additional files to a folder, example:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 my_package\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 file1.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 file2.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 preprocessing.py\n<\/code><\/pre>\n<p>Then send this entire folder as another input under the same <code>\/opt\/ml\/processing\/input\/code\/<\/code>, example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor = SKLearnProcessor(\n    framework_version=&quot;0.20.0&quot;,\n    role=role,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    instance_count=1,\n)\n\nsklearn_processor.run(\n    code=&quot;preprocessing.py&quot;,  # &lt;- this gets uploaded as \/opt\/ml\/processing\/input\/code\/preprocessing.py\n    inputs=[\n        ProcessingInput(source=input_data, destination='\/opt\/ml\/processing\/input'),\n        # Send my_package as \/opt\/ml\/processing\/input\/code\/my_package\/\n        ProcessingInput(source='my_package\/', destination=&quot;\/opt\/ml\/processing\/input\/code\/my_package\/&quot;)\n    ],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],\n)\n<\/code><\/pre>\n<p>What happens is that <code>sagemaker-python-sdk<\/code> is going to put your argument <code>code=&quot;preprocessing.py&quot;<\/code> under <code>\/opt\/ml\/processing\/input\/code\/<\/code> and you will have <code>my_package\/<\/code> under the same directory.<\/p>\n<p><strong>Edit:<\/strong><\/p>\n<p>For the <code>requirements.txt<\/code>, you can add to your <code>preprocessing.py<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport subprocess\n\nsubprocess.check_call([\n    sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, &quot;-r&quot;,\n    &quot;\/opt\/ml\/processing\/input\/code\/my_package\/requirements.txt&quot;,\n])\n<\/code><\/pre>",
        "Answer_comment_count":5,
        "Answer_creation_time":1637782762627,
        "Answer_score":17,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":1637783228436,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69046990",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65464181,
        "Question_title":"An alternative to tf.distribute.cluster_resolver.TPUClusterResolver( tpu_name) to be used in Sagemaker?",
        "Question_body":"<ol>\n<li><p>task : object_detection<\/p>\n<\/li>\n<li><p>environment: AWS sagemaker<\/p>\n<\/li>\n<li><p>instance type: 'ml.p2.xlarge' | num_instances = 1<\/p>\n<\/li>\n<li><p>Main file to be run: <a href=\"https:\/\/github.com\/tensorflow\/models\/blob\/master\/research\/object_detection\/model_main_tf2.py\" rel=\"nofollow noreferrer\">original<\/a><\/p>\n<\/li>\n<li><p>Problematic code segment from the main file:<\/p>\n<pre><code>    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n    FLAGS.tpu_name)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n    elif FLAGS.num_workers &gt; 1:\n        strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n    else:\n        strategy = tf.compat.v2.distribute.MirroredStrategy()\n<\/code><\/pre>\n<\/li>\n<li><p>Problem : Can't find the proper value to be given as <code>tpu_name<\/code> argument.<\/p>\n<\/li>\n<li><p>My research on the problem:<\/p>\n<\/li>\n<\/ol>\n<p>According to the tensorflow documentation in <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/cluster_resolver\/TPUClusterResolver\" rel=\"nofollow noreferrer\">tf.distribute.cluster_resolver.TPUClusterResolver<\/a>, it says that this resolver works only on Google Cloud platform.<\/p>\n<blockquote>\n<p>This is an implementation of cluster resolvers for the Google Cloud\nTPU service.<\/p>\n<p>TPUClusterResolver supports the following distinct environments:\nGoogle Compute Engine Google Kubernetes Engine Google internal<\/p>\n<p>It can be passed into tf.distribute.TPUStrategy to support TF2\ntraining on Cloud TPUs.<\/p>\n<\/blockquote>\n<p>But from <a href=\"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/39721\" rel=\"nofollow noreferrer\">this issue in github<\/a>, I found out that a similar code also works in Azure.<\/p>\n<ol start=\"8\">\n<li>My question :<\/li>\n<\/ol>\n<p>Is there a way I can bypass this resolver and initialize my tpu in <strong>sagemaker<\/strong> ?<\/p>\n<p>Even better, if I can find a way to insert the name or url of sagemaker gpu to the resolver and initiate it from there ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1609059338347,
        "Question_score":0,
        "Question_tags":"python|tensorflow|gpu|amazon-sagemaker|tpu",
        "Question_view_count":355,
        "Owner_creation_time":1517147266417,
        "Owner_last_access_time":1663959762520,
        "Owner_location":null,
        "Owner_reputation":65,
        "Owner_up_votes":89,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":1609063955520,
        "Answer_body":"<p>Let me clarify some confusion here. TPUs are only offered on Google Cloud and the <code>TPUClusterResolver<\/code> implementation queries GCP APIs to get the cluster config for the TPU node. Thus, no you can't use <code>TPUClusterResolver<\/code> with AWS sagemaker, but you should try it out with TPUs on GCP instead or try find some other documentation on Sagemaker's end on how they enable cluster resolving on their end (if they do).<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1614007459500,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65464181",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57622122,
        "Question_title":"Custom sagemaker container for training, write forecast to AWS RDS, on a daily basis",
        "Question_body":"<p>I have 3 main process to perform using Amazon SageMaker.<\/p>\n\n<ol>\n<li>Using own training python script, (not using sagemaker container, inbuilt algorithm) [Train.py]<\/li>\n<\/ol>\n\n<p>-> For this, I have referred to this link:<br>\n<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/train-and-host-scikit-learn-models-in-amazon-sagemaker-by-building-a-scikit-docker-container\/\" rel=\"nofollow noreferrer\">Bring own algorithm to AWS sagemaker<\/a>\nand it seems that we can bring our own training script to sagemaker managed training setup, and model artifacts can be uploaded to s3 etc.\nNote: I am using Light GBM model for training.<\/p>\n\n<ol start=\"2\">\n<li>Writing forecast to AWS RDS DB:<\/li>\n<\/ol>\n\n<p>-> There is no need to deploy model and create endpoint, because training will happen everyday, and will create forecast as soon as training completes. (Need to generate forecast in train.py itself)<\/p>\n\n<p>-> <strong>Challenge is how can I write forecast in AWS RDS DB from train.py script. (Given that script is running in Private VPC)<\/strong><\/p>\n\n<ol start=\"3\">\n<li>Scheduling this process as daily job:<\/li>\n<\/ol>\n\n<p>--> I have gone through AWS step functions and seems to be the way to trigger daily training and write forecast to RDS.<\/p>\n\n<p>--> <strong>Challenge is how to use step function for time based trigger and not event based.<\/strong><\/p>\n\n<p>Any suggestions on how to do this? Any best practices to follow? Thank you in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1566547602673,
        "Question_score":0,
        "Question_tags":"python|amazon-rds|amazon-ecs|amazon-sagemaker|aws-step-functions",
        "Question_view_count":466,
        "Owner_creation_time":1469183608840,
        "Owner_last_access_time":1657205920673,
        "Owner_location":"Gurugram, Haryana, India",
        "Owner_reputation":624,
        "Owner_up_votes":95,
        "Owner_down_votes":0,
        "Owner_views":67,
        "Question_last_edit_time":1566550066240,
        "Answer_body":"<p>The way to trigger Step Functions on schedule is by using CloudWatch Events (sort of cron). Check out this tutorial: <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/tutorial-cloudwatch-events-target.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/tutorial-cloudwatch-events-target.html<\/a><\/p>\n\n<p>Don't write to the RDS from your Python code! It is better to write the output to S3 and then \"copy\" the files from S3 into the RDS. Decoupling these batches will make a more reliable and scalable process. You can trigger the bulk copy into the RDS when the files are written to S3 or to a later time when your DB is not too busy. <\/p>",
        "Answer_comment_count":5,
        "Answer_creation_time":1566713256897,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57622122",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73676684,
        "Question_title":"How to run SageMaker Distributed training from SageMaker Studio?",
        "Question_body":"<p>The sample notebooks for <strong>SageMaker Distributed training<\/strong>, like here:\u00a0https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/distributed_tensorflow_mask_rcnn\/mask-rcnn-scriptmode-s3.ipynb rely on the\u00a0<code>docker build .<\/code> and\u00a0<code>docker push .<\/code> commands, which are not available or installable in <a href=\"https:\/\/aws.amazon.com\/sagemaker\/studio\/\" rel=\"nofollow noreferrer\">Amazon SageMaker Studio<\/a>.<\/p>\n<p>Are there alternatives of these notebooks that are compatible with the SageMaker Studio?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662868156347,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|distributed-training|amazon-machine-learning|amazon-sagemaker-studio",
        "Question_view_count":16,
        "Owner_creation_time":1389887039673,
        "Owner_last_access_time":1664076128463,
        "Owner_location":"Singapore",
        "Owner_reputation":5854,
        "Owner_up_votes":155,
        "Owner_down_votes":70,
        "Owner_views":794,
        "Question_last_edit_time":null,
        "Answer_body":"<p>SageMaker Studio does not support Docker, since the Studio apps are containers themselves. You can use the SageMaker Docker Build tool to build docker images from Studio (uses CodeBuild in the backend). See the blog <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks\/\" rel=\"nofollow noreferrer\">Using the Amazon SageMaker Studio Image Build CLI to build container images from your Studio notebooks<\/a> and the <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli\" rel=\"nofollow noreferrer\">Github repo<\/a> for details.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1663095381000,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73676684",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67707288,
        "Question_title":"Do you get charged for production variants on sagemaker that have no traffic going through them?",
        "Question_body":"<p>I have a deployed model on sagemaker with two production variants. I was wondering if you get charged for both variants even if I set all the traffic to just go through one of them.<\/p>\n<p>The docs on pricing are found below but I couldn't seem to find the answer to this.<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622040262077,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":40,
        "Owner_creation_time":1521737124657,
        "Owner_last_access_time":1663943374747,
        "Owner_location":"Milton Keynes",
        "Owner_reputation":738,
        "Owner_up_votes":22,
        "Owner_down_votes":7,
        "Owner_views":69,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You will be charged as long as the model is running on an instance regardless of whether the traffic is going through it or not as still the model is running on an instance.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1622110871903,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67707288",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65881699,
        "Question_title":"SageMaker failed to extract model data archive tar.gz for container when deploying",
        "Question_body":"<p>I am trying in Amazon Sagemaker to deploy an existing Scikit-Learn model. So a model that wasn't trained on SageMaker, but locally on my machine.<\/p>\n<p>On my local (windows) machine I've saved my model as model.joblib and tarred the model to model.tar.gz.<\/p>\n<p>Next, I've uploaded this model to my S3 bucket ('my_bucket') in the following path s3:\/\/my_bucket\/models\/model.tar.gz. I can see the tar file in S3.<\/p>\n<p>But when I'm trying to deploy the model, it keeps giving the error message &quot;Failed to extract model data archive&quot;.<\/p>\n<p>The .tar.gz is generated on my local machine by running 'tar -czf model.tar.gz model.joblib' in a powershell command window.<\/p>\n<p>The code for uploading to S3<\/p>\n<pre><code>import boto3\ns3 = boto3.client(&quot;s3&quot;, \n              region_name='eu-central-1', \n              aws_access_key_id=AWS_KEY_ID, \n              aws_secret_access_key=AWS_SECRET)\ns3.upload_file(Filename='model.tar.gz', Bucket=my_bucket, Key='models\/model.tar.gz')\n<\/code><\/pre>\n<p>The code for creating the estimator and deploying:<\/p>\n<pre><code>import boto3\nfrom sagemaker.sklearn.estimator import SKLearnModel\n\n...\n\nmodel_data = 's3:\/\/my_bucket\/models\/model.tar.gz'\nsklearn_model = SKLearnModel(model_data=model_data,\n                             role=role,\n                             entry_point=&quot;my-script.py&quot;,\n                             framework_version=&quot;0.23-1&quot;)\npredictor = sklearn_model.deploy(instance_type=&quot;ml.t2.medium&quot;, initial_instance_count=1)                             \n<\/code><\/pre>\n<p>The error message:<\/p>\n<blockquote>\n<p>error message: UnexpectedStatusException: Error hosting endpoint\nsagemaker-scikit-learn-2021-01-24-17-24-42-204: Failed. Reason: Failed\nto extract model data archive for container &quot;container_1&quot; from URL\n&quot;s3:\/\/my_bucket\/models\/model.tar.gz&quot;. Please ensure that the object\nlocated at the URL is a valid tar.gz archive<\/p>\n<\/blockquote>\n<p>Is there a way to see why the archive is invalid?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1611565411923,
        "Question_score":4,
        "Question_tags":"machine-learning|scikit-learn|deployment|amazon-sagemaker",
        "Question_view_count":1859,
        "Owner_creation_time":1472970520890,
        "Owner_last_access_time":1660385188500,
        "Owner_location":"Amersfoort, Nederland",
        "Owner_reputation":424,
        "Owner_up_votes":15,
        "Owner_down_votes":3,
        "Owner_views":21,
        "Question_last_edit_time":1611823707607,
        "Answer_body":"<p>I had a similar issue as well, along with a similar fix to Bas (per comment above).<\/p>\n<p>I was finding I wasn't necessarily having issues with the .tar.gz step, this command does work fine:<\/p>\n<p><code>tar -czf &lt;filename&gt; .\/&lt;directory-with-files&gt;<\/code><\/p>\n<p>but rather with the uploading step.<\/p>\n<p>Manually uploading to S3 should take care of this, however, if you're doing this step programmatically, you might need to double check the steps taken. Bas appears to have had filename issues, mine were around using boto properly. Here's some code that works (Python only here, but watch for similar issues with other libraries):<\/p>\n<pre><code>bucket = 'bucket-name'\nkey = 'directory-inside-bucket'\nfile = 'the file name of the .tar.gz'\n\ns3_client = boto3.client('s3')\ns3_client.upload_file(file, bucket, key)\n<\/code><\/pre>\n<p>Docs: <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1611852069143,
        "Answer_score":1,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65881699",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57661142,
        "Question_title":"AWS S3 and Sagemaker: No such file or directory",
        "Question_body":"<p>I have created an S3 bucket 'testshivaproject' and uploaded an image in it. When I try to access it in sagemaker notebook, it throws an error 'No such file or directory'.<\/p>\n\n<pre><code># import libraries\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np                                   \n\n# Define IAM role\nrole = get_execution_role()\n\nmy_region = boto3.session.Session().region_name # set the region of the instance\n\nprint(\"success :\"+my_region)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> success :us-east-2<\/p>\n\n<pre><code>role\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> 'arn:aws:iam::847047967498:role\/service-role\/AmazonSageMaker-ExecutionRole-20190825T121483'<\/p>\n\n<pre><code>bucket = 'testprojectshiva2' \ndata_key = 'ext_image6.jpg' \ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key) \nprint(data_location)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> s3:\/\/testprojectshiva2\/ext_image6.jpg<\/p>\n\n<pre><code>test = load_img(data_location)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> No such file or directory<\/p>\n\n<p>There are similar questions raised (<a href=\"https:\/\/stackoverflow.com\/questions\/48264656\/load-s3-data-into-aws-sagemaker-notebook\">Load S3 Data into AWS SageMaker Notebook<\/a>) but did not find any solution?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1566834582570,
        "Question_score":1,
        "Question_tags":"amazon-s3|amazon-sagemaker",
        "Question_view_count":4622,
        "Owner_creation_time":1442731325233,
        "Owner_last_access_time":1649309665537,
        "Owner_location":"Hyderabad",
        "Owner_reputation":187,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Thanks for using Amazon SageMaker!<\/p>\n\n<p>I sort of guessed from your description, but are you trying to use the Keras load_img function to load images directly from your S3 bucket?<\/p>\n\n<p>Unfortunately, <a href=\"https:\/\/github.com\/keras-team\/keras\/issues\/11684\" rel=\"nofollow noreferrer\">the load_img function is designed to only load files from disk<\/a>, so passing an s3:\/\/ URL to that function will always return a <code>FileNotFoundError<\/code>.<\/p>\n\n<p>It's common to first download images from S3 before using them, so you can use boto3 or the AWS CLI to download the file before calling load_img.<\/p>\n\n<p><strong>Alternatively<\/strong>, since the load_img function simply creates a <a href=\"https:\/\/en.wikipedia.org\/wiki\/Python_Imaging_Library\" rel=\"nofollow noreferrer\">PIL Image<\/a> object, you can create the PIL object directly from the data in S3 using boto3, and not use the load_img function at all.<\/p>\n\n<p>In other words, you could do something like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from PIL import Image\n\ns3 = boto3.client('s3')\ntest = Image.open(BytesIO(\n    s3.get_object(Bucket=bucket, Key=data_key)['Body'].read()\n    ))\n<\/code><\/pre>\n\n<p>Hope this helps you out in your project!<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1567829268830,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57661142",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51803032,
        "Question_title":"AWS SageMaker S3 os.listdir() Access denied",
        "Question_body":"<p>I'm Stumped.  <\/p>\n\n<p>I took my TensorFlow model and moved it up into SageMaker to try it out.  I put my own data up into an s3 bucket, set all the IAM roles\/access (or so I think).  I can read a file from s3.  I can push a new file to s3. I can read local directories from my SageMaker local directories.<\/p>\n\n<p><strong>I cannot traverse my s3 bucket directories.<\/strong>  I turned on logging and I get AccessDenied messages whenever I try access a URI of this format <strong>'s3:\/\/my_bucketName_here\/Directory_of_my_data\/'<\/strong>.<\/p>\n\n<p>Here is what I've done:\nI've confirmed that my notebook uses the AmazonSageMaker-ExecutionRole-***\nI've added AmazonSageMakerFullAccess Policy to that default role\nI've subsequently added AmazonS3FullAccess Policy as well<\/p>\n\n<p>I then created a bucket policy specifically granting s3:* access on the specific bucket to that specific role.<\/p>\n\n<p>Heck, I eventually made the bucket public with ListObjects = Yes.<\/p>\n\n<p>os.listdir() simply fails with file or directory not found and a lot message is created with AccessDenied. (TensorFlow libraries just didn't work, so I went with os.listdir() to simplify things.<\/p>\n\n<p>Finally, I test my access from the Policy Simulator - I selected the Role mentioned above, selected to test s3 and selected all 69 items and they all passed.<\/p>\n\n<p>But I continue to log AccessDenied and cannot actually list the contents of a directory from my SageMaker jupyter notebook.<\/p>\n\n<p>I'm at a loss.  Thoughts?<\/p>\n\n<p>EDIT:\nPer suggestion below, I have the following:\nbucket name contains sagemaker: '[redacted]-test-sagemaker'\nPublic access is off, and the only account is my root account.\n<code>\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::[redacted]-test-sagemaker\"\n            ]\n        },\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::[redacted]-test-sagemaker\/*\"\n            ]\n        }\n    ]\n}<\/code>\nand\narn:aws:iam::aws:policy\/AmazonSageMakerFullAccess<\/p>\n\n<p>Finally the bucket policy after the above failed:\n<code>{\n  \"Id\": \"Policy1534116031672\",\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Stmt1534116026409\",\n      \"Action\": \"s3:*\",\n      \"Effect\": \"Allow\",\n      \"Resource\": \"arn:aws:s3:::[redacted]-test-sagemaker\",\n      \"Principal\": {\n        \"AWS\": [\n          \"arn:aws:iam::[id]:role\/service-role\/AmazonSageMaker-ExecutionRole-***\"\n        ]\n      }\n    }\n  ]\n}<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1534015647103,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":2961,
        "Owner_creation_time":1449682589303,
        "Owner_last_access_time":1634314186743,
        "Owner_location":"Phoenix, AZ, United States",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1534116796512,
        "Answer_body":"<p>So you need to troubleshoot. Here are a few things to check:   <\/p>\n\n<p>0) Make sure the bucket is in the SageMaker region.<\/p>\n\n<p>1) Include the string \"sagemaker\" in your bucket name (e.g., <em>my_bucketName_here-sagemaker<\/em>, SageMaker has out of the box access to buckets named this way.<\/p>\n\n<p>2) Try using the SageMaker S3 <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/session.py\" rel=\"nofollow noreferrer\">default_bucket()<\/a>:<\/p>\n\n<pre><code>import sagemaker\ns = sagemaker.Session()\ns.upload_data(path='somefile.csv', bucket=s.default_bucket(), key_prefix='data\/train')\n<\/code><\/pre>\n\n<p>3) Open terminal on the Notebook instance, to try to list your bucket using AWS CLI in bash:<\/p>\n\n<pre><code>aws iam get-user\naws s3 ls my_bucketName_here\n<\/code><\/pre>\n\n<p>Finally, pasting the bucket's access and resource policy in your question could help others to answer you.<\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1534057177853,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51803032",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62313532,
        "Question_title":"xgboost on Sagemaker notebook import fails",
        "Question_body":"<p>I am trying to use XGBoost on Sagemaker notebook.<\/p>\n\n<p>I am using <code>conda_python3<\/code> kernel, and the following packages are installed:<\/p>\n\n<ul>\n<li>py-xgboost-mutex<\/li>\n<li>libxgboost<\/li>\n<li>py-xgboost<\/li>\n<li>py-xgboost-gpu<\/li>\n<\/ul>\n\n<p>But once I am trying to import xgboost it fails on import:<\/p>\n\n<pre><code>ModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-5-5943d1bfe3f1&gt; in &lt;module&gt;()\n----&gt; 1 import xgboost as xgb\n\nModuleNotFoundError: No module named 'xgboost'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1591825043587,
        "Question_score":0,
        "Question_tags":"python|jupyter-notebook|conda|xgboost|amazon-sagemaker",
        "Question_view_count":1532,
        "Owner_creation_time":1391632571720,
        "Owner_last_access_time":1663859036253,
        "Owner_location":"Tel Aviv-Yafo, Israel",
        "Owner_reputation":1248,
        "Owner_up_votes":97,
        "Owner_down_votes":1,
        "Owner_views":137,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In Sagemaker notebooks  use the below steps <\/p>\n\n<h3>a) If in Notebook<\/h3>\n\n<p>i)  <code>!type python3<\/code><\/p>\n\n<p>ii) Say the above is \/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python3 for you <\/p>\n\n<p>iii) <code>!\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python3 -m pip install  xgboost<\/code><\/p>\n\n<p>iv)  <code>import xgboost<\/code><\/p>\n\n<hr>\n\n<h3>b) If using Terminal<\/h3>\n\n<p>i) <code>conda activate conda_python3<\/code><br>\nii) <code>pip install xgboost<\/code><\/p>\n\n<p>Disclaimer :  sometimes the installation would fail with gcc version ,in that case  update pip version before running install<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1592571589163,
        "Answer_score":3,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":1592573769990,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62313532",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":48365866,
        "Question_title":"How to call Sagemaker training model endpoint API in C#",
        "Question_body":"<p>I have implemented machine learning algorithms through sagemaker.<\/p>\n\n<p>I have installed SDK for .net, and tried by executing below code.<\/p>\n\n<pre><code>Uri sagemakerEndPointURI = new Uri(\"https:\/\/runtime.sagemaker.us-east-2.amazonaws.com\/endpoints\/MyEndpointName\/invocations\");\nAmazon.SageMakerRuntime.Model.InvokeEndpointRequest request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest();\nrequest.EndpointName = \"MyEndpointName\";\nAmazonSageMakerRuntimeClient aawsClient = new AmazonSageMakerRuntimeClient(myAwsAccessKey,myAwsSecreteKey);            \nAmazon.SageMakerRuntime.Model.InvokeEndpointResponse resposnse= aawsClient.InvokeEndpoint(request);\n<\/code><\/pre>\n\n<p>By executing this, I am getting validation error as \"<code>1 validation error detected: Value at 'body' failed to satisfy constraint: Member must not be null<\/code>\"<\/p>\n\n<p>Can anyone guide me on how and what more input data I need to pass to call the given API?<\/p>\n\n<p>EDIT<\/p>\n\n<p>Further I'd tried by provinding body parameter which contains a MemoryStream written by a '.gz' or '.pkl' file, and it giving me error as : \"Error unmarshalling response back from AWS,  HTTP content length exceeded 5246976 bytes.\"<\/p>\n\n<p>EDIT 1\/23\/2018<\/p>\n\n<p>Further I came up with the error message as <\/p>\n\n<blockquote>\n  <p>ERROR - model server - 'TypeError' object has no attribute 'message'<\/p>\n<\/blockquote>\n\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1516531050743,
        "Question_score":8,
        "Question_tags":"c#|amazon-web-services|amazon-s3|sparkr|amazon-sagemaker",
        "Question_view_count":2093,
        "Owner_creation_time":1337759214690,
        "Owner_last_access_time":1532954663377,
        "Owner_location":"Pune India",
        "Owner_reputation":1036,
        "Owner_up_votes":34,
        "Owner_down_votes":1,
        "Owner_views":124,
        "Question_last_edit_time":1516720777516,
        "Answer_body":"<p>Later solved it by <code>Encoding.ASCII.GetBytes<\/code>as in below code.<\/p>\n\n<pre><code> byte[] bytes = System.IO.File.ReadAllBytes(@\"EXCEL_FILE_PATH\");\n    string listA = \"\";\n    while (!reader.EndOfStream)\n        {\n            var line = reader.ReadLine();\n            listA = listA + line + \"\\n\";\n        }\n    byte[] bytes = Encoding.ASCII.GetBytes(listA);\n    request.Body = new MemoryStream(bytes);\n    InvokeEndpointResponse response = sagemakerRunTimeClient.InvokeEndpoint(request);\n    string predictions = Encoding.UTF8.GetString(response.Body.ToArray());\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1519637555373,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48365866",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":64286191,
        "Question_title":"How to add keyboard shortcuts to AWS Ground Truth labeler UI?",
        "Question_body":"<p>I'm using AWS Sagemaker Ground Truth for a Custom Labeling Task that involves editing bounding boxes and their labels.  Ground Truth's UI has built-in keyboard shortcuts for doing things like choosing the label for a box, but it seems to lack shortcuts for other built-in UI elements like &quot;No adjustments needed&quot; or the &quot;Submit&quot; button.<\/p>\n<p>Is there a way to add such shortcuts?  I've looked at the crowd-html-elements for customizing the appearance of the page, but can't find anything in there about keyboard shortcuts.  It doesn't even look like crowd-button or crowd-icon-button support specifying a shortcut as an attribute.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1602271699933,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker|labeling",
        "Question_view_count":412,
        "Owner_creation_time":1289772110723,
        "Owner_last_access_time":1663029415383,
        "Owner_location":"Mt Kisco, NY",
        "Owner_reputation":309,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":40,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Could try something like:<\/p>\n<pre><code>document.addEventListener('keydown', function(event) {\n  if (event.shiftKey &amp;&amp; event.keyCode === 13) {\n    document.getElementsByTagName('crowd-bounding-box')[0].shadowRoot.getElementById('nothing-to-adjust').querySelector('label').click();\n  }\n});\n<\/code><\/pre>",
        "Answer_comment_count":1,
        "Answer_creation_time":1605831050090,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64286191",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54638364,
        "Question_title":"SageMaker Script Mode + Pipe Mode",
        "Question_body":"<p>I'm training in SageMaker using TensorFlow + Script Mode and currently using 'File' input mode for my data.<\/p>\n\n<p>Has anyone figured out how to stream data using 'Pipe' data format in conjunction with Script Mode training?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1549915827960,
        "Question_score":3,
        "Question_tags":"python|tensorflow|streaming|amazon-sagemaker",
        "Question_view_count":1385,
        "Owner_creation_time":1361339272693,
        "Owner_last_access_time":1663965928400,
        "Owner_location":"NYC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Question_last_edit_time":1549923138163,
        "Answer_body":"<p>You can import <code>sagemaker_tensorflow<\/code> from the training script as follows:<\/p>\n\n<pre><code>from sagemaker_tensorflow import PipeModeDataset\nfrom tensorflow.contrib.data import map_and_batch\n\nchannel = 'my-pipe-channel-name'\n\nds = PipeModeDataset(channel)\nds = ds.repeat(EPOCHS)\nds = ds.prefetch(PREFETCH_SIZE)\nds = ds.apply(map_and_batch(parse, batch_size=BATCH_SIZE,\n                            num_parallel_batches=NUM_PARALLEL_BATCHES))\n<\/code><\/pre>\n\n<p>You can find the full example here: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_pipemode_example\/pipemode.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_pipemode_example\/pipemode.py<\/a><\/p>\n\n<p>You can find documentation about sagemaker_tensorflow here <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset<\/a><\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1551201776033,
        "Answer_score":4,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54638364",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55674959,
        "Question_title":"Importing a file into jupyterlabs from s3",
        "Question_body":"<p>I have a file I want to import into a Sagemaker Jupyter notebook python 3 instance for use.  The exact code would be 'import lstm.'  I can store the file in s3 (which would probably be ideal) or locally, whichever you prefer.  I have been searching the internet for a while and have been unable to find a solution to this.  I am actually just trying to run\/understand this code from Suraj Raval's youtube channel: <a href=\"https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot\" rel=\"nofollow noreferrer\">https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot<\/a>.  The 'import lstm' line is failing when I run, and I am trying to figure out how to make this work.  <\/p>\n\n<p>I have tried:\nfrom s3:\/\/... import lstm.  failed\nI have tried some boto3 methods and wasn't able to get it to work.  <\/p>\n\n<pre><code>import time\nimport threading\nimport lstm, etl, json. ##this line\nimport numpy as np\nimport pandas as pd\nimport h5py\nimport matplotlib.pyplot as plt\nconfigs = json.loads(open('configs.json').read())\ntstart = time.time()\n<\/code><\/pre>\n\n<p>I would just like to be able to import the lstm file and all the others into a Jupyter notebook instance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1555242786160,
        "Question_score":3,
        "Question_tags":"amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":1863,
        "Owner_creation_time":1555241852167,
        "Owner_last_access_time":1635370371113,
        "Owner_location":"Chicago, IL, USA",
        "Owner_reputation":393,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":81,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I think you should  be cloning the Github repo in SageMaker instance and not importing the files from S3. I was able to reproduce the Bitcoin Trading Bot notebook from SageMaker by cloning it. You can follow the below steps<\/p>\n\n<h3>Cloning Github Repo to SageMaker Notebook<\/h3>\n\n<ol>\n<li>Open JupyterLab from the AWS SageMaker console.<\/li>\n<li>From the JupyterLab  Launcher, open the Terminal.<\/li>\n<li>Change directory to SageMaker<\/li>\n<\/ol>\n\n<pre><code>cd ~\/SageMaker\n<\/code><\/pre>\n\n<ol start=\"4\">\n<li>Clone the BitCoin Trading Bot <a href=\"https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot\" rel=\"nofollow noreferrer\">git repo<\/a><\/li>\n<\/ol>\n\n<pre><code>git clone https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot.git\ncd Bitcoin_Trading_Bot\n<\/code><\/pre>\n\n<ol start=\"5\">\n<li>Now you can open the notebook <code>Bitcoin LSTM Prediction.ipynb<\/code> and select the Tensorflow Kernel to run the notebook.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YbKic.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YbKic.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<h3>Adding files from local machine to SageMaker Notebook<\/h3>\n\n<p>To add files from your local machine to SageMaker Notebook instance, you can use <a href=\"https:\/\/jupyterlab.readthedocs.io\/en\/stable\/user\/files.html\" rel=\"nofollow noreferrer\">file upload<\/a> functionality in JupyterLab<\/p>\n\n<h3>Adding files from S3 to SageMaker Notebook<\/h3>\n\n<p>To add files from S3 to SageMaker Notebook instance, use AWS CLI or Python SDK to upload\/download files. <\/p>\n\n<p>For example, to download <code>lstm.py<\/code> file from S3 to SageMaker using AWS CLI<\/p>\n\n<pre><code>aws s3 cp s3:\/\/mybucket\/bot\/src\/lstm.py .\n<\/code><\/pre>\n\n<p>Using <code>boto3<\/code> API<\/p>\n\n<pre><code>import boto3\ns3 = boto3.resource('s3')\ns3.meta.client.download_file('mybucket', 'bot\/src\/lstm.py', '.\/lstm.py')\n<\/code><\/pre>",
        "Answer_comment_count":2,
        "Answer_creation_time":1555252725130,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1555346571920,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55674959",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52876202,
        "Question_title":"How to bulk test the Sagemaker Object detection model with a .mat dataset or S3 folder of images?",
        "Question_body":"<p>I have trained the following Sagemaker model: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco<\/a><\/p>\n\n<p>I've tried both the JSON and RecordIO version. In both, the algorithm is tested on ONE sample image. However, I have a dataset of 2000 pictures, which I would like to test. I have saved the 2000 jpg pictures in a folder within an S3 bucket and I also have two .mat files (pics + ground truth). How can I apply this model to all 2000 pictures at once and then save the results, rather than doing it one picture at a time?<\/p>\n\n<p>I am using the code below to load a single picture from my S3 bucket:<\/p>\n\n<pre><code>object = bucket.Object('pictures\/pic1.jpg')\nobject.download_file('pic1.jpg')\nimg=mpimg.imread('pic1.jpg')\nimg_name = 'pic1.jpg'\nimgplot = plt.imshow(img)\nplt.show(imgplot)\n\nwith open(img_name, 'rb') as image:\n    f = image.read()\n    b = bytearray(f)\n    ne = open('n.txt','wb')\n    ne.write(b)\n\nimport json\nobject_detector.content_type = 'image\/jpeg'\nresults = object_detector.predict(b)\ndetections = json.loads(results)\nprint (detections['prediction'])\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1539872573950,
        "Question_score":1,
        "Question_tags":"python-3.x|amazon-web-services|amazon-s3|mat|amazon-sagemaker",
        "Question_view_count":112,
        "Owner_creation_time":1489873508190,
        "Owner_last_access_time":1648846099743,
        "Owner_location":null,
        "Owner_reputation":509,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":84,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I'm not sure if I understood your question correctly. However, if you want to feed multiple images to the model at once, you can create a multi-dimensional array of images (byte arrays) to feed the model.<\/p>\n\n<p>The code would look something like this.<\/p>\n\n<pre><code>import numpy as np\n...\n\n#  predict_images_list is a Python list of byte arrays\npredict_images = np.stack(predict_images_list)\n\nwith graph.as_default():\n    #  results is an list of typical results you'd get.\n    results = object_detector.predict(predict_images)\n<\/code><\/pre>\n\n<p>But, I'm not sure if it's a good idea to feed 2000 images at once. Better to batch them in 20-30 images at a time and predict. <\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1539965456440,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52876202",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51792005,
        "Question_title":"Sagemaker: DeepAR Hyperparameter Tuning Error",
        "Question_body":"<p>Running into a new issue with tuning DeepAR on SageMaker when trying to initialize a hyperparameter tuning job - this error also occurs when calling the test:mean_wQuantileLoss. I've upgraded the sagemaker package, restarted my instance, restarted the kernel (using a juptyer notebook), and yet the problem persists. <\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the \nCreateHyperParameterTuningJob operation: The objective metric type, [Maximize], that you specified for objective metric, [test:RMSE], isn\u2019t valid for the [156387875391.dkr.ecr.us-west-2.amazonaws.com\/forecasting-deepar:1] algorithm. Choose a valid objective metric type.\n<\/code><\/pre>\n\n<p>Code:<\/p>\n\n<pre><code>my_tuner = HyperparameterTuner(estimator=estimator,\n                               objective_metric_name=\"test:RMSE\",\n                               hyperparameter_ranges=hyperparams,\n                               max_jobs=20,\n                               max_parallel_jobs=2)\n\n# Start hyperparameter tuning job\nmy_tuner.fit(inputs=data_channels)\n\nStack Trace:\nClientError                               Traceback (most recent call last)\n&lt;ipython-input-66-9d6d8de89536&gt; in &lt;module&gt;()\n      7 \n      8 # Start hyperparameter tuning job\n----&gt; 9 my_tuner.fit(inputs=data_channels)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in fit(self, inputs, job_name, include_cls_metadata, **kwargs)\n    255 \n    256         self._prepare_for_training(job_name=job_name, include_cls_metadata=include_cls_metadata)\n--&gt; 257         self.latest_tuning_job = _TuningJob.start_new(self, inputs)\n    258 \n    259     @classmethod\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in start_new(cls, tuner, inputs)\n    525                                                output_config=(config['output_config']),\n    526                                                resource_config=(config['resource_config']),\n--&gt; 527                                                stop_condition=(config['stop_condition']), tags=tuner.tags)\n    528 \n    529         return cls(tuner.sagemaker_session, tuner._current_job_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in tune(self, job_name, strategy, objective_type, objective_metric_name, max_jobs, max_parallel_jobs, parameter_ranges, static_hyperparameters, image, input_mode, metric_definitions, role, input_config, output_config, resource_config, stop_condition, tags)\n    348         LOGGER.info('Creating hyperparameter tuning job with name: {}'.format(job_name))\n    349         LOGGER.debug('tune request: {}'.format(json.dumps(tune_request, indent=4)))\n--&gt; 350         self.sagemaker_client.create_hyper_parameter_tuning_job(**tune_request)\n    351 \n    352     def stop_tuning_job(self, name):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    312                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    313             # The \"self\" in this scope is referring to the BaseClient.\n--&gt; 314             return self._make_api_call(operation_name, kwargs)\n    315 \n    316         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    610             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n    611             error_class = self.exceptions.from_code(error_code)\n--&gt; 612             raise error_class(parsed_response, operation_name)\n    613         else:\n    614             return parsed_response\n\nClientError: An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: \nThe objective metric type, [Maximize], that you specified for objective metric, [test:RMSE], isn\u2019t valid for the [156387875391.dkr.ecr.us-west-2.amazonaws.com\/forecasting-deepar:1] algorithm. \nChoose a valid objective metric type.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1533924861517,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1355,
        "Owner_creation_time":1378935265347,
        "Owner_last_access_time":1541614115827,
        "Owner_location":null,
        "Owner_reputation":27,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It looks like you are trying to maximize this metric, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar-tuning.html\" rel=\"nofollow noreferrer\">test:RMSE can only be minimized<\/a> by SageMaker HyperParameter Tuning. <\/p>\n\n<p>To achieve this in the SageMaker Python SDK, create your HyperparameterTuner with objective_type='Minimize'. You can see the signature of the init method <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tuner.py#L158\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Here is the change you should make to your call to HyperparameterTuner:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>my_tuner = HyperparameterTuner(estimator=estimator,\n                               objective_metric_name=\"test:RMSE\",\n                               objective_type='Minimize',\n                               hyperparameter_ranges=hyperparams,\n                               max_jobs=20,\n                               max_parallel_jobs=2)\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1533929944953,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51792005",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65619881,
        "Question_title":"SageMaker tuning job cannot use P2 or P3 instances",
        "Question_body":"<p>I am trying to use AWS SageMaker Hyperparameter tuning job. I can use C5 instance, however, when trying to use either p2 or p3 I get this error.<\/p>\n<pre><code>{{botocore.errorfactory.ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateHyperParameterTuningJob operation: The account-level service limit 'ml.p3.2xlarge for training job usage' is 2 Instances, with current utilization of 0 Instances and a request delta of 5 Instances. Please contact AWS support to request an increase for this limit.\n}}\n<\/code><\/pre>\n<p>Does anybody have idea about it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1610053277393,
        "Question_score":0,
        "Question_tags":"amazon-ec2|amazon-sagemaker|auto-tuning",
        "Question_view_count":209,
        "Owner_creation_time":1495636394673,
        "Owner_last_access_time":1663616691417,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There is a limitation in our account so we had to request for using the instances and increasing the available resource from AWS.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1613677698377,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65619881",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":67131617,
        "Question_title":"Connect to redshift using sagemaker notebook instances",
        "Question_body":"<p>I would like to connect to redshift using sagemaker notebook instances. I want to run Unload commands to unload data from redshift to s3 using IAM role and schedule the sagemaker notebook.\nI want to know how I can import db credentials in sagemaker without hardcoding.<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618603786843,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-redshift|amazon-sagemaker",
        "Question_view_count":1504,
        "Owner_creation_time":1468599558957,
        "Owner_last_access_time":1658530343460,
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":107,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use <a href=\"https:\/\/docs.aws.amazon.com\/secretsmanager\/latest\/userguide\/intro.html\" rel=\"nofollow noreferrer\">AWS Secrets Manager<\/a> to store and access credentials. Your Sagemaker execution role should have permission to read from Secrets Manager (AFAIK AWS managed-role does have it). This is the same mechanism that's used by Sagemaker notebooks to get access to github repo, for example<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1618908321397,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67131617",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65102618,
        "Question_title":"struggling to install python package via amazon sagemaker",
        "Question_body":"<p>I am using Amazon Sagemaker and trying to install gaapi4py package via anaconda python3 notebook.<\/p>\n<p>So far I've tried the following commands:<\/p>\n<pre><code>%conda install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>conda install gaapi4py\n\nGot same error:\n\nCollecting package metadata (current_repodata.json): failed\n\nCondaHTTPError: HTTP 000 CONNECTION FAILED for url &lt;https:\/\/conda.anaconda.org\/conda-forge\/linux-64\/current_repodata.json&gt;\nElapsed: -\n\nAn HTTP error occurred when trying to retrieve this URL.\nHTTP errors are often intermittent, and a simple retry will get you on your way.\n'https:\/\/conda.anaconda.org\/conda-forge\/linux-64'\n\n\n\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>Alternatively I've tried the below but it failed as well:<\/p>\n<pre><code>pip install gaapi4py\n<\/code><\/pre>\n<p>Error text:<\/p>\n<pre><code>WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803c50&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c8035f8&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803550&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803400&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803358&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nERROR: Could not find a version that satisfies the requirement gaapi4py (from versions: none)\nERROR: No matching distribution found for gaapi4py\nWARNING: You are using pip version 20.0.2; however, version 20.3 is available.\nYou should consider upgrading via the '\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python -m pip install --upgrade pip' command.\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>What am I doing wrong? All previous packages worked well.<\/p>\n<p>UPD:<\/p>\n<p>Tried also as recommended in amazon book:<\/p>\n<pre><code>import sys\n!{sys.executable} -m pip install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>import sys\n!conda install -y --prefix {sys.prefix} gaapi4py\n<\/code><\/pre>\n<p>Both didn't work neither, getting same errors as above.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1606886205577,
        "Question_score":1,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":1148,
        "Owner_creation_time":1474967309677,
        "Owner_last_access_time":1661467549403,
        "Owner_location":null,
        "Owner_reputation":59,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":66,
        "Question_last_edit_time":1606887753310,
        "Answer_body":"<p>After talking back-in-forth with our IT department I figured out that custom libraries installation was blocked for security reasons.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1644193786213,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65102618",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55531608,
        "Question_title":"Accessing Google BigQuery from AWS SageMaker",
        "Question_body":"<p>When running locally, my Jupyter notebook is able to reference Google BigQuery like so:<\/p>\n\n<pre><code>%%bigquery some_bq_table\n\nSELECT *\nFROM\n  `some_bq_dataset.some_bq_table` \n<\/code><\/pre>\n\n<p>So that later in my notebook I can reference some_bq_table as a pandas dataframe, as exemplified here: <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/visualize-jupyter\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/bigquery\/docs\/visualize-jupyter<\/a><\/p>\n\n<p>I want to run my notebook on AWS SageMaker to test a few things. To authenticate with BigQuery it seems that the only two ways are using a service account on GCP (or locally) or pointing the the SDK to a credentials JSON using an env var (as explained here: <a href=\"https:\/\/cloud.google.com\/docs\/authentication\/getting-started\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/docs\/authentication\/getting-started<\/a>).<\/p>\n\n<p>For example<\/p>\n\n<pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"\/home\/user\/Downloads\/[FILE_NAME].json\"\n<\/code><\/pre>\n\n<p>Is there an easy way to connect to bigquery from SageMaker? My best idea right now is to download the JSON from somewhere to the SageMaker instnace and then set the env var from the python code.<\/p>\n\n<p>For example, I would do this:<\/p>\n\n<pre><code>os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"\/home\/user\/Downloads\/[FILE_NAME].json\"\n<\/code><\/pre>\n\n<p>However, this isn't very secure - I don't like the idea of downloading my credentials JSON to a SageMaker instance (this means I would have to upload the credentials to some private s3 bucket and then store them on the SageMaker instance). Not the end of the world but I rather avoid this. <\/p>\n\n<p>Any ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1554454321543,
        "Question_score":4,
        "Question_tags":"python|google-bigquery|amazon-sagemaker",
        "Question_view_count":1078,
        "Owner_creation_time":1372517243827,
        "Owner_last_access_time":1663839924047,
        "Owner_location":null,
        "Owner_reputation":5921,
        "Owner_up_votes":333,
        "Owner_down_votes":19,
        "Owner_views":388,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As you mentioned GCP currently authenticates using service account, credentials JSON and API tokens. Instead of storing credentials in S3 bucket you can consider using AWS Secrets Manager or AWS Systems Manager Parameter Store to store the GCP credentials and then fetch them in Jupyter notebook. This way credentials can be secured and the credentials file will be created from Secrets Manager only when needed. <\/p>\n\n<p>This is sample code I used previously to connect to BigQuery from SageMaker instance.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nimport boto3\nfrom google.cloud.bigquery import magics\nfrom google.oauth2 import service_account\n\ndef get_gcp_credentials_from_ssm(param_name):\n    # read credentials from SSM parameter store\n    ssm = boto3.client('ssm')\n    # Get the requested parameter\n    response = ssm.get_parameters(Names=[param_name], WithDecryption=True)\n    # Store the credentials in a variable\n    gcp_credentials = response['Parameters'][0]['Value']\n    # save credentials temporarily to a file\n    credentials_file = '\/tmp\/.gcp\/service_credentials.json'\n    with open(credentials_file, 'w') as outfile:  \n        json.dump(json.loads(gcp_credentials), outfile)\n    # create google.auth.credentials.Credentials to use for queries \n    credentials = service_account.Credentials.from_service_account_file(credentials_file)\n    # remove temporary file\n    if os.path.exists(credentials_file):\n        os.remove(credentials_file)\n    return credentials\n\n# this will set the context credentials to use for queries performed in jupyter \n# using bigquery cell magic\nmagics.context.credentials = get_gcp_credentials_from_ssm('my_gcp_credentials')\n<\/code><\/pre>\n\n<p>Please note that SageMaker execution role should have access to SSM and of course other necessary route to connect to GCP. I am not sure if this is the best way though. Hope someone has better way.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1554514774277,
        "Answer_score":7,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55531608",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72203674,
        "Question_title":"Deploy AWS SageMaker pipeline using the cloud development kit (CDK)",
        "Question_body":"<p>I'm looking to deploy the SageMaker pipeline using CDK (<a href=\"https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html<\/a>) but could not find any code examples. Any pointers?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1652282728443,
        "Question_score":0,
        "Question_tags":"terraform-provider-aws|aws-cdk|amazon-sagemaker",
        "Question_view_count":237,
        "Owner_creation_time":1489685785577,
        "Owner_last_access_time":1663881721683,
        "Owner_location":"Canada",
        "Owner_reputation":65,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Question_last_edit_time":null,
        "Answer_body":"<p>CDK L1 Constructs correspond 1:1 to a CloudFormation resource of the same name. The construct props match the resouce properties.  The go-to source is therefore the CloudFormation docs.<\/p>\n<p>The <code>AWS::SageMaker::Pipeline<\/code> <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html#aws-resource-sagemaker-pipeline--examples\" rel=\"nofollow noreferrer\">docs have a more complete example<\/a>.<\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1652283812907,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72203674",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63199239,
        "Question_title":"Why are shh keys lost on reboot of AWS ec2 instance (sage maker)?",
        "Question_body":"<p>I have an AWS SageMaker notebook running some ML stuff for work, and I have a private github repo with some of my commonly used functions which is formatted in such a way to be pip install-able, so I set up an SSH key by doing this:<\/p>\n<pre><code>ssh-keygen \n\n-t rsa -b 4096 -C &quot;danielwarfield1@gmail.com&quot;\n<\/code><\/pre>\n<p>enter, enter, enter (default save location no password)<\/p>\n<pre><code>eval $(ssh-agent -s)\nssh-add ~\/.ssh\/id_rs\n<\/code><\/pre>\n<p>then I copy the public key into github, then I run this to install my library<\/p>\n<pre><code>$PWD\/pip install git+ssh:\/\/git@github.com\/...\n<\/code><\/pre>\n<p>where <code>$PWD<\/code> is the directory containing pip for the conda env I'm using (tensorflow2_p36 specifically, the one that AWS provides)<\/p>\n<p>this works fine, until I restart the EC2, then it appears my shh key (along with all my other installs) are lost, and I have to repeat the process. I expect the modules to be lost, I know SageMaker manages the environments, but me loosing my ssh key seems peculiar, is there a place I can save my ssh key wher it wont get lost, but I can still find it when I pip install?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596230563253,
        "Question_score":0,
        "Question_tags":"linux|amazon-web-services|amazon-ec2|conda|amazon-sagemaker",
        "Question_view_count":278,
        "Owner_creation_time":1545360696800,
        "Owner_last_access_time":1664070875743,
        "Owner_location":"Earth",
        "Owner_reputation":1011,
        "Owner_up_votes":218,
        "Owner_down_votes":5,
        "Owner_views":93,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The <code>\/home\/ec2-user\/SageMaker<\/code> location is persisted even when you switch down the notebook instance, you can try saving things here to get them persisted. Things saved elsewhere will be lost when you switch off the instance<\/p>\n<p>Regarding private git integration, you can use the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-git-repo.html\" rel=\"nofollow noreferrer\">SageMaker git Notebook integration<\/a>, which uses Secrets Manager to safely handle your credentials<\/p>\n<p>You can perform steps automatically when the notebook starts with a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">lifecycle configuration<\/a>. This is useful for example to standardise and automatise copying of data and environment customization<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1596232881237,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63199239",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62860539,
        "Question_title":"How can I clean memory or use SageMaker instead to avoid MemoryError: Unable to allocate for an array with shape (25000, 2000) and data type float64",
        "Question_body":"<p>I'm using keras to train a model on SageMaker, here's the code I'm using but I hit the error:<\/p>\n<pre><code>MemoryError: Unable to allocate 381. MiB for an array with shape (25000, 2000) \n    and data type float64\n<\/code><\/pre>\n<p>Here's the code:<\/p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom keras.datasets import imdb\nfrom keras import models, layers, optimizers, losses, metrics\nimport matplotlib.pyplot as plt\n\n# load imbd preprocessed dataset\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n    num_words=2000)\n\n# one-hot encoding all the integer into a binary matrix\ndef vectorize_sequences(sequences, dimension=2000):\n    results = np.zeros((len(sequences), dimension))        \n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.                          \n    return results\n\nx_train = vectorize_sequences(train_data)                  \nx_test = vectorize_sequences(test_data)\n<\/code><\/pre>\n<p>Then I get the error.<\/p>\n<p>The first time when I run this code it works but it failed when I tried to re-run it, how I can fix it by cleaning the memory or is there a way that I can use the memory on SageMaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1594553641273,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|tensorflow|keras|amazon-sagemaker",
        "Question_view_count":1210,
        "Owner_creation_time":1540920956270,
        "Owner_last_access_time":1663875036883,
        "Owner_location":"United Kingdom",
        "Owner_reputation":2385,
        "Owner_up_votes":1007,
        "Owner_down_votes":16,
        "Owner_views":585,
        "Question_last_edit_time":1594581031423,
        "Answer_body":"<p>I wouldn't know about SageMaker or AWS specifically, but something you can do is cast your input to <code>float32<\/code>, which takes less memory space. You can cast it like this:<\/p>\n<pre><code>train_data = tf.cast(train_data, tf.float32)\n<\/code><\/pre>\n<p><code>float32<\/code> is the default value of Tensorflow weights so you don't need <code>float64<\/code> anyway. Proof:<\/p>\n<pre><code>import tensorflow as tf\nlayer = tf.keras.layers.Dense(8)\nprint(layer(tf.random.uniform((10, 100), 0, 1)).dtype)\n<\/code><\/pre>\n<pre><code>&lt;dtype: 'float32'&gt;\n<\/code><\/pre>\n<p>My other suggestions are to get less words from your dataset, or to not one-hot encode them. If you're planning on training a recurrent model with an embedding layer, you won't need to anyway.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1594554603093,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62860539",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55112494,
        "Question_title":"Install graphiz on AWS Sagemaker",
        "Question_body":"<p>I'm on a Jupyter notebook using Python3 and trying to plot a tree with code like this:<\/p>\n\n<pre><code>import xgboost as xgb\nfrom xgboost import plot_tree\n\nplot_tree(model, num_trees=4)\n<\/code><\/pre>\n\n<p>On the last line I get:<\/p>\n\n<pre><code>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/xgboost\/plotting.py in to_graphviz(booster, fmap, num_trees, rankdir, yes_color, no_color, **kwargs)\n196         from graphviz import Digraph\n197     except ImportError:\n--&gt; 198         raise ImportError('You must install graphviz to plot tree')\n199 \n200     if not isinstance(booster, (Booster, XGBModel)):\n\nImportError: You must install graphviz to plot tree\n<\/code><\/pre>\n\n<p>How do I install graphviz so I can see the plot_tree?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1552345636840,
        "Question_score":1,
        "Question_tags":"xgboost|amazon-sagemaker",
        "Question_view_count":572,
        "Owner_creation_time":1357263005087,
        "Owner_last_access_time":1663816028883,
        "Owner_location":"Granada Hills, Los Angeles, CA, United States",
        "Owner_reputation":6262,
        "Owner_up_votes":1258,
        "Owner_down_votes":9,
        "Owner_views":428,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I was finally able to learn that Conda has a package which can install it for you. I was able to get it installed by running the command:<\/p>\n\n<pre><code>!conda install python-graphviz --yes\n<\/code><\/pre>\n\n<p>Note the <code>--yes<\/code> is only needed if the installation needs to verify adding\/changing other packages since the Jupyter notebook is not interactive once it is running.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1552366926347,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55112494",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57347278,
        "Question_title":"Does AWS Sagemaker charge for S3 streamed data in PIPE mode (for model training)?",
        "Question_body":"<p>On the AWS developer docs for Sagemaker, they recommend us to use PIPE mode to directly stream large datasets from S3 to the model training containers (since it's faster, uses less disk storage, reduces training time, etc.).<\/p>\n\n<p>However, they don't include information on whether this data streaming transfer is charged for (they only include data transfer pricing for their model building &amp; deployment stages, not training).<\/p>\n\n<p>So, I wanted to ask if anyone knew whether this data transfer in PIPE mode is charged for, since if it is, I don't get how this would be recommended for large datasets, since streaming a few epochs for each model iteration can get prohibitively expensive for large datasets (my dataset, for example, is 6.3TB on S3).<\/p>\n\n<p>Thank you!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1564927931767,
        "Question_score":1,
        "Question_tags":"amazon-web-services|tensorflow|amazon-s3|amazon-sagemaker",
        "Question_view_count":828,
        "Owner_creation_time":1401475981033,
        "Owner_last_access_time":1664084530580,
        "Owner_location":null,
        "Owner_reputation":440,
        "Owner_up_votes":13,
        "Owner_down_votes":1,
        "Owner_views":63,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You are charged for the S3 GET calls that you do similarly to what you would be charged if you used the FILE option of the training. However, these charges are usually marginal compared to the alternatives. <\/p>\n\n<p>When you are using the FILE mode, you need to pay for the local EBS on the instances, and for the extra time that your instances are up and only copying the data from S3. If you are running multiple epochs, you will not benefit much from the PIPE mode, however, when you have so much data (6.3 TB), you don't really need to run multiple epochs. <\/p>\n\n<p>The best usage of PIPE mode is when you can use a <strong>single pass<\/strong> over the data. In the era of big data, this is a better model of operation, as you can't retrain your models often. In SageMaker, you can point to your \"old\" model in the \"model\" channel, and your \"new\" data in the \"train\" channel and benefit from the PIPE mode to the maximum. <\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1564985810717,
        "Answer_score":3,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":1565038250956,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57347278",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":55132599,
        "Question_title":"Difference in usecases for AWS Sagemaker vs Databricks?",
        "Question_body":"<p>I was looking at Databricks because it integrates with AWS services like Kinesis, but it looks to me like SageMaker is a direct competitor to Databricks? We are heavily using AWS, is there any reason to add DataBricks into the stack or odes SageMaker fill the same role?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1552436606600,
        "Question_score":9,
        "Question_tags":"apache-spark|pyspark|databricks|amazon-sagemaker",
        "Question_view_count":11894,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":null,
        "Answer_body":"<p>SageMaker is a great tool for deployment, it simplifies a lot of processes configuring containers, you only need to write 2-3 lines to deploy the model as an endpoint and use it.  SageMaker also provides the dev platform (Jupyter Notebook) which supports Python and Scala (sparkmagic kernal) developing, and i managed installing external scala kernel in jupyter notebook. Overall, SageMaker provides end-to-end ML services. Databricks has unbeatable Notebook environment for Spark development. <\/p>\n\n<p>Conclusion<\/p>\n\n<ol>\n<li><p>Databricks is a better platform for Big data(scala, pyspark) Developing.(unbeatable notebook environment)<\/p><\/li>\n<li><p>SageMaker is better for Deployment. and if you are not working on big data, SageMaker is a perfect choice working with (Jupyter notebook + Sklearn + Mature containers + Super easy deployment). <\/p><\/li>\n<li><p>SageMaker provides \"real time inference\", very easy to build and deploy, very impressive. you can check the official SageMaker Github.\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline<\/a><\/p><\/li>\n<\/ol>",
        "Answer_comment_count":4,
        "Answer_creation_time":1553118034270,
        "Answer_score":14,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55132599",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68790568,
        "Question_title":"\"errorMessage\": \"Parameter validation failed in Lambda calling SageMaker endpoint",
        "Question_body":"<p>I am trying to invoke a SageMaker enpoint from AWS Lambda using a lambda function.<\/p>\n<p>This is a sample API call to the endpoint from SageMaker Studio, working as expected:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3iTPN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3iTPN.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>here's my Lambda function (<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">inspired from documentation<\/a>):<\/p>\n<pre><code>import os\nimport io\nimport boto3\nimport json\n\n\nENDPOINT_NAME = 'iris-autoscale-6'\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    # print(&quot;Received event: &quot; + json.dumps(event, indent=2))\n    payload = json.loads(json.dumps(event))\n    print(payload)\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\n    print(response)\n    result = json.loads(response['Body'].read().decode())\n    print(result)\n    \n    return result\n<\/code><\/pre>\n<p>My error message:<\/p>\n<pre><code>Test Event Name\nProperTest\n\nResponse\n{\n  &quot;errorMessage&quot;: &quot;Parameter validation failed:\\nInvalid type for parameter Body, value: {'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object&quot;,\n  &quot;errorType&quot;: &quot;ParamValidationError&quot;,\n  &quot;stackTrace&quot;: [\n    &quot;  File \\&quot;\/var\/task\/lambda_function.py\\&quot;, line 17, in lambda_handler\\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 386, in _api_call\\n    return self._make_api_call(operation_name, kwargs)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 678, in _make_api_call\\n    api_params, operation_model, context=request_context)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 726, in _convert_to_request_dict\\n    api_params, operation_model)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/validate.py\\&quot;, line 319, in serialize_to_request\\n    raise ParamValidationError(report=report.generate_report())\\n&quot;\n  ]\n}\n\nFunction Logs\nSTART RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512 Version: $LATEST\n{'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}\n[ERROR] ParamValidationError: Parameter validation failed:\nInvalid type for parameter Body, value: {'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\nTraceback (most recent call last):\n\u00a0\u00a0File &quot;\/var\/task\/lambda_function.py&quot;, line 17, in lambda_handler\n\u00a0\u00a0\u00a0\u00a0response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 386, in _api_call\n\u00a0\u00a0\u00a0\u00a0return self._make_api_call(operation_name, kwargs)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 678, in _make_api_call\n\u00a0\u00a0\u00a0\u00a0api_params, operation_model, context=request_context)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 726, in _convert_to_request_dict\n\u00a0\u00a0\u00a0\u00a0api_params, operation_model)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/validate.py&quot;, line 319, in serialize_to_request\n\u00a0\u00a0\u00a0\u00a0raise ParamValidationError(report=report.generate_report())\nEND RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512\nREPORT RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512  Duration: 26.70 ms  Billed Duration: 27 ms  Memory Size: 128 MB Max Memory Used: 76 MB  Init Duration: 343.10 ms\n<\/code><\/pre>\n<p>Here's the policy attached to the lambda function:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;VisualEditor0&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:sagemaker:ap-south-1:&lt;my-account-id&gt;:endpoint\/iris-autoscale-6&quot;\n        }\n    ]\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629022235437,
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-lambda|boto3|amazon-sagemaker",
        "Question_view_count":499,
        "Owner_creation_time":1559910246180,
        "Owner_last_access_time":1664039951323,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":2046,
        "Owner_up_votes":2858,
        "Owner_down_votes":5,
        "Owner_views":369,
        "Question_last_edit_time":1629023309567,
        "Answer_body":"<p>The issue is that your <code>payload<\/code> has invalid format. It should be one of:<\/p>\n<pre><code>&lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\n<\/code><\/pre>\n<p>The following should address the error (note: you may have many other issues in your code):<\/p>\n<pre><code>    payload = json.dumps(event)\n    print(payload)\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload.encode())\n<\/code><\/pre>",
        "Answer_comment_count":1,
        "Answer_creation_time":1629024434667,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68790568",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65054187,
        "Question_title":"Can you use sagemaker Python libraries on my localhost?",
        "Question_body":"<p>I am interested if one can import sagemaker packages on your own local Python environment or whether they are restricted to AWS Sagemaker?<\/p>\n<pre><code>from sagemaker_automl import AutoMLInteractiveRunner, AutoMLLocalCandidate\n<\/code><\/pre>\n<p>For instance can I somehow download the sagemaker_automl?<\/p>\n<p>I know the there are no sagemaker packages available in the conda repository. Perhaps there is some other way to get them.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1606595637157,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":162,
        "Owner_creation_time":1310821356880,
        "Owner_last_access_time":1664050779590,
        "Owner_location":"Slovenia",
        "Owner_reputation":14913,
        "Owner_up_votes":307,
        "Owner_down_votes":1,
        "Owner_views":1093,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The Sagemaker Python SDK is <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">open source and on GitHub<\/a>, as well as published on <a href=\"https:\/\/pypi.org\/project\/sagemaker\/\" rel=\"nofollow noreferrer\">Pypi<\/a>.<\/p>\n<p>You can install it by running:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install sagemaker\n<\/code><\/pre>",
        "Answer_comment_count":2,
        "Answer_creation_time":1606601111440,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65054187",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51790720,
        "Question_title":"Brewing up custom ML models on AWS SageMaker",
        "Question_body":"<p>Iam new with SageMaker and I try to use my own sickit-learn algorithm . For this I use Docker.\nI try to do the same task as described here in this github account : <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a><\/p>\n\n<p>My question is should I create manually the repository <strong><code>\/opt\/ml<\/code><\/strong>  (I work with windows OS) ?<\/p>\n\n<p>Can you explain me please?<\/p>\n\n<p>thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1533919042793,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":106,
        "Owner_creation_time":1518617852857,
        "Owner_last_access_time":1572431644507,
        "Owner_location":null,
        "Owner_reputation":495,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":81,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You don't need to create <code>\/opt\/ml<\/code>, SageMaker will do it for you when it launches your training job.<\/p>\n\n<p>The contents of the <code>\/opt\/ml<\/code> directory are determined by the parameters you pass to the CreateTrainingJob API call. The scikit example notebook you linked to describes this (look at the <strong>Running your container<\/strong> sections). You can find more info about this in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">Create a Training Job<\/a> section of the main SageMaker documentation.<\/p>\n\n<hr>",
        "Answer_comment_count":0,
        "Answer_creation_time":1533928824743,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51790720",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66656120,
        "Question_title":"SageMaker TF 2.3 distributed training",
        "Question_body":"<p>Using SageMaker v2.29.2 and Tensorflow v2.3.2 I'm trying to implement distributed training as explained in the following blogpost:<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23<\/a><\/p>\n<p>However I'm having difficulties importing the smdistributed script.<\/p>\n<p>Here is my code:<\/p>\n<pre><code>import tensorflow as tf\nimport smdistributed.modelparallel.tensorflow as smp\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;temp.py&quot;, line 2, in &lt;module&gt;\n    import smdistributed.modelparallel.tensorflow as smp\nModuleNotFoundError: No module named 'smdistributed'\n<\/code><\/pre>\n<p>What am I missing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1615901050403,
        "Question_score":2,
        "Question_tags":"python|tensorflow|amazon-sagemaker",
        "Question_view_count":383,
        "Owner_creation_time":1324808381143,
        "Owner_last_access_time":1663842092260,
        "Owner_location":null,
        "Owner_reputation":9050,
        "Owner_up_votes":1458,
        "Owner_down_votes":21,
        "Owner_views":1750,
        "Question_last_edit_time":1615921589292,
        "Answer_body":"<p>smdistributed is only available on the SageMaker containers. It is supported for specific TensorFlow versions and you must add:<\/p>\n<pre><code>distribution={'smdistributed': {\n            'dataparallel': {\n                'enabled': True\n            }\n        }}\n<\/code><\/pre>\n<p>On the estimator code in order to enable it<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1623834809930,
        "Answer_score":0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66656120",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73058582,
        "Question_title":"Access denied for aws public sagemaker xgboost registry",
        "Question_body":"<p>I am trying to pull a prebuilt xgboost image from the public aws xgboost registry specified here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ecr-us-west-2.html#xgboost-us-west-2.title\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ecr-us-west-2.html#xgboost-us-west-2.title<\/a>, however whenever I run the sagemaker pipeline I get the error:<\/p>\n<pre><code>ClientError: Failed to invoke sagemaker:CreateModelPackage. \nError Details: Access denied for registry ID: 246618743249, repository name: sagemaker-xgboost. \nPlease check if your ECR image exists and has proper pull permissions for SageMaker.\n<\/code><\/pre>\n<p>Here is the attached role boundary I am using to run the pipeline:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Action&quot;: [\n                &quot;codebuild:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;codepipeline:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;events:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;logs:CreateLogGroup&quot;,\n                &quot;logs:CreateLogStream&quot;,\n                &quot;logs:DescribeLogStreams&quot;,\n                &quot;logs:DescribeLogGroups&quot;,\n                &quot;logs:PutLogEvents&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;iam:PassRole&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:iam::xxxxxxxxxxxx:role\/ml-*&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;ecr:*&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:ecr:us-west-2:246618743249:repository\/246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-xgboost:1.0-1&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;ecr:GetAuthorizationToken&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;*&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        }\n    ]\n}\n<\/code><\/pre>\n<p>and below is the attached policies for the role:<\/p>\n<pre><code>{\n    &quot;Statement&quot;: [\n        {\n            &quot;Action&quot;: &quot;ecr:*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Sid&quot;: &quot;&quot;\n        }\n    ],\n    &quot;Version&quot;: &quot;2012-10-17&quot;\n}\n<\/code><\/pre>\n<p>plus the AWSCodePipelineFullAccess, AWSCodeBuildAdminAccess, and AmazonSageMakerFullAccess managed policies.<\/p>\n<p>Why can't I access the image\/why am I getting this error? As you can see I gave my role full permissions for the ecr registry in the boundary, and full permissions for ecr in the attached policy.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1658354562373,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-iam|amazon-sagemaker|amazon-ecr",
        "Question_view_count":61,
        "Owner_creation_time":1653511725307,
        "Owner_last_access_time":1663251784530,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":1658354892732,
        "Answer_body":"<p>I had to change the boundary to be this: <code> arn:aws:ecr:us-west-2:246618743249:repository\/sagemaker-xgboost<\/code><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1658508649573,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73058582",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58237749,
        "Question_title":"AWS Glue Sagemaker Notebook \"No module named awsglue.transforms\"",
        "Question_body":"<p>I've created a Sagemaker notebook to dev AWS Glue jobs, but when running through the provided example (\"Joining, Filtering, and Loading Relational Data with AWS Glue\") I get the following error:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/3hF5s.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3hF5s.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Does anyone know what I've setup wrong\/haven't setup to cause the import to not work?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1570197038960,
        "Question_score":3,
        "Question_tags":"amazon-web-services|aws-glue|amazon-sagemaker",
        "Question_view_count":3551,
        "Owner_creation_time":1384795490587,
        "Owner_last_access_time":1663932905217,
        "Owner_location":null,
        "Owner_reputation":1012,
        "Owner_up_votes":1106,
        "Owner_down_votes":1,
        "Owner_views":102,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You'll need to download the library files from <a href=\"https:\/\/s3.amazonaws.com\/aws-glue-jes-prod-us-east-1-assets\/etl\/python\/PyGlue.zip\" rel=\"nofollow noreferrer\">here<\/a> for Glue 0.9 or <a href=\"https:\/\/s3.amazonaws.com\/aws-glue-jes-prod-us-east-1-assets\/etl-1.0\/python\/PyGlue.zip\" rel=\"nofollow noreferrer\">here<\/a> for Glue 1.0 (Check your Glue jobs for the version). <\/p>\n\n<p>Put the zip in S3 and reference it in the \"Python library path\" on your Dev Endpoint.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1570201013273,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58237749",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62142825,
        "Question_title":"How to Enable SageMaker Debugger in the SageMaker AutoPilot",
        "Question_body":"<p>I'd like to (a) plot SHAP values out of the SageMaker (b) AutoML pipeline. To achieve (a), debugger shall be used according to: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/<\/a>.<\/p>\n\n<p>But how to enable the debug model in the AutoPilot without hacking into the background?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591056499550,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|amazon-machine-learning",
        "Question_view_count":215,
        "Owner_creation_time":1392607100777,
        "Owner_last_access_time":1664069299613,
        "Owner_location":"Sydney NSW, Australia",
        "Owner_reputation":133,
        "Owner_up_votes":304,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Question_last_edit_time":null,
        "Answer_body":"<p>SageMaker Autopilot doesn't support SageMaker Debugger out of the box currently (as of Dec 2020). You can hack the Hyperparameter Tuning job to pass in a debug parameter.<\/p>\n<p>However, there is a way to use SHAP with Autopilot models. Take a look at this blog post explaining how to use SHAP with SageMaker Autopilot: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/<\/a>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1607117580710,
        "Answer_score":0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62142825",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62319753,
        "Question_title":"How to I pass secrets stored in AWS Secret Manager to a Docker container in Sagemaker?",
        "Question_body":"<p>My code is in R. And I need to excess external database. I am storing database credentials in AWS Secret Manager.<\/p>\n<p>So I first tried using paws library to get aws secrets in R but that would require storing access key, secret id and session token, and I want to avoid that.<\/p>\n<p>Is there a better way to do this? I have created IAM role for Sagemaker. Is it possible to pass secrets as environment variables?<\/p>\n<p>Edit: I wanted to trigger Sagemaker Processing<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1591862670733,
        "Question_score":0,
        "Question_tags":"r|amazon-web-services|docker|amazon-sagemaker",
        "Question_view_count":1063,
        "Owner_creation_time":1444035983570,
        "Owner_last_access_time":1656492622493,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":1593501623132,
        "Answer_body":"<p>I found a simple solution to it. Env variables can be passed via Sagemaker sdk. It minimizes the dependencies.<\/p>\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html<\/a><\/p>\n<p>As another answer suggested, paws can be used as well to get secrets from aws. This would be a better approach<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1593501738047,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1602059220016,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62319753",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52684987,
        "Question_title":"AWS NoCredentials in training",
        "Question_body":"<p>I am attempting to run the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">example code<\/a> for Amazon Sagemaker on a local GPU.  I have copied the code from the Jupyter notebook to the following Python script:<\/p>\n\n<pre><code>import boto3\nimport subprocess\nimport sagemaker\nfrom sagemaker.mxnet import MXNet\nfrom mxnet import gluon\nfrom sagemaker import get_execution_role\nimport os\n\nsagemaker_session = sagemaker.Session()\ninstance_type = 'local'\nif subprocess.call('nvidia-smi') == 0:\n    # Set type to GPU if one is present\n    instance_type = 'local_gpu'\n# role = get_execution_role()\n\ngluon.data.vision.MNIST('.\/data\/train', train=True)\ngluon.data.vision.MNIST('.\/data\/test', train=False)\n\n# successfully connects and uploads data\ninputs = sagemaker_session.upload_data(path='data', key_prefix='data\/mnist')\n\nhyperparameters = {\n    'batch_size': 100,\n    'epochs': 20,\n    'learning_rate': 0.1,\n    'momentum': 0.9,\n    'log_interval': 100\n}\n\nm = MXNet(\"mnist.py\",\n          role=role,\n          train_instance_count=1,\n          train_instance_type=instance_type,\n          framework_version=\"1.1.0\",\n          hyperparameters=hyperparameters)\n\n# fails in Docker container\nm.fit(inputs)\npredictor = m.deploy(initial_instance_count=1, instance_type=instance_type)\nm.delete_endpoint()\n<\/code><\/pre>\n\n<p>where the referenced <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mnist.py\" rel=\"nofollow noreferrer\">mnist.py<\/a> file is exactly as specified on Github. The script fails on <code>m.fit<\/code> in Docker container with the following error: <\/p>\n\n<pre><code>algo-1-1DUU4_1  | Downloading s3:\/\/&lt;S3-BUCKET&gt;\/sagemaker-mxnet-2018-10-07-00-47-10-435\/source\/sourcedir.tar.gz to \/tmp\/script.tar.gz\nalgo-1-1DUU4_1  | 2018-10-07 00:47:29,219 ERROR - container_support.training - uncaught exception during training: Unable to locate credentials\nalgo-1-1DUU4_1  | Traceback (most recent call last):\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/training.py\", line 36, in start\nalgo-1-1DUU4_1  |     fw.train()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/mxnet_container\/train.py\", line 169, in train\nalgo-1-1DUU4_1  |     mxnet_env.download_user_module()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/environment.py\", line 89, in download_user_module\nalgo-1-1DUU4_1  |     cs.download_s3_resource(self.user_script_archive, tmp)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/utils.py\", line 37, in download_s3_resource\nalgo-1-1DUU4_1  |     script_bucket.download_file(script_key_name, target)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 246, in bucket_download_file\nalgo-1-1DUU4_1  |     ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 172, in download_file\nalgo-1-1DUU4_1  |     extra_args=ExtraArgs, callback=Callback)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/transfer.py\", line 307, in download_file\nalgo-1-1DUU4_1  |     future.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 73, in result\nalgo-1-1DUU4_1  |     return self._coordinator.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 233, in result\nalgo-1-1DUU4_1  |     raise self._exception\nalgo-1-1DUU4_1  | NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n\n<p>I am confused that I can authenticate to S3 outside of the container (to pload the training\/test data) but I cannot within the Docker container.  So I am guessing the issues has to do with passing the AWS credentials to the Docker container.  Here is the generated Docker-compose file:<\/p>\n\n<pre><code>networks:\n  sagemaker-local:\n    name: sagemaker-local\nservices:\n  algo-1-1DUU4:\n    command: train\n    environment:\n    - AWS_REGION=us-west-2\n    - TRAINING_JOB_NAME=sagemaker-mxnet-2018-10-07-00-47-10-435\n    image: 123456789012.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.1.0-gpu-py2\n    networks:\n      sagemaker-local:\n        aliases:\n        - algo-1-1DUU4\n    stdin_open: true\n    tty: true\n    volumes:\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/input:\/opt\/ml\/input\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output:\/opt\/ml\/output\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output\/data:\/opt\/ml\/output\/data\n    - \/tmp\/tmpSkaR3x\/model:\/opt\/ml\/model\nversion: '2.1'\n<\/code><\/pre>\n\n<p>Should the AWS credentials be passed in as enviromental variables?<\/p>\n\n<p>I upgraded my <code>sagemaker<\/code> install to after reading <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/403\" rel=\"nofollow noreferrer\">Using boto3 in install local mode?<\/a>, but that had no effect.  I checked the credentials that are being fetched in the Sagemaker session (outside the container) and they appear to be blank, even though I have an <code>~\/.aws\/config<\/code> and <code>~\/.aws\/credentials<\/code> file:<\/p>\n\n<pre><code>{'_token': None, '_time_fetcher': &lt;function _local_now at 0x7f4dbbe75230&gt;, '_access_key': None, '_frozen_credentials': None, '_refresh_using': &lt;bound method AssumeRoleCredentialFetcher.fetch_credentials of &lt;botocore.credentials.AssumeRoleCredentialFetcher object at 0x7f4d2de48bd0&gt;&gt;, '_secret_key': None, '_expiry_time': None, 'method': 'assume-role', '_refresh_lock': &lt;thread.lock object at 0x7f4d9f2aafd0&gt;}\n<\/code><\/pre>\n\n<p>I am new to AWS so I do not know how to diagnose the issue regarding AWS credentials.  My <code>.aws\/config<\/code> file has the following information (with placeholder values):<\/p>\n\n<pre><code>[default]\noutput = json\nregion = us-west-2\nrole_arn = arn:aws:iam::123456789012:role\/SageMakers\nsource_profile = sagemaker-test\n\n[profile sagemaker-test]\noutput = json\nregion = us-west-2\n<\/code><\/pre>\n\n<p>Where the <code>sagemaker-test<\/code> profile has <code>AmazonSageMakerFullAccess<\/code> in the IAM Management Console.<\/p>\n\n<p>The <code>.aws\/credentials<\/code> file has the following information (represented by placeholder values):<\/p>\n\n<pre><code>[default]\naws_access_key_id = 1234567890\naws_secret_access_key = zyxwvutsrqponmlkjihgfedcba\n[sagemaker-test]\naws_access_key_id = 0987654321\naws_secret_access_key = abcdefghijklmopqrstuvwxyz\n<\/code><\/pre>\n\n<p>Lastly, these are versions of the applicable libraries from a <code>pip freeze<\/code>:<\/p>\n\n<pre><code>awscli==1.16.19\nboto==2.48.0\nboto3==1.9.18\nbotocore==1.12.18\ndocker==3.5.0\ndocker-compose==1.22.0\nmxnet-cu91==1.1.0.post0\nsagemaker==1.11.1\n<\/code><\/pre>\n\n<p>Please let me know if I left out any relevant information and thanks for any help\/feedback that you can provide.<\/p>\n\n<p><strong>UPDATE<\/strong>: Thanks for your help, everyone! While attempting some of your suggested fixes, I noticed that <code>boto3<\/code> was out of date, and update it (to <code>boto3-1.9.26<\/code> and <code>botocore-1.12.26<\/code>) which appeared to resolve the issue.  I was not able to find any documentation on that being an issue with <code>boto3==1.9.18<\/code>.  If someone could help me understand what the issue was with <code>boto3<\/code>, I would happy to make mark their answer as correct.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":3,
        "Question_creation_time":1538879533133,
        "Question_score":1,
        "Question_tags":"python|python-2.7|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":1374,
        "Owner_creation_time":1380496984177,
        "Owner_last_access_time":1663610467730,
        "Owner_location":"Gloucester, VA, USA",
        "Owner_reputation":544,
        "Owner_up_votes":174,
        "Owner_down_votes":1,
        "Owner_views":85,
        "Question_last_edit_time":1539893383687,
        "Answer_body":"<p>SageMaker local mode is designed to pick up whatever credentials are available in your boto3 session, and pass them into the docker container as environment variables. <\/p>\n\n<p>However, the version of the sagemaker sdk that you are using (1.11.1 and earlier) will ignore the credentials if they include a token, because that usually indicates short-lived credentials that won't remain valid long enough for a training job to complete or endpoint to be useful.<\/p>\n\n<p>If you are using temporary credentials, try replacing them with permanent ones, or running from an ec2 instance (or SageMaker notebook!) that has an appropriate instance role assigned.<\/p>\n\n<p>Also, the sagemaker sdk's handling of credentials changed in v1.11.2 and later -- temporary credentials will be passed to local mode containers, but with a warning message. So you could just upgrade to a newer version and try again (<code>pip install -U sagemaker<\/code>). <\/p>\n\n<p>Also, try upgrading <code>boto3<\/code> can change, so try using the latest version.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1539818808057,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1542529234207,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52684987",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":51725489,
        "Question_title":"QuickSight using ML",
        "Question_body":"<p>I would like to use the ML model I created in AWS in my QuickSight reports.<\/p>\n\n<ul>\n<li>Is there a way to consume the ML endpoint in order to run batch predictions in QuickSight?<\/li>\n<li>Can I define a 'calculated field' in order to do that?<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1533640683263,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-quicksight",
        "Question_view_count":490,
        "Owner_creation_time":1501420854057,
        "Owner_last_access_time":1663846017710,
        "Owner_location":null,
        "Owner_reputation":507,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":78,
        "Question_last_edit_time":null,
        "Answer_body":"<p>At this time there is no direct integration with AWS SageMaker and QuickSight, however you can use utilize SageMaker's batch transform jobs to convert data outside of QuickSight and then import this information into QuickSight for visualization. The output format for SageMaker's batch transform jobs is S3, which is a supported input data source for QuickSight.<\/p>\n\n<ul>\n<li><a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2018\/07\/amazon-sagemaker-supports-high-throughput-batch-transform-jobs-for-non-real-time-inferencing\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/about-aws\/whats-new\/2018\/07\/amazon-sagemaker-supports-high-throughput-batch-transform-jobs-for-non-real-time-inferencing\/<\/a><\/li>\n<\/ul>\n\n<p>Depending on how fancy you want to be, you can also integrate calls to AWS services such as AWS Lambda or AWS SageMaker as a user-defined function (UDF) within your datastore. Here are a few resources that may help:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/redshift\/latest\/dg\/user-defined-functions.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/redshift\/latest\/dg\/user-defined-functions.html<\/a><\/li>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/big-data\/from-sql-to-microservices-integrating-aws-lambda-with-relational-databases\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/big-data\/from-sql-to-microservices-integrating-aws-lambda-with-relational-databases\/<\/a><\/li>\n<\/ul>\n\n<p>Calculated fields will probably not help you in this regard - calculated fields are restricted to a relatively small set of operations, and none of these operations support calls to external sources.<\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/quicksight\/latest\/user\/calculated-field-reference.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/quicksight\/latest\/user\/calculated-field-reference.html<\/a> <\/li>\n<\/ul>",
        "Answer_comment_count":0,
        "Answer_creation_time":1534265395897,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51725489",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68396088,
        "Question_title":"Why AWS Lambda Internel Server Error 500 but successfully \/invocations POST 200 in Endpoint SageMaker?",
        "Question_body":"<pre><code>import os\nimport io\nimport boto3\nimport json\nimport csv\n\n\n# grab environment variables\nENDPOINT_NAME = os.environ['ENDPOINT_NAME']\n# grab runtime client\nruntime = boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    # Load data from POST request\n    data = json.loads(json.dumps(event))\n    \n    # Grab the payload\n    payload = data['body']\n    \n    # Invoke the model. In this case the data type is a JSON but can be other things such as a CSV\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                   ContentType='application\/json',\n                                   Body=payload)\n    \n    # Get the body of the response from the model\n    result = response['Body'].read().decode()\n\n    # Return it along with the status code of 200 meaning this was succesful \n    return {\n        'statusCode': 200,\n        'body': result\n    }\n<\/code><\/pre>\n<p><strong>response from AWS Lambda<\/strong><\/p>\n<pre><code>{\n  &quot;errorMessage&quot;: &quot;'body'&quot;,\n  &quot;errorType&quot;: &quot;KeyError&quot;,\n  &quot;stackTrace&quot;: [\n    [\n      &quot;\/var\/task\/lambda_function.py&quot;,\n      18,\n      &quot;lambda_handler&quot;,\n      &quot;payload = data['body']&quot;\n    ]\n  ]\n}\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/h8wvA.png\" rel=\"nofollow noreferrer\">response from Postman 500 Internal Server Error<\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/cuknX.png\" rel=\"nofollow noreferrer\">but successfully invoke POST 200 in SageMaker Endpoint<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1626360722473,
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":277,
        "Owner_creation_time":1512933739527,
        "Owner_last_access_time":1643017987330,
        "Owner_location":"Petaling Jaya, Selangor, Malaysia",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1626360827356,
        "Answer_body":"<p>The issue is when you are trying to parse your payload with data['body']. The data is not being passed in the format that the endpoint is expecting. Use the following code snippet to properly format\/serialize your data for the endpoint. Also to make all this clearer make sure to check for your payload type to make sure you have not serialized again by accident.<\/p>\n<pre><code>    data = json.loads(json.dumps(event))\n    payload = json.dumps(data)\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                       ContentType='application\/json',\n                                       Body=payload)\n    result = json.loads(response['Body'].read().decode())\n<\/code><\/pre>\n<p>I work for AWS &amp; my opinions are my own<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1626975923230,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1626979396583,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68396088",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69335737,
        "Question_title":"How to call a python file in aws sagemaker from angular application?",
        "Question_body":"<p>I have created an angular application that takes an image as input, the image is then passed to a python script that performs a neural style transfer and returns the stylized image. I have created the python file and the angular frontend seperately and I'm stuck on the integration. I am using aws sagemaker to run the python script (due to its computation speed) but I have no idea how to call the python script with the image passed to it from angular. Any suggestions would be really appreciated. Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1632665409997,
        "Question_score":0,
        "Question_tags":"python|angular|amazon-web-services|amazon-sagemaker",
        "Question_view_count":85,
        "Owner_creation_time":1601448674660,
        "Owner_last_access_time":1654501250620,
        "Owner_location":null,
        "Owner_reputation":45,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can create a lambda function and expose it using API gateway to be called by your angular app. this lambda in return will call the  Sagemaker function you have<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1632665752713,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69335737",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54154455,
        "Question_title":"How do you invoke a sagemaker xgboost endpoint from a chalice app?",
        "Question_body":"<p>I built a chalice web-app that is hosted in an s3 bucket and calls an xgboost endpoint. I keep getting an error when I invoke the model through the web-app. When I looked into the Lambda log files I discovered my input is not properly decoding. <code>input_text = app.current_request.raw_body.decode()<\/code> What would be the correct code to decode the input from binary so I can pass in a regular string to my endpoint?<\/p>\n\n<p>Here is the error:<\/p>\n\n<p>botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message \"could not convert string to float: user_input=1%\". <\/p>\n\n<p>Here is my index.html file:<\/p>\n\n<pre><code>&lt;html&gt;\n&lt;head&gt;&lt;\/head&gt;\n&lt;body&gt;\n&lt;form method=\"post\" action=\"&lt;chalice_deployed_http&gt;\"&gt;\n\n&lt;input type=\"text\" name=\"user_input\"&gt;&lt;br&gt;\n\n&lt;input type=\"submit\" value=\"Submit\"&gt;\n&lt;\/form&gt;\n&lt;\/body&gt;\n&lt;\/html&gt;\n<\/code><\/pre>\n\n<p>Here is my app.py file:<\/p>\n\n<pre><code>try:\n    from StringIO import StringIO\nexcept ImportError:\n    from io import StringIO\n\nfrom io import BytesIO\nimport csv\nimport sys, os, base64, datetime, hashlib, hmac\nfrom chalice import Chalice, NotFoundError, BadRequestError\nimport boto3\n\n\napp = Chalice(app_name='&lt;name_of_chalice_app&gt;')\napp.debug = True\n\nsagemaker = boto3.client('sagemaker-runtime')\n\n@app.route('\/', methods=['POST'], content_types=['application\/x-www-form-urlencoded'])\ndef handle_data():\n    input_text = app.current_request.raw_body.decode()\n\n    res = sagemaker.invoke_endpoint(\n                    EndpointName='&lt;endpoint_name&gt;',\n                    Body=input_text,\n                    ContentType='text\/csv',\n                    Accept='Accept'\n                )\n    return res['Body'].read().decode()[0]\n<\/code><\/pre>\n\n<p>I should be able to pass in a string like this:<\/p>\n\n<p>'1,4,26,0.076923077,2,3,1,0.611940299,0.7818181820000001,0.40376569,0.571611506,0.12,12,1,0.0,2,1.0,1,2,6,3,1,1,1,1,1,3,1,0.000666667,1,1,2,2,-1.0,0.490196078,-1.0,0.633928571,6.0,145,2,2,1,3,2,2,1,3,2,3,3,-1.0,1,3,1,1,2,1,2,3,1,3,3,1,3,2,3,-1.0,3,3,1,2,2,1,3,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,0.3497921158934803,0'<\/p>\n\n<p>and get output like this:<\/p>\n\n<p>'5'<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/sb5Nw.jpg\" rel=\"nofollow noreferrer\">When I run it in a jupyter notebook it works.<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1547243089770,
        "Question_score":0,
        "Question_tags":"python|web-applications|chalice|amazon-sagemaker",
        "Question_view_count":805,
        "Owner_creation_time":1546566576913,
        "Owner_last_access_time":1614803818790,
        "Owner_location":null,
        "Owner_reputation":25,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1547479657316,
        "Answer_body":"<p>This worked:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>    input_text = app.current_request.raw_body\n    d = parse_qs(input_text)\n    lst = d[b'user_input'][0].decode()\n    res = sagemaker.invoke_endpoint(\n                    EndpointName='&lt;name-of-SageMaker-Endpoint&gt;',\n                    Body=lst,\n                    ContentType='text\/csv',\n                    Accept='Accept'\n                )\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1547657753670,
        "Answer_score":1,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54154455",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73520188,
        "Question_title":"How to get batch predictions with jsonl data in sagemaker?",
        "Question_body":"<p>I have a pytorch model that i have tested as a real-time endpoint in sagemaker, now i want to test it with batch inference. I am using jsonl data, and setting up a batch transform job as documented in aws documentation, in addition, i'm using my own inference.py (see sample below). I'm getting a json decode error inside the input_fn , function, when i try =&gt; json.loads(request_body).<\/p>\n<p>the error is =&gt; raise JSONDecodeError(&quot;Extra data&quot;, s, end)<\/p>\n<p>has anyone tried this? I sucessfully tested this model and json input with a real time endpoint in sagemaker, but now i'm trying to switch to batch and it is erroring it out.<\/p>\n<p>inference.py<\/p>\n<pre><code>def model_fn(model_dir):\n   ....\n\n\ndef input_fn(request_body, request_content_type):\n    data = json.loads(request_body)\n    return data\n\ndef predict_fn(data, model)\n  ...\n<\/code><\/pre>\n<p>set up for batch job via lambda<\/p>\n<pre><code>response = client.create_transform_job(\n    TransformJobName='some-job',\n    ModelName='mypytorchmodel',\n    ModelClientConfig={\n        'InvocationsTimeoutInSeconds': 3600,\n        'InvocationsMaxRetries': 1\n    },\n    BatchStrategy='MultiRecord',\n    TransformInput={\n        'DataSource': {\n            'S3DataSource': {\n                'S3DataType': 'S3Prefix',\n                'S3Uri': 's3:\/\/inputpath'\n            }\n        },\n        'ContentType': 'application\/json',\n        'SplitType': 'Line'\n    },\n    TransformOutput={\n        'S3OutputPath': 's3:\/\/outputpath',\n        'Accept': 'application\/json',\n        'AssembleWith': 'Line',\n    },\n    TransformResources={\n        'InstanceType': 'ml.g4dn.xlarge'\n        'InstanceCount': 1\n    }\n)\n<\/code><\/pre>\n<p>input file<\/p>\n<pre><code>{&quot;input&quot; : &quot;input line one&quot;}\n{&quot;input&quot; : &quot;input line two&quot;}\n{&quot;input&quot; : &quot;input line three&quot;}\n{&quot;input&quot; : &quot;input line four&quot;}\n{&quot;input&quot; : &quot;input line five&quot;}\n...\n\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661702506533,
        "Question_score":1,
        "Question_tags":"python|aws-lambda|amazon-sagemaker|inference",
        "Question_view_count":52,
        "Owner_creation_time":1584308275360,
        "Owner_last_access_time":1664049064830,
        "Owner_location":null,
        "Owner_reputation":365,
        "Owner_up_votes":53,
        "Owner_down_votes":2,
        "Owner_views":94,
        "Question_last_edit_time":null,
        "Answer_body":"<p>What is your client side code where you are invoking the endpoint? You should also be properly serializing the data on the client side and handling it in your inference script. Example:<\/p>\n<pre><code>import json\ndata = json.loads(json.dumps(request_body))\npayload = json.dumps(data)\nresponse = client.invoke_endpoint(\n    EndpointName=endpoint_name,\n    ContentType=content_type,\n    Body=payload)\nresult = json.loads(response['Body'].read().decode())['Output']\nresult\n<\/code><\/pre>\n<p>Make sure to also specify your content_type appropriately &quot;application\/jsonlines&quot;.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1661785882720,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73520188",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":62919671,
        "Question_title":"conda env build fails with \"[Errno 28] No space left on device\"",
        "Question_body":"<p>I'm trying to build a new conda environment in our Sagemaker ec2 environment in a terminal session.  Packages in the original copy of the environment were corrupted, and the environment became unusable. The issue couldn't be fixed by removing packages and re-installing or using <code>conda update<\/code>.<\/p>\n<p>I nuked the environment with <code>conda env remove -n python3-cn<\/code> and then attempted to recreate the environment with:<\/p>\n<pre><code>conda env create -p \/home\/ec2-user\/SageMaker\/anaconda3\/envs\/python3-cn --file=${HOME}\/SageMaker\/efs\/.sagemaker\/python3-cn_environment.yml --force\n<\/code><\/pre>\n<p>This environment has been created a number of times in several ec2 instances for individual Sagemaker users.<\/p>\n<p>Conda logs the following:<\/p>\n<pre><code>Collecting package metadata (repodata.json): done\nSolving environment: done\n\nDownloading and Extracting Packages\npytest-arraydiff-0.2 | 14 KB     | ##################################################################################################### | 100% \npartd-0.3.8          | 32 KB     | ##################################################################################################### | 100% \n\n... several progress bar lines later...\n\npsycopg2-2.7.5       | 507 KB    | ##################################################################################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\nERROR conda.core.link:_execute(700): An error occurred while installing package 'defaults::mkl-2018.0.3-1'.\nRolling back transaction: done\n\n[Errno 28] No space left on device\n()\n<\/code><\/pre>\n<p>The <code>No space left on device<\/code> error is consistent. I've tried<\/p>\n<ul>\n<li><code>conda clean --all<\/code>, removing the environment, re-building the environment<\/li>\n<li>removing the caches, removing the environment, re-building the environment<\/li>\n<li>removing the environment, shutting down and restarting JuypiterLab (our Sagemaker is configured to create <code>python3-cn<\/code> if the environment doesn't exist when JupyterLab starts)<\/li>\n<\/ul>\n<p>In the first two, I get <code>Errno 28<\/code>.<\/p>\n<p>In the last one, the instance is not created, <code>conda env list<\/code> does not show the <code>python3-cn<\/code>, but I see there is a <code>python3-cn<\/code> directory in the <code>anaconda\/envs\/<\/code> directory. If I do <code>conda activate python3-cn<\/code>, I see the prompt change, but the environment is unusuable. If I try <code>conda update --all<\/code>, I get a notification that one of the package files has been corrupted.<\/p>\n<p>Not really sure what to do here. I'm looking for space hogs, but not really finding anything significant.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1594831106687,
        "Question_score":1,
        "Question_tags":"conda|amazon-sagemaker",
        "Question_view_count":4104,
        "Owner_creation_time":1311046709507,
        "Owner_last_access_time":1662426840390,
        "Owner_location":null,
        "Owner_reputation":56,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Try increasing the ebs volume amount of your notebook ... this blog explains it well: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-notebook-volume-size-up-to-16-tb-with-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-notebook-volume-size-up-to-16-tb-with-amazon-sagemaker\/<\/a><\/p>\n<p>Also, best practice is to use lifecycle configuration scripts to build\/add new dependencies ... official docs: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html<\/a><\/p>\n<p>This github page has some great template examples ... for example setting up specific configs like conda, etc: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1594867069027,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62919671",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":58992447,
        "Question_title":"AWS SageMaker notebook list tables using boto3 and PySpark",
        "Question_body":"<p>Having some difficulty executing the following code in AWS SageMaker. It is supposed to just list all of the tables in DynamoDB.<\/p>\n\n<pre><code>import boto3\nresource = boto3.resource('dynamodb', region_name='xxxx')\nresponse  = resource.tables.all()\nfor r in response:\n    print(r.name)\n<\/code><\/pre>\n\n<p>If the SageMaker notebook kernel is set to \"conda_python3\" the code executes fine and the tables are listed out in the notebook as expected (this happens pretty much instantly).<\/p>\n\n<p>However, if I set the kernel to \"Sparkmagic (PySpark)\" the same code infinitely runs and doesn't output the table list at all.<\/p>\n\n<p>Does anyone know why this would happen for the PySpark kernel but not for the conda3 kernel? Ideally I need to run this code as part of a bigger script that relies on PySpark, so would like to get it working with PySpark.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1574419108777,
        "Question_score":1,
        "Question_tags":"pyspark|amazon-dynamodb|boto3|amazon-sagemaker",
        "Question_view_count":440,
        "Owner_creation_time":1384795490587,
        "Owner_last_access_time":1663932905217,
        "Owner_location":null,
        "Owner_reputation":1012,
        "Owner_up_votes":1106,
        "Owner_down_votes":1,
        "Owner_views":102,
        "Question_last_edit_time":1574419700060,
        "Answer_body":"<p>Figured out what the issue was, you need to end an endpoint to tour VPC for DyanmoDB.<\/p>\n\n<p>To do this navigate to:<\/p>\n\n<ol>\n<li>AWS VPC<\/li>\n<li>Endpoints<\/li>\n<li>Create Endpoint<\/li>\n<li>Select the dynamodb service (will be type Gateway)<\/li>\n<li>Select the VPC your Notebook is using<\/li>\n<\/ol>",
        "Answer_comment_count":0,
        "Answer_creation_time":1574430450807,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58992447",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":47840209,
        "Question_title":"How to Curl an Amazon Sagemaker Endpoint",
        "Question_body":"<p>What is a is the curl command to make a POST request to sage-maker and receive a ML inference?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":1,
        "Question_creation_time":1513373969900,
        "Question_score":11,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":6889,
        "Owner_creation_time":1443201378360,
        "Owner_last_access_time":1653587950970,
        "Owner_location":null,
        "Owner_reputation":749,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Rather than using curl, it's recommended that you use the SageMaker Runtime client to send data and get back inferences from a SageMaker Endpoint:<\/p>\n\n<p><a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html\" rel=\"nofollow noreferrer\">http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1514326523573,
        "Answer_score":4,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47840209",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68741326,
        "Question_title":"Sagemaker Instance not utilising GPU during training",
        "Question_body":"<p>I'm training a Seq2Seq model on Tensorflow on a ml.p3.2xlarge instance. When I tried running the code on google colab, the time per epoch was around 40 mins. However on the instance it's around 5 hours!<\/p>\n<p>This is my training code<\/p>\n<pre><code>def train_model(train_translator, dataset, path, num=8):\n\n  with tf.device(&quot;\/GPU:0&quot;):\n    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=path,\n                                                save_weights_only=True,\n                                                 verbose=1)\n    batch_loss = BatchLogs('batch_loss')\n    train_translator.fit(dataset, epochs=num,callbacks=[batch_loss,cp_callback])  \n\n  return train_translator\n<\/code><\/pre>\n<p>I have also tried without the <code>tf.device<\/code> command and I still get the same timing. Am I doing something wrong?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1628682115840,
        "Question_score":1,
        "Question_tags":"python-3.x|amazon-web-services|machine-learning|tensorflow2.0|amazon-sagemaker",
        "Question_view_count":995,
        "Owner_creation_time":1580472638260,
        "Owner_last_access_time":1663426586317,
        "Owner_location":null,
        "Owner_reputation":127,
        "Owner_up_votes":282,
        "Owner_down_votes":1,
        "Owner_views":32,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I had to force GPU use with the help of<\/p>\n<pre><code>with tf.device('\/device:GPU:0')\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1628917656377,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68741326",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":73674278,
        "Question_title":"How to use multiple instances with the SageMaker XGBoost built-in algorithm?",
        "Question_body":"<p>If we use multiple instances for training will the built-in algorithm automatically exploit it? For example, what if we used 2 instances for training using built-in XGBoost container and we used the same customer churn example? Will one instance be ignored?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662834395330,
        "Question_score":0,
        "Question_tags":"amazon-web-services|xgboost|amazon-sagemaker|distributed-training",
        "Question_view_count":19,
        "Owner_creation_time":1507661294190,
        "Owner_last_access_time":1663873580420,
        "Owner_location":null,
        "Owner_reputation":98,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":1662834724687,
        "Answer_body":"<p>Yes SageMaker XGBoost supports distributed training. If you set instance count &gt; 1, SageMaker XGBoost will distribute the files from S3 to individual instances and perform distributed training. This, however, requires number of files on S3 &gt;= number of instances. Otherwise, you will be charged for using two training instances without the benefit of using distributed training.<\/p>\n<p>You can find an example here<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone_dist_script_mode.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone_dist_script_mode.ipynb<\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1663199408803,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73674278",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52404879,
        "Question_title":"Efficient management of large amounts of data with SageMaker for training a keras model",
        "Question_body":"<p>I'm working on a deep learning project with about 700GB of table-like time series data in thousands of .csv files (each about 15MB). <br>\nAll the data is on S3 and it needs some preprocessing before being fed into the model. The question is how to best go about automating the process of loading, preprocessing and training. <br><br>Is a custom keras generator with some built in preprocessing the best solution?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1537356638503,
        "Question_score":2,
        "Question_tags":"amazon-s3|keras|bigdata|amazon-sagemaker",
        "Question_view_count":423,
        "Owner_creation_time":1488416220370,
        "Owner_last_access_time":1615723626053,
        "Owner_location":"London, UK",
        "Owner_reputation":142,
        "Owner_up_votes":0,
        "Owner_down_votes":1,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Preprocessing implies that this is something you might want to decouple from the model execution and run separately, possibly on a schedule or in response to new data flowing in.<\/p>\n\n<p>If so, you'll probably want to do the preprocessing outside of SageMaker. You could orchestrate it using <a href=\"https:\/\/aws.amazon.com\/glue\/\" rel=\"nofollow noreferrer\">Glue<\/a>, or you could write a custom job and run it through <a href=\"https:\/\/aws.amazon.com\/batch\/\" rel=\"nofollow noreferrer\">AWS Batch<\/a> or alternatively on an EMR cluster.<\/p>\n\n<p>That way, your Keras notebook can load the already preprocessed data, train and test through SageMaker.<\/p>\n\n<p>With a little care, you should be able to perform at least some of the heavy lifting incrementally in the preprocessing step, saving both time and cost downstream in the Deep Learning pipeline.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1537380210983,
        "Answer_score":1,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52404879",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69666500,
        "Question_title":"Training Job is Stopping in Sagemaker",
        "Question_body":"<p>Recently, I have changed account on AWS and faced with weird error in Sagemaker.<\/p>\n<p>Basically, I'm just checking <code>xgboost<\/code> algo with some toy dataset in this manner:<\/p>\n<pre><code>from sagemaker import image_uris\n\nxgb_image_uri = image_uris.retrieve(&quot;xgboost&quot;, boto3.Session().region_name, &quot;1&quot;)\n\nclf = sagemaker.estimator.Estimator(xgb_image_uri,\n                   role, 1, 'ml.c4.2xlarge',\n                   output_path=&quot;s3:\/\/{}\/output&quot;.format(session.default_bucket()),\n                   sagemaker_session=session)\n\nclf.fit(location_data)\n<\/code><\/pre>\n<p>Then the training job is starting to be executed but for some reason, on downloading data step it stops the training job and displays the following message:<\/p>\n<pre><code>2021-10-21 17:33:27 Downloading - Downloading input data\n2021-10-21 17:33:27 Stopping - Stopping the training job\n2021-10-21 17:33:27 Stopped - Training job stopped\nProfilerReport-1634837444: Stopping\n..\nJob ended with status 'Stopped' rather than 'Completed'. This could mean the job timed out or stopped early for some other reason: Consider checking whether it completed as you expect.\n<\/code><\/pre>\n<p>Also, when I'm trying to go back to training jobs section and check for logs in cloudwatch there is nothing to be displayed. Is it common issue and who had faced with that? Are there any workarounds?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634838048463,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|xgboost|amazon-sagemaker",
        "Question_view_count":361,
        "Owner_creation_time":1386491614717,
        "Owner_last_access_time":1663783708293,
        "Owner_location":null,
        "Owner_reputation":2778,
        "Owner_up_votes":138,
        "Owner_down_votes":1,
        "Owner_views":352,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The problem was most likely with templates for sagemaker that was runned before creating the instance.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1634844298240,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69666500",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63114905,
        "Question_title":"Sagemaker tensorflow endpoint not calling the input_handler when being invoked for a prediction",
        "Question_body":"<p>I'm deploying a <code>tensorflow.serving<\/code> endpoint with a custom <code>inference.py<\/code> script via the <code>entry point<\/code> parameter<\/p>\n<pre><code>model = Model(role='xxx',\n              framework_version='2.2.0',\n              entry_point='inference.py',\n              model_data='xxx')\n\npredictor = model.deploy(instance_type='xxx',\n                         initial_instance_count=1,\n                         endpoint_name='xxx')\n<\/code><\/pre>\n<p>inference.py constains an <code>input_handler<\/code> and an <code>output_handler<\/code> functions, but when i call predict with:<\/p>\n<pre><code>model = Predictor(endpoint_name='xxx')\nurl = 'xxx'\n\ninput = {\n    'instances': [url]\n}\n\npredictions = model.predict(input)\n<\/code><\/pre>\n<p>I'm getting the following <code>error<\/code>:<\/p>\n<p><em>botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;{&quot;error&quot;: &quot;Failed to process element: 0 of 'instances' list. Error: Invalid argument: JSON Value: &quot;xxx&quot; Type: String is not of expected type: float&quot; }&quot;<\/em><\/p>\n<p>It seems the function is never calling the <code>input_handler<\/code> function in inference.py script. Do you know why this might be happening?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1595851293460,
        "Question_score":2,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":474,
        "Owner_creation_time":1402755934690,
        "Owner_last_access_time":1664075083037,
        "Owner_location":"Porto, Portugal",
        "Owner_reputation":5998,
        "Owner_up_votes":2638,
        "Owner_down_votes":56,
        "Owner_views":426,
        "Question_last_edit_time":1595931884943,
        "Answer_body":"<p>Found the problem thanks to AWS support:<\/p>\n<p>I was creating an endpoint that already had an endpoint configuration with the same name and the new configuration wasn't being utilized.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1595952661830,
        "Answer_score":0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63114905",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":54521080,
        "Question_title":"aws sagemaker for detecting text in an image",
        "Question_body":"<p>I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.<\/p>\n\n<p>I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1549300262230,
        "Question_score":4,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":2255,
        "Owner_creation_time":1412300825550,
        "Owner_last_access_time":1663958318850,
        "Owner_location":"West Lafayette, IN, United States",
        "Owner_reputation":463,
        "Owner_up_votes":50,
        "Owner_down_votes":0,
        "Owner_views":111,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The different services will all provide different levels of abstraction for Optical Character Recognition (OCR) depending on what parts of the pipeline you are most comfortable with working with, and what you prefer to have abstracted.<\/p>\n\n<p>Here are a few options:<\/p>\n\n<ul>\n<li><p><strong>Rekognition<\/strong> will provide out of the box OCR with the <a href=\"https:\/\/docs.aws.amazon.com\/rekognition\/latest\/dg\/text-detecting-text-procedure.html\" rel=\"nofollow noreferrer\">DetectText<\/a> feature. However, it seems you will need to perform some sort of pre-processing on your images in your current case in order to get better results. This can be done through any method of your choice (Lambda, EC2, etc).<\/p><\/li>\n<li><p><strong>SageMaker<\/strong> is a tool that will enable you to easily train and deploy your own models (of any type). You have two primary options with SageMaker:<\/p>\n\n<ol>\n<li><p><em>Do-it-yourself option<\/em>: If you're looking to go the route of labeling your own data, gathering a sizable training set, and training your own OCR model, this is possible by training and deploying your own model via SageMaker.<\/p><\/li>\n<li><p><em>Existing OCR algorithm<\/em>: There are many algorithms out there that all have different potential tradeoffs for OCR. One example would be <a href=\"https:\/\/github.com\/tesseract-ocr\/tesseract\" rel=\"nofollow noreferrer\">Tesseract<\/a>. Using this, you can more closely couple your pre-processing step to the text detection.<\/p><\/li>\n<\/ol><\/li>\n<li><p><a href=\"https:\/\/aws.amazon.com\/textract\/\" rel=\"nofollow noreferrer\"><strong>Amazon Textract<\/strong><\/a> (In preview) is a purpose-built dedicated OCR service that may offer better performance depending on what your images look like and the settings you choose. <\/p><\/li>\n<\/ul>\n\n<p>I would personally recommend looking into <a href=\"https:\/\/docparser.com\/blog\/improve-ocr-accuracy\/\" rel=\"nofollow noreferrer\">pre-processing for OCR<\/a> to see if it improves Rekognition accuracy before moving onto the other options. Even if it doesn't improve Rekognition's accuracy, it will still be valuable for most of the other options!<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1549313202943,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54521080",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":72376872,
        "Question_title":"entry_point file using XGBoost as a framework in sagemaker",
        "Question_body":"<p>Looking at the following source code taken from <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">here<\/a> (SDK v2):<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker.xgboost.estimator import XGBoost\nfrom sagemaker.session import Session\nfrom sagemaker.inputs import TrainingInput\n\n# initialize hyperparameters\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;verbosity&quot;:&quot;1&quot;,\n        &quot;objective&quot;:&quot;reg:linear&quot;,\n        &quot;num_round&quot;:&quot;50&quot;}\n\n# set an output path where the trained model will be saved\nbucket = sagemaker.Session().default_bucket()\nprefix = 'DEMO-xgboost-as-a-framework'\noutput_path = 's3:\/\/{}\/{}\/{}\/output'.format(bucket, prefix, 'abalone-xgb-framework')\n\n# construct a SageMaker XGBoost estimator\n# specify the entry_point to your xgboost training script\nestimator = XGBoost(entry_point = &quot;your_xgboost_abalone_script.py&quot;, \n                    framework_version='1.2-2',\n                    hyperparameters=hyperparameters,\n                    role=sagemaker.get_execution_role(),\n                    instance_count=1,\n                    instance_type='ml.m5.2xlarge',\n                    output_path=output_path)\n\n# define the data type and paths to the training and validation datasets\ncontent_type = &quot;libsvm&quot;\ntrain_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'train'), content_type=content_type)\nvalidation_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'validation'), content_type=content_type)\n\n# execute the XGBoost training job\nestimator.fit({'train': train_input, 'validation': validation_input})\n<\/code><\/pre>\n<p>I wonder where the your_xgboost_abalone_script.py file has to be placed please? So far I used XGBoost as a built-in algorithm from my local machine with similar code (i.e. I span up a training job remotely). Thanks!<\/p>\n<p>PS:<\/p>\n<p>Looking at <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html\" rel=\"nofollow noreferrer\">this<\/a>, and source_dir, I wonder if one can upload Python files to S3. In this case, I take it is has to be tar.gz? Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653478466700,
        "Question_score":0,
        "Question_tags":"python|xgboost|amazon-sagemaker",
        "Question_view_count":74,
        "Owner_creation_time":1267440784443,
        "Owner_last_access_time":1664045779313,
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Question_last_edit_time":1653482394636,
        "Answer_body":"<p><code>your_xgboost_abalone_script.py<\/code> can be created locally. The path you provide is relative to where the code is running.<\/p>\n<p>I.e. <code>your_xgboost_abalone_script.py<\/code> can be located in the same directory where you are running the SageMaker SDK (&quot;source code&quot;).<\/p>\n<p>For example if you have <code>your_xgboost_abalone_script.py<\/code> in the same directory as the source code:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 source_code.py\n\u2514\u2500\u2500 your_xgboost_abalone_script.py\n<\/code><\/pre>\n<p>Then you can point to this file exactly how the documentation depicts:<\/p>\n<pre><code>estimator = XGBoost(entry_point = &quot;your_xgboost_abalone_script.py&quot;, \n.\n.\n.\n)\n<\/code><\/pre>\n<p>The SDK will take <code>your_xgboost_abalone_script.py<\/code> repackage it into a model tar ball and upload it to S3 on your behalf.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1654714910813,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1654796658207,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72376872",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":68009703,
        "Question_title":"Can \"Invoke_endpoint\" calls timeout a lambda function?",
        "Question_body":"<p>I am attempting to pass json data into my sagemaker model through a lambda function. Currently, I am using a testing model that makes relatively quick inferences and returns them to the lambda function through the invoke_endpoint call. However, eventually a more advanced model will be implemented which might take longer than a lambda function can fun for (15 minutes maximum) to produce inferences. In the case that I call invoke_endpoint in one lambda function, can I return the response to another lambda function which is invoked by the sagemaker endpoint response? Even better, can I shut down the current lambda function after sending the data to sagemaker, and re-invoke it upon a response? I need to store the inference in DynamoDB, which is why I need a response (Unless I can update the saved model to store inferences directly, in which case I need the lambda function to not expect a response from invoke_endpoint). Sorry for my ignorance, I am a bit new to sagemaker.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1623875736457,
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":181,
        "Owner_creation_time":1622063222450,
        "Owner_last_access_time":1658543544890,
        "Owner_location":null,
        "Owner_reputation":5,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When calling <code>invoke_endpoint<\/code>, the underlying model invocation must take <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-code-how-containe-serves-requests\" rel=\"nofollow noreferrer\">less than 1 minute<\/a>. If a single model execution needs more time to execute, consider running the model in Lambda itself, in SageMaker Training API (if its coldstart is acceptable) or in a custom service. If the invocation is made of several shorter calls you can also chain multiple services together with Step Functions.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1623878015320,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68009703",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":50732094,
        "Question_title":"Sagemaker PySpark: Kernel Dead",
        "Question_body":"<p>I followed the instructions <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/\" rel=\"nofollow noreferrer\">here<\/a> to set up an EMR cluster and a SageMaker notebook. I did not have any errors until the last step.<\/p>\n\n<p>When I open a new Notebook in Sagemaker, I get the message:<\/p>\n\n<pre><code>The kernel appears to have died. It will restart automatically.\n<\/code><\/pre>\n\n<p>And then:<\/p>\n\n<pre><code>        The kernel has died, and the automatic restart has failed.\n        It is possible the kernel cannot be restarted. \n        If you are not able to restart the kernel, you will still be able to save the \nnotebook, but running code will no longer work until the notebook is reopened.\n<\/code><\/pre>\n\n<p>This only happens when I use the pyspark\/Sparkmagic kernel. Notebooks opened with the Conda kernel or any other kernel work fine. <\/p>\n\n<p>My EMR cluster is set up exactly as in the instructions, with an added rule:<\/p>\n\n<pre><code>[\n  {\n    \"Classification\": \"spark\",\n    \"Properties\": {\n      \"maximizeResourceAllocation\": \"true\"\n    }\n  }\n]\n<\/code><\/pre>\n\n<p>I'd appreciate any pointers on why this is happening and how I can debug\/fix.<\/p>\n\n<p>P.S.: I've done this successfully in the past without any issues. When I tried re-doing this today, I ran into this issue. I tried re-creating the EMR clusters and Sagemaker notebooks, but that didn't help. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1528337832390,
        "Question_score":2,
        "Question_tags":"pyspark|jupyter|amazon-sagemaker",
        "Question_view_count":1768,
        "Owner_creation_time":1430255275880,
        "Owner_last_access_time":1557543667777,
        "Owner_location":"Pittsburgh, PA",
        "Owner_reputation":125,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Thank you for using Amazon SageMaker.<\/p>\n\n<p>The issue here is Pandas 0.23.0 changed the location of a core class named DataError and SparkMagic has not been updated to require DataError from correct namespace.<\/p>\n\n<p>The workaround for this issue is to downgrade Pandas version in SageMaker Notebook Instance with <code>pip install pandas==0.22.0<\/code>.<\/p>\n\n<p>You can get more information in this open github issue <a href=\"https:\/\/github.com\/jupyter-incubator\/sparkmagic\/issues\/458\" rel=\"noreferrer\">https:\/\/github.com\/jupyter-incubator\/sparkmagic\/issues\/458<\/a>.<\/p>\n\n<p>Let us know if there is any other way we can be of assistance.<\/p>\n\n<p>Thanks,<br>\nNeelam<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1531254842950,
        "Answer_score":5,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50732094",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71385524,
        "Question_title":"Sagemaker training job Fatal error: cannot open file 'train': No such file or directory",
        "Question_body":"<p>I am trying work on bring your own model. I have R code. when i try to run the job its failing.<\/p>\n<p><strong>Training Image:<\/strong><\/p>\n<pre><code>FROM r-base:3.6.3\n\nMAINTAINER Amazon SageMaker Examples &lt;amazon-sagemaker-examples@amazon.com&gt;\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n    wget \\\n    r-base \\\n    r-base-dev \\\n    apt-transport-https \\\n    ca-certificates \\\n    python3 python3-dev pip\n\nENV AWS_DEFAULT_REGION=&quot;us-east-2&quot;\nRUN R -e &quot;install.packages('reticulate', dependencies = TRUE, warning = function(w) stop(w))&quot;\nRUN R -e &quot;install.packages('readr', dependencies = TRUE, warning = function(w) stop(w))&quot;\nRUN R -e &quot;install.packages('dplyr', dependencies = TRUE, warning = function(w) stop(w))&quot;\n\nRUN pip install --quiet --no-cache-dir \\\n    'boto3&gt;1.0&lt;2.0' \\\n    'sagemaker&gt;2.0&lt;3.0'    \n\nENTRYPOINT [&quot;\/usr\/bin\/Rscript&quot;]\n<\/code><\/pre>\n<p><strong>Source code:<\/strong><\/p>\n<pre><code>rcode\n    \u2514\u2500\u2500 train.R\n    \u2514\u2500\u2500 train.tar.gz\n<\/code><\/pre>\n<p>Build<\/p>\n<pre><code>- aws s3 cp $CODEBUILD_SRC_DIR\/rcode\/ s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/code\/training --recursive\n<\/code><\/pre>\n<p><strong>Serverless.com yaml<\/strong><\/p>\n<pre><code>           SagemakerRCodeTrainingStep:\n            Type: Task\n            Resource: ${self:custom.sageMakerTrainingJob}\n            Parameters:\n              TrainingJobName.$: &quot;$.sageMakerTrainingJobName&quot;\n              DebugHookConfig:\n                S3OutputPath: &quot;s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/models\/rmodel&quot;\n              AlgorithmSpecification:\n                TrainingImage: ${self:custom.sagemakerRExecutionContainerURI}\n                TrainingInputMode: &quot;File&quot;\n              OutputDataConfig:\n                S3OutputPath: &quot;s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/models\/rmodel&quot;\n              StoppingCondition:\n                MaxRuntimeInSeconds: ${self:custom.maxRuntime}\n              ResourceConfig:\n                InstanceCount: 1\n                InstanceType: &quot;ml.m5.xlarge&quot;\n                VolumeSizeInGB: 30\n              RoleArn: ${self:custom.stateMachineRoleARN}\n              InputDataConfig:\n                - DataSource:\n                    S3DataSource:\n                      S3DataType: &quot;S3Prefix&quot;\n                      S3Uri: &quot;s3:\/\/${self:custom.datasetsFilePath}\/data\/processed\/train&quot;\n                      S3DataDistributionType: &quot;FullyReplicated&quot;\n                  ChannelName: &quot;train&quot;\n              HyperParameters:\n                sagemaker_submit_directory: &quot;s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/code\/training\/train.tar.gz&quot;\n                sagemaker_program: &quot;train.R&quot;\n                sagemaker_enable_cloudwatch_metrics: &quot;false&quot;\n                sagemaker_container_log_level: &quot;20&quot;\n                sagemaker_job_name: &quot;sagemaker-r-learn-2022-02-28-09-56-33-234&quot;\n                sagemaker_region: ${self:provider.region}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646676784137,
        "Question_score":1,
        "Question_tags":"r|amazon-sagemaker|serverless.com",
        "Question_view_count":369,
        "Owner_creation_time":1285763771347,
        "Owner_last_access_time":1658319981990,
        "Owner_location":"Bangalore, India",
        "Owner_reputation":6958,
        "Owner_up_votes":123,
        "Owner_down_votes":1,
        "Owner_views":787,
        "Question_last_edit_time":1646720798767,
        "Answer_body":"<p>I am not sure which <code>TrainingImage<\/code> you are using and all the files in your container.\nThat being said, I suspect you are using a custom container.<\/p>\n<p>SageMaker Training Jobs look for a <code>train<\/code> file and run your container as <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">follows<\/a>:<\/p>\n<pre><code>docker run image train\n<\/code><\/pre>\n<p>You can change this behavior by setting the <code>ENTRYPOINT<\/code> in your Dockerfile. Please see this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/r_examples\/r_byo_r_algo_hpo\/Dockerfile#L47\" rel=\"nofollow noreferrer\">Dockerfile<\/a> from the <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/main\/r_examples\/r_byo_r_algo_hpo\" rel=\"nofollow noreferrer\">r_byo_r_algo_hpo<\/a> example.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1646690790177,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71385524",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":57808963,
        "Question_title":"Sagemaker instance does automatic cd at startup",
        "Question_body":"<p>I have a Sagemaker instance that's linked to a github repo <code>my-repo<\/code>, and every time I open a new terminal, I see this immediately at startup: <\/p>\n\n<pre><code>sh-4.2$ cd \"my-repo\"\nsh: cd: my-repo: No such file or directory\n<\/code><\/pre>\n\n<p>I assumed something was in the .bashrc or .bash_profile that prompted this (failed) <code>cd<\/code> but it's not in there. Any ideas where I should look for what's causing this behavior? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1567698318190,
        "Question_score":0,
        "Question_tags":"shell|command-line|amazon-sagemaker",
        "Question_view_count":304,
        "Owner_creation_time":1386023479737,
        "Owner_last_access_time":1614533016280,
        "Owner_location":null,
        "Owner_reputation":725,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The issue is not specific to SageMaker Notebook instances. Rather, it is a bug in the Git extension of JupyterLab. You can find details around this here: <a href=\"https:\/\/github.com\/jupyterlab\/jupyterlab-git\/issues\/346\" rel=\"nofollow noreferrer\">https:\/\/github.com\/jupyterlab\/jupyterlab-git\/issues\/346<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1567699632857,
        "Answer_score":1,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57808963",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52940677,
        "Question_title":"AWS Sagemaker: AttributeError: module 'pandas' has no attribute 'core'",
        "Question_body":"<p>Let me prefix this by saying I'm very new to tensorflow and even newer to AWS Sagemaker.<\/p>\n\n<p>I have some tensorflow\/keras code that I wrote and tested on a local dockerized Jupyter notebook and it runs fine. In it, I import a csv file as my input.<\/p>\n\n<p>I use Sagemaker to spin up a jupyter notebook instance with conda_tensorflow_p36. I modified the pandas.read_csv() code to point to my input file, now hosted on a S3 bucket.<\/p>\n\n<p>So I changed this line of code from<\/p>\n\n<pre><code>import pandas as pd\n\ndata = pd.read_csv(\"\/input.csv\", encoding=\"latin1\")\n<\/code><\/pre>\n\n<p>to this<\/p>\n\n<pre><code>import pandas as pd\n\ndata = pd.read_csv(\"https:\/\/s3.amazonaws.com\/my-sagemaker-bucket\/input.csv\", encoding=\"latin1\")\n<\/code><\/pre>\n\n<p>and I get this error<\/p>\n\n<pre><code>AttributeError: module 'pandas' has no attribute 'core'\n<\/code><\/pre>\n\n<p>I'm not sure if it's a permissions issue. I read that as long as I name my bucket with the string \"sagemaker\" it should have access to it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1540264856293,
        "Question_score":0,
        "Question_tags":"pandas|tensorflow|amazon-sagemaker",
        "Question_view_count":1028,
        "Owner_creation_time":1319234288810,
        "Owner_last_access_time":1663375872053,
        "Owner_location":null,
        "Owner_reputation":4966,
        "Owner_up_votes":744,
        "Owner_down_votes":11,
        "Owner_views":304,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Pull our data from S3 for example:<\/p>\n\n<pre><code>import boto3\nimport io\nimport pandas as pd\n\n\n# Set below parameters\nbucket = '&lt;bucket name&gt;'\nkey = 'data\/training\/iris.csv'\nendpointName = 'decision-trees'\n\n# Pull our data from S3\ns3 = boto3.client('s3')\nf = s3.get_object(Bucket=bucket, Key=key)\n\n# Make a dataframe\nshape = pd.read_csv(io.BytesIO(f['Body'].read()), header=None)\n<\/code><\/pre>",
        "Answer_comment_count":1,
        "Answer_creation_time":1540283269857,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52940677",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":69782294,
        "Question_title":"AWS Sagemaker output how to read file with multiple json objects spread out over multiple lines",
        "Question_body":"<p>I have a bunch of json files that look like this<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.01027186680585146, 0.04181386157870293, -0.07363887131214142, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.011699287220835686, 0.04741542786359787, -0.07899319380521774, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n<\/code><\/pre>\n<p>Which I can read in with<\/p>\n<pre><code>f = open(file_name)\ndata = []\nfor line in f:\n   data.append(json.dumps(line))\n<\/code><\/pre>\n<p>But I have another file with output like this<\/p>\n<pre><code>{\n    &quot;predictions&quot;: [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]\n    ]\n}\n<\/code><\/pre>\n<p>I.e. the json is formatted over several lines, so I can't simply read the json in line for line. Is there an easy way to parse this? Or do I have to write something that stitches together each json object line by line and the does json.loads?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635629350997,
        "Question_score":1,
        "Question_tags":"python|json|amazon-sagemaker",
        "Question_view_count":248,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Question_last_edit_time":1635655049276,
        "Answer_body":"<p>Hmm,  as far as I know there's unfortunately no way to load a <a href=\"https:\/\/jsonlines.org\/\" rel=\"nofollow noreferrer\">JSONL<\/a> format data using <code>json.loads<\/code>. One option though, is to come up with a helper function that can convert it to a valid JSON string, as below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import json\n\nstring = &quot;&quot;&quot;\n{\n    &quot;predictions&quot;: [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]\n    ]\n}\n&quot;&quot;&quot;\n\n\ndef json_lines_to_json(s: str) -&gt; str:\n    # replace the first occurrence of '{'\n    s = s.replace('{', '[{', 1)\n\n    # replace the last occurrence of '}\n    s = s.rsplit('}', 1)[0] + '}]'\n\n    # now go in and replace all occurrences of '}' immediately followed\n    # by newline with a '},'\n    s = s.replace('}\\n', '},\\n')\n\n    return s\n\n\nprint(json.loads(json_lines_to_json(string)))\n<\/code><\/pre>\n<p>Prints:<\/p>\n<pre><code>[{'predictions': [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]]}, {'predictions': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]}, {'predictions': [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]]}]\n<\/code><\/pre>\n<p><strong>Note:<\/strong> your first example actually doesn't seem like valid JSON (or at least JSON lines from my understanding). In particular, this part appears to be invalid due to a trailing comma after the last array element:<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664, ], ...}\n<\/code><\/pre>\n<p>To ensure it's valid after calling the helper function, you'd also need to remove the trailing commas, so each line is in the below format:<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], ...},\n<\/code><\/pre>\n<hr \/>\n<p>There also appears to be a <a href=\"https:\/\/stackoverflow.com\/questions\/50475635\/loading-jsonl-file-as-json-objects\/50475669\">similar question<\/a> where they suggest splitting on newlines and calling <code>json.loads<\/code> on each line; actually it should be (slightly) less performant to call <code>json.loads<\/code> multiple times on each object, rather than once on the list, as I show below.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from timeit import timeit\nimport json\n\n\nstring = &quot;&quot;&quot;\\\n{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.01027186680585146, 0.04181386157870293, -0.07363887131214142 ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.011699287220835686, 0.04741542786359787, -0.07899319380521774 ], &quot;word&quot;: &quot;blah blah blah&quot;}\\\n&quot;&quot;&quot;\n\n\ndef json_lines_to_json(s: str) -&gt; str:\n\n    # Strip newlines from end, then replace all occurrences of '}' followed\n    # by a newline, by a '},' followed by a newline.\n    s = s.rstrip('\\n').replace('}\\n', '},\\n')\n\n    # return string value wrapped in brackets (list)\n    return f'[{s}]'\n\n\nn = 10_000\n\nprint('string replace:        ', timeit(r'json.loads(json_lines_to_json(string))', number=n, globals=globals()))\nprint('json.loads each line:  ', timeit(r'[json.loads(line) for line in string.split(&quot;\\n&quot;)]', number=n, globals=globals()))\n<\/code><\/pre>\n<p>Result:<\/p>\n<pre><code>string replace:         0.07599360000000001\njson.loads each line:   0.1078384\n<\/code><\/pre>",
        "Answer_comment_count":2,
        "Answer_creation_time":1635633656723,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1635655742576,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69782294",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":56835306,
        "Question_title":"Download file using boto3 within Docker container deployed on Sagemaker Endpoint",
        "Question_body":"<p>I have built my own Docker container that provides inference code to be deployed as endpoint on Amazon Sagemaker. However, this container needs to have access to some files from s3. The used IAM role has access to all s3 buckets that I am trying to reach.<\/p>\n\n<p>Code to download files using a boto3 client:<\/p>\n\n<pre><code>import boto3\n\nmodel_bucket = 'my-bucket'\n\ndef download_file_from_s3(s3_path, local_path):\n    client = boto3.client('s3')\n    client.download_file(model_bucket, s3_path, local_path)\n<\/code><\/pre>\n\n<p>The IAM role's policies:<\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::my-bucket\/*\"\n            ]\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>Starting the docker container locally allows me to download files from s3 just like expected. <\/p>\n\n<p>Deploying as an endpoint on Sagemaker, however, the request times out:<\/p>\n\n<pre><code>botocore.vendored.requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='my-bucket.s3.eu-central-1.amazonaws.com', port=443): Max retries exceeded with url: \/path\/to\/my-file (Caused by ConnectTimeoutError(&lt;botocore.awsrequest.AWSHTTPSConnection object at 0x7f66244e69b0&gt;, 'Connection to my-bucket.s3.eu-central-1.amazonaws.com timed out. (connect timeout=60)'))\n<\/code><\/pre>\n\n<p>Any help is appreciated!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1561982365937,
        "Question_score":4,
        "Question_tags":"python-3.x|amazon-web-services|boto3|amazon-sagemaker",
        "Question_view_count":1171,
        "Owner_creation_time":1456411465890,
        "Owner_last_access_time":1663933136733,
        "Owner_location":null,
        "Owner_reputation":105,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For anyone coming across this question, when creating a model, the 'Enable Network Isolation' property defaults to True.\nFrom AWS docs:<\/p>\n<blockquote>\n<p>If you enable network isolation, the containers are not able to make any outbound network calls, even to other AWS services such as Amazon S3. Additionally, no AWS credentials are made available to the container runtime environment.<\/p>\n<\/blockquote>\n<p>So this property needs to be set to False in order to connect to any other AWS service.<\/p>\n<p><img src=\"https:\/\/i.stack.imgur.com\/5qERm.jpg\" alt=\"AWS Sagemaker UI Network Isolation set to False\" \/><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1562142120397,
        "Answer_score":0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1641299334400,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56835306",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71126832,
        "Question_title":"Amazon Sagemaker: User Input data validation in Inference Endpoint",
        "Question_body":"<p>I have successfully built a Sagemaker endpoint using a Tensorflow model. The pre and post processing is done inside &quot;inference.py&quot; which calls a handler function based on this tutorial: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s<\/a><\/p>\n<p>My questions are:<\/p>\n<ul>\n<li>Which method is good for validating user input data within inference.py?<\/li>\n<li>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/li>\n<li>How is this compatible with the API gateway placed above the endpoint?<\/li>\n<\/ul>\n<p>Here is the structure of the inference.py with the desired validation check as a comment:<\/p>\n<pre><code>import json\nimport requests\n\n\ndef handler(data, context):\n    &quot;&quot;&quot;Handle request.\n    Args:\n        data (obj): the request data\n        context (Context): an object containing request and configuration details\n    Returns:\n        (bytes, string): data to return to client, (optional) response content type\n    &quot;&quot;&quot;\n    processed_input = _process_input(data, context)\n    response = requests.post(context.rest_uri, data=processed_input)\n    return _process_output(response, context)\n\n\ndef _process_input(data, context):\n    if context.request_content_type == 'application\/json':\n        # pass through json (assumes it's correctly formed)\n        d = data.read().decode('utf-8')\n        data_dict = json.loads(data)\n\n\n        # -----&gt;   if data_dict['input_1'] &gt; 25000:\n        # -----&gt;       return some error specific message with status code 123\n\n\n        return some_preprocessing_function(data_dict)\n\n    raise ValueError('{{&quot;error&quot;: &quot;unsupported content type {}&quot;}}'.format(\n        context.request_content_type or &quot;unknown&quot;))\n\n\ndef _process_output(data, context):\n    if data.status_code != 200:\n        raise ValueError(data.content.decode('utf-8'))\n\n    response_content_type = context.accept_header\n    prediction = data.content\n    return prediction, response_content_type\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644929807890,
        "Question_score":0,
        "Question_tags":"python|validation|amazon-sagemaker|endpoint|inference",
        "Question_view_count":245,
        "Owner_creation_time":1613661928947,
        "Owner_last_access_time":1663929308913,
        "Owner_location":null,
        "Owner_reputation":160,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":1644931522732,
        "Answer_body":"<p>I will answer your questions inline below:<\/p>\n<ol>\n<li><em>Which method is good for validating user input data within inference.py?<\/em><\/li>\n<\/ol>\n<p>Seeing that you have a <code>handler<\/code> function, <code>input_handler<\/code> and <code>output_handler<\/code> are ignored. Thus, inside your <code>handler<\/code> function (as you are correctly doing) you can have the validation logic.<\/p>\n<ol start=\"2\">\n<li><em>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/em><\/li>\n<\/ol>\n<p>I like to think of my SageMaker endpoint as a web server. Thus, you can return any valid HTTP response code with a response message. Please see this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_batch_transform\/tensorflow_cifar-10_with_inference_script\/code\/inference.py#L47\" rel=\"nofollow noreferrer\">inference.py<\/a> file that I found as a reference.<\/p>\n<pre><code>_return_error(\n            415, 'Unsupported content type &quot;{}&quot;'.format(context.request_content_type or &quot;Unknown&quot;)\n        )\n\ndef _return_error(code, message):\n    raise ValueError(&quot;Error: {}, {}&quot;.format(str(code), message))\n<\/code><\/pre>\n<ol start=\"3\">\n<li><em>How is this compatible with the API gateway placed above the endpoint?<\/em><\/li>\n<\/ol>\n<p>Please see this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">link<\/a> for details on Creating a machine learning-powered REST API with Amazon API Gateway mapping templates and Amazon SageMaker.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1645567161520,
        "Answer_score":1,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71126832",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":71340893,
        "Question_title":"When I get a prediction from sagemaker endpoint, what does the endpoint do?",
        "Question_body":"<p>In sagemaker, the docs talk about inference scripts requiring to have 4 specific functions. When we get a prediction, the python SDK sends a request to the endpoint.<\/p>\n<p>Then the inference script runs. But I cannot find where in the SDK the inference script is run.<\/p>\n<p>When I navigate through the sdk code the <code>Predictor.predict()<\/code> method calls the sagemaker session to post a request to the endpoint and get a response. That is the final step in the sdk. Sagemaker is obviously doing something when it receives that request.<\/p>\n<p>What is the code that it runs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646326739290,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":405,
        "Owner_creation_time":1578932319743,
        "Owner_last_access_time":1663935286190,
        "Owner_location":"Ireland",
        "Owner_reputation":1012,
        "Owner_up_votes":91,
        "Owner_down_votes":96,
        "Owner_views":66,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The endpoint is essentially a Flask web server running in a Docker container<\/p>\n<p>If it's a scikit-learn image, when you invoke the endpoint, it loads your script from S3, then...<\/p>\n<p>It calls <code>input_fn(request_body: bytearray, content_type) -&gt; np.ndarray<\/code> to parse the <code>request_body<\/code> into a numpy array<\/p>\n<p>Then it calls your <code>model_fn(model_dir: str) -&gt; object<\/code> function to load the model from <code>model_dir<\/code> and return the model<\/p>\n<p>Then it calls <code>predict_fn(input_object: np.ndarray, model: object) -&gt; np.array<\/code>, which calls your <code>model.predict()<\/code> function and returns the prediction<\/p>\n<p>Then it calls <code>output_fn(prediction: np.array, accept: str)<\/code> to take the result from <code>predict_fn<\/code> and encode it to the <code>accept<\/code> type<\/p>\n<p>You don't need to implement all of these functions yourself, as there are defaults<\/p>\n<p>You <strong>do<\/strong> need to implement <code>model_fn<\/code><\/p>\n<p>You only need to implement <code>input_fn<\/code> if you have non numeric data<\/p>\n<p>You only need to implement <code>predict_fn<\/code> if your model uses something other than <code>.predict()<\/code><\/p>\n<p>You can see how the default function implementations work <a href=\"https:\/\/github.com\/aws\/sagemaker-scikit-learn-container\/blob\/master\/src\/sagemaker_sklearn_container\/serving.py\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1646329084320,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1646761223947,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71340893",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":63305569,
        "Question_title":"How to mount an EFS volume on AWS Sagemaker Studio",
        "Question_body":"<p>I have tried to follow the normal (non-studio) documentation on mounting an EFS file system, as can be found <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/mount-an-efs-file-system-to-an-amazon-sagemaker-notebook-with-lifecycle-configurations\/\" rel=\"nofollow noreferrer\">here<\/a>, however, these steps don't work in a studio notebook. Specifically, the <code>sudo mount -t nfs ...<\/code> does not work in both the Image terminal and the system terminal.<\/p>\n<p>How do I mount an EFS file system that already exists to amazon Sagemaker, so I can access the data\/ datasets I stored in them?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596816839977,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":2331,
        "Owner_creation_time":1483370766803,
        "Owner_last_access_time":1664056213153,
        "Owner_location":"London, UK",
        "Owner_reputation":15819,
        "Owner_up_votes":827,
        "Owner_down_votes":21,
        "Owner_views":1395,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Update: I spoke to an AWS Solutions Architect, and he confirms that EFS is not supported on Sagemaker Studio.<\/p>\n<hr \/>\n<p><strong>Workaround:<\/strong><\/p>\n<p>Instead of mounting your old EFS, you can mount the SageMaker studio EFS onto an EC2 instance, and copy over the data manually. You would need the correct EFS storage volume id, and you'll find your newly copied data available in Sagemaker Studio. <em>I have not actually done this though.<\/em><\/p>\n<p>To find the EFS id, look at the section &quot;Manage your storage volume&quot; <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tasks.html#manage-your-storage-volume\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1596816839977,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1597991730412,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63305569",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":66561959,
        "Question_title":"Sagemaker Endpoint BrokenPipeError at DeepAR Prediction",
        "Question_body":"<p>I've created an SageMaker Endpoint from a trained DeepAR-Model using following code:<\/p>\n<pre><code>job_name = estimator.latest_training_job.job_name\n\nendpoint_name = sagemaker_session.endpoint_from_job(\n    job_name=job_name,\n    initial_instance_count=1,\n    instance_type=&quot;ml.m4.xlarge&quot;,\n    image_uri=image_uri,\n    role=role\n)\n<\/code><\/pre>\n<p>Now I want to test my model using a <code>test.json<\/code>-Dataset (<strong>66.2MB<\/strong>).\nI've created that file according to various tutorials\/sample-notebooks (same as <code>train.json<\/code>, but with <code>prediction-length<\/code>-less values.<\/p>\n<p>For that, I've written the following code:<\/p>\n<pre><code>class DeepARPredictor(sagemaker.predictor.Predictor):\n    def set_prediction_parameters(self, freq, prediction_length):\n        self.freq = freq\n        self.prediction_length = prediction_length\n\n    def predict(self, ts, num_samples=100, quantiles=[&quot;0.1&quot;, &quot;0.5&quot;, &quot;0.9&quot;]):\n        prediction_times = [x.index[-1] + pd.Timedelta(1, unit=self.freq) for x in ts]\n        req = self.__encode_request(ts, num_samples, quantiles)\n        res = super(DeepARPredictor, self).predict(req, initial_args={&quot;ContentType&quot;: &quot;application\/json&quot;})\n        return self.__decode_response(res, prediction_times)\n\n    def __encode_request(self, ts, num_samples, quantiles):\n        instances = [{&quot;start&quot;: str(ts[k].index[0]), &quot;target&quot;: list(ts[k])} for k in range(len(ts))]\n        configuration = {\n            &quot;num_samples&quot;: num_samples,\n            &quot;output_types&quot;: [&quot;quantiles&quot;],\n            &quot;quantiles&quot;: quantiles,\n        }\n        http_request_data = {&quot;instances&quot;: instances, &quot;configuration&quot;: configuration}\n        return json.dumps(http_request_data).encode( &quot;utf-8&quot;)\n\n    def __decode_response(self, response, prediction_times):\n        response_data = json.loads(response.decode(&quot;utf-8&quot;))\n        list_of_df = []\n        for k in range(len(prediction_times)):\n            prediction_index = pd.date_range(\n                start=prediction_times[k], freq=self.freq, periods=self.prediction_length\n            )\n            list_of_df.append(\n                pd.DataFrame(data=response_data[&quot;predictions&quot;][k][&quot;quantiles&quot;], index=prediction_index)\n            )\n        return list_of_df\n<\/code><\/pre>\n<p>But after running the following block:<\/p>\n<pre><code>predictor = DeepARPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\npredictor.set_prediction_parameters(freq, prediction_length)\nlist_of_df = predictor.predict(time_series_training)\n<\/code><\/pre>\n<p>I've getting a BrokenPipeError:<\/p>\n<pre><code>---------------------------------------------------------------------------\nBrokenPipeError                           Traceback (most recent call last)\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    676                 headers=headers,\n--&gt; 677                 chunked=chunked,\n    678             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\n    391         else:\n--&gt; 392             conn.request(method, url, **httplib_request_kw)\n    393 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in request(self, method, url, body, headers, encode_chunked)\n   1261         &quot;&quot;&quot;Send a complete request to the server.&quot;&quot;&quot;\n-&gt; 1262         self._send_request(method, url, body, headers, encode_chunked)\n   1263 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_request(self, method, url, body, headers, *args, **kwargs)\n     92         rval = super(AWSConnection, self)._send_request(\n---&gt; 93             method, url, body, headers, *args, **kwargs)\n     94         self._expect_header_set = False\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in _send_request(self, method, url, body, headers, encode_chunked)\n   1307             body = _encode(body, 'body')\n-&gt; 1308         self.endheaders(body, encode_chunked=encode_chunked)\n   1309 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in endheaders(self, message_body, encode_chunked)\n   1256             raise CannotSendHeader()\n-&gt; 1257         self._send_output(message_body, encode_chunked=encode_chunked)\n   1258 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_output(self, message_body, *args, **kwargs)\n    119             message_body = None\n--&gt; 120         self.send(msg)\n    121         if self._expect_header_set:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in send(self, str)\n    203             return\n--&gt; 204         return super(AWSConnection, self).send(str)\n    205 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in send(self, data)\n    995         try:\n--&gt; 996             self.sock.sendall(data)\n    997         except TypeError:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in sendall(self, data, flags)\n    974                 while count &lt; amount:\n--&gt; 975                     v = self.send(byte_view[count:])\n    976                     count += v\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in send(self, data, flags)\n    943                     self.__class__)\n--&gt; 944             return self._sslobj.write(data)\n    945         else:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in write(self, data)\n    641         &quot;&quot;&quot;\n--&gt; 642         return self._sslobj.write(data)\n    643 \n\nBrokenPipeError: [Errno 32] Broken pipe\n\nDuring handling of the above exception, another exception occurred:\n\nProtocolError                             Traceback (most recent call last)\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/httpsession.py in send(self, request)\n    319                 decode_content=False,\n--&gt; 320                 chunked=self._chunked(request.headers),\n    321             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    726             retries = retries.increment(\n--&gt; 727                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n    728             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/util\/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)\n    378             # Disabled, indicate to re-raise the error.\n--&gt; 379             raise six.reraise(type(error), error, _stacktrace)\n    380 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/packages\/six.py in reraise(tp, value, tb)\n    733             if value.__traceback__ is not tb:\n--&gt; 734                 raise value.with_traceback(tb)\n    735             raise value\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    676                 headers=headers,\n--&gt; 677                 chunked=chunked,\n    678             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\n    391         else:\n--&gt; 392             conn.request(method, url, **httplib_request_kw)\n    393 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in request(self, method, url, body, headers, encode_chunked)\n   1261         &quot;&quot;&quot;Send a complete request to the server.&quot;&quot;&quot;\n-&gt; 1262         self._send_request(method, url, body, headers, encode_chunked)\n   1263 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_request(self, method, url, body, headers, *args, **kwargs)\n     92         rval = super(AWSConnection, self)._send_request(\n---&gt; 93             method, url, body, headers, *args, **kwargs)\n     94         self._expect_header_set = False\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in _send_request(self, method, url, body, headers, encode_chunked)\n   1307             body = _encode(body, 'body')\n-&gt; 1308         self.endheaders(body, encode_chunked=encode_chunked)\n   1309 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in endheaders(self, message_body, encode_chunked)\n   1256             raise CannotSendHeader()\n-&gt; 1257         self._send_output(message_body, encode_chunked=encode_chunked)\n   1258 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_output(self, message_body, *args, **kwargs)\n    119             message_body = None\n--&gt; 120         self.send(msg)\n    121         if self._expect_header_set:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in send(self, str)\n    203             return\n--&gt; 204         return super(AWSConnection, self).send(str)\n    205 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in send(self, data)\n    995         try:\n--&gt; 996             self.sock.sendall(data)\n    997         except TypeError:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in sendall(self, data, flags)\n    974                 while count &lt; amount:\n--&gt; 975                     v = self.send(byte_view[count:])\n    976                     count += v\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in send(self, data, flags)\n    943                     self.__class__)\n--&gt; 944             return self._sslobj.write(data)\n    945         else:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in write(self, data)\n    641         &quot;&quot;&quot;\n--&gt; 642         return self._sslobj.write(data)\n    643 \n\nProtocolError: ('Connection aborted.', BrokenPipeError(32, 'Broken pipe'))\n\nDuring handling of the above exception, another exception occurred:\n\nConnectionClosedError                     Traceback (most recent call last)\n&lt;ipython-input-14-95dda20e8a70&gt; in &lt;module&gt;\n      1 predictor = DeepARPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n      2 predictor.set_prediction_parameters(freq, prediction_length)\n----&gt; 3 list_of_df = predictor.predict(time_series_training)\n\n&lt;ipython-input-13-a0fbac2b9b07&gt; in predict(self, ts, num_samples, quantiles)\n      7         prediction_times = [x.index[-1] + pd.Timedelta(1, unit=self.freq) for x in ts]\n      8         req = self.__encode_request(ts, num_samples, quantiles)\n----&gt; 9         res = super(DeepARPredictor, self).predict(req, initial_args={&quot;ContentType&quot;: &quot;application\/json&quot;})\n     10         return self.__decode_response(res, prediction_times)\n     11 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, target_variant)\n    123 \n    124         request_args = self._create_request_args(data, initial_args, target_model, target_variant)\n--&gt; 125         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n    126         return self._handle_response(response)\n    127 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    355                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    356             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 357             return self._make_api_call(operation_name, kwargs)\n    358 \n    359         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    661         else:\n    662             http, parsed_response = self._make_request(\n--&gt; 663                 operation_model, request_dict, request_context)\n    664 \n    665         self.meta.events.emit(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_request(self, operation_model, request_dict, request_context)\n    680     def _make_request(self, operation_model, request_dict, request_context):\n    681         try:\n--&gt; 682             return self._endpoint.make_request(operation_model, request_dict)\n    683         except Exception as e:\n    684             self.meta.events.emit(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in make_request(self, operation_model, request_dict)\n    100         logger.debug(&quot;Making request for %s with params: %s&quot;,\n    101                      operation_model, request_dict)\n--&gt; 102         return self._send_request(request_dict, operation_model)\n    103 \n    104     def create_request(self, params, operation_model=None):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _send_request(self, request_dict, operation_model)\n    135             request, operation_model, context)\n    136         while self._needs_retry(attempts, operation_model, request_dict,\n--&gt; 137                                 success_response, exception):\n    138             attempts += 1\n    139             # If there is a stream associated with the request, we need\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _needs_retry(self, attempts, operation_model, request_dict, response, caught_exception)\n    254             event_name, response=response, endpoint=self,\n    255             operation=operation_model, attempts=attempts,\n--&gt; 256             caught_exception=caught_exception, request_dict=request_dict)\n    257         handler_response = first_non_none_response(responses)\n    258         if handler_response is None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in emit(self, event_name, **kwargs)\n    354     def emit(self, event_name, **kwargs):\n    355         aliased_event_name = self._alias_event_name(event_name)\n--&gt; 356         return self._emitter.emit(aliased_event_name, **kwargs)\n    357 \n    358     def emit_until_response(self, event_name, **kwargs):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in emit(self, event_name, **kwargs)\n    226                  handlers.\n    227         &quot;&quot;&quot;\n--&gt; 228         return self._emit(event_name, kwargs)\n    229 \n    230     def emit_until_response(self, event_name, **kwargs):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in _emit(self, event_name, kwargs, stop_on_response)\n    209         for handler in handlers_to_call:\n    210             logger.debug('Event %s: calling handler %s', event_name, handler)\n--&gt; 211             response = handler(**kwargs)\n    212             responses.append((handler, response))\n    213             if stop_on_response and response is not None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempts, response, caught_exception, **kwargs)\n    181 \n    182         &quot;&quot;&quot;\n--&gt; 183         if self._checker(attempts, response, caught_exception):\n    184             result = self._action(attempts=attempts)\n    185             logger.debug(&quot;Retry needed, action of: %s&quot;, result)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    249     def __call__(self, attempt_number, response, caught_exception):\n    250         should_retry = self._should_retry(attempt_number, response,\n--&gt; 251                                           caught_exception)\n    252         if should_retry:\n    253             if attempt_number &gt;= self._max_attempts:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in _should_retry(self, attempt_number, response, caught_exception)\n    275             # If we've exceeded the max attempts we just let the exception\n    276             # propogate if one has occurred.\n--&gt; 277             return self._checker(attempt_number, response, caught_exception)\n    278 \n    279 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    315         for checker in self._checkers:\n    316             checker_response = checker(attempt_number, response,\n--&gt; 317                                        caught_exception)\n    318             if checker_response:\n    319                 return checker_response\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    221         elif caught_exception is not None:\n    222             return self._check_caught_exception(\n--&gt; 223                 attempt_number, caught_exception)\n    224         else:\n    225             raise ValueError(&quot;Both response and caught_exception are None.&quot;)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in _check_caught_exception(self, attempt_number, caught_exception)\n    357         # the MaxAttemptsDecorator is not interested in retrying the exception\n    358         # then this exception just propogates out past the retry code.\n--&gt; 359         raise caught_exception\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _do_get_response(self, request, operation_model)\n    198             http_response = first_non_none_response(responses)\n    199             if http_response is None:\n--&gt; 200                 http_response = self._send(request)\n    201         except HTTPClientError as e:\n    202             return (None, e)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _send(self, request)\n    267 \n    268     def _send(self, request):\n--&gt; 269         return self.http_session.send(request)\n    270 \n    271 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/httpsession.py in send(self, request)\n    349                 error=e,\n    350                 request=request,\n--&gt; 351                 endpoint_url=request.url\n    352             )\n    353         except Exception as e:\n\nConnectionClosedError: Connection was closed before we received a valid response from endpoint URL\n<\/code><\/pre>\n\n<p>Somebody know's why this happens?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1615369010763,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|machine-learning|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":244,
        "Owner_creation_time":1607069622450,
        "Owner_last_access_time":1662644221550,
        "Owner_location":null,
        "Owner_reputation":101,
        "Owner_up_votes":40,
        "Owner_down_votes":5,
        "Owner_views":63,
        "Question_last_edit_time":1615369103569,
        "Answer_body":"<p>I believe that Tarun might on the right path. The BrokenPipeError that you got is thrown when the connection is abruptly closed. See <a href=\"https:\/\/docs.python.org\/3\/library\/exceptions.html#BrokenPipeError\" rel=\"nofollow noreferrer\">the python docs for BrokenPipeError<\/a>.\nThe SageMaker endpoint probably drops the connection as soon as you go over the limit of 5MB. I suggest you try a smaller dataset. Also the data you send might get enlarged because of how sagemaker.tensorflow.model.TensorFlowPredictor encodes the data according to <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/799#issuecomment-492698717\" rel=\"nofollow noreferrer\">this comment<\/a> on a similar issue.<\/p>\n<p>If that doesn't work I've also seen a couple of people having problems with their networks in general. Specifically firewall\/antivirus (<a href=\"https:\/\/github.com\/aws\/aws-cli\/issues\/3999#issuecomment-531151161\" rel=\"nofollow noreferrer\">for example this comment<\/a>) or network timeout.<\/p>\n<p>Hope this points you in the right direction.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1616165232250,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66561959",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":65889143,
        "Question_title":"upload image dataset to S3 sagemaker",
        "Question_body":"<p>my dataset is 3 folders (train, validation and test) of images. each folder has two subfolders (cat1 and cat2). I am using AWS sage maker to preprocess my data and train my model. we all know that we have to upload the training data to S3 bucket before starting the &quot;.fit&quot; process.\nI want to know how to upload my data set to S3<\/p>\n<pre><code># general prefix\nprefix='chest-xray'\n#unique train\/test prefixes\ntrain_prefix   = '{}\/{}'.format(prefix, 'train')\nval_prefix   = '{}\/{}'.format(prefix, 'validation')\ntest_prefix    = '{}\/{}'.format(prefix, 'test')\n\n# uploading data to S3, and saving locations\ntrain_path  = sagemaker_session.upload_data(train_data, bucket=bucket, key_prefix=train_prefix)\n<\/code><\/pre>\n<p>what the train_data parameters should look like<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611594075077,
        "Question_score":0,
        "Question_tags":"amazon-s3|amazon-sagemaker",
        "Question_view_count":444,
        "Owner_creation_time":1477757915557,
        "Owner_last_access_time":1663322043803,
        "Owner_location":null,
        "Owner_reputation":79,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":"<p>According to the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/utility\/session.html#sagemaker.session.Session.upload_data\" rel=\"nofollow noreferrer\">documentation<\/a> <code>train_data<\/code> is the local path of the file to upload to S3, so you need this file locally where you are launching the training job. If you are using a notebook this is not the way to do. You have instead to manually upload your dataset in a S3 bucket. I suggest to preprocess your dataset in a single file (tfrecord for example if you are using TF) and upload that file to S3. You can do it using the AWS web console or using the AWS-CLI with the <code>aws s3 cp yourfile s3:\/\/your-bucket <\/code>command.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1611678605710,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65889143",
        "Question_exclusive_tag":"Amazon SageMaker"
    },
    {
        "Question_id":52278613,
        "Question_title":"Add custom packages to Azure Machine Learing Studio",
        "Question_body":"<p>I need to use the function tsCV on azure machine learning studio to evaluate models of forecast, but i got the error <\/p>\n\n<pre><code>could not find function \"tsCV\n<\/code><\/pre>\n\n<p>I'm trying to update the forecast package, but no package are loaded.\nI followed this tutorial\n<a href=\"http:\/\/blog.revolutionanalytics.com\/2015\/10\/using-minicran-in-azure-ml.html\" rel=\"noreferrer\">http:\/\/blog.revolutionanalytics.com\/2015\/10\/using-minicran-in-azure-ml.html<\/a>\nand \n<a href=\"https:\/\/blog.tallan.com\/2016\/12\/27\/adding-r-packages-in-azure-ml\/\" rel=\"noreferrer\">https:\/\/blog.tallan.com\/2016\/12\/27\/adding-r-packages-in-azure-ml\/<\/a>\nbut i dont get the same result.\nNo packages are load.<\/p>\n\n<p>I need an example of a package with R code that works o Azure ML or an update of forecast package to use tsCV function.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1536677172073,
        "Question_score":3,
        "Question_tags":"r|azure-machine-learning-studio",
        "Question_view_count":404,
        "Owner_creation_time":1515518171123,
        "Owner_last_access_time":1657119408507,
        "Owner_location":null,
        "Owner_reputation":1108,
        "Owner_up_votes":33,
        "Owner_down_votes":2,
        "Owner_views":183,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I have installed the latest version of the forecast package and here are the steps I followed during the installation. <\/p>\n\n<ol>\n<li>Download latest version of CRAN<\/li>\n<li>Be sure that tsCV is working locally<\/li>\n<li>Zip all the dependencies + forecast package<\/li>\n<li>Zip all the generated zips together and upload it to the AMLStudio<\/li>\n<li>Run the following code:<\/li>\n<\/ol>\n\n<blockquote>\n<pre><code>install.packages(\"src\/glue.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/stringi.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/assertthat.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/fansi.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/utf8.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/stringr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/labeling.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/munsell.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/R6.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/RColorBrewer.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/cli.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/crayon.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/pillar.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/xts.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/TTR.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/curl.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/digest.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/gtable.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/lazyeval.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/plyr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/reshape2.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/rlang.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/scales.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/tibble.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/viridisLite.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/withr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/quadprog.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/quantmod.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/colorspace.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/fracdiff.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/ggplot2.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/lmtest.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/magrittr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/Rcpp.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/timeDate.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/tseries.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/urca.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/uroot.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/zoo.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/RcppArmadillo.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/forecast.zip\", lib = \".\", repos = NULL, verbose = TRUE)\n\nlibrary(forecast, lib.loc=\".\", verbose=TRUE)\nfar2 &lt;- function(x, h){forecast(Arima(x, order=c(2,0,0)), h=h)}\ne &lt;- tsCV(lynx, far2, h=1)\n<\/code><\/pre>\n<\/blockquote>\n\n<p><a href=\"https:\/\/drive.google.com\/open?id=10Bj0RGCmRFrRECLQrVc26nbx3T-bNSL6\" rel=\"nofollow noreferrer\">Here is the zip I have generated:<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/bbowH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bbowH.png\" alt=\"My experiment\"><\/a><\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1537192338793,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52278613",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62457880,
        "Question_title":"AML - Web service TimeoutError",
        "Question_body":"<p>We created a webservice endpoint and tested it with the following code, and also with POSTMAN.<\/p>\n\n<p>We deployed the service to an AKS in the same resource group and subscription as the AML resource.<\/p>\n\n<p><strong>UPDATE: the attached AKS had a custom networking configuration and rejected external connections.<\/strong><\/p>\n\n<pre><code>import numpy\nimport os, json, datetime, sys\nfrom operator import attrgetter\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.image import Image\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.authentication import AzureCliAuthentication\n\ncli_auth = AzureCliAuthentication()\n# Get workspace\nws = Workspace.from_config(auth=cli_auth)\n\n# Get the AKS Details\ntry:\n    with open(\"..\/aml_config\/aks_webservice.json\") as f:\n        config = json.load(f)\nexcept:\n    print(\"No new model, thus no deployment on AKS\")\n    # raise Exception('No new model to register as production model perform better')\n    sys.exit(0)\n\nservice_name = config[\"aks_service_name\"]\n# Get the hosted web service\nservice = Webservice(workspace=ws, name=service_name)\n\n# Input for Model with all features\ninput_j = [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]]\nprint(input_j)\ntest_sample = json.dumps({\"data\": input_j})\ntest_sample = bytes(test_sample, encoding=\"utf8\")\ntry:\n    prediction = service.run(input_data=test_sample)\n    print(prediction)\nexcept Exception as e:\n    result = str(e)\n    print(result)\n    raise Exception(\"AKS service is not working as expected\")\n<\/code><\/pre>\n\n<p>In AML Studio, the deployment state is \"Healthy\".<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/RTB10.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RTB10.png\" alt=\"Endpoint attributes\"><\/a><\/p>\n\n<p>We get the following error when testing:<\/p>\n\n<pre><code>Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'\n<\/code><\/pre>\n\n<p><strong>Log just after deploying the AKS Webservice <a href=\"http:\/\/t.ly\/t79b\" rel=\"nofollow noreferrer\">here<\/a>.<\/strong><\/p>\n\n<p><strong>Log after running the test script <a href=\"http:\/\/t.ly\/79k5\" rel=\"nofollow noreferrer\">here<\/a>.<\/strong><\/p>\n\n<p>How can we know what is causing this problem and fix it?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1592508291480,
        "Question_score":2,
        "Question_tags":"azure-machine-learning-studio|azure-aks|azure-machine-learning-service",
        "Question_view_count":332,
        "Owner_creation_time":1585590244877,
        "Owner_last_access_time":1593367986573,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":1592590061567,
        "Answer_body":"<p>We checked the AKS networking configuration and realized it has an Azure CNI profile.<\/p>\n\n<p>In order to test the webservice we need to do it from inside the created virtual network.\nIt worked well!<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1592590202853,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62457880",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":32451243,
        "Question_title":"How to load images faster from Azure Blob?",
        "Question_body":"<p>I've been trying to upload some images to azure blob and then using <strong>ImageReader<\/strong> in <strong>Azure ML studio<\/strong> to read them from the blob. The problem is that ImageReader takes a lot of time to load images and I need it in real time. <br>\nI also tried making a <strong>csv<\/strong> of <strong>4 images (four rows)<\/strong> containing 800x600 pixels as columns <strong>(500,000 cols. approx)<\/strong> and tried simple <strong>Reader<\/strong>. Reader took <strong>31 mins<\/strong> to read the file from the blob.<br>\nI want to know the alternate methods of loading and reading images in Azure ML studio. If anyone know any other method or can share a helpful and relevant link.<br>\nPlease share if i can speed up ImageReader by any means.\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1441695780200,
        "Question_score":1,
        "Question_tags":"opencv|azure|azure-machine-learning-studio",
        "Question_view_count":803,
        "Owner_creation_time":1387426285030,
        "Owner_last_access_time":1659463166403,
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":2128,
        "Owner_up_votes":128,
        "Owner_down_votes":8,
        "Owner_views":211,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Look at the Azure CDN <a href=\"http:\/\/azure.microsoft.com\/en-us\/services\/cdn\/\" rel=\"nofollow\">http:\/\/azure.microsoft.com\/en-us\/services\/cdn\/<\/a> , after which the blobs will get an alternative url. My blob downloads became about 4 times faster after switching.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1441739836497,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/32451243",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67015185,
        "Question_title":"How can I match my local azure automl python sdk version to the remote version?",
        "Question_body":"<p>I'm using the azure automl python sdk to download and save a model then reload it. I get the following error:<\/p>\n<pre><code>anaconda3\\envs\\automl_21\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator Pipeline from version 0.22.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n  UserWarning)\n<\/code><\/pre>\n<p>How can I ensure that the versions match?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617943623520,
        "Question_score":0,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":62,
        "Owner_creation_time":1324351066393,
        "Owner_last_access_time":1663204677817,
        "Owner_location":"Christchurch, New Zealand",
        "Owner_reputation":1306,
        "Owner_up_votes":49,
        "Owner_down_votes":5,
        "Owner_views":81,
        "Question_last_edit_time":null,
        "Answer_body":"<p>My Microsoft contact says -<\/p>\n<p>&quot;For this, their best  bet is probably to see what the training env was pinned to and install those same pins. They can get that env by running child_run.get_environment() and then pip install all the pkgs listed in there with the pins listed there.&quot;<\/p>\n<p>A useful code snippet.<\/p>\n<pre><code>for run in experiment.get_runs():\n    tags_dictionary = run.get_tags()\n    best_run = AutoMLRun(experiment, tags_dictionary['automl_best_child_run_id'])\n    env = best_run.get_environment()\n    print(env.python.conda_dependencies.serialize_to_string())\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1618176409273,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1618178182616,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67015185",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71649163,
        "Question_title":"Can azureml pass variables from one step to another?",
        "Question_body":"<p>I have a requirement to use azure machine learning to develop a pipeline. In this pipeline we don't pass data as inputs\/outputs but variables (for example a list or an int). I have looked on the Microsoft documentation but could not seem to find something fitting my case. Also tried to use the PipelineData class but could not retrieve my variables.<\/p>\n<ol>\n<li>Is this possible?<\/li>\n<li>Is this a good approach?<\/li>\n<\/ol>\n<p>Thanks for your help.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1648478111533,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-studio",
        "Question_view_count":399,
        "Owner_creation_time":1648477773363,
        "Owner_last_access_time":1663242033903,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I know I'm a bit late to the party but here we go:<\/p>\n<p><strong>Passing variables between AzureML Pipeline Steps<\/strong><\/p>\n<p>To directly answer your question, to my knowledge it is not possible to pass variables directly between PythonScriptSteps in an AzureML Pipeline.<\/p>\n<p>The reason for that is that the steps are executed in isolation, i.e. the code is run in different processes or even computes. The only interface a PythonScriptStep has is (a) command line arguments that need to be set prior to submission of the pipeline and (b) data.<\/p>\n<p><strong>Using datasets to pass information between PythonScriptSteps<\/strong><\/p>\n<p>As a workaround you can use PipelineData to pass data between steps.\nThe previously posted blog post may help: <a href=\"https:\/\/vladiliescu.net\/3-ways-to-pass-data-between-azure-ml-pipeline-steps\/\" rel=\"nofollow noreferrer\">https:\/\/vladiliescu.net\/3-ways-to-pass-data-between-azure-ml-pipeline-steps\/<\/a><\/p>\n<p>As for your concrete problem:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># pipeline.py\n\n# This will make Azure create a unique directory on the datastore everytime the pipeline is run.\nvariables_data = PipelineData(&quot;variables_data&quot;, datastore=datastore)\n\n# `variables_data` will be mounted on the target compute and a path is given as a command line argument\nwrite_variable = PythonScriptStep(\n    script_name=&quot;write_variable.py&quot;,\n    arguments=[\n        &quot;--data_path&quot;,\n        variables_data\n    ],\n    outputs=[variables_data],\n)\n\nread_variable = PythonScriptStep(\n    script_name=&quot;read_variable.py&quot;,\n    arguments=[\n        &quot;--data_path&quot;,\n        variables_data\n    ],\n    inputs=[variables_data],\n)\n\n<\/code><\/pre>\n<p>In your script you'll want to serialize the variable \/ object that you're trying to pass between steps:<\/p>\n<p>(You could of course use JSON or any other serialization method)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># write_variable.py\n\nimport argparse\nimport pickle\nfrom pathlib import Path\n\nparser = argparse.ArgumentParser()\nparser.add_argument(&quot;--data_path&quot;)\nargs = parser.parse_args()\n\nobj = [1, 2, 3, 4]\n\nPath(args.data_path).mkdir(parents=True, exist_ok=True)\nwith open(args.data_path + &quot;\/obj.pkl&quot;, &quot;wb&quot;) as f:\n    pickle.dump(obj, f)\n<\/code><\/pre>\n<p>Finally, you can read the variable in the next step:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># read_variable.py\n\nimport argparse\nimport pickle\n\nparser = argparse.ArgumentParser()\nparser.add_argument(&quot;--data_path&quot;)\nargs = parser.parse_args()\n\n\nwith open(args.data_path + &quot;\/obj.pkl&quot;, &quot;rb&quot;) as f:\n    obj = pickle.load(f)\n\nprint(obj)\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1658826630380,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1658826747247,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71649163",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":68172002,
        "Question_title":"How to trigger Azure ML Pipeline from Power Automate",
        "Question_body":"<p>I have a published Azure ML Pipeline that I am trying to trigger from an Automate Flow I have that triggers when users edit a document. Since I have the REST Endpoint for the Published Pipeline, I figured I should be able to make a POST request using the HTTP module available in Power Automate to trigger the pipeline.<\/p>\n<p>However, when I actually try this, I get an authentication error. I assume this is because I need to include some access token with the REST Endpoint, but I can't find any documentation that will tell me where to get that token from. Please note that I do not need to pass any data to the Pipeline, it handles its own data collection, I literally just need a way to trigger it.<\/p>\n<p>Does anybody know how to trigger a Published Azure ML Pipeline using the REST Endpoint? Does it make sense to use the HTTP module, or is there a better way to achieve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624934847563,
        "Question_score":1,
        "Question_tags":"azure|rest|power-automate|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":305,
        "Owner_creation_time":1611181716003,
        "Owner_last_access_time":1639435799460,
        "Owner_location":null,
        "Owner_reputation":119,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1624937966803,
        "Answer_body":"<p>So I figured out how to do it by following the directions contained within this piece of Microsoft Documentation:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-rest\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-rest<\/a><\/p>\n<p>Specifically, it required performing two of the calls in the documentation;<\/p>\n<ul>\n<li>The first to get an AAD token using an Azure Service Principle that is authorised to access the Machine Learning Instance.<\/li>\n<\/ul>\n<blockquote>\n<p>curl -X POST <a href=\"https:\/\/login.microsoftonline.com\/\" rel=\"nofollow noreferrer\">https:\/\/login.microsoftonline.com\/<\/a>\/oauth2\/token -d &quot;grant_type=client_credentials&amp;resource=https%3A%2F%2Fmanagement.azure.com%2F&amp;client_id=&amp;client_secret=&quot;<\/p>\n<\/blockquote>\n<ul>\n<li>The second to use this token to trigger your pipeline from its rest endpoint. This one I had to figure out myself a little, but below is the basic structure I used.<\/li>\n<\/ul>\n<blockquote>\n<p>curl -X POST {PIPELINE_REST_ENDPOINT} -H &quot;Authorisation:Bearer {AAD_TOKEN}&quot; -H &quot;Content-Type: application\/json&quot; -d &quot;{&quot;ExperimentName&quot;: &quot;{EXPERIMENT_NAME}&quot;,&quot;ParameterAssignments&quot;: {}}&quot;<\/p>\n<\/blockquote>",
        "Answer_comment_count":0,
        "Answer_creation_time":1630892629443,
        "Answer_score":0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68172002",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":59829017,
        "Question_title":"Azure Machine Learning - Memory Error while creating dataframe",
        "Question_body":"<p>I am getting memory error while creating simple dataframe read from CSV file on Azure Machine Learning using notebook VM as compute instance. The VM has config of DS 13 56gb RAM, 8vcpu, 112gb storage on Ubuntu (Linux (ubuntu 16.04). CSV file is 5gb file. <\/p>\n\n<pre><code>blob_service = BlockBlobService(account_name,account_key)\nblobstring = blob_service.get_blob_to_text(container,filepath).content\ndffinaldata = pd.read_csv(StringIO(blobstring), sep=',')\n<\/code><\/pre>\n\n<p>What I am doing wrong here ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579544749467,
        "Question_score":1,
        "Question_tags":"python|azure|azure-machine-learning-service",
        "Question_view_count":507,
        "Owner_creation_time":1500744375327,
        "Owner_last_access_time":1660004233300,
        "Owner_location":null,
        "Owner_reputation":255,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":130,
        "Question_last_edit_time":1579556126092,
        "Answer_body":"<p>you need to provide the right encoding when calling get_blob_to_text, please refer to the <a href=\"https:\/\/github.com\/Azure\/azure-storage-python\/blob\/master\/samples\/blob\/block_blob_usage.py#L390\" rel=\"nofollow noreferrer\">sample<\/a>.<\/p>\n\n<p>The code below is what  normally use for reading data file in blob storages. Basically, you can use blob\u2019s url along with sas token and use a request method. However, You might want to edit the \u2018for loop\u2019 depending what types of data you have (e.g. csv, jpg, and etc).<\/p>\n\n<p>-- Python code below --<\/p>\n\n<pre><code>import requests\nfrom azure.storage.blob import BlockBlobService, BlobPermissions\nfrom azure.storage.blob.baseblobservice import BaseBlobService\nfrom datetime import datetime, timedelta\n\naccount_name = '&lt;account_name&gt;'\naccount_key = '&lt;account_key&gt;'\ncontainer_name = '&lt;container_name&gt;'\n\nblob_service=BlockBlobService(account_name,account_key)\ngenerator = blob_service.list_blobs(container_name)\n\nfor blob in generator:\n    url = f\"https:\/\/{account_name}.blob.core.windows.net\/{container_name}\"\n    service = BaseBlobService(account_name=account_name, account_key=account_key)\n    token = service.generate_blob_shared_access_signature(container_name, img_name, permission=BlobPermissions.READ, expiry=datetime.utcnow() + timedelta(hours=1),)\n    url_with_sas = f\"{url}?{token}\"\n    response = requests.get(url_with_sas)\n<\/code><\/pre>\n\n<p>Please follow the below link to read data on Azure Blob Storage.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1579583849897,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1579586200472,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59829017",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":57923187,
        "Question_title":"How to modify the input\/output schema for an Azure deployment service?",
        "Question_body":"<p>I started to develop machine learning models on The Microsoft Azure Machine Learning Studio service. The tutorials and information related to this service are rather clear but I am looking for some information that I did not find concerning the deployment of the service.<\/p>\n\n<p>I would like to understand why the input schema requires the definition of the variable to predict and why the output returns all variable fields given in entry. In this response\/request exchange a part of information transmitted is useless. I wondering if it is possible to modify manually this schema.<\/p>\n\n<p>I searched in the configuration tab of the web service panel but I did not find any information to modify the schema passed to the model.<\/p>\n\n<p>The code below is the input schema that the model requires and the value to predict is <code>WallArea<\/code>. It is not really useful to pass this variable because it is the one we try to predict. (except if we want to compare the actual value and the predicted one for test purpose).<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"WallArea\",\n        \"RoofArea\",\n        \"OverallHeight\",\n        \"GlazingArea\",\n        \"HeatingLoad\"\n      ],\n      \"Values\": [\n        [\n          \"0\",\n          \"0\",\n          \"0\",\n          \"0\",\n          \"0\"\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>The json returned by the model with the predicted value sent all data. It is much more info to what we really need (\"Scored Label Mean\" and \"Scored Label Standard Deviation\")<\/p>\n\n<pre><code>{\n  \"Results\": {\n    \"output1\": {\n      \"type\": \"DataTable\",\n      \"value\": {\n        \"ColumnNames\": [\n          \"WallArea\",\n          \"RoofArea\",\n          \"OverallHeight\",\n          \"GlazingArea\",\n          \"HeatingLoad\",\n          \"Scored Label Mean\",\n          \"Scored Label Standard Deviation\"\n        ],\n        \"ColumnTypes\": [\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\"\n        ],\n        \"Values\": [\n          [\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\"\n          ]\n        ]\n      }\n    }\n  }\n}\n<\/code><\/pre>\n\n<p>My question is how to reduce\/synthesize the input\/output schema if it is possible and why the variable to predict must be sent with the input schema?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1568375681780,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":471,
        "Owner_creation_time":1492331396980,
        "Owner_last_access_time":1651244309180,
        "Owner_location":null,
        "Owner_reputation":197,
        "Owner_up_votes":35,
        "Owner_down_votes":1,
        "Owner_views":40,
        "Question_last_edit_time":1568376011443,
        "Answer_body":"<p>I found the solution.<\/p>\n\n<p>For those who have the same problem, it is pretty simple in fact. You need to add two <strong>Select Columns in Dataset<\/strong> box in your <strong>Predictive experiment<\/strong> schema.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/JA3q2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JA3q2.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Update 2020:<\/strong> Following some updates done on the service, the solution proposed is partially broken. Indeed, if you decide to not include the outcome in the first Select columns box, you well not be able to retrieve it in the second <strong><em>Select Column box<\/em><\/strong> leading to an error. To solve that, you have to remove the first Select Column box and take all features. For the second <strong><em>Select Column box<\/em><\/strong> nothing change, you select the features you want for your predictive response.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1568622249777,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1583405648423,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57923187",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":38500359,
        "Question_title":"Azure: importing not already existing packages in 'src'",
        "Question_body":"<p>I have an experiment in which a module R script uses functions defined in a zip source (Data Exploration). <a href=\"https:\/\/blogs.msdn.microsoft.com\/benjguin\/2014\/09\/24\/how-to-upload-an-r-package-to-azure-machine-learning\/\" rel=\"nofollow noreferrer\">Here<\/a> it's described how to do about the packages not already existing in the Azure environment.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/jKLlP.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jKLlP.png\" alt=\"enter image description here\"><\/a> <\/p>\n\n<p>The DataExploration module has been imported from a file Azure.zip containing all the packages and functions I need (as shown in the next picture).<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WlrVE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WlrVE.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When I run the experiment nothing goes wrong. At the contrary, watching the log it seems clear that Azure is able to manage the source.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/AuJLD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AuJLD.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The problem is that, when I deploy the web service (classic), if I run the experiment I get the following error:<\/p>\n\n<blockquote>\n  <p>FailedToEvaluateRScript: The following error occurred during\n  evaluation of R script: R_tryEval: return error: Error in\n  .zip.unpack(pkg, tmpDir) : zip file 'src\/scales_0.4.0.zip' not found ,\n  Error code: LibraryExecutionError, Http status code: 400, Timestamp:\n  Thu, 21 Jul 2016 09:05:25 GMT<\/p>\n<\/blockquote>\n\n<p>It's like he cannot see the scales_0.4.0.zip into the 'src' folder.<\/p>\n\n<p>The strange fact is that all used to work until some days ago. Then I have copied the experiment on a second workspace and it gives me the above error. <\/p>\n\n<p>I have also tried to upload again the DataExploration module on the new workspace, but it's the same.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1469093505773,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio|azure-web-app-service",
        "Question_view_count":144,
        "Owner_creation_time":1436432728610,
        "Owner_last_access_time":1663607665487,
        "Owner_location":"Colleferro, Italy",
        "Owner_reputation":809,
        "Owner_up_votes":109,
        "Owner_down_votes":0,
        "Owner_views":361,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I have \"solved\" thanks to the help of the AzureML support: it is a bug they are trying to solve right now.<\/p>\n\n<p>The bug shows up when you have <strong>more R script modules<\/strong>, and the <strong>first has no a zip<\/strong> input module while the following have. <\/p>\n\n<p><em>Workaround<\/em>: connect the zip input module to the first R script module too.\n<a href=\"https:\/\/i.stack.imgur.com\/C4fV9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/C4fV9.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1469606718630,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/38500359",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58278844,
        "Question_title":"Does the Azure CLI ML \"service run\" command work?",
        "Question_body":"<p>I have deployed a Model to an ACI container and have an endpoint that I can hit in Postman or using python SDK. I use Python to hit the endpoint as well as Postman and I get a response and the Container Instance logging records the event. I now what to use the AZ ML CLI to run the service and pass in some hardcoded JSON:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/cli\/azure\/ext\/azure-cli-ml\/ml\/service?view=azure-cli-latest#ext-azure-cli-ml-az-ml-service-run\" rel=\"nofollow noreferrer\">From the Azure ML CLI docs<\/a>:  <\/p>\n\n<pre><code>az ml service run --name (-n) --input-data (-d)\n<\/code><\/pre>\n\n<p>I run this <\/p>\n\n<pre><code>az ml service run -n \"rj-aci-5\" -d {\\\"input_df\\\": [{\\\"width\\\": 50, \\\"shoe_size\\\": 28}]}\n<\/code><\/pre>\n\n<p>There is no output or error. The logs do not record any invocation. Has anyone used the Azure CLI ML extensions to run a service in the manner above?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1570495889007,
        "Question_score":0,
        "Question_tags":"azure|azure-cli|azure-machine-learning-service",
        "Question_view_count":63,
        "Owner_creation_time":1256089885500,
        "Owner_last_access_time":1663046676847,
        "Owner_location":"Sydney, Australia",
        "Owner_reputation":4947,
        "Owner_up_votes":277,
        "Owner_down_votes":8,
        "Owner_views":531,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The az cli is likely failing to parse the provided data input. If I attempt to run the same command I see the following error:<\/p>\n\n<p><code>az: error: unrecognized arguments: [{\"width\": 50, \"shoe_size\": 28}]}<\/code><\/p>\n\n<p>You need to wrap the input in quotes for it to appropriately be taken as a single input parameter:<\/p>\n\n<p><code>az ml service run -n \"rj-aci-5\" -d \"{\\\"input_df\\\": [{\\\"width\\\": 50, \\\"shoe_size\\\": 28}]}\"<\/code><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1570637042413,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58278844",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":37462268,
        "Question_title":"Python in AzureML fail to pass dataframe without changes",
        "Question_body":"<p>when trying to pass data without doing anything in python, getting this error:<\/p>\n\n<pre><code>Error 0085: The following error occurred during script evaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\nCaught exception while executing function: Traceback (most recent call last):\n  File \"C:\\server\\invokepy.py\", line 175, in batch\n    rutils.RUtils.DataFrameToRFile(outlist[i], outfiles[i])\n  File \"C:\\server\\RReader\\rutils.py\", line 28, in DataFrameToRFile\n    rwriter.write_attribute_list(attributes)\n  File \"C:\\server\\RReader\\rwriter.py\", line 59, in write_attribute_list\n    self.write_object(value);\n  File \"C:\\server\\RReader\\rwriter.py\", line 121, in write_object\n    write_function(flags, value.values())\n  File \"C:\\server\\RReader\\rwriter.py\", line 104, in write_objects\n    self.write_object(value)\n  File \"C:\\server\\RReader\\rwriter.py\", line 121, in write_object\n    write_function(flags, value.values())\n  File \"C:\\server\\RReader\\rwriter.py\", line 71, in write_integers\n    self.write_integer(value)\n  File \"C:\\server\\RReader\\rwriter.py\", line 147, in write_integer\n    self.writer.WriteInt32(value)\n  File \"C:\\server\\RReader\\BinaryIO\\binarywriter.py\", line 26, in WriteInt32\n    self.WriteData(self.Int32Format, data)\n  File \"C:\\server\\RReader\\BinaryIO\\binarywriter.py\", line 14, in WriteData\n    self.stream.write(pack(format, data))\nerror: cannot convert argument to integer\n\n---------- End of error message from Python  interpreter  ----------\nStart time: UTC 05\/26\/2016 13:16:01\nEnd time: UTC 05\/26\/2016 13:16:13\n<\/code><\/pre>\n\n<p>here is the data i'm trying to pass:\n<a href=\"https:\/\/i.stack.imgur.com\/ysG36.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ysG36.png\" alt=\"the data\"><\/a><\/p>\n\n<p>here is the experiment:\n<a href=\"https:\/\/i.stack.imgur.com\/vgdSn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vgdSn.png\" alt=\"the experiment\"><\/a><\/p>\n\n<p>and the python code:\n<a href=\"https:\/\/i.stack.imgur.com\/LRSAE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LRSAE.png\" alt=\"python code\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1464269301867,
        "Question_score":3,
        "Question_tags":"python|python-2.7|pandas|cortana-intelligence|azure-machine-learning-studio",
        "Question_view_count":739,
        "Owner_creation_time":1320061998253,
        "Owner_last_access_time":1656424560827,
        "Owner_location":null,
        "Owner_reputation":778,
        "Owner_up_votes":176,
        "Owner_down_votes":6,
        "Owner_views":89,
        "Question_last_edit_time":null,
        "Answer_body":"<p>after talking to Microsoft support, the problem was that the \"Execute Python Script\" module cannot return empty values.\nthis can be solved by adding a \"Clean Missing Data\" module before reading it from python:\n<a href=\"https:\/\/i.stack.imgur.com\/BzCUZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BzCUZ.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1468139774183,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37462268",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":59328925,
        "Question_title":"Is it possible to speed up local AzureML model deployment?",
        "Question_body":"<p>We want to be able to quickly test changes to <code>entry_script.py<\/code>. We can test minor changes with unit tests but we want to run a model in the context of our other backend pieces, locally. <\/p>\n\n<p>So we run <code>az ml model deploy<\/code> with a deployment config with a <code>computeType<\/code> of <code>LOCAL<\/code>. Non-local deployment is slow, but we were hoping that local deployment would be faster. Unfortunately it isn't. In some cases it can take up to 20 minutes to deploy a model to a local endpoint.<\/p>\n\n<p>Is there a way to speed this up for faster edit-debug loops or a better way of handling this scenario?<\/p>\n\n<p>Few things I was thinking of:<\/p>\n\n<ul>\n<li>I was thinking <code>az ml service update<\/code> could be an option but even that takes a long time.<\/li>\n<li>Editing the file directly in the container is an option, but this is still annoying to manually synchronize with changes in your local filesystem.<\/li>\n<li>I was thinking of a folder mount in the container, but it seems there is some magic AzureML does, for example copying the <code>entry_script.py<\/code> to <code>\/var\/azureml-app\/main.py<\/code>. We could maybe emulate this by creating a <code>dist<\/code> folder locally that matches the layout and mounting that to the container, but I'm not sure if this folder layout would change or there's other things that AzureML does.<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1576266473653,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":339,
        "Owner_creation_time":1254279877887,
        "Owner_last_access_time":1663949658720,
        "Owner_location":"Seattle, WA",
        "Owner_reputation":153,
        "Owner_up_votes":101,
        "Owner_down_votes":1,
        "Owner_views":111,
        "Question_last_edit_time":1576476751263,
        "Answer_body":"<p>Please follow the below notebook, If you want to test deploying a model rapidly you should check out \n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/tree\/master\/how-to-use-azureml\/deployment\/deploy-to-local\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/tree\/master\/how-to-use-azureml\/deployment\/deploy-to-local<\/a><\/p>\n\n<p>the SDK enables building and running the docker locally and updating in place as you iterate on your script to save time.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1576497182733,
        "Answer_score":1,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59328925",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":36763479,
        "Question_title":"How to tell the learner type of machine learning models",
        "Question_body":"<p>It is my first time to use Azure Machine Learning...<\/p>\n\n<p>When I have trained 2 models using the same training data and testing data, when it comes to evaluate model, it shows error<\/p>\n\n<blockquote>\n  <p>All models must have the same learner type<\/p>\n<\/blockquote>\n\n<p>Do you know what is \"learner type\" of machine learning models and how to tell the learner type of a model?<\/p>\n\n<p>Below is the screenshot of my basic practice on Azure Machine Learning:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/plx4V.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/plx4V.png\" alt=\"Azure Machine Learning practice\"><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1461225939707,
        "Question_score":0,
        "Question_tags":"machine-learning|azure-machine-learning-studio",
        "Question_view_count":516,
        "Owner_creation_time":1361240380857,
        "Owner_last_access_time":1663870148467,
        "Owner_location":null,
        "Owner_reputation":3349,
        "Owner_up_votes":865,
        "Owner_down_votes":1,
        "Owner_views":465,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The models you compare should be of the same type - binary classification, regression, multi-class classification etc. For example, you can't compare effectiveness of linear regression to the effectiveness of logistics regression. They solve absolutely different tasks.<\/p>\n\n<p>This is the case for you - you try to compare linear regression (which outputs real value) with the multiclass decision forest, which tries to classify input to some class.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1461249085857,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36763479",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":50334563,
        "Question_title":"Deployment of an Azure ML Experiment as a Web Service through Azure Machine Learning Studio",
        "Question_body":"<p>I used Machine learning tutorial: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/create-experiment\" rel=\"nofollow noreferrer\">Create your first data science experiment in Azure Machine Learning Studio<\/a> to create an <code>Experiment<\/code> and then converted it to a <code>predictive experiment<\/code>. Now I'm trying to deploy it as a Web Service by following this article that was referenced in the above article: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/publish-a-machine-learning-web-service#deploy-it-as-a-web-service\" rel=\"nofollow noreferrer\">Deploy it as a web service<\/a>. But when I click on <code>Run<\/code> and then on <code>Deploy Web Service<\/code>, I don't see the <code>Price Plan<\/code> dropdown and <code>Plan Name<\/code> input box etc as mentioned in the section <code>Machine Learning Web Service portal Deploy Experiment Page<\/code> of the second article above. After I clicked on Deploy Web Service link in ML studio, I got the page shown below.<strong>Question<\/strong>: What I may be doing wrong?<\/p>\n\n<p>Note: You can click on the picture to get a larger view.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/G3TKo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/G3TKo.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1526313605237,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":330,
        "Owner_creation_time":1330144099340,
        "Owner_last_access_time":1664039192277,
        "Owner_location":null,
        "Owner_reputation":19815,
        "Owner_up_votes":2703,
        "Owner_down_votes":22,
        "Owner_views":2272,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I think it depends on what workspace you're in. If you're in the free one then you get the screen that you already get, but if you create a workspace in the Azure portal and use that one, then you will get a screen like below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/drRpa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/drRpa.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>To create a new workspace, in the Azure Portal, create a new \"Machine Learning Studio Workspace\" and when you go to Azure ML Studio select the new workspace from the top right.<\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1526322971967,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50334563",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":42324035,
        "Question_title":"How to select Scored Probabilities from azure prediction model",
        "Question_body":"<p>I have a model in AzureML that scores incoming values from a csv.<\/p>\n\n<p>The flow is ...->(Score Model using one-class SVM)->(Normalize Data)->(Convert to CSV)->(Convert to Dataset)->(Web Service Output)<\/p>\n\n<p>When the experiment is run I can download the csv from the (Convert to CSV) module output and it will contain Scored Probabilities column.<\/p>\n\n<p>But when I'm using a streaming job I don't know how to access the Scored Probabilities column using Query SQL. How do I do it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1487482693377,
        "Question_score":1,
        "Question_tags":"azure|azure-stream-analytics|azure-machine-learning-studio",
        "Question_view_count":576,
        "Owner_creation_time":1300047702250,
        "Owner_last_access_time":1563817416587,
        "Owner_location":null,
        "Owner_reputation":586,
        "Owner_up_votes":33,
        "Owner_down_votes":3,
        "Owner_views":108,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can access the response using the amlresult.[Scored Probabilities] notation, where amlresult is an alias for the return value from your AzureML call.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1488576068267,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/42324035",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":69355385,
        "Question_title":"Size of the input \/ output parameters in the pipeline step in Azure",
        "Question_body":"<p>while running pipeline creation python script facing the following error.\n&quot;AzureMLCompute job failed. JobConfigurationMaxSizeExceeded: The specified job configuration exceeds the max allowed size of 32768 characters. Please reduce the size of the job's command line arguments and environment settings&quot;<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1632798980813,
        "Question_score":4,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":208,
        "Owner_creation_time":1632461310820,
        "Owner_last_access_time":1650860148037,
        "Owner_location":null,
        "Owner_reputation":107,
        "Owner_up_votes":49,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When we tried to pass a quite lengthy content as argument value to a Pipeline. You can try to upload file to blob, optionally create a dataset, then pass on dataset name or file path to AML pipeline as parameter. The pipeline step will read content of the file from the blob.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1632803094283,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69355385",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67161293,
        "Question_title":"Issues accessing a FileDataset created from HTTP URIs in a PythonScriptStep",
        "Question_body":"<p>I\u2019m having some issues trying to access a FileDataset created from two http URIs in an Azure ML Pipeline PythonScriptStep.<\/p>\n<p>In the step, I\u2019m only getting a single file named <code>['https%3A\u2019]<\/code> when doing an <code>os.listdir()<\/code> on my mount point. I would have expected two files, with their actual names instead. This happens both when sending the dataset <code>as_upload<\/code> and <code>as_mount<\/code>. Even happens when I send the dataset reference to the pipeline step and mount it directly from the step.<\/p>\n<p>The dataset is registered in a notebook, the same notebook that creates and invokes the pipeline, as seen below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>tempFileData = Dataset.File.from_files(\n        ['https:\/\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg',\n        'https:\/\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg'])\ntempFileData.register(ws, name='FileData', create_new_version=True)\n\n#...\n\nread_datasets_step = PythonScriptStep(\n    name='The Dataset Reader',\n    script_name='read-datasets.py',\n    inputs=[fileData.as_named_input('Files'), fileData.as_named_input('Files_mount').as_mount(), fileData.as_named_input('Files_download').as_download()],\n    compute_target=compute_target,\n    source_directory='.\/dataset-reader',\n    allow_reuse=False,\n)\n\n<\/code><\/pre>\n<p>The <code>FileDataset<\/code> seems to be registered properly, if I examine it within the notebook I get the following result:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;source&quot;: [\n    &quot;https:\/\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg&quot;,\n    &quot;https:\/\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg&quot;\n  ],\n  &quot;definition&quot;: [\n    &quot;GetFiles&quot;\n  ],\n  &quot;registration&quot;: {\n    &quot;id&quot;: &quot;...&quot;,\n    &quot;name&quot;: &quot;FileData&quot;,\n    &quot;version&quot;: 4,\n    &quot;workspace&quot;: &quot;Workspace.create(...)&quot;\n  }\n}\n<\/code><\/pre>\n<p>For reference, the machine running the notebook is using AML SDK v1.24, whereas the node running the pipeline steps is running v1.25.<\/p>\n<p>Has anybody encountered anything like this? Is there a way to make it work?<\/p>\n<p>Note that I'm specifically looking at file datasets created from web uris, and not necessarily interested in getting a <code>FileDataset<\/code> to work with blob storage or similar.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618832324140,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":91,
        "Owner_creation_time":1250158552417,
        "Owner_last_access_time":1663847198323,
        "Owner_location":"Romania",
        "Owner_reputation":7916,
        "Owner_up_votes":1735,
        "Owner_down_votes":33,
        "Owner_views":801,
        "Question_last_edit_time":1618849094430,
        "Answer_body":"<p>The files should've been mounted at path &quot;https%3A\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg&quot; and &quot;https%3A\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg&quot;.<\/p>\n<p>We retain the directory structure following the url structure to avoid potential conflicts.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1618855169223,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67161293",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":61546680,
        "Question_title":"ModuleNotFoundError: No module named 'pyspark.dbutils'",
        "Question_body":"<p>I am running pyspark from an Azure Machine Learning notebook. I am trying to move a file using the dbutil module.<\/p>\n\n<pre><code>from pyspark.sql import SparkSession\n    spark = SparkSession.builder.getOrCreate()\n    def get_dbutils(spark):\n        try:\n            from pyspark.dbutils import DBUtils\n            dbutils = DBUtils(spark)\n        except ImportError:\n            import IPython\n            dbutils = IPython.get_ipython().user_ns[\"dbutils\"]\n        return dbutils\n\n    dbutils = get_dbutils(spark)\n    dbutils.fs.cp(\"file:source\", \"dbfs:destination\")\n<\/code><\/pre>\n\n<p>I got this error: \nModuleNotFoundError: No module named 'pyspark.dbutils'\nIs there a workaround for this? <\/p>\n\n<p>Here is the error in another Azure Machine Learning notebook:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-1-183f003402ff&gt; in get_dbutils(spark)\n      4         try:\n----&gt; 5             from pyspark.dbutils import DBUtils\n      6             dbutils = DBUtils(spark)\n\nModuleNotFoundError: No module named 'pyspark.dbutils'\n\nDuring handling of the above exception, another exception occurred:\n\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-1-183f003402ff&gt; in &lt;module&gt;\n     10         return dbutils\n     11 \n---&gt; 12 dbutils = get_dbutils(spark)\n\n&lt;ipython-input-1-183f003402ff&gt; in get_dbutils(spark)\n      7         except ImportError:\n      8             import IPython\n----&gt; 9             dbutils = IPython.get_ipython().user_ns[\"dbutils\"]\n     10         return dbutils\n     11 \n\nKeyError: 'dbutils'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1588351032100,
        "Question_score":2,
        "Question_tags":"pyspark|databricks|azure-databricks|azure-machine-learning-service|dbutils",
        "Question_view_count":7629,
        "Owner_creation_time":1330373362200,
        "Owner_last_access_time":1663907622027,
        "Owner_location":"Kennett Square, PA",
        "Owner_reputation":445,
        "Owner_up_votes":377,
        "Owner_down_votes":0,
        "Owner_views":104,
        "Question_last_edit_time":1591892764407,
        "Answer_body":"<p>This is a known issue with Databricks Utilities - DButils.<\/p>\n\n<p>Most of DButils aren't supported for Databricks Connect. The only parts that do work are <strong>fs<\/strong> and <strong>secrets<\/strong>. <\/p>\n\n<p><strong>Reference:<\/strong> <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/dev-tools\/databricks-connect#limitations\" rel=\"nofollow noreferrer\">Databricks Connect - Limitations<\/a> and <a href=\"https:\/\/datathirst.net\/blog\/2019\/3\/7\/databricks-connect-limitations\" rel=\"nofollow noreferrer\">Known issues<\/a>.<\/p>\n\n<p><strong>Note:<\/strong> Currently fs and secrets work (locally). Widgets (!!!), libraries etc do not work. This shouldn\u2019t be a major issue. If you execute on Databricks using the Python Task dbutils will fail with the error:<\/p>\n\n<pre><code>ImportError: No module named 'pyspark.dbutils'\n<\/code><\/pre>\n\n<p>I'm able to execute the query successfully by running as a notebook.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/RFVm8.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RFVm8.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1588593659053,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61546680",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":43249220,
        "Question_title":"Need more than 2 datasets for \u201cExecute R Script\u201d module in \u201cAzure Machine Learning Studio\u201d",
        "Question_body":"<p>Since connecting to Azure SQL database from \u201cExecute R Script\u201d module in \u201cAzure Machine Learning Studio\u201d is not possible, and using Import Data modules (a.k.a Readers) is the only recommended approach, my question is that what can I do when I need more than 2 datasets as input for \"Execute R Script module\"?<\/p>\n\n<pre><code>\/\/ I'm already doing the following to get first 2 datasets,\ndataset1 &lt;- maml.mapInputPort(1)\ndataset2 &lt;- maml.mapInputPort(2)\n<\/code><\/pre>\n\n<p>How can I \"import\" a dataset3?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1491465771163,
        "Question_score":0,
        "Question_tags":"r|azure|azure-sql-database|azure-machine-learning-studio",
        "Question_view_count":337,
        "Owner_creation_time":1487901477287,
        "Owner_last_access_time":1664081754313,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>One thing you can do is combining two data-sets together and selecting the appropriate fields using the R script. That would be an easy workaround.   <\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1491492855983,
        "Answer_score":0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/43249220",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":36132719,
        "Question_title":"Select columns dynamically in Azure ML model",
        "Question_body":"<p>I have deployed a model as a Webservice in Azure ML.Its a simple one and all it does is do a linear Regression .The underlying code is python . Now i need to pass which all columns have to selected as independent variables, dynamically, from the client side . How may i do this in Azure ML studio?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1458567955050,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-studio",
        "Question_view_count":238,
        "Owner_creation_time":1422821109973,
        "Owner_last_access_time":1664007162760,
        "Owner_location":"India",
        "Owner_reputation":1238,
        "Owner_up_votes":45,
        "Owner_down_votes":5,
        "Owner_views":172,
        "Question_last_edit_time":1458612577147,
        "Answer_body":"<p>Based on my understanding, I think you want to dynamically get the selected columns data via request the Azure ML webservice with some parameters on the client.<\/p>\n\n<p>You can refer to the offical document <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-web-service-parameters\/\" rel=\"nofollow\">Use Azure Machine Learning Web Service Parameters<\/a> and the blog <a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2014\/11\/25\/azureml-web-service-parameters\/\" rel=\"nofollow\">AzureML Web Service Parameters<\/a> to know how to set and use the web service parameters to implement your needs via add the selected column names as array into the json parameter <code>GlobalParameters<\/code>.<\/p>\n\n<p>Meanwhile, there is a client sample on GitHub <a href=\"https:\/\/github.com\/nk773\/AzureML_RRSApp\" rel=\"nofollow\">https:\/\/github.com\/nk773\/AzureML_RRSApp<\/a>. Althought it was writen in Java, I think it is easy to understand, then you can rewrite in Python with <code>requests<\/code> package.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1458632118243,
        "Answer_score":0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1458699277763,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36132719",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72775967,
        "Question_title":"R, pins and AzureStor: unused argument (azure_storage_progress_bar = progress)",
        "Question_body":"<pre><code>pins 1.0.1\nAzureStor 3.7.0\n<\/code><\/pre>\n<p>I'm getting this error<\/p>\n<pre><code>Error in withr::local_options(azure_storage_progress_bar = progress, .local_envir = env) : \n  unused argument (azure_storage_progress_bar = progress)\nCalls: %&gt;% ... pin_meta.pins_board_azure -&gt; azure_download -&gt; local_azure_progress\nExecution halted\n<\/code><\/pre>\n<p>when running <code>pin_read()<\/code> in the following code (<code>pin_list()<\/code> works fine)<\/p>\n<pre><code>bl_endp_key &lt;- storage_endpoint(endpoint = &lt;endpoint URL&gt;, key =&lt;endpoint key&gt;&quot;)\ncontainer &lt;- storage_container(endpoint = bl_endp_key, name = &lt;blob name&gt;)\nboard &lt;- board_azure(container = container, path = &quot;accidentsdata&quot;)\ncat(&quot;Testing pins:\\n&quot;)\nprint(board %&gt;% pin_list())\naccidents2 &lt;- board %&gt;% pins::pin_read('accidents') %&gt;% as_tibble()\n<\/code><\/pre>\n<p>My goal is to &quot;pin_read&quot; a dataset located on a Azure Blob Storage from an R script being run from <strong>pipelineJoB (YAML)<\/strong> including a <code>command: Rscript script.R ...<\/code> and an <code>environment:<\/code> based on a dockerfile installing <strong>R version 4.0.0<\/strong> (2020-04-24) -- &quot;Arbor Day&quot;<\/p>\n<p>The pipelineJob is being called from an Azure DevOps Pipeline task with <code>az ml job create &lt;pipelineJob YAML&gt; &lt;resource grp&gt; &lt;aml workspace name&gt;<\/code>.<\/p>\n<p>Note: the R script runs fine on my Windows RStudio desktop, with R version 4.1.3 (2022-03-10) -- &quot;One Push-Up&quot;.<\/p>\n<p>I've already tried with<\/p>\n<p><code>options(azure_storage_progress_bar=FALSE)<\/code> or<\/p>\n<p><code>withr::local_options(azure_storage_progress_bar=FALSE)<\/code><\/p>\n<p>but I'm getting the same <code>unused argument (azure_storage_progress_bar ...<\/code> error.<\/p>\n<p>FYI: <code>local_azure_progress<\/code> is defined here <a href=\"https:\/\/rdrr.io\/github\/rstudio\/pins\/src\/R\/board_azure.R#sym-local_azure_progress\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1656349974407,
        "Question_score":0,
        "Question_tags":"r|azure-devops|azure-pipelines|azure-machine-learning-service|pins",
        "Question_view_count":80,
        "Owner_creation_time":1384730587840,
        "Owner_last_access_time":1657810539597,
        "Owner_location":"France",
        "Owner_reputation":717,
        "Owner_up_votes":143,
        "Owner_down_votes":0,
        "Owner_views":112,
        "Question_last_edit_time":1656680662489,
        "Answer_body":"<p>Issue has been filed in <a href=\"https:\/\/github.com\/rstudio\/pins\/issues\/624\" rel=\"nofollow noreferrer\">pins<\/a>, it seems that is not an AzureStor issue.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1656936659777,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72775967",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":34047782,
        "Question_title":"Jupyter notebook kernel dies when creating dummy variables with pandas",
        "Question_body":"<p>I am working on the Walmart Kaggle competition and I'm trying to create a dummy column of of the \"FinelineNumber\" column. For context, <code>df.shape<\/code> returns <code>(647054, 7)<\/code>. I am trying to make a dummy column for <code>df['FinelineNumber']<\/code>, which has 5,196 unique values. The results should be a dataframe of shape <code>(647054, 5196)<\/code>, which I then plan to <code>concat<\/code> to the original dataframe. <\/p>\n\n<p>Nearly every time I run <code>fineline_dummies = pd.get_dummies(df['FinelineNumber'], prefix='fl')<\/code>, I get the following error message <code>The kernel appears to have died. It will restart automatically.<\/code> I am running python 2.7 in jupyter notebook on a MacBookPro with 16GB RAM.<\/p>\n\n<p>Can someone explain why this is happening (and why it happens most of the time but not every time)? Is it a jupyter notebook or pandas bug? Also, I thought it might have to do with not enough RAM but I get the same error on a Microsoft Azure Machine Learning notebook with >100 GB of RAM. On Azure ML, the kernel dies every time - almost immediately.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1449073471520,
        "Question_score":5,
        "Question_tags":"python|pandas|ipython-notebook|azure-machine-learning-studio",
        "Question_view_count":5063,
        "Owner_creation_time":1373475615300,
        "Owner_last_access_time":1663780318293,
        "Owner_location":"New York, United States",
        "Owner_reputation":2037,
        "Owner_up_votes":61,
        "Owner_down_votes":1,
        "Owner_views":193,
        "Question_last_edit_time":1454300528203,
        "Answer_body":"<p>It very much could be memory usage - a 647054, 5196 data frame has 3,362,092,584 elements, which would be 24GB just for the pointers to the objects on a 64-bit system.  On AzureML while the VM has a large amount of memory you're actually limited in how much memory you have available (currently 2GB, soon to be 4GB) - and when you hit the limit the kernel typically dies.  So it seems very likely it is a memory usage issue.<\/p>\n\n<p>You might try doing <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/stable\/sparse.html\" rel=\"noreferrer\">.to_sparse()<\/a> on the data frame first before doing any additional manipulations.  That should allow Pandas to keep most of the data frame out of memory.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1449768300887,
        "Answer_score":8,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34047782",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60638587,
        "Question_title":"How to get insights in exceptions and logging of AzureML endpoint deployment",
        "Question_body":"<p>Because of a faulty score.py file in my InferenceConfig, a Model.Deploy failed to Azure Machine Learning, using ACI.  I wanted to create the endpoint in the cloud, but the only state I can see in the portal is Unhealthy.  My local script to deploy the model (using ) keeps running, until it times out. (using the <code>service.wait_for_deployment(show_output=True)<\/code>statement).<\/p>\n\n<p>Is there an option to get more insights in the actual reason\/error message of the deployment turning \"Unhealthy\"?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1583937572180,
        "Question_score":3,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":364,
        "Owner_creation_time":1360655430743,
        "Owner_last_access_time":1663784892907,
        "Owner_location":"Belgium",
        "Owner_reputation":2947,
        "Owner_up_votes":297,
        "Owner_down_votes":16,
        "Owner_views":355,
        "Question_last_edit_time":1584215688009,
        "Answer_body":"<p>Usually the timeout is caused by an error in init() function in scoring script. You can get the detailed logs using <code>print(service.get_logs())<\/code> to find the Python error.<\/p>\n\n<p>For more comprehensive troubleshooting guide, see:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1584133364177,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60638587",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":57436140,
        "Question_title":"How to reuse successfully built docker images in Azure ML?",
        "Question_body":"<p>In our company I use Azure ML and I have the following issue. I specify a <em>conda_requirements.yaml<\/em> file with the PyTorch estimator class, like so (... are placeholders so that I do not have to type everything out):<\/p>\n\n<pre><code>from azureml.train.dnn import PyTorch\nest = PyTorch(source_directory=\u2019.\u2019, script_params=..., compute_target=..., entry_script=..., conda_dependencies_file_path=\u2019conda_requirements.yaml\u2019, environment_variables=..., framework_version=\u20191.1\u2019)\n<\/code><\/pre>\n\n<p>The <em>conda_requirements.yaml<\/em> (shortened version of the pip part) looks like this:<\/p>\n\n<pre><code>dependencies:\n  -  conda=4.5.11\n  -  conda-package-handling=1.3.10\n  -  python=3.6.2\n  -  cython=0.29.10\n  -  scikit-learn==0.21.2\n  -  anaconda::cloudpickle==1.2.1\n  -  anaconda::cffi==1.12.3\n  -  anaconda::mxnet=1.1.0\n  -  anaconda::psutil==5.6.3\n  -  anaconda::pip=19.1.1\n  -  anaconda::six==1.12.0\n  -  anaconda::mkl==2019.4\n  -  conda-forge::openmpi=3.1.2\n  -  conda-forge::pycparser==2.19\n  -  tensorboard==1.13.1\n  -  tensorflow==1.13.1\n  -  pip:\n        - torch==1.1.0\n        - torchvision==0.2.1\n<\/code><\/pre>\n\n<p>This successfully builds on Azure. Now in order to reuse the resulting docker image in that case, I use the <code>custom_docker_image<\/code> parameter to pass to the <\/p>\n\n<pre><code>from azureml.train.estimator import Estimator\nest = Estimator(source_directory=\u2019.\u2019, script_params=..., compute_target=..., entry_script=..., custom_docker_image=\u2019&lt;container registry name&gt;.azurecr.io\/azureml\/azureml_c3a4f...\u2019, environment_variables=...)\n<\/code><\/pre>\n\n<p>But now Azure somehow seems to rebuild the image again and when I run the experiment it cannot install torch. So it seems to only install the conda dependencies and not the pip dependencies, but actually I do not want Azure to rebuild the image. Can I solve this somehow?<\/p>\n\n<p>I attempted to somehow build a docker image from my Docker file and then add to the registry. I can do az login and according to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/container-registry\/container-registry-authentication\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/container-registry\/container-registry-authentication<\/a> I then should also be able to do an acr login and push. This does not work. \nEven using the credentials from<\/p>\n\n<pre><code>az acr credential show \u2013name &lt;container registry name&gt;\n<\/code><\/pre>\n\n<p>and then doing a <\/p>\n\n<pre><code>docker login &lt;container registry name&gt;.azurecr.io \u2013u &lt;username from credentials above&gt; -p &lt;password from credentials above&gt;\n<\/code><\/pre>\n\n<p>does not work.\nThe error message is <em>authentication required<\/em> even though I used <\/p>\n\n<pre><code>az login\n<\/code><\/pre>\n\n<p>successfully. Would also be happy if someone could explain that to me in addition to how to reuse docker images when using Azure ML.\nThank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1565379277197,
        "Question_score":1,
        "Question_tags":"azure|docker|pip|conda|azure-machine-learning-service",
        "Question_view_count":604,
        "Owner_creation_time":1524670343370,
        "Owner_last_access_time":1663769326433,
        "Owner_location":"Z\u00fcrich, Schweiz",
        "Owner_reputation":460,
        "Owner_up_votes":668,
        "Owner_down_votes":0,
        "Owner_views":52,
        "Question_last_edit_time":null,
        "Answer_body":"<p>AzureML should actually cache your docker image once it was created. The service will hash the base docker info and the contents of the conda.yaml file and will use that as the hash key -- unless you change any of that information, the docker should come from the ACR. <\/p>\n\n<p>As for the custom docker usage, did you set the parameter <code>user_managed=True<\/code>? Otherwise, AzureML will consider your docker to be a base image on top of which it will create the conda environment per your yaml file.<br>\nThere is an example of how to use a custom docker image in this notebook:\n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4170a394edd36413edebdbab347afb0d833c94ee\/how-to-use-azureml\/training-with-deep-learning\/how-to-use-estimator\/how-to-use-estimator.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4170a394edd36413edebdbab347afb0d833c94ee\/how-to-use-azureml\/training-with-deep-learning\/how-to-use-estimator\/how-to-use-estimator.ipynb<\/a><\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1565501907287,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57436140",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":51088145,
        "Question_title":"Azure Python SDK & Machine Learning Studio Web Service Batch Execution Snippet: TypeError",
        "Question_body":"<p><strong>First Issue resolved, please read scroll down to EDIT2<\/strong><\/p>\n\n<p>I'm trying to access a Web Service deployed via Azure Machine Learning Studio, using the Batch Execution-Sample Code for Python on the bottom of below page:<\/p>\n\n<p><a href=\"https:\/\/studio.azureml.net\/apihelp\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/webservices\/e4e3d2d32ec347ae9a829b200f7d31cd\/endpoints\/61670382104542bc9533a920830b263c\/jobs\" rel=\"nofollow noreferrer\">https:\/\/studio.azureml.net\/apihelp\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/webservices\/e4e3d2d32ec347ae9a829b200f7d31cd\/endpoints\/61670382104542bc9533a920830b263c\/jobs<\/a><\/p>\n\n<p>I have already fixed an Issue according to this question (replaced BlobService by BlobBlockService and so on):<\/p>\n\n<p><a href=\"https:\/\/studio.azureml.net\/apihelp\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/webservices\/e4e3d2d32ec347ae9a829b200f7d31cd\/endpoints\/61670382104542bc9533a920830b263c\/jobs\" rel=\"nofollow noreferrer\">https:\/\/studio.azureml.net\/apihelp\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/webservices\/e4e3d2d32ec347ae9a829b200f7d31cd\/endpoints\/61670382104542bc9533a920830b263c\/jobs<\/a><\/p>\n\n<p>And I also have entered the API-Key, Container-Name, URL, account_key and account_name according to the instructions.<\/p>\n\n<p>However it seems that today the Code Snippet is even more outdated than it was back then because I receive a different error now:<\/p>\n\n<pre><code>File \"C:\/Users\/Alex\/Desktop\/scripts\/BatchExecution.py\", line 80, in uploadFileToBlob\n    blob_service = asb.BlockBlobService(account_name=storage_account_name, account_key=storage_account_key)\n\n  File \"C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\azure\\storage\\blob\\blockblobservice.py\", line 145, in __init__\n\n  File \"C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\azure\\storage\\blob\\baseblobservice.py\", line 205, in __init__\n\nTypeError: get_service_parameters() got an unexpected keyword argument 'token_credential'\n<\/code><\/pre>\n\n<p>I also noticed, that when installing the Azure SDK for Python via pip, I get the following warnings in the end of the process (installation is successful however):<\/p>\n\n<pre><code>azure-storage-queue 1.3.0 has requirement azure-storage-common&lt;1.4.0,&gt;=1.3.0, but you'll have azure-storage-common 1.1.0 which is incompatible.\n\nazure-storage-file 1.3.0 has requirement azure-storage-common&lt;1.4.0,&gt;=1.3.0, but you'll have azure-storage-common 1.1.0 which is incompatible.\n\nazure-storage-blob 1.3.0 has requirement azure-storage-common&lt;1.4.0,&gt;=1.3.0, but you'll have azure-storage-common 1.1.0 which is incompatible.\n<\/code><\/pre>\n\n<p>I can't find anything about all this in the latest documentation for the Python SDK (the word 'token_credential' is not even contained):<\/p>\n\n<p><a href=\"https:\/\/media.readthedocs.org\/pdf\/azure-storage\/latest\/azure-storage.pdf\" rel=\"nofollow noreferrer\">https:\/\/media.readthedocs.org\/pdf\/azure-storage\/latest\/azure-storage.pdf<\/a><\/p>\n\n<p>Does anyone have a clue what's going wrong during the installation or why the type-error with the 'token_credential' pops up during execution?<\/p>\n\n<p>Or does anyone know how I can install the necessary version of azure-storage-common or azure-storage-blob?<\/p>\n\n<p>EDIT: Here's a my code (however not-reproducible because I changed the keys before posting)<\/p>\n\n<pre><code># How this works:\n#\n# 1. Assume the input is present in a local file (if the web service accepts input)\n# 2. Upload the file to an Azure blob - you\"d need an Azure storage account\n# 3. Call BES to process the data in the blob. \n# 4. The results get written to another Azure blob.\n\n# 5. Download the output blob to a local file\n#\n# Note: You may need to download\/install the Azure SDK for Python.\n# See: http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/python-how-to-install\/\n\nimport urllib\n# If you are using Python 3+, import urllib instead of urllib2\n\nimport json\nimport time\nimport azure.storage.blob as asb          # replaces BlobService by BlobBlockService\n\n\ndef printHttpError(httpError):\n    print(\"The request failed with status code: \" + str(httpError.code))\n\n    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n    print(httpError.info())\n\n    print(json.loads(httpError.read()))\n    return\n\n\ndef saveBlobToFile(blobUrl, resultsLabel):\n    output_file = \"myresults.csv\" # Replace this with the location you would like to use for your output file\n    print(\"Reading the result from \" + blobUrl)\n    try:\n        # If you are using Python 3+, replace urllib2 with urllib.request in the following code\n        response = urllib.request.urlopen(blobUrl)\n    except urllib.request.HTTPError:\n        printHttpError(urllib.HTTPError)\n        return\n\n    with open(output_file, \"w+\") as f:\n        f.write(response.read())\n    print(resultsLabel + \" have been written to the file \" + output_file)\n    return\n\n\ndef processResults(result):\n\n\n    first = True\n    results = result[\"Results\"]\n    for outputName in results:\n        result_blob_location = results[outputName]\n        sas_token = result_blob_location[\"SasBlobToken\"]\n        base_url = result_blob_location[\"BaseLocation\"]\n        relative_url = result_blob_location[\"RelativeLocation\"]\n\n        print(\"The results for \" + outputName + \" are available at the following Azure Storage location:\")\n        print(\"BaseLocation: \" + base_url)\n        print(\"RelativeLocation: \" + relative_url)\n        print(\"SasBlobToken: \" + sas_token)\n\n\n        if (first):\n            first = False\n            url3 = base_url + relative_url + sas_token\n            saveBlobToFile(url3, \"The results for \" + outputName)\n    return\n\n\n\ndef uploadFileToBlob(input_file, input_blob_name, storage_container_name, storage_account_name, storage_account_key):\n    blob_service = asb.BlockBlobService(account_name=storage_account_name, account_key=storage_account_key)\n\n    print(\"Uploading the input to blob storage...\")\n    data_to_upload = open(input_file, \"r\").read()\n    blob_service.put_blob(storage_container_name, input_blob_name, data_to_upload, x_ms_blob_type=\"BlockBlob\")\n\ndef invokeBatchExecutionService():\n    storage_account_name = \"storage1\" # Replace this with your Azure Storage Account name\n    storage_account_key = \"kOveEtQMoP5zbUGfFR47\" # Replace this with your Azure Storage Key\n    storage_container_name = \"input\" # Replace this with your Azure Storage Container name\n    connection_string = \"DefaultEndpointsProtocol=https;AccountName=\" + storage_account_name + \";AccountKey=\" + storage_account_key #\"DefaultEndpointsProtocol=https;AccountName=mayatostorage1;AccountKey=aOYA2P5VQPR3ZQCl+aWhcGhDRJhsR225teGGBKtfXWwb2fNEo0CrhlwGWdfbYiBTTXPHYoKZyMaKuEAU8A\/Fzw==;EndpointSuffix=core.windows.net\"\n    api_key = \"5wUaln7n99rt9k+enRLG2OrhSsr9VLeoCfh0q3mfYo27hfTCh32f10PsRjJtuA==\" # Replace this with the API key for the web service\n    url = \"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/306bc1f050\/services\/61670382104542bc9533a920830b263c\/jobs\" #\"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/services\/61670382104542bc9533a920830b263c\/jobs\/job_id\/start?api-version=2.0\"\n\n\n\n    uploadFileToBlob(r\"C:\\Users\\Alex\\Desktop\\16_da.csv\", # Replace this with the location of your input file\n                     \"input1datablob.csv\", # Replace this with the name you would like to use for your Azure blob; this needs to have the same extension as the input file \n                     storage_container_name, storage_account_name, storage_account_key)\n\n    payload =  {\n\n        \"Inputs\": {\n\n            \"input1\": { \"ConnectionString\": connection_string, \"RelativeLocation\": \"\/\" + storage_container_name + \"\/input1datablob.csv\" },\n        },     \n\n        \"Outputs\": {\n\n            \"output1\": { \"ConnectionString\": connection_string, \"RelativeLocation\": \"\/\" + storage_container_name + \"\/output1results.csv\" },\n        },\n        \"GlobalParameters\": {\n}\n    }\n\n    body = str.encode(json.dumps(payload))\n    headers = { \"Content-Type\":\"application\/json\", \"Authorization\":(\"Bearer \" + api_key)}\n    print(\"Submitting the job...\")\n\n    # If you are using Python 3+, replace urllib2 with urllib.request in the following code\n\n    # submit the job\n    req = urllib.request.Request(url + \"?api-version=2.0\", body, headers)\n    try:\n        response = urllib.request.urlopen(req)\n    except urllib.request.HTTPError:\n        printHttpError(urllib.HTTPError)\n        return\n\n    result = response.read()\n    job_id = result[1:-1] # remove the enclosing double-quotes\n    print(\"Job ID: \" + job_id)\n\n\n    # If you are using Python 3+, replace urllib2 with urllib.request in the following code\n    # start the job\n    print(\"Starting the job...\")\n    req = urllib.request.Request(url + \"\/\" + job_id + \"\/start?api-version=2.0\", \"\", headers)\n    try:\n        response = urllib.request.urlopen(req)\n    except urllib.request.HTTPError:\n        printHttpError(urllib.HTTPError)\n        return\n\n    url2 = url + \"\/\" + job_id + \"?api-version=2.0\"\n\n    while True:\n        print(\"Checking the job status...\")\n        # If you are using Python 3+, replace urllib2 with urllib.request in the follwing code\n        req = urllib.request.Request(url2, headers = { \"Authorization\":(\"Bearer \" + api_key) })\n\n        try:\n            response = urllib.request.urlopen(req)\n        except urllib.request.HTTPError:\n            printHttpError(urllib.HTTPError)\n            return    \n\n        result = json.loads(response.read())\n        status = result[\"StatusCode\"]\n        if (status == 0 or status == \"NotStarted\"):\n            print(\"Job \" + job_id + \" not yet started...\")\n        elif (status == 1 or status == \"Running\"):\n            print(\"Job \" + job_id + \" running...\")\n        elif (status == 2 or status == \"Failed\"):\n            print(\"Job \" + job_id + \" failed!\")\n            print(\"Error details: \" + result[\"Details\"])\n            break\n        elif (status == 3 or status == \"Cancelled\"):\n            print(\"Job \" + job_id + \" cancelled!\")\n            break\n        elif (status == 4 or status == \"Finished\"):\n            print(\"Job \" + job_id + \" finished!\")\n\n            processResults(result)\n            break\n        time.sleep(1) # wait one second\n    return\n\ninvokeBatchExecutionService()\n<\/code><\/pre>\n\n<p>EDIT 2: The above issue has been resolved thanks to jon and the csv gets uploaded in blob storage.<\/p>\n\n<p>However now there is an HTTPError, when the job gets submitted in Line 130:<\/p>\n\n<pre><code>   raise HTTPError(req.full_url, code, msg, hdrs, fp)  HTTPError: Bad Request\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1530205297260,
        "Question_score":0,
        "Question_tags":"python|rest|azure|azure-blob-storage|azure-machine-learning-studio",
        "Question_view_count":499,
        "Owner_creation_time":1513445313103,
        "Owner_last_access_time":1646340377377,
        "Owner_location":"NE",
        "Owner_reputation":29,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":1530279219952,
        "Answer_body":"<p>I think the code they give may be pretty old at this point.<\/p>\n\n<p>The <a href=\"https:\/\/pypi.org\/project\/azure-storage-blob\/#history\" rel=\"nofollow noreferrer\">latest version<\/a> of <code>azure.storage.blob<\/code> is 1.3. So perhaps a <code>pip install azure.storage.blob --update<\/code> or simply uninstalling and reinstalling would help.<\/p>\n\n<p>Once you got the latest version, try using the <code>create_blob_from_text<\/code> method to load the file to your storage container.<\/p>\n\n<pre><code>from azure.storage.blob import BlockBlobService\n\nblobService = BlockBlobService(account_name=\"accountName\", account_key=\"accountKey)\n\nblobService.create_blob_from_text(\"containerName\", \"fileName\", csv_file)\n<\/code><\/pre>\n\n<p>Hope that works to help lead you down the right path, but if not we can work through it. :)<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1530266278530,
        "Answer_score":1,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51088145",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":37520849,
        "Question_title":"Can't approximate simple multiplication function in neural network with 1 hidden layer",
        "Question_body":"<p>I just wanted to test how good can neural network approximate multiplication function (regression task). \nI am using Azure Machine Learning Studio. I have 6500 samples, 1 hidden layer\n(I have tested 5 \/30 \/100 neurons per hidden layer), no normalization. And default parameters \n<a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn906030.aspx\" rel=\"nofollow\">Learning rate - 0.005, Number of learning iterations - 200, The initial learning weigh - 0.1,\n The momentum - 0 [description]<\/a>. I got extremely bad accuracy, close to 0.\n<em>At the same time boosted Decision forest regression shows very good approximation.<\/em><\/p>\n\n<p>What am I doing wrong? This task should be very easy for NN.<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1464596393980,
        "Question_score":3,
        "Question_tags":"neural-network|deep-learning|azure-machine-learning-studio",
        "Question_view_count":4342,
        "Owner_creation_time":1354644345813,
        "Owner_last_access_time":1624458498500,
        "Owner_location":null,
        "Owner_reputation":3969,
        "Owner_up_votes":674,
        "Owner_down_votes":17,
        "Owner_views":356,
        "Question_last_edit_time":1491916922983,
        "Answer_body":"<p>Big multiplication function gradient forces the net probably almost immediately into some horrifying state where all its hidden nodes have zero gradient.\nWe can use two approaches:<\/p>\n\n<p>1) Devide by constant. We are just deviding everything before the learning and multiply after.<\/p>\n\n<p>2) Make log-normalization. It makes multiplication into addition:<\/p>\n\n<pre><code>m = x*y =&gt; ln(m) = ln(x) + ln(y).\n<\/code><\/pre>",
        "Answer_comment_count":1,
        "Answer_creation_time":1465277784230,
        "Answer_score":5,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":1465820459196,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37520849",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":66923216,
        "Question_title":"Change Disk Type Azure ML",
        "Question_body":"<p>I have azure ml , I created compute for learning.\nCost for instance is 2-5usd with my use. But cost for p10(premium SSD) Disk 17usd.<\/p>\n<p>I don't know how change it because its not appear in azure Disk and in ML studio i cant find option for manage storage type for compute.<\/p>\n<p>Some one know how change it ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617385541497,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-service",
        "Question_view_count":150,
        "Owner_creation_time":1561015783553,
        "Owner_last_access_time":1664083277457,
        "Owner_location":null,
        "Owner_reputation":97,
        "Owner_up_votes":167,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There is no possible way to change the compute disk type if you use the Azure ML compute cluster and compute instance. Only when you use the extra computer, you can manage the separate resources such as the disk, network, and so on. For example, you attach a VM as the target computer to the Azure ML. Then when you create the VM you can set the disk type with HDD.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1617778657403,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66923216",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":68422680,
        "Question_title":"how to write to Azure PipelineData properly?",
        "Question_body":"<p>Im trying to learn Azure, with little luck (yet). All the tutorials show using PipelineData just as a file, when configured in &quot;upload&quot; mode. However, im getting &quot;FileNotFoundError: [Errno 2] No such file or directory: ''&quot; error. I would love to ask a more specific question, but i just can't see what im doing wrong.<\/p>\n<pre><code>from azureml.core import Workspace, Datastore,Dataset,Environment\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.core.runconfig import RunConfiguration\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.pipeline.steps import PythonScriptStep\nfrom azureml.pipeline.core import Pipeline, PipelineData\nimport os\n\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\n\ncompute_name = &quot;cpucluster&quot;\ncompute_target = ComputeTarget(workspace=ws, name=compute_name)\naml_run_config = RunConfiguration()\naml_run_config.target = compute_target\naml_run_config.environment.python.user_managed_dependencies = False\naml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['pandas','scikit-learn'], \n    pip_packages=['azureml-sdk', 'azureml-dataprep[fuse,pandas]'], \n    pin_sdk_version=False)\n\noutput1 = PipelineData(&quot;processed_data1&quot;,datastore=datastore, output_mode=&quot;upload&quot;)\nprep_step = PythonScriptStep(\n    name=&quot;dataprep&quot;,\n    script_name=&quot;dataprep.py&quot;,\n    source_directory=os.path.join(os.getcwd(),'dataprep'),\n    arguments=[&quot;--output&quot;, output1],\n    outputs = [output1],\n    compute_target=compute_target,\n    runconfig=aml_run_config,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>In the dataprep.py i hve the following:<\/p>\n<pre><code>import numpy, argparse, pandas\nfrom azureml.core import Run\nrun = Run.get_context()\nparser = argparse.ArgumentParser()\nparser.add_argument('--output', dest='output', required=True)\nargs = parser.parse_args()\ndf = pandas.DataFrame(numpy.random.rand(100,3))\ndf.iloc[:, 2] = df.iloc[:,0] + df.iloc[:,1]\nprint(df.iloc[:5,:])\ndf.to_csv(args.output)\n\n<\/code><\/pre>\n<p>So, yeah. pd is supposed to write to the output, but my compute cluster says the following:<\/p>\n<pre><code>&quot;User program failed with FileNotFoundError: [Errno 2] No such file or directory: ''\\&quot;.\n<\/code><\/pre>\n<p>When i dont include the to_csv() function, the cluster does not complain<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1626541888290,
        "Question_score":4,
        "Question_tags":"python|azure|azure-machine-learning-service",
        "Question_view_count":404,
        "Owner_creation_time":1588424911653,
        "Owner_last_access_time":1664069253107,
        "Owner_location":null,
        "Owner_reputation":59,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":1626559748576,
        "Answer_body":"<p>Here is an <a href=\"https:\/\/github.com\/james-tn\/highperformance_python_in_azure\/blob\/master\/parallel_python_processing\/pipeline_definition.ipynb\" rel=\"nofollow noreferrer\">example<\/a> for PRS.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipelinedata?view=azure-ml-py\" rel=\"nofollow noreferrer\">PipelineData<\/a> was intended to represent &quot;transient&quot; data from one step to the next one, while OutputDatasetConfig was intended for capturing the final state of a dataset (and hence why you see features like lineage, ADLS support, etc). PipelineData always outputs data in a folder structure like {run_id}{output_name}. OutputDatasetConfig allows to decouple the data from the run and hence it allows you to control where to land the data (although by default it will produce similar folder structure). The OutputDatasetConfig allows even to register the output as a Dataset, where getting rid of such folder structure makes sense. From the docs itself: &quot;Represent how to copy the output of a run and be promoted as a FileDataset. The OutputFileDatasetConfig allows you to specify how you want a particular local path on the compute target to be uploaded to the specified destination&quot;.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-pipeline-batch-scoring-classification#create-dataset-objects\" rel=\"nofollow noreferrer\">OutFileDatasetConfig<\/a> is a control plane concept to pass data between pipeline steps.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1626667519373,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1626668067887,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68422680",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58040933,
        "Question_title":"Error in connecting Azure SQL database from Azure Machine Learning Service using python",
        "Question_body":"<p>I am trying to connect <strong>Azure SQL Database<\/strong> from <strong>Azure Machine Learning service<\/strong>, but I got the below error.<\/p>\n\n<p><strong>Please check Error: -<\/strong><\/p>\n\n<pre><code>**('IM002', '[IM002] [unixODBC][Driver Manager]Data source name not found and no default driver specified (0) (SQLDriverConnect)')**\n<\/code><\/pre>\n\n<p>Please Check the below code that I have used for database connection: -<\/p>\n\n<pre><code>import pyodbc\n\nclass DbConnect:\n    # This class is used for azure database connection using pyodbc\n    def __init__(self):\n        try:\n            self.sql_db = pyodbc.connect(SERVER=&lt;servername&gt;;PORT=1433;DATABASE=&lt;databasename&gt;;UID=&lt;username&gt;;PWD=&lt;password&gt;')\n\n            get_name_query = \"select name from contacts\"\n            names = self.sql_db.execute(get_name_query)\n            for name in names:\n                print(name)\n\n        except Exception as e:\n            print(\"Error in azure sql server database connection : \", e)\n            sys.exit()\n\nif __name__ == \"__main__\":\n    class_obj = DbConnect()\n<\/code><\/pre>\n\n<p>Is there any way to solve the above error? Please let me know if there is any way.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1569073789150,
        "Question_score":1,
        "Question_tags":"python|sql|azure|azure-sql-database|azure-machine-learning-service",
        "Question_view_count":1013,
        "Owner_creation_time":1554466050937,
        "Owner_last_access_time":1578409023530,
        "Owner_location":null,
        "Owner_reputation":219,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I'd consider using <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-dataprep\/azureml.dataprep?view=azure-dataprep-py\" rel=\"nofollow noreferrer\"><code>azureml.dataprep<\/code><\/a> over pyodbc for this task (the API may change, but this worked last time I tried):<\/p>\n\n<pre><code>import azureml.dataprep as dprep\n\nds = dprep.MSSQLDataSource(server_name=&lt;server-name,port&gt;,\n                           database_name=&lt;database-name&gt;,\n                           user_name=&lt;username&gt;,\n                           password=&lt;password&gt;)\n<\/code><\/pre>\n\n<p>You should then be able to collect the result of an SQL query in pandas e.g. via<\/p>\n\n<pre><code>dataflow = dprep.read_sql(ds, \"SELECT top 100 * FROM [dbo].[MYTABLE]\")\ndataflow.to_pandas_dataframe()\n<\/code><\/pre>",
        "Answer_comment_count":3,
        "Answer_creation_time":1569108921710,
        "Answer_score":1,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1569146709727,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58040933",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58500807,
        "Question_title":"Run failed: User program failed with ModuleNotFoundError: No module named 'amlrun' in Azure ML Experiment",
        "Question_body":"<p>I'm using VS Code to submit a Machine Learning experiment in Azure Portal. When running the experiment I'm obtaining the following error:<\/p>\n\n<p>Run failed: User program failed with ModuleNotFoundError: No module named 'amlrun'<\/p>\n\n<p>This is the code structure:<\/p>\n\n<p>.vscode (json configuration file)<\/p>\n\n<p>aml_config<\/p>\n\n<p>scripts<\/p>\n\n<p>----- amlrun.py (a script with some functions)<\/p>\n\n<p>----- model_training.py (a script creating and saving the model)<\/p>\n\n<p>This is the configuration file:<\/p>\n\n<pre><code>{\n    \"script\": \"model_training.py\",\n    \"framework\": \"Python\",\n    \"communicator\": \"None\",\n    \"target\": \"testazure\",\n    \"environment\": {\n        \"python\": {\n            \"userManagedDependencies\": false,\n            \"condaDependencies\": {\n                \"dependencies\": [\n                    \"python=3.6.2\",\n                    \"scikit-learn\",\n                    \"numpy\",\n                    \"pandas\",\n                    {\n                        \"pip\": [\n                            \"azureml-defaults\"\n                        ]\n                    }\n                ]\n            }\n        },\n        \"docker\": {\n            \"baseImage\": \"mcr.microsoft.com\/azureml\/base:0.2.4\",\n            \"enabled\": true,\n            \"baseImageRegistry\": {\n                \"address\": null,\n                \"username\": null,\n                \"password\": null\n            }\n        }\n    },\n    \"history\": {\n        \"outputCollection\": true,\n        \"snapshotProject\": false,\n        \"directoriesToWatch\": [\n            \"logs\"\n        ]\n    }\n}\n<\/code><\/pre>\n\n<p>Am I missing something?\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1571735433200,
        "Question_score":2,
        "Question_tags":"python|azure|azure-machine-learning-service",
        "Question_view_count":901,
        "Owner_creation_time":1547138031703,
        "Owner_last_access_time":1607969329513,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When your training script is running in azure, it's not able to find all your local imports i.e. <code>amlrun.py<\/code> script. <\/p>\n\n<p>The submitted training job to azure builds a docker image with your files first and runs the experiment; but in this case the extension hasn't included <code>amlrun.py<\/code>. <\/p>\n\n<p>This is probably because when you have submit the training job with the extension, the visual studio code window opened is not pointing to be in <code>scripts<\/code> folder.<\/p>\n\n<p>Taken from one of the replies to a <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/24032\" rel=\"nofollow noreferrer\">previously raised github issue<\/a>:<\/p>\n\n<blockquote>\n  <p>The extension currently requires the script you are working on to be\n  in the folder that is open in VS Code and not in a sub-directory.<\/p>\n<\/blockquote>\n\n<hr>\n\n<p>To fix this you can do <strong>either<\/strong> of the following:<\/p>\n\n<ol>\n<li><p>You would need to re-open Visual Studio Code in <code>scripts<\/code> folder instead of parent directory.<\/p><\/li>\n<li><p>Move all files in <code>script<\/code> directory to be in it's parent directory.<\/p><\/li>\n<\/ol>\n\n<hr>\n\n<p>If you're looking for more flexible way to submit training jobs and managing aml - you can use the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/intro?view=azure-ml-py\" rel=\"nofollow noreferrer\">azure machine learning sdk<\/a> for python.<\/p>\n\n<p>Some examples of using the SDK to manage expirements can be found in the links below:<\/p>\n\n<ol>\n<li><p><a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/service\/tutorial-train-models-with-aml.md\" rel=\"nofollow noreferrer\">Scikit Learn Model Training Docs<\/a> <\/p><\/li>\n<li><p><a href=\"https:\/\/github.com\/rithinch\/heartfulness-similar-content-service\" rel=\"nofollow noreferrer\">Basic Pytorch Model Training and Deployment Example Repo<\/a><\/p><\/li>\n<\/ol>",
        "Answer_comment_count":2,
        "Answer_creation_time":1571837118727,
        "Answer_score":1,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":1571838372440,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58500807",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60523435,
        "Question_title":"How do I version control Azure ML workspaces with custom environments and pipelines?",
        "Question_body":"<p>I'm trying to figure out how viable Azure ML in production; I would like to accomplish the following:<\/p>\n\n<ol>\n<li>Specify <em>custom environments<\/em> for my pipelines using a <em>pip file<\/em> and use them in a pipeline<\/li>\n<li><em>Declaratively<\/em> specify my workspace, environments and pipelines in an <em>Azure DevOps repo<\/em><\/li>\n<li><em>Reproducibly<\/em> deploy my Azure ML workspace to my subscription using an <em>Azure DevOps pipeline<\/em><\/li>\n<\/ol>\n\n<p>I found an <a href=\"https:\/\/stackoverflow.com\/questions\/60506398\/how-do-i-use-an-environment-in-an-ml-azure-pipeline\">explanation of how to specify environments using notebooks<\/a> but this seems ill-suited for the second and third requirements I have.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1583316022363,
        "Question_score":1,
        "Question_tags":"azure|azure-devops|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":659,
        "Owner_creation_time":1317398821727,
        "Owner_last_access_time":1660507764847,
        "Owner_location":null,
        "Owner_reputation":1263,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Question_last_edit_time":1583348292107,
        "Answer_body":"<p>Currently, we have a python script, <code>pipeline.py<\/code> that uses the <code>azureml-sdk<\/code>to create, register and run all of our ML artifacts (envs, pipelines, models). We call this script in our Azure DevOps CI pipeline with a Python Script task after building the right pip env from the requirements file in our repo.<\/p>\n\n<p>However, it is worth noting there is YAML support for ML artifact definition. Though I don't know if the existing support will cover all of your bases (though that is the plan).<\/p>\n\n<p>Here's some great docs from MSFT to get you started:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/microsoft\/MLOpsPython\" rel=\"nofollow noreferrer\">GitHub Template repo of an end-to-end example of ML pipeline + deployment<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments\" rel=\"nofollow noreferrer\">How to define\/create an environment (using Pip or Conda) and use it in a remote compute context<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/targets\/azure-machine-learning?context=azure%2Fmachine-learning%2Fservice%2Fcontext%2Fml-context&amp;view=azure-devops&amp;tabs=yaml\" rel=\"nofollow noreferrer\">Azure Pipelines guidance on CI\/CD for ML Service<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-pipeline-yaml\" rel=\"nofollow noreferrer\">Defining ML pipelines in YAML<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0,
        "Answer_creation_time":1583340584893,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1583340824232,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60523435",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58323029,
        "Question_title":"Azure ML deploy locally: tarfile.ReadError: file could not be opened successfully",
        "Question_body":"<p>I am trying to deploy(local) using this line:<\/p>\n\n<pre><code>local_service = Model.deploy(ws, \"test\", [model], inference_config, deployment_config)\n<\/code><\/pre>\n\n<p>Then I get this output in the terminal:<\/p>\n\n<pre><code>tarfile.ReadError: file could not be opened successfully\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WXbPe.png\" rel=\"nofollow noreferrer\">Screenshot of the output<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1570710941163,
        "Question_score":1,
        "Question_tags":"python|deployment|azure-machine-learning-service",
        "Question_view_count":77,
        "Owner_creation_time":1570704481987,
        "Owner_last_access_time":1594546331983,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There was a bug with the retry logic when files were being uploaded. That bug has since been fixed, so updating your SDK should fix the issue.<\/p>\n\n<p>Similar post: <a href=\"https:\/\/stackoverflow.com\/questions\/57854136\/registering-and-downloading-a-fasttext-bin-model-fails-with-azure-machine-learn\">Registering and downloading a fastText .bin model fails with Azure Machine Learning Service<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1570725963837,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58323029",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":69036277,
        "Question_title":"Run independent `PythonScriptStep` steps in parallel",
        "Question_body":"<p>In my pipeline multiple steps are independent and so I would like them to run in parallel based on input dependencies.<\/p>\n<p>As the compute I use has multiple nodes I would have expected this to be the default.<\/p>\n<p>For example:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Iye85.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Iye85.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>All 3 upper steps should run in parallel, then both <code>finetune<\/code> steps in parallel as soon as their inputs are satisfied and the same for <code>rgb_test<\/code>.<\/p>\n<p>Currently only 1 step runs at a time, the other are <code>Queued<\/code>.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1630612166280,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":156,
        "Owner_creation_time":1327588060553,
        "Owner_last_access_time":1664036801580,
        "Owner_location":null,
        "Owner_reputation":802,
        "Owner_up_votes":288,
        "Owner_down_votes":0,
        "Owner_views":91,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It ended up being because of vCPU quota.<\/p>\n<p>After increasing the quota, parallel tasks can run at the same time as expected.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1632769372630,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69036277",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":33229576,
        "Question_title":"Arduino Uno - WebService (AzureML)",
        "Question_body":"<p>I'd like to connect to an AzureML Web Service. I have looked into the POST Method on the Arduino Homepage and also here <a href=\"https:\/\/iotguys.wordpress.com\/2014\/12\/25\/communicating-with-microsoft-azure-eventhub-using-arduino\/\" rel=\"nofollow\">https:\/\/iotguys.wordpress.com\/2014\/12\/25\/communicating-with-microsoft-azure-eventhub-using-arduino\/<\/a><\/p>\n\n<p>Here is my Setup method:<\/p>\n\n<pre><code>    void setup()\n    {\n      Serial.begin(9600);\n      while (!Serial) {\n      ; \/\/ wait for serial port to connect.\n      }\n\n     Serial.println(\"ethernet\");\n\n     if (Ethernet.begin(mac) == 0) {\n       Serial.println(\"ethernet failed\");\n       for (;;) ;\n     }\n    \/\/ give the Ethernet shield a second to initialize:\n    delay(1000);\n }\n<\/code><\/pre>\n\n<p>The Post Method is based on this: <a href=\"http:\/\/playground.arduino.cc\/Code\/WebClient\" rel=\"nofollow\">http:\/\/playground.arduino.cc\/Code\/WebClient<\/a><\/p>\n\n<p>I just added <code>sprintf(outBuf, \"Authorization: Bearer %s\\r\\n\", api_key);<\/code> to the header, with <code>char* api_key = \"the ML Web Service API KEY\"<\/code><\/p>\n\n<p>Also, unlike specified in the WebClient I use the whole WebService URI as url and do not specify a page name.<\/p>\n\n<p>This doesn't work.<\/p>\n\n<p>The Network to which I am connecting has Internet Access.<\/p>\n\n<p>What am I doing wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1445322870153,
        "Question_score":0,
        "Question_tags":"web-services|azure|arduino-uno|azure-machine-learning-studio",
        "Question_view_count":154,
        "Owner_creation_time":1369151239453,
        "Owner_last_access_time":1653069662220,
        "Owner_location":"Germany",
        "Owner_reputation":516,
        "Owner_up_votes":5,
        "Owner_down_votes":1,
        "Owner_views":57,
        "Question_last_edit_time":1459290344956,
        "Answer_body":"<p>Machine Learning Studio services that you create needs to receive requests from a device that has SSL capabilities to perform HTTPS requests. AFAIK, Arduino doesn't support SSL capabilities.<\/p>\n\n<p>One usual scenario is to attach the Arduino to a third device like Raspberry Pi 2 etc to use it as a gateway and do the call from the Pi itself.<\/p>\n\n<p>Here's a sample <a href=\"https:\/\/github.com\/Azure\/connectthedots\/blob\/master\/GettingStarted.md\" rel=\"nofollow\">project<\/a> from Microsoft Open Technologies team that utilizes Arduino Uno, Raspberry pi and Azure stuff.<\/p>\n\n<p>Hope this helps!<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1450337261483,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1450346645647,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/33229576",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72518344,
        "Question_title":"Logging and Fetching Run Parameters in AzureML",
        "Question_body":"<p>I am able to log and fetch metrics to AzureML using Run.log, however, I need a way to also log run parameters, like Learning Rate, or Momentum. I can't seem to find anything in the AzureML Python SDK documentation to achieve this. However, if I use MLflow's mlflow.log_param, I am able to log parameters, and they even nicely show up on the AzureML Studio Dashboard (bottom right of the image):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Again, I am able to fetch this using MLflow's get_params() function, but I can't find a way to do this using just AzureML's Python SDK. Is there a way to do this directly using <code>azureml<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654521747343,
        "Question_score":0,
        "Question_tags":"azure|mlflow|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":73,
        "Owner_creation_time":1554497484963,
        "Owner_last_access_time":1664005308093,
        "Owner_location":"Hyderabad, Telangana, India",
        "Owner_reputation":438,
        "Owner_up_votes":15,
        "Owner_down_votes":2,
        "Owner_views":120,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The retrieving of log run parameters like <strong>Learning Rate, or Momentum<\/strong> is not possible with <strong>AzureML<\/strong> alone. Because it was tied with <strong>MLFlow<\/strong> and <strong>azureml-core<\/strong>. without those two involvements, we cannot retrieve the log run parameters.<\/p>\n<pre><code>pip install azureml-core mlflow azureml-mlflow\n<\/code><\/pre>\n<p>Need to install these three for getting run parameters. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics\" rel=\"nofollow noreferrer\">Link<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1656998954310,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72518344",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58060865,
        "Question_title":"Deploy Azure Machine Learning models using Custom Docker Image on Azure Container Regisrty",
        "Question_body":"<p>I want to train <strong>Azure Machine Learning model<\/strong> on azure using <strong>Azure Machine Learning Service.<\/strong> But I want to use the <strong>custom Docker image<\/strong> for deploying the model on azure. I am not able to understand how to deploy Machine Learning models using Custom Docker Image.<\/p>\n\n<p>Please share me if there is any tutorial or blog about the deploy ml models using a custom image.<\/p>\n\n<p>Please check the below Docker file commands:-<\/p>\n\n<pre><code># Set locale\nRUN apt-get update\nRUN apt-get install locales\nRUN locale-gen en_US.UTF-8\nRUN update-locale LANG=en_US.UTF-8\n\n# Install MS SQL v13 driver for PyOdbc\nRUN apt-get install -y curl\nRUN apt-get install apt-transport-https\nRUN curl https:\/\/packages.microsoft.com\/keys\/microsoft.asc | apt-key add - \nRUN curl https:\/\/packages.microsoft.com\/config\/ubuntu\/16.04\/prod.list &gt; \/etc\/apt\/sources.list.d\/mssql-release.list\nRUN exit\nRUN apt-get update\n\nRUN ACCEPT_EULA=Y apt-get install -y msodbcsql\nRUN apt-get install -y unixodbc-dev\n<\/code><\/pre>\n\n<p>I want to use the <strong>Azure Container Registry<\/strong> for push the docker image and use the <strong>Custom Docker Image.<\/strong> Please let me know if there is any way.<\/p>\n\n<p><strong>Is there any way to Deploy Azure ML Models using Custom docker images?<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1569236101160,
        "Question_score":0,
        "Question_tags":"python|python-3.x|azure|docker|azure-machine-learning-service",
        "Question_view_count":864,
        "Owner_creation_time":1554466050937,
        "Owner_last_access_time":1578409023530,
        "Owner_location":null,
        "Owner_reputation":219,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Question_last_edit_time":1569244969489,
        "Answer_body":"<p>You can do following:<\/p>\n\n<ol>\n<li>Create an [Environment][1] with the coordinates of your custom Docker image specified in Docker section.<\/li>\n<li>Create [InferenceConfig][2] with that Environment as argument, and use it when deploying the model.<\/li>\n<\/ol>\n\n<p>For example, assuming you have a model already and eliding other arguments:<\/p>\n\n<pre><code>from azureml.core.environment import Environment\nfrom azureml.core.model import InferenceConfig\n\nenv = Environment(name=\"myenv\")\nenv.docker.base_image = \"mybaseimage\"\nenv.docker.base_image_registry.address = \"ip-address\"\nenv.docker.base_image_registry.username = \"my-username\"\nenv.docker.base_image_registry.password = \"my-password\"\n\nic = InferenceConfig(\u2026,environment = env)\nmodel.deploy(\u2026,inference_config = ic)\n<\/code><\/pre>\n\n<pre><code>\n  [1]: https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment.environment?view=azure-ml-py\n  [2]: https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py\n<\/code><\/pre>",
        "Answer_comment_count":5,
        "Answer_creation_time":1569245799797,
        "Answer_score":1,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1569412440892,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58060865",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":56240481,
        "Question_title":"Can I import data from On-Premises SQL Server Database to Azure Machine Learning virtual machine?",
        "Question_body":"<p>On the limited Azure Machine Learning Studio, one can import data from an On-Premises SQL Server Database.\nWhat about the ability to do the exact same thing on a python jupyter notebook on a virtual machine from the Azure Machine Learning Services workspace ?<\/p>\n\n<p>It does not seem possible from what I've found in the documentation.\nData sources would be limited in Azure ML Services : \"Currently, the list of supported Azure storage services that can be registered as datastores are Azure Blob Container, Azure File Share, Azure Data Lake, Azure Data Lake Gen2, Azure SQL Database, Azure PostgreSQL, and Databricks File System\"<\/p>\n\n<p>Thank you in advance for your assistance<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1558448804447,
        "Question_score":0,
        "Question_tags":"python|sql|azure|jupyter-notebook|azure-machine-learning-service",
        "Question_view_count":1147,
        "Owner_creation_time":1558447987353,
        "Owner_last_access_time":1650446213527,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As of today, <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-load-data#load-sql-data\" rel=\"nofollow noreferrer\">you can load SQL data, but only a MS SQL Server source (also on-premise) is supported<\/a>.<\/p>\n\n<p>Using <code>azureml.dataprep<\/code>, code would read along the lines of<\/p>\n\n<pre><code>import azureml.dataprep as dprep\n\nsecret = dprep.register_secret(value=\"[SECRET-PASSWORD]\", id=\"[SECRET-ID]\")\n\nds = dprep.MSSQLDataSource(server_name=\"[SERVER-NAME]\",\n                           database_name=\"[DATABASE-NAME]\",\n                           user_name=\"[DATABASE-USERNAME]\",\n                           password=secret)\n\ndflow = dprep.read_sql(ds, \"SELECT top 100 * FROM [YourDB].[ATable]\")\n# print first records\ndflow.head(5)\n<\/code><\/pre>\n\n<p>As far as I understand the APIs are under heavy development and <code>azureml.dataprep<\/code> may be soon superseded by functionality provided by the <a href=\"https:\/\/aka.ms\/azureml\/concepts\/datasets\" rel=\"nofollow noreferrer\">Dataset class<\/a>.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1558479194140,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56240481",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":66348756,
        "Question_title":"Input of type 'Unknown' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset, azureml.pipeline.core",
        "Question_body":"<p>According to this documentation:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py<\/a><\/p>\n<p>training_data can be either a dataframe or a dataset.<\/p>\n<p>However when I use a dataframe I get this error:<\/p>\n<pre><code>\nConfigException: ConfigException:\n    Message: Input of type 'Unknown' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset, azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset]\n    InnerException: None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;Input of type 'Unknown' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset, azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset]&quot;,\n        &quot;details_uri&quot;: &quot;https:\/\/aka.ms\/AutoMLConfig&quot;,\n        &quot;target&quot;: &quot;training_data&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;BadArgument&quot;,\n            &quot;inner_error&quot;: {\n                &quot;code&quot;: &quot;ArgumentInvalid&quot;,\n                &quot;inner_error&quot;: {\n                    &quot;code&quot;: &quot;InvalidInputDatatype&quot;\n                }\n            }\n        }\n    }\n}\n<\/code><\/pre>\n<p>My code is really simple:<\/p>\n<pre><code>\nclient = CosmosClient(HOST, MASTER_KEY)\ndatabase = client.get_database_client(database=DATABASE_ID)\ncontainer = database.get_container_client(CONTAINER_ID)\n\nitem_list = list(container.read_all_items(max_item_count=10))\ndf = pd.DataFrame(item_list)\n\nfrom azureml.core.workspace import Workspace\nws = Workspace.from_config()\n\nfrom azureml.automl.core.forecasting_parameters import ForecastingParameters\n\nforecasting_parameters = ForecastingParameters(time_column_name='EventEnqueuedUtcTime', \n                                               forecast_horizon=50,\n                                               time_series_id_column_names=[&quot;eui&quot;],\n                                               freq='H',\n                                               target_lags='auto',\n                                               target_rolling_window_size=10)\n\nfrom azureml.core.workspace import Workspace\nfrom azureml.core.experiment import Experiment\nfrom azureml.train.automl import AutoMLConfig\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nimport logging\n\namlcompute_cluster_name = &quot;computecluster&quot;\ncompute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\nexperiment_name = 'iot-forecast'\n\nexperiment = Experiment(ws, experiment_name)\n\nautoml_config = AutoMLConfig(task='forecasting',\n                             primary_metric='normalized_root_mean_squared_error',\n                             experiment_timeout_minutes=100,\n                             enable_early_stopping=True,\n                             training_data=df,\n                             compute_target = compute_target,\n                             label_column_name='TempC_DS',\n                             n_cross_validations=5,\n                             enable_ensembling=False,\n                             verbosity=logging.INFO,\n                             forecasting_parameters=forecasting_parameters)\n\nremote_run = experiment.submit(automl_config, show_output=True)\n<\/code><\/pre>\n<p>what am I missing here?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1614161544217,
        "Question_score":1,
        "Question_tags":"python|python-3.x|pandas|machine-learning|azure-machine-learning-studio",
        "Question_view_count":315,
        "Owner_creation_time":1302030303093,
        "Owner_last_access_time":1663332147473,
        "Owner_location":"Brussels, B\u00e9lgica",
        "Owner_reputation":30340,
        "Owner_up_votes":1667,
        "Owner_down_votes":79,
        "Owner_views":2937,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Looks like you are trying to run the experiment remotely, AFAIK and as per the doc <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train#data-source-and-format?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">here<\/a> :<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/CXYyE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CXYyE.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>You could refer this article to understand creating <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-datasets?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">Azure ML TabularDataset<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1614750517020,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66348756",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":63811726,
        "Question_title":"How to execute python commands from a conda .yaml specification file?",
        "Question_body":"<p>I am trying to list conda dependencies using a .yaml file for an AzureML <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#define-an-inference-configuration\" rel=\"nofollow noreferrer\">environment<\/a>. I do not want to use a custom docker image just for a few variations. I wonder if there is a way to instruct the build to run python commands using the .yaml file. Here are excerpts of what I have tried as of now:<\/p>\n<pre><code>name: classifer_environment\ndependencies:\n- python=3.6.2\n\n- pip:\n  - azureml-defaults&gt;=1.0.45\n  - nltk==3.4.5\n  - spacy\n\n- command: \n  - bash -c &quot;python -m nltk.downloader stopwords&quot;\n  - bash -c &quot;python -m spacy download en_core_web_sm&quot;\n<\/code><\/pre>\n<p>I also tried this:<\/p>\n<pre><code>name: classifer_environment\ndependencies:\n- python=3.6.2\n\n- pip:\n  - azureml-defaults&gt;=1.0.45\n  - nltk==3.4.5\n  - spacy\n\n- python: \n  - nltk.downloader stopwords\n  - spacy download en_core_web_sm\n<\/code><\/pre>\n<p>I do not have much clarity about yaml specifications. Both the specifications fail with the following messages respectively in the build logs: <br>\n&quot;Unable to install package for command.&quot; <br>\n&quot;Unable to install package for python.&quot;<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1599654539997,
        "Question_score":0,
        "Question_tags":"python-3.x|yaml|conda|azure-machine-learning-service",
        "Question_view_count":408,
        "Owner_creation_time":1555424023330,
        "Owner_last_access_time":1663862330683,
        "Owner_location":"Bhubaneswar, Odisha, India",
        "Owner_reputation":328,
        "Owner_up_votes":31,
        "Owner_down_votes":4,
        "Owner_views":53,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This might be a neat feature to have, but for now it's not a thing - at least not directly in the YAML like this.<\/p>\n<p>Instead, the unit of computation in Conda is the <em>package<\/em>. That is, if you need to run additional scripts or commands at environment creation, it can be achieved by building a custom package and including this package in the YAML as a dependency. The package itself could be pretty much empty, but whatever code one needs to run would be included via <a href=\"https:\/\/docs.conda.io\/projects\/conda-build\/en\/latest\/resources\/link-scripts.html\" rel=\"nofollow noreferrer\">some installation scripts<\/a>.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1599673418303,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63811726",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":52003180,
        "Question_title":"How to understand output from a Multiclass Neural Network",
        "Question_body":"<p>Built a flow in Azure ML using a Neural network Multiclass module (for settings see picture). \n <a href=\"https:\/\/i.stack.imgur.com\/6xTYY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6xTYY.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Some more info about the Multiclass:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/lLUVC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lLUVC.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The data flow is simple, split of 80\/20. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/k72ZX.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/k72ZX.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Preparation of the data is made before it goes into Azure. Data looks like this: <a href=\"https:\/\/i.stack.imgur.com\/SoSUa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/SoSUa.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>My problem comes when I want to make sense of the output and if possible transform\/calculate the output to probabilities. Output looks like this: <a href=\"https:\/\/i.stack.imgur.com\/kBZOM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kBZOM.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>My question: If scored probabilities output for my model is 0.6 and scored labels = 1, how sure is the model of the scored labels 1? And how sure can I be that actual outcome will be a 1?<\/p>\n\n<p>Can I safely assume that a scored probabilities of 0.80 = 80% chance of outcome? Or what type of outcomes should I watch out for?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1535108970810,
        "Question_score":0,
        "Question_tags":"machine-learning|neural-network|azure-machine-learning-studio|multiclass-classification",
        "Question_view_count":263,
        "Owner_creation_time":1466013021617,
        "Owner_last_access_time":1550758296713,
        "Owner_location":"Malm\u00f6, Sverige",
        "Owner_reputation":103,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Question_last_edit_time":1535126329423,
        "Answer_body":"<p>To start with, your are in a <em>binary<\/em> classification setting, not in a multi-class one (we normally use this term when number of classes > 2).<\/p>\n\n<blockquote>\n  <p>If scored probabilities output for my model is 0.6 and scored labels = 1, how sure is the model of the scored labels 1? <\/p>\n<\/blockquote>\n\n<p>In <em>practice<\/em>, the scored probabilities are routinely interpreted as the <em>confidence<\/em> of the model; so, in this example, we would say that your model has 60% confidence that the particular sample belongs to class 1 (and, complementary, 40% confidence that it belongs to class 0).<\/p>\n\n<blockquote>\n  <p>And how sure can I be that actual outcome will be a 1?<\/p>\n<\/blockquote>\n\n<p>If you don't have any alternate means of computing such outcomes yourself (e.g. a different model), I cannot see how this question is different from your previous one.<\/p>\n\n<blockquote>\n  <p>Can I safely assume that a scored probabilities of 0.80 = 80% chance of outcome? <\/p>\n<\/blockquote>\n\n<p>This is the kind of statement that would drive a professional statistician mad; nevertheless, the clarifications above regarding the confidence should be enough for your purposes (they are enough indeed for ML practitioners).<\/p>\n\n<p>My answer in <a href=\"https:\/\/stackoverflow.com\/questions\/51367755\/predict-classes-or-class-probabilities\/51423325#51423325\">Predict classes or class probabilities?<\/a> should also be helpful.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1535111519160,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1535111877860,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52003180",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":36450108,
        "Question_title":"Azure Recommendations API's Parameter",
        "Question_body":"<p>I would like to make a recommendation model using Recommendations API on Azure MS Cognitive Services. I can't understand three API's parameters below for \"Create\/Trigger a build.\" What do these parameters mean?<\/p>\n\n<p><a href=\"https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/Recommendations.V4.0\/operations\/56f30d77eda5650db055a3d0\" rel=\"nofollow\">https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/Recommendations.V4.0\/operations\/56f30d77eda5650db055a3d0<\/a><\/p>\n\n<blockquote>\n  <p>EnableModelingInsights<br> Allows you to compute metrics on the\n  recommendation model. <br> Valid Values: True\/False<\/p>\n  \n  <p>AllowColdItemPlacement<br> Indicates if the recommendation should also\n  push cold items via feature similarity. <br> Valid Values: True\/False<\/p>\n  \n  <p>ReasoningFeatureList<br> Comma-separated list of feature names to be\n  used for reasoning sentences (e.g. recommendation explanations).<br>\n  Valid Values: Feature names, up to 512 chars<\/p>\n<\/blockquote>\n\n<p>Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1459942893017,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":279,
        "Owner_creation_time":1459941581603,
        "Owner_last_access_time":1469314422527,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>That page is missing references to content mentioned at other locations.  See this page for a more complete guide...<\/p>\n\n<p><a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-recommendation-api-documentation\/\" rel=\"nofollow\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-recommendation-api-documentation\/<\/a><\/p>\n\n<p>It describes Cold Items in the Rank Build section in the document as...<\/p>\n\n<p>Features can enhance the recommendation model, but to do so requires the use of meaningful features. For this purpose a new build was introduced - a rank build. This build will rank the usefulness of features. A meaningful feature is a feature with a rank score of 2 and up. After understanding which of the features are meaningful, trigger a recommendation build with the list (or sublist) of meaningful features. It is possible to use these feature for the enhancement of both warm items and cold items. In order to use them for warm items, the UseFeatureInModel build parameter should be set up. In order to use features for cold items, the AllowColdItemPlacement build parameter should be enabled. Note: It is not possible to enable AllowColdItemPlacement without enabling UseFeatureInModel.<\/p>\n\n<p>It also describes the ReasoningFeatureList in the Recommendation Reasoning section as...<\/p>\n\n<p>Recommendation reasoning is another aspect of feature usage. Indeed, the Azure Machine Learning Recommendations engine can use features to provide recommendation explanations (a.k.a. reasoning), leading to more confidence in the recommended item from the recommendation consumer. To enable reasoning, the AllowFeatureCorrelation and ReasoningFeatureList parameters should be setup prior to requesting a recommendation build.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1460495281970,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36450108",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":37418265,
        "Question_title":"Azure Machine Learning using Javascript Ajax call",
        "Question_body":"<p>I wanted to know if there is a way to call the Azure Machine Learning webservice using JavaScript Ajax.<\/p>\n\n<p>The Azure ML gives sample code for C#, Python and R.<\/p>\n\n<p>I did try out to call the webservice using JQuery Ajax but it returns a failure.<\/p>\n\n<p>I am able to call the same service using a python script.<\/p>\n\n<p>Here is my Ajax code : <\/p>\n\n<pre><code>  $.ajax({\n        url: webserviceurl,\n        type: \"POST\",           \n        data: sampleData,            \n        dataType:'jsonp',                        \n        headers: {\n        \"Content-Type\":\"application\/json\",            \n        \"Authorization\":\"Bearer \" + apiKey                       \n        },\n        success: function (data) {\n          console.log('Success');\n        },\n        error: function (data) {\n           console.log('Failure ' +  data.statusText + \" \" + data.status);\n        },\n  });\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_time":1464104729493,
        "Question_score":2,
        "Question_tags":"javascript|ajax|azure|azure-machine-learning-studio",
        "Question_view_count":1607,
        "Owner_creation_time":1460664823627,
        "Owner_last_access_time":1504814384707,
        "Owner_location":null,
        "Owner_reputation":81,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1526047792276,
        "Answer_body":"<p>Well after a lot of RnD, I was able to finally call Azure ML using some workarounds.<\/p>\n\n<p>Wrapping Azure ML webservice on Azure API is one option.<\/p>\n\n<p>But, what I did was that I created a python webservice which calls the Azure webservice.<\/p>\n\n<p>So now my HTML App calls the python webservice which calls Azure ML and returns data to the HTML App.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1464718210400,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37418265",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":31130629,
        "Question_title":"Azure ML web service times out",
        "Question_body":"<p>I have created a simple experiment in Azure ML and trigger it with an http client. In Azure ML workspace, everything works ok when executed. However, the experiment times out and fails when I trigger the experiment using an http client. Setting a timeout value for the http client does not seem to work.<\/p>\n\n<p>Is there any way we can set this timeout value so that the experiment does not fail?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1435643857760,
        "Question_score":2,
        "Question_tags":"c#|azure|azure-machine-learning-studio",
        "Question_view_count":1681,
        "Owner_creation_time":1313488868533,
        "Owner_last_access_time":1663152489797,
        "Owner_location":null,
        "Owner_reputation":209,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Looks like it isn't possible to set this timeout based on <a href=\"https:\/\/feedback.azure.com\/forums\/257792-machine-learning\/suggestions\/6472562-configurable-timeout-for-experiments-and-web-servi\" rel=\"nofollow noreferrer\">a feature request that is still marked as \"planned\" as of 4\/1\/2018<\/a>.<\/p>\n\n<p>The recommendation from <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/sqlserver\/en-US\/cb4ee96d-c2ca-4c65-b02f-0ccb26181f7f\/timeout-in-web-service?forum=MachineLearning\" rel=\"nofollow noreferrer\">MSDN forums from 2017<\/a> is to use the Batch Execution Service, which starts the machine learning experiment and then asynchronously asks whether it's done.<\/p>\n\n<p>Here's a code snippet from the Azure ML Web Services Management Sample Code (all comments are from their sample code):<\/p>\n\n<pre><code>        using (HttpClient client = new HttpClient())\n        {\n            var request = new BatchExecutionRequest()\n            {\n\n                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt; () {\n                    {\n                        \"output\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = storageConnectionString,\n                            RelativeLocation = string.Format(\"{0}\/outputresults.file_extension\", StorageContainerName) \/*Replace this with the location you would like to use for your output file, and valid file extension (usually .csv for scoring results, or .ilearner for trained models)*\/\n                        }\n                    },\n                },    \n\n                GlobalParameters = new Dictionary&lt;string, string&gt;() {\n                }\n            };\n\n            client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", apiKey);\n\n            \/\/ WARNING: The 'await' statement below can result in a deadlock\n            \/\/ if you are calling this code from the UI thread of an ASP.Net application.\n            \/\/ One way to address this would be to call ConfigureAwait(false)\n            \/\/ so that the execution does not attempt to resume on the original context.\n            \/\/ For instance, replace code such as:\n            \/\/      result = await DoSomeTask()\n            \/\/ with the following:\n            \/\/      result = await DoSomeTask().ConfigureAwait(false)\n\n            Console.WriteLine(\"Submitting the job...\");\n\n            \/\/ submit the job\n            var response = await client.PostAsJsonAsync(BaseUrl + \"?api-version=2.0\", request);\n\n            if (!response.IsSuccessStatusCode)\n            {\n                await WriteFailedResponse(response);\n                return;\n            }\n\n            string jobId = await response.Content.ReadAsAsync&lt;string&gt;();\n            Console.WriteLine(string.Format(\"Job ID: {0}\", jobId));\n\n            \/\/ start the job\n            Console.WriteLine(\"Starting the job...\");\n            response = await client.PostAsync(BaseUrl + \"\/\" + jobId + \"\/start?api-version=2.0\", null);\n            if (!response.IsSuccessStatusCode)\n            {\n                await WriteFailedResponse(response);\n                return;\n            }\n\n            string jobLocation = BaseUrl + \"\/\" + jobId + \"?api-version=2.0\";\n            Stopwatch watch = Stopwatch.StartNew();\n            bool done = false;\n            while (!done)\n            {\n                Console.WriteLine(\"Checking the job status...\");\n                response = await client.GetAsync(jobLocation);\n                if (!response.IsSuccessStatusCode)\n                {\n                    await WriteFailedResponse(response);\n                    return;\n                }\n\n                BatchScoreStatus status = await response.Content.ReadAsAsync&lt;BatchScoreStatus&gt;();\n                if (watch.ElapsedMilliseconds &gt; TimeOutInMilliseconds)\n                {\n                    done = true;\n                    Console.WriteLine(string.Format(\"Timed out. Deleting job {0} ...\", jobId));\n                    await client.DeleteAsync(jobLocation);\n                }\n                switch (status.StatusCode) {\n                    case BatchScoreStatusCode.NotStarted:\n                        Console.WriteLine(string.Format(\"Job {0} not yet started...\", jobId));\n                        break;\n                    case BatchScoreStatusCode.Running:\n                        Console.WriteLine(string.Format(\"Job {0} running...\", jobId));\n                        break;\n                    case BatchScoreStatusCode.Failed:\n                        Console.WriteLine(string.Format(\"Job {0} failed!\", jobId));\n                        Console.WriteLine(string.Format(\"Error details: {0}\", status.Details));\n                        done = true;\n                        break;\n                    case BatchScoreStatusCode.Cancelled:\n                        Console.WriteLine(string.Format(\"Job {0} cancelled!\", jobId));\n                        done = true;\n                        break;\n                    case BatchScoreStatusCode.Finished:\n                        done = true;\n                        Console.WriteLine(string.Format(\"Job {0} finished!\", jobId));\n                        ProcessResults(status);\n                        break;\n                }\n\n                if (!done) {\n                    Thread.Sleep(1000); \/\/ Wait one second\n                }\n            }\n        }\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1522603988173,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/31130629",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":65795579,
        "Question_title":"Debugging AML Model Deployment",
        "Question_body":"<p>I have an ML model (trained locally) in python. Previously the model has been deployed to a Windows IIS server and it's working fine.<\/p>\n<p>Now, I am trying to deploy it as a service on Azure container instance (ACI) with 1 core, and 1 GB of memory. I took references from <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python\" rel=\"nofollow noreferrer\">one<\/a> and <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-existing-model\" rel=\"nofollow noreferrer\">two<\/a> Microsoft docs. The docs use SDK for all the steps, but <strong>I am using the GUI feature from the Azure portal<\/strong>.<\/p>\n<p>After registering the model, I created an entry script and a conda environment YAML file (see below), and uploaded both to &quot;Custom deployment asset&quot; (at Deploy model area).<\/p>\n<p>Unfortunately, after hitting deploy, the Deployment state is stuck at Transitioning state. Even after 4 hours, the state remains the same and there were no Deployment logs too, so I am unable to find what I am doing wrong here.<\/p>\n<blockquote>\n<p>NOTE: below is just an excerpt of the entry script<\/p>\n<\/blockquote>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport pickle\nimport re, json\nimport numpy as np\nimport sklearn\n\ndef init():\n    global model \n    global classes\n    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'randomForest50.pkl')\n    model = pickle.load(open(model_path, &quot;rb&quot;))\n\n    classes = lambda x : [&quot;F&quot;, &quot;M&quot;][x]\n\ndef run(data):\n    try:\n        namesList = json.loads(data)[&quot;data&quot;][&quot;names&quot;]\n        pred = list(map(classes, model.predict(preprocessing(namesList))))\n        return str(pred[0])\n    except Exception as e:\n        error = str(e)\n        return error\n<\/code><\/pre>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: gender_prediction\ndependencies:\n- python\n- numpy\n- scikit-learn\n- pip:\n    - pandas\n    - pickle\n    - re\n    - json\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1611073723990,
        "Question_score":1,
        "Question_tags":"machine-learning|yaml|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":106,
        "Owner_creation_time":1582182357613,
        "Owner_last_access_time":1664032157167,
        "Owner_location":null,
        "Owner_reputation":155,
        "Owner_up_votes":112,
        "Owner_down_votes":0,
        "Owner_views":27,
        "Question_last_edit_time":1611079944120,
        "Answer_body":"<p>The issue was in the YAML file. <strong>The dependencies\/libraries in the YAML should be according to conda environment<\/strong>. So, I changed everything accordingly, and it worked.<\/p>\n<p>Modified YAML file:<\/p>\n<pre><code>name: gender_prediction\ndependencies:\n- python=3.7\n- numpy\n- scikit-learn\n- pip:\n    - azureml-defaults\n    - pandas\n    - pickle4\n    - regex\n    - inference-schema[numpy-support]   \n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1612491964670,
        "Answer_score":0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65795579",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":53588040,
        "Question_title":"No module named 'automl' when unpickle auto-trained model",
        "Question_body":"<p>I'm trying to reproduce 2 tutorials below using my own dataset instead of MNIST dataset.\n<a href=\"https:\/\/docs.microsoft.com\/ja-jp\/azure\/machine-learning\/service\/tutorial-auto-train-models\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/ja-jp\/azure\/machine-learning\/service\/tutorial-auto-train-models<\/a>\n<a href=\"https:\/\/docs.microsoft.com\/ja-jp\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/ja-jp\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml<\/a><\/p>\n\n<p>About\n'\/notebooks\/tutorials\/03.auto-train-models.ipynb'\nthere's no problem. I've got 'model.pkl'.<\/p>\n\n<p>However, \n'\/notebooks\/tutorials\/02.deploy-models.ipynb'\nhas an error below in 'Predict test data' cell.\nI guess it's a matter of 'pickle' and 'import'.<\/p>\n\n<p>Tell me solutions, please.<\/p>\n\n<pre><code>ModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-6-11cf888b622f&gt; in &lt;module&gt;\n      2 from sklearn.externals import joblib\n      3 \n----&gt; 4 clf = joblib.load('.\/model.pkl')\n      5 # clf = joblib.load('.\/sklearn_mnist_model.pkl')\n      6 y_hat = clf.predict(X_test)\n\n~\/anaconda3_501\/lib\/python3.6\/site-packages\/sklearn\/externals\/joblib\/numpy_pickle.py in load(filename, mmap_mode)\n    576                     return load_compatibility(fobj)\n    577 \n--&gt; 578                 obj = _unpickle(fobj, filename, mmap_mode)\n    579 \n    580     return obj\n\n~\/anaconda3_501\/lib\/python3.6\/site-packages\/sklearn\/externals\/joblib\/numpy_pickle.py in _unpickle(fobj, filename, mmap_mode)\n    506     obj = None\n    507     try:\n--&gt; 508         obj = unpickler.load()\n    509         if unpickler.compat_mode:\n    510             warnings.warn(\"The file '%s' has been generated with a \"\n\n~\/anaconda3_501\/lib\/python3.6\/pickle.py in load(self)\n   1048                     raise EOFError\n   1049                 assert isinstance(key, bytes_types)\n-&gt; 1050                 dispatch[key[0]](self)\n   1051         except _Stop as stopinst:\n   1052             return stopinst.value\n\n~\/anaconda3_501\/lib\/python3.6\/pickle.py in load_global(self)\n   1336         module = self.readline()[:-1].decode(\"utf-8\")\n   1337         name = self.readline()[:-1].decode(\"utf-8\")\n-&gt; 1338         klass = self.find_class(module, name)\n   1339         self.append(klass)\n   1340     dispatch[GLOBAL[0]] = load_global\n\n~\/anaconda3_501\/lib\/python3.6\/pickle.py in find_class(self, module, name)\n   1386             elif module in _compat_pickle.IMPORT_MAPPING:\n   1387                 module = _compat_pickle.IMPORT_MAPPING[module]\n-&gt; 1388         __import__(module, level=0)\n   1389         if self.proto &gt;= 4:\n   1390             return _getattribute(sys.modules[module], name)[0]\n\nModuleNotFoundError: No module named 'automl'\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1543815453630,
        "Question_score":5,
        "Question_tags":"python|pickle|azure-machine-learning-studio|automl",
        "Question_view_count":4788,
        "Owner_creation_time":1543814686770,
        "Owner_last_access_time":1544423620710,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":null,
        "Answer_body":"<p>you have to include azureml-train-automl package. and you have to do this:<\/p>\n\n<p>import azureml.train.automl<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1543816325650,
        "Answer_score":5,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53588040",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":52603929,
        "Question_title":"AML Studio: Register mutliple gateways on the same server",
        "Question_body":"<p>I am struggling to find a way to register multiple gateways. I have a local instance of my SQL server and have created a gateway to access to it from the AML Studio workspace. It works fine but now I would like to access to the same SQL server instance from another workspace. So the question is: how to register a new gateway without removing the previous one?\nI followed this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/use-data-from-an-on-premises-sql-server\" rel=\"nofollow noreferrer\">documentation<\/a>.\nDoes the following explanation mean that there is no way to do that?<\/p>\n\n<blockquote>\n  <p>You can create and set up multiple gateways in Studio for each workspace. For example, you may have a gateway that you want to connect to your test data sources during development, and a different gateway for your production data sources. Azure Machine Learning gives you the flexibility to set up multiple gateways depending upon your corporate environment. Currently you can\u2019t share a gateway between workspaces and only one gateway can be installed on a single computer.<\/p>\n<\/blockquote>\n\n<p>It is quite limiting as connecting to the same server from multiple workspaces may be sometimes crucial.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1538466090420,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":40,
        "Owner_creation_time":1528790837107,
        "Owner_last_access_time":1660146049397,
        "Owner_location":"Paris, France",
        "Owner_reputation":610,
        "Owner_up_votes":143,
        "Owner_down_votes":0,
        "Owner_views":203,
        "Question_last_edit_time":1538485474347,
        "Answer_body":"<p>Well, finally I have found a way to bypass this limitation. From this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/use-data-from-an-on-premises-sql-server\" rel=\"nofollow noreferrer\">documentation<\/a> I have found that: <\/p>\n\n<blockquote>\n  <p>The IR does not need to be on the same machine as the data source. But staying closer to the data source reduces the time for the gateway to connect to the data source. We recommend that you install the IR on a machine that's different from the one that hosts the on-premises data source so that the gateway and data source don't compete for resources.<\/p>\n<\/blockquote>\n\n<p>So the  logic is pretty simple. You provide access to your local server to another machine on vpn and install your gateway there. Important: I have set up the firewall rules on the server before, to be able to establish the connection remotely.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1538648158570,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52603929",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":40018320,
        "Question_title":"Is it secure to pass the DB query to AzureML as a global parameter?",
        "Question_body":"<p>When using AzureMLBatchExecution activity in Azure Data Factory, is it secure to pass the DB query as a global parameter to the AzureML web service? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1476354083450,
        "Question_score":2,
        "Question_tags":"azure|azure-data-factory|azure-machine-learning-studio",
        "Question_view_count":68,
        "Owner_creation_time":1452608563363,
        "Owner_last_access_time":1562103924250,
        "Owner_location":null,
        "Owner_reputation":105,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When you talk about \"secure\", are you worried about secure transmission between AML and ADF, or secure storage of your DB query information? For the former, all communication between these two services will be done with HTTPS. For the latter, our production storage has its strict access control. Besides, we only log the count of the global parameters and never the values. I believe it's secure to pass your DB query as a global parameter to the AzureML web service.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1476431199480,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40018320",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64137409,
        "Question_title":"How can I create an Azure dataset in Azure ML studio (through the GUI) from a parquet file created with Azure Spark",
        "Question_body":"<p>I'm trying to load files as a dataset in the GUI of Azure ML Studio. These parquet files have been created through Spark.<\/p>\n<p>In my folder, Spark creates files such as &quot;_SUCCESS&quot; or &quot;_committed_8998000&quot;.<\/p>\n<p>Azure ML Studio is not able to read them or ignore them and tells me:<\/p>\n<pre><code>The provided file(s) have invalid byte(s) for the specified file encoding.\n{\n  &quot;message&quot;: &quot; &quot;\n}\n<\/code><\/pre>\n<p>I selected &quot;Ignore unmatched files path&quot; and yet, it still does not work.<\/p>\n<p>If I remove the &quot;_SUCCESS&quot; and other Spark files, it works.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1601468292143,
        "Question_score":2,
        "Question_tags":"azure|apache-spark|parquet|azure-machine-learning-studio",
        "Question_view_count":178,
        "Owner_creation_time":1423640080283,
        "Owner_last_access_time":1663943557963,
        "Owner_location":"Lyon, France",
        "Owner_reputation":457,
        "Owner_up_votes":19,
        "Owner_down_votes":2,
        "Owner_views":125,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Thanks for the feedback. You can use globing in path. e.g. path = '**\/*.parquet' to select only the parquet files<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1601483944070,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64137409",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62780977,
        "Question_title":"Threshold for allowed amount of failed Hyperdrive runs",
        "Question_body":"<p>Because &quot;reasons&quot;, we know that when we use <code>azureml-sdk<\/code>'s <code>HyperDriveStep<\/code> we expect a number of <code>HyperDrive<\/code> runs to fail -- normally around 20%. How can we handle this without failing the entire <code>HyperDriveStep<\/code> (and then all downstream steps)? Below is an example of the pipeline.<\/p>\n<p>I thought there would be an <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.hyperdrive.hyperdriverunconfig?view=azure-ml-py\" rel=\"nofollow noreferrer\"><code>HyperDriveRunConfig<\/code><\/a> param to allow for this, but it doesn't seem to exist. Perhaps this is controlled on the Pipeline itself with the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipeline(class)?view=azure-ml-py#remarks\" rel=\"nofollow noreferrer\"><code>continue_on_step_failure<\/code><\/a> param?<\/p>\n<p>The workaround we're considering is to catch the failed run within our <code>train.py<\/code> script and manually log the primary_metric as zero.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/U8iNL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/U8iNL.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1594143669730,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":185,
        "Owner_creation_time":1405457120427,
        "Owner_last_access_time":1663947733100,
        "Owner_location":"Seattle, WA, USA",
        "Owner_reputation":3359,
        "Owner_up_votes":1187,
        "Owner_down_votes":14,
        "Owner_views":555,
        "Question_last_edit_time":1594159181910,
        "Answer_body":"<p>thanks for your question.<\/p>\n<p>I'm assuming that HyperDriveStep is one of the steps in your Pipeline and that you want the remaining Pipeline steps to continue, when HyperDriveStep fails, is that correct?\nEnabling continue_on_step_failure, should allow the rest of the pipeline steps to continue, when any single steps fails.<\/p>\n<p>Additionally, the HyperDrive run consists of multiple child runs, controlled by the HyperDriveConfig. If the first 3 child runs explored by HyperDrive fail (e.g. with user script errors), the system automatically cancels the entire HyperDrive run, in order to avoid further wasting resources.<\/p>\n<p>Are you looking to continue other Pipeline steps when the HyperDriveStep fails? or are you looking to continue other child runs within the HyperDrive run, when the first 3 child runs fail?<\/p>\n<p>Thanks!<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1594154863553,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62780977",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":49016896,
        "Question_title":"How to bypass ID column without being used in the training model but have it as output - Azure ML",
        "Question_body":"<p>The input data in the model includes column ControlNo.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/eqULH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eqULH.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But I don't want this column being part of learning process so I'm using <code>Select Columns in Dataset<\/code> to exclude <code>ControlNo<\/code> column.<\/p>\n<p>But as a output I want those columns:<\/p>\n<pre><code>ControlNo, Score Label, Score Probability\n<\/code><\/pre>\n<p>So basically I need NOT to include column <code>ControlNo<\/code> into learning process,\nbut have it as output along with <code>Score Label<\/code> column.<\/p>\n<p>How can I do that?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/iPlpa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/iPlpa.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1519761299607,
        "Question_score":2,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio|ml-studio",
        "Question_view_count":582,
        "Owner_creation_time":1457596845393,
        "Owner_last_access_time":1663977598457,
        "Owner_location":"San Diego, CA, United States",
        "Owner_reputation":4046,
        "Owner_up_votes":505,
        "Owner_down_votes":7,
        "Owner_views":825,
        "Question_last_edit_time":1592644375060,
        "Answer_body":"<p>Instead of removing the ControlNo column from the dataset, you can use the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/edit-metadata\" rel=\"nofollow noreferrer\">Edit Metadata<\/a> module to clear its \"Feature\" flag - just select the column and set <strong>Fields<\/strong> to <strong>Clear feature<\/strong>. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/EUy9A.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EUy9A.png\" alt=\"Edit Metadata settings\"><\/a><\/p>\n\n<p>This will cause the Azure ML Studio algorithms to ignore it during training, and you'll be able to return it as part of your output. <\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1533111465720,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49016896",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62407943,
        "Question_title":"Restrict the number of nodes used by an Azure Machine Learning pipeine",
        "Question_body":"<p>I have written a pipeline that I want to run on a remote compute cluster within Azure Machine Learning. My aim is to process a large amount of historical data, and to do this I will need to run the pipeline on a large number of input parameter combinations.<\/p>\n\n<p>Is there a way to restrict the number of nodes that the pipeline uses on the cluster? By default it will use all the nodes available to the cluster, and I would like to restrict it so that it only uses a pre-defined maximum. This allows me to leave the rest of the cluster free for other users.<\/p>\n\n<p>My current code to start the pipeline looks like this:<\/p>\n\n<pre><code># Setup the pipeline\nsteps = [data_import_step] # Contains PythonScriptStep\npipeline = Pipeline(workspace=ws, steps=steps)\npipeline.validate()\n\n# Big long list of historical dates that I want to process data for\ndts = pd.date_range('2019-01-01', '2020-01-01', freq='6H', closed='left')\n# Submit the pipeline job\nfor dt in dts:\n    pipeline_run = Experiment(ws, 'my-pipeline-run').submit(\n        pipeline,\n        pipeline_parameters={\n            'import_datetime': dt.strftime('%Y-%m-%dT%H:00'),\n        }\n    )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1592308824897,
        "Question_score":3,
        "Question_tags":"python|azure|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":274,
        "Owner_creation_time":1403698315160,
        "Owner_last_access_time":1664014370483,
        "Owner_location":"London, UK",
        "Owner_reputation":1534,
        "Owner_up_votes":171,
        "Owner_down_votes":3,
        "Owner_views":56,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For me, the killer feature of Azure ML is not having to worry about load balancing like this. Our team has a compute target with <code>max_nodes=100<\/code> for every feature branch and we have <code>Hyperdrive<\/code> pipelines that result in 130 runs for each pipeline.<\/p>\n<p>We can submit multiple <code>PipelineRun<\/code>s back-to-back and the orchestrator does the heavy lifting of queuing, submitting, all the runs so that the <code>PipelineRun<\/code>s execute in the serial order I submitted them, and that the cluster is never overloaded. This works without issue for us 99% of the time.<\/p>\n<p>If what you're looking for is that you'd like the <code>PipelineRun<\/code>s to be executed in parallel, then you should check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-parallel-run-step#build-and-run-the-pipeline-containing-parallelrunstep\" rel=\"nofollow noreferrer\"><code>ParallelRunStep<\/code><\/a>.<\/p>\n<p>Another option is to isolate your computes. You can have up to 200 <code>ComputeTarget<\/code>s per workspace. Two 50-node <code>ComputeTarget<\/code>s cost the same as one 100-node <code>ComputeTarget<\/code>.<\/p>\n<p>On our team, we use <a href=\"https:\/\/www.pygit2.org\/\" rel=\"nofollow noreferrer\"><code>pygit2<\/code><\/a> to have a <code>ComputeTarget<\/code> created for each feature branch, so that, as data scientists, we can be confident that we're not stepping on our coworkers' toes.<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1592597207007,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1592842949400,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62407943",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":52537861,
        "Question_title":"What is the role of feature type in AzureML?",
        "Question_body":"<p>I want to know what is the difference between <code>feature numeric<\/code> and <code>numeric<\/code> columns in Azure Machine Learning Studio.<\/p>\n\n<p>The <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/edit-metadata\" rel=\"nofollow noreferrer\">documentation site<\/a> states: <\/p>\n\n<blockquote>\n  <p>Because all columns are initially treated as features, for modules\n  that perform mathematical operations, you might need to use this\n  option to prevent numeric columns from being treated as variables.<\/p>\n<\/blockquote>\n\n<p>But nothing more. Not what a feature is, in which modules you need features. Nothing. <\/p>\n\n<p>I specifically would like to understand if the <code>clear feature<\/code> dropdown option in the <code>fields<\/code> in the <code>edit metadata<\/code>-module has any effect. Can somebody give me a szenario where this <code>clear feature<\/code>-operation changes the ML outcome? Thank you<\/p>\n\n<p>According to the documentation in ought to have an effect:<\/p>\n\n<blockquote>\n  <p>Use the Fields option if you want to change the way that Azure Machine\n  Learning uses the data in a model.<\/p>\n<\/blockquote>\n\n<p>But what can this effect be? Any example might help<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1538054010173,
        "Question_score":1,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio|code-documentation",
        "Question_view_count":70,
        "Owner_creation_time":1368739128833,
        "Owner_last_access_time":1653605354277,
        "Owner_location":"Riga, Latvia",
        "Owner_reputation":1763,
        "Owner_up_votes":527,
        "Owner_down_votes":52,
        "Owner_views":380,
        "Question_last_edit_time":1538054394440,
        "Answer_body":"<p>As you suspect, setting a column as <code>feature<\/code> does have an effect, and it's actually quite important - when training a model, the algorithms will only take into account columns with the <code>feature<\/code> flag, effectively ignoring the others. <\/p>\n\n<p>For example, if you have a dataset with columns <code>Feature1<\/code>, <code>Feature2<\/code>, and <code>Label<\/code> and you want to try out just <code>Feature1<\/code>, you would apply <code>clear feature<\/code> to the <code>Feature2<\/code> column (while making sure that <code>Feature1<\/code> has the <code>feature<\/code> label set, of course).<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1538116098500,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52537861",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":55353889,
        "Question_title":"Azure container instances deployment failed",
        "Question_body":"<p>I am deploying a machine learning image to Azure Container Instances from Azure Machine Learning services according to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml\" rel=\"nofollow noreferrer\">this article<\/a>, but am always stuck with the error message:<\/p>\n\n<blockquote>\n  <p>Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.<br>\n  Please check the logs for your container instance xxxxxxx'.<\/p>\n<\/blockquote>\n\n<p>I tried:<\/p>\n\n<ol>\n<li>increasing memory_gb=4 in aci_config.<\/li>\n<li>I did\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-troubleshoot-deployment#debug-the-docker-image-locally\" rel=\"nofollow noreferrer\">troubleshooting<\/a> locally, but I could not have found any.<\/li>\n<\/ol>\n\n<p>Below is my score.py<\/p>\n\n<pre><code>def init():\n    global model\n    model_path = Model.get_model_path('pofc_fc_model')\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    data = np.array(json.loads(raw_data)['data'])\n    y_hat = model.predict(data)\n    return y_hat.tolist()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1553593173310,
        "Question_score":2,
        "Question_tags":"python|containers|azure-container-instances|azure-machine-learning-service",
        "Question_view_count":3020,
        "Owner_creation_time":1510960409297,
        "Owner_last_access_time":1640215803973,
        "Owner_location":"Bangkok Thailand",
        "Owner_reputation":306,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Question_last_edit_time":1562618473092,
        "Answer_body":"<p>Have you registered the model <code>'pofc_fc_model'<\/code> in your workspace using the <code>register()<\/code> function on the model object? If not, there will be no model path and that can cause failure.<\/p>\n\n<p>See this section on model registration: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#registermodel\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#registermodel<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1553715289510,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55353889",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":65556574,
        "Question_title":"How to make prediction after model registration in azure?",
        "Question_body":"<p>I created a simply model and then registered in azure. How can I make a prediction?<\/p>\n<pre><code>from sklearn import svm\nimport joblib\nimport numpy as np\n\n# customer ages\nX_train = np.array([50, 17, 35, 23, 28, 40, 31, 29, 19, 62])\nX_train = X_train.reshape(-1, 1)\n# churn y\/n\ny_train = [&quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;]\n\nclf = svm.SVC(gamma=0.001, C=100.)\nclf.fit(X_train, y_train)\n\njoblib.dump(value=clf, filename=&quot;churn-model.pkl&quot;)\n<\/code><\/pre>\n<p>Registration:<\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.get(name=&quot;myworkspace&quot;, subscription_id='My_subscription_id', resource_group='ML_Lingaro')\n\nfrom azureml.core.model import Model\nmodel = Model.register(workspace=ws, model_path=&quot;churn-model.pkl&quot;, model_name=&quot;churn-model-test&quot;)\n<\/code><\/pre>\n<p>Prediction:<\/p>\n<pre><code>from azureml.core.model import Model\nimport os\n\nmodel = Model(workspace=ws, name=&quot;churn-model-test&quot;)\nX_test = np.array([50, 17, 35, 23, 28, 40, 31, 29, 19, 62])\nmodel.predict(X_test) ???? \n<\/code><\/pre>\n<p>Error: <code>AttributeError: 'Model' object has no attribute 'predict'<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1609722575690,
        "Question_score":3,
        "Question_tags":"azure|azure-machine-learning-service",
        "Question_view_count":540,
        "Owner_creation_time":1605834001337,
        "Owner_last_access_time":1618085210227,
        "Owner_location":null,
        "Owner_reputation":73,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":1609723457016,
        "Answer_body":"<p>great question -- I also had the same misconception starting out. The missing piece is that there's a difference between model 'registration' and model 'deployment'. Registration is simply for tracking and for easy downloading at a later place and time. Deployment is what you're after, making a model available to be scored against.<\/p>\n<p>There's a <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python&amp;WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">whole section in the docs about deployment<\/a>. My suggestion would be to deploy it locally first for testing.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1609737626627,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65556574",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":69764100,
        "Question_title":"Endpoints cost on Azure Machine Learning",
        "Question_body":"<p>I have been following the learning path for <a href=\"https:\/\/docs.microsoft.com\/en-us\/learn\/certifications\/exams\/ai-900\" rel=\"nofollow noreferrer\">Microsoft Azure AI 900<\/a>. In the second module, I have deployed my model as an endpoint. It says Container instances for compute type. How much will this cost me. Azure doesn't seem to show any pricing for this. Is this endpoint always active? If yes how much does it cost?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1635485800157,
        "Question_score":3,
        "Question_tags":"azure|azure-web-app-service|azure-machine-learning-studio",
        "Question_view_count":633,
        "Owner_creation_time":1566078293737,
        "Owner_last_access_time":1663949466117,
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":449,
        "Owner_up_votes":370,
        "Owner_down_votes":2,
        "Owner_views":77,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The price depends on the number of <strong>vCPU<\/strong> and <strong>GBs<\/strong> of memory requested for the container group. You are charged based on the <strong>vCPU request<\/strong> for your container group rounded up to the nearest whole number for the duration (measured in seconds) <strong>your instance is running<\/strong>. You are also charged for the <strong>GB request<\/strong> for your container group rounded up to the nearest tenths place for the duration (measured in seconds) your <strong>container group is running<\/strong>. There is an additional charge of $0.000012 per vCPU second for Windows software duration on Windows container groups. Check here <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/container-instances\/\" rel=\"nofollow noreferrer\">Pricing - Container Instances | Microsoft Azure<\/a> for details<\/p>\n<ul>\n<li>After Deployed the Azure Machine Learning managed online endpoint (preview).<\/li>\n<li>Have at least <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/role-based-access-control\/role-assignments-portal.md\" rel=\"nofollow noreferrer\">Billing Reader<\/a> access on the subscription where the endpoint is deployed<\/li>\n<\/ul>\n<p>To know the costs estimation<\/p>\n<ol>\n<li><p>In the <a href=\"https:\/\/portal.azure.com\/\" rel=\"nofollow noreferrer\">Azure portal<\/a>, Go to your subscription<\/p>\n<\/li>\n<li><p>Select <strong>Cost Analysis<\/strong> for your subscription.<\/p>\n<\/li>\n<\/ol>\n<p><img src=\"https:\/\/i.imgur.com\/W2eaRIO.png\" alt=\"enter image description here\" \/><\/p>\n<p>Create a filter to scope data to your Azure Machine learning workspace resource:<\/p>\n<ol>\n<li><p>At the top navigation bar, select <strong>Add filter<\/strong>.<\/p>\n<\/li>\n<li><p>In the first filter dropdown, select <strong>Resource<\/strong> for the filter type.<\/p>\n<\/li>\n<li><p>In the second filter dropdown, select your Azure Machine Learning workspace.<\/p>\n<\/li>\n<\/ol>\n<p><img src=\"https:\/\/i.imgur.com\/HEvprph.png\" alt=\"enter image description here\" \/><\/p>\n<p>Create a tag filter to show your managed online endpoint and\/or managed online deployment:<\/p>\n<ol>\n<li><p>Select <strong>Add filter<\/strong> &gt; <strong>Tag<\/strong> &gt; <strong>azuremlendpoint<\/strong>: &quot;&lt; your endpoint name&gt;&quot;<\/p>\n<\/li>\n<li><p>Select <strong>Add filter<\/strong> &gt; <strong>Tag<\/strong> &gt; <strong>azuremldeployment<\/strong>: &quot;&lt; your deployment name&gt;&quot;.<\/p>\n<\/li>\n<\/ol>\n<p><img src=\"https:\/\/i.imgur.com\/1aapYGB.png\" alt=\"enter image description here\" \/><\/p>\n<p>Refer  <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/how-to-view-online-endpoints-costs.md\" rel=\"nofollow noreferrer\">here <\/a> for more detailed steps<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1635505501917,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69764100",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71321757,
        "Question_title":"What are valid Azure ML Workspace connection argument options?",
        "Question_body":"<p>I want to build an Azure ML environment with two python packages that I have in Azure Devops.\nFor this I need a workspace connection to Azure Devops. One package is published to an artifact feed and I can access it using the python SDK using a personal access token:<\/p>\n<pre><code>ws.set_connection(name=&quot;ConnectionName&quot;, \n                  category= &quot;PythonFeed&quot;, \n                  target = &quot;https:\/\/pkgs.dev.azure.com\/&quot;, \n                  authType = &quot;PAT&quot;, \n                  value = PAT_TOKEN)\n<\/code><\/pre>\n<p>However, for the other I need to get the package from the git repository in Azure Devops. The documentation of the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace.workspace?view=azure-ml-py#azureml-core-workspace-workspace-set-connection\" rel=\"nofollow noreferrer\">Python SDK<\/a> and the underlying <a href=\"https:\/\/docs.microsoft.com\/en-us\/rest\/api\/azureml\/workspace-connections\/create\" rel=\"nofollow noreferrer\">REST API<\/a> don't give the options for the arguments, only that they need to be strings (see links).<\/p>\n<p>My question: what are the options for the following arguments:<\/p>\n<ul>\n<li>authType<\/li>\n<li>category<\/li>\n<li>valueFormat<\/li>\n<\/ul>\n<p>And what do I need to set for target argument, so that I can connect to the Azure DevOps repository with potentially different authentication?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646219844250,
        "Question_score":0,
        "Question_tags":"python|azure-devops|azure-machine-learning-studio|azureml-python-sdk|azure-python-sdk",
        "Question_view_count":93,
        "Owner_creation_time":1646219230530,
        "Owner_last_access_time":1663852436163,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>To get the package from a Azure DevOps git repository you can change the target to the repository URL:<\/p>\n<pre><code>ws.set_connection(\n    name=&quot;ConnectionName&quot;, \n    category = &quot;PythonFeed&quot;,\n    target = &quot;https:\/\/dev.azure.com\/&lt;MY-ORG&gt;\/&lt;MY-PROJECT&gt;\/_git\/&lt;MY-REPO&gt;&quot;, \n    authType = &quot;PAT&quot;, \n    value = &lt;PAT-TOKEN&gt;)\n<\/code><\/pre>\n<p>Note here that there is no user specified in the URL (the standard &quot;clone&quot; URL in Azure DevOps also contains &quot;DevOps-Vx@&quot;).<\/p>\n<p>As for any other options for &quot;authType&quot;, &quot;category&quot; and &quot;valueFormat&quot;, I don't know.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1659435013617,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71321757",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":63691515,
        "Question_title":"Azure Machine Learning Studio: cannot create Datastore from Azure SQL Database",
        "Question_body":"<p>I am trying to connect to an Azure SQL Database from inside Azure Machine Learning Studio. Based on <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py<\/a>, it seems that the recommended pattern is to create a Datastore using the Datastore.register_azure_sql_database method as follows:<\/p>\n<pre><code>import os\nfrom azureml.core import Workspace, Datastore\n\nws = Workspace.from_config() # asks for interactive authentication the first time\n\nsql_datastore_name  = &quot;datastore_test_01&quot; # any name should be fine\nserver_name         = os.getenv(&quot;SQL_SERVERNAME&quot;    , &quot;{SQL_SERVERNAME}&quot;) # Name of the Azure SQL server\ndatabase_name       = os.getenv(&quot;SQL_DATABASENAME&quot;  , &quot;{SQL_DATABASENAME}&quot;) # Name of the Azure SQL database\nusername            = os.getenv(&quot;SQL_USER_NAME&quot;     , &quot;{SQL_USER_NAME}&quot;) # The username of the database user.\npassword            = os.getenv(&quot;SQL_USER_PASSWORD&quot; , &quot;{SQL_USER_PASSWORD}&quot;) # The password of the database user.\n\nsql_datastore = Datastore.register_azure_sql_database(workspace      = ws,\n                                                      datastore_name = sql_datastore_name,\n                                                      server_name    = server_name,\n                                                      database_name  = database_name,\n                                                      username       = username,\n                                                      password       = password)\n<\/code><\/pre>\n<p>I am pretty sure I have set all parameters right, having copied them from the ADO.NET connection string at my SQL Database resource --&gt; Settings --&gt; Connection strings:<\/p>\n<pre><code>Server=tcp:{SQL_SERVERNAME}.database.windows.net,1433;Initial Catalog={SQL_DATABASENAME};Persist Security Info=False;User ID={SQL_USER_NAME};Password={SQL_USER_PASSWORD};MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;\n<\/code><\/pre>\n<p>However, I get the following error:<\/p>\n<pre><code>Registering datastore failed with a 400 error code and error message 'Azure SQL Database Error -2146232060: Please check the correctness of the datastore information.'\n<\/code><\/pre>\n<p>Am I missing something? E.g., a firewall rule? I have also tried adding the Azure ML compute resource's public IP address to the list of allowed IP addresses in my SQL Database resource, but still no success.<\/p>\n<hr \/>\n<p><strong>UPDATE<\/strong>: adding <code>skip_validation = True<\/code> to <code>Datastore.register_azure_sql_database<\/code> solves the issue. I can then query the data with<\/p>\n<pre><code>from azureml.core import Dataset\nfrom azureml.data.datapath import DataPath\n\nquery   = DataPath(sql_datastore, 'SELECT * FROM my_table')\ntabular = Dataset.Tabular.from_sql_query(query, query_timeout = 10)\ndf = tabular.to_pandas_dataframe()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1598976793243,
        "Question_score":1,
        "Question_tags":"azure|azure-sql-database|azure-machine-learning-studio|azure-machine-learning-service|azure-sdk-python",
        "Question_view_count":793,
        "Owner_creation_time":1502454917960,
        "Owner_last_access_time":1611741145200,
        "Owner_location":"Milano, MI, Italia",
        "Owner_reputation":132,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":41,
        "Question_last_edit_time":1599032818923,
        "Answer_body":"<p>is the datastore behind vnet? where are you running the registration code above? On a compute instance behind the same vnet?\nhere is the doc that describe what you need to do to connect to data behind vnet:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-virtual-network#use-datastores-and-datasets\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-virtual-network#use-datastores-and-datasets<\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1598979186260,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63691515",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60490195,
        "Question_title":"Unable to build local AMLS environment with private wheel",
        "Question_body":"<p>I am trying to write a small program using the AzureML Python SDK (v1.0.85) to register an Environment in AMLS and use that definition to construct a local Conda environment when experiments are being run (for a pre-trained model). The code works fine for simple scenarios where all dependencies are loaded from Conda\/ public PyPI, but when I introduce a private dependency (e.g. a utils library) I am getting a InternalServerError with the message \"Error getting recipe specifications\".<\/p>\n\n<p>The code I am using to register the environment is (after having authenticated to Azure and connected to our workspace):<\/p>\n\n<pre><code>environment_name = config['environment']['name']\npy_version = \"3.7\"\nconda_packages = [\"pip\"]\npip_packages = [\"azureml-defaults\"]\nprivate_packages = [\".\/env-wheels\/utils-0.0.3-py3-none-any.whl\"]\n\nprint(f\"Creating environment with name {environment_name}\")\nenvironment = Environment(name=environment_name)\nconda_deps = CondaDependencies()\n\nprint(f\"Adding Python version: {py_version}\")\nconda_deps.set_python_version(py_version)\n\nfor conda_pkg in conda_packages:\n    print(f\"Adding Conda denpendency: {conda_pkg}\")\n    conda_deps.add_conda_package(conda_pkg)\n\nfor pip_pkg in pip_packages:\n    print(f\"Adding Pip dependency: {pip_pkg}\")\n    conda_deps.add_pip_package(pip_pkg)\n\nfor private_pkg in private_packages:\n    print(f\"Uploading private wheel from {private_pkg}\")\n    private_pkg_url = Environment.add_private_pip_wheel(workspace=ws, file_path=Path(private_pkg).absolute(), exist_ok=True)\n    print(f\"Adding private Pip dependency: {private_pkg_url}\")\n    conda_deps.add_pip_package(private_pkg_url)\n\nenvironment.python.conda_dependencies = conda_deps\nenvironment.register(workspace=ws)\n<\/code><\/pre>\n\n<p>And the code I am using to create the local Conda environment is:<\/p>\n\n<pre><code>amls_environment = Environment.get(ws, name=environment_name, version=environment_version)\n\nprint(f\"Building environment...\")\namls_environment.build_local(workspace=ws)\n<\/code><\/pre>\n\n<p>The exact error message being returned when <code>build_local(...)<\/code> is called is:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"C:\\Anaconda\\envs\\AMLSExperiment\\lib\\site-packages\\azureml\\core\\environment.py\", line 814, in build_local\n    raise error\n  File \"C:\\Anaconda\\envs\\AMLSExperiment\\lib\\site-packages\\azureml\\core\\environment.py\", line 807, in build_local\n    recipe = environment_client._get_recipe_for_build(name=self.name, version=self.version, **payload)\n  File \"C:\\Anaconda\\envs\\AMLSExperiment\\lib\\site-packages\\azureml\\_restclient\\environment_client.py\", line 171, in _get_recipe_for_build\n    raise Exception(message)\nException: Error getting recipe specifications. Code: 500\n: {\n  \"error\": {\n    \"code\": \"ServiceError\",\n    \"message\": \"InternalServerError\",\n    \"detailsUri\": null,\n    \"target\": null,\n    \"details\": [],\n    \"innerError\": null,\n    \"debugInfo\": null\n  },\n  \"correlation\": {\n    \"operation\": \"15043e1469e85a4c96a3c18c45a2af67\",\n    \"request\": \"19231be75a2b8192\"\n  },\n  \"environment\": \"westeurope\",\n  \"location\": \"westeurope\",\n  \"time\": \"2020-02-28T09:38:47.8900715+00:00\"\n}\n\nProcess finished with exit code 1\n<\/code><\/pre>\n\n<p>Has anyone seen this error before or able to provide some guidance around what the issue may be?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1583156836040,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":239,
        "Owner_creation_time":1526889513900,
        "Owner_last_access_time":1621512507353,
        "Owner_location":null,
        "Owner_reputation":81,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The issue was with out firewall blocking the required requests between AMLS and the storage container (I presume to get the environment definitions\/ private wheels).<\/p>\n\n<p>We resolved this by updating the firewall with appropriate ALLOW rules for the AMLS service to contact and read from the attached storage container.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1583243758050,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60490195",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":30859824,
        "Question_title":"Use Azure Machine learning to detect symbol within an image",
        "Question_body":"<p>4 years ago I posted <a href=\"https:\/\/stackoverflow.com\/q\/6999920\/411094\">this question<\/a> and got a few answers that were unfortunately outside my skill level.  I just attended a build tour conference where they spoke about machine learning and this got me thinking of the possibility of using ML as a solution to my problem.  i found <a href=\"https:\/\/gallery.azureml.net\/MachineLearningAPI\/02ce55bbc0ab4fea9422fe019995c02f\" rel=\"noreferrer\">this<\/a> on the azure site but i dont think it will help me because its scope is pretty narrow.<\/p>\n\n<p>Here is what i am trying to achieve:<\/p>\n\n<p>i have a source image:<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/6y76s.jpg\" alt=\"source image\"><\/p>\n\n<p>and i want to which one of the following symbols (if any) are contained in the image above:<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/SuHkU.jpg\" alt=\"symbols\"><\/p>\n\n<p>the compare needs to support minor distortion, scaling, color differences, rotation, and brightness differences.<\/p>\n\n<p>the number of symbols to match will ultimately at least be greater than 100.<\/p>\n\n<p>is ML a good tool to solve this problem?  if so, any starting tips?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1434434333360,
        "Question_score":14,
        "Question_tags":"opencv|azure|image-processing|machine-learning|azure-machine-learning-studio",
        "Question_view_count":6324,
        "Owner_creation_time":1280943666900,
        "Owner_last_access_time":1663962994233,
        "Owner_location":"Los Angeles, CA",
        "Owner_reputation":1141,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":109,
        "Question_last_edit_time":1495541445110,
        "Answer_body":"<p>As far as I know, Project Oxford (MS Azure CV API) wouldn't be suitable for your task. Their APIs are very focused to Face related tasks (detection, verification, etc), OCR and Image description. And apparently you can't extend their models or train new ones from the existing ones.<\/p>\n\n<p>However, even though I don't know an out of the box solution for your object detection problem; there are easy enough approaches that you could try and that would give you some start point results.<\/p>\n\n<p>For instance, here is a naive method you could use:<\/p>\n\n<p><strong>1) Create your dataset:<\/strong>\n    This is probably the more tedious step and paradoxically a crucial one. I will assume you have a good amount of images to work with. What would you need to do is to pick a fixed window size and extract positive and negative examples.\n<img src=\"https:\/\/i.stack.imgur.com\/H4uC5.png\" alt=\"enter image description here\"><\/p>\n\n<p>If some of the images in your dataset are in different sizes you would need to rescale them to a common size. You don't need to get too crazy about the size, probably 30x30 images would be more than enough. To make things easier I would turn the images to gray scale too. <\/p>\n\n<p><strong>2) Pick a classification algorithm and train it:<\/strong>\n    There is an awful amount of classification algorithms out there. But if you are new to machine learning I will pick the one I would understand the most. Keeping that in mind, I would check out logistic regression which give decent results, it's easy enough for starters and have a lot of libraries and tutorials. For instance, <a href=\"http:\/\/blog.yhathq.com\/posts\/logistic-regression-and-python.html\" rel=\"noreferrer\">this one<\/a> or <a href=\"https:\/\/msdn.microsoft.com\/en-us\/magazine\/dn948113.aspx\" rel=\"noreferrer\">this one<\/a>. At first I would say to focus in a binary classification problem (like if there is an UD logo in the picture or not) and when you master that one you can jump to the multi-class case. There are resources for that <a href=\"http:\/\/www.codeproject.com\/Articles\/821347\/MultiClass-Logistic-Classifier-in-Python\" rel=\"noreferrer\">too<\/a> or you can always have several models one per logo and run this recipe for each one separately. <\/p>\n\n<p>To train your model, you just need to read the images generated in the step 1 and turn them into a vector and label them accordingly. That would be the  dataset that will feed your model. If you are using images in gray scale, then each position in the vector would correspond to a pixel value in the range 0-255. Depending on the algorithm you might need to rescale those values to the range [0-1] (this is because some algorithms perform better with values in that range). Notice that rescaling the range in this case is fairly easy (new_value = value\/255).<\/p>\n\n<p>You also need to split your dataset, reserving some examples for training, a subset for validation and another one for testing. Again, there are different ways to do this, but I'm keeping this answer as naive as possible.<\/p>\n\n<p><strong>3) Perform the detection:<\/strong>\n    So now let's start the fun part. Given any image you want to run your model and produce coordinates in the picture where there is a logo. There are different ways to do this and I will describe one that probably <strong>is not the best nor the more efficient<\/strong>, but it's easier to develop in my opinion.<\/p>\n\n<p>You are going to scan the picture, extracting the pixels in a \"window\", rescaling those pixels to the size you selected in step 1 and then feed them to your model. <\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/VGk3f.png\" alt=\"Extracting windows to feed the model\"><\/p>\n\n<p>If the model give you a positive answer then you mark that window in the original image. Since the logo might appear in different scales you need to repeat this process with different window sizes. You also would need to tweak the amount of space between windows.<\/p>\n\n<p><strong>4) Rinse and repeat:<\/strong>\n    At the first iteration it's very likely that you will get a lot of false positives. Then you need to take those as negative examples and retrain your model. This would be an iterative process and hopefully on each iteration you will have less and less false positives and fewer false negatives.<\/p>\n\n<p>Once you are reasonable happy with your solution, you might want to improve it. You might want to try other classification algorithms like <a href=\"https:\/\/en.wikipedia.org\/wiki\/Support_vector_machine\" rel=\"noreferrer\">SVM<\/a> or <a href=\"https:\/\/en.wikipedia.org\/wiki\/Deep_learning\" rel=\"noreferrer\">Deep Learning Artificial Neural Networks<\/a>, or to try better object detection frameworks like <a href=\"https:\/\/en.wikipedia.org\/wiki\/Viola%E2%80%93Jones_object_detection_framework\" rel=\"noreferrer\">Viola-Jones<\/a>. Also, you will probably need to use <a href=\"https:\/\/en.wikipedia.org\/wiki\/Cross-validation_%28statistics%29\" rel=\"noreferrer\">crossvalidation<\/a> to compare all your solutions (you can actually use crossvalidation from the beginning). By this moment I bet you would be confident enough that you would like to use OpenCV or another ready to use framework in which case you will have a fair understanding of what is going on under the hood. <\/p>\n\n<p>Also you could just disregard all this answer and go for an OpenCV object detection tutorial like this <a href=\"http:\/\/note.sonots.com\/SciSoftware\/haartraining.html\" rel=\"noreferrer\">one<\/a>. Or take another answer from another question like this <a href=\"https:\/\/stackoverflow.com\/questions\/10168686\/algorithm-improvement-for-coca-cola-can-shape-recognition?rq=1\">one<\/a>. Good luck!<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1434469130290,
        "Answer_score":22,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":1495541910023,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/30859824",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":51238413,
        "Question_title":"Azure Machine Learning Workbench hangs while creating new project",
        "Question_body":"<p>I was trying Azure Machine Learning Services following this tutorial (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/quickstart-installation\" rel=\"nofollow noreferrer\">Link<\/a>). After successfully creating the Azure Machine Learning services accounts, I successfully installed the Workbench on my Windows 10 Laptop (Behind Proxy; Proxy has been configured at the WorkBench). Next, I was trying to create project following this section (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/quickstart-installation#create-a-project-in-workbench\" rel=\"nofollow noreferrer\">Link<\/a>). Once I click on the Create button, it goes to \"Creating\" state and stays there for ever. The errors displayed at Errors.log is the following. Any suggestion will be appreciated. <\/p>\n\n<pre><code>[2018-07-09 09:47:08.437] [ERROR] HttpService - {\"event\":\"HttpService\",\"task\":\"Failed\",\"data\":{\"url\":\"http:\/\/localhost:54240\/projects\/v1.0\/create\/template\",\"status\":500,\"statusText\":\"INKApi Error\",\"jsonError\":null,\"requestId\":null,\"sessionType\":\"Workbench\"},\"sid\":\"365395c0-832b-11e8-b4ce-e5d7046c6143\"}\n\n[2018-07-09 09:47:08.960] [ERROR] CreateProjectForm - {\"event\":\"CreateProject\",\"task\":\"Error\",\"data\":{\"_body\":null,\"status\":500,\"ok\":false,\"statusText\":\"INKApi Error\",\"headers\":{\"Date\":[\"Mon\",\" 09 Jul 2018 04:17:06 GMT\"],\"Via\":[\"1.1 localhost.localdomain\"],\"Proxy-Connection\":[\"close\"],\"Content-Length\":[\"0\"],\"Content-Type\":[\"text\/html\"]},\"type\":2,\"url\":\"http:\/\/localhost:54240\/projects\/v1.0\/create\/template\"},\"sid\":\"365395c0-832b-11e8-b4ce-e5d7046c6143\"}\n\n[2018-07-09 09:47:08.963] [FATAL] ExceptionLogger - {\"event\":\"exception\",\"task\":\"\",\"data\":{\"message\":\"Cannot read property 'error' of null\",\"name\":\"TypeError\",\"stack\":\"TypeError: Cannot read property 'error' of null\\n    at SafeSubscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:61476:58)\\n    at SafeSubscriber.__tryOrUnsub (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212279:20)\\n    at SafeSubscriber.error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212241:30)\\n    at Subscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212172:30)\\n    at Subscriber.error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212146:22)\\n    at MergeMapSubscriber.OuterSubscriber.notifyError (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:210968:30)\\n    at InnerSubscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:211072:25)\\n    at InnerSubscriber.Subscriber.error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212146:22)\\n    at DeferSubscriber.OuterSubscriber.notifyError (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:210968:30)\\n    at InnerSubscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:211072:25)\"},\"sid\":\"365395c0-832b-11e8-b4ce-e5d7046c6143\"}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1531112127690,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":67,
        "Owner_creation_time":1280505139753,
        "Owner_last_access_time":1663935737867,
        "Owner_location":"Bangalore, India",
        "Owner_reputation":4265,
        "Owner_up_votes":315,
        "Owner_down_votes":11,
        "Owner_views":403,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It was happening because of the Proxy (although I have configured the Proxy on the Workbench). When I am connected to internet directly, everything works fine (Able to create project, train, compare models etc). However the Workbench should return meaningful error instead of hanging or simply waiting while creating the project.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1531201276973,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51238413",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73760033,
        "Question_title":"How to transfer a csv file from notebook folder to a datastore",
        "Question_body":"<p>I want to transfer a generated csv file <code>test_df.csv<\/code> from my Azure ML notebook folder which has a path <code>\/Users\/Ankit19.Gupta\/test_df.csv<\/code> to a datastore which has a web path <code>https:\/\/abc.blob.core.windows.net\/azureml\/LocalUpload\/f3db18b6<\/code>. I have written the python code as<\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\n    \ndatastore.upload_files('\/Users\/Ankit19.Gupta\/test_df.csv',\n                  target_path='https:\/\/abc.blob.core.windows.net\/azureml\/LocalUpload\/f3db18b6',\n                  overwrite=True)\n<\/code><\/pre>\n<p>But it is showing the following error message:<\/p>\n<pre><code>UserErrorException: UserErrorException:\n    Message: '\/' does not point to a file. Please upload the file to cloud first if running in a cloud notebook.\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;'\/' does not point to a file. Please upload the file to cloud first if running in a cloud notebook.&quot;\n    }\n}\n<\/code><\/pre>\n<p>I have tried <a href=\"https:\/\/stackoverflow.com\/questions\/67897947\/how-to-transfer-data-from-azure-ml-notebooks-to-a-storage-container\">this<\/a> but it is not working for me. Can anyone please help me to resolve this issue. Any help would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663473633117,
        "Question_score":0,
        "Question_tags":"python|azure|file-upload|azure-machine-learning-service|movefile",
        "Question_view_count":43,
        "Owner_creation_time":1540154634483,
        "Owner_last_access_time":1664030893903,
        "Owner_location":null,
        "Owner_reputation":237,
        "Owner_up_votes":60,
        "Owner_down_votes":0,
        "Owner_views":173,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The way the path was mentioned is not accurate. The datastore path will be different manner.\nReplace the below code for the small change in the calling path.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4o1WU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4o1WU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\n    \ndatastore.upload_files('.\/Users\/foldername\/filename.csv',\n                  target_path=\u2019your targetfolder',\n                  overwrite=True)\n<\/code><\/pre>\n<p>We need to call all the parent folders before the folder.  <strong><code>\u201c.\/\u201d<\/code><\/strong> is the way we can call the dataset from datastore.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1663658453173,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73760033",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70929123,
        "Question_title":"AzureMLCompute job failed with `FailedLoginToImageRegistry`",
        "Question_body":"<p>I've been trying to send a train job through azure ml python sdk with:<\/p>\n<pre><code>from azureml.core import Workspace, Experiment, ScriptRunConfig \n\nif __name__ == &quot;__main__&quot;:\n    ws = Workspace.from_config()\n    experiment = Experiment(workspace=ws, name='ConstructionTopicsModel')\n\n    config = ScriptRunConfig(source_directory='.\/',\n                         script='src\/azureml\/train.py',\n                         arguments=None,\n                         compute_target='ComputeTargetName',\n                         )\n\n    env = ws.environments['test-env']\n    config.run_config.environment = env\n    run = experiment.submit(config)\n    \n    run.wait_for_completion(show_output=True)\n\n    aml_url = run.get_portal_url()\n    print(aml_url)\n<\/code><\/pre>\n<p>But I was getting the <code>ServiceError<\/code> message:<\/p>\n<pre><code>AzureMLCompute job failed. FailedLoginToImageRegistry: Unable to login to docker image repo\nReason: Failed to login to the docker registry\nerror: WARNING! Using --password via the CLI is insecure. Use --password-stdin. Error saving credentials: error storing credentials - err: exit status 1, out: `Cannot autolaunch D-Bus without X11 $DISPLAY`\n\nserviceURL: 7ac86b04d6564d36aa80ae2ad090582c.azurecr.io\nReason: WARNING! Using --password via the CLI is insecure. Use --password-stdin. Error saving credentials: error storing credentials - err: exit status 1, out: `Cannot autolaunch D-Bus without X11 $DISPLAY`\n\nInfo: Failed to setup runtime for job execution: Job environment preparation failed on 10.0.0.5 with err exit status 1.\n<\/code><\/pre>\n<p>I also tried using the azure cli without success, same error message<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643645330913,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service|azureml-python-sdk|azuremlsdk",
        "Question_view_count":202,
        "Owner_creation_time":1589293508567,
        "Owner_last_access_time":1663681781073,
        "Owner_location":null,
        "Owner_reputation":833,
        "Owner_up_votes":9,
        "Owner_down_votes":9,
        "Owner_views":55,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The only way I've found so far to make this work, was to run it on a terminal of the compute-target itself. That's how the docker error goes away. Trying to run the experiment from a terminal of a different compute instance raises the exception.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1643645330913,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70929123",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":49130977,
        "Question_title":"Does a call to \"Deploy web service(via API key) \" re run trained Azure ML model again",
        "Question_body":"<p>I wanted to know how exactly the following works in backend<\/p>\n\n<p><strong>Scenario :<\/strong> <\/p>\n\n<blockquote>\n  <p>-> We get data from Edgex foundry in UTC format and we it store it in Azure Document DB in (CST\/CDT timezone) format<\/p>\n  \n  <p>-> We trained ML model on data(with Date in CST\/CDT timezone) and Deploy web service.<\/p>\n<\/blockquote>\n\n<p><strong>So I have few basic doubts below<\/strong><\/p>\n\n<blockquote>\n  <ol>\n  <li><p>When web job hits our predictive webservice , will the trained ML model be run again?<\/p><\/li>\n  <li><p>Do we need to convert the UTC timezone for new incoming test data( which we want to predict) into CST\/CDT timezone, as TimeStamp does\n  matter for our prediction?<\/p><\/li>\n  <li><p>What happens in backend when predictive webservice API is called?<\/p><\/li>\n  <\/ol>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1520339633757,
        "Question_score":0,
        "Question_tags":"azure|azure-web-app-service|azure-machine-learning-studio",
        "Question_view_count":92,
        "Owner_creation_time":1504867604870,
        "Owner_last_access_time":1562061256807,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":391,
        "Owner_up_votes":49,
        "Owner_down_votes":0,
        "Owner_views":125,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This is only based on my experience with Azure ML, but I think I can help with your questions.<\/p>\n\n<blockquote>\n  <p>When web job hits our predictive webservice, will the trained ML model be run again?<\/p>\n<\/blockquote>\n\n<p>Yes, in the sense that it will call the <code>predict<\/code> (or similar) method on the model on the new data. For instance, in <code>scikit-learn<\/code> you would train your model using the <code>fit<\/code> method. Once the model is in production, only the <code>predict<\/code> method would be called.<\/p>\n\n<p>It will also run the whole workflow you have set up to be deployed as the web service. As an example below is a workflow I've played around with before. Each time the web service is run with new data, this whole thing will be run. This is like creating a Pipeline in <code>scikit-learn<\/code>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YMFZb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YMFZb.png\" alt=\"Azure ML Workflow\"><\/a><\/p>\n\n<blockquote>\n  <p>Do we need to convert the UTC timezone for new incoming test data( which we want to predict) into CST\/CDT timezone, as TimeStamp does matter for our prediction?<\/p>\n<\/blockquote>\n\n<p>I would say yes, you would need to convert to the timezone that was used when training in the model. This can be done by adding a step in your workflow then when you call the web service it will do the necessary converting for you before making a prediction.<\/p>\n\n<blockquote>\n  <p>What happens in backend when predictive webservice API is called?<\/p>\n<\/blockquote>\n\n<p>I'm not sure if anyone knows for sure other than the folks at Microsoft, but for sure it will run the workflow you have set up.<\/p>\n\n<hr>\n\n<p>I know it's not much, but I hope this helps or at least gets you on the right track for what you need.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1521026355920,
        "Answer_score":1,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49130977",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60539094,
        "Question_title":"Is it possible to share compute instance with other user?",
        "Question_body":"<p>I create one compute instance 'yhd-notebook' in Azure Machine Learning compute with user1. When I login with user2, and try to open the JupyterLab of this compute instance, it shows an error message like below.<\/p>\n\n<blockquote>\n  <p>User user2 does not have access to compute instance yhd-notebook.<\/p>\n  \n  <p>Only the creator can access a compute instance.<\/p>\n  \n  <p>Click here to sign out and sign in again with a different account.<\/p>\n<\/blockquote>\n\n<p>Is it possible to share compute instance with another user? BTW, both user1 and user2 have Owner role with the Azure subscription.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1583388412943,
        "Question_score":8,
        "Question_tags":"azure|azure-machine-learning-service",
        "Question_view_count":3705,
        "Owner_creation_time":1582361231693,
        "Owner_last_access_time":1655516080367,
        "Owner_location":"Guangzhou, China",
        "Owner_reputation":393,
        "Owner_up_votes":37,
        "Owner_down_votes":1,
        "Owner_views":58,
        "Question_last_edit_time":null,
        "Answer_body":"<p>According to MS, all users in the workspace contributor and owner role can create, delete, start, stop, and restart compute instances across the workspace. However, only the creator of a specific compute instance is allowed to access Jupyter, JupyterLab, and RStudio on that compute instance. The creator of the compute instance has the compute instance dedicated to them, have root access, and can terminal in through Jupyter. Compute instance will have single-user login of creator user and all actions will use that user\u2019s identity for RBAC and attribution of experiment runs. SSH access is controlled through public\/private key mechanism.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1583393458820,
        "Answer_score":7,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60539094",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64486262,
        "Question_title":"Is there a way to un-register an environment in Azure ML studio",
        "Question_body":"<p>I am trying to deploy a model in Azure ML and kept on getting the error 'model not found' from my score.py. So I decided to start from scratch again. I had my custom environment registered, and the Azure ML API for Environment class doesn't seem to have anything like 'delete' or 'unregister'. is there a way to work around this? Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603383120473,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":424,
        "Owner_creation_time":1595118700083,
        "Owner_last_access_time":1629382612393,
        "Owner_location":"Toronto, ON, Canada",
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1604274985247,
        "Answer_body":"<p>You can use the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py&amp;preserve-view=true#delete--\" rel=\"nofollow noreferrer\">delete<\/a> method in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py\" rel=\"nofollow noreferrer\">Model<\/a> class to delete a registered model.<\/p>\n<p>This can also be done via the Azure CLI as:<\/p>\n<pre><code>az ml model delete &lt;model id&gt;\n<\/code><\/pre>\n<p>Other commands can be found here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/cli\/azure\/ext\/azure-cli-ml\/ml\/model?view=azure-cli-latest\" rel=\"nofollow noreferrer\">az ml model<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1604151289467,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64486262",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":68303285,
        "Question_title":"'MSIAuthentication' object has no attribute 'get_token'",
        "Question_body":"<p>On Azure ML Workspace Notebook, I'm trying to get my workspace instance, as seen at<\/p>\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-auto-train-models#configure-workspace.<\/a><\/p>\n<p>I have a config file and I am running the notebook in an Azure compute instance.<\/p>\n<p>I tried to execute Workspace.from_config().<\/p>\n<p>As a result, I'm getting the 'MSIAuthentication' object has no attribute 'get_token' error.<\/p>\n<p>I tried to submit both <code>MsiAuthentication<\/code> and <code>InteractiveLoginAuthentication<\/code>, as suggested in<\/p>\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb.<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625753450150,
        "Question_score":2,
        "Question_tags":"azure-machine-learning-studio|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":1670,
        "Owner_creation_time":1589293508567,
        "Owner_last_access_time":1663681781073,
        "Owner_location":null,
        "Owner_reputation":833,
        "Owner_up_votes":9,
        "Owner_down_votes":9,
        "Owner_views":55,
        "Question_last_edit_time":null,
        "Answer_body":"<p><strong>There are 2 solutions I've found:<\/strong><\/p>\n<p>1.- Use the kernel &quot;Python 3.6 - AzureML&quot;<\/p>\n<p>2.- <code>pip install azureml-core --upgrade<\/code><\/p>\n<p>This will <strong>upgrade<\/strong><\/p>\n<blockquote>\n<p>azureml-core to 1.32.0<\/p>\n<\/blockquote>\n<p>But will <strong>downgrade<\/strong>:<\/p>\n<blockquote>\n<p>azure-mgmt-resource to 13.0.0 (was 18.0.0)<\/p>\n<\/blockquote>\n<blockquote>\n<p>azure-mgmt-storage down to 11.2.0 (was 18.0.0)<\/p>\n<\/blockquote>\n<blockquote>\n<p>urllib3 to 1.26.5 (was 1.26.6)<\/p>\n<\/blockquote>\n<p>This upgrade \/ downgrade allows the same package versions as in the python 3.6 anaconda install<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1625753450150,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68303285",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67713876,
        "Question_title":"Is there a way to stop Azure ML throwing an error when exporting zero lines of data?",
        "Question_body":"<p>I am currently developing an Azure ML pipeline that as one of its outputs is maintaining a SQL table holding all of the unique items that are fed into it. There is no way to know in advance if the data fed into the pipeline is new unique items or repeats of previous items, so before updating the table that it maintains it pulls the data already in that table and drops any of the new items that already appear.<\/p>\n<p>However, due to this there are cases where this self-reference results in zero new items being found, and as such there is nothing to export to the SQL table. When this happens Azure ML throws an error, as it is considered an error for there to be zero lines of data to export. In my case, however, this is expected behaviour, and as such absolutely fine.<\/p>\n<p>Is there any way for me to suppress this error, so that when it has zero lines of data to export it just skips the export module and moves on?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1622071490553,
        "Question_score":1,
        "Question_tags":"azure|azure-sql-database|azure-machine-learning-studio",
        "Question_view_count":94,
        "Owner_creation_time":1611181716003,
        "Owner_last_access_time":1639435799460,
        "Owner_location":null,
        "Owner_reputation":119,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This issue has been resolved by an update to Azure Machine Learning; You can now run pipelines with a flag set to &quot;Continue on Failure Step&quot;, which means that steps following the failed data export will continue to run.<\/p>\n<p>This does mean you will need to design your pipeline to be able to handles upstream failures in its downstream modules; this must be done very carefully.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1630895031177,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67713876",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":35870839,
        "Question_title":"Is it possible to import python scripts in Azure?",
        "Question_body":"<p>I have a python script that is written in different files (one for importing, one for calculations, et cetera). These are all in the same folder, and when I need a function from another function I do something like<\/p>\n\n<pre><code>import file_import\nfile_import.do_something_usefull()\n<\/code><\/pre>\n\n<p>where, of course, in the <code>file_import<\/code> there is a function <code>do_something_usefull()<\/code> that, uhm, does something usefull. How can I accomplish the same in Azure?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1457449397920,
        "Question_score":1,
        "Question_tags":"python|azure|cortana-intelligence|azure-machine-learning-studio",
        "Question_view_count":1473,
        "Owner_creation_time":1379265931347,
        "Owner_last_access_time":1663852656720,
        "Owner_location":"Rotterdam, Netherlands",
        "Owner_reputation":6502,
        "Owner_up_votes":1650,
        "Owner_down_votes":188,
        "Owner_views":561,
        "Question_last_edit_time":1457540530820,
        "Answer_body":"<p>I found it out myself. It is documenten on Microsoft's site <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-execute-python-scripts\/\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>The steps, very short, are:<\/p>\n\n<ol>\n<li>Include all the python you want in a .zip<\/li>\n<li>Upload that zip as a dataset<\/li>\n<li>Drag the dataset as the third option parameter in the 'execute python'-block (example below)<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9FuMr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9FuMr.png\" alt=\"Example dragging zip to Python script\"><\/a><\/p>\n\n<ol start=\"4\">\n<li>execute said function by importing <code>import Hello<\/code> (the name of the file, not the zip) and running <code>Hello.do_something_usefull()<\/code><\/li>\n<\/ol>",
        "Answer_comment_count":0,
        "Answer_creation_time":1457451133737,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1457451850803,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35870839",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":31507547,
        "Question_title":"Azure Machine Learning - batch execution partially working",
        "Question_body":"<p>I have been following this <a href=\"https:\/\/gallery.azureml.net\/Experiment\/370c80490e774a6cb26edba69c583c9b\" rel=\"nofollow\">gallery sample<\/a> but I just can't seem to get batch execution to return multiple scores in one job.<\/p>\n\n<p>Everything works fine i.e. can deploy the prediction web API and request a single scoring. But whenever I send a batch execution job (using the <a href=\"https:\/\/studio.azureml.net\/apihelp\/workspaces\/9dbdce0846e64a5f9c925116e0cb6388\/webservices\/7df2a06ad50348d78f0e1cb81f3742ab\/endpoints\/87bfd4ea0615412bac19c34422ced730\/jobs\" rel=\"nofollow\">sample C# codes<\/a>) with more than one request e.g.:<\/p>\n\n<pre><code>ID1,ID2\n1,2\n3,1\n5,1\n<\/code><\/pre>\n\n<p>Azure ML will only return the prediction scores for the first request <code>1,2<\/code> but not for the other rows.<\/p>\n\n<p>I'm not sure where I'm doing wrong but I should be expecting results for all three requests. Any help would be appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1437353901330,
        "Question_score":1,
        "Question_tags":"c#|azure|machine-learning|azure-machine-learning-studio",
        "Question_view_count":269,
        "Owner_creation_time":1316278453427,
        "Owner_last_access_time":1663640052820,
        "Owner_location":"Singapore",
        "Owner_reputation":1365,
        "Owner_up_votes":544,
        "Owner_down_votes":512,
        "Owner_views":310,
        "Question_last_edit_time":1437413945292,
        "Answer_body":"<p>It looks like you've chosen an unfortunate example: the custom scripts in the Retail Forecasting web service explicitly drop all but the first ID pair. To see this, try loading the \"Retail Forecasting: Step 6A of 6\" experiment and check out the code in the \"Create a complete time series. Add future time stamps\" module. You will find the following:<\/p>\n\n<pre><code>all.time &lt;- data.frame(ID1 = data$ID1[1], ID2 = data$ID2[1], time = all.time)\ndata &lt;- join(all.time, data, by = c(\"ID1\", \"ID2\", \"time\"), type = \"left\")\nmaml.mapOutputPort(\"data\");\n<\/code><\/pre>\n\n<p>The left join statement will ignore any rows where data$ID1 != data$ID1[1]  and data$ID2 != data$ID2[1]. That is why you are losing everything but the first ID pair.<\/p>\n\n<p>It appears batch prediction for multiple ID pairs in a single job was not a use case that the custom script authors envisioned for their web service. If you are proficient in R and particularly interested in this use case, you could modify the scripts in this experiment to support processing multiple time series concurrently. Otherwise, you might want to simply try another example experiment.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1472479254437,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/31507547",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":52705769,
        "Question_title":"Azure ML Tune Model Hyper Parameters",
        "Question_body":"<p>Here's question proposed at the end of the chapter in 70-774 exam reference book. <\/p>\n\n<blockquote>\n  <p>If you connect a neural network with a Tune Model Hyperparameters module configured\n  with Random Sweep and Maximum number of runs on random sweep = 1, how\n  many neural networks are trained during the execution of the experiment? Why? If you\n  connect a validation dataset to the third input of the Tune Model Hyperparameters\n  module, how many neural networks are trained now?<\/p>\n<\/blockquote>\n\n<p>And the answer is :<\/p>\n\n<blockquote>\n  <p>Without validation dataset 11 (10 of k-fold cross validation + 1 trained with all the data\n  with the best combination of hyperparameters). With the validation set only 1 neural\n  network is trained, so the best model is not trained using the validation set if you provide\n  it.<\/p>\n<\/blockquote>\n\n<p>Where does 10 come from? As far as I understand the number should be 2 and 1 respectively. Shouldn't it create n-folds where n is equal to the number of runs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1539013176610,
        "Question_score":0,
        "Question_tags":"machine-learning|neural-network|azure-machine-learning-studio",
        "Question_view_count":329,
        "Owner_creation_time":1528790837107,
        "Owner_last_access_time":1660146049397,
        "Owner_location":"Paris, France",
        "Owner_reputation":610,
        "Owner_up_votes":143,
        "Owner_down_votes":0,
        "Owner_views":203,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When you use the Tune Model Hyperparameters module without a validation dataset, this means, when you use only the 2nd input data port, the module works in cross-validation mode. So the best-parameters model is found by doing cross-validation over the provided dataset, and to do this, the dataset is splitted in k-folds. By default, the module splits the data in 10 folds. In case you want to split the data in a different number of folds, you can connect a Partition and Sample module at the 2nd input, selecting Assign to Folds and indicating the number of folds desired. In many cases k=5 is a reasonable option.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1539202336323,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52705769",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":45328657,
        "Question_title":"Scheduling Azure Machine Learning Experimnets",
        "Question_body":"<p>How do i schedule Azure ML Experiments which is not deployed as web service?<\/p>\n\n<p>I have developed a Azure Experiment which imports data from on-premise database and exports data to SQL db. How can i schedule that to run weekly?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1501076325577,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":520,
        "Owner_creation_time":1359784845430,
        "Owner_last_access_time":1654012514917,
        "Owner_location":"India",
        "Owner_reputation":400,
        "Owner_up_votes":40,
        "Owner_down_votes":2,
        "Owner_views":147,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use <strong>Azure PowerShell<\/strong> for automating this task, and use <strong>Windows Task Scheduler<\/strong> to schedule this script to run automatically.<\/p>\n\n<p>For Azure PowerShell,<\/p>\n\n<p>You may visit <a href=\"https:\/\/github.com\/hning86\/azuremlps\" rel=\"nofollow noreferrer\"><strong>this page<\/strong><\/a> to setup an Azure PowerShell script. It's a long journey, but it's worth it. Make sure to <strong><em>follow the prerequisites to be installed on your local PC (Azure-PowerShell v4.0.1)<\/em><\/strong>.<\/p>\n\n<p>For Windows Task Scheduler,<\/p>\n\n<p>Visit <a href=\"https:\/\/www.metalogix.com\/help\/Content%20Matrix%20Console\/SharePoint%20Edition\/002_HowTo\/004_SharePointActions\/012_SchedulingPowerShell.htm\" rel=\"nofollow noreferrer\"><strong>this link<\/strong><\/a> to schedule your created Azure PowerShell script to run at a scheduled\/repeated time.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1502973065180,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45328657",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":35631267,
        "Question_title":"Azure ML in \"Execute Python Script\" module :Common table expressions is not supported in sqlite3",
        "Question_body":"<p>I ran into this issue yesterday, while trying to use the same sqlite script I used in \"Apply SQL Transformation\" module in Azure ML, in Sqlite over Python module in Azure ML:<\/p>\n\n<pre><code>with tbl as (select * from t1)\nselect * from tbl\n<\/code><\/pre>\n\n<p>Here is the error I got:<\/p>\n\n<pre><code>[Critical]     Error: Error 0085: The following error occurred during script evaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\n  File \"C:\\server\\invokepy.py\", line 169, in batch\ndata:text\/plain,Caught exception while executing function: Traceback (most recent call last):\n    odfs = mod.azureml_main(*idfs)\n  File \"C:\\pyhome\\lib\\site-packages\\pandas\\io\\sql.py\", line 388, in read_sql\n  File \"C:\\temp\\azuremod.py\", line 193, in azureml_main\n    results = pd.read_sql(query,con)\n    coerce_float=coerce_float, parse_dates=parse_dates)\n  File \"C:\\pyhome\\lib\\site-packages\\pandas\\io\\sql.py\", line 1017, in execute\n  File \"C:\\pyhome\\lib\\site-packages\\pandas\\io\\sql.py\", line 1022, in read_sql\n    cursor = self.execute(*args)\n    raise_with_traceback(ex)\n  File \"C:\\pyhome\\lib\\site-packages\\pandas\\io\\sql.py\", line 1006, in execute\n---------- End of error message from Python  interpreter  ----------\n    cur.execute(*args)\nDatabaseError: Execution failed on sql:  with tbl as (select * from t1)\n                    select * from tbl\n<\/code><\/pre>\n\n<p>and the Python code:<\/p>\n\n<pre><code>def azureml_main(dataframe1 = None, dataframe2 = None):\n    import pandas as pd\n    import sqlite3 as lite\n    import sys\n    con = lite.connect('data1.db')\n    con.text_factory = str\n    with con:\n        cur = con.cursor()\n\n        if (dataframe1 is not None):\n            cur.execute(\"DROP TABLE IF EXISTS t1\")\n            dataframe1.to_sql('t1',con)\n        query = '''with tbl as (select * from t1)\n                    select * from tbl'''                      \n        results = pd.read_sql(query,con)    \n\n    return results,\n<\/code><\/pre>\n\n<p>when replacing the query with:<\/p>\n\n<pre><code>select * from t1\n<\/code><\/pre>\n\n<p>It worked as expected.\nAs you probably know, Common table expressions is a key feature in Sqlite, the ability to run recursive code is a \"must have\" in any functional language such as Sqlite.<\/p>\n\n<p>I also tried to run my Python script in Jupyter Notebook in Azure, that also worked as expected.<\/p>\n\n<p>Is it possible we have a different configuration for Sqlite in the Python module than in Jupyter Notebook and in \"Apply SQL Transformation\" module?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1456413421317,
        "Question_score":2,
        "Question_tags":"python|sqlite|common-table-expression|azure-machine-learning-studio|cortana-intelligence",
        "Question_view_count":280,
        "Owner_creation_time":1320061998253,
        "Owner_last_access_time":1656424560827,
        "Owner_location":null,
        "Owner_reputation":778,
        "Owner_up_votes":176,
        "Owner_down_votes":6,
        "Owner_views":89,
        "Question_last_edit_time":1456812883803,
        "Answer_body":"<p>I reproduced your issue and reviewed the <code>SQL Queries<\/code> doc of <code>pandas.io.sql<\/code> at <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/stable\/io.html#sql-queries\" rel=\"nofollow\">http:\/\/pandas.pydata.org\/pandas-docs\/stable\/io.html#sql-queries<\/a>. I tried to use <code>read_sql_query<\/code> to solve it, but failed.<\/p>\n\n<p>According to the <code>pandas<\/code> doc, tt seems that <code>Pandas<\/code> not support the usage for this SQL syntax.<\/p>\n\n<p>Base on my experience and according to your SQL, I tried to do the SQL <code>select * from (select * from t1) as tbl<\/code> instead of your SQL that work for <code>Pandas<\/code>.<\/p>\n\n<p>Hope it helps. Best Regards. <\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1456485065183,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35631267",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":46500756,
        "Question_title":"Azure ML Studio workspace from 3rd party does not show up in workspace list on https:\/\/studio.azureml.net\/",
        "Question_body":"<p>I have had a few azure ml workspaces though my own Azure account for a while. Recently I was added as \"Contributor\" to a new Azure workspace as Contributor. In Azure Portal I can see it clearly and have access to it. When going to <a href=\"https:\/\/studio.azureml.net\/\" rel=\"nofollow noreferrer\">https:\/\/studio.azureml.net\/<\/a> It does not show up in the list of workspaces of the correct region, nor in any other region. <\/p>\n\n<p>In Azure Portal I have to change \"Directory\" (top right account menu) to the 3rd party directory to see it.<\/p>\n\n<p>Is there a way to do that in azureml.net ? Or is there something else that might be wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1506758082960,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":56,
        "Owner_creation_time":1403185510317,
        "Owner_last_access_time":1598430888713,
        "Owner_location":null,
        "Owner_reputation":383,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":36,
        "Question_last_edit_time":null,
        "Answer_body":"<p>access to the workspace is controlled by the workspace owner from the settings page inside of the Azure ML workspace. the owners\/contributors etc. listed in Azure portal does NOT grant you access to the workspace. <\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1506907193290,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46500756",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":56021977,
        "Question_title":"Replace values in a column based on a condition in Azure ML Studio",
        "Question_body":"<p>How do I replace the values in a specific column with a particular value based on a condition in Azure ML Studio. I can do this using pandas in python as foolows:<\/p>\n\n<pre><code>df.loc[df['col_name'] &gt; 1990, 'col_name'] = 1\n<\/code><\/pre>\n\n<p>I'm trying to find a Module in Azure Machine Learning Studio that does the equivalent of this. <\/p>\n\n<p>I understand there is a replace option under the ConverToDataset module and a Replace Discrete Values module. But neither of these seems to do what I want. Is there an option to replace the values in just one column to a specific value based on a condition?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1557229871357,
        "Question_score":3,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":564,
        "Owner_creation_time":1450260166773,
        "Owner_last_access_time":1663955203357,
        "Owner_location":null,
        "Owner_reputation":1587,
        "Owner_up_votes":123,
        "Owner_down_votes":8,
        "Owner_views":540,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use either the more general <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/apply-sql-transformation\" rel=\"nofollow noreferrer\">Apply SQL Transformation<\/a>, or the dedicated <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/clip-values\" rel=\"nofollow noreferrer\">Clip Values<\/a> module. If all else fails, there's also <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/execute-python-script\" rel=\"nofollow noreferrer\">Execute Python Script<\/a>.<\/p>\n\n<p>Personally, for your example I'd use <code>Clip Values<\/code> with <code>Clip Peaks<\/code> and <code>Upper Threshold<\/code> set. For more complex rules I'd use either <code>Apply SQL Transformation<\/code> or <code>Execute Python Script<\/code>, depending on the rules but favouring SQL :).<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1557391929597,
        "Answer_score":1,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56021977",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":61392212,
        "Question_title":"Authentication Error: Compute instance for Azure Machine Learning",
        "Question_body":"<p>I created Compute instance in Azure Machine Learning in the Edge browser right after logging in. When it was started, I clicked on the Jupyter link. <\/p>\n\n<p>I got the following authentication error: \"User live.com#myname@outlook.com does not have access to compute instance vm-aml-lab4.\nOnly the creator can access a compute instance.\"<\/p>\n\n<p>Is there a way to avoid this error?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1587659313437,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":953,
        "Owner_creation_time":1330373362200,
        "Owner_last_access_time":1663907622027,
        "Owner_location":"Kennett Square, PA",
        "Owner_reputation":445,
        "Owner_up_votes":377,
        "Owner_down_votes":0,
        "Owner_views":104,
        "Question_last_edit_time":1591885030160,
        "Answer_body":"<p>Currently the AML compute instance only allows the creator to access the CI.It's known bug, once it's fixed we will update you. We think it is related to MSA accounts.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1587713029463,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1598335141843,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61392212",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70571948,
        "Question_title":"Why is env different in an Azure ML notbook and an Azure ML terminal?",
        "Question_body":"<p>I'm using MS Azure ML and have found that when I start a Notebook (from the Azure ML Studio) it is executing in a a different environment than if I create a Python script and run it from the studio. I want to be able to create a specific environment and have the Notebook use that. The environment that the Notebook seems to run does not contain the packages I need and I want to preserve different environments.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1641247442447,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-service|azureml-python-sdk|azuremlsdk",
        "Question_view_count":225,
        "Owner_creation_time":1639768329610,
        "Owner_last_access_time":1644962247200,
        "Owner_location":"Eden Prarie, MN",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>First open a terminal, using the same compute target as you want to use with your Notebook afterwards, and to use and <strong>existing environment<\/strong> you can do:<\/p>\n<pre><code>conda activate existing_env\nconda install ipykernel\npython -m ipykernel install --user --name existing_env --display-name &quot;Python 3.8 - Existing Environment&quot;   \n<\/code><\/pre>\n<p>However, to create a <strong>new environment<\/strong> and use it in you AzureML Notebook, you have to do the following commands:<\/p>\n<pre><code>conda create --name new_env python=3.8\nconda activate new_env\nconda install pip\nconda install ipykernel\npython -m ipykernel install --user --name new_env --display-name &quot;Python 3.8 - New Environment&quot;\n<\/code><\/pre>\n<p>And then last, but not least, you have to edit the Jupyter Kernel display names:<\/p>\n<p><strong>IMPORTANT<\/strong> Please ensure you are comfortable running all these steps:<\/p>\n<pre><code>jupyter kernelspec list\ncd &lt;folder-that-matches-the-kernel-of-your-environment&gt;\nsudo nano kernel.json\n<\/code><\/pre>\n<p>Then edit the name to match what you want and save the file.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1641553768433,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1641841183276,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70571948",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":34990561,
        "Question_title":"Azure Machine Learning Request Response latency",
        "Question_body":"<p>I have made an Azure Machine Learning Experiment which takes a small dataset (12x3 array) and some parameters and does some calculations using a few Python modules (a linear regression calculation and some more). This all works fine.<\/p>\n\n<p>I have deployed the experiment and now want to throw data at it from the front-end of my application. The API-call goes in and comes back with correct results, but it takes up to 30 seconds to calculate a simple linear regression. Sometimes it is 20 seconds, sometimes only 1 second. I even got it down to 100 ms one time (which is what I'd like), but 90% of the time the request takes more than 20 seconds to complete, which is unacceptable.<\/p>\n\n<p>I guess it has something to do with it still being an experiment, or it is still in a development slot, but I can't find the settings to get it to run on a faster machine.<\/p>\n\n<p>Is there a way to speed up my execution?<\/p>\n\n<p>Edit: To clarify: The varying timings are obtained with the same test data, simply by sending the same request multiple times. This made me conclude it must have something to do with my request being put in a queue, there is some start-up latency or I'm throttled in some other way.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1453718439993,
        "Question_score":8,
        "Question_tags":"python|azure|azure-machine-learning-studio",
        "Question_view_count":1128,
        "Owner_creation_time":1446116840793,
        "Owner_last_access_time":1589929968357,
        "Owner_location":"Antwerp, Belgium",
        "Owner_reputation":311,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Question_last_edit_time":1453911336527,
        "Answer_body":"<p>First, I am assuming you are doing your timing test on the published AML endpoint.<\/p>\n\n<p>When a call is made to the AML the first call must warm up the container. By default a web service has 20 containers. Each container is cold, and a cold container can cause a large(30 sec) delay. In the string returned by the AML endpoint, only count requests that have the <code>isWarm<\/code> flag set to true. By smashing the service with MANY requests(relative to how many containers you have running) can get all your containers warmed.<\/p>\n\n<p>If you are sending out dozens of requests a instance, the endpoint might be getting throttled. You can adjust the number of calls your endpoint can accept by going to manage.windowsazure.com\/<\/p>\n\n<ol>\n<li>manage.windowsazure.com\/<\/li>\n<li>Azure ML Section from left bar<\/li>\n<li>select your workspace<\/li>\n<li>go to web services tab<\/li>\n<li>Select your web service from list<\/li>\n<li>adjust the number of calls with slider<\/li>\n<\/ol>\n\n<p>By enabling debugging onto your endpoint you can get logs about the execution time for each of your modules to complete. You can use this to determine if a module is not running as you intended which may add to the time.<\/p>\n\n<p>Overall, there is an overhead when using the Execute python module, but I'd expect this request to complete in under 3 secs. <\/p>",
        "Answer_comment_count":11,
        "Answer_creation_time":1453832406127,
        "Answer_score":8,
        "Question_favorite_count":5.0,
        "Answer_last_edit_time":1453911048927,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34990561",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":59637596,
        "Question_title":"Where does Python modules installed on Azure Machine Learning Studio",
        "Question_body":"<p>I understand the azure machine learning studio (classic) version using Anaconda distribution but my question is where would the python modules like pandas\/tensorflow are installed when using <strong>IPython interface of Azure ML<\/strong>. Is this on AML studio itself or in azure blob (AML studio uses blob as backend store)? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1578440246977,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-studio",
        "Question_view_count":136,
        "Owner_creation_time":1500744375327,
        "Owner_last_access_time":1660004233300,
        "Owner_location":null,
        "Owner_reputation":255,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":130,
        "Question_last_edit_time":1578440835316,
        "Answer_body":"<p>Here is my screenshots for tabs <code>EXPERIMENTS<\/code> and <code>NOTEBOOKS<\/code> in Azure Machine Learning Studio (classic), as the figures below.<\/p>\n\n<p>Fig 1. I created a <code>Excute Python Script<\/code> module with the code to print the <code>sys.path<\/code> and the real path of <code>pandas<\/code> installed.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/KdkJa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KdkJa.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 2. The <code>View output log<\/code> page of the code in Fig 1 shows <code>EXPERIMENTS<\/code> is a runtime of Anaconda on Windows. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/zo9te.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zo9te.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 3. I created a notebook named <code>demo<\/code> and run the same code as Fig 1, the result shows <code>NOTEBOOKS<\/code> is a runtime of Anaconda on Linux, even the notenooks url is started with <code>notebooks.azure.com<\/code>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/uAGRk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uAGRk.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 4. I used different commands like <code>lsb_release -a<\/code>, <code>fdisk -l<\/code>, <code>lsdev<\/code>, <code>ls \/dev<\/code>, <code>df -a<\/code> to try to see the Linux version and its disk or partition information, the result shows it's a Ubuntu Linux container.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/R4wC6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/R4wC6.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Other infomation what you want to know, you can try to check by yourself.<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1578465543193,
        "Answer_score":0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59637596",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":37519858,
        "Question_title":"What type should the returned scores from an R scoring script?",
        "Question_body":"<p>I am attempting to develop an Azure ML experiment that uses R to perform predictions of a continuous response variable. The initial experiment is relatively simple, incorporating only a few experiment items, including \"Create R Model\", \"Train Model\" and \"Score Model\", along with some data input.<\/p>\n\n<p>I have written a training script and a scoring script, both of which appear to execute without errors when I run the experiment within ML Studio. However, when I examine the scored dataset, the score values are all missing values. So I am concerned that my scoring script could be returning scores incorrectly. Can anyone advise what type I should be returning? Is it meant to be a single column data.frame, or something else?<\/p>\n\n<p>It is also possible that my scores are not being properly calculated within the scoring script, although I have run the training and scoring scripts within R Studio, which shows the expected results. It would also be helpful if someone could suggest how to perform debugging of my scoring script in some way, so that I could determine whereabouts the code is failing to behave as expected.<\/p>\n\n<p>Thanks, Paul<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1464593030317,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":238,
        "Owner_creation_time":1464571689107,
        "Owner_last_access_time":1470182061490,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Try using this sample and compare with yours - <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Compare-Sample-5-in-R-vs-Azure-ML-1\" rel=\"nofollow\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Compare-Sample-5-in-R-vs-Azure-ML-1<\/a><\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1464675335140,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37519858",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73702976,
        "Question_title":"How to split Azure ML pipeline steps to debug",
        "Question_body":"<p>I have created an Azure ML pipeline with different steps (data preprocess, train, validation ...). And for pass data from one step to the next I have used the PipelineData object.<\/p>\n<p><em>Example passing the model from train step to validate one:<\/em><\/p>\n<pre><code>    # Create a PipelineData to pass model from train to register\n    model_path = PipelineData('model')\n\n    # Step 2\n    train_step = PythonScriptStep(\n        name = 'Train the Model',\n        script_name = 'TrainStepScript.py',\n        source_directory = source_folder,\n        arguments = ['--training_data', prepped_data,\n                     '--model_name', model_name,\n                     '--model_path', model_path],\n        outputs = [model_path],\n        compute_target = compute_target,\n        runconfig = aml_run_config,\n        allow_reuse = True\n    )\n\n    # Step 3\n    third_step = PythonScriptStep(\n        name = 'Evaluate &amp; register the Model',\n        script_name = 'ValidateStepScript.py',\n        source_directory = source_folder,\n        arguments = ['--model_name', model_name,\n                     '--model_path', model_path],\n        inputs = [model_path],\n        compute_target = compute_target,\n        runconfig = aml_run_config,\n        allow_reuse = True\n    )\n<\/code><\/pre>\n<p>Now for debugging and development purposes I want to create a script to run separately the different steps using a ScriptRunConfig (with the same environment and arguments of the StepScript in the pipeline). But the problem is I don't know how to simulate the data input\/output of each step, because the DataPipeline object is not working for this purpose.<\/p>\n<p>Just for clarification, my goal is to NOT modify the original pipeline StepScripts, so I can use them after debugging in the final pipeline. To sum up, my question is: how can I emulate the DataPipeline object (if possible) in this case?<\/p>\n<p><em>Example of what I'm trying to build:<\/em><\/p>\n<pre><code># Passing in some way the model path (from local)\nmodel_path = PipelineData('model')\n\n# Create a script config for validate step\nvalidate_script_config = ScriptRunConfig(\n    source_directory = source_folder,\n    script = 'ValidateStepScript.py',\n    arguments = ['--model_name', model_name,\n                 '--model_path', model_path],\n    environment = experiment_env,\n    docker_runtime_config = DockerConfiguration(use_docker=True)\n)\n\nexperiment = Experiment(workspace=ws, name=experiment_name)\ndata_run = experiment.submit(config=data_script_config)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663071829967,
        "Question_score":0,
        "Question_tags":"python|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":41,
        "Owner_creation_time":1661942018833,
        "Owner_last_access_time":1663930344050,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>We can download the output of the model in a repository and make them as the source file for the later steps as required. The below code block can be incorporated in the same pipeline which is being used now.<\/p>\n<p>To download the model output:<\/p>\n<pre><code>train_step = pipeline_run1.find_step_run('train.py')\n\nif train_step:\n    train_step_obj = train_step[0] \n    train_step_obj.get_output_data('processed_data1').download(&quot;.\/outputs&quot;) # download the output to current directory\n<\/code><\/pre>\n<p>after downloading the model, then use that as the parent source directory in source_directory<\/p>\n<pre><code>from azureml.core import ScriptRunConfig, Experiment\n   # create or load an experiment\n   experiment = Experiment(workspace, 'MyExperiment')\n   # create or retrieve a compute target\n   cluster = workspace.compute_targets['MyCluster']\n   # create or retrieve an environment\n   env = Environment.get(ws, name='MyEnvironment')\n   # configure and submit your training run\n   config = ScriptRunConfig(source_directory='.',\n                            command=['python', 'train.py'],\n                            compute_target=cluster,\n                            environment=env)\n   script_run = experiment.submit(config)\n<\/code><\/pre>",
        "Answer_comment_count":4,
        "Answer_creation_time":1663156202290,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73702976",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":42844593,
        "Question_title":"Comparing brier score for Azure ML classifier",
        "Question_body":"<p>I'm trying to compare the brier score for two classifiers in Azure ML studio:<\/p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import brier_score_loss\n\ndef azureml_main(dataframe1, dataframe2):\n    colnames_1 = dataframe1.columns\n    y_true_1 = np.array(dataframe1[colnames_1[1]])\n    y_prob_1 = np.array(dataframe1[colnames_1[-1]])\n    brier_score_1 = brier_score_loss(y_true_1, y_prob_1)\n\n    colnames_2 = dataframe2.columns\n    y_true_2 = np.array(dataframe2[colnames_2[1]])\n    y_prob_2 = np.array(dataframe2[colnames_2[-1]])\n    brier_score_2 = brier_score_loss(y_true_2, y_prob_2)\n\n    data = {'brier_score': [brier_score_1, brier_score_2]}\n    result = pd.DataFrame(data, columns=['brier_score'])\n\n    return result\n<\/code><\/pre>\n\n<p>My problem is that the script only outputs a value in the first row with the brier score of the first dataset. The second row is empty. This is how I have connected the script: \n<img src=\"https:\/\/anonimag.es\/i\/azure0f4ae.png\" alt=\"azure\"><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1489697707193,
        "Question_score":0,
        "Question_tags":"python|python-3.x|azure|azure-machine-learning-studio",
        "Question_view_count":97,
        "Owner_creation_time":1424620848827,
        "Owner_last_access_time":1663938801623,
        "Owner_location":null,
        "Owner_reputation":2026,
        "Owner_up_votes":599,
        "Owner_down_votes":15,
        "Owner_views":130,
        "Question_last_edit_time":1489716097823,
        "Answer_body":"<p>I turned out that the problem was caused by a few NaN values in the second dataframe.\nAdding <code>dataframe2 = dataframe2.dropna()<\/code> to the top of the script solved the problem.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1490900323347,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/42844593",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":38927230,
        "Question_title":"Panda AssertionError columns passed, passed data had 2 columns",
        "Question_body":"<p>I am working on Azure ML implementation on text analytics with NLTK, the following execution is throwing <\/p>\n\n<pre><code>AssertionError: 1 columns passed, passed data had 2 columns\\r\\nProcess returned with non-zero exit code 1\n<\/code><\/pre>\n\n<p>Below is the code <\/p>\n\n<pre><code># The script MUST include the following function,\n# which is the entry point for this module:\n# Param&lt;dataframe1&gt;: a pandas.DataFrame\n# Param&lt;dataframe2&gt;: a pandas.DataFrame\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    # import required packages\n    import pandas as pd\n    import nltk\n    import numpy as np\n    # tokenize the review text and store the word corpus\n    word_dict = {}\n    token_list = []\n    nltk.download(info_or_id='punkt', download_dir='C:\/users\/client\/nltk_data')\n    nltk.download(info_or_id='maxent_treebank_pos_tagger', download_dir='C:\/users\/client\/nltk_data')\n    for text in dataframe1[\"tweet_text\"]:\n        tokens = nltk.word_tokenize(text.decode('utf8'))\n        tagged = nltk.pos_tag(tokens)\n\n\n      # convert feature vector to dataframe object\n    dataframe_output = pd.DataFrame(tagged, columns=['Output'])\n    return [dataframe_output]\n<\/code><\/pre>\n\n<p>Error is throwing here <\/p>\n\n<pre><code> dataframe_output = pd.DataFrame(tagged, columns=['Output'])\n<\/code><\/pre>\n\n<p>I suspect this to be the tagged data type passed to dataframe, can some one let me know the right approach to add this to dataframe.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1471040597197,
        "Question_score":7,
        "Question_tags":"python|pandas|dataframe|nltk|azure-machine-learning-studio",
        "Question_view_count":48200,
        "Owner_creation_time":1370924418390,
        "Owner_last_access_time":1663478900357,
        "Owner_location":"Toronto, ON, Canada",
        "Owner_reputation":1748,
        "Owner_up_votes":136,
        "Owner_down_votes":55,
        "Owner_views":339,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Try this:<\/p>\n\n<pre><code>dataframe_output = pd.DataFrame(tagged, columns=['Output', 'temp'])\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1471040769603,
        "Answer_score":13,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/38927230",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58035744,
        "Question_title":"AML run.log() and run.log_list() fail without error",
        "Question_body":"<p>I have a Pipeline with DatabricksSteps each containing:<\/p>\n\n<pre><code>from azureml.core.run import Run\nrun = Run.get_context()\n#do stuff\nrun.log(name, val, desc)\nrun.log_list(name, vals, desc)\nrun.log_image(title, fig, desc)\n<\/code><\/pre>\n\n<p>Only <code>log_image()<\/code> seems to work.  The image appears in the \"images\" section of the AML experiment workspace as expected, but the \"tracked metrics\" and \"charts\" areas are blank.  In an interactive job, <code>run.log()<\/code> and <code>run.log_list()<\/code> work as expected.  I tested that there is no problem with the arguments by using <code>print()<\/code> instead of <code>run.log()<\/code>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1569018204223,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":121,
        "Owner_creation_time":1465320834943,
        "Owner_last_access_time":1617290067470,
        "Owner_location":"Redmond, WA, USA",
        "Owner_reputation":677,
        "Owner_up_votes":13,
        "Owner_down_votes":2,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Add run.flush() at the end of the script.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1569427861030,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58035744",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":56024354,
        "Question_title":"\"Session has expired\" message with Machine Learning Studio",
        "Question_body":"<p>I am getting consistent error \"Your session has expired\" (screenshot below), after logging in to machine learning studio. <\/p>\n\n<p>I have tried chrome incognito and guest windows, but no difference. <\/p>\n\n<p>I am using a new account and have signed up for Free workspace. Any suggestion to get past this or delete workspace, to start again?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/fsqtw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fsqtw.png\" alt=\"Error screenshot\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1557237613340,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":482,
        "Owner_creation_time":1255194026493,
        "Owner_last_access_time":1617715031493,
        "Owner_location":"Bedford, MA, USA",
        "Owner_reputation":300,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":63,
        "Question_last_edit_time":1557240710416,
        "Answer_body":"<p>I can reproduce your issue, I sign out and log in <a href=\"https:\/\/studio.azureml.net\/\" rel=\"nofollow noreferrer\">https:\/\/studio.azureml.net\/<\/a> again, it solved my problem. Or you can try to clear the browsing data or change a browser. Anyway, the issue should be caused by the browser, not azure. Even if your account is not the owner of the workspace, when you click <code>Sign In<\/code> in <a href=\"https:\/\/studio.azureml.net\/\" rel=\"nofollow noreferrer\">https:\/\/studio.azureml.net\/<\/a> , it will create a free workspace(with a different workspace id) for you automatically.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/MXDJC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/MXDJC.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>If you want to delete the workspace, you need to let the owner of the workspace delete it, navigate to the <code>SETTINGS<\/code> on the left of the studio -> <code>NAME<\/code> -> <code>DELETE WORKSPACE<\/code>. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/E6aUl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E6aUl.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1557300039457,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1557300456467,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56024354",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":38119062,
        "Question_title":"Pulling data from Stream Analytics to Azure Machine Learning",
        "Question_body":"<p>Working on a IoT telemetry project that receives humidity and weather pollution data from different sites on the field. I will then apply Machine Learning on the collected data. I'm using Event Hubs and Stream Analytics. Is there a way of pulling the data to Azure Machine Learning without the hassle of writing an application to get it from Stream Analytics and push to AML web service?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1467278958850,
        "Question_score":0,
        "Question_tags":"azure|iot|azure-stream-analytics|azure-machine-learning-studio",
        "Question_view_count":627,
        "Owner_creation_time":1311017514580,
        "Owner_last_access_time":1664027428383,
        "Owner_location":"Beirut, Lebanon",
        "Owner_reputation":408,
        "Owner_up_votes":88,
        "Owner_down_votes":6,
        "Owner_views":32,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Stream Analytics has a functionality called the \u201c<a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/\" rel=\"nofollow\">Functions<\/a>\u201d. You can call any web service you\u2019ve published using AML from within Stream Analytics and apply it within your Stream Analytics query. Check this <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/stream-analytics-machine-learning-integration-tutorial\/\" rel=\"nofollow\">link for a tutorial<\/a>.\nExample workflow in your case would be like the following;<\/p>\n\n<ul>\n<li>Telemetry arrives and reaches Stream Analytics<\/li>\n<li>Streaming Analytics (SA) calls the Machine Learning function to apply it on the data<\/li>\n<li>SA redirects it to the output accordingly, here you can use the PowerBI to create a predictions dashboards.<\/li>\n<\/ul>\n\n<p>Another way would be using R, and here\u2019s a good tutorial showing that <a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/\" rel=\"nofollow\">https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/<\/a> . \nIt is more work of course but can give you more control as you control the code.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1467284456457,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/38119062",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64597526,
        "Question_title":"Provision AKS with internal load balancer from AMLS on Azure",
        "Question_body":"<p>I would like to provision an AKS cluster that is connected to a vnet and has an internal load balancer on Azure. I am using code from <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-inferencing-vnet?tabs=python\" rel=\"nofollow noreferrer\">here<\/a> that looks like this:<\/p>\n<pre><code>import azureml.core\nfrom azureml.core.compute import AksCompute, ComputeTarget\n\n# Verify that cluster does not exist already\ntry:\n    aks_target = AksCompute(workspace=ws, name=aks_cluster_name)\n    print(&quot;Found existing aks cluster&quot;)\n\nexcept:\n    print(&quot;Creating new aks cluster&quot;)\n\n    # Subnet to use for AKS\n    subnet_name = &quot;default&quot;\n    # Create AKS configuration\n    prov_config=AksCompute.provisioning_configuration(load_balancer_type=&quot;InternalLoadBalancer&quot;)\n    # Set info for existing virtual network to create the cluster in\n    prov_config.vnet_resourcegroup_name = &quot;myvnetresourcegroup&quot;\n    prov_config.vnet_name = &quot;myvnetname&quot;\n    prov_config.service_cidr = &quot;10.0.0.0\/16&quot;\n    prov_config.dns_service_ip = &quot;10.0.0.10&quot;\n    prov_config.subnet_name = subnet_name\n    prov_config.docker_bridge_cidr = &quot;172.17.0.1\/16&quot;\n\n    # Create compute target\n    aks_target = ComputeTarget.create(workspace = ws, name = &quot;myaks&quot;, provisioning_configuration = prov_config)\n    # Wait for the operation to complete\n    aks_target.wait_for_completion(show_output = True)\n<\/code><\/pre>\n<p>However, I get the following error<\/p>\n<pre><code>K8s failed to assign an IP for Load Balancer after waiting for an hour.\n<\/code><\/pre>\n<p>Is this because the AKS cluster does not yet have a 'network contributor' role for the vnet resource group? Is the only way to get this to work to first create AKS outside of AMLS, grant the network contributor role to the vnet resource group, then attach the AKS cluster to AMLS and configure the internal load balancer afterwards?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603997735143,
        "Question_score":1,
        "Question_tags":"azure-aks|azure-machine-learning-service|vnet|internal-load-balancer",
        "Question_view_count":352,
        "Owner_creation_time":1589738451347,
        "Owner_last_access_time":1656358607687,
        "Owner_location":null,
        "Owner_reputation":179,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":53,
        "Question_last_edit_time":1604002322987,
        "Answer_body":"<p>I was able to get this to work by first creating an AKS resource without an internal load balancer, then separately updating the load balancer following this code:<\/p>\n<pre><code>import azureml.core\nfrom azureml.core.compute.aks import AksUpdateConfiguration\nfrom azureml.core.compute import AksCompute\n\n# ws = workspace object. Creation not shown in this snippet\naks_target = AksCompute(ws,&quot;myaks&quot;)\n\n# Change to the name of the subnet that contains AKS\nsubnet_name = &quot;default&quot;\n# Update AKS configuration to use an internal load balancer\nupdate_config = AksUpdateConfiguration(None, &quot;InternalLoadBalancer&quot;, subnet_name)\naks_target.update(update_config)\n# Wait for the operation to complete\naks_target.wait_for_completion(show_output = True)\n<\/code><\/pre>\n<p>No network contributor role was required.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1604359053333,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64597526",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70268372,
        "Question_title":"How to adjust feature importance in Azure AutoML",
        "Question_body":"<p>I am hoping to have some <strong>low code model<\/strong> using Azure AutoML, which is really just going to the AutoML tab, running a classification experiment with my dataset, after it's done, I deploy the best selected model.<\/p>\n<p>The model kinda works (meaning, I publish the endpoint and then I do some manual validation, seems accurate), however, I am not confident enough, because when I am looking at the explanation, I can see something like this:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qM51x.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qM51x.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>4 top features are not really closely important. The most &quot;important&quot; one is really not the one I prefer it to use. I am hoping it will use the <code>Title<\/code> feature more.<\/p>\n<p>Is there such a thing I can adjust the importance of individual features, like ranking all features before it starts the experiment?<\/p>\n<p>I would love to do more reading, but I only found this:<\/p>\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/52484233\/increase-feature-importance\">Increase feature importance<\/a><\/p>\n<p>The only answer seems to be about how to measure if a feature is important.<\/p>\n<p>Hence, does it mean, if I want to customize the experiment, such as selecting which features to &quot;focus&quot;, I should learn how to use the &quot;designer&quot; part in Azure ML? Or is it something I can't do, even with the designer. I guess my confusion is, with ML being such a big topic, I am looking for a direction of learning, in this case of what I am having, so I can improve my current model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638921326880,
        "Question_score":2,
        "Question_tags":"machine-learning|azure-machine-learning-studio|azure-machine-learning-service|azure-auto-ml",
        "Question_view_count":119,
        "Owner_creation_time":1296571549840,
        "Owner_last_access_time":1663809103810,
        "Owner_location":"Seattle, WA",
        "Owner_reputation":1021,
        "Owner_up_votes":79,
        "Owner_down_votes":7,
        "Owner_views":138,
        "Question_last_edit_time":1638925038329,
        "Answer_body":"<p>Here is <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-automated-ml#customize-featurization\" rel=\"nofollow noreferrer\">link<\/a> to the document for feature customization.<\/p>\n<p>Using the SDK you can specify &quot;feauturization&quot;: 'auto' \/ 'off' \/ 'FeaturizationConfig' in your <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py\" rel=\"nofollow noreferrer\">AutoMLConfig<\/a> object. Learn more about <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-features\" rel=\"nofollow noreferrer\">enabling featurization<\/a>.<\/p>\n<p>Automated ML tries out different ML models that have different settings which control for overfitting.  Automated ML will pick which overfitting parameter configuration is best based on the best score (e.g. accuracy) it gets from hold-out data.  The kind of overfitting settings these models has includes:<\/p>\n<ul>\n<li>Explicitly penalizing overly-complex models in the loss function that the ML model is optimizing<\/li>\n<li>Limiting model complexity before training, for example by limiting the size of trees in an ensemble tree learning model (e.g. gradient boosting trees or random forest)<\/li>\n<\/ul>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1641210936730,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70268372",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":68505595,
        "Question_title":"failing to create image in azure ml workspace",
        "Question_body":"<p>I am able to create image and run azure ml service in one env but when I am moving to another env its not able to create image and failing with this error -<\/p>\n<p>Message: Received bad response from Model Management Service:\nResponse Code: 500\n{&quot;code&quot;:&quot;InternalServerError&quot;,&quot;statusCode&quot;:500,&quot;message&quot;:&quot;An internal server error occurred. Please try again. If the problem persists, contact support.&quot;,&quot;correlation&quot;:{&quot;RequestId&quot;:&quot;8667981d-ef71-4e7c-a735-c43ef07b51b8&quot;}}'<\/p>\n<p>these logs are not helpful to find issue<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1627079442010,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":110,
        "Owner_creation_time":1567209656790,
        "Owner_last_access_time":1663349187993,
        "Owner_location":null,
        "Owner_reputation":417,
        "Owner_up_votes":53,
        "Owner_down_votes":0,
        "Owner_views":233,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As the error message said, this issue is an internal issue, please raise a support ticket to assign a support engineer to investigate it.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1628037949483,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1629745090063,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68505595",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64816630,
        "Question_title":"AuthenticationException when creating Azure ML Dataset from Azure Data Lake Gen2 Datastore",
        "Question_body":"<p>I have an Azure Data Lake Gen2 with public endpoint and a standard Azure ML instance.\nI have created both components with my user and I am listed as Contributor.<\/p>\n<p>I want to use data from this data lake in Azure ML.<\/p>\n<p>I have added the data lake as a Datastore using Service Principal authentication.<\/p>\n<p>I then try to create a Tabular Dataset using the Azure ML GUI I get the following error:<\/p>\n<p>Access denied\nYou do not have permission to the specified path or file.<\/p>\n<pre><code>{\n  &quot;message&quot;: &quot;ScriptExecutionException was caused by StreamAccessException.\\n  StreamAccessException was caused by AuthenticationException.\\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for '[REDACTED]' on storage failed with status code 'Forbidden' (This request is not authorized to perform this operation using this permission.), client request ID '1f9e329b-2c2c-49d6-a627-91828def284e', request ID '5ad0e715-a01f-0040-24cb-b887da000000'. Error message: [REDACTED]\\n&quot;\n}\n<\/code><\/pre>\n<p>I have tried having our Azure Portal Admin, with Admin access to both Azure ML and Data Lake try the same and she gets the same error.<\/p>\n<p>I tried creating the Dataset using Python sdk and get a similar error:<\/p>\n<pre><code>ExecutionError: \nError Code: ScriptExecution.StreamAccess.Authentication\nFailed Step: 667ddfcb-c7b1-47cf-b24a-6e090dab8947\nError Message: ScriptExecutionException was caused by StreamAccessException.\n  StreamAccessException was caused by AuthenticationException.\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for 'https:\/\/mydatalake.dfs.core.windows.net\/mycontainer?directory=mydirectory\/csv&amp;recursive=true&amp;resource=filesystem' on storage failed with status code 'Forbidden' (This request is not authorized to perform this operation using this permission.), client request ID 'a231f3e9-b32b-4173-b631-b9ed043fdfff', request ID 'c6a6f5fe-e01f-0008-3c86-b9b547000000'. Error message: {&quot;error&quot;:{&quot;code&quot;:&quot;AuthorizationPermissionMismatch&quot;,&quot;message&quot;:&quot;This request is not authorized to perform this operation using this permission.\\nRequestId:c6a6f5fe-e01f-0008-3c86-b9b547000000\\nTime:2020-11-13T06:34:01.4743177Z&quot;}}\n| session_id=75ed3c11-36de-48bf-8f7b-a0cd7dac4d58\n<\/code><\/pre>\n<p>I have created Datastore and Datasets of both a normal blob storage and a managed sql database with no issues and I have only contributor access to those so I cannot understand why I should not be Authorized to add data lake. The fact that our admin gets the same error leads me to believe there are some other issue.<\/p>\n<p>I hope you can help me identify what it is or give me some clue of what more to test.<\/p>\n<p>Edit:\nI see I might have duplicated this post: <a href=\"https:\/\/stackoverflow.com\/questions\/63891547\/how-to-connect-amls-to-adls-gen-2\">How to connect AMLS to ADLS Gen 2?<\/a>\nI will test that solution and close this post if it works<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1605250166337,
        "Question_score":2,
        "Question_tags":"azure|azure-active-directory|azure-data-lake|azure-data-lake-gen2|azure-machine-learning-service",
        "Question_view_count":3179,
        "Owner_creation_time":1288196087393,
        "Owner_last_access_time":1663939895430,
        "Owner_location":"Oslo, Norge",
        "Owner_reputation":1010,
        "Owner_up_votes":1344,
        "Owner_down_votes":1,
        "Owner_views":119,
        "Question_last_edit_time":1605254071243,
        "Answer_body":"<p>This was actually a duplicate of <a href=\"https:\/\/stackoverflow.com\/questions\/63891547\/how-to-connect-amls-to-adls-gen-2\">How to connect AMLS to ADLS Gen 2?<\/a>.<\/p>\n<p>The solution is to give the service principal that Azure ML uses to access the data lake the Storage Blob Data Reader access. And note you have to wait at least some minutes for this to have effect.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1605260557303,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64816630",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":57596224,
        "Question_title":"How to expose port from locally-deployed AzureML container?",
        "Question_body":"<p>I want to be able to debug a running <code>entry_script.py<\/code> script in VSCode. This code runs in the container created through <code>az ml deploy<\/code> with its own docker run command. This is a local deployment so I'm using a deployment config that looks like this:<\/p>\n\n<pre><code>{\n    \"computeType\": \"LOCAL\",\n    \"port\": 32267\n}\n<\/code><\/pre>\n\n<p>I was thinking about using <code>ptvsd<\/code> to set up a VSCode server but I need to also expose\/map the 5678 port in addition to that 32267 port for the endpoint itself. So it's not clear to me how to map an additional exposed port (typically using the <code>-p<\/code> or <code>-P<\/code> flags in the <code>docker run<\/code> command). <\/p>\n\n<p>Sure, I can <code>EXPOSE<\/code> it in the <code>extra_dockerfile_steps<\/code> configuration but that won't actually map it to a host port that I can connect to\/attach to in VSCode.<\/p>\n\n<p>I tried to determine the run command and maybe modify it but I couldn't find out what that run command is. If I knew how to run the image that's created through AzureML local deployment then I could modify these flags. <\/p>\n\n<p>Ultimately it felt too hacky - if there was a more supported way through <code>az ml deploy<\/code> or through the deployment configuration that would be preferred.<\/p>\n\n<p>This is the code I'm using at the start of the entry_script to enable attachment via <code>ptvsd<\/code>: <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># 5678 is the default attach port in the VS Code debug configurations\nprint(\"Waiting for debugger attach\")\nptvsd.enable_attach(address=('localhost', 5678), redirect_output=True)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1566406835043,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":206,
        "Owner_creation_time":1254279877887,
        "Owner_last_access_time":1663949658720,
        "Owner_location":"Seattle, WA",
        "Owner_reputation":153,
        "Owner_up_votes":101,
        "Owner_down_votes":1,
        "Owner_views":111,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Unfortunately az ml deploy local doesn't support binding any ports other then the port hosting the scoring server. <\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1566419274223,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57596224",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":40007515,
        "Question_title":"Azure Machine Learning Studio: how to add a dataset from a local Excel file?",
        "Question_body":"<p>Despite prominent how-to posts on how to add datasets to Azure Machine Learning that say Excel is supported, when I actually go to add a dataset and select a local Excel file, there's no option for \"Excel\" in the required datatype property dropdown. I'm surprised that Azure wouldn't support Excel (right?) - am I missing something?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1476303145067,
        "Question_score":2,
        "Question_tags":"excel|azure|machine-learning|azure-machine-learning-studio",
        "Question_view_count":1250,
        "Owner_creation_time":1357592818807,
        "Owner_last_access_time":1621964227647,
        "Owner_location":"Ann Arbor, MI",
        "Owner_reputation":99,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The dropdown list indicates the \"Destination\" datatype for the new DATASET file you are creating, not the source type.<\/p>\n\n<p>I just uploaded a <code>.xlsx<\/code> file successfully into a <code>.CSV<\/code> file in AML.<\/p>",
        "Answer_comment_count":6,
        "Answer_creation_time":1476307260447,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1476377527560,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40007515",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62140446,
        "Question_title":"Dependency missing when running AzureML Estimator in docker environment",
        "Question_body":"<h3>Scenario description<\/h3>\n\n<p>I'm trying to submit a training script to AzureML (want to use AmlCompute, but I'm starting\/testing locally first, for debugging purposes).<\/p>\n\n<p>The <code>train.py<\/code> script I have uses a custom package (<code>arcus.ml<\/code>) and I believe I have specified the right settings and dependencies, but still I get the error: <\/p>\n\n<p><code>User program failed with ModuleNotFoundError: No module named 'arcus.ml'<\/code><\/p>\n\n<h3>Code and reproduction<\/h3>\n\n<p>This the python code I have:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>name='test'\nscript_params = {\n    '--test-par': 0.2\n}\n\nest = Estimator(source_directory='.\/' + name,\n                   script_params=script_params,\n                   compute_target='local',\n                   entry_script='train.py',\n                   pip_requirements_file='requirements.txt',\n                   conda_packages=['scikit-learn','tensorflow', 'keras'])\n\nrun = exp.submit(est)\nprint(run.get_portal_url())\n<\/code><\/pre>\n\n<p>This is the (fully simplified) train.py script in the <code>test<\/code>directory:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from arcus.ml import dataframes as adf\nfrom azureml.core import Workspace, Dataset, Datastore, Experiment, Run\n\n# get hold of the current run\nrun = Run.get_context()\nws = run.get_environment()\n\nprint('training finished')\n<\/code><\/pre>\n\n<p>And this is my requirements.txt file<\/p>\n\n<pre><code>arcus-azureml\narcus-ml\nnumpy\npandas\nazureml-core\ntqdm\njoblib\nscikit-learn\nmatplotlib\ntensorflow\nkeras\n<\/code><\/pre>\n\n<h3>Logs<\/h3>\n\n<p>In the logs file of the run, I can see this section, sot it seems the external module is being installed anyhow.<\/p>\n\n<pre><code>Collecting arcus-azureml\n  Downloading arcus_azureml-1.0.3-py3-none-any.whl (3.1 kB)\nCollecting arcus-ml\n  Downloading arcus_ml-1.0.6-py3-none-any.whl (2.1 kB)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1591043054577,
        "Question_score":1,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":208,
        "Owner_creation_time":1360655430743,
        "Owner_last_access_time":1663784892907,
        "Owner_location":"Belgium",
        "Owner_reputation":2947,
        "Owner_up_votes":297,
        "Owner_down_votes":16,
        "Owner_views":355,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I think this error isn't necessarily about Azure ML. I think the error has to do w\/ the difference b\/w using a hyphen and a period in your package name. But I'm a python packaging newb. \nIn a new conda environment on my laptop, I ran the following<\/p>\n\n<pre><code>&gt; conda create -n arcus python=3.6 -y\n&gt; conda activate arcus\n&gt; pip install arcus-ml\n&gt; python\n&gt;&gt;&gt; from arcus.ml import dataframes as adf\nModuleNotFoundError: No module named 'arcus'\n<\/code><\/pre>\n\n<p>When I look in the env's site packages folder, I didn't see the <code>arcus\/ml<\/code> folder structure I was expecting. There's no arcus code there at all, only the <code>.dist-info<\/code> file<\/p>\n\n<h3><code>~\/opt\/anaconda3\/envs\/arcus\/lib\/python3.6\/site-packages<\/code><\/h3>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/caExn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/caExn.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1591043858447,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1591044471110,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62140446",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":57600154,
        "Question_title":"How to correctly specify a private ACR Docker image in an Azure ML Pipeline?",
        "Question_body":"<p>I created a private Azure Container Registry, and pushed a docker image to that registry. I was trying to understand the correct way to access that registry in my pipeline, and my understanding was that I needed to set the following info in the run configuration:<\/p>\n\n<pre><code>        run_config.environment.docker.base_image = \"myprivateacr.azurecr.io\/mydockerimage:0.0.1\"\n        run_config.environment.docker.base_image_registry.username = \"MyPrivateACR\"\n        run_config.environment.docker.base_image_registry.password = \"&lt;the password for the registry&gt;\"\n<\/code><\/pre>\n\n<p>Let's assume that I correctly provided the username and password. Any idea why this didn't work? Or: is there an example of a pipeline notebook that uses a docker image that's in a private docker registry, and thus deals with this type of authentication issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1566428573660,
        "Question_score":0,
        "Question_tags":"docker|azure-machine-learning-service",
        "Question_view_count":278,
        "Owner_creation_time":1491928725063,
        "Owner_last_access_time":1583476614733,
        "Owner_location":"Redmond, WA, United States",
        "Owner_reputation":45,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There's a separate address property for a custom image registry. Try specifying it this way:<\/p>\n\n<pre><code>run_config.environment.docker.base_image = \"mydockerimage:0.0.1\"\nrun_config.environment.docker.base_image_registry.address = \"myprivateacr.azurecr.io\"\nrun_config.environment.docker.base_image_registry.username = \"MyPrivateACR\"\nrun_config.environment.docker.base_image_registry.password = \"&lt;the password for the registry&gt;\"\n<\/code><\/pre>",
        "Answer_comment_count":1,
        "Answer_creation_time":1566481255417,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57600154",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":55837639,
        "Question_title":"How to enable authentication for an ACI webservice in Azure Machine Learning service?",
        "Question_body":"<p>I am able to deploy a Azure Machine learning prediction service in my workspace <code>ws<\/code> using the syntax<\/p>\n\n<pre><code>aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                                               memory_gb=8, \n                                               tags={\"method\" : \"some method\"}, \n                                               description='Predict something')\n<\/code><\/pre>\n\n<p>and then<\/p>\n\n<pre><code>service = Webservice.deploy_from_image(deployment_config = aciconfig,\n                                       image = image,\n                                       name = service_name,\n                                       workspace = ws)\n<\/code><\/pre>\n\n<p>as described in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#aci\" rel=\"nofollow noreferrer\">documentation<\/a>.<br>\nHowever, this exposes a service publicly and this is not really optimal.<\/p>\n\n<p>What's the easiest way to shield the ACI service? I understand that passing an <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.aciwebservice?view=azure-ml-py#deploy-configuration-cpu-cores-none--memory-gb-none--tags-none--properties-none--description-none--location-none--auth-enabled-none--ssl-enabled-none--enable-app-insights-none--ssl-cert-pem-file-none--ssl-key-pem-file-none--ssl-cname-none-\" rel=\"nofollow noreferrer\"><code>auth_enabled=True<\/code><\/a> parameter may do the job, but then how can I instruct a client (say, using <code>curl<\/code> or Postman) to use the service afterwards? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1556135731340,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-service|azure-container-instances",
        "Question_view_count":676,
        "Owner_creation_time":1415722650717,
        "Owner_last_access_time":1664051478173,
        "Owner_location":"Verona, VR, Italy",
        "Owner_reputation":4811,
        "Owner_up_votes":376,
        "Owner_down_votes":73,
        "Owner_views":713,
        "Question_last_edit_time":1556186547292,
        "Answer_body":"<p>See <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service#call-the-service-c\" rel=\"nofollow noreferrer\">here<\/a> for an example (in C#). When you enable auth, you will need to send the API key in the \"Authorization\" header in the HTTP request:<\/p>\n\n<pre><code>client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", authKey);\n<\/code><\/pre>\n\n<p>See <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service#authentication-key\" rel=\"nofollow noreferrer\">here<\/a> how to retrieve the key.<\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1556182170523,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1556183204092,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55837639",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72083832,
        "Question_title":"Send alert if Azure ML pipeline fails",
        "Question_body":"<p>I am trying to add an alert if Azure ML pipeline fails. It looks that one of the ways is to create a monitor in the Azure Portal. The problem is that I cannot find a correct signal name (required when setting up condition), which would identify pipeline fail. What signal name should I use? Or is there another way to send an email if Azure pipeline fails?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651478303433,
        "Question_score":1,
        "Question_tags":"azure|azureportal|azure-machine-learning-service",
        "Question_view_count":158,
        "Owner_creation_time":1313536247313,
        "Owner_last_access_time":1662987058983,
        "Owner_location":"Vilnius, Lithuania",
        "Owner_reputation":563,
        "Owner_up_votes":76,
        "Owner_down_votes":4,
        "Owner_views":108,
        "Question_last_edit_time":1651956277630,
        "Answer_body":"<blockquote>\n<p>What signal name should I use?<\/p>\n<\/blockquote>\n<p>You can use <code>PipelineChangeEvent<\/code> category of <code>AmlPipelineEvent<\/code> table to view events when ML pipeline draft or endpoint or module are accessed (read, created, or deleted).<\/p>\n<p>For example, according to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/monitor-azure-machine-learning#analyzing-logs\" rel=\"nofollow noreferrer\">documentation<\/a>, use <code>AmlComputeJobEvent<\/code> to get failed jobs in the last five days:<\/p>\n<pre><code>AmlComputeJobEvent\n| where TimeGenerated &gt; ago(5d) and EventType == &quot;JobFailed&quot;\n| project  TimeGenerated , ClusterId , EventType , ExecutionState , ToolType\n<\/code><\/pre>\n<p><strong>Updated answer:<\/strong><\/p>\n<p>According to <a href=\"https:\/\/stackoverflow.com\/users\/897665\/laurynas-g\">Laurynas G<\/a>:<\/p>\n<pre><code>AmlRunStatusChangedEvent \n| where Status == &quot;Failed&quot; or Status == &quot;Canceled&quot;\n<\/code><\/pre>\n<p>You can refer to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/monitor-azure-machine-learning#analyzing-logs\" rel=\"nofollow noreferrer\">Monitor Azure Machine Learning<\/a>, <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics\" rel=\"nofollow noreferrer\">Log &amp; view metrics and log files<\/a> and <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-debug-pipelines\" rel=\"nofollow noreferrer\">Troubleshooting machine learning pipelines<\/a><\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1651723432757,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1658464027763,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72083832",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":37807158,
        "Question_title":"Train multiple models with various measures and accumulate predictions",
        "Question_body":"<p>So I have been playing around with Azure ML lately, and I got one dataset where I have multiple values I want to predict. All of them uses different algorithms and when I try to train multiple models within one experiment; it says the \u201ctrain model can only predict one value\u201d, and there are not enough input ports on the train-model to take in multiple values even if I was to use the same algorithm for each measure. I tried launching the column selector and making rules, but I get the same error as mentioned. How do I predict multiple values and later put the predicted columns together for the web service output so I don\u2019t have to have multiple API\u2019s?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1465894075710,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":1763,
        "Owner_creation_time":1463041289043,
        "Owner_last_access_time":1465915063043,
        "Owner_location":null,
        "Owner_reputation":25,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>What you would want to do is to train each model and save them as already trained models.\nSo create a new experiment, train your models and save them by right clicking on each model and they will show up in the left nav bar in the Studio. Now you are able to drag your models into the canvas and have them score predictions where you eventually make them end up in the same output as I have done in my example through the \u201cAdd columns\u201d module. I made this example for Ronaldo (Real Madrid CF player) on how he will perform in match after training day. You can see my demo on <a href=\"http:\/\/ronaldoinform.azurewebsites.net\" rel=\"nofollow noreferrer\">http:\/\/ronaldoinform.azurewebsites.net<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZwzUy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZwzUy.png\" alt=\"Ronaldo InForm\"><\/a><\/p>\n\n<p>For more detailed explanation on how to save the models and train multiple values; you can check out Raymond Langaeian (MSFT) answer in the comment section on this link:\n<a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-convert-training-experiment-to-scoring-experiment\/\" rel=\"nofollow noreferrer\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-convert-training-experiment-to-scoring-experiment\/<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1465904564903,
        "Answer_score":2,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37807158",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":34320449,
        "Question_title":"Use Azure ML methods like an API",
        "Question_body":"<p>Is that possible to use machine learning methods from Microsoft Azure Machine Learning  as an API from my own code (without ML Studio) with possibility to calculate everything on their side?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1450294016510,
        "Question_score":1,
        "Question_tags":"frameworks|rapidminer|azure-machine-learning-studio",
        "Question_view_count":180,
        "Owner_creation_time":1336227824220,
        "Owner_last_access_time":1663836582560,
        "Owner_location":null,
        "Owner_reputation":834,
        "Owner_up_votes":159,
        "Owner_down_votes":2,
        "Owner_views":122,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-publish-a-machine-learning-web-service\/\" rel=\"nofollow\">publish<\/a> an experiment (machine learning functions you hooked together in Azure ML Studio) as an API. When you call that API in your custom code you give it your data and all the computation runs in the cloud in Azure ML. <\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1450317613533,
        "Answer_score":1,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34320449",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64097278,
        "Question_title":"Why is the field \"compute target\" for data drift monitoring in Azure ML studio still blank whereas I have a compute instance?",
        "Question_body":"<p>I have created a compute instance:<\/p>\n<p>Virtual machine size\nSTANDARD_DS3_V2 (4 Cores, 14 GB RAM, 28 GB Disk)<\/p>\n<p>Processing Unit\nCPU - General purpose<\/p>\n<p>But, I'm not able to access it when trying to set it for data drift monitoring.\nThe dropdown list is empty. I can't understand why. Can you help me please?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Pf10h.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Pf10h.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1601276103867,
        "Question_score":0,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio",
        "Question_view_count":88,
        "Owner_creation_time":1423640080283,
        "Owner_last_access_time":1663943557963,
        "Owner_location":"Lyon, France",
        "Owner_reputation":457,
        "Owner_up_votes":19,
        "Owner_down_votes":2,
        "Owner_views":125,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I found the answer. You must give a <strong>cluster<\/strong> compute instance to do data drift in Azure Machine Learning Studio. As it is not clear, I'm planning to add something in the documentation of Azure.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1601278985530,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64097278",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67818831,
        "Question_title":"Azure Batch API rising 'AttributeError' in ML notebook",
        "Question_body":"<p>I am trying to interact with Azure Batch with python API, in the following way:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azure.batch import BatchServiceClient\nbatch = BatchServiceClient('&lt;mycredential&gt;','https:\/\/&lt;mybatchaccount&gt;.&lt;region&gt;.batch.azure.com')\nnext(batch.job.list())\n<\/code><\/pre>\n<p>This is run in a ML Studio notebook.<\/p>\n<p>However the following error appears: <code>AttributeError: 'str' object has no attribute 'signed_session'<\/code>.<br \/>\nI am taking the url and credentials from my batch console UI:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/lc9n4m.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lc9n4m.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As a credential I tried both Primary and Secondary access keys amd &quot;URL&quot; as batch url.<br \/>\nAm I doing anything wrong?<br \/>\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622712274873,
        "Question_score":0,
        "Question_tags":"python|azure|azure-batch|azure-machine-learning-service",
        "Question_view_count":52,
        "Owner_creation_time":1436006890427,
        "Owner_last_access_time":1663945829127,
        "Owner_location":"Amsterdam, Paesi Bassi",
        "Owner_reputation":959,
        "Owner_up_votes":136,
        "Owner_down_votes":31,
        "Owner_views":204,
        "Question_last_edit_time":1622714028187,
        "Answer_body":"<p><code>&lt;mycredential&gt;<\/code> should not be your bare auth key string. You need to create a shared auth key object.<\/p>\n<pre><code>credentials = batchauth.SharedKeyCredentials(BATCH_ACCOUNT_NAME, BATCH_ACCOUNT_KEY)\nbatch_client = batch.BatchServiceClient(credentials, base_url=BATCH_ACCOUNT_URL)\n<\/code><\/pre>\n<p>Please see the <a href=\"https:\/\/docs.microsoft.com\/azure\/batch\/tutorial-parallel-python\" rel=\"nofollow noreferrer\">Azure Batch Python tutorial<\/a>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1622734661130,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67818831",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":68959934,
        "Question_title":"Contextual version conflict error, Microsoft Azure Machine Learning Studio",
        "Question_body":"<p>I'm trying to run this 10 line .ipynb file from Google Colab in Microsoft Azure Machine Learning Studio<\/p>\n<p><a href=\"https:\/\/colab.research.google.com\/drive\/1o_-QIR8yVphfnbNZGYemyEr111CHHxSv?usp=sharing\" rel=\"nofollow noreferrer\">https:\/\/colab.research.google.com\/drive\/1o_-QIR8yVphfnbNZGYemyEr111CHHxSv?usp=sharing<\/a><\/p>\n<p>When I get to this step:<\/p>\n<pre><code>import gradio as gr\nimport tensorflow as tf\nfrom transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n<\/code><\/pre>\n<p>I get this error:<\/p>\n<pre><code>ContextualVersionConflict: (Flask 1.0.3 (\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages), Requirement.parse('Flask&gt;=1.1.1'), {'gradio'})\n<\/code><\/pre>\n<p>I tried to install the Flask 1.1.1 version but I get more errors. Any idea what I should do to get past this step in Azure ML Studio?<\/p>\n<pre><code>!pip install \u2013force-reinstall Flask==1.1.1\n\/\/ More errors\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630103458310,
        "Question_score":2,
        "Question_tags":"python|jupyter-notebook|google-colaboratory|azure-machine-learning-studio",
        "Question_view_count":237,
        "Owner_creation_time":1630103231523,
        "Owner_last_access_time":1664075236640,
        "Owner_location":null,
        "Owner_reputation":17,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1630103807136,
        "Answer_body":"<p>The issue is because <code>gradio<\/code> package using existing Flask package (version 1.0.3). But as your application required Flask&gt;=1.1.1, therefore it is showing error. You need to uninstall the existing Flask package and then install the latest required version.<\/p>\n<p>To uninstall the existing package:\n<code>!pip uninstall Flask -y<\/code><\/p>\n<p>To install latest package:\n<code>!pip install Flask&gt;=1.1.1<\/code><\/p>\n<p><strong>Then, make sure to restart your runtime to pick up the new Flask using the Runtime -&gt; Restart runtime menu.<\/strong><\/p>\n<p>Finally, import gradio.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1630303052770,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68959934",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":69356567,
        "Question_title":"How to access local files from AzureML File Share?",
        "Question_body":"<p>Earlier when using AzureML from the Notebooks blade of Azure ML UI, we could access the local files in AzureML using simple relative paths:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bKZ0W.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bKZ0W.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For example, in the above image to access the CSV from the <code>test.ipynb<\/code> we could just mention the relative path:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pandas.read_csv('WHO-COVID-19-global-data.csv')\n<\/code><\/pre>\n<p>However, we are not able to do that anymore.<\/p>\n<p>Also when we run<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nos.getcwd()\n<\/code><\/pre>\n<p>We see the output as\n<code>'\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/&lt;cluster-name&gt;'<\/code>.<\/p>\n<p>Hence, we are unable to access the files in the FileStore which was not the case earlier.<\/p>\n<p>When we run the same from the JuyterLab environment of the compute environment we get:<\/p>\n<p><code>'\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/&lt;cluster-name&gt;\/code\/Users\/&lt;current-user-name&gt;\/temp'<\/code>.<\/p>\n<p>We can easily solve it by adding the path <code>'\/code\/Users\/&lt;current-user-name&gt;\/temp'<\/code> at the base and use that instead. But this is not recommended as with a change in the environment we are using the code needs to change every time. How do we resolve this issue without going through this path appending method.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1632809238003,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":375,
        "Owner_creation_time":1601729162437,
        "Owner_last_access_time":1663774065773,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":887,
        "Owner_up_votes":187,
        "Owner_down_votes":32,
        "Owner_views":130,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I work on the Notebooks team in AzureML, I just tried this. Did this just start happening today?<\/p>\n<p>It seems like things are working as expected: <a href=\"https:\/\/i.stack.imgur.com\/xeDIT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xeDIT.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1632845944177,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69356567",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":41743792,
        "Question_title":"Is there an Azure Machine Learning Studio module that works like the Pandas 'mask' method?",
        "Question_body":"<p>I'm trying to perform the following Python Pandas operation in Azure Machine Learning Studio, but cannot find a module that handles it:<\/p>\n\n<pre><code>df.credit_score = df.credit_score.mask(df.credit_score &gt; 800, df.credit_score \/ 10)\n<\/code><\/pre>\n\n<p>So I'm effectively just trying to find all values in my 'credit_score' column that are greater than 800 and divide them by 10.  I have been unable so far to find a module in AML Studio that does that.<\/p>\n\n<p>Also, I should add that I'm having issues with my Python script in AML Studio, which is why I'm attempting to replicate all of my code using AML built-in modules.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1484834877413,
        "Question_score":1,
        "Question_tags":"pandas|azure-machine-learning-studio",
        "Question_view_count":57,
        "Owner_creation_time":1483888458947,
        "Owner_last_access_time":1556908168660,
        "Owner_location":null,
        "Owner_reputation":107,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":1484835455796,
        "Answer_body":"<p>To my knowledge, there's no built-in module to do this succinctly (to my knowledge). If you prefer to use built-ins, you could:<\/p>\n\n<ol>\n<li>Use a Split Dataset module to split the entries based on credit\nscore<\/li>\n<li>Divide the credit score in large-credit-score rows by 10 using\nApply Math Operation<\/li>\n<li>Concatenate the two datasets row-wise with an Add Rows module<\/li>\n<\/ol>",
        "Answer_comment_count":1,
        "Answer_creation_time":1485350815923,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41743792",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":45582412,
        "Question_title":"How to build a Convolution Neural Net in Azure Machine Learning?",
        "Question_body":"<p>Someone should add \"net#\" as a tag. I'm trying to improve my neural network in Azure Machine Learning Studio by turning it into a convolution neural net using this tutorial:<\/p>\n\n<p><a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Neural-Network-Convolution-and-pooling-deep-net-2\" rel=\"noreferrer\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Neural-Network-Convolution-and-pooling-deep-net-2<\/a><\/p>\n\n<p>The differences between mine and the tutorial is I'm doing regression with 35 features and 1 label and they're doing classification with 28x28 features and 10 labels. <\/p>\n\n<p>I start with the basic and 2nd example and get them working with:<\/p>\n\n<pre><code>input Data [35];\n\nhidden H1 [100]\n    from Data all;\n\nhidden H2 [100]\n    from H1 all;\n\noutput Result [1] linear\n    from H2 all;\n<\/code><\/pre>\n\n<p>Now the transformation to convolution I misunderstand. In the tutorial and documentation here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-netsharp-reference-guide\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-netsharp-reference-guide<\/a> it doesn't mention how the node tuple values are calculated for the hidden layers. The tutorial says:<\/p>\n\n<pre><code>hidden C1 [5, 12, 12]\n  from Picture convolve {\n    InputShape  = [28, 28];\n    KernelShape = [ 5,  5];\n    Stride      = [ 2,  2];\n    MapCount = 5;\n  }\n\nhidden C2 [50, 4, 4]\n   from C1 convolve {\n     InputShape  = [ 5, 12, 12];\n     KernelShape = [ 1,  5,  5];\n     Stride      = [ 1,  2,  2];\n     Sharing     = [ F,  T,  T];\n     MapCount = 10;\n  }\n<\/code><\/pre>\n\n<p>Seems like the [5, 12, 12] and [50,4,4] pop out of no where along with the KernalShape, Stride, and MapCount. How do I know what values are valid for my example? I tried using the same values, but it didn't work and I have a feeling since he has a [28,28] input and I have a [35], I need tuples with 2 integers not 3. <\/p>\n\n<p>I just tried with random values that seem to correlate with the tutorial:<\/p>\n\n<pre><code>const { T = true; F = false; }\n\ninput Data [35];\n\nhidden C1 [7, 23]\n  from Data convolve {\n    InputShape  = [35];\n    KernelShape = [7];\n    Stride      = [2];\n    MapCount = 7;\n  }\n\nhidden C2 [200, 6]\n   from C1 convolve {\n     InputShape  = [ 7, 23];\n     KernelShape = [ 1,  7];\n     Stride      = [ 1,  2];\n     Sharing     = [ F,  T];\n     MapCount = 14;\n  }\n\nhidden H3 [100]\n  from C2 all;\n\noutput Result [1] linear\n  from H3 all;\n<\/code><\/pre>\n\n<p>Right now it seems impossible to debug because the only error code Azure Machine Learning Studio ever gives is:<\/p>\n\n<pre><code>Exception\":{\"ErrorId\":\"LibraryException\",\"ErrorCode\":\"1000\",\"ExceptionType\":\"ModuleException\",\"Message\":\"Error 1000: TLC library exception: Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown.\",\"Exception\":{\"Library\":\"TLC\",\"ExceptionType\":\"LibraryException\",\"Message\":\"Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown.\"}}}Error: Error 1000: TLC library exception: Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown. Process exited with error code -2\n<\/code><\/pre>\n\n<p>Lastly my setup is <a href=\"https:\/\/i.stack.imgur.com\/PBN9L.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PBN9L.png\" alt=\"Azure Machine Learning Setup\"><\/a> <\/p>\n\n<p>Thanks for the help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1502257048253,
        "Question_score":9,
        "Question_tags":"machine-learning|convolution|azure-machine-learning-studio|net#",
        "Question_view_count":1268,
        "Owner_creation_time":1418505926277,
        "Owner_last_access_time":1664040770893,
        "Owner_location":"Missouri",
        "Owner_reputation":1454,
        "Owner_up_votes":117,
        "Owner_down_votes":15,
        "Owner_views":328,
        "Question_last_edit_time":1502686418056,
        "Answer_body":"<p>The correct network definition for 35-column length input with given kernels and strides would be following:<\/p>\n\n<pre><code>const { T = true; F = false; }\n\ninput Data [35];\n\nhidden C1 [7, 15]\n  from Data convolve {\n    InputShape  = [35];\n    KernelShape = [7];\n    Stride      = [2];\n    MapCount = 7;\n  }\n\nhidden C2 [14, 7, 5]\n   from C1 convolve {\n     InputShape  = [ 7, 15];\n     KernelShape = [ 1,  7];\n     Stride      = [ 1,  2];\n     Sharing     = [ F,  T];\n     MapCount = 14;\n  }\n\nhidden H3 [100]\n  from C2 all;\n\noutput Result [1] linear\n  from H3 all;\n<\/code><\/pre>\n\n<p>First, the C1 = [7,15]. The first dimension is simply the MapCount. For the second dimension, the kernel shape defines the length of the \"window\" that's used to scan the input columns, and the stride defines how much it moves at each step. So the kernel windows would cover columns 1-7, 3-9, 5-11,...,29-35, yielding the second dimension of 15 when you tally the windows.<\/p>\n\n<p>Next, the C2 = [14,7,5]. The first dimension is again the MapCount. For the second and third dimension, the 1-by-7 kernel \"window\" has to cover the input size of 7-by-15, using steps of 1 and 2 along corresponding dimensions. <\/p>\n\n<p>Note that you could specify C2 hidden layer shape of [98,5] or even [490], if you wanted to flatten the outputs. <\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1503342256370,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45582412",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73192053,
        "Question_title":"azure cli not recognizing the following command az ml data create -f <file-name>.yml",
        "Question_body":"<p>got a folder called data-asset which contains a yaml file with the following<\/p>\n<pre><code>type: uri_folder\nname: &lt;name_of_data&gt;\ndescription: &lt;description goes here&gt;\npath: &lt;path&gt;\n<\/code><\/pre>\n<p>In a pipeline am referencing this using azure cli inline script using the following command az ml data create -f .yml but getting error<\/p>\n<p>full error-D:\\a\\1\\s\\ETL\\data-asset&gt;az ml data create -f data-asset.yml\nERROR: 'ml' is misspelled or not recognized by the system.<\/p>\n<p>Examples from AI knowledge base:\naz extension add --name anextension\nAdd extension by name<\/p>\n<p>trying to implement this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-data-assets?tabs=CLI\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-data-assets?tabs=CLI<\/a><\/p>\n<p>how can a resolve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659348144177,
        "Question_score":0,
        "Question_tags":"azure|yaml|azure-cli|azure-machine-learning-service|azuremlsdk",
        "Question_view_count":112,
        "Owner_creation_time":1606756004663,
        "Owner_last_access_time":1661935729383,
        "Owner_location":null,
        "Owner_reputation":49,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":"<p>One of the workaround you can follow to resolve the above issue;<\/p>\n<p>Based on this <a href=\"https:\/\/github.com\/Azure\/azure-cli\/issues\/21390#issuecomment-1161782243\" rel=\"nofollow noreferrer\"><em><strong>GitHub issue<\/strong><\/em><\/a> as suggested by @<em>adba-msft<\/em> .<\/p>\n<blockquote>\n<p><strong>Please make sure that you have upgraded your azure cli to latest and<\/strong>\n<strong>Azure CLI ML extension v2 is being used.<\/strong><\/p>\n<\/blockquote>\n<p>To check and upgrade the cli we can use the below <code>cmdlts<\/code>:<\/p>\n<pre><code>az version\n\naz upgrade\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Uopde.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Uopde.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For more information please refer this similar <a href=\"https:\/\/stackoverflow.com\/questions\/73110661\/create-is-misspelled-or-not-recognized-by-the-system-on-az-ml-dataset-create\"><em><strong>SO THREAD|'create' is misspelled or not recognized by the system on az ml dataset create<\/strong><\/em><\/a> .<\/p>\n<p>I did observe the same issue after trying the aforementioned suggestion by @<em>Dor Lugasi-Gal<\/em> it works for me with (in my case <code>az ml -h<\/code>) after installed the extension with  <code>az extension add -n ml -y<\/code> can able to get the result of <code>az ml -h<\/code> without any error.<\/p>\n<p><em><strong>SCREENSHOT FOR REFERENCE:-<\/strong><\/em><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/39LHa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/39LHa.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1659361184080,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1659362947923,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73192053",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":65997961,
        "Question_title":"How to trigger an AzureML Pipeline from Azure DevOps?",
        "Question_body":"<p>If we have an AzureML Pipeline published, how can we trigger it from Azure DevOps <strong>without using Python Script Step or Azure CLI Step<\/strong>?<\/p>\n<p>The AzureML Steps supported natively in Azure DevOps include Model_Deployment and Model_Profiling.<\/p>\n<p>Is there any step in Azure DevOps which can be used to directly trigger a published Azure Machine Learning Pipeline while maintaining capabilities like using Service Connections and passing environmental variables, Gated Release (Deployment)?<\/p>\n<p>Edit:\nThis process can then be used to run as an agentless job.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1612203126923,
        "Question_score":2,
        "Question_tags":"azure|azure-devops|azure-machine-learning-service",
        "Question_view_count":1923,
        "Owner_creation_time":1601729162437,
        "Owner_last_access_time":1663774065773,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":887,
        "Owner_up_votes":187,
        "Owner_down_votes":32,
        "Owner_views":130,
        "Question_last_edit_time":1612249107729,
        "Answer_body":"<p>Assumptions:<\/p>\n<ol>\n<li>An AzureML Pipeline is published and the REST endpoint is ready- To be referred to in this answer as &lt;AML_PIPELINE_REST_URI&gt;. And Published Pipeline ID is also ready- To be referred to in this answer as &lt;AML_PIPELINE_ID&gt;<\/li>\n<li>You have the Azure Machine Learning Extension installed: <a href=\"https:\/\/marketplace.visualstudio.com\/items?itemName=ms-air-aiagility.vss-services-azureml&amp;ssr=false#review-details\" rel=\"nofollow noreferrer\">Azure Machine Learning Extension<\/a><\/li>\n<\/ol>\n<p>To Invoke the Azure Machine Learning Pipeline we use the <code>Invoke ML Pipeline<\/code> step available in Azure DevOps. It is available when running an Agentless Job.<\/p>\n<p>To trigger it the workflow is as follows:<\/p>\n<ol>\n<li>Create a New Pipeline. Using the Classic Editor, delete the default Agent Job 1 stage.\n<a href=\"https:\/\/i.stack.imgur.com\/phzL3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/phzL3.png\" alt=\"enter image description here\" \/><\/a><\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/QkiPY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QkiPY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ol start=\"2\">\n<li><p>Add an agentless job:\n<a href=\"https:\/\/i.stack.imgur.com\/0PXwg.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0PXwg.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Add a task to this Agentless Job:\n<a href=\"https:\/\/i.stack.imgur.com\/trW7j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/trW7j.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Use AzureML Published Pipeline Task:\n<a href=\"https:\/\/i.stack.imgur.com\/3rl4z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3rl4z.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Use the Service Connection Mapped to the AML Workspace. You can find more on this at the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/library\/service-endpoints?view=azure-devops&amp;tabs=yaml\" rel=\"nofollow noreferrer\">official documentation<\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/mnV36.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mnV36.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Choose the Pipeline to trigger using the &lt;AML_PIPELINE_ID&gt;:\n<a href=\"https:\/\/i.stack.imgur.com\/fbpQW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fbpQW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Give The experiment name and Pipeline Parameters if any:\n<a href=\"https:\/\/i.stack.imgur.com\/og1kx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/og1kx.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>That's it, you can Save and Queue:\n<a href=\"https:\/\/i.stack.imgur.com\/iCwdl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/iCwdl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<\/ol>\n<p>Alternatively, you can simply use the following jobs:<\/p>\n<pre><code>- job: Job_2\n  displayName: Agentless job\n  pool: server\n  steps:\n  - task: MLPublishedPipelineRestAPITask@0\n    displayName: Invoke ML pipeline\n    inputs:\n      connectedServiceName: &lt;REDACTED-AML-WS-Level-Service_Connection-ID&gt;\n      PipelineId: &lt;AML_PIPELINE_ID&gt;\n      ExperimentName: experimentname\n      PipelineParameters: ''\n<\/code><\/pre>",
        "Answer_comment_count":1,
        "Answer_creation_time":1612256282837,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1612281801243,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65997961",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":50576929,
        "Question_title":"Replacing values in dataset within Azure Machine Learning Studio",
        "Question_body":"<p>In Azure Machine Learning studio I need to convert a column of data that has three categorical values 'yes', 'no' and 'maybe', and wish to combine the 'no' and 'maybe' values as just 'no'. <\/p>\n\n<p>I can do this easily using SQL, R, or Python but for these purposes I need to show if it is possible to do this without using these languages. I can't seem to find a way to do this. <\/p>\n\n<p>Does anyone have any ideas? I'm fine if the answer is no but I don't want to say it's not possible if it is. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1527572052450,
        "Question_score":2,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":484,
        "Owner_creation_time":1367782461047,
        "Owner_last_access_time":1574911198153,
        "Owner_location":null,
        "Owner_reputation":1647,
        "Owner_up_votes":365,
        "Owner_down_votes":8,
        "Owner_views":321,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It can be done! :)<\/p>\n\n<p>You would just use the \"Group Categorical Values\" module. Choose the column that has the data you want to group, and you can set the values like the following:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/UGhrR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/UGhrR.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>What's going on here is that the default, which will get used if the other levels aren't caught, is set to \"yes\". Then when any values are \"no\", or \"maybe\", it gets grouped into a category of \"no\".<\/p>\n\n<p>However, this will error unless you make that column a categorical type, so you would need to use the \"Edit Metadata\" module to do that.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/45s2Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/45s2Q.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The example I used is <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Replace-Values-in-Dataset\" rel=\"nofollow noreferrer\">published to the gallery<\/a>, if you need to reference it.<\/p>\n\n<p>If you need more info, just let me know.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1527597469393,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50576929",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":45009184,
        "Question_title":"Powershell AzureML Get-AmlWorkspace",
        "Question_body":"<pre><code>Get-AmlWorkspace : One or more errors occurred.\nAt line:1 char:1\n+ Get-AmlWorkspace\n+ ~~~~~~~~~~~~~~~~\n+ CategoryInfo          : NotSpecified: (:) [Get-AmlWorkspace], \nAggregateException\n+ FullyQualifiedErrorId : \nSystem.AggregateException,AzureML.PowerShell.GetWorkspace\n<\/code><\/pre>\n\n<p>I am trying to use Powershell to connect to Azure ML studio as it looks like an easier way to manage a workspace. I've downloaded the dll file from <a href=\"https:\/\/github.com\/hning86\/azuremlps\" rel=\"nofollow noreferrer\">https:\/\/github.com\/hning86\/azuremlps<\/a> and changed my config.json file, but get the error above if I try to run any AzureML commands. I've unblocked the DLL file and imported the AzureMLPS module, and I can see the module and commands I am trying to use have been imported by doing <code>Get-Module<\/code> and <code>Get-Command<\/code><\/p>\n\n<p>For info I've not used Powershell before.<\/p>\n\n<p>Any suggestions much appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1499681034153,
        "Question_score":1,
        "Question_tags":"powershell|azure-machine-learning-studio",
        "Question_view_count":428,
        "Owner_creation_time":1397507727100,
        "Owner_last_access_time":1663075667463,
        "Owner_location":null,
        "Owner_reputation":340,
        "Owner_up_votes":60,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Have you installed Azure PowerShell Installer on your local machine?\n<strong><a href=\"https:\/\/github.com\/Azure\/azure-powershell\/releases\" rel=\"nofollow noreferrer\">Click here<\/a><\/strong> for more info.<\/p>\n\n<p>Download the latest <strong>Azure PowerShell Installer (4.3.1)<\/strong>, then install on your local machine. Then retry using Azure PowerShell module and commands.<\/p>\n\n<p>I installed mine last May, using Azure PowerShell 4.0.1, and the command Get-AmlWorkspace is working.<\/p>\n\n<pre><code># Set local folder location\nSet-Location -Path \"C:\\Insert here the location of AzureMLPS.dll\"\n\n# Unblock and import Azure Powershell Module (leverages config.json file)\nUnblock-File .\\AzureMLPS.dll\nImport-Module .\\AzureMLPS.dll\n\n# Get Azure ML Workspace info\nGet-AmlWorkspace\n<\/code><\/pre>\n\n<p>The output on my side looks like this:\n<a href=\"https:\/\/i.stack.imgur.com\/mEGeT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mEGeT.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1503389654690,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45009184",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":50954802,
        "Question_title":"Recommender Split Returning Empty Dataset",
        "Question_body":"<p>I'm using a \"Split Data\" module set to recommender split to split data for training and testing a matchbox recommender. The input data is a valid user-item-rating tuple (for example, 575978 - 157381 - 3) and I've left the parameters for the recommender split as default (0s for everything), besides changing it to a .75 and .25 split. However, when this module finishes, it returns the complete, unsplit dataset for dataset1 and a completely empty (but labelled) dataset for dataset2. This also happens when doing a stratified split using the \"Split Rows\" mode. Any idea what's going on?<\/p>\n\n<p>Thanks.<\/p>\n\n<p>Edit: Including a sample of my data.<\/p>\n\n<pre><code>UserID  ItemID  Rating\n835793  165937  3\n154738  11214   3\n938459  748288  3\n819375  789768  6\n738571  98987   3\n847509  153777  3\n991757  124458  3\n968685  288070  2\n236349  8337    3\n127299  545885  3\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1529519087767,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":88,
        "Owner_creation_time":1436818579270,
        "Owner_last_access_time":1656621730207,
        "Owner_location":"Eugene, OR, USA",
        "Owner_reputation":474,
        "Owner_up_votes":322,
        "Owner_down_votes":12,
        "Owner_views":28,
        "Question_last_edit_time":1529527790943,
        "Answer_body":"<p>Figured it out. In my \"Remove Duplicate Rows\" module up the chain a bit I was only removing duplicates by UserID instead of UserID <em>and<\/em> ItemID. This still left quite a bit of rows but I'm assuming it messed with the stratification. <\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1529598877083,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50954802",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":65112585,
        "Question_title":"Pip installation stuck in infinite loop if unresolvable conflicts in dependencies",
        "Question_body":"<p>Pip installation is stuck in an infinite loop if there are unresolvable conflicts in dependencies. To reproduce, <code>pip==20.3.0<\/code> and:<\/p>\n<pre><code>pip install pyarrow==2.0.0 azureml-defaults==1.18.0\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1606928045387,
        "Question_score":13,
        "Question_tags":"python|pip|azure-machine-learning-service",
        "Question_view_count":2146,
        "Owner_creation_time":1568658032913,
        "Owner_last_access_time":1625529665640,
        "Owner_location":"Redmond, WA, USA",
        "Owner_reputation":133,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1608739608112,
        "Answer_body":"<p>Workarounds:<\/p>\n<p>Local environment:\nDowngrade pip to &lt; 20.3<\/p>\n<p>Conda environment created from yaml:\nThis will be seen only if conda-forge is highest priority channel, anaconda channel doesn't have pip 20.3 (as of now). To mitigate the issue please explicitly specify pip&lt;20.3 (!=20.3 or =20.2.4 pin to other version) as a conda dependency in the conda specification file<\/p>\n<p>AzureML experimentation:\nFollow the case above to make sure pinned pip resulted as a conda dependency in the environment object, either from yml file or programmatically<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1606928528563,
        "Answer_score":12,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65112585",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":30068341,
        "Question_title":"How to write the results of Azure ML web service to the azure sql database (The output of Azure ML web service is in Json structure)",
        "Question_body":"<p>The results can be written to SQL Azure using the writer module in the experiment but after publishing the web service the output comes in the Json Structure and it doesn't go to the writer module <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1430890566837,
        "Question_score":0,
        "Question_tags":"azure-sql-database|azure-scheduler|azure-machine-learning-studio",
        "Question_view_count":990,
        "Owner_creation_time":1403541426413,
        "Owner_last_access_time":1592469887883,
        "Owner_location":"Bengaluru, India",
        "Owner_reputation":191,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Question_last_edit_time":1431065815883,
        "Answer_body":"<p>Don't set output port and use Batch execution service - details are provided here - <a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-publish-a-machine-learning-web-service\/\" rel=\"nofollow\">Publish web service<\/a> and <a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-consume-web-services\/\" rel=\"nofollow\">consume web service<\/a><\/p>",
        "Answer_comment_count":5,
        "Answer_creation_time":1430897690130,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/30068341",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":65099376,
        "Question_title":"Segmentation fault error in importing sentence_transformers in Azure Machine Learning Service Nvidia Compute",
        "Question_body":"<p>I would like to use sentence_transformers in AML to run XLM-Roberta model for sentence embedding. I have a script in which I import sentence_transformers:<\/p>\n<pre><code>from sentence_transformers import SentenceTransformer\n<\/code><\/pre>\n<p>Once I run my AML pipeline, the run fails on this script with the following error:<\/p>\n<pre><code>AzureMLCompute job failed.\nUserProcessKilledBySystemSignal: Job failed since the user script received system termination signal usually due to out-of-memory or segfault.\n    Cause: segmentation fault\n    TaskIndex: \n    NodeIp: #####\n    NodeId: #####\n<\/code><\/pre>\n<p>I'm pretty sure that this import is causing this error, because if I comment out this import, the rest of the script will run.\nThis is weird because the installation of the sentence_transformers succeed.<\/p>\n<p>This is the details of my compute:<\/p>\n<pre><code>Virtual machine size\nSTANDARD_NV24 (24 Cores, 224 GB RAM, 1440 GB Disk)\nProcessing Unit\nGPU - 4 x NVIDIA Tesla M60\n<\/code><\/pre>\n<p>Agent Pool:<\/p>\n<pre><code>Azure Pipelines\n<\/code><\/pre>\n<p>Agent Specification:<\/p>\n<pre><code>ubuntu-16.04\n<\/code><\/pre>\n<p>requirements.txt file:<\/p>\n<pre><code>torch==1.4.0\nsentence-transformers\n<\/code><\/pre>\n<p>Does anyone have a solution for this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1606861020137,
        "Question_score":1,
        "Question_tags":"azure|nvidia|azure-machine-learning-service|roberta-language-model|sentence-transformers",
        "Question_view_count":530,
        "Owner_creation_time":1450889293150,
        "Owner_last_access_time":1663403554823,
        "Owner_location":"Finland",
        "Owner_reputation":398,
        "Owner_up_votes":21,
        "Owner_down_votes":1,
        "Owner_views":28,
        "Question_last_edit_time":1606866767496,
        "Answer_body":"<p>I fixed the issue by changing the pytorch version from 1.4.0 to 1.6.0.\nSo the requirements.txt looks like this:<\/p>\n<pre><code>torch==1.6.0\nsentence-transformers\n<\/code><\/pre>\n<p>At first I tried one of the older versions of sentence-transformers which was compatible with pytorch 1.4.0. But the older version doesn't support &quot;xml-roberta-base&quot; model, so I tried to upgrade the pytorch version.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1606866538210,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1606867069223,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65099376",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":40844351,
        "Question_title":"R code on Azure Machine Learning is slow compared to local execution time",
        "Question_body":"<p>Getting straight to it:\nWhy is my R code doing fine on my local CPU (under one minute), but tens of times slower on Azure Machine Learning, using one R script block (over 18 minutes)?<\/p>\n\n<p>I assume that it has to do with the resources allocated to the experiment, but how can I be sure? Can I obtain details about the resource allocated to the R script block from somwehere hidden in the Azure-ML Studio machinery?<\/p>\n\n<p>Thank you, Flo<\/p>\n\n<p>Later Edit:\nAs it often happens, I did finally find some information, which still doesn't solve my issue. According to <a href=\"https:\/\/msdn.microsoft.com\/library\/en-us\/Dn905952.aspx#Technical%20Notes\" rel=\"nofollow noreferrer\">https:\/\/msdn.microsoft.com\/library\/en-us\/Dn905952.aspx#Technical%20Notes<\/a> \"User-specified R code is run by a 64-bit R interpreter that runs in Azure using an A8 virtual machine with 56 GB of RAM.\"<\/p>\n\n<p>This is more than my local machine has, the R code is still much slower on the Azure-ML studio.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1480336059453,
        "Question_score":0,
        "Question_tags":"r|azure-machine-learning-studio",
        "Question_view_count":733,
        "Owner_creation_time":1459503032960,
        "Owner_last_access_time":1512467737783,
        "Owner_location":"Vienna, Austria",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":1480338194649,
        "Answer_body":"<p>Consider using rbenchmark or other benchmarking tools to get an idea of the runtime and complexity of your code. In general for loops tend to be slow.<\/p>\n\n<p>It's very possible that the server has less resources available (ram, cpu) or that you have to wait in a que before you get served. Without any more code it's hard to comment further on this issue.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1480339657187,
        "Answer_score":0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40844351",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":36285329,
        "Question_title":"How to import a third party library \"causalImpact\" using R script in AzureML studio?",
        "Question_body":"<p>I tried to import causalImpact library from github using \"devtools\" in AzureML studio for one of my projects.\ncode used was:<\/p>\n\n<pre><code>library(devtools)\ndevtools::install_github(\"google\/CausalImpact\")\n<\/code><\/pre>\n\n<p>Unfortunately, Azure doesn't support this.So tried importing it following the procedure in this <a href=\"https:\/\/blogs.msdn.microsoft.com\/benjguin\/2014\/09\/24\/how-to-upload-an-r-package-to-azure-machine-learning\/\" rel=\"nofollow\">blog<\/a>.It is giving multiple errors on the name of dependent packages of casualImpact(i.e. BOOM, BH etc.). Can anyone help me out in importing this package on Azure?<\/p>\n\n<p>This is the R-script I used following the link given above:<\/p>\n\n<pre><code>library(assertthat)\nlibrary(dplyr)\nlibrary(hflights)\nlibrary(Lahman)\nlibrary(magrittr)\nlibrary(LGPL)\ninstall.packages(\"src\/BH_1.55.0-3.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"BH \", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n\nlibrary(BH)\ninstall.packages(\"src\/Boom_0.1.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"Boom \", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n\ninstall.packages(\"src\/BoomSpikeSlab.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"BoomSpikeSlab\", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n\ninstall.packages(\"src\/bsts_0.5.1.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"bsts\", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\nlibrary(zoo)\nlibrary(xts)\ninstall.packages(\"src\/CausalImpact.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"CausalImpact\", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1459257130213,
        "Question_score":2,
        "Question_tags":"r|github|cortana-intelligence|azure-machine-learning-studio",
        "Question_view_count":326,
        "Owner_creation_time":1423214734843,
        "Owner_last_access_time":1470891961370,
        "Owner_location":"Bangalore",
        "Owner_reputation":55,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Question_last_edit_time":1459402444996,
        "Answer_body":"<p>You will have to upload all dependent packages of casualImpact as a zip file - see sample <a href=\"http:\/\/gallery.azureml.net\/Details\/7507f907deb845d9b9b193b455a8615d\" rel=\"nofollow\">here<\/a> which shows uploading two packages required for xgboost<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1459314427723,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36285329",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":68658385,
        "Question_title":"Azure Machine Learning Studio designer - \"create new version\" unexpected when registering a data set",
        "Question_body":"<p>I am trying to register a data set as a Python step with the Azure Machine Learning Studio designer. Here is my code:<\/p>\n<pre><code>import pandas as pd\nfrom azureml.core import Workspace, Run, Dataset\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    run = Run.get_context()\n    ws = run. experiment.workspace\n    ds = Dataset.from_pandas_dataframe(dataframe1)\n    ds.register(workspace = ws,\n                name = &quot;data set name&quot;,\n                description = &quot;example description&quot;,\n                create_new_version = True)\n    return dataframe1, \n<\/code><\/pre>\n<p>I get an error saying that &quot;create_new_version&quot; in the ds.register line was an unexpected keyword argument. However, this keyword appears in the documentation and I need it to keep track of new versions of the file.<\/p>\n<p>If I remove the argument, I get a different error: &quot;Local data source path not supported for this operation&quot;, so it still does not work. Any help is appreciated. Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1628113771587,
        "Question_score":2,
        "Question_tags":"python|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":565,
        "Owner_creation_time":1371499229817,
        "Owner_last_access_time":1664024865427,
        "Owner_location":null,
        "Owner_reputation":1111,
        "Owner_up_votes":124,
        "Owner_down_votes":2,
        "Owner_views":191,
        "Question_last_edit_time":null,
        "Answer_body":"<h2>update<\/h2>\n<p>sharing OP's solution here for easier discovery<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom azureml.core import Workspace, Run, Dataset\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    run = Run.get_context()\n    ws = run. experiment.workspace\n    datastore = ws.get_default_datastore()\n    ds = Dataset.Tabular.register_pandas_dataframe(\n        dataframe1, datastore, 'data_set_name',\n        description = 'data set description.')\n    return dataframe1,\n<\/code><\/pre>\n<h2>original answer<\/h2>\n<p>Sorry you're struggling. You're very close!<\/p>\n<p>A few things may be the culprit here.<\/p>\n<ol>\n<li>It looks like you're using the <code>Dataset<\/code> class, which has been deprecated. I recommend trying <code>Dataset.Tabular.register_pandas_dataframe()<\/code> (<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.dataset_factory.tabulardatasetfactory?view=azure-ml-py#register-pandas-dataframe-dataframe--target--name--description-none--tags-none--show-progress-true-\" rel=\"nofollow noreferrer\">docs link<\/a>) instead of <code>Dataset.from_pandas_dataframe()<\/code>. (<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/dataset-api-change-notice.md\" rel=\"nofollow noreferrer\">more about the Dataset API deprecation<\/a>)<\/li>\n<li>More conjectire here, but another thing is there might be some limitations to using dataset registration within an &quot;Execute Python Script&quot; (EPS) module due to:\n<ol>\n<li>the workspace object might not have the right permissions<\/li>\n<li>you might not be able to use the <code>register_pandas_dataframe<\/code> method inside the EPS module, but might have better luck with save the dataframe first to parquet, then calling <code>Dataset.Tabular.from_parquet_files<\/code><\/li>\n<\/ol>\n<\/li>\n<\/ol>\n<p>Hopefully something works here!<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1628119362627,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1628180472980,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68658385",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58031370,
        "Question_title":"How to cancel a running job from the UI?",
        "Question_body":"<p>Am I missing something but how can I cancel a run in my workspace from <a href=\"https:\/\/ms.portal.azure.com\/\" rel=\"nofollow noreferrer\">https:\/\/ms.portal.azure.com\/<\/a> ? The cancel button is always greyed out.<\/p>\n\n<p>I know I can use use the sdk to cancel a run using:<\/p>\n\n<pre><code>run = [ r for r in Experiment(ws, 'myExp').get_runs() if r.id == '899b8314-26b6-458f-9f5c-539ffbf01b91'].pop()\nrun.cancel()\n<\/code><\/pre>\n\n<p>But it would be more convenient to be able to do it from the UI<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1568993659347,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":204,
        "Owner_creation_time":1565794118450,
        "Owner_last_access_time":1664072200673,
        "Owner_location":null,
        "Owner_reputation":113,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>What kind of run is this? Canceling is not currently enabled for pipeline runs in the UI, but is supported for other run types.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1569333175230,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1569508172863,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58031370",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":41903982,
        "Question_title":"Web service output - Azure ML Studio",
        "Question_body":"<p>I am new to Azure ML Studio. I tried creating an experiment that takes a numeric value as input and a gives a data table type output. I works fine when I run it in the portal , but not when I run it as a web service. It shows a single value numeric output , when it has to be a data table type.<\/p>\n\n<p>Is there a way to change the output type of web service output? <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/oq5Xb.png\" rel=\"nofollow noreferrer\">Visualizing output in portal<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/wUmN7.png\" rel=\"nofollow noreferrer\">Test RRS output(web service)<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1485555724117,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":587,
        "Owner_creation_time":1449123268407,
        "Owner_last_access_time":1660064690440,
        "Owner_location":"East Newark, NJ, United States",
        "Owner_reputation":33,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":43,
        "Question_last_edit_time":1485558200660,
        "Answer_body":"<p>Make is a classic web service and see the JSON output getting from it. If it's providing all data you need.. go for it.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1486009486973,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41903982",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64338898,
        "Question_title":"How to load an experiment in azureml?",
        "Question_body":"<p>I have many experiment, like:<\/p>\n<p><img src=\"https:\/\/user-images.githubusercontent.com\/40580910\/95883598-82a07d00-0d51-11eb-847d-872452f6caa4.png\" alt=\"image\" \/><\/p>\n<p>and now, i want load an experiment<\/p>\n<pre><code>#%% sumonando os pacotes e verificando azureml.core\nimport azureml.core\nimport pandas as pd\nimport numpy as np\nimport logging\n\nprint(&quot;AzureML SDK Version: &quot;, azureml.core.VERSION)\n\n#%% Conectando ao azure e crinado o exparimento\n\nfrom azureml.core import Workspace, Experiment\n\nws = Workspace.from_config() \nprint(Experiment.list(ws))\n#%%\nExperiment = Experiment.from_directory('teste2-Monitor-Runs') `\n<\/code><\/pre>\n<p>but<\/p>\n<pre><code>&quot;error&quot;: {\n    &quot;message&quot;: &quot;No cache found for current project, try providing resource group and workspace \narguments&quot;\n}`\n<\/code><\/pre>\n<hr \/>\n<p>Content: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.experiment(class)?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.core.Experiment class - Azure Machine Learning Python<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1602604847240,
        "Question_score":1,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":254,
        "Owner_creation_time":1522634496793,
        "Owner_last_access_time":1663943775803,
        "Owner_location":"Rio de Janeiro, RJ, Brasil",
        "Owner_reputation":264,
        "Owner_up_votes":276,
        "Owner_down_votes":2,
        "Owner_views":23,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I believe it is that way.<\/p>\n<pre><code>from azureml.core import Experiment, Workspace\nExperiment = ws.experiments[&quot;teste2-Monitor-Runs&quot;]\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1602700300720,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64338898",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":36563769,
        "Question_title":"How to download the entire scored dataset from Azure machine studio?",
        "Question_body":"<p>I have an experiment in azure machine learning studio, and I would like to the see entire scored dataset.<\/p>\n\n<p>Naturally I used the 'visualise' option on the scored dataset but these yields only 100 rows (the test dataset is around 500 rows)<\/p>\n\n<p>I also tired the 'save as dataset' option, but then file does not open well with excel or text editor (special character encoding)<\/p>\n\n<p>Basically, I want to see the entire test data with scored labels as table or download as .csv maybe<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1460436159040,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":5296,
        "Owner_creation_time":1406266059940,
        "Owner_last_access_time":1663232189147,
        "Owner_location":"Link\u00f6ping, Sweden",
        "Owner_reputation":1677,
        "Owner_up_votes":82,
        "Owner_down_votes":2,
        "Owner_views":221,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Please try the Convert to CSV module: <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/faa6ba63-383c-4086-ba58-7abf26b85814\" rel=\"noreferrer\">https:\/\/msdn.microsoft.com\/library\/azure\/faa6ba63-383c-4086-ba58-7abf26b85814<\/a><\/p>\n\n<p>After you run the experiment, right click on the output of the module to download the CSV file.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1460437058280,
        "Answer_score":14,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36563769",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":46523924,
        "Question_title":"Adding python modules to AzureML workspace",
        "Question_body":"<p>I've been working recently on deploying a machine learning model as a web service. I used Azure Machine Learning Studio for creating my own Workspace ID and Authorization Token. Then, I trained LogisticRegressionCV model from <strong>sklearn.linear_model<\/strong> locally on my machine (using python 2.7.13) and with the usage of below code snippet I wanted to publish my model as web service:<\/p>\n\n<pre><code>from azureml import services\n\n@services.publish('workspaceID','authorization_token')\n@services.types(var_1= float, var_2= float)\n@services.returns(int)\n\ndef predicting(var_1, var_2):\n    input = np.array([var_1, var_2].reshape(1,-1)\nreturn model.predict_proba(input)[0][1]\n<\/code><\/pre>\n\n<p>where <em>input<\/em> variable is a list with data to be scored and <em>model<\/em> variable contains trained classifier. Then after defining above function I want to make a prediction on sample input vector:<\/p>\n\n<pre><code>predicting.service(1.21, 1.34)\n<\/code><\/pre>\n\n<p>However following error occurs:<\/p>\n\n<pre><code>RuntimeError: Error 0085: The following error occurred during script \nevaluation, please view the output log for more information:\n<\/code><\/pre>\n\n<p>And the most important message in log is: <\/p>\n\n<pre><code>AttributeError: 'module' object has no attribute 'LogisticRegressionCV'\n<\/code><\/pre>\n\n<p>The error is strange to me because when I was using normal <em>sklearn.linear_model.LogisticRegression<\/em> everything was fine. I was able to make predictions sending POST requests to created endpoint, so I guess <strong>sklearn<\/strong> worked correctly. \nAfter changing to <em>LogisticRegressionCV<\/em> it does not. <\/p>\n\n<p>Therefore I wanted to update sklearn on my workspace.<\/p>\n\n<p>Do you have any ideas how to do it? Or even more general question: how to install any python module on azure machine learning studio in a way to use predict functions of any model I develpoed locally?<\/p>\n\n<p>Thanks<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1506940629737,
        "Question_score":2,
        "Question_tags":"python|azure|scikit-learn|python-module|azure-machine-learning-studio",
        "Question_view_count":2578,
        "Owner_creation_time":1458750704640,
        "Owner_last_access_time":1662640939317,
        "Owner_location":"Warszawa, Polska",
        "Owner_reputation":186,
        "Owner_up_votes":108,
        "Owner_down_votes":1,
        "Owner_views":55,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For installing python module on Azure ML Studio, there is a section <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/cdb56f95-7f4c-404d-bde7-5bb972e6f232\/#Anchor_3\" rel=\"nofollow noreferrer\"><code>Technical Notes<\/code><\/a> of the offical document <code>Execute Python Script<\/code> which introduces it.<\/p>\n\n<p>The general steps as below.<\/p>\n\n<ol>\n<li>Create a Python project via <code>virtualenv<\/code> and active it.<\/li>\n<li>Install all packages you want via <code>pip<\/code> on the virtual Python environment, and then<\/li>\n<li>Package all files and directorys under the path <code>Lib\\site-packages<\/code> of your project as a zip file.<\/li>\n<li>Upload the zip package into your Azure ML WorkSpace as a dataSet.<\/li>\n<li>Follow the offical <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/execute-python-scripts#importing-existing-python-script-modules\" rel=\"nofollow noreferrer\">document<\/a> to import Python Module for your <code>Execute Python Script<\/code>.<\/li>\n<\/ol>\n\n<p>For more details, you can refer to the other similar SO thread <a href=\"https:\/\/stackoverflow.com\/questions\/46222606\/updating-pandas-to-version-0-19-in-azure-ml-studio\/46232963#46232963\">Updating pandas to version 0.19 in Azure ML Studio<\/a>, it even introduced how to update the version of Python packages installed by Azure.<\/p>\n\n<p>Hope it helps.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1507014239333,
        "Answer_score":2,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46523924",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":46963846,
        "Question_title":"Azure ML Workbench Kubernetes Deployment Failed",
        "Question_body":"<p>I am trying to deploy a prediction web service to Azure using ML Workbench process using cluster mode in this tutorial (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/preview\/tutorial-classifying-iris-part-3#prepare-to-operationalize-locally\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/preview\/tutorial-classifying-iris-part-3#prepare-to-operationalize-locally<\/a>)<\/p>\n\n<p>The model gets sent to the manifest, the scoring script and schema <\/p>\n\n<blockquote>\n  <p>Creating\n  service..........................................................Error\n  occurred: {'Error': {'Code': 'KubernetesDeploymentFailed', 'Details':\n  [{'Message': 'Back-off 40s restarting failed container=...pod=...',\n  'Code': 'CrashLoopBackOff'}], 'StatusCode': 400, 'Message':\n  'Kubernetes Deployment failed'}, 'OperationType': 'Service',\n  'State':'Failed', 'Id': '...', 'ResourceLocation':\n  '\/api\/subscriptions\/...', 'CreatedTime':\n  '2017-10-26T20:30:49.77362Z','EndTime': '2017-10-26T20:36:40.186369Z'}<\/p>\n<\/blockquote>\n\n<p>Here is the result of checking the ml service realtime logs <\/p>\n\n<pre><code>C:\\Users\\userguy\\Documents\\azure_ml_workbench\\projecto&gt;az ml service logs realtime -i projecto\n2017-10-26 20:47:16,118 CRIT Supervisor running as root (no user in config file)\n2017-10-26 20:47:16,120 INFO supervisord started with pid 1\n2017-10-26 20:47:17,123 INFO spawned: 'rsyslog' with pid 9\n2017-10-26 20:47:17,124 INFO spawned: 'program_exit' with pid 10\n2017-10-26 20:47:17,124 INFO spawned: 'nginx' with pid 11\n2017-10-26 20:47:17,125 INFO spawned: 'gunicorn' with pid 12\n2017-10-26 20:47:18,160 INFO success: rsyslog entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2017-10-26 20:47:18,160 INFO success: program_exit entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2017-10-26 20:47:22,164 INFO success: nginx entered RUNNING state, process has stayed up for &gt; than 5 seconds (startsecs)\n2017-10-26T20:47:22.519159Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting gunicorn 19.6.0\n2017-10-26T20:47:22.520097Z, INFO, 00000000-0000-0000-0000-000000000000, , Listening at: http:\/\/127.0.0.1:9090 (12)\n2017-10-26T20:47:22.520375Z, INFO, 00000000-0000-0000-0000-000000000000, , Using worker: sync\n2017-10-26T20:47:22.521757Z, INFO, 00000000-0000-0000-0000-000000000000, , worker timeout is set to 300\n2017-10-26T20:47:22.522646Z, INFO, 00000000-0000-0000-0000-000000000000, , Booting worker with pid: 22\n2017-10-26 20:47:27,669 WARN received SIGTERM indicating exit request\n2017-10-26 20:47:27,669 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\n2017-10-26T20:47:27.669556Z, INFO, 00000000-0000-0000-0000-000000000000, , Handling signal: term\n2017-10-26 20:47:30,673 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\n2017-10-26 20:47:33,675 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\nInitializing logger\n2017-10-26T20:47:36.564469Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting up app insights client\n2017-10-26T20:47:36.564991Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting up request id generator\n2017-10-26T20:47:36.565316Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting up app insight hooks\n2017-10-26T20:47:36.565642Z, INFO, 00000000-0000-0000-0000-000000000000, , Invoking user's init function\n2017-10-26 20:47:36.715933: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instruc\ntions, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36,716 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\n2017-10-26 20:47:36.716376: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instruc\ntions, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36.716542: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructio\nns, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36.716703: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructi\nons, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36.716860: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructio\nns, but these are available on your machine and could speed up CPU computations.\nthis is the init\n2017-10-26T20:47:37.551940Z, INFO, 00000000-0000-0000-0000-000000000000, , Users's init has completed successfully\nUsing TensorFlow backend.\n2017-10-26T20:47:37.553751Z, INFO, 00000000-0000-0000-0000-000000000000, , Worker exiting (pid: 22)\n2017-10-26T20:47:37.885303Z, INFO, 00000000-0000-0000-0000-000000000000, , Shutting down: Master\n2017-10-26 20:47:37,885 WARN killing 'gunicorn' (12) with SIGKILL\n2017-10-26 20:47:37,886 INFO stopped: gunicorn (terminated by SIGKILL)\n2017-10-26 20:47:37,889 INFO stopped: nginx (exit status 0)\n2017-10-26 20:47:37,890 INFO stopped: program_exit (terminated by SIGTERM)\n2017-10-26 20:47:37,891 INFO stopped: rsyslog (exit status 0)\n\nReceived 41 lines of log\n<\/code><\/pre>\n\n<p>My best guess is theres something silent happening to cause \"WARN received SIGTERM indicating exit request\". The rest of the scoring.py script seems to kick off - see tensorflow get initiated and the \"this is the init\" print statement.<\/p>\n\n<p><a href=\"http:\/\/127.0.0.1:63437\" rel=\"nofollow noreferrer\">http:\/\/127.0.0.1:63437<\/a> is accessible from my local machine, but the ui endpoint is blank.<\/p>\n\n<p>Any ideas on how to get this up and running in an Azure cluster? I'm not very familiar with how Kubernetes works, so any basic debugging guidance would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1509052128353,
        "Question_score":2,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-workbench",
        "Question_view_count":447,
        "Owner_creation_time":1421081882987,
        "Owner_last_access_time":1663964806323,
        "Owner_location":null,
        "Owner_reputation":588,
        "Owner_up_votes":47,
        "Owner_down_votes":3,
        "Owner_views":64,
        "Question_last_edit_time":1511310430992,
        "Answer_body":"<p>We discovered a bug in our system that could have caused this. The fix was deployed last night. Can you please try again and let us know if you still encounter this issue?<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1509114740967,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46963846",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":66592313,
        "Question_title":"Get local workspace in azureml",
        "Question_body":"<p>I am trying to run a machine learning experiment in azureml.<\/p>\n<p>I can't figure out how to get the workspace context from the control script.  Examples like <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-bring-data#control-script\" rel=\"nofollow noreferrer\">this one<\/a> in the microsoft docs use Workspace.from_config().  When I use this in the control script I get the following error:<\/p>\n<blockquote>\n<p>&quot;message&quot;: &quot;We could not find config.json in: [path] or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.&quot;<\/p>\n<\/blockquote>\n<p>I've also tried including my subscription id and the resource specs like so:<\/p>\n<pre><code>subscription_id = 'id'\nresource_group = 'name'\nworkspace_name = 'name'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n<\/code><\/pre>\n<p>In this case I have to monitor the log and authenticate on each run as I would locally.<\/p>\n<p>How do you get the local workspace from a control script for azureml?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1615507417267,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":333,
        "Owner_creation_time":1512520584493,
        "Owner_last_access_time":1663886987607,
        "Owner_location":"Bloomington, IN, USA",
        "Owner_reputation":868,
        "Owner_up_votes":109,
        "Owner_down_votes":4,
        "Owner_views":51,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This had no answers for 10 months, and now they are coming in :).  I figuerd this out quite a while ago but haven't gotten around to posting the answer.  Here it is.<\/p>\n<p>From the training script, you can get the workspace from the run context as follows:<\/p>\n<pre><code>from azureml.core import Run\nRun.get_context()\nws = run.experiment.workspace\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1641958092267,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66592313",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":68349739,
        "Question_title":"Packaging multiple models from Azure ML experiment",
        "Question_body":"<p>So I have started to create a MLOps pipeline that is training multiple models within multiple pipeline steps.<\/p>\n<p>The picture below is the graphical representation of the coded pipeline steps within the Azure ML Studio.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/QvZxX.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QvZxX.png\" alt=\"pipeline steps\" \/><\/a><\/p>\n<p>These steps run fine and both the end steps produce the models I want (Train Data - Non EOW TFIDF and Train Data - EOW TFIDF)...<\/p>\n<p>However this is where I get stuck with registering and packaging the model parts for deployment. These models get produced and stored within the individual pipeline step (see below for the model output of Train Data - Non EOW TFIDF)<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/MoE5W.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/MoE5W.png\" alt=\"non eow step output\" \/><\/a><\/p>\n<p>but I don't know how I would register the model outputs from both pipeline steps together as the docs I have read for registering a model seem to only reference the ability to register one model from one path.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none--sample-input-dataset-none--sample-output-dataset-none--resource-configuration-none-\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none--sample-input-dataset-none--sample-output-dataset-none--resource-configuration-none-<\/a><\/p>\n<p>Basically, is it possible to produce multiple model outputs from multiple pipeline steps and register them together as one??<\/p>\n<p>Thanks in advance for the help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626102561633,
        "Question_score":1,
        "Question_tags":"python|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":578,
        "Owner_creation_time":1567669433587,
        "Owner_last_access_time":1663943001867,
        "Owner_location":null,
        "Owner_reputation":130,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1626351577672,
        "Answer_body":"<p>Here is sample for Multi-model Register and deploy. <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-multi-model\/multi-model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-multi-model\/multi-model-register-and-deploy.ipynb<\/a><\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1627574880243,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68349739",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":29485562,
        "Question_title":"Machine Learning-Classifying web page as address and no-address by content",
        "Question_body":"<p>Currently I am using azure machine learning .I train my ML with sets of data of two types they are nothing but web page content with address and without address<\/p>\n\n<p><strong><em>TRAINING INPUT:<\/em><\/strong><\/p>\n\n<pre><code>i.e)\nthis is a address no 24\/5    address\nthis is no address    no-address \n<\/code><\/pre>\n\n<p>I am using two-class bayesian classification to classify them should i use any other method <\/p>\n\n<p><strong><em>GIVEN INPUT:<\/em><\/strong><\/p>\n\n<pre><code>i.e)\nThis a address 12\/4 \n<\/code><\/pre>\n\n<p><strong><em>OBTAINED OUTPUT:<\/em><\/strong><\/p>\n\n<pre><code>i.e)\ncontent    score    probability\nThis a address 12\/4    no-address    0.54\n<\/code><\/pre>\n\n<p><strong><em>EXPECTED OUTPUT:<\/em><\/strong><\/p>\n\n<pre><code>i.e)\ncontent    score    probability\nThis a address 12\/4    address    with higher probability \n<\/code><\/pre>\n\n<p>My experiment looks like :<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/exzqV.png\" alt=\"enter image description here\"><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1428389541860,
        "Question_score":0,
        "Question_tags":"machine-learning|azure-machine-learning-studio",
        "Question_view_count":195,
        "Owner_creation_time":1415957539617,
        "Owner_last_access_time":1664070583007,
        "Owner_location":"Chennai, India",
        "Owner_reputation":7743,
        "Owner_up_votes":2734,
        "Owner_down_votes":4,
        "Owner_views":1464,
        "Question_last_edit_time":1428389971683,
        "Answer_body":"<p>You need to use the Feature Hashing module to convert the text into word features. This, however, might not be enough as words are not good features for your problem. You may want to do some processing of the text and create more useful features (perhaps detecting the presence of zip codes, positions of numbers, etc...)<\/p>\n\n<p>Edit: Using the raw text column as one feature will not get you anywhere. You don\u2019t want your model to learn the addresses the way they are written. Instead, you need to learn patterns in the text that provide evidence for address vs. non-address instances.\nWhen you use feature hashing, the text column will be transformed to multiple word (or n-gram) columns, where the values represent counts of those words in each text input. The problem here is overfitting. For example, these two addresses have no words in common:\n\u201c100 broadway st, GA\u201d and \u201c200 main rd, NY\u201d but it\u2019s clear they have similar structure. One way to create \u2018useful features\u2019 is to replace the words with tags: \u201c#NUM #TXT, #STATE\u201d and use feature hashing (bi-grams) to create features such as \u201c#NUM #TXT\u201d and \u201c, #STATE\u201d. As you can see, these bi-grams count as evidence in both addresses and suggest some kind of similarity between them (compared to other non-address instances). Of course this is an oversimplification of the problem but I hope you see why you can\u2019t use the raw text or plain feature hashing.<br>\nYou can still use the Azure ML modules for feature hashing, training, and scoring in addition to an \u2018Execute R\u2019 module to do the text processing before training.<\/p>\n\n<p>Edit: Example of feature hashing usage: <a href=\"http:\/\/gallery.azureml.net\/Details\/cf65bf129fee4190b6f48a53e599a755\" rel=\"nofollow\">http:\/\/gallery.azureml.net\/Details\/cf65bf129fee4190b6f48a53e599a755<\/a> <\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1428422166660,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1428492704452,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/29485562",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":51780562,
        "Question_title":"How to prevent Azure ML Studio from converting a feature column to DateTime while importing a dataset",
        "Question_body":"<p>I\u2019m having some issues trying to load a dataset in Azure ML Studio, a dataset containing a column that looks like a DateTime, but is in fact a string. Azure ML Studio converts the values to DateTimes internally, and no amount of wrangling seems to convince it of the that they\u2019re in fact strings.<\/p>\n\n<p>This is an issue, because during conversion the values lose precision and start appearing as duplicates whereas in fact they are unique. Does anybody know if ML Studio can be configured not to infer data types for columns while importing a dataset?<\/p>\n\n<p>Now, for the long(er) story :)<\/p>\n\n<p>I\u2019m working here with a public dataset - specifically <a href=\"https:\/\/www.kaggle.com\/c\/new-york-city-taxi-fare-prediction\" rel=\"noreferrer\">Kaggle\u2019s New York City Fare Prediction<\/a> competition. I wanted to see if I could do a quick-and-dirty solution using Azure ML Studio, however the dataset\u2019s unique key values are of the form\n<code>\n    2015-01-27 13:08:24.0000003\n    2015-01-27 13:08:24.0000002\n    2011-10-06 12:10:20.0000001\n<\/code>\nand so on. <\/p>\n\n<p>When importing them in my experiment the key values get converted to DateTime, making them no longer unique, even though they\u2019re unique in the csv. Needless to say, this prevents me from submitting any solution to Kaggle, since I can\u2019t identify the rows uniquely :).<\/p>\n\n<p>I\u2019ve tried the following:<\/p>\n\n<ul>\n<li>edit the metadata of the dataset after it has been loaded and setting the data type of the column to string, but this doesn\u2019t do much as the precision has already been lost<\/li>\n<li>import the dataset from an Azure blob, convert it to csv and then loading it in Jupyter\/Python - this brings me the same (duplicated) keys. <\/li>\n<li>loading the dataset locally with pandas works, as expected.<\/li>\n<\/ul>\n\n<p>I\u2019ve reproduced this behavior with both the big, 5.5GB <code>train<\/code> dataset, but also with the more manageable <code>sample_submission<\/code> dataset. <\/p>\n\n<p>Curious to know if there is some sort of workaround to tell ML Studio not to try converting this column while loading the dataset. I'm looking here specifically for Azure ML Studio-only solutions, as I don't want to do any preprocessing on the dataset.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1533883639527,
        "Question_score":5,
        "Question_tags":"azure|azure-machine-learning-studio|kaggle|ml-studio",
        "Question_view_count":401,
        "Owner_creation_time":1250158552417,
        "Owner_last_access_time":1663847198323,
        "Owner_location":"Romania",
        "Owner_reputation":7916,
        "Owner_up_votes":1735,
        "Owner_down_votes":33,
        "Owner_views":801,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I have tried with you sample data and here is my quick and dirty solution:\n1) Add any symbol (I've added the '#') in front of each date\n2) Load it to AML Studio (it is now considered as a string feature)\n3) Add a Python\/R component to remove the '#' symbol and explicitly convert the column to string (as.string(columnname) or str(columnname))<\/p>\n\n<p>Hope this helps<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1534433513133,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51780562",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67639665,
        "Question_title":"Azure ML not able to create conda environment (exit code: -15)",
        "Question_body":"<p>When I try to run the experiment defined in <a href=\"https:\/\/github.com\/MicrosoftLearning\/mslearn-dp100\/blob\/main\/06%20-%20Work%20with%20Data.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a> in  notebook, I encountered an error when it is creating the conda env. The error occurs when the below cell is executed:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Experiment, ScriptRunConfig, Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.widgets import RunDetails\n\n\n# Create a Python environment for the experiment\nsklearn_env = Environment(&quot;sklearn-env&quot;)\n\n# Ensure the required packages are installed (we need scikit-learn, Azure ML defaults, and Azure ML dataprep)\npackages = CondaDependencies.create(conda_packages=['scikit-learn','pip'],\n                                    pip_packages=['azureml-defaults','azureml-dataprep[pandas]'])\nsklearn_env.python.conda_dependencies = packages\n\n# Get the training dataset\ndiabetes_ds = ws.datasets.get(&quot;diabetes dataset&quot;)\n\n# Create a script config\nscript_config = ScriptRunConfig(source_directory=experiment_folder,\n                              script='diabetes_training.py',\n                              arguments = ['--regularization', 0.1, # Regularizaton rate parameter\n                                           '--input-data', diabetes_ds.as_named_input('training_data')], # Reference to dataset\n                              environment=sklearn_env)\n\n# submit the experiment\nexperiment_name = 'mslearn-train-diabetes'\nexperiment = Experiment(workspace=ws, name=experiment_name)\nrun = experiment.submit(config=script_config)\nRunDetails(run).show()\nrun.wait_for_completion() \n<\/code><\/pre>\n<p>Everytime I run this, I always faced the issue of creating the conda env as below:<\/p>\n<pre><code>Creating conda environment...\nRunning: ['conda', 'env', 'create', '-p', '\/home\/azureuser\/.azureml\/envs\/azureml_000000000000', '-f', 'azureml-environment-setup\/mutated_conda_dependencies.yml']\nCollecting package metadata (repodata.json): ...working... done\nSolving environment: ...working... done\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\n\nInstalling pip dependencies: ...working... \n\nAttempting to clean up partially built conda environment: \/home\/azureuser\/.azureml\/envs\/azureml_000000000000\nRemove all packages in environment \/home\/azureuser\/.azureml\/envs\/azureml_000000000000:\nCreating conda environment failed with exit code: -15\n<\/code><\/pre>\n<p>I could not find anything useful on the internet and this is not the only script where it fail. When I am try to run other experiments I have sometimes faced this issue. One solution which worked in the above case is I moved the pandas from pip to conda and it was able to create the coonda env. Example below:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Ensure the required packages are installed (we need scikit-learn, Azure ML defaults, and Azure ML dataprep)\npackages = CondaDependencies.create(conda_packages=['scikit-learn','pip'],\n                                    pip_packages=['azureml-defaults','azureml-dataprep[pandas]'])\n\n<\/code><\/pre>\n<pre class=\"lang-py prettyprint-override\"><code># Ensure the required packages are installed (we need scikit-learn, Azure ML defaults, and Azure ML dataprep)\npackages = CondaDependencies.create(conda_packages=['scikit-learn','pip','pandas'],\n                                    pip_packages=['azureml-defaults','azureml-dataprep'])\n\n<\/code><\/pre>\n\n<p>The error message (or the logs from Azure) is also not much help. Would apprecite if a proper solution is available.<\/p>\n<p>Edit: I have recently started learning to use Azure for Machine learning and so if I am not sure if I am missing something? I assume the example notebooks should work as is hence raised this question.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1621610368967,
        "Question_score":4,
        "Question_tags":"anaconda|azure-machine-learning-service",
        "Question_view_count":2373,
        "Owner_creation_time":1488207114693,
        "Owner_last_access_time":1655016627737,
        "Owner_location":"India",
        "Owner_reputation":493,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1621615281776,
        "Answer_body":"<h2>short answer<\/h2>\n<p>Totally been in your shoes before. This code sample seems a smidge out of date. Using <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/datasets-tutorial\/train-with-datasets\/train-with-datasets.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a> as a reference, can you try the following?<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>packages = CondaDependencies.create(\n    pip_packages=['azureml-defaults','scikit-learn']\n)\n<\/code><\/pre>\n<h2>longer  answer<\/h2>\n<p><a href=\"https:\/\/www.anaconda.com\/blog\/using-pip-in-a-conda-environment\" rel=\"nofollow noreferrer\">Using pip with Conda<\/a> is not always smooth sailing. In this instance, conda isn't reporting up the issue that pip is having. The solution is to create and test this environment locally where we can get more information, which will at least will give you a more informative error message.<\/p>\n<ol>\n<li>Install anaconda  or miniconda (or use an Azure ML Compute Instance which has conda pre-installed)<\/li>\n<li>Make a  file called environment.yml that looks like this<\/li>\n<\/ol>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: aml_env\ndependencies:\n - python=3.8\n - pip=21.0.1\n - pip:\n    - azureml-defaults\n    - azureml-dataprep[pandas]\n    - scikit-learn==0.24.1\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Create this environment with the command <code>conda env create -f environment.yml<\/code>.<\/li>\n<li>respond to any discovered error message<\/li>\n<li>If there' no error, use this new <code>environment.yml<\/code> with Azure ML like so<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>sklearn_env = Environment.from_conda_specification(name = 'sklearn-env', file_path = '.\/environment.yml')\n<\/code><\/pre>\n<h2>more context<\/h2>\n<p>the error I'm guessing that's happening is when you reference a pip requirements file from a conda environment file. In this scenario, conda calls <code>pip install -r  requirements.txt<\/code> and if that command errors out, conda can't report the error.<\/p>\n<h3><code>requirements.txt<\/code><\/h3>\n<pre><code>scikit-learn==0.24.1\nazureml-dataprep[pandas]\n<\/code><\/pre>\n<h3><code>environment.yml<\/code><\/h3>\n<pre><code>name: aml_env\ndependencies:\n - python=3.8\n - pip=21.0.1\n - pip:\n    - -rrequirements.txt\n<\/code><\/pre>",
        "Answer_comment_count":6,
        "Answer_creation_time":1621615054950,
        "Answer_score":3,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1621619024732,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67639665",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":39150834,
        "Question_title":"probability on azure recommendations api",
        "Question_body":"<p>I am using the azure recommendation api on <a href=\"http:\/\/recommendations.azurewebsites.net\/\" rel=\"nofollow noreferrer\">http:\/\/recommendations.azurewebsites.net\/<\/a>.\nI prepared the catalog to be like <code>&lt;Item Id&gt;<\/code>, <code>&lt;Item Name&gt;<\/code>, <code>&lt;Item Category&gt;<\/code>, <code>&lt;Features list&gt;<\/code> and the usage file : <code>&lt;userId&gt;<\/code>, <code>&lt;ItemId&gt;<\/code>.\nNow when I test the recommender, I always get a probability of 0.5 for all items, so I had to presume something is not right.\nIn order to know what's the problem I added two items to the catalog \none with same features as an other item but with different name and id,\nand an other item with different id and one different feature.\nI still get the 0.5 probability and now i'm sure something is not right but I still can figure out what the problem.<\/p>\n\n<p>here is a screenshot of what I get when I add the item to the cart<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/zhwiq.png\" alt=\"\"><\/p>\n\n<p>Is there any possibility to use the azure ml matchbox recommender with features and without ratings? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1472144085380,
        "Question_score":0,
        "Question_tags":"recommendation-engine|azure-machine-learning-studio",
        "Question_view_count":98,
        "Owner_creation_time":1447254948300,
        "Owner_last_access_time":1663768823953,
        "Owner_location":"Tunis, Gouvernorat de Tunis, Tunisie",
        "Owner_reputation":31,
        "Owner_up_votes":61,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Question_last_edit_time":1491136060303,
        "Answer_body":"<p>Tayehi, <\/p>\n\n<p>Nice to meet you. I am the program manager in charge of the recommendations API.\n2 things:<\/p>\n\n<ol>\n<li><p>If you get a 0.5 probability you are most likely getting \"default recommendations\". This usually means that you do not have enough training data or that there are not enough co-occurrences for the item you are testing in the data. To describe the extreme case, imagine an item A that only gets purchased with an item B only one or two times -- it would be hard to say with confidence (statistical significance) that someone that likes item A is also likely to like item B.<\/p><\/li>\n<li><p>It looks like you are still using the old recommendations API. I would like to encourage you to use our newer version (the Recommendations API cognitive service). Please take a look at <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/cognitive-services-migration-from-dm\/to\" rel=\"nofollow\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/cognitive-services-migration-from-dm\/to<\/a> help you in this process.<\/p><\/li>\n<\/ol>\n\n<p>Thanks!\nLuis Cabrera\nCortana Intelligence Applications.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1476373208683,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1476476017407,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/39150834",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62623166,
        "Question_title":"Azure ML Error: TimeSeriesImputer object has no attribute '_known_df'",
        "Question_body":"<p>Running <a href=\"http:\/\/%20https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-orange-juice-sales\/auto-ml-forecasting-orange-juice-sales.ipynb\" rel=\"nofollow noreferrer\">this orange juice sales notebook<\/a> I get the below error with the <code>.forecast()<\/code> method.<\/p>\n<h3>code<\/h3>\n<pre class=\"lang-py prettyprint-override\"><code># The featurized data, aligned to y, will also be returned.\n# This contains the assumptions that were made in the forecast\n# and helps align the forecast to the original data\ny_predictions, X_trans = fitted_model.forecast(X_test)\n<\/code><\/pre>\n<h3>Error (<a href=\"https:\/\/gist.github.com\/swanderz\/201819978b6719bbed1826a02bb2fb47\" rel=\"nofollow noreferrer\">full stacktrace<\/a>):<\/h3>\n<pre><code>**AttributeError: 'TimeSeriesImputer' object has no attribute '_known_df'**\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593350881953,
        "Question_score":1,
        "Question_tags":"time-series|azure-machine-learning-studio|forecast|azure-machine-learning-service",
        "Question_view_count":256,
        "Owner_creation_time":1444548825377,
        "Owner_last_access_time":1641987792040,
        "Owner_location":null,
        "Owner_reputation":303,
        "Owner_up_votes":105,
        "Owner_down_votes":7,
        "Owner_views":60,
        "Question_last_edit_time":1594142196016,
        "Answer_body":"<p>This is commonly fixed by upgrading to the latest SDK. You can do this by running <code>pip install --upgrade azureml-sdk[explain,automl]<\/code>.<\/p>\n<p>Thanks,\nSabina<\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1593542625630,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62623166",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":69768602,
        "Question_title":"How to output data to Azure ML Batch Endpoint correctly using python?",
        "Question_body":"<p>When invoking Azure ML Batch Endpoints (creating jobs for inferencing), the run() method should return a pandas DataFrame or an array as explained <a href=\"https:\/\/i.stack.imgur.com\/azJDX.png\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<p>However this example shown, doesn't represent an output with headers for a csv, as it is often needed.<\/p>\n<p>The first thing I've tried was to return the data as a <em>pandas DataFrame<\/em> and the result is just a simple csv with a single column and without the headers.<\/p>\n<p>When trying to pass the values with several columns and it's corresponding headers, to be later saved as csv, as a result, I'm getting awkward square brackets (representing the lists in python) and the apostrophes (representing strings)<\/p>\n<p>I haven't been able to find documentation elsewhere, to fix this:\n<a href=\"https:\/\/i.stack.imgur.com\/azJDX.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/azJDX.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635509192960,
        "Question_score":2,
        "Question_tags":"azure|batch-processing|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":295,
        "Owner_creation_time":1589293508567,
        "Owner_last_access_time":1663681781073,
        "Owner_location":null,
        "Owner_reputation":833,
        "Owner_up_votes":9,
        "Owner_down_votes":9,
        "Owner_views":55,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This is the way I found to create a clean output in csv format using python, from a batch endpoint invoke in AzureML:<\/p>\n<pre><code>def run(mini_batch):\n    batch = []\n    for file_path in mini_batch:\n        df = pd.read_csv(file_path)\n        \n        # Do any data quality verification here:\n        if 'id' not in df.columns:\n            logger.error(&quot;ERROR: CSV file uploaded without id column&quot;)\n            return None\n        else:\n            df['id'] = df['id'].astype(str)\n\n        # Now we need to create the predictions, with previously loaded model in init():\n        df['prediction'] = model.predict(df)\n        # or alternative, df[MULTILABEL_LIST] = model.predict(df)\n\n        batch.append(df)\n\n    batch_df = pd.concat(batch)\n\n    # After joining all data, we create the columns headers as a string,\n    # here we remove the square brackets and apostrophes:\n    azureml_columns = str(batch_df.columns.tolist())[1:-1].replace('\\'','')\n    result = []\n    result.append(azureml_columns)\n\n    # Now we have to parse all values as strings, row by row, \n    # adding a comma between each value\n    for row in batch_df.iterrows():\n        azureml_row = str(row[1].values).replace(' ', ',')[1:-1].replace('\\'','').replace('\\n','')\n        result.append(azureml_row)\n\n    logger.info(&quot;Finished Run&quot;)\n    return result\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1635509192960,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1638266985652,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69768602",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":50035628,
        "Question_title":"Include additional scripts when deploying a Azure ML experimentation service",
        "Question_body":"<p>When training my model the data I start with consist of rows of json data and the expected values I would like to predict from that json data. The json data follows the schema I my deployed service will receive the input as. Before training I run a number of python functions to transform the data and extract features calculated from the raw json data. It is that transformed data which my model is trained on.<\/p>\n\n<p>I have extracted the code to transform the json data into the input my model expects into a separate python file. Now I would like to have my scoring script use that python script to prepare the input sent to the service before feeding it into my trained model.<\/p>\n\n<p>Is there a way to include the data transformation script with the scoring script when deploying my service using the cli command:<\/p>\n\n<pre><code>az ml service create realtime \n    -f &lt;scoring-script&gt;.py \n    --model-file model.pkl \n    -s service_schema.json \n    -n &lt;some-name&gt; \n    -r python \n    --collect-model-data true \n    -c aml_config\\conda_dependencies.yml\n<\/code><\/pre>\n\n<p><em>(the new lines in the above command added for clarity)<\/em><\/p>\n\n<p>The two ways I've come up with is to either:<\/p>\n\n<ul>\n<li>Create my own base docker image that contains the transformation script and use that image as the base for my service. Seems a bit cumbersome to do if I need similar (but different) data transformations for later models.<\/li>\n<li>Concatenate the transformation script with my scoring script into a single file. Seems a bit hacky.<\/li>\n<\/ul>\n\n<p><strong>Is there another way to achive my goal of having a separate data transformation script used both in training and in scoring?<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1524721574190,
        "Question_score":1,
        "Question_tags":"python|azure-machine-learning-studio|azure-machine-learning-workbench",
        "Question_view_count":252,
        "Owner_creation_time":1275401694660,
        "Owner_last_access_time":1664029322520,
        "Owner_location":"Gothenburg, Sweden",
        "Owner_reputation":18969,
        "Owner_up_votes":699,
        "Owner_down_votes":20,
        "Owner_views":517,
        "Question_last_edit_time":1524806425600,
        "Answer_body":"<p>So running <code>az ml service create realtime -h<\/code> provides information about the <code>-d<\/code> flag.<\/p>\n\n<p><code>-d : Files and directories required by the service. Multiple dependencies can be specified with additional -d arguments.<\/code><\/p>\n\n<p>Please try using this flag and provide the additional python file that you would like to call too from your <code>score.py<\/code><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1524806224530,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50035628",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":37540703,
        "Question_title":"Test multiple algorithms in one experiment",
        "Question_body":"<p>Is there any way to test multiple algorithms rather than doing it once for each and every algorithm; then checking the result? There are a lot of times where I don\u2019t really know which one to use, so I would like to test multiple and get the result (error rate) fairly quick in Azure Machine Learning Studio.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_time":1464683630827,
        "Question_score":1,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio",
        "Question_view_count":425,
        "Owner_creation_time":1456309738853,
        "Owner_last_access_time":1471868452243,
        "Owner_location":null,
        "Owner_reputation":39,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1465977920520,
        "Answer_body":"<p>The module you are looking for, is the one called \u201c<strong>Cross-Validate Model<\/strong>\u201d. It basically splits whatever comes in from the input-port (dataset) into 10 pieces, then reserves the last piece as the \u201canswer\u201d; and trains the nine other subset models and returns a set of accuracy statistics measured towards the last subset. What you would look at is the column called \u201cMean absolute error\u201d which is the average error for the trained models. You can connect whatever algorithm you want to one of the ports, and subsequently you will receive the result for that algorithm in particular after you \u201cright-click\u201d the port which gives the score.<\/p>\n\n<p>After that you can assess which algorithm did the best. And as a pro-tip; you could use the <strong>Filter-based-feature selection<\/strong> to actually see which column had a significant impact on the result.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1464685093653,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37540703",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":45051055,
        "Question_title":"Read multiple CSV files in Azure ML Python Script",
        "Question_body":"<p>I have 4 csv files that are inputs to the python script in azure ML, but the widget has only 2 inputs for dataframes and the third for a zip file. I tried to put the csv files in a zipped folder and connect it to the third input for the script but that also did not work :\n<a href=\"https:\/\/i.stack.imgur.com\/FkxRE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FkxRE.png\" alt=\"Image of workspace\"><\/a><\/p>\n\n<p>I would like to know how to read multiple csv files in the python script.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1499844354733,
        "Question_score":1,
        "Question_tags":"python|csv|azure|azure-machine-learning-studio",
        "Question_view_count":944,
        "Owner_creation_time":1470376815797,
        "Owner_last_access_time":1660941481503,
        "Owner_location":null,
        "Owner_reputation":596,
        "Owner_up_votes":72,
        "Owner_down_votes":3,
        "Owner_views":57,
        "Question_last_edit_time":1499850837700,
        "Answer_body":"<p>Here's some more detail on the approach others have outlined above. Try replacing the code currently in the \"Execute Python Script\" module with the following:<\/p>\n\n<pre><code>import pandas as pd\nimport os\ndef azureml_main(dataframe1=None, dataframe2=None):\n    print(os.listdir('.'))\n    return(pd.DataFrame([]))\n<\/code><\/pre>\n\n<p>After running the experiment, click on the module. There should be a \"View output log\" link now in the right-hand bar. I get something like the following:<\/p>\n\n<pre><code>[Information]         Started in [C:\\temp]\n[Information]         Running in [C:\\temp]\n[Information]         Executing 4af67c05ba02417a980f6a16e84e61dc with inputs [] and generating outputs ['.maml.oport1']\n[Information]         Extracting Script Bundle.zip to .\\Script Bundle\n[Information]         File Name                                             Modified             Size\n[Information]         temp.csv                                       2016-05-06 13:16:56           52\n[Information]         [ READING ] 0:00:00\n[Information]         ['4af67c05ba02417a980f6a16e84e61dc.py', 'Script Bundle', 'Script Bundle.zip']\n<\/code><\/pre>\n\n<p>This tells me that the contents of my zip file have been extracted to the <code>C:\\temp\\Script Bundle<\/code> folder. In my case the zip file contained just one CSV file, <code>temp.csv<\/code>: your output would probably have four files. You may also have zipped a folder containing your four files, in which case the filepath would be one layer deeper. You can use the <code>os.listdir()<\/code> to explore your directory structure further if necessary.<\/p>\n\n<p>Once you think you know the full filepaths for your CSV files, edit your Execute Python Script module's code to load them, e.g.:<\/p>\n\n<pre><code>import pandas as pd\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    df = pd.read_csv('C:\/temp\/Script Bundle\/temp.csv')\n    # ...load other files and merge into a single dataframe...\n    return(df)\n<\/code><\/pre>\n\n<p>Hope that helps!<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1500412322170,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45051055",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72637756,
        "Question_title":"Azure Machine Learning Model deployment as AKS web service from multiple workspaces",
        "Question_body":"<p>I am trying to determine how the Az ML model deployment works with AKS. If you have a single AKS cluster but attach from two separate workspaces, will models from both workspaces get deployed into different azureml-fe's with different IP addresses OR a single azureml-fe with a single IP address? Reason I ask is because I want to purchase a certificate but am unsure if all the models (regardless of workspace) will get exposed under the same IP Address OR multiple IP Addresses? If its the former, I can do it with one certificate...otherwise I have to do it with multiple certificates or SAN based certificates. So if anyone has experience with this, please let me know!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655326759547,
        "Question_score":0,
        "Question_tags":"azure-aks|azure-machine-learning-service",
        "Question_view_count":127,
        "Owner_creation_time":1376577570773,
        "Owner_last_access_time":1663954112673,
        "Owner_location":null,
        "Owner_reputation":301,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":52,
        "Question_last_edit_time":null,
        "Answer_body":"<p>By checking AKS webservice class, we can do the multiple services links to single AKS cluster. The endpoint management was described in <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.aks.akswebservice?view=azure-ml-py\" rel=\"nofollow noreferrer\">document<\/a>, but this is representing one-to-one cluster and service. For multiple workspaces refer <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.aksendpoint?view=azure-ml-py#azureml-core-webservice-aksendpoint-create-version\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>\n<p>Regarding <strong><code>azureml-fe<\/code><\/strong>. There will be one <strong>azureml-fe<\/strong> for one cluster. That means, when we are using different workspaces for deployment into one AKS, then only <strong>one<\/strong> <strong>azureml-fe<\/strong> and can be considered to take <strong>one certificate.<\/strong><\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-deploy-azure-kubernetes-service?tabs=python#azure-ml-router\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-deploy-azure-kubernetes-service?tabs=python#azure-ml-router<\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1655345708050,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72637756",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":48879595,
        "Question_title":"ImportError: No module named cassandra in Azure Machine Learning Studio",
        "Question_body":"<p>I am trying to install python package cassandra driver in Azure Machine Learning studio. I am following this answer from <a href=\"https:\/\/stackoverflow.com\/questions\/44371692\/install-python-packages-in-azure-ml\">here<\/a>. Unfortunately i don't see any wheel file for cassandra-driver <a href=\"https:\/\/pypi.python.org\/pypi\/cassandra-driver\/\" rel=\"nofollow noreferrer\">https:\/\/pypi.python.org\/pypi\/cassandra-driver\/<\/a> so i downloaded the .tar file and converted to zip.<\/p>\n<p>I included this .zip file as dataset and connected to python script<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/omsO9.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/omsO9.jpg\" alt=\"jpg1\" \/><\/a><\/p>\n<p>But when i run it, it says No module named cassandra\n<a href=\"https:\/\/i.stack.imgur.com\/4DKTB.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4DKTB.jpg\" alt=\"jpg2\" \/><\/a><\/p>\n<p>Does this work only with wheel file? Any solution is much appreciated.<\/p>\n<p>I am using Python Version :  Anoconda 4.0\/Python 3.5<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1519110374087,
        "Question_score":1,
        "Question_tags":"python|python-3.x|azure|cassandra|azure-machine-learning-studio",
        "Question_view_count":500,
        "Owner_creation_time":1489644560420,
        "Owner_last_access_time":1646025882010,
        "Owner_location":"Planet Earth",
        "Owner_reputation":791,
        "Owner_up_votes":55,
        "Owner_down_votes":4,
        "Owner_views":253,
        "Question_last_edit_time":1592644375060,
        "Answer_body":"<p>I got it working. Changed the folder inside .zip file to <code>\"cassandra\"<\/code> (just like cassandra package). <\/p>\n\n<p>And in the Python script, i added <\/p>\n\n<pre><code>from cassandra import *\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1519124227917,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48879595",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73187536,
        "Question_title":"Error while detaching AKS cluster through Azure ML SDK extension",
        "Question_body":"<p>I created an AKS cluster using Azure Machine Learning SDK extension and I attached to the workspace created. When the cluster is created and attached, I doesn't show any error. When I am trying to detach it from workspace, it is not accepting the operations.<\/p>\n<p>I would like to detach the existing AKS cluster from workspace either by program manner, using CLI or even using Azure portal.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659308799773,
        "Question_score":0,
        "Question_tags":"azure|azure-aks|azure-machine-learning-studio",
        "Question_view_count":47,
        "Owner_creation_time":1651093614703,
        "Owner_last_access_time":1659335138797,
        "Owner_location":"Netherland",
        "Owner_reputation":19,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If we are using any <strong>extensions of SDK or Azure CLI<\/strong> for machine learning to detach AKS cluster, it <strong>will not work<\/strong> and it will not get deleted or detached. Instead, we need to use <strong>Azure CLI with AKS<\/strong>. There are two types of implementations we can perform.<\/p>\n<p><strong>Python:<\/strong><\/p>\n<pre><code>Aks_target.detach()\n<\/code><\/pre>\n<p><strong>Azure CLI:<\/strong><\/p>\n<p>Before performing this step, we need to get the details of the working AKS cluster name attached to our workspace. Resource Group details and workspace name<\/p>\n<pre><code>az ml computertarget detach -n youraksname -g yourresourcegroup -w yourworkspacename\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1659333006090,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73187536",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71499094,
        "Question_title":"Python AzureML Hello world - Can't find module azureml",
        "Question_body":"<p>Python 3.10, Pip install azureml-sdk 1.39.0.<br \/>\nEnvironments: Win10 PS, VS2022, and a docker image- all same results . Pip show shows the azureml-core package.<\/p>\n<p>Simple (I thought) script, but it can't find &quot;azureml.core&quot;   No module named azureml is the error.\nHow do I make it &quot;find&quot; it? I'm new at python so it could be syntax.<\/p>\n<pre><code>import os\nfrom azureml.core import Workspace, Experiment, Environment, Model,Dataset,Datastore,ScriptRunConfig\n     \n    # check core SDK version number\n    print(&quot;Azure ML SDK Version: &quot;, azureml.core.VERSION)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1647441787870,
        "Question_score":0,
        "Question_tags":"python|azure-machine-learning-service|azureml-python-sdk|azuremlsdk",
        "Question_view_count":310,
        "Owner_creation_time":1263312456837,
        "Owner_last_access_time":1662141819340,
        "Owner_location":null,
        "Owner_reputation":49,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":null,
        "Answer_body":"<p>azureml python sdk does not support py3.10 yet, AutoML sdk supports py&lt;=3.8.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1648177993987,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71499094",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71747545,
        "Question_title":"Commands in the Azure ML yml files",
        "Question_body":"<p>When reading the examples from Microsoft on azure ML CLI v2, they use the symbols:\n&quot;|&quot;, &quot;&gt;&quot;, etc., in their yml files.<\/p>\n<p>What do they mean, and where can I find explanations of possible syntax for the Azure CLI v2 engine?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649142429250,
        "Question_score":0,
        "Question_tags":"yaml|command-line-interface|azure-machine-learning-studio",
        "Question_view_count":100,
        "Owner_creation_time":1620049475610,
        "Owner_last_access_time":1663926087860,
        "Owner_location":"Denmark",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":"<p>| - This pipe symbol in YAML document is used for <em><strong>&quot;Multiple line statements&quot;<\/strong><\/em><\/p>\n<pre><code>description: |\n  # Azure Machine Learning &quot;hello world&quot; job\n\n  This is a &quot;hello world&quot; job running in the cloud via Azure Machine Learning!\n\n  ## Description\n\n  Markdown is supported in the studio for job descriptions! You can edit the description there or via CLI.\n<\/code><\/pre>\n<p>in the above example, we need to write some multiple line description. So, we need to use &quot;|&quot; symbol<\/p>\n<p>&quot;&gt;&quot; - This symbol is used to save some content directly to a specific location document.<\/p>\n<pre><code>command: echo &quot;hello world&quot; &gt; .\/outputs\/helloworld.txt\n<\/code><\/pre>\n<p>In this above command, we need to post <strong>&quot;hello world&quot;<\/strong> to <em><strong>&quot;helloworld.txt&quot;<\/strong><\/em><\/p>\n<p>Check the below link for complete documentation regarding YAML files.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-yaml-job-command\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-yaml-job-command<\/a><\/p>\n<p>All these symbols are the YAML job commands which are used to accomplish a specific task through CLI.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1649231701843,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71747545",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":63766714,
        "Question_title":"Run.get_context() gives the same run id",
        "Question_body":"<p>I am submitting the training through a script file. Following is the content of the <code>train.py<\/code> script. Azure ML is treating all these as one run (instead of run per alpha value as coded below) as <code>Run.get_context()<\/code> is returning the same Run id.<\/p>\n<p><strong>train.py<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.opendatasets import Diabetes\nfrom azureml.core import Run\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.externals import joblib\n\nimport math\nimport os\nimport logging\n\n# Load dataset\ndataset = Diabetes.get_tabular_dataset()\nprint(dataset.take(1))\n\ndf = dataset.to_pandas_dataframe()\ndf.describe()\n\n# Split X (independent variables) &amp; Y (target variable)\nx_df = df.dropna()      # Remove rows that have missing values\ny_df = x_df.pop(&quot;Y&quot;)    # Y is the label\/target variable\n\nx_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=66)\nprint('Original dataset size:', df.size)\nprint(&quot;Size after dropping 'na':&quot;, x_df.size)\nprint(&quot;Training split size: &quot;, x_train.size)\nprint(&quot;Test split size: &quot;, x_test.size)\n\n# Training\nalphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] # Define hyperparameters\n\n# Create and log interactive runs\n\noutput_dir = os.path.join(os.getcwd(), 'outputs')\n\nfor hyperparam_alpha in alphas:\n    # Get the experiment run context\n    run = Run.get_context()\n    print(&quot;Started run: &quot;, run.id)\n    run.log(&quot;train_split_size&quot;, x_train.size)\n    run.log(&quot;test_split_size&quot;, x_train.size)\n    run.log(&quot;alpha_value&quot;, hyperparam_alpha)\n\n    # Train\n    print(&quot;Train ...&quot;)\n    model = Ridge(hyperparam_alpha)\n    model.fit(X = x_train, y = y_train)\n    \n    # Predict\n    print(&quot;Predict ...&quot;)\n    y_pred = model.predict(X = x_test)\n\n    # Calculate &amp; log error\n    rmse = math.sqrt(mean_squared_error(y_true = y_test, y_pred = y_pred))\n    run.log(&quot;rmse&quot;, rmse)\n    print(&quot;rmse&quot;, rmse)\n\n    # Serialize the model to local directory\n    if not os.path.isdir(output_dir):\n        os.makedirs(output_dir, exist_ok=True) \n\n    print(&quot;Save model ...&quot;)\n    model_name = &quot;model_alpha_&quot; + str(hyperparam_alpha) + &quot;.pkl&quot; # Pickle file\n    file_path = os.path.join(output_dir, model_name)\n    joblib.dump(value = model, filename = file_path)\n\n    # Upload the model\n    run.upload_file(name = model_name, path_or_stream = file_path)\n\n    # Complete the run\n    run.complete()\n<\/code><\/pre>\n<p><strong>Experiments view<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/E6xAG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E6xAG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Authoring code (i.e. control plane)<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nfrom azureml.core import Workspace, Experiment, RunConfiguration, ScriptRunConfig, VERSION, Run\n\nws = Workspace.from_config()\nexp = Experiment(workspace = ws, name = &quot;diabetes-local-script-file&quot;)\n\n# Create new run config obj\nrun_local_config = RunConfiguration()\n\n# This means that when we run locally, all dependencies are already provided.\nrun_local_config.environment.python.user_managed_dependencies = True\n\n# Create new script config\nscript_run_cfg = ScriptRunConfig(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    script = 'train.py',\n    run_config = run_local_config) \n\nrun = exp.submit(script_run_cfg)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1599411710610,
        "Question_score":3,
        "Question_tags":"azure|hyperparameters|azure-machine-learning-service",
        "Question_view_count":2523,
        "Owner_creation_time":1245726715290,
        "Owner_last_access_time":1664077528337,
        "Owner_location":"Cumming, GA",
        "Owner_reputation":77230,
        "Owner_up_votes":2724,
        "Owner_down_votes":43,
        "Owner_views":6359,
        "Question_last_edit_time":1599438192360,
        "Answer_body":"<h2>Short Answer<\/h2>\n<h3>Option 1: create child runs within run<\/h3>\n<p><code>run = Run.get_context()<\/code> assigns the run object of the run that you're currently in to <code>run<\/code>. So in every iteration of the hyperparameter search, you're logging to the same run. To solve this, you need to create child (or sub-) runs for each hyperparameter value. You can do this with <code>run.child_run()<\/code>. Below is the template for making this happen.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>run = Run.get_context()\n\nfor hyperparam_alpha in alphas:\n    # Get the experiment run context\n    run_child = run.child_run()\n    print(&quot;Started run: &quot;, run_child.id)\n    run_child.log(&quot;train_split_size&quot;, x_train.size)\n<\/code><\/pre>\n<p>On the <code>diabetes-local-script-file<\/code> Experiment page, you can see that Run <code>9<\/code> was the parent run and Runs <code>10-19<\/code> were the child runs if you click &quot;Include child runs&quot; page. There is also a &quot;Child runs&quot; tab on Run 9 details page.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/5IMcz.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5IMcz.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Long answer<\/h2>\n<p>I highly recommend abstracting the hyperparameter search away from the data plane (i.e. <code>train.py<\/code>) and into the control plane (i.e. &quot;authoring code&quot;). This becomes especially valuable as training time increases and you can arbitrarily parallelize and also choose Hyperparameters more intelligently by using Azure ML's <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters\" rel=\"noreferrer\"><code>Hyperdrive<\/code><\/a>.<\/p>\n<h3>Option 2 Create runs from control plane<\/h3>\n<p>Remove the loop from your code, add the code like below (<a href=\"https:\/\/gist.github.com\/swanderz\/f5c0dc1aefc796d37f6bc3600f2ae3cd\" rel=\"noreferrer\">full data and control here<\/a>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import argparse\nfrom pprint import pprint\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--alpha', type=float, default=0.5)\nargs = parser.parse_args()\nprint(&quot;all args:&quot;)\npprint(vars(args))\n\n# use the variable like this\nmodel = Ridge(args.alpha)\n<\/code><\/pre>\n<p>below is how to submit a single run using a script argument. To submit multiple runs, just use a loop in the control plane.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>alphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] # Define hyperparameters\n\nlist_rcs = [ScriptRunConfig(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    script = 'train.py',\n    arguments=['--alpha',a],\n    run_config = run_local_config) for a in alphas]\n\nlist_runs = [exp.submit(rc) for rc in list_rcs]\n\n<\/code><\/pre>\n<h3>Option 3 Hyperdrive (IMHO the recommended approach)<\/h3>\n<p>In this way you outsource the hyperparameter source to <code>Hyperdrive<\/code>. The UI will also report results exactly how you want them, and via the API you can easily download the best model.  Note you can't use this locally anymore and must use <code>AMLCompute<\/code>, but to me it is a worthwhile trade-off.<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters#configure-experiment\" rel=\"noreferrer\">This is a great overview<\/a>. Excerpt below (<a href=\"https:\/\/gist.github.com\/swanderz\/f5c0dc1aefc796d37f6bc3600f2ae3cd#file-hyperdrive-ipynb\" rel=\"noreferrer\">full code here<\/a>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>param_sampling = GridParameterSampling( {\n        &quot;alpha&quot;: choice(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)\n    }\n)\n\nestimator = Estimator(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    entry_script = 'train.py',\n    compute_target=cpu_cluster,\n    environment_definition=Environment.get(workspace=ws, name=&quot;AzureML-Tutorial&quot;)\n)\n\nhyperdrive_run_config = HyperDriveConfig(estimator=estimator,\n                          hyperparameter_sampling=param_sampling, \n                          policy=None,\n                          primary_metric_name=&quot;rmse&quot;, \n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                          max_total_runs=10,\n                          max_concurrent_runs=4)\n\nrun = exp.submit(hyperdrive_run_config)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q1AhJ.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q1AhJ.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1599436122230,
        "Answer_score":7,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63766714",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":63458904,
        "Question_title":"Azure-ML Deployment does NOT see AzureML Environment (wrong version number)",
        "Question_body":"<p>I've followed the documentation pretty well as outlined <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-custom-docker-image\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>I've setup my azure machine learning environment the following way:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace\n\n# Connect to the workspace\nws = Workspace.from_config()\n\nfrom azureml.core import Environment\nfrom azureml.core import ContainerRegistry\n\nmyenv = Environment(name = &quot;myenv&quot;)\n\nmyenv.inferencing_stack_version = &quot;latest&quot;  # This will install the inference specific apt packages.\n\n# Docker\nmyenv.docker.enabled = True\nmyenv.docker.base_image_registry.address = &quot;myazureregistry.azurecr.io&quot;\nmyenv.docker.base_image_registry.username = &quot;myusername&quot;\nmyenv.docker.base_image_registry.password = &quot;mypassword&quot;\nmyenv.docker.base_image = &quot;4fb3...&quot; \nmyenv.docker.arguments = None\n\n# Environment variable (I need python to look at folders \nmyenv.environment_variables = {&quot;PYTHONPATH&quot;:&quot;\/root&quot;}\n\n# python\nmyenv.python.user_managed_dependencies = True\nmyenv.python.interpreter_path = &quot;\/opt\/miniconda\/envs\/myenv\/bin\/python&quot; \n\nfrom azureml.core.conda_dependencies import CondaDependencies\nconda_dep = CondaDependencies()\nconda_dep.add_pip_package(&quot;azureml-defaults&quot;)\nmyenv.python.conda_dependencies=conda_dep\n\nmyenv.register(workspace=ws) # works!\n<\/code><\/pre>\n<p>I have a score.py file configured for inference (not relevant to the problem I'm having)...<\/p>\n<p>I then setup inference configuration<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.model import InferenceConfig\ninference_config = InferenceConfig(entry_script=&quot;score.py&quot;, environment=myenv)\n<\/code><\/pre>\n<p>I setup my compute cluster:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.compute import ComputeTarget, AksCompute\nfrom azureml.exceptions import ComputeTargetException\n\n# Choose a name for your cluster\naks_name = &quot;theclustername&quot; \n\n# Check to see if the cluster already exists\ntry:\n    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n    print('Found existing compute target')\nexcept ComputeTargetException:\n    print('Creating a new compute target...')\n    prov_config = AksCompute.provisioning_configuration(vm_size=&quot;Standard_NC6_Promo&quot;)\n\n    aks_target = ComputeTarget.create(workspace=ws, name=aks_name, provisioning_configuration=prov_config)\n\n    aks_target.wait_for_completion(show_output=True)\n\nfrom azureml.core.webservice import AksWebservice\n\n# Example\ngpu_aks_config = AksWebservice.deploy_configuration(autoscale_enabled=False,\n                                                    num_replicas=3,\n                                                    cpu_cores=4,\n                                                    memory_gb=10)\n<\/code><\/pre>\n<p>Everything succeeds; then I try and deploy the model for inference:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.model import Model\n\nmodel = Model(ws, name=&quot;thenameofmymodel&quot;)\n\n# Name of the web service that is deployed\naks_service_name = 'tryingtodeply'\n\n# Deploy the model\naks_service = Model.deploy(ws,\n                           aks_service_name,\n                           models=[model],\n                           inference_config=inference_config,\n                           deployment_config=gpu_aks_config,\n                           deployment_target=aks_target,\n                           overwrite=True)\n\naks_service.wait_for_deployment(show_output=True)\nprint(aks_service.state)\n<\/code><\/pre>\n<p>And it fails saying that it can't find the environment. More specifically, my environment version is <strong>version 11<\/strong>, but it keeps trying to find an environment with a version number that is 1 higher (i.e., <strong>version 12<\/strong>) than the current environment:<\/p>\n<pre><code>FailedERROR - Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: 0f03a025-3407-4dc1-9922-a53cc27267d4\nMore information can be found here: \nError:\n{\n  &quot;code&quot;: &quot;BadRequest&quot;,\n  &quot;statusCode&quot;: 400,\n  &quot;message&quot;: &quot;The request is invalid&quot;,\n  &quot;details&quot;: [\n    {\n      &quot;code&quot;: &quot;EnvironmentDetailsFetchFailedUserError&quot;,\n      &quot;message&quot;: &quot;Failed to fetch details for Environment with Name: myenv Version: 12.&quot;\n    }\n  ]\n}\n\n<\/code><\/pre>\n<p>I have tried to manually edit the environment JSON to match the version that azureml is trying to fetch, but nothing works. Can anyone see anything wrong with this code?<\/p>\n<h1>Update<\/h1>\n<p>Changing the name of the environment (e.g., <code>my_inference_env<\/code>) and passing it to <code>InferenceConfig<\/code> seems to be on the right track. However, the error now changes to the following<\/p>\n<pre><code>Running..........\nFailed\nERROR - Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: f0dfc13b-6fb6-494b-91a7-de42b9384692\nMore information can be found here: https:\/\/some_long_http_address_that_leads_to_nothing\nError:\n{\n  &quot;code&quot;: &quot;DeploymentFailed&quot;,\n  &quot;statusCode&quot;: 404,\n  &quot;message&quot;: &quot;Deployment not found&quot;\n}\n<\/code><\/pre>\n<h1>Solution<\/h1>\n<p>The answer from Anders below is <strong>indeed correct<\/strong> regarding the use of azure ML environments. However, the last error I was getting was because I was setting the <em>container image<\/em> using the digest value (a sha) and NOT the image name and tag (e.g., <code>imagename:tag<\/code>). Note the line of code in the first block:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>myenv.docker.base_image = &quot;4fb3...&quot; \n<\/code><\/pre>\n<p>I reference the digest value, but it should be changed to<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>myenv.docker.base_image = &quot;imagename:tag&quot;\n<\/code><\/pre>\n<p>Once I made that change, the deployment succeeded! :)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1597699827673,
        "Question_score":3,
        "Question_tags":"azure|azure-aks|azure-sdk-python|azure-machine-learning-service",
        "Question_view_count":1768,
        "Owner_creation_time":1432406490590,
        "Owner_last_access_time":1663530034717,
        "Owner_location":"Milwaukee, WI",
        "Owner_reputation":381,
        "Owner_up_votes":48,
        "Owner_down_votes":4,
        "Owner_views":62,
        "Question_last_edit_time":1599771558392,
        "Answer_body":"<p>One concept that took me a while to get was the bifurcation of registering and using an Azure ML <code>Environment<\/code>. If you have already registered your env, <code>myenv<\/code>, and none of the details of the your environment have changed, there is no need re-register it with <code>myenv.register()<\/code>. You can simply get the already register env using <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment.environment?view=azure-ml-py#get-workspace--name--version-none-\" rel=\"nofollow noreferrer\"><code>Environment.get()<\/code><\/a> like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>myenv = Environment.get(ws, name='myenv', version=11)\n<\/code><\/pre>\n<p>My recommendation would be to name your environment something new: like <code>&quot;model_scoring_env&quot;<\/code>. Register it once, then pass it to the <code>InferenceConfig<\/code>.<\/p>",
        "Answer_comment_count":9,
        "Answer_creation_time":1597702121697,
        "Answer_score":1,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63458904",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58906453,
        "Question_title":"H2O Download CSV in Azure Machine Learning",
        "Question_body":"<p>I am trying to build a ML model in Azure Machine Learning using H2o AutoML and could successfully create the model and do prediction.\nWhat I am struggling with is to download the result as csv (ideally to my local PC).<\/p>\n\n<p>The code I used is :<\/p>\n\n<pre><code>#Predict on the whole dataset\npred = best_model.predict(data)\ndata_pred = data.cbind(pred)\n\n# Download as csv\nh2o.download_csv(data_pred,'data_pred .csv')\n<\/code><\/pre>\n\n<p>The above code runs without any error &amp; shows <strong><em>'\/mnt\/azmnt\/code\/Users\/SA\/data_pred.csv'<\/em><\/strong> as the result message. I assume the csv has been created succesfully.<\/p>\n\n<p>But I don't know where to locate it.\nI searched in AzureML datasets but there is none. Appreciate if someone can help me with this. Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1574035772023,
        "Question_score":0,
        "Question_tags":"python|download|h2o|automl|azure-machine-learning-service",
        "Question_view_count":60,
        "Owner_creation_time":1444403737137,
        "Owner_last_access_time":1663734909200,
        "Owner_location":null,
        "Owner_reputation":167,
        "Owner_up_votes":264,
        "Owner_down_votes":0,
        "Owner_views":69,
        "Question_last_edit_time":null,
        "Answer_body":"<p>H2O Documentation says that:<\/p>\n\n<p><code>h2o.h2o.download_csv(data, filename)<\/code><\/p>\n\n<p><code>data : H2OFrame<\/code> An H2OFrame object to be downloaded.<\/p>\n\n<p><code>filename : str<\/code> A string indicating the name that the CSV file should be should be saved to.<\/p>\n\n<p>Additionally, as you have written in your question <code>\/mnt\/azmnt\/code\/Users\/SA\/data_pred.csv'<\/code> should be the path.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1574036423173,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58906453",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":42766263,
        "Question_title":"How to Find Azure ML Experiment based on Deployed Web Service",
        "Question_body":"<p>I have a list of ML experiments which I have created in Azure Machine Learning Studio.  I have deployed them as web services (the new version, not classic).  <\/p>\n\n<p>How can I go into Azure Machine Learning Web Services, click on a web service (which was deployed from an experiment), then navigate back to the experiment \/ predictive model which feeds it?<\/p>\n\n<p>The only link I can find between the two is by updating the web service from the predictive experiment, which then confirms what the web service is. I can see that the \"ExperimentId\" is a GUID in the URL when in the experiment and the web service, so hopefully this is possible.<\/p>\n\n<p>My reasoning is that relying on matching naming conventions, etc., to select the appropriate model to update is subject to human error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1489415750577,
        "Question_score":7,
        "Question_tags":"web-services|azure|azure-machine-learning-studio",
        "Question_view_count":224,
        "Owner_creation_time":1377703476330,
        "Owner_last_access_time":1626302147190,
        "Owner_location":"Bristol, United Kingdom",
        "Owner_reputation":592,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":43,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The <em>new<\/em> web service does not store any information about the experiment or workspace that was deployed (not all <em>new<\/em> web services are deployed from an experiment).<\/p>\n\n<p>Here are the options available to track the relationship between the experiment and a <em>new<\/em> web service.<\/p>\n\n<h2>last deployment<\/h2>\n\n<p>However, the experiment keeps track of the <strong>last<\/strong> <em>new<\/em> web service that was deployed from the experiment. each deployment to a <em>new<\/em> web service overwrites this value.<\/p>\n\n<p>The value is stored in the experiment graph. One way to get the graph is to use the powershell module <a href=\"http:\/\/aka.ms\/amlps\" rel=\"nofollow noreferrer\">amlps<\/a><\/p>\n\n<p><code>Export-AmlExperimentGraph -ExperimentId &lt;Experiment Id&gt; -OutputFile e.json<\/code><\/p>\n\n<p><strong>e.json<\/strong><\/p>\n\n<pre><code>{\n\"ExperimentId\":\"&lt;Experiment Id&gt;\",\n\/\/ . . .\n\"WebService\":{\n\/\/ . . .\n\"ArmWebServiceId\":\"&lt;Arm Id&gt;\"\n},\n\/\/ . . . \n}\n<\/code><\/pre>\n\n<h2>azure resource tags<\/h2>\n\n<p>The tags feature for Azure resources is supported by the <em>new<\/em> web services. Setting a <code>tag<\/code> on the web service programmatically, with powershell or via the azure portal UX can be used to store a reference to the experiment on the <em>new<\/em> web service.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1490283526690,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/42766263",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":42010405,
        "Question_title":"The way to pass input for azure machine experiment from app ( for example console app )",
        "Question_body":"<p>I'm trying to do some kind of web job application that can run for period time and make prediction on azure machine learning studio. After that i want get the result of this experiment and do something with that in my console application. What is the best way to do this in azure with machine learning or maybe some similiar stuff to prediction data from data series ? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1486062508683,
        "Question_score":0,
        "Question_tags":"azure|prediction|azure-machine-learning-studio",
        "Question_view_count":62,
        "Owner_creation_time":1432141466930,
        "Owner_last_access_time":1591285402750,
        "Owner_location":null,
        "Owner_reputation":327,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":78,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can try using Azure Data Factory to create a Machine Learning pipeline or use Azure ML Studio's Predictive Web Services.<\/p>\n\n<ol>\n<li><p>With Azure Data Factory\nFollow <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/data-factory\/data-factory-azure-ml-batch-execution-activity\" rel=\"nofollow noreferrer\">this link<\/a> for details. Azure Data Factory implementations would seem difficult at first but they do work great with Azure ML experiments. <\/p>\n\n<p>Azure Data Factory can run your ML Experiment on a schedule or one-off at a specified time (I guess you can set only for UTC Timezone right now) and monitor it through a dashboard (which is pretty cool).<\/p>\n\n<p>As an example you can look @ <a href=\"https:\/\/github.com\/Microsoft\/azure-docs\/blob\/master\/articles\/data-factory\/data-factory-azure-ml-batch-execution-activity.md\" rel=\"nofollow noreferrer\">ML Batch Execution<\/a>. I used this in one of our implementations (we do have latency issues, but trying to solve that).<\/p><\/li>\n<li><p>If you directly want to use the experiment in your console (assuming it is a web application), use create a Predictive Web service out of your ML Experiment, details <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-walkthrough-5-publish-web-service\" rel=\"nofollow noreferrer\">here<\/a><\/p><\/li>\n<\/ol>\n\n<p>I couldn't exactly understand your use case so I posted two alternatives that should help you. Hope this might lead you to a better solution\/approach.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1486537080117,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1486537417792,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/42010405",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":57660058,
        "Question_title":"Azure ML SDK DataReference - File Pattern - MANY files",
        "Question_body":"<p>I\u2019m building out a pipeline that should execute and train fairly frequently.  I\u2019m following this: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline<\/a> <\/p>\n\n<p>Anyways, I\u2019ve got a stream analytics job dumping telemetry into .json files on blob storage (soon to be adls gen2).  Anyways, I want to find all .json files and use all of those files to train with.  I could possibly use just new .json files as well (interesting option honestly).<\/p>\n\n<p>Currently I just have the store mounted to a data lake and available; and it just iterates the mount for the data files and loads them up.<\/p>\n\n<ol>\n<li>How can I use data references for this instead?<\/li>\n<li>What does data references do for me that mounting time stamped data does not?\na.  From an audit perspective, I have version control, execution time and time stamped read only data.  Albeit, doing a replay on this would require additional coding, but is do-able.<\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1566830274697,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":296,
        "Owner_creation_time":1384802035143,
        "Owner_last_access_time":1663094482000,
        "Owner_location":"Miami Beach, FL",
        "Owner_reputation":2682,
        "Owner_up_votes":75,
        "Owner_down_votes":4,
        "Owner_views":1006,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You could pass pointer to folder as an input parameter for the pipeline, and then your step can mount the folder to iterate over the json files.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1566854588780,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57660058",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67533091,
        "Question_title":"where are registered models in azure machine learning",
        "Question_body":"<p>I try to use azuremlsdk to deploy a locally trained model (a perfectly valid use case AFIK). I follow <a href=\"https:\/\/cran.r-project.org\/web\/packages\/azuremlsdk\/vignettes\/train-and-deploy-first-model.html\" rel=\"nofollow noreferrer\">this<\/a> and managed to create a ML workspace and register a &quot;model&quot; like so:<\/p>\n<pre><code>library(azuremlsdk)\n\ninteractive_auth &lt;- interactive_login_authentication(tenant_id=&quot;xxx&quot;)\nws &lt;- get_workspace(\n        name = &quot;xxx&quot;, \n        subscription_id = &quot;xxx&quot;, \n        resource_group =&quot;xxx&quot;, \n        auth = interactive_auth\n)\n\nadd &lt;- function(a, b) {\n    return(a + b)\n}\n\nadd(1,2)\n\nsaveRDS(add, file = &quot;D:\/add.rds&quot;)\n\nmodel &lt;- register_model(\n    ws, \n    model_path = &quot;D:\/add.rds&quot;, \n    model_name = &quot;add_model&quot;,\n    description = &quot;An amazing model&quot;\n)\n<\/code><\/pre>\n<p>This seemed to work fine, as I get some nice log messages telling me that the model was registered. For my sanity, I wonder where can I find this registered (&quot;materialised&quot;) model\/object\/function in the Azure UI please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620989367110,
        "Question_score":1,
        "Question_tags":"r|azure-machine-learning-service|azuremlsdk",
        "Question_view_count":41,
        "Owner_creation_time":1267440784443,
        "Owner_last_access_time":1664045779313,
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Question_last_edit_time":null,
        "Answer_body":"<p>On ml.azure.com, there is a &quot;Models&quot; option on the left-hand blade.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Y7cZe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Y7cZe.png\" alt=\"UI Sidebar\" \/><\/a><\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1621001633740,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67533091",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71169178,
        "Question_title":"azureml.contrib.dataset vs azureml.data",
        "Question_body":"<p>Looks like AzureML Python SDK has two Dataset packages exposed over API:<\/p>\n<ol>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.contrib.dataset<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-contrib-dataset\/azureml.contrib.dataset.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.data<\/a><\/li>\n<\/ol>\n<p>The documentation doesn't clearly mention the difference or when should we use which one? But, it creates confusion for sure. For example, There are two Tabular Dataset classes exposed over API. And they have different APIs for different functions:<\/p>\n<ol>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.data.TabularDataset<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-contrib-dataset\/azureml.contrib.dataset.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.contrib.dataset.TabularDataset<\/a><\/li>\n<\/ol>\n<p>Any suggestion about when should I use which package will be helpful.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645165311677,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":24,
        "Owner_creation_time":1280505139753,
        "Owner_last_access_time":1663935737867,
        "Owner_location":"Bangalore, India",
        "Owner_reputation":4265,
        "Owner_up_votes":315,
        "Owner_down_votes":11,
        "Owner_views":403,
        "Question_last_edit_time":1645167982083,
        "Answer_body":"<p>As per the <a href=\"https:\/\/pypi.org\/project\/azureml-contrib-dataset\/\" rel=\"nofollow noreferrer\">PyPi<\/a>, <code>azureml.contrib.dataset<\/code> has been deprecated and <code>azureml.data<\/code> should be used instead:<\/p>\n<blockquote>\n<p>The azureml-contrib-dataset package has been deprecated and might not\nreceive future updates and removed from the distribution altogether.\nPlease use azureml-core instead.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0,
        "Answer_creation_time":1645168074897,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71169178",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":59653641,
        "Question_title":"Missing module tensorflow on iPython azure machine learning (Classic)",
        "Question_body":"<p>Yesterday I have install tensorflow module from iPython notebook from Azure machine learning studio (classic) version. The import worked well after installing the module using (!pip install tensorflow). But today when tried to import this module got this \"missing module\" error and when I tried reinstalling the module it works well. Am I missing anything here? \nDo I need to install the module each and everyday, before using it? Can someone please explain?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/rI7hE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rI7hE.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1578516766930,
        "Question_score":0,
        "Question_tags":"python|python-3.x|azure|tensorflow|azure-machine-learning-studio",
        "Question_view_count":140,
        "Owner_creation_time":1500744375327,
        "Owner_last_access_time":1660004233300,
        "Owner_location":null,
        "Owner_reputation":255,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":130,
        "Question_last_edit_time":1578577931443,
        "Answer_body":"<p>For Azure Machine Learning (Classic) Studio notebooks, you need to install Tensorflow. Furthermore, the notebook server session times out after a period of inactivity, hence, you need to re-install Tensorflow once the server shuts down or after starting a new session. Thanks.<\/p>\n\n<p>Here are some references:<\/p>\n\n<p><a href=\"https:\/\/notebooks.azure.com\/help\/jupyter-notebooks\/timeouts\" rel=\"nofollow noreferrer\">https:\/\/notebooks.azure.com\/help\/jupyter-notebooks\/timeouts<\/a><\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/notebooks\/install-packages-jupyter-notebook\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/notebooks\/install-packages-jupyter-notebook<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1578608056697,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59653641",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":62815426,
        "Question_title":"Azure ML: How to train a model on multiple instances",
        "Question_body":"<p>I have a AML compute cluster with the min &amp; max nodes set to 2. When I execute a pipeline, I expect the cluster to run the training on both instances in parallel. But the cluster status reports that only one node is busy and the other is idle.<\/p>\n<p>Here's my code to submit the pipeline, as you can see, I'm resolving the cluster name and passing that to my Step1, thats training a model on Keras.<\/p>\n<pre><code>aml_compute = AmlCompute(ws, &quot;cluster-name&quot;)\nstep1 = PythonScriptStep(name=&quot;train_step&quot;,\n                         script_name=&quot;Train.py&quot;, \n                         arguments=[&quot;--sourceDir&quot;, os.path.realpath(source_directory) ],\n                         compute_target=aml_compute, \n                         source_directory=source_directory,\n                         runconfig=run_config,\n                         allow_reuse=False)\npipeline_run = Experiment(ws, 'MyExperiment').submit(pipeline1, regenerate_outputs=False)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1594299602573,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":623,
        "Owner_creation_time":1330016065410,
        "Owner_last_access_time":1662160983830,
        "Owner_location":null,
        "Owner_reputation":1704,
        "Owner_up_votes":61,
        "Owner_down_votes":7,
        "Owner_views":232,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Each python script step runs on a single node even if you allocate multiple nodes in your cluster. I'm not sure whether training on different instances is possible off-the-shelf in AML, but there's definitely the possibility to use that single node more effectively (looking into using all your cores, etc.)<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1594303708130,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62815426",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":30133814,
        "Question_title":"How to build an image classification dataset in Azure?",
        "Question_body":"<p>I've a set of images that have a single classification of OPEN (they show something that is open).  I couldn't find a way to directly add a status of open to the image reader dataset so I have FULL OUTER JOIN-ed a single ENTER DATA to an IMAGE READER as per the following.  This seems like a hack, does anyone know the \"right\" way to do this?\n<img src=\"https:\/\/i.stack.imgur.com\/Kt1Rv.png\" alt=\"enter image description here\"><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1431124435077,
        "Question_score":16,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio",
        "Question_view_count":1403,
        "Owner_creation_time":1322863592970,
        "Owner_last_access_time":1664017479013,
        "Owner_location":null,
        "Owner_reputation":373,
        "Owner_up_votes":119,
        "Owner_down_votes":4,
        "Owner_views":34,
        "Question_last_edit_time":1446192398649,
        "Answer_body":"<p>Another way is to have R or python code that replicates the status for each image and then use add-columns. I think R\/Python code to just replicate the status for each image may be easier and faster than outer join.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1431886381620,
        "Answer_score":5,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/30133814",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":44881303,
        "Question_title":"Best way to import MongoDB data in Azure Machine Learning",
        "Question_body":"<p>I have a MongoDB database (the Bitnami one) hosted on Azure. I want to import the data there to use it in my Azure Machine Learning experiment.<\/p>\n\n<p>Currently, I am exporting the data to <strong>.csv<\/strong> using <strong>mongoexport<\/strong> and then copy\/pasting it to the <strong>\"Enter Manually Data\"<\/strong> module. This is fine for small amounts of data but I would prefer to have a more robust technique for larger databases.<\/p>\n\n<p>I also thought about using the <strong>\"Import Data\"<\/strong> module from http url along with the <strong>http port (28017) of my mongodb<\/strong> instance but read this was not the recommended use of the http mongodb feature.<\/p>\n\n<p>Finally, I have installed <strong>cosmosDB<\/strong> instead of my bitnami MongoDB and it worked fine but this thing <strong>costs an arm<\/strong> when used with sitecore (it reaches around 100\u20ac per day) and we can't afford it so I switched back to by Mongo.<\/p>\n\n<p>So is there a better way to export data from Mongo to Azure ML ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1499071780183,
        "Question_score":1,
        "Question_tags":"mongodb|azure|azure-machine-learning-studio",
        "Question_view_count":724,
        "Owner_creation_time":1441267698017,
        "Owner_last_access_time":1663761804133,
        "Owner_location":null,
        "Owner_reputation":781,
        "Owner_up_votes":516,
        "Owner_down_votes":0,
        "Owner_views":97,
        "Question_last_edit_time":null,
        "Answer_body":"<p>one way is to use a Python code block in AzureML, something like this:<\/p>\n\n<pre><code>import pandas as p\nimport pymongo as m\n\ndef azureml_main():\n    c = m.MongoClient(host='host_IP')\n    a = p.DataFrame(c.database_names())\n    return a\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1504686538047,
        "Answer_score":1,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44881303",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":63891547,
        "Question_title":"How to connect AMLS to ADLS Gen 2?",
        "Question_body":"<p>I would like to register a dataset from ADLS Gen2 in my Azure Machine Learning workspace (<code>azureml-core==1.12.0<\/code>). Given that service principal information is not required in the Python SDK <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py#register-azure-data-lake-gen2-workspace--datastore-name--filesystem--account-name--tenant-id-none--client-id-none--client-secret-none--resource-url-none--authority-url-none--protocol-none--endpoint-none--overwrite-false-\" rel=\"noreferrer\">documentation<\/a> for <code>.register_azure_data_lake_gen2()<\/code>, I successfully used the following code to register ADLS gen2 as a datastore:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Datastore\n\nadlsgen2_datastore_name = os.environ['adlsgen2_datastore_name']\naccount_name=os.environ['account_name'] # ADLS Gen2 account name\nfile_system=os.environ['filesystem']\n\nadlsgen2_datastore = Datastore.register_azure_data_lake_gen2(\n    workspace=ws,\n    datastore_name=adlsgen2_datastore_name,\n    account_name=account_name, \n    filesystem=file_system\n)\n<\/code><\/pre>\n<p>However, when I try to register a dataset, using<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Dataset\nadls_ds = Datastore.get(ws, datastore_name=adlsgen2_datastore_name)\ndata = Dataset.Tabular.from_delimited_files((adls_ds, 'folder\/data.csv'))\n<\/code><\/pre>\n<p>I get an error<\/p>\n<blockquote>\n<p>Cannot load any data from the specified path. Make sure the path is accessible and contains data.\n<code>ScriptExecutionException<\/code> was caused by <code>StreamAccessException<\/code>.\nStreamAccessException was caused by AuthenticationException.\n<code>'AdlsGen2-ReadHeaders'<\/code> for '[REDACTED]' on storage failed with status code 'Forbidden' (This request is not authorized to perform this operation using this permission.), client request ID &lt;CLIENT_REQUEST_ID&gt;, request ID &lt;REQUEST_ID&gt;. Error message: [REDACTED]\n| session_id=&lt;SESSION_ID&gt;<\/p>\n<\/blockquote>\n<p>Do I need the to enable the service principal to get this to work? Using the ML Studio UI, it appears that the service principal is required even to register the datastore.<\/p>\n<p>Another issue I noticed is that AMLS is trying to access the dataset here:\n<code>https:\/\/adls_gen2_account_name.**dfs**.core.windows.net\/container\/folder\/data.csv<\/code> whereas the actual URI in ADLS Gen2 is: <code>https:\/\/adls_gen2_account_name.**blob**.core.windows.net\/container\/folder\/data.csv<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1600115991930,
        "Question_score":7,
        "Question_tags":"python|azure-machine-learning-service|azure-data-lake-gen2",
        "Question_view_count":3331,
        "Owner_creation_time":1589738451347,
        "Owner_last_access_time":1656358607687,
        "Owner_location":null,
        "Owner_reputation":179,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":53,
        "Question_last_edit_time":1600160631356,
        "Answer_body":"<p>According to this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#azure-data-lake-storage-generation-2\" rel=\"noreferrer\">documentation<\/a>,you need to enable the service principal.<\/p>\n<p>1.you need to register your application and grant the service principal with <strong>Storage Blob Data Reader access<\/strong>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/FZl8O.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FZl8O.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>2.try this code:<\/p>\n<pre><code>adlsgen2_datastore = Datastore.register_azure_data_lake_gen2(workspace=ws,\n                                                             datastore_name=adlsgen2_datastore_name,\n                                                             account_name=account_name,\n                                                             filesystem=file_system,\n                                                             tenant_id=tenant_id,\n                                                             client_id=client_id,\n                                                             client_secret=client_secret\n                                                             )\n\nadls_ds = Datastore.get(ws, datastore_name=adlsgen2_datastore_name)\ndataset = Dataset.Tabular.from_delimited_files((adls_ds,'sample.csv'))\nprint(dataset.to_pandas_dataframe())\n<\/code><\/pre>\n<p><strong>Result:<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/50mit.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/50mit.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1600155716360,
        "Answer_score":9,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1600166834147,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63891547",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":52294404,
        "Question_title":"R Server on Azure",
        "Question_body":"<p>I need to execute R code as webservice, so i tried MLS and it works ok. \nThe problem is that the packages are too old, and i need functions that are not implemented on old packages. \nI asked microsoft support about it, and they have no data up upgrade it, and the new packages require a upgrade of it.<\/p>\n\n<p>How can i do that using other resources, like webapi instead of MLS?\nAll solutions i found requires R installed on machine, wich is a problem for create an azure webapp, function, or api.\nI need an endpoint for forecast on-demand.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1536752847560,
        "Question_score":2,
        "Question_tags":"r|azure-web-app-service|azure-functions|azure-machine-learning-studio",
        "Question_view_count":327,
        "Owner_creation_time":1515518171123,
        "Owner_last_access_time":1657119408507,
        "Owner_location":null,
        "Owner_reputation":1108,
        "Owner_up_votes":33,
        "Owner_down_votes":2,
        "Owner_views":183,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I found one way to execute R on Azure functions.\nthe solutions is copy R-Portable in\n<a href=\"https:\/\/sourceforge.net\/projects\/rportable\/\" rel=\"nofollow noreferrer\">https:\/\/sourceforge.net\/projects\/rportable\/<\/a>\nunzip it using powershell and create a process on function code. In my case i used the code:<\/p>\n\n<pre><code>System.Diagnostics.Process process = new System.Diagnostics.Process();\n            process.StartInfo.WorkingDirectory = @\"D:\\home\\site\\tools\\R-Portable\\App\\R-Portable\\bin\\\";\n            process.StartInfo.FileName = @\"D:\\home\\site\\tools\\R-Portable\\App\\R-Portable\\bin\\Rscript.exe\";\n            process.StartInfo.Arguments = \"-e \\\"print('Hello world')\\\"\";\n            process.StartInfo.UseShellExecute = false;\n            process.StartInfo.RedirectStandardOutput = true;\n            process.StartInfo.RedirectStandardError = true;\n            process.Start();\n            string outputt = process.StandardOutput.ReadToEnd();\n            string err = process.StandardError.ReadToEnd();\n            process.WaitForExit();\n<\/code><\/pre>\n\n<p>On your script you can access csv files or write, and after on function read and return that file.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1537264424307,
        "Answer_score":0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52294404",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72210450,
        "Question_title":"Is there any way to create or delete workspaces in AML studio using powershell?",
        "Question_body":"<p>I am working on a prediction model and am about to use the azure machine learning studio resources. The main operation is to create a workspace on azure ML studio through Powershell. I would like to operate my workspace through the command line. Is there any way to develop and operate the ML Studio workspace through Powershell?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1652332894457,
        "Question_score":0,
        "Question_tags":"azure|powershell|azure-machine-learning-studio",
        "Question_view_count":104,
        "Owner_creation_time":1652331444420,
        "Owner_last_access_time":1652336195337,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":"<p>According to the requirements, there is no procedure developed to create\/delete workspaces through PowerShell in machine learning studio. For reference of creation of workspaces, you can check the below link and the point to be noted is we can create\/delete workspaces using <em><strong>Az<\/strong><\/em><\/p>\n<p>Here is the table link to check PowerShell support table<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/previous-versions\/azure\/machine-learning\/classic\/powershell-module\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/previous-versions\/azure\/machine-learning\/classic\/powershell-module<\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PGfhb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PGfhb.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1652333522490,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72210450",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":37943572,
        "Question_title":"Azure ML Web Service for R models shows unpredictable",
        "Question_body":"<p>When publishing an Azure ML Web Service and preloading data in our R model we see inconsistent performance. First calls are slow but following calls are fast, waiting a bit (couple of minutes) for the next call ends up showing longer response times.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1466509663807,
        "Question_score":0,
        "Question_tags":"azure-web-app-service|azure-machine-learning-studio",
        "Question_view_count":69,
        "Owner_creation_time":1466503688920,
        "Owner_last_access_time":1467292958780,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":64,
        "Question_last_edit_time":1466610116060,
        "Answer_body":"<p>The way Azure ML Web Services work in the background means that instances hosting the models are provisioned and moved in a very dynamic multi-tenant environment. Caching data (warming up) can be helpful but this doesn't mean all subsequent calls will land on the same instance with the same data available in the cache. <\/p>\n\n<p>For models that need a lot of in-memory data there is a limit to what the Azure ML Web Services hosting layer can offer at this point. Microsoft R server could be an alternative to host these big ML workloads and looking at Service Fabric to scale <\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1466599962023,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37943572",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":61279914,
        "Question_title":"AzureML: ResolvePackageNotFound azureml-dataprep",
        "Question_body":"<p>I've got a basic ScriptStep in my AML Pipeline and it's just trying to read an attached dataset. When i execute this simple example, the pipeline fails with the following in the driver log:<\/p>\n\n<blockquote>\n  <p>ImportError: azureml-dataprep is not installed. Dataset cannot be used\n  without azureml-dataprep. Please make sure\n  azureml-dataprep[fuse,pandas] is installed by specifying it in the\n  conda dependencies. pandas is optional and should be only installed if\n  you intend to create a pandas DataFrame from the dataset.<\/p>\n<\/blockquote>\n\n<p>I then modified my step to include the conda package but then the driver fails with \"ResolvePackageNotFound: azureml-dataprep\". The entire log file can be accessed <a href=\"https:\/\/www.dropbox.com\/s\/372ht6jkvzu9loo\/conda.err.txt?dl=0\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<pre><code># create a new runconfig object\nrun_config = RunConfiguration()\nrun_config.environment.docker.enabled = True\nrun_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\nrun_config.environment.python.user_managed_dependencies = False\nrun_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['azureml-dataprep[pandas,fuse]'])\n\nsource_directory = '.\/read-step'\nprint('Source directory for the step is {}.'.format(os.path.realpath(source_directory)))\nstep2 = PythonScriptStep(name=\"read_step\",\n                         script_name=\"Read.py\", \n                         arguments=[\"--dataFilePath\", dataset.as_named_input('local_ds').as_mount() ],\n                         compute_target=aml_compute, \n                         source_directory=source_directory,\n                         runconfig=run_config,\n                         allow_reuse=False)\n<\/code><\/pre>\n\n<p>I'm out of ideas, would deeply appreciate any help here!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1587154334353,
        "Question_score":1,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":1244,
        "Owner_creation_time":1330016065410,
        "Owner_last_access_time":1662160983830,
        "Owner_location":null,
        "Owner_reputation":1704,
        "Owner_up_votes":61,
        "Owner_down_votes":7,
        "Owner_views":232,
        "Question_last_edit_time":1591825691630,
        "Answer_body":"<p>The <code>azureml-sdk<\/code> isn't available on conda, you need to install it with <code>pip<\/code>.<\/p>\n\n<pre><code>myenv = Environment(name=\"myenv\")\nconda_dep = CondaDependencies().add_pip_package(\"azureml-dataprep[pandas,fuse]\")\nmyenv.python.conda_dependencies=conda_dep\nrun_config.environment = myenv\n<\/code><\/pre>\n\n<p>For more information, about this error, the logs tab has a log named <code>20_image_build_log.txt<\/code> which Docker build logs. It contains the error where <code>conda<\/code> failed to failed to find <code>azureml-dataprep<\/code><\/p>\n\n<p>EDIT:<\/p>\n\n<p>Soon, you won't have to specify this dependency anymore. the Azure Data4ML team says <code>azureml-dataprep[pandas,fuse]<\/code> is getting added as a dependency for <code>azureml-defaults<\/code> which is automatically installed on all images. <\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1587162956780,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1587416360076,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61279914",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73318372,
        "Question_title":"several dependency errors causing in Azure AutoML while running model",
        "Question_body":"<p>I am trying to work on different models on a small piece of ML project which needs to work on azure platform and get the score.py with all the values. It is getting not a single library issue, but getting multiple <strong>Module errors<\/strong> and <strong>Attribute errors<\/strong>. I am using latest SDK version only, but I am not sure, where I am going side path.<\/p>\n<p>Any previous observations on this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660210046063,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":29,
        "Owner_creation_time":1651094469217,
        "Owner_last_access_time":1660762207473,
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The compatibility break is there for the newer version of the packages based on the current version of <strong>SDK<\/strong>. If the current SDK version is <strong>1.13.0<\/strong> and above, previous versions of packages are not in working stage. The compatibility issue is raising because of support of packages from SDK for different versions. It differs from version-to-version package support from <strong>SDK<\/strong>.<\/p>\n<p>Because of this we are getting Module not found,  <code>ImportError and AttributeError<\/code>.<\/p>\n<p>This solution depends on the AutoML SDK training version.<\/p>\n<ul>\n<li>If you are using 1.13.0 above version of SDK, update the versions of pandas to 0.25.1 and scikit-learn to 0.22.1<\/li>\n<\/ul>\n<p>Using the following command in  <code>BASH<\/code>  to upgrade the versions.<\/p>\n<pre><code>pip install \u2013upgrade pandas==0.25.1\n\npip install \u2013upgrade sickit-learn==0.22.1\n\n<\/code><\/pre>\n<p>The generic syntax for upgrading is:<\/p>\n<pre><code>pip install \u2013upgrade package_name==version\n\n<\/code><\/pre>\n<ul>\n<li>If the error occurs in AutoML Configuration file, then need to upgrade that also.<\/li>\n<li>But it is suggestable to uninstall and reinstall  <code>AutoMLConfig<\/code>.<\/li>\n<\/ul>\n<pre><code>pip uninstall azureml-train automl\n\n<\/code><\/pre>\n<p>Then reinstall using the below code,<\/p>\n<pre><code>pip install azureml-train automl\n\n<\/code><\/pre>\n<p>If you are using windows operating system, then install  <a href=\"https:\/\/docs.conda.io\/en\/latest\/miniconda.html\" rel=\"nofollow noreferrer\">Miniconda<\/a>.<\/p>\n<p>If you are a linux user, then using sudo or conda syntaxes for the same operation.<\/p>\n<p>Some of the advanced libraries of computer vision supportive like TensorFlow will be installed by default. Then we need to install them from dependencies.<\/p>\n<blockquote>\n<pre><code>azureml.core.runconfig import RunConfiguration from\nazureml.core.conda_dependencies import CondaDependencies run_config =\nRunConfiguration() run_config.environment.python.conda_dependencies =\nCondaDependencies.create(conda_packages=['tensorflow==1.12.0']) \n\n<\/code><\/pre>\n<\/blockquote>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-auto-ml#tensorflow\" rel=\"nofollow noreferrer\">Documentation<\/a>  credit to @Larry Franks.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1660213117330,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73318372",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":57500954,
        "Question_title":"Automatically delete files in storage",
        "Question_body":"<p>So I've noticed that whenever I do a machine learning train\/retrain (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/retrain-machine-learning-model\" rel=\"nofollow noreferrer\">from here<\/a>), it generates a lot of files in my Azure blob storage as shown here<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/QN08i.png\" alt=\"Screenshot\"><\/p>\n\n<p>I wanted to ask if it was possible to automatically delete all these files or prevent them from ever being generated?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1565811221207,
        "Question_score":1,
        "Question_tags":"azure|azure-storage|azure-machine-learning-studio",
        "Question_view_count":2077,
        "Owner_creation_time":1526863814910,
        "Owner_last_access_time":1599796576180,
        "Owner_location":null,
        "Owner_reputation":45,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Question_last_edit_time":1565832393772,
        "Answer_body":"<p>For automatically delete all these files in blob storage, you can use the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-lifecycle-management-concepts#azure-portal-list-view\" rel=\"nofollow noreferrer\">Lifecycle Management<\/a> of blob storage.<\/p>\n<p>It's easy to set up a rule and filter, after the rule is set up, all the files will be deleted as per the rule you defined.<\/p>\n<p>Simple steps:<\/p>\n<p>1.Nav to azure portal -&gt; your storage account -&gt; Blob services -&gt; Lifecycle Management, then click &quot;Add rule&quot;.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/n2Wne.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/n2Wne.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>2.In the &quot;Action set&quot; tab, select Delete blob and fill in the textbox; Then in &quot;Filter set&quot; tab, select a path.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/a2cdQ.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/a2cdQ.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For more details\/instructions, please follow this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-lifecycle-management-concepts#azure-portal-list-view\" rel=\"nofollow noreferrer\">article<\/a>.<\/p>\n<p>Also note that the rule runs once per day, and for the first time, it may take 24 hours to take effect.<\/p>",
        "Answer_comment_count":5,
        "Answer_creation_time":1565832229083,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1592644375060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57500954",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":53908529,
        "Question_title":"How to fix ModuleNotFoundError in azureml-sdk when installed inside conda environment",
        "Question_body":"<p>I'm setting up a conda environment on Windows 10 Pro x64 using Miniconda 4.5.12 and have done a pip install of azureml-sdk inside the environment but get a ModuleNotFoundError when attempting to execute the following code:<\/p>\n\n<pre><code>import azureml.core\nazureml.core.VERSION\n<\/code><\/pre>\n\n<p>This is the output:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"azureml.py\", line 1, in &lt;module&gt;\n    import azureml.core\n  File \"D:\\Projects\\style-transfer\\azureml.py\", line 1, in &lt;module&gt;\n    import azureml.core\nModuleNotFoundError: No module named 'azureml.core'; 'azureml' is not a package\n<\/code><\/pre>\n\n<p>The code above has been run from the conda prompt, with the test environment active as well as in vscode with the same environment selected.<\/p>\n\n<p>I setup the conda environment as per the following:<\/p>\n\n<ol>\n<li>Created the conda environment <code>conda create -n test<\/code>.<\/li>\n<li>Activated the environment <code>activate test<\/code>.<\/li>\n<li>Installed pip <code>conda install pip<\/code>.<\/li>\n<li>Installed azureml-sdk <code>pip install azureml-sdk<\/code>.<\/li>\n<\/ol>\n\n<p>This results in the following packages being installed in the environment as per <code>conda list<\/code>:<\/p>\n\n<pre><code>adal                      1.2.0                     &lt;pip&gt;\nantlr4-python3-runtime    4.7.2                     &lt;pip&gt;\napplicationinsights       0.11.7                    &lt;pip&gt;\nargcomplete               1.9.4                     &lt;pip&gt;\nasn1crypto                0.24.0                    &lt;pip&gt;\nazure-cli-command-modules-nspkg 2.0.2                     &lt;pip&gt;\nazure-cli-core            2.0.54                    &lt;pip&gt;\nazure-cli-nspkg           3.0.3                     &lt;pip&gt;\nazure-cli-profile         2.1.2                     &lt;pip&gt;\nazure-cli-telemetry       1.0.0                     &lt;pip&gt;\nazure-common              1.1.16                    &lt;pip&gt;\nazure-graphrbac           0.53.0                    &lt;pip&gt;\nazure-mgmt-authorization  0.51.1                    &lt;pip&gt;\nazure-mgmt-containerregistry 2.5.0                     &lt;pip&gt;\nazure-mgmt-keyvault       1.1.0                     &lt;pip&gt;\nazure-mgmt-nspkg          3.0.2                     &lt;pip&gt;\nazure-mgmt-resource       2.0.0                     &lt;pip&gt;\nazure-mgmt-storage        3.1.0                     &lt;pip&gt;\nazure-nspkg               3.0.2                     &lt;pip&gt;\nazure-storage-blob        1.4.0                     &lt;pip&gt;\nazure-storage-common      1.4.0                     &lt;pip&gt;\nazure-storage-nspkg       3.1.0                     &lt;pip&gt;\nazureml-core              1.0.6                     &lt;pip&gt;\nazureml-pipeline          1.0.6                     &lt;pip&gt;\nazureml-pipeline-core     1.0.6                     &lt;pip&gt;\nazureml-pipeline-steps    1.0.6                     &lt;pip&gt;\nazureml-sdk               1.0.6                     &lt;pip&gt;\nazureml-telemetry         1.0.6                     &lt;pip&gt;\nazureml-train             1.0.6                     &lt;pip&gt;\nazureml-train-core        1.0.6                     &lt;pip&gt;\nazureml-train-restclients-hyperdrive 1.0.6                     &lt;pip&gt;\nbackports.tempfile        1.0                       &lt;pip&gt;\nbackports.weakref         1.0.post1                 &lt;pip&gt;\nbcrypt                    3.1.5                     &lt;pip&gt;\nca-certificates           2018.03.07                    0\ncertifi                   2018.11.29               py37_0\ncffi                      1.11.5                    &lt;pip&gt;\nchardet                   3.0.4                     &lt;pip&gt;\ncolorama                  0.4.1                     &lt;pip&gt;\ncontextlib2               0.5.5                     &lt;pip&gt;\ncryptography              2.4.2                     &lt;pip&gt;\ndocker                    3.6.0                     &lt;pip&gt;\ndocker-pycreds            0.4.0                     &lt;pip&gt;\nfutures                   3.1.1                     &lt;pip&gt;\nhumanfriendly             4.17                      &lt;pip&gt;\nidna                      2.8                       &lt;pip&gt;\nisodate                   0.6.0                     &lt;pip&gt;\njmespath                  0.9.3                     &lt;pip&gt;\njsonpickle                1.0                       &lt;pip&gt;\nknack                     0.5.1                     &lt;pip&gt;\nmsrest                    0.6.2                     &lt;pip&gt;\nmsrestazure               0.6.0                     &lt;pip&gt;\nndg-httpsclient           0.5.1                     &lt;pip&gt;\noauthlib                  2.1.0                     &lt;pip&gt;\nopenssl                   1.1.1a               he774522_0\nparamiko                  2.4.2                     &lt;pip&gt;\npathspec                  0.5.9                     &lt;pip&gt;\npip                       18.1                     py37_0\nportalocker               1.2.1                     &lt;pip&gt;\npyasn1                    0.4.4                     &lt;pip&gt;\npycparser                 2.19                      &lt;pip&gt;\nPygments                  2.3.1                     &lt;pip&gt;\nPyJWT                     1.7.1                     &lt;pip&gt;\nPyNaCl                    1.3.0                     &lt;pip&gt;\npyOpenSSL                 18.0.0                    &lt;pip&gt;\npypiwin32                 223                       &lt;pip&gt;\npyreadline                2.1                       &lt;pip&gt;\npython                    3.7.1                h8c8aaf0_6\npython-dateutil           2.7.5                     &lt;pip&gt;\npytz                      2018.7                    &lt;pip&gt;\npywin32                   224                       &lt;pip&gt;\nPyYAML                    3.13                      &lt;pip&gt;\nrequests                  2.21.0                    &lt;pip&gt;\nrequests-oauthlib         1.0.0                     &lt;pip&gt;\nruamel.yaml               0.15.51                   &lt;pip&gt;\nSecretStorage             2.3.1                     &lt;pip&gt;\nsetuptools                40.6.3                   py37_0\nsix                       1.12.0                    &lt;pip&gt;\nsqlite                    3.26.0               he774522_0\ntabulate                  0.8.2                     &lt;pip&gt;\nurllib3                   1.23                      &lt;pip&gt;\nvc                        14.1                 h0510ff6_4\nvs2015_runtime            14.15.26706          h3a45250_0\nwebsocket-client          0.54.0                    &lt;pip&gt;\nwheel                     0.32.3                   py37_0\nwheel                     0.30.0                    &lt;pip&gt;\nwincertstore              0.2                      py37_0\n<\/code><\/pre>\n\n<p>If I run <code>which pip<\/code>, I get the following output, which confirms that I used the pip inside the environment to install azureml-sdk, I think:<\/p>\n\n<pre><code>\/c\/Users\/allan\/Miniconda3\/envs\/test\/Scripts\/pip\n<\/code><\/pre>\n\n<p>I can also see that the azureml packages do in fact exist within the environment folder structure.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1545617082117,
        "Question_score":0,
        "Question_tags":"python|conda|azure-machine-learning-studio",
        "Question_view_count":6343,
        "Owner_creation_time":1460456204197,
        "Owner_last_access_time":1610092459520,
        "Owner_location":"Australia",
        "Owner_reputation":140,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It's probably because the name if your python file is the same as a module name you are trying import. In this case, rename the file to something other than <code>azureml.py<\/code>.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1545633498167,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53908529",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":36249716,
        "Question_title":"Automating Azure Machine Learning",
        "Question_body":"<p>Is there a way of automating the calls to the Azure Machine Learning Service (AML)? <\/p>\n\n<p>I\u2019ve created the web service from AML. Now I have to do the calls the automated way. I\u2019m trying to build a system, that connects to a Raspberry Pi for sensor data and gets a prediction from the ML service to be saved with the data itself. <\/p>\n\n<p>Is there something in Azure to automate this or should I do it within the application?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1459095760807,
        "Question_score":0,
        "Question_tags":"azure|cortana-intelligence|azure-machine-learning-studio",
        "Question_view_count":341,
        "Owner_creation_time":1459065054367,
        "Owner_last_access_time":1459181119923,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1459199614030,
        "Answer_body":"<p>I'm assuming you've created the webservice from the experiment and asking about the consumption of the webservice. You can consume the webservice from anything that can do an API call to the endpoint. I don't know the exact architecture of your solution but take a look at this as it might suit your scenario. <\/p>\n\n<p>Stream analytics on Azure has a new feature called Functions(just a heads-up, its still in preview) that can automate the usage of deployed ML services from your account.Since you are trying to gather info from IoT devices, you might use <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/event-hubs-csharp-ephcs-getstarted\/\" rel=\"nofollow\">Event Hubs<\/a> or <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/iot-hub-csharp-csharp-getstarted\/\" rel=\"nofollow\">IoT Hubs<\/a> to get the data and process it using Stream Analytics and during the process you can use the Webservice as Function in SA to achieve on-the-go ML results.<\/p>\n\n<p>Usage is relatively simple if you are familiar with Stream Analytics or SQL queries in general.This <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/stream-analytics-machine-learning-integration-tutorial\/\" rel=\"nofollow\">link<\/a> shows the step by step implementation and the usage is below;<\/p>\n\n<pre><code>    WITH subquery AS (  \n    SELECT text, \"webservicealias\"(text) as result from input  \n    )  \n\n    Select text, result.[Score]  \n    Into output  \n    From subquery  \n<\/code><\/pre>\n\n<p>Hope this helps!<\/p>\n\n<p>Mert<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1459099456053,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36249716",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":50078161,
        "Question_title":"Azure ML Prediction Is Constant",
        "Question_body":"<p>I am using the Azure ML model available at <a href=\"https:\/\/gallery.azure.ai\/Experiment\/Weather-prediction-model-1\" rel=\"nofollow noreferrer\">https:\/\/gallery.azure.ai\/Experiment\/Weather-prediction-model-1<\/a> to design a prediction mechanism based on temperature and humidity. I haven't done any changes to the existing model and feeding in data from a simulator. The prediction output is stuck at 0.489944100379944. I have taken over 17k samples and still, the prediction is constant at this value. <\/p>\n\n<p>Any help will be highly appreciated.<\/p>\n\n<p><em>N.B. - This is my first stint with ML<\/em><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":10,
        "Question_creation_time":1524929526273,
        "Question_score":0,
        "Question_tags":"azure|machine-learning|data-science|azure-machine-learning-studio|machine-learning-model",
        "Question_view_count":163,
        "Owner_creation_time":1338385871600,
        "Owner_last_access_time":1664032112660,
        "Owner_location":null,
        "Owner_reputation":118,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":52,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This was caused by the training dataset. The dataset had characters in the humidity and temperature columns. This led to the model expecting characters but operating on floating point numbers. I cleaned the dataset and ensured that there are only floats in the temperature and humidity columns. Then I used this training data for the model and phew!!!! Everything's working now. <\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1525107860377,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50078161",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60525454,
        "Question_title":"Intel optimized Python on Machine Learning Service Compute",
        "Question_body":"<p>Is it possible to run a Python script or Estimator step on the Azure Machine Learning Service in a container with the intel optimized Python distribution?\nI understand this is available on the <a href=\"https:\/\/azure.microsoft.com\/en-in\/blog\/intel-and-microsoft-bring-optimizations-to-deep-learning-on-azure\/\" rel=\"nofollow noreferrer\">Azure Data Science VMs<\/a> (<a href=\"https:\/\/www.intel.ai\/intel-optimized-data-science-virtual-machine-azure\/#gs.yuntlp\" rel=\"nofollow noreferrer\">or described here<\/a>), but I could not find out how to use this as an Azure Machine Learning Service Compute target.<\/p>\n\n<p>For my current use case I am specifically interested in using an mkl linked numpy package in the aml service container.<\/p>\n\n<p>Note: Running numpy.show_config() inside the container suggests numpy is linked against openblas and not mkl<\/p>\n\n<pre><code>blas_mkl_info:\n  NOT AVAILABLE\nblis_info:\n  NOT AVAILABLE\nopenblas_info:\n    libraries = ['openblas', 'openblas']\n    library_dirs = ['\/usr\/local\/lib']\n    language = c\n    define_macros = [('HAVE_CBLAS', None)]\nblas_opt_info:\n    libraries = ['openblas', 'openblas']\n    library_dirs = ['\/usr\/local\/lib']\n    language = c\n    define_macros = [('HAVE_CBLAS', None)]\nlapack_mkl_info:\n  NOT AVAILABLE\nopenblas_lapack_info:\n    libraries = ['openblas', 'openblas']\n    library_dirs = ['\/usr\/local\/lib']\n    language = c\n    define_macros = [('HAVE_CBLAS', None)]\nlapack_opt_info:\n    libraries = ['openblas', 'openblas']\n    library_dirs = ['\/usr\/local\/lib']\n    language = c\n    define_macros = [('HAVE_CBLAS', None)]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1583322849310,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":81,
        "Owner_creation_time":1582187824717,
        "Owner_last_access_time":1618135237967,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1583354777670,
        "Answer_body":"<p>The Azure ML base images use <a href=\"https:\/\/docs.conda.io\/en\/latest\/miniconda.html\" rel=\"nofollow noreferrer\">Miniconda<\/a> Python distribution, which uses MKL.<\/p>\n\n<p>You can find the details of the base images here:<a href=\"https:\/\/github.com\/Azure\/AzureML-Containers\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/AzureML-Containers<\/a><\/p>\n\n<p>Also, if you install using Anaconda numpy in following way<\/p>\n\n<pre><code>conda_dep.add_conda_package(\"numpy\")\nrunconfig.run_config.environment.python.conda_dependencies = conda_dep\n<\/code><\/pre>\n\n<p>you should see this kind of output from <code>numpy.show_config()<\/code>. <\/p>\n\n<blockquote>\n  <p>blas_mkl_info:<\/p>\n  \n  <p>libraries = ['blas', 'cblas', 'lapack', 'pthread', 'blas', 'cblas',\n  'lapack']<\/p>\n  \n  <p>library_dirs =\n  ['\/azureml-envs\/azureml_a8ad8e485613e21e6e8adc1bfda86b40\/lib']<\/p>\n  \n  <p>define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]<\/p>\n  \n  <p>include_dirs =\n  ['\/azureml-envs\/azureml_a8ad8e485613e21e6e8adc1bfda86b40\/include']<\/p>\n<\/blockquote>",
        "Answer_comment_count":5,
        "Answer_creation_time":1583345664733,
        "Answer_score":-1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1583437574647,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60525454",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":63296185,
        "Question_title":"How to write Azure machine learning batch scoring results to data lake?",
        "Question_body":"<p>I'm trying to write the output of batch scoring into datalake:<\/p>\n<pre><code>    parallel_step_name = &quot;batchscoring-&quot; + datetime.now().strftime(&quot;%Y%m%d%H%M&quot;)\n    \n    output_dir = PipelineData(name=&quot;scores&quot;, \n                              datastore=def_ADL_store,\n                              output_mode=&quot;upload&quot;,\n                              output_path_on_compute=&quot;path in data lake&quot;)\n\nparallel_run_config = ParallelRunConfig(\n    environment=curated_environment,\n    entry_script=&quot;use_model.py&quot;,\n    source_directory=&quot;.\/&quot;,\n    output_action=&quot;append_row&quot;,\n    mini_batch_size=&quot;20&quot;,\n    error_threshold=1,\n    compute_target=compute_target,\n    process_count_per_node=2,\n    node_count=2\n)\n    \n    batch_score_step = ParallelRunStep(\n        name=parallel_step_name,\n        inputs=[test_data.as_named_input(&quot;test_data&quot;)],\n        output=output_dir,\n        parallel_run_config=parallel_run_config,\n        allow_reuse=False\n    )\n<\/code><\/pre>\n<p>However I meet the error: &quot;code&quot;: &quot;UserError&quot;,\n&quot;message&quot;: &quot;User program failed with Exception: Missing argument --output or its value is empty.&quot;<\/p>\n<p>How can I write results of batch score to data lake?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596780750297,
        "Question_score":2,
        "Question_tags":"azure-data-lake|azure-machine-learning-service",
        "Question_view_count":277,
        "Owner_creation_time":1513841518107,
        "Owner_last_access_time":1663924369323,
        "Owner_location":"China",
        "Owner_reputation":71,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I don\u2019t think ADLS is supported for <code>PipelineData<\/code>. My suggestion is to use the workspace\u2019s default blob store for the <code>PipelineData<\/code>, then use a <code>DataTransferStep<\/code> for after the <code>ParallelRunStep<\/code> is completed.<\/p>",
        "Answer_comment_count":6,
        "Answer_creation_time":1596781453977,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63296185",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72798225,
        "Question_title":"Remote Connection fails in setup of Python data-science client for SQL Server Machine Learning Services",
        "Question_body":"<p>I am trying to test the remote connection of a Python data-science client with SQL Server Machine Learning Services following this guide: <a href=\"https:\/\/docs.microsoft.com\/en-us\/sql\/machine-learning\/python\/setup-python-client-tools-sql\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/sql\/machine-learning\/python\/setup-python-client-tools-sql<\/a> (section 6).\nRunning the following script<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def send_this_func_to_sql():\n    from revoscalepy import RxSqlServerData, rx_import\n    from pandas.tools.plotting import scatter_matrix\n    import matplotlib.pyplot as plt\n    import io\n    \n    # remember the scope of the variables in this func are within our SQL Server Python Runtime\n    connection_string = &quot;Driver=SQL Server;Server=localhost\\instance02;Database=testmlsiris;Trusted_Connection=Yes;&quot;\n    \n    # specify a query and load into pandas dataframe df\n    sql_query = RxSqlServerData(connection_string=connection_string, sql_query = &quot;select * from iris_data&quot;)\n    df = rx_import(sql_query)\n    \n    scatter_matrix(df)\n    \n    # return bytestream of image created by scatter_matrix\n    buf = io.BytesIO()\n    plt.savefig(buf, format=&quot;png&quot;)\n    buf.seek(0)\n    \n    return buf.getvalue()\n\nnew_db_name = &quot;testmlsiris&quot;\nconnection_string = &quot;driver={sql server};server=sqlrzs\\instance02;database=%s;trusted_connection=yes;&quot; \n\nfrom revoscalepy import RxInSqlServer, rx_exec\n\n# create a remote compute context with connection to SQL Server\nsql_compute_context = RxInSqlServer(connection_string=connection_string%new_db_name)\n\n# use rx_exec to send the function execution to SQL Server\nimage = rx_exec(send_this_func_to_sql, compute_context=sql_compute_context)[0]\n<\/code><\/pre>\n<p>yields the following error message returned by rx_exec (stored in the <em>image<\/em> variable)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>connection_string: &quot;driver={sql server};server=sqlrzs\\instance02;database=testmlsiris;trusted_connection=yes;&quot;\nnum_tasks: 1\nexecution_timeout_seconds: 0\nwait: True\nconsole_output: False\nauto_cleanup: True\npackages_to_load: []\ndescription: &quot;sqlserver&quot;\nversion: &quot;1.0&quot;\nXXX lineno: 2, opcode: 0\nTraceback (most recent call last):\n  File &quot;&lt;string&gt;&quot;, line 3, in &lt;module&gt;\n  File &quot;E:\\SQL\\MSSQL15.INSTANCE02\\PYTHON_SERVICES\\lib\\site-packages\\revoscalepy\\computecontext\\RxInSqlServer.py&quot;, line 664, in rx_sql_satellite_pool_call\n    exec(inputfile.read())\n  File &quot;&lt;string&gt;&quot;, line 34, in &lt;module&gt;\n  File &quot;E:\\SQL\\MSSQL15.INSTANCE02\\PYTHON_SERVICES\\lib\\site-packages\\revoscalepy\\computecontext\\RxInSqlServer.py&quot;, line 886, in rx_remote_call\n    results = rx_resumeexecution(state_file = inputfile, patched_server_name=args[&quot;hostname&quot;])\n  File &quot;E:\\SQL\\MSSQL15.INSTANCE02\\PYTHON_SERVICES\\lib\\site-packages\\revoscalepy\\computecontext\\RxInSqlServer.py&quot;, line 135, in rx_resumeexecution\n    return _state[&quot;function&quot;](**_state[&quot;args&quot;])\n  File &quot;C:\\Users\\username\\sendtosql.py&quot;, line 2, in send_this_func_to_sql\nSystemError: unknown opcode\n====== sqlrzs ( process 0 ) has started run at 2022-06-29 13:47:04 W. Europe Daylight Time ======\n{'local_state': {}, 'args': {}, 'function': &lt;function send_this_func_to_sql at 0x0000020F5810F1E0&gt;}\n<\/code><\/pre>\n<p>What is going wrong here? Line 2 in the script is just an import (which works when testing Python scripts on SQL Server directly). Any help is appreciated - thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1656491540923,
        "Question_score":0,
        "Question_tags":"python|sql-server|azure-machine-learning-studio|microsoft-machine-learning-server",
        "Question_view_count":54,
        "Owner_creation_time":1656427742040,
        "Owner_last_access_time":1664010979637,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1656503806376,
        "Answer_body":"<p>I just figured out the reason. As of today, the Python versions for the data clients in <a href=\"https:\/\/docs.microsoft.com\/de-de\/sql\/machine-learning\/python\/setup-python-client-tools-sql?view=sql-server-ver15\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/de-de\/sql\/machine-learning\/python\/setup-python-client-tools-sql?view=sql-server-ver15<\/a> are not the newest (revoscalepy Version 9.3), while the version of Machine Learning Services that we have running in our SQL Server is already 9.4.7.\nHowever, the revoscalepy libraries for the client and server must be the same, otherwise the deserialization fails server-sided.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1656513437860,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72798225",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72399408,
        "Question_title":"How to get list of compute instance size under Azure Machine Learning and Azure Databricks?",
        "Question_body":"<p>Goal here is to query a list of frequently used compute instance size under Azure Machine Learning and Azure Databricks using Azure Resource Graph Explorer from Azure Portal using Kusto query. From the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/governance\/resource-graph\/reference\/supported-tables-resources#resources\" rel=\"nofollow noreferrer\">documentation<\/a> here, there is a list of resources can be queried but there isn't any compute under <code>microsoft.machinelearningservices\/<\/code>(not classic studio) and <code>Microsoft.Databricks\/workspaces<\/code>.<\/p>\n<p>Below is what was tried, to get VM instance size but not showing what we have under Azure Machine Learning\/Azure Databricks.<\/p>\n<pre><code>Resources\n| project name, location, type, vmSize=tostring(properties.hardwareProfile.vmSize)\n| where type =~ 'Microsoft.Compute\/virtualMachines'\n| order by name desc\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653612949060,
        "Question_score":0,
        "Question_tags":"azure|size|azure-databricks|azure-machine-learning-service|azure-resource-graph",
        "Question_view_count":153,
        "Owner_creation_time":1568185673007,
        "Owner_last_access_time":1663257666153,
        "Owner_location":"Kuala Lumpur, Federal Territory of Kuala Lumpur, Malaysia",
        "Owner_reputation":383,
        "Owner_up_votes":70,
        "Owner_down_votes":1,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":"<blockquote>\n<p>Unfortunately, Azure Resource Graph Explorer doesn't provide any query\nto get any compute related information from both, Azure Machine\nLearning and Databricks.<\/p>\n<\/blockquote>\n<p>Though Azure Resource Graph Explorer supports join functionality, allowing for more advanced exploration of your Azure environment by enabling you to correlate between resources and their properties. But these services only applicable on few Azure resources like VM, storage account, Cosmos DB, SQL databases, Network Security Groups, public IP addresses, etc.<\/p>\n<p><strong>Hence, there is no such Kusto query available in Azure Resource Graph Explorer which can list compute instance size of Machine Learning service and Databricks.<\/strong><\/p>\n<p><strong>Workarounds<\/strong><\/p>\n<p>Machine Learning Service<\/p>\n<p>For machine learning service you can manage the compute instance directly from ML service by using Python SDK. Refer <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=python#manage\" rel=\"nofollow noreferrer\">Python SDK azureml v1<\/a> to know more.<\/p>\n<p>Azure Databricks<\/p>\n<p>Cluster is the computational resource in Databricks. You can <strong>filter the cluster list<\/strong> from Databricks UI and manage the same. Features like cluster configuration, cluster cloning, access control, etc. are available which you can used based on your requirement. For more details, please check <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/clusters\/clusters-manage#filter-cluster-list\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1653626541543,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72399408",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":48267427,
        "Question_title":"Trouble in creating graphics with matplotlib in a Jupyter notebook",
        "Question_body":"<p>Following the pandas documentation for visualization (<a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/visualization.html#visualization-hist\" rel=\"nofollow noreferrer\">https:\/\/pandas.pydata.org\/pandas-docs\/stable\/visualization.html#visualization-hist<\/a>) I am trying to create the following graphics:<\/p>\n\n<pre><code>import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n## A data set in my AzureML workplace experiment \ndf = ds.to_dataframe()\nplt.figure(); \ndf.plot.hist(stacked=True, bins=20) \nplt.figure();df.boxplot()\n<\/code><\/pre>\n\n<p>However, the output is limited to <code>\"&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd12e15dc18&gt;\"<\/code> (for the histogram(=) and <code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd12e0ce828&gt;\"<\/code> (to the box plot), but no image appearing. Can anyone help me to identify what I'm missing out? Thanks!<\/p>\n\n<p>I'm using Python 3 in Jupyter Notebook in AzureML. <\/p>\n\n<p>The <code>df.describe()<\/code> method works properly (there is a dataFrame)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1516035090403,
        "Question_score":0,
        "Question_tags":"python|pandas|matplotlib|azure-machine-learning-studio",
        "Question_view_count":378,
        "Owner_creation_time":1316565559337,
        "Owner_last_access_time":1655585549090,
        "Owner_location":"Brazil",
        "Owner_reputation":2563,
        "Owner_up_votes":368,
        "Owner_down_votes":2,
        "Owner_views":503,
        "Question_last_edit_time":1516035342392,
        "Answer_body":"<p>Have you set the backend?<\/p>\n\n<pre><code>%matplotlib inline\n<\/code><\/pre>\n\n<p>Worth reading about what this does for a notebook here too\n<a href=\"https:\/\/stackoverflow.com\/questions\/43027980\/purpose-of-matplotlib-inline\/43028034\">Purpose of &quot;%matplotlib inline&quot;<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1516035353070,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48267427",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":46340959,
        "Question_title":"Multiple Inputs\/Outputs from Execute R Script",
        "Question_body":"<p>Assume I have an Execute R Script that calculates multiple variables, say X and Y.\nIs it possible to save X as a dataset ds_X and Y as a dataset ds_Y?<\/p>\n\n<p>The problem is that there is only 1 output port available that needs to be mapped to a data.frame. Am I missing an option to add more output ports?\nSame problem for input ports. I may connect 2 of the \"Enter Data Manually\" modules to it, but what if I need 3? The current workaround is to put CSV files in a ZIP file and connect that. Are there easier solution?<\/p>\n\n<p><strong>Example of what i tried:<\/strong><\/p>\n\n<p>I tried adding ds_X and ds_Y to a list. The idea is to pass this list to multiple \"Execute R Script\" modules and use the required list elements there.\nMapping a list to an output port does not seem to work though:<\/p>\n\n<pre><code># Calculate lots of stuff - results are ds_X and ds_Y\nds_X &lt;- mtcars\nds_Y &lt;- cars\nout &lt;- list(ds_X, ds_Y)\n\nmaml.mapOutputPort(\"out\")\n<\/code><\/pre>\n\n<p>results in an error:<\/p>\n\n<pre><code>Error: Mapped variable must be of class type data.frame at this time.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1505987948480,
        "Question_score":0,
        "Question_tags":"r|azure-machine-learning-studio",
        "Question_view_count":754,
        "Owner_creation_time":1389795136837,
        "Owner_last_access_time":1663761689240,
        "Owner_location":null,
        "Owner_reputation":625,
        "Owner_up_votes":88,
        "Owner_down_votes":3,
        "Owner_views":36,
        "Question_last_edit_time":1505991363609,
        "Answer_body":"<p>You can author custom R Modules. <\/p>\n\n<p>Here is some documentation: \n<a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/04\/23\/build-your-own-r-modules-in-azure-ml\/\" rel=\"nofollow noreferrer\">https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/04\/23\/build-your-own-r-modules-in-azure-ml\/<\/a>\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-custom-r-modules\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-custom-r-modules<\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1506004266757,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46340959",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":56418684,
        "Question_title":"Possible to access the internal representation of a neural network trained in Azure Machine Learning Service or Azure Machine Learning Studio?",
        "Question_body":"<p>I'm working with data scientists who would like to gain insight and understanding of the neural network models that they train using the visual interfaces in Azure Machine Learning Studio\/Service. Is it possible to dump out and inspect the internal representation of a neural network model? Is there a way that I could write code that accesses the nodes and weights of a trained neural network in order to visualize the network as a graph structure? Or if Azure Machine Learning Studio\/Service doesn't support this I'd appreciate advice on a different machine learning framework that might be more appropriate for this kind of analysis.<\/p>\n\n<p>Things I have tried:<\/p>\n\n<ul>\n<li>Train Model outputs an ILearnerDotNet (AML Studio) or Model (AML Service). I looked for items to drag into the workspace where I could write custom code such as Execute Python Script. They seem to accept datasets, but not ILearnerDotNet\/Model as input.<\/li>\n<li>I wasn't able to locate documentation about the ILearnerDotNet\/Model interfaces.<\/li>\n<li>Selecting the Train Model output offers the option to Save as Trained Model. This creates a trained model object and that would help me reference the trained model in other places, but I didn't find a way to use this to get at its internals.<\/li>\n<\/ul>\n\n<p>I'm new to the Azure Machine Learning landscape, and could use some help with how to get started on how to access this data.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559506858340,
        "Question_score":2,
        "Question_tags":"neural-network|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":167,
        "Owner_creation_time":1427643193810,
        "Owner_last_access_time":1663710082517,
        "Owner_location":null,
        "Owner_reputation":45,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Quote from Azure ML Exam reference:<\/p>\n\n<blockquote>\n  <p>By default, the architecture of neural networks is limited to a single\n  hidden layer with sigmoid as the activation function and softmax in\n  the last layer. You can change this in the properties of the model,\n  opening the Hidden layer specification dropdown list, and selecting a\n  Custom definition script. A text box will appear in which you will be\n  able to insert a Net# script. This script language allows you to\n  define neural networks architectures.<\/p>\n<\/blockquote>\n\n<p>For instance, if you want to create a two layer network, you may put the following code.<\/p>\n\n<pre><code>input Picture [28, 28];\nhidden H1 [200] from Picture all;\nhidden H2 [200] from H1 all;\noutput Result [10] softmax from H2 all;\n<\/code><\/pre>\n\n<p>Nevertheless, with Net# you will face certain limitations as, it does not accept regularization (neither L2 nor dropout). Also, there is no ReLU activation that are\ncommonly used in deep learning due to their benefits in backpropagation. You cannot modify the batch size of the Stochastic Gradient Descent (SGD). Besides that, you cannot use other optimization algorithms. You can use SGD with momentum, but not others like Adam, or RMSprop. You cannot define recurrent or recursive neural networks.<\/p>\n\n<p>Another great tool is CNTK (Cognitive Toolkit) that allows you defining your computational graph and create a fully customizable model.\nQuote from documentation<\/p>\n\n<blockquote>\n  <p>It is a Microsoft open source deep learning toolkit. Like other deep\n  learning tools, CNTK is based on the construction of computational\n  graphs and their optimization using automatic differentiation. The\n  toolkit is highly optimized and scales efficiently (from CPU, to GPU,\n  to multiple machines). CNTK is also very portable and flexible; you\n  can use it with programming languages like Python, C#, or C++, but you\n  can also use a model description language called BrainScript.<\/p>\n<\/blockquote>",
        "Answer_comment_count":2,
        "Answer_creation_time":1559896269913,
        "Answer_score":0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56418684",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":32422626,
        "Question_title":"Part of speech tagging and entity recognition - python",
        "Question_body":"<p>I want to perform part of speech tagging and entity recognition in python similar to Maxent_POS_Tag_Annotator and Maxent_Entity_Annotator functions of openNLP in R.  I would prefer a code in python which takes input as textual sentence and gives output as different features- like number of \"CC\", number of \"CD\", number of \"DT\" etc.. CC, CD, DT are POS tags as used in Penn Treebank. So there should be 36 columns\/features for POS tagging corresponding to 36 POS tags as in <a href=\"http:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html\" rel=\"nofollow\">Penn Treebank POS<\/a>. I want to implement this on Azure ML \"Execute Python Script\" module and Azure ML supports python 2.7.7. I heard nltk in python may does the job, but I am a beginner on python. Any help would be appreciated. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1441535794287,
        "Question_score":0,
        "Question_tags":"python|azure|named-entity-recognition|part-of-speech|azure-machine-learning-studio",
        "Question_view_count":1014,
        "Owner_creation_time":1431324152360,
        "Owner_last_access_time":1444093237570,
        "Owner_location":null,
        "Owner_reputation":19,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":1441776651843,
        "Answer_body":"<p>Take a look at <a href=\"http:\/\/www.nltk.org\/book\/ch05.html\" rel=\"nofollow\">NTLK book<\/a>, Categorizing and Tagging Words section.<\/p>\n\n<p>Simple example, it uses the Penn Treebank tagset:<\/p>\n\n<pre><code>from nltk.tag import pos_tag\nfrom nltk.tokenize import word_tokenize\npos_tag(word_tokenize(\"John's big idea isn't all that bad.\")) \n\n[('John', 'NNP'),\n(\"'s\", 'POS'),\n ('big', 'JJ'),\n ('idea', 'NN'),\n ('is', 'VBZ'),\n (\"n't\", 'RB'),\n ('all', 'DT'),\n ('that', 'DT'),\n ('bad', 'JJ'),\n ('.', '.')]\n<\/code><\/pre>\n\n<p>Then you can use<\/p>\n\n<pre><code>from collections import defaultdict\ncounts = defaultdict(int)\nfor (word, tag) in pos_tag(word_tokenize(\"John's big idea isn't all that bad.\")):\n    counts[tag] += 1\n<\/code><\/pre>\n\n<p>to get frequencies:<\/p>\n\n<pre><code>defaultdict(&lt;type 'int'&gt;, {'JJ': 2, 'NN': 1, 'POS': 1, '.': 1, 'RB': 1, 'VBZ': 1, 'DT': 2, 'NNP': 1})\n<\/code><\/pre>",
        "Answer_comment_count":7,
        "Answer_creation_time":1441539548050,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/32422626",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58377444,
        "Question_title":"swagger.json example json for forecast model doesn't seem to return predictions",
        "Question_body":"<p>When trying to make predictions for forecasting models using Azure ML Service, the swagger.json includes the following schema for input:<\/p>\n\n<pre><code>\"example\": {\"data\": [{\"date\": \"2019-08-30T00:00:00.000Z\", \"y_query\": 1.0}]}\n<\/code><\/pre>\n\n<p>However, when I feed this as an input to generate predictions, I receive the following error:<\/p>\n\n<pre><code>data= {\"data\": [{\"date\": \"2019-08-30T00:00:00.000Z\", \"y_query\": 1 }]}\n# Convert to JSON string\ninput_data = json.dumps(data)\n\n# Set the content type\nheaders = {'Content-Type': 'application\/json'}\n# If authentication is enabled, set the authorization header\n#headers['Authorization'] = f'Bearer {key}'\n\n# Make the request and display the response\nresp = requests.post(scoring_uri, input_data, headers=headers)\nprint(resp.text)\n\n<\/code><\/pre>\n\n<pre><code>\"{\\\"error\\\": \\\"DataException:\\\\n\\\\tMessage: y values are present for each date. Nothing to forecast.\\\\n\\\\tInnerException None\\\\n\\\\tErrorResponse \\\\n{\\\\n    \\\\\\\"error\\\\\\\": {\\\\n        \\\\\\\"code\\\\\\\": \\\\\\\"UserError\\\\\\\",\\\\n        \\\\\\\"inner_error\\\\\\\": {\\\\n            \\\\\\\"code\\\\\\\": \\\\\\\"InvalidData\\\\\\\"\\\\n        },\\\\n        \\\\\\\"message\\\\\\\": \\\\\\\"y values are present for each date. Nothing to forecast.\\\\\\\"\\\\n    }\\\\n}\\\"}\"\n<\/code><\/pre>\n\n<p>I have tried not passing a y value, which causes an 'expected two axis got one' and passing 0 as the y_query. Any guidance on how to make predictions using this approach would be greatly appreciated. <\/p>\n\n<p>The documentation for web services is here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":6,
        "Question_creation_time":1571058071523,
        "Question_score":1,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":530,
        "Owner_creation_time":1464949169833,
        "Owner_last_access_time":1657738559383,
        "Owner_location":null,
        "Owner_reputation":25,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Try using nan as the value for y_query. and make sure the date is the next time unit after the one that was used in the training set.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1571417536483,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58377444",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":61168984,
        "Question_title":"Azure ML free trial: how to submit pipeline?",
        "Question_body":"<p>I'm using a free trial account on MS Azure and I'm following this tutorial.<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score<\/a><\/p>\n\n<p>I'm stuck when I try to \"submit the pipeline\".<\/p>\n\n<p>The reason seems to be that I can't create a compute instance or a training cluster on a free plan.\nI still have 200USDs of free credits. I guess there must be a solution?<\/p>\n\n<hr>\n\n<p>Error messages:<\/p>\n\n<pre><code>Invalid graph: The pipeline compute target is invalid.\n\n400: Compute Test3 in state Failed, which is not able to use\n\nCompute instance: creation failed\nThe specified subscription has a total vCPU quota of 0 and is less than the requested compute training cluster and\/or compute instance's min nodes of 1 which maps to 4 vCPUs\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586681686103,
        "Question_score":2,
        "Question_tags":"azure|azure-machine-learning-service",
        "Question_view_count":370,
        "Owner_creation_time":1343682543650,
        "Owner_last_access_time":1655966822477,
        "Owner_location":null,
        "Owner_reputation":135,
        "Owner_up_votes":51,
        "Owner_down_votes":0,
        "Owner_views":45,
        "Question_last_edit_time":1586690281396,
        "Answer_body":"<p>Please check the announcement from MS Team regarding this:<\/p>\n\n<p><a href=\"https:\/\/azure.microsoft.com\/blog\/our-commitment-to-customers-and-microsoft-cloud-services-continuity\/\" rel=\"nofollow noreferrer\">https:\/\/azure.microsoft.com\/blog\/our-commitment-to-customers-and-microsoft-cloud-services-continuity\/<\/a><\/p>\n\n<p>All the free trials will not work as of now<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1586681759587,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61168984",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":37375506,
        "Question_title":"AzureML: \"Train Matchbox Recommender\" is not working and does not descibe the error",
        "Question_body":"<p>I tried to create my own experiment using the module, but failed to make it work.\nhere is the exception i got:  <\/p>\n\n<blockquote>\n  <p>Error 0018: Training dataset of user-item-rating triples contains invalid data.\n  [Critical]     {\"InputParameters\":{\"DataTable\":[{\"Rows\":14,\"Columns\":3,\"estimatedSize\":12668928,\"ColumnTypes\":{\"System.String\":1,\"System.Int32\":1,\"System.Double\":1},\"IsComplete\":true,\"Statistics\":{\"0\":[10,0],\"1\":[5422.0,5999.0,873.0,6616.0,1758.0582820478173,7.0,0.0],\"2\":[1.0,1.0,1.0,1.0,0.0,1.0,0.0]}},{\"Rows\":2338,\"Columns\":3,\"estimatedSize\":1404928,\"ColumnTypes\":{\"System.String\":1,\"System.Int32\":1,\"System.Double\":1},\"IsComplete\":true,\"Statistics\":{\"0\":[2338,0],\"1\":[7.5367835757057318,3.0,0.0,704.0,17.738259318519511,64.0,0.0],\"2\":[3.3737234816082085,1.5,0.0,352.0,8.3956874404883841,122.0,0.0]}},{\"Rows\":2532,\"Columns\":22,\"estimatedSize\":4648960,\"ColumnTypes\":{\"System.Int32\":10,\"System.String\":5,\"System.Double\":6,\"System.Boolean\":1},\"IsComplete\":true,\"Statistics\":{\"0\":[4575.7263033175359,5326.5,539.0,6871.0,1987.9561375024909,2532.0,0.0],\"1\":[4575.7263033175359,5326.5,539.0,6871.0,1987.9561375024909,2532.0,0.0],\"2\":[613.0,613.0,613.0,613.0,0.0,1.0,0.0],\"3\":[0,2532],\"4\":[0,2532],\"5\":[4575.7263033175359,5326.5,539.0,6871.0,1987.9561375024909,2532.0,0.0],\"6\":[23.647231437598673,19.99,1.99,149.99,17.237723488320938,90.0,0.0],\"7\":[0.043827014218009476,0.0,0.0,45.99,1.3460680431173562,3.0,0.0],\"8\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"9\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"10\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"11\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"12\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"13\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"14\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"15\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"16\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"17\":[0.0,0.0,0.0,0.0,0.0,1.0,0.0],\"18\":[2524,0],\"19\":[242,18],\"20\":[1,0],\"21\":[2524,0]}}],\"Generic\":{\"traitCount\":10,\"iterationCount\":5,\"batchCount\":4}},\"OutputParameters\":[],\"ModuleType\":\"Microsoft.Analytics.Modules.MatchboxRecommender.Dll\",\"ModuleVersion\":\" Version=6.0.0.0\",\"AdditionalModuleInfo\":\"Microsoft.Analytics.Modules.MatchboxRecommender.Dll, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca;Microsoft.Analytics.Modules.MatchboxRecommender.Dll.MatchboxRecommender;Train\",\"Errors\":\"Microsoft.Analytics.Exceptions.ErrorMapping+ModuleException: Error 0018: Training dataset of user-item-rating triples contains invalid data.\\r\\n   at Microsoft.Analytics.Modules.MatchboxRecommender.Dll.Utilities.UpdateRatingMetadata(DataTable dataset, String datasetName) in d:\\_Bld\\8833\\7669\\Sources\\Product\\Source\\Modules\\MatchboxRecommender.Dll\\Utilities.cs:line 179\\r\\n   at Microsoft.Analytics.Modules.MatchboxRecommender.Dll.MatchboxRecommender.TrainImpl(DataTable userItemRatingTriples, DataTable userFeatures, DataTable itemFeatures, Int32 traitCount, Int32 iterationCount, Int32 batchCount) in d:\\_Bld\\8833\\7669\\Sources\\Product\\Source\\Modules\\MatchboxRecommender.Dll\\MatchboxRecommender.cs:line 62\",\"Warnings\":[],\"Duration\":\"00:00:00.6722068\"}\n  Module finished after a runtime of 00:00:01.1250071 with exit code -2\n  Module failed due to negative exit code of -2<\/p>\n<\/blockquote>\n\n<p>i've check the input data i'm setting as input user-place-rating table, record by record (no worries it's only 14 records) here it is: <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/LjyD6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LjyD6.png\" alt=\"the input data\"><\/a><\/p>\n\n<p>Here is a screenshot of the experiment:\n<a href=\"https:\/\/i.stack.imgur.com\/I43tG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/I43tG.png\" alt=\"the experiment\"><\/a><\/p>\n\n<p>since the error message is not very informative, I don't know where to start, so, if anybody has an idea, I would be happy to hear about it.<\/p>\n\n<p>Update:\nA friend of mine suggested to add \"Edit Metadata\" module to change the \"rating\" feature into \"int\" or \"float\" types, and the two other(placeID and userID) into string features. that didn't help as well.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1463926726100,
        "Question_score":3,
        "Question_tags":"azure|machine-learning|cortana-intelligence|azure-machine-learning-studio",
        "Question_view_count":1187,
        "Owner_creation_time":1320061998253,
        "Owner_last_access_time":1656424560827,
        "Owner_location":null,
        "Owner_reputation":778,
        "Owner_up_votes":176,
        "Owner_down_votes":6,
        "Owner_views":89,
        "Question_last_edit_time":1463935698456,
        "Answer_body":"<p>The matchbox recommender requires that ratings be numerical or categorical. Also when training, your ratings cannot all be the same.<\/p>\n\n<p>You need to use a metadata editor <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn905986.aspx\" rel=\"nofollow\">https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn905986.aspx<\/a> to convert the ratings into numerical features and you need to make sure you are using a range of ratings.<\/p>\n\n<p>Then this should work!<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1464083199417,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37375506",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64054587,
        "Question_title":"How can I remove the wrapper around the input when using Inference Schema",
        "Question_body":"<p>When using Inference Schema to autogenerate the swagger doc for my AzureML endpoint (as detailed <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">here<\/a> and <a href=\"https:\/\/github.com\/Azure\/InferenceSchema\" rel=\"nofollow noreferrer\">here<\/a>), I see that it creates a wrapper around my input_sample. Is there a way to\nnot wrap the input inside this &quot;data&quot; wrapper?<\/p>\n<p>Here is what my score.py looks like:<\/p>\n<pre><code>input_sample = {\n                &quot;id&quot;: 123,\n                &quot;language&quot;: &quot;en&quot;\n                &quot;items&quot;: [{\n                    &quot;item&quot;: 1,\n                    &quot;desc&quot;: &quot;desc&quot;\n                }]\n            }\noutput_sample = [{'prediction': 'true', 'predictionConfidence': 0.8279970776764844}]\n\n@input_schema('data', StandardPythonParameterType(input_sample))\n@output_schema(StandardPythonParameterType(output_sample))\ndef run(data):\n&quot;&quot;&quot;\n    {\n        data: { --&gt; DON'T WANT this &quot;data&quot; wrapper\n                &quot;id&quot;: 123,\n                &quot;language&quot;: &quot;en&quot;\n                &quot;items&quot;: [{\n                    &quot;item&quot;: 1,\n                    &quot;desc&quot;: &quot;desc&quot;\n                }]\n            }\n    }\n    &quot;&quot;&quot;\n    try:\n        id = data['id']\n        ...\n        \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1600982458753,
        "Question_score":2,
        "Question_tags":"python|inference|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":173,
        "Owner_creation_time":1406298639460,
        "Owner_last_access_time":1663860224643,
        "Owner_location":null,
        "Owner_reputation":435,
        "Owner_up_votes":36,
        "Owner_down_votes":1,
        "Owner_views":48,
        "Question_last_edit_time":1601039285472,
        "Answer_body":"<p>InferenceSchema used with Azure Machine Learning deployments, then the code for this package was recently published at <a href=\"https:\/\/github.com\/Azure\/InferenceSchema\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/InferenceSchema<\/a> under an MIT license. So you could possibly use that to create a version specific to your needs.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1601883755637,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1601885390512,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64054587",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":39251701,
        "Question_title":"AzureML: experiment working for a subset and not for the whole dataset",
        "Question_body":"<p>some times ago I had written a code in AzureML meeting \"out of memory\" issues. So I tried to split the code in three different codes and that partially worked. It remains a part that (I think) is affected by memory issues too.<\/p>\n\n<p>I have created an experiment that I have published in this <a href=\"http:\/\/gallery.cortanaintelligence.com\/Experiment\/TextMining-sample-NA-v1-1\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n\n<p>There is a module that considers only a sample of my dataset, and it does work. This means that the code is supposed to work correctly. If you remove the sampling code (the second module starting from the top) <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Cbzhj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Cbzhj.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>and you connect directly the original dataset you have the following situation<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/XOo8e.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XOo8e.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>producing the following error:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/mRSSQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mRSSQ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Does someone have some way to understand where Azure crashes?<\/p>\n\n<p>Thanks you,<\/p>\n\n<p>Andrea<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1472651915230,
        "Question_score":1,
        "Question_tags":"r|azure-machine-learning-studio",
        "Question_view_count":102,
        "Owner_creation_time":1436432728610,
        "Owner_last_access_time":1663607665487,
        "Owner_location":"Colleferro, Italy",
        "Owner_reputation":809,
        "Owner_up_votes":109,
        "Owner_down_votes":0,
        "Owner_views":361,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Thanks so much for publishing the example -- this really helped to understand the issue. I suspect that you want to modify the <code>gsub()<\/code> calls in your script by adding the argument \"<code>fixed=TRUE<\/code>\" to each. (The documentation for this function is <a href=\"https:\/\/stat.ethz.ch\/R-manual\/R-devel\/library\/base\/html\/grep.html\" rel=\"nofollow\">here<\/a>.)<\/p>\n\n<p>What appears to have happened is that somewhere in your full dataset -- but not in the subsampled dataset -- there is some text that winds up being included in <code>df[i, \"names\"]<\/code> as \"<code>(art.<\/code>\".  Your script pads this into \"<code>\\\\b(art.\\\\b<\/code>\". The <code>gsub()<\/code> function tries to interpret this as a regular expression instead of a simple string, then throws an error because it is not a valid regular expression: it contains an opening parenthesis but no closing parenthesis. I believe that you actually did not want <code>gsub()<\/code> to interpret the input as a regular expression in the first place, and specifying <code>gsub(..., fixed=TRUE)<\/code> will correct that.<\/p>\n\n<p>I believe the reason why this error disappears when you add the sample\/partition module is because, by chance, the problematic input value was dropped on subsampling. I do not think it is an issue of available resources on Azure ML. (Caveat: I cannot confirm the fix works yet; I made the suggested update and started running the experiment, but it has not yet completed successfully.)<\/p>",
        "Answer_comment_count":6,
        "Answer_creation_time":1472676812497,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/39251701",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":30092360,
        "Question_title":"How do i schedule Azure Machine learning web service in Azure Scheduler?",
        "Question_body":"<p>I have published the web service from Azure Machine Learning experiment and now i want this web service to be scheduled using Azure Scheduler<\/p>\n\n<p>Can somebody please state the procedure?<\/p>\n\n<p>I got the API KEY, REQUEST\/RESPONSE and Batch Execution URI from the web service homepage.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1430975121057,
        "Question_score":2,
        "Question_tags":"azure|machine-learning|azure-scheduler|azure-machine-learning-studio",
        "Question_view_count":1301,
        "Owner_creation_time":1403541426413,
        "Owner_last_access_time":1592469887883,
        "Owner_location":"Bengaluru, India",
        "Owner_reputation":191,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Question_last_edit_time":1431617131740,
        "Answer_body":"<p>You will need to first create a new job in the Azure management portal (<a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn495651.aspx\" rel=\"nofollow\">https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn495651.aspx<\/a>), where you can configure the URL and the HTTP method to POST, and specify the body. However, the initial configuration screens don't let you add any headers, so once you have created the job, go in and edit it to add the following headers:<\/p>\n\n<p>Content-Type: application\/json<br>\nAccept: application\/json<br>\nAuthorization: Bearer <\/p>\n\n<p>This will work, but am wondering if this actually serves your purpose. If you're calling the synchronous (request response) endpoint of the AzureML service, you need to specify the inputs in the request payload, which is statically configured with the Azure Scheduler job. So you will effectively be repeating the same call over and over again. You may also want to explore <a href=\"http:\/\/azure.microsoft.com\/en-us\/services\/data-factory\/\" rel=\"nofollow\">Azure Data Factory<\/a> if your needs are served by calling the asynchronous (batch) endpoint of the AzureML service.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1431016140103,
        "Answer_score":2,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/30092360",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":35970126,
        "Question_title":"Increase the size of \/dev\/shm in Azure ML Studio",
        "Question_body":"<p>I'm trying to execute the following code in Azure ML Studio notebook:<\/p>\n\n<pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.cross_validation import KFold, cross_val_score\n\nfor C in np.linspace(0.01, 0.2, 30):\n    cv = KFold(n=X_train.shape[0], n_folds=7, shuffle=True, random_state=12345)\n    clf = LogisticRegression(C=C, random_state=12345)\n    print C, sum(cross_val_score(clf, X_train_scaled, y_train, scoring='roc_auc', cv=cv, n_jobs=2)) \/ 7.0\n<\/code><\/pre>\n\n<p>and I'm getting this error:<\/p>\n\n<pre><code>Failed to save &lt;type 'numpy.ndarray'&gt; to .npy file:\nTraceback (most recent call last):\n  File \"\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/sklearn\/externals\/joblib\/numpy_pickle.py\", line 271, in save\n    obj, filename = self._write_array(obj, filename)\n  File \"\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/sklearn\/externals\/joblib\/numpy_pickle.py\", line 231, in _write_array\n    self.np.save(filename, array)\n  File \"\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/numpy\/lib\/npyio.py\", line 491, in save\n    pickle_kwargs=pickle_kwargs)\n  File \"\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/numpy\/lib\/format.py\", line 585, in write_array\n    array.tofile(fp)\nIOError: 19834920 requested and 8384502 written\n\n---------------------------------------------------------------------------\nIOError                                   Traceback (most recent call last)\n&lt;ipython-input-29-9740e9942629&gt; in &lt;module&gt;()\n      6     cv = KFold(n=X_train.shape[0], n_folds=7, shuffle=True, random_state=12345)\n      7     clf = LogisticRegression(C=C, random_state=12345)\n----&gt; 8     print C, sum(cross_val_score(clf, X_train_scaled, y_train, scoring='roc_auc', cv=cv, n_jobs=2)) \/ 7.0\n\n\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/sklearn\/cross_validation.pyc in cross_val_score(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\n   1431                                               train, test, verbose, None,\n   1432                                               fit_params)\n-&gt; 1433                       for train, test in cv)\n   1434     return np.array(scores)[:, 0]\n   1435 \n\n\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/sklearn\/externals\/joblib\/parallel.pyc in __call__(self, iterable)\n    808                 # consumption.\n    809                 self._iterating = False\n--&gt; 810             self.retrieve()\n    811             # Make sure that we get a last message telling us we are done\n    812             elapsed_time = time.time() - self._start_time\n\n\/home\/nbcommon\/env\/lib\/python2.7\/site-packages\/sklearn\/externals\/joblib\/parallel.pyc in retrieve(self)\n    725                 job = self._jobs.pop(0)\n    726             try:\n--&gt; 727                 self._output.extend(job.get())\n    728             except tuple(self.exceptions) as exception:\n    729                 # Stop dispatching any new job in the async callback thread\n\n\/home\/nbcommon\/env\/lib\/python2.7\/multiprocessing\/pool.pyc in get(self, timeout)\n    565             return self._value\n    566         else:\n--&gt; 567             raise self._value\n    568 \n    569     def _set(self, i, obj):\n\nIOError: [Errno 28] No space left on device\n<\/code><\/pre>\n\n<p>With <code>n_jobs=1<\/code> it works fine.<\/p>\n\n<p>I think this is because <code>joblib<\/code> library tries to save my data to <code>\/dev\/shm<\/code>. The problem is that it has only 64M capacity:<\/p>\n\n<pre><code>Filesystem         Size  Used Avail Use% Mounted on\nnone               786G  111G  636G  15% \/\ntmpfs               56G     0   56G   0% \/dev\nshm                 64M     0   64M   0% \/dev\/shm\ntmpfs               56G     0   56G   0% \/sys\/fs\/cgroup\n\/dev\/mapper\/crypt  786G  111G  636G  15% \/etc\/hosts\n<\/code><\/pre>\n\n<p>I can't change this folder by setting <code>JOBLIB_TEMP_FOLDER<\/code> environment variable (<code>export<\/code> doesn't work).<\/p>\n\n<pre><code>In [35]: X_train_scaled.nbytes\n\nOut[35]: 158679360\n<\/code><\/pre>\n\n<p>Thanks for any advice!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1457871506333,
        "Question_score":2,
        "Question_tags":"python|azure|scikit-learn|joblib|azure-machine-learning-studio",
        "Question_view_count":630,
        "Owner_creation_time":1452022246890,
        "Owner_last_access_time":1657271456120,
        "Owner_location":"Moscow, Russia",
        "Owner_reputation":708,
        "Owner_up_votes":34,
        "Owner_down_votes":69,
        "Owner_views":167,
        "Question_last_edit_time":1457939841649,
        "Answer_body":"<p>The <code>\/dev\/shm<\/code> is a virtual filesystem for passing data between programs that implementation of traditional shared memory on Linux.<\/p>\n\n<p>So you could not increase it via set up some options on Application Layout.<\/p>\n\n<p>But for example, you can remount <code>\/dev\/shm<\/code> with 8G size in Linux Shell with administrator permission like <code>root<\/code> as follows.<\/p>\n\n<p><code>mount -o remount,size=8G \/dev\/shm<\/code><\/p>\n\n<p>However, it seems that Azure ML studio not support remote access via SSH protocol, so the feasible plan is upgrade the standard tier if using free tier at present.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1457947589353,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35970126",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71014584,
        "Question_title":"Azure ML Tabular Dataset : missing 1 required positional argument: 'stream_column'",
        "Question_body":"<p>For the Python API for tabular dataset of AzureML (<code>azureml.data.TabularDataset<\/code>), there are two experimental methods which have been introduced:<\/p>\n<ol>\n<li><code>download(stream_column, target_path=None, overwrite=False, ignore_not_found=True)<\/code><\/li>\n<li><code>mount(stream_column, mount_point=None)<\/code><\/li>\n<\/ol>\n<p>Parameter <code>stream_column<\/code> has been defined as The stream column to mount or download.<\/p>\n<p>What is the actual meaning of <code>stream_column<\/code>? I don't see any example any where?<\/p>\n<p>Any pointer will be helpful.<\/p>\n<p>The stack trace:<\/p>\n<pre><code>Method download: This is an experimental method, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n\/tmp\/ipykernel_11561\/3904436543.py in &lt;module&gt;\n----&gt; 1 tab_dataset.download(target_path=&quot;..\/data\/tabular&quot;)\n\n\/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/_base_sdk_common\/_docstring_wrapper.py in wrapped(*args, **kwargs)\n     50     def wrapped(*args, **kwargs):\n     51         module_logger.warning(&quot;Method {0}: {1} {2}&quot;.format(func.__name__, _method_msg, _experimental_link_msg))\n---&gt; 52         return func(*args, **kwargs)\n     53     return wrapped\n     54 \n\n\/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/data\/_loggerfactory.py in wrapper(*args, **kwargs)\n    130             with _LoggerFactory.track_activity(logger, func.__name__, activity_type, custom_dimensions) as al:\n    131                 try:\n--&gt; 132                     return func(*args, **kwargs)\n    133                 except Exception as e:\n    134                     if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):\n\nTypeError: download() missing 1 required positional argument: 'stream_column'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1644217302490,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-service|azureml-python-sdk|azuremlsdk",
        "Question_view_count":356,
        "Owner_creation_time":1280505139753,
        "Owner_last_access_time":1663935737867,
        "Owner_location":"Bangalore, India",
        "Owner_reputation":4265,
        "Owner_up_votes":315,
        "Owner_down_votes":11,
        "Owner_views":403,
        "Question_last_edit_time":1645197572643,
        "Answer_body":"<p><strong>Update on 5th March, 2022<\/strong><\/p>\n<p>I posted this as a support ticket with Azure. Following is the answer I have received:<\/p>\n<blockquote>\n<p>As you can see from our documentation of <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">TabularDataset Class<\/a>,\nthe \u201cstream_column\u201d parameter is required. So, that error is occurring\nbecause you are not passing any parameters when you are calling the\ndownload method.    The \u201cstream_column\u201d parameter should have the\nstream column to download\/mount. So, you need to pass the column name\nthat contains the paths from which the data will be streamed.<br \/>\nPlease find an example <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-labeled-dataset#explore-labeled-datasets-via-pandas-dataframe\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0,
        "Answer_creation_time":1646484376340,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1648643627872,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71014584",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58301879,
        "Question_title":"Unable to upload statsmodels 0.9rc1 python package in Azure ML studio",
        "Question_body":"<p>I'm not able to upload statsmodels 0.9rc1 python package in Azure ML studio for Time series analysis.<\/p>\n\n<p>I have downloaded <a href=\"https:\/\/files.pythonhosted.org\/packages\/df\/6f\/df6cf5faecd8082ee23916ff45d396dfee5a1f17aa275da7bab4f5c8926a\/statsmodels-0.9.0rc1-cp36-cp36m-win_amd64.whl\" rel=\"nofollow noreferrer\">statsmodels 0.9rc1<\/a>, unzipped contents and added statsmodels folder and model.pkl file to zip folder.<\/p>\n\n<p>But, while uploading to Microsoft Azure ML studio it says <strong>failed to build schema and visualization<\/strong><\/p>\n\n<p>I'm using this external package in <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/execute-python-scripts\" rel=\"nofollow noreferrer\">Execute Python script<\/a><\/p>\n\n<p>PS: I have succesfully uploaded packages like Adal, dateutils etc.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1570616046857,
        "Question_score":2,
        "Question_tags":"python|azure|machine-learning|statsmodels|azure-machine-learning-studio",
        "Question_view_count":141,
        "Owner_creation_time":1548341556520,
        "Owner_last_access_time":1663921446773,
        "Owner_location":"Mumbai, Maharashtra, India",
        "Owner_reputation":2907,
        "Owner_up_votes":354,
        "Owner_down_votes":2,
        "Owner_views":238,
        "Question_last_edit_time":1570619708227,
        "Answer_body":"<p>I have switched to Azure Jupyter Notebook where I installed package using pip<\/p>\n\n<pre><code>!pip install statsmodels==0.9.0rc1\n<\/code><\/pre>",
        "Answer_comment_count":2,
        "Answer_creation_time":1573144655863,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58301879",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":49818134,
        "Question_title":"how connect to Azure Machine Learning Web service using PowerShell?",
        "Question_body":"<p>To use Azure Machine Learning Web service <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/consume-web-services\" rel=\"nofollow noreferrer\">here<\/a> you can find some sample code in C#, R, Python and JavaScript. I want to use it in PowerShell.\nI found <a href=\"https:\/\/www.sepago.com\/blog\/2015\/11\/30\/zugriff-mit-powershell-auf-azure-machine-learning-api-azureml\" rel=\"nofollow noreferrer\">this<\/a> tutorial, but when I am running bellow line of code, it will return error that it is not recognized:<\/p>\n\n<pre><code>Set-AzureMLWebServiceConnection -URI $Url -APIKey $API_key\n\nOutput:\nSet-AzureMLWebServiceConnection : The term 'Set-AzureMLWebServiceConnection' is not recognized as the name of a cmdlet, function, script file, or operable \nprogram. Check the spelling of the name, or if a path was included, verify that the path is correct and try again.\nAt C:\\Users\\Reza\\Desktop\\ndbench\\Azure\\Automation\\01_get_metrics\\add_target_to_tables - runbook_01.ps1:33 char:1\n+ Set-AzureMLWebServiceConnection -URI $Url -APIKey $API_key\n+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    + CategoryInfo          : ObjectNotFound: (Set-AzureMLWebServiceConnection:String) [], CommandNotFoundException\n    + FullyQualifiedErrorId : CommandNotFoundException\n<\/code><\/pre>\n\n<p>I can't found <code>Set-AzureMLWebServiceConnection<\/code> in my PowerShell command-list and I don't know how I can enable\/install it.\n<a href=\"https:\/\/i.stack.imgur.com\/YMso7.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YMso7.jpg\" alt=\"enter image description here\"><\/a>\nCan you please guide me, how I can connect to Azure Machine Learning Web service using PowerShell?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1523625908693,
        "Question_score":0,
        "Question_tags":"powershell|azure|azure-powershell|azure-machine-learning-studio",
        "Question_view_count":303,
        "Owner_creation_time":1433870950220,
        "Owner_last_access_time":1663930681917,
        "Owner_location":"Tehran, Iran",
        "Owner_reputation":1316,
        "Owner_up_votes":180,
        "Owner_down_votes":11,
        "Owner_views":201,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The comment @gvee mentioned may be the best to use going forward though it is in beta.<\/p>\n\n<p>However, to answer your question, use the <code>Install-Module -Name AzureML<\/code> <a href=\"https:\/\/www.powershellgallery.com\/packages\/AzureML\/1.0.1\" rel=\"nofollow noreferrer\">command<\/a> to get access to the Azure ML commands.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZerMp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZerMp.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1523724686213,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49818134",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71783228,
        "Question_title":"How do I specify nodeSelector while deploying an Azure ML model to an AKS Cluster?",
        "Question_body":"<p>I am currently deploying a model trained using AzureML to an AKS cluster as follows:<\/p>\n<pre><code>deployment_config_aks = AksWebservice.deploy_configuration(\n    cpu_cores = 1, \n    memory_gb = 1)\n\nservice = Model.deploy(ws, &quot;test&quot;, [model], inference_config, deployment_config_aks, aks_target)\n\n<\/code><\/pre>\n<p>I would like this service to be scheduled on a specific nodepool. With normal Kubernetes deployment, I can specify a <code>nodeSelector<\/code> like:<\/p>\n<pre><code>spec:\n  nodeSelector:\n    myNodeName: alpha\n<\/code><\/pre>\n<p>How do I specify a <code>nodeSelector<\/code> while deploying an Azure ML model to an AKS Cluster? Or in general, is there a way to merge my custom pod spec with the one generated by Azure ML library?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649338837060,
        "Question_score":0,
        "Question_tags":"azure|azure-aks|azure-machine-learning-service",
        "Question_view_count":289,
        "Owner_creation_time":1338974093053,
        "Owner_last_access_time":1663927880003,
        "Owner_location":"Beijing, China",
        "Owner_reputation":1830,
        "Owner_up_votes":413,
        "Owner_down_votes":3,
        "Owner_views":155,
        "Question_last_edit_time":null,
        "Answer_body":"<blockquote>\n<p>How do I specify a nodeSelector while deploying an Azure ML model to an AKS Cluster? Or in general, is there a way to merge my custom pod spec with the one generated by Azure ML library?<\/p>\n<\/blockquote>\n<p>As per <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-attach-arc-kubernetes?tabs=studio\" rel=\"nofollow noreferrer\">Configure Kubernetes clusters for machine learning<\/a>:<\/p>\n<p><code>nodeSelector<\/code> : Set the node selector so the extension components and the training\/inference workloads will only be deployed to the nodes with all specified selectors.<\/p>\n<p>For example:<\/p>\n<p><code>nodeSelector.key=value<\/code> , <code>nodeSelector.node-purpose=worker<\/code> and <code>nodeSelector.node-region=eastus<\/code><\/p>\n<p>You can refer to <a href=\"https:\/\/kubernetes.io\/docs\/concepts\/scheduling-eviction\/assign-pod-node\/#built-in-node-labels\" rel=\"nofollow noreferrer\">Assigning Pods to Nodes<\/a> and <a href=\"https:\/\/github.com\/Azure\/AKS\/issues\/2866\" rel=\"nofollow noreferrer\">Cannot create nodepool with node-restriction.kubernetes.io\/ prefix label<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1650522075520,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71783228",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":39941622,
        "Question_title":"Parallel *apply in Azure Machine Learning Studio",
        "Question_body":"<p>I have just started to get myself acquainted with parallelism in R. <\/p>\n\n<p>As I am planning to use <a href=\"https:\/\/studio.azureml.net\/\" rel=\"nofollow\">Microsoft Azure Machine Learning Studio<\/a> for my project, I have started investigating what <a href=\"https:\/\/mran.revolutionanalytics.com\/documents\/rro\/multithread\/\" rel=\"nofollow\">Microsoft R Open<\/a> offers for parallelism, and thus, I found <a href=\"https:\/\/mran.revolutionanalytics.com\/documents\/rro\/multithread\/\" rel=\"nofollow\">this<\/a>, in which it says that parallelism is done under the hood that leverages the benefit of all available cores, without changing the R code. The article also shows some performance benchmarks, however, most of them demonstrate the performance benefit in doing mathematical operations.<\/p>\n\n<p>This was good so far. In addition, I am also interested to know whether it also parallelize the <code>*apply<\/code> functions under the hood or not. I also found these 2 articles that describes how to parallelize <code>*apply<\/code> functions in general:<\/p>\n\n<ol>\n<li><a href=\"https:\/\/www.r-bloggers.com\/quick-guide-to-parallel-r-with-snow\/\" rel=\"nofollow\">Quick guide to parallel R with snow<\/a>: describes facilitating parallelism using <a href=\"https:\/\/cran.r-project.org\/web\/packages\/snow\/snow.pdf\" rel=\"nofollow\"><code>snow<\/code><\/a> package, <code>par*apply<\/code> function family, and <code>clusterExport<\/code>.<\/li>\n<li><a href=\"http:\/\/www.win-vector.com\/blog\/2016\/01\/parallel-computing-in-r\/\" rel=\"nofollow\">A gentle introduction to parallel computing in R<\/a>: using <code>parallel<\/code> package, <code>par*apply<\/code> function family, and binding values to environment.<\/li>\n<\/ol>\n\n<p>So my question is when I will be using <code>*apply<\/code> functions in Microsoft Azure Machine Learning Studio, will that be parallelized under the hood by default, or I need to make use of packages like <code>parallel<\/code>, <code>snow<\/code> etc.?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1476002256777,
        "Question_score":1,
        "Question_tags":"r|parallel-processing|azure-machine-learning-studio|microsoft-r",
        "Question_view_count":501,
        "Owner_creation_time":1365684640140,
        "Owner_last_access_time":1664014410763,
        "Owner_location":"Paderborn, Germany",
        "Owner_reputation":4588,
        "Owner_up_votes":1194,
        "Owner_down_votes":3,
        "Owner_views":453,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Personally, I think we could have marketed MRO a bit differently, without making such a big deal about parallelism\/multithreading. Ah well.<\/p>\n\n<p>R comes with an Rblas.dll\/.so which implements the routines used for linear algebra computations. These routines are used in various places, but one common use case is for fitting regression models. With MRO, we replace the standard Rblas with one that uses the <a href=\"https:\/\/software.intel.com\/en-us\/intel-mkl\" rel=\"noreferrer\">Intel Math Kernel Library<\/a>. When you call a function like <code>lm<\/code> or <code>glm<\/code>, MRO will use multiple threads and optimized CPU instructions to fit the model, which can get you dramatic speedups over the standard implementation.<\/p>\n\n<p>MRO isn't the only way you can get this sort of speedup; you can also compile\/download other BLAS implementations that are similarly optimized. We just make it an easy one-step download.<\/p>\n\n<p>Note that the MKL only affects code that involves linear algebra. It isn't a general-purpose speedup tool; any R code that doesn't do matrix computations won't see a performance improvement. In particular, it won't speed up any code that involves <em>explicit<\/em> parallelism, such as code using the parallel package, SNOW, or other cluster computing tools.<\/p>\n\n<p>On the other hand, it won't <em>degrade<\/em> them either. You can still use packages like parallel, SNOW, etc to create compute clusters and distribute your code across multiple processes. MRO works just like regular CRAN R in this respect. (One thing you might want to do, though, if you're creating a cluster of nodes on the one machine, is reduce the number of MKL threads. Otherwise you risk contention between the nodes for CPU cores, which will degrade performance.)<\/p>\n\n<p>Disclosure: I work for Microsoft.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1476024698777,
        "Answer_score":5,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/39941622",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":34545078,
        "Question_title":"Azure ML vs Cortana Analytics Suite",
        "Question_body":"<p>I am wondering what is the difference between Cortana Analytics and Azure ML ?<\/p>\n\n<ul>\n<li>those are 2 distincts solutions ? <\/li>\n<li>one is part of the other ?<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1451558192573,
        "Question_score":2,
        "Question_tags":"azure-machine-learning-studio|cortana-intelligence",
        "Question_view_count":1150,
        "Owner_creation_time":1425637780190,
        "Owner_last_access_time":1605735689050,
        "Owner_location":null,
        "Owner_reputation":559,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Question_last_edit_time":1543674264230,
        "Answer_body":"<p>Azure Machine Learning is part of the Cortana analytics suite<\/p>\n\n<p>You will find more info with the link below<\/p>\n\n<p><a href=\"http:\/\/www.sqlchick.com\/entries\/2015\/8\/22\/what-is-the-cortana-analytics-suite\" rel=\"nofollow\">All the details on the Cortana link here<\/a><\/p>\n\n<p>All the best<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1451558834643,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1457447097076,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34545078",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":35304901,
        "Question_title":"Azure Machine Learning Reader + Table Storage",
        "Question_body":"<p>Duplicating: <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/6560c2d6-9836-41a1-8076-caf0d514222a\/azure-machine-learning-reader-table-storage?forum=MachineLearning\" rel=\"nofollow\">https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/6560c2d6-9836-41a1-8076-caf0d514222a\/azure-machine-learning-reader-table-storage?forum=MachineLearning<\/a><\/p>\n\n<p>I currently have a table storage setup which is constantly performing insertions. There is approximately 260 million rows in the table storage. <\/p>\n\n<p>I have set up two machine learning experiments to use a 'Reader' to read the data from the 'Azure Table'. <\/p>\n\n<p>Experiment 1 is set to read all the rows to train the model.<\/p>\n\n<p>Experiment 2 is set to read only the top 1,000 rows to train the model.<\/p>\n\n<p>Experiment 1 has been running for over 5 hours with no results.<\/p>\n\n<p>Experiment 2 has been running for over 1 hour with no results.<\/p>\n\n<p>It is stuck on the 'Reader' process.<\/p>\n\n<p>I do not understand why experiment 2 is taking so long. I know I have set this up right as I tested the 'Reader's with another table storage. Thanks in advance for any help\/suggestions.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1455064585877,
        "Question_score":0,
        "Question_tags":"azure|azure-table-storage|azure-machine-learning-studio",
        "Question_view_count":104,
        "Owner_creation_time":1373050450247,
        "Owner_last_access_time":1663971630963,
        "Owner_location":null,
        "Owner_reputation":498,
        "Owner_up_votes":24,
        "Owner_down_votes":0,
        "Owner_views":47,
        "Question_last_edit_time":null,
        "Answer_body":"<p>A lot of this will probably depend on the design of your tables. Table Storage is a key \/ value store (think of it as a dictionary). It has some capabilities for scanning within a partition and across partitions - but the latencies will differ greatly. Ideally if you want to query 1000 rows they should be localized within a partition. See <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/storage-table-design-guide\/\" rel=\"nofollow\">Table Design Guide<\/a> and <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/storage-performance-checklist\/\" rel=\"nofollow\">Perf and Scalability Checklist<\/a> for full details.  <\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1455137464833,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35304901",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":59881727,
        "Question_title":"Debugging R Scripts in azure-ml: Where can stdout and stderr logs be found? (or why are they empty?)",
        "Question_body":"<p>I'm using \"studio (preview)\" from Microsoft Azure Machine Learning to create a pipeline that applies machine learning to a dataset in a blob storage that is connected to our data warehouse.<\/p>\n\n<p>In the \"Designer\", an \"Exectue R Script\" action can be added to the pipeline. I'm using this functionality to execute some of my own machine learning algorithms.<\/p>\n\n<p>I've got a 'hello world' version of this script working (including using the \"script bundle\" to load the functions in my own R files). It applies a very simple manipulation (compute the days difference with the date in the date column and 'today'), and stores the output as a new file. Given that the exported file has the correct information, I know that the R script works well.<\/p>\n\n<p>The script looks like this:<\/p>\n\n<pre><code># R version: 3.5.1\n# The script MUST contain a function named azureml_main\n# which is the entry point for this module.\n\n# The entry point function can contain up to two input arguments:\n#   Param&lt;medals&gt;: a R DataFrame\n#   Param&lt;matches&gt;: a R DataFrame\n\nazureml_main &lt;- function(dataframe1, dataframe2){\n\n  message(\"STARTING R script run.\")\n\n  # If a zip file is connected to the third input port, it is\n  # unzipped under \".\/Script Bundle\". This directory is added\n  # to sys.path.\n\n  message('Adding functions as source...')\n\n  if (FALSE) {\n    # This works...\n      source(\".\/Script Bundle\/first_function_for_script_bundle.R\")\n  } else {\n    # And this works as well!\n    message('Sourcing all available functions...')\n    functions_folder = '.\/Script Bundle'\n\n    list.files(path = functions_folder)\n    list_of_R_functions &lt;- list.files(path = functions_folder, pattern = \"^.*[Rr]$\", include.dirs = FALSE, full.names = TRUE)\n    for (fun in list_of_R_functions) {\n\n      message(sprintf('Sourcing &lt;%s&gt;...', fun))\n\n      source(fun)\n\n    }\n  }\n\n  message('Executing R pipeline...')\n  dataframe1 = calculate_days_difference(dataframe = dataframe1)\n\n  # Return datasets as a Named List\n  return(list(dataset1=dataframe1, dataset2=dataframe2))\n}\n<\/code><\/pre>\n\n<p>And although I do print some messages in the R Script, I haven't been able to find the \"stdoutlogs\" nor the \"stderrlogs\" that should contain these printed messages.<\/p>\n\n<p>I need the printed messages for 1) information on how the analysis went and -most importantly- 2) debugging in case the code failed.<\/p>\n\n<p>Now, I have found (on multiple locations) the files \"stdoutlogs.txt\" and \"stderrlogs.txt\". These can be found under \"Logs\" when I click on \"Exectue R Script\" in the \"Designer\".\nI can also find \"stdoutlogs.txt\" and \"stderrlogs.txt\" files under \"Experiments\" when I click on a finished \"Run\" and then both under the tab \"Outputs\" and under the tab \"Logs\".\nHowever... all of these files are empty.<\/p>\n\n<p>Can anyone tell me how I can print messages from my R Script and help me locate where I can find the printed information?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579792546180,
        "Question_score":1,
        "Question_tags":"r|azure|machine-learning|rstudio|azure-machine-learning-service",
        "Question_view_count":287,
        "Owner_creation_time":1534511592567,
        "Owner_last_access_time":1663852418420,
        "Owner_location":"Netherlands",
        "Owner_reputation":423,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Can you please click on the \"Execute R module\" and download the 70_driver.log? I tried message(\"STARTING R script run.\") in an R sample and can found the output there.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Z7s7h.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Z7s7h.png\" alt=\"view logs for a execute R script module\"><\/a><\/p>",
        "Answer_comment_count":5,
        "Answer_creation_time":1579829908723,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1579846977960,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59881727",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58885873,
        "Question_title":"Is it possible to use DistributedDataParallel with PyTorch Estimator",
        "Question_body":"<p>We know that Horovod is suppported. Is there an example script which uses DistributedDataParallel and Pytorch estimator?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1573859552880,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":98,
        "Owner_creation_time":1488866265607,
        "Owner_last_access_time":1638567942300,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You should be able to specify nccl or gloo as distributed data parallel backend, in addition to MPI with Horovod. See the <em>distributed_training<\/em> parameter of <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.dnn.pytorch?view=azure-ml-py\" rel=\"nofollow noreferrer\">PyTorch Estimator<\/a>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1574100412503,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58885873",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":32252282,
        "Question_title":"Where does AzureML run its analytics?",
        "Question_body":"<p>If I have data in a Hadoop Cluster or SQL Elastic DB, is ML bringing that data onto ML servers, or leaving it on Hadoop\/sql and running its analysis there?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1440686059093,
        "Question_score":0,
        "Question_tags":"azure|bigdata|azure-machine-learning-studio",
        "Question_view_count":82,
        "Owner_creation_time":1384802035143,
        "Owner_last_access_time":1663094482000,
        "Owner_location":"Miami Beach, FL",
        "Owner_reputation":2682,
        "Owner_up_votes":75,
        "Owner_down_votes":4,
        "Owner_views":1006,
        "Question_last_edit_time":1440686537416,
        "Answer_body":"<p>Currently, Azure Machine Learning will bring that data onto ML servers.  <\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1440686498027,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/32252282",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58691120,
        "Question_title":"Metric Document is too large in Azure ML Service",
        "Question_body":"<p>I am trying to save metrics : loss, validation loss and mAP at every epoch during 100 and 50 epochs but at the end of the experiment I have this error: \nRun failed: RunHistory finalization failed: ServiceException: Code: 400 Message: (ValidationError) Metric Document is too large<\/p>\n\n<p>I am using this code to save the metrics<\/p>\n\n<pre><code>run.log_list(\"loss\", history.history[\"loss\"])\nrun.log_list(\"val_loss\", history.history[\"val_loss\"])\nrun.log_list(\"val_mean_average_precision\", history.history[\"val_mean_average_precision\"])\n<\/code><\/pre>\n\n<p>I don't understand why trying to save only 3 metrics exceeds the limits of Azure ML Service.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1572862036240,
        "Question_score":3,
        "Question_tags":"azure|azure-machine-learning-service",
        "Question_view_count":368,
        "Owner_creation_time":1488384135020,
        "Owner_last_access_time":1663838517883,
        "Owner_location":"Paris, France",
        "Owner_reputation":620,
        "Owner_up_votes":589,
        "Owner_down_votes":5,
        "Owner_views":54,
        "Question_last_edit_time":1572863952216,
        "Answer_body":"<p>You could break the run history list writes into smaller blocks like this:<\/p>\n\n<pre><code>run.log_list(\"loss\", history.history[\"loss\"][:N])\nrun.log_list(\"loss\", history.history[\"loss\"][N:])\n<\/code><\/pre>\n\n<p>Internally, the run history service concatenates the blocks with same metric name into a contiguous list.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1572880035733,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58691120",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72641789,
        "Question_title":"Azure Machine Learning workspace's storage account permission issue",
        "Question_body":"<p>Was working on az ml cli v2 to deploy real-time endpoint with command <code>az ml online-deployment<\/code> through Azure pipeline. had double confirmed that the service connection used in this pipeline task had added the permissions below in Azure Portal but still showing the same error.<\/p>\n<pre><code>ERROR: Error with code: You don't have permission to alter this storage account. Ensure that you have been assigned both Storage Blob Data Reader and Storage Blob Data Contributor roles.\n<\/code><\/pre>\n<p>Using the same service connection, we are able to perform the creation of online endpoint with <code>az ml online-endpoint create<\/code> in the same and other workspaces.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655363403560,
        "Question_score":0,
        "Question_tags":"azure-devops|azure-storage|azure-machine-learning-service",
        "Question_view_count":129,
        "Owner_creation_time":1568185673007,
        "Owner_last_access_time":1663257666153,
        "Owner_location":"Kuala Lumpur, Federal Territory of Kuala Lumpur, Malaysia",
        "Owner_reputation":383,
        "Owner_up_votes":70,
        "Owner_down_votes":1,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Issue was resolved. I did not change anything in the service principal and running it on second day using same yml got through the issue. I guess there might be some propagation issue, but longer than usual.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1655429828623,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72641789",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":50219664,
        "Question_title":"Azure ML Experiment Batch Webservice Call Fails with Invalid Output Extension",
        "Question_body":"<p>I have an Azure webjob that is calling a ML training experiment via HttpRequests, leveraging the code generated in the ML webportal:<\/p>\n\n<pre><code>var request = new BatchExecutionRequest()\n            {\n                Inputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"input1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{trainingDataFileName}\"\n                        }\n                    },\n                },\n\n                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"output1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = \"azureStorageConnectionString\",\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/Model_2018421.ilearner\"\n                        }\n                    },\n                },\n\n                GlobalParameters = new Dictionary&lt;string, string&gt;()\n                {\n                }\n            };\n<\/code><\/pre>\n\n<p>However, the request fails with the following message:<\/p>\n\n<blockquote>\n  <p>The blob reference:\n  experiments\/experimentId\/TenantId\/Model_2018421.ilearner\n  has an invalid or missing file extension. Supported file extensions\n  for this output type are: \\\\\".csv, .tsv, .arff\\\\\"<\/p>\n<\/blockquote>\n\n<p>I'm pretty confused about this, since it's written right the documentation all over the place that if I'm expecting a trained model to use \".ilearner\" as the file extension for the model.<\/p>\n\n<p>I've seen <a href=\"https:\/\/stackoverflow.com\/questions\/47920098\/use-azure-data-factory-updating-azure-machine-learning-models\">this question<\/a> asking about the same error leveraging the DataFactory, and also <a href=\"https:\/\/datascience.stackexchange.com\/questions\/27397\/azure-machine-learning-model-retraining-problem\">this question on datascience.stackexchange<\/a>. Neither one had any clues, answers, or other follow up.<\/p>\n\n<p>Any insight on what I'm missing would be greatly appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1525714637430,
        "Question_score":0,
        "Question_tags":"c#|rest|azure|azure-machine-learning-studio",
        "Question_view_count":83,
        "Owner_creation_time":1477057589223,
        "Owner_last_access_time":1663770639160,
        "Owner_location":"Columbus, OH, United States",
        "Owner_reputation":547,
        "Owner_up_votes":1387,
        "Owner_down_votes":1,
        "Owner_views":45,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For anyone looking for your \"Don't Overthink It\" moment of the day:<\/p>\n\n<p>I needed to provide TWO output blob file references:<\/p>\n\n<pre><code>var request = new BatchExecutionRequest()\n            {\n                Inputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"input1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{trainingDataFileName}.csv\"\n                        }\n                    },\n                },\n\n                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"output1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{outputFileNameCsv}.csv\"\n                        }\n                    },\n                    {\n                        \"output2\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{outputFileNameIlearner}.ilearner\"\n                        }\n                    },\n                },\n\n                GlobalParameters = new Dictionary&lt;string, string&gt;()\n                {\n                }\n            };\n<\/code><\/pre>\n\n<p>There's an old saying in American English about not making assumptions, and I assumed the second output was an optional parameter used in batch operations. Since I'm not actually looking for more than one result from each call, I thought I was safe to remove the second output parameter.<\/p>\n\n<p>TL\/DR: Keep all the parameters the webservice portal's \"Consume\" tab generates, and make sure the first one is a .csv file reference.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1525787996893,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50219664",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":26193051,
        "Question_title":"Replacing specific values in dataset with Azure ML",
        "Question_body":"<p>Lately i've been testing Azure Machine Learning, and i like it. However, when i try to transform my dataset, there's a step that i can't perform easily : replacing a specific value in a column by another one.<\/p>\n\n<p>The <code>Missing Values Scrubber<\/code> module allows me to deal with undefined values, but in my case i need to change a specific value, or remove rows where that value appears. I don't see which module meets my requirement.<\/p>\n\n<p>Do you have any suggestion about this issue ? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1412427424453,
        "Question_score":1,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio",
        "Question_view_count":3772,
        "Owner_creation_time":1345114008840,
        "Owner_last_access_time":1495626547660,
        "Owner_location":"Lyon, France",
        "Owner_reputation":4233,
        "Owner_up_votes":67,
        "Owner_down_votes":1,
        "Owner_views":151,
        "Question_last_edit_time":1446191259743,
        "Answer_body":"<p>I found a solution <a href=\"http:\/\/social.msdn.microsoft.com\/Forums\/en-US\/bf8f76c7-f976-4552-8553-8e54133ff2c6\/replacing-specific-values-in-dataset-with-azure-ml?forum=MachineLearning\" rel=\"nofollow\">there<\/a>, by using a <code>Convert to Dataset<\/code> module.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1412637208267,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/26193051",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64315239,
        "Question_title":"Azure ML Inference Schema - \"List index out of range\" error",
        "Question_body":"<p>I have an ML model deployed on Azure ML Studio and I was updating it with an inference schema to allow compatibility with Power BI as described <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>When sending data up to the model via REST api (before adding this inference schema), everything works fine and I get results returned. However, once adding the schema as described in the instructions linked above and personalising to my data, the same data sent via REST api only returns the error &quot;list index out of range&quot;. The deployment goes ahead fine and is designated as &quot;healthy&quot; with no error messages.<\/p>\n<p>Any help would be greatly appreciated. Thanks.<\/p>\n<p>EDIT:<\/p>\n<p>Entry script:<\/p>\n<pre><code> import numpy as np\n import pandas as pd\n import joblib\n from azureml.core.model import Model\n    \n from inference_schema.schema_decorators import input_schema, output_schema\n from inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType\n from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n    \n def init():\n     global model\n     #Model name is the name of the model registered under the workspace\n     model_path = Model.get_model_path(model_name = 'databricksmodelpowerbi2')\n     model = joblib.load(model_path)\n    \n #Provide 3 sample inputs for schema generation for 2 rows of data\n numpy_sample_input = NumpyParameterType(np.array([[2400.0, 78.26086956521739, 11100.0, 3.612565445026178, 3.0, 0.0], [368.55, 96.88311688311687, 709681.1600000012, 73.88059701492537, 44.0, 0.0]], dtype = 'float64'))\n pandas_sample_input = PandasParameterType(pd.DataFrame({'1': [2400.0, 368.55], '2': [78.26086956521739, 96.88311688311687], '3': [11100.0, 709681.1600000012], '4': [3.612565445026178, 73.88059701492537], '5': [3.0, 44.0], '6': [0.0, 0.0]}))\n standard_sample_input = StandardPythonParameterType(0.0)\n    \n # This is a nested input sample, any item wrapped by `ParameterType` will be described by schema\n sample_input = StandardPythonParameterType({'input1': numpy_sample_input, \n                                             'input2': pandas_sample_input, \n                                             'input3': standard_sample_input})\n    \n sample_global_parameters = StandardPythonParameterType(1.0) #this is optional\n sample_output = StandardPythonParameterType([1.0, 1.0])\n    \n @input_schema('inputs', sample_input)\n @input_schema('global_parameters', sample_global_parameters) #this is optional\n @output_schema(sample_output)\n    \n def run(inputs, global_parameters):\n     try:\n         data = inputs['input1']\n         # data will be convert to target format\n         assert isinstance(data, np.ndarray)\n         result = model.predict(data)\n         return result.tolist()\n     except Exception as e:\n         error = str(e)\n         return error\n<\/code><\/pre>\n<p>Prediction script:<\/p>\n<pre><code> import requests\n import json\n from ast import literal_eval\n    \n # URL for the web service\n scoring_uri = ''\n ## If the service is authenticated, set the key or token\n #key = '&lt;your key or token&gt;'\n    \n # Two sets of data to score, so we get two results back\n data = {&quot;data&quot;: [[2400.0, 78.26086956521739, 11100.0, 3.612565445026178, 3.0, 0.0], [368.55, 96.88311688311687, 709681.1600000012, 73.88059701492537, 44.0, 0.0]]}\n # Convert to JSON string\n input_data = json.dumps(data)\n    \n # Set the content type\n headers = {'Content-Type': 'application\/json'}\n ## If authentication is enabled, set the authorization header\n #headers['Authorization'] = f'Bearer {key}'\n    \n # Make the request and display the response\n resp = requests.post(scoring_uri, input_data, headers=headers)\n print(resp.text)\n    \n result = literal_eval(resp.text)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1602495267847,
        "Question_score":1,
        "Question_tags":"azure|powerbi|schema|endpoint|azure-machine-learning-studio",
        "Question_view_count":785,
        "Owner_creation_time":1600260166047,
        "Owner_last_access_time":1615561616230,
        "Owner_location":null,
        "Owner_reputation":15,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1615561798207,
        "Answer_body":"<p>The Microsoft <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#power-bi-compatible-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> say's: &quot;In order to generate conforming swagger for automated web service consumption, scoring script run() function must have API shape of:<\/p>\n<blockquote>\n<p>A first parameter of type &quot;StandardPythonParameterType&quot;, named\n<strong>Inputs<\/strong> and nested.<\/p>\n<p>An optional second parameter of type &quot;StandardPythonParameterType&quot;,\nnamed GlobalParameters.<\/p>\n<p>Return a dictionary of type &quot;StandardPythonParameterType&quot; named\n<strong>Results<\/strong> and nested.&quot;<\/p>\n<\/blockquote>\n<p>I've already test this and it is case sensitive\nSo it will be like this:<\/p>\n<pre><code>import numpy as np\nimport pandas as pd\nimport joblib\n\nfrom azureml.core.model import Model\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.standard_py_parameter_type import \n    StandardPythonParameterType\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\nfrom inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n\ndef init():\n    global model\n    # Model name is the name of the model registered under the workspace\n    model_path = Model.get_model_path(model_name = 'databricksmodelpowerbi2')\n    model = joblib.load(model_path)\n\n# Provide 3 sample inputs for schema generation for 2 rows of data\nnumpy_sample_input = NumpyParameterType(np.array([[2400.0, 78.26086956521739, 11100.0, \n3.612565445026178, 3.0, 0.0], [368.55, 96.88311688311687, 709681.1600000012, \n73.88059701492537, 44.0, 0.0]], dtype = 'float64'))\n\npandas_sample_input = PandasParameterType(pd.DataFrame({'value': [2400.0, 368.55], \n'delayed_percent': [78.26086956521739, 96.88311688311687], 'total_value_delayed': \n[11100.0, 709681.1600000012], 'num_invoices_per30_dealing_days': [3.612565445026178, \n73.88059701492537], 'delayed_streak': [3.0, 44.0], 'prompt_streak': [0.0, 0.0]}))\n\nstandard_sample_input = StandardPythonParameterType(0.0)\n\n# This is a nested input sample, any item wrapped by `ParameterType` will be described \nby schema\nsample_input = StandardPythonParameterType({'input1': numpy_sample_input, \n                                         'input2': pandas_sample_input, \n                                         'input3': standard_sample_input})\n\nsample_global_parameters = StandardPythonParameterType(1.0) #this is optional\n\nnumpy_sample_output = NumpyParameterType(np.array([1.0, 2.0]))\n\n# 'Results' is case sensitive\nsample_output = StandardPythonParameterType({'Results': numpy_sample_output})\n\n# 'Inputs' is case sensitive\n@input_schema('Inputs', sample_input)\n@input_schema('global_parameters', sample_global_parameters) #this is optional\n@output_schema(sample_output)\ndef run(Inputs, global_parameters):\n    try:\n        data = inputs['input1']\n        # data will be convert to target format\n        assert isinstance(data, np.ndarray)\n        result = model.predict(data)\n        return result.tolist()\n    except Exception as e:\n        error = str(e)\n        return error\n<\/code><\/pre>\n<p>`<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1614800344830,
        "Answer_score":1,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1614801695372,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64315239",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":51702359,
        "Question_title":"In Azure ML Studio, score model doesn't return predicted values from an R model",
        "Question_body":"<p>I built a multiclass SVM model in R and used Create R model module from azure to train and predict my testing dataset. Here are the trainer and the score R scripts.<\/p>\n\n<p><strong>Trainer R script:<\/strong> <\/p>\n\n<pre><code>library(e1071)\nfeatures &lt;- get.feature.columns(dataset)\nlabels   &lt;- as.factor(get.label.column(dataset))\ntrain.data &lt;- data.frame(features, labels)\nfeature.names &lt;- get.feature.column.names(dataset)\nnames(train.data) &lt;- c(feature.names, \"Class\")\nmodel &lt;- svm(Class ~ . , train.data)\n<\/code><\/pre>\n\n<p><strong>Scores R script:<\/strong><\/p>\n\n<pre><code>library(e1071)    \nclasses &lt;- predict(model, dataset)\nclasses &lt;- as.factor(classes)\nres &lt;- data.frame(classes, probabilities = 0.5)\nprint(str(res))\nprint(res)\nscores &lt;- res\n<\/code><\/pre>\n\n<p>Note in my code, I hardcoded the probability values to simplify the code.<\/p>\n\n<p>Here is my component design in Azure: <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/hjMC4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hjMC4.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When I run the experiment, all the components work fine. However, in the score model, the scored dataset port does not show the predicted values. It only shows feature values from the testing dataset. I checked the output log of <em>Score model<\/em> and I could see the model has nicely predicted the testing data (note I added print commands in the Scores R script). But this is not enough and I need the prediction returned from the score model so I can pass it via API.<\/p>\n\n<p>Has anyone faced this issue before?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1533539693247,
        "Question_score":0,
        "Question_tags":"r|azure|azure-machine-learning-studio|ml-studio",
        "Question_view_count":657,
        "Owner_creation_time":1501114346137,
        "Owner_last_access_time":1605792960497,
        "Owner_location":"Australia",
        "Owner_reputation":37,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":1533560847647,
        "Answer_body":"<p>I found an answer for this. In fact, I cannot see the result in the outcome of the scoring model but when I linked it to a <em>select column in the dataset<\/em> module, I see the predicted columns there.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1534294797823,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51702359",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58816515,
        "Question_title":"Databricks UDF calling an external web service cannot be serialised (PicklingError)",
        "Question_body":"<p>I am using Databricks and have a column in a dataframe that I need to update for every record with an external web service call. In this case it is using the Azure Machine Learning Service SDK and does a service call. This code works fine when not run as a UDF in spark (ie. just python) however it throws a serialization error when I try to call it as a UDF. The same happens if I use a lambda and a map with an rdd.<\/p>\n\n<p>The model uses fastText and can be invoked fine from Postman or python via a normal http call or using the WebService SDK from AMLS - it's just when it is a UDF that it fails with this message:<\/p>\n\n<p>TypeError: can't pickle _thread._local objects<\/p>\n\n<p>The only workaround I can think of is to loop through each record in the dataframe sequentially and update the record with a call, however this is not very efficient. I don't know if this is a spark error or because the service is loading a fasttext model. When I use the UDF and mock a return value it works though.<\/p>\n\n<p>Error at bottom...<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice, AciWebservice\nfrom azureml.core import Workspace\n\ndef predictModelValue2(summary, modelName, modelLabel):  \n    raw_data = '[{\"label\": \"' + modelLabel + '\", \"model\": \"' + modelName + '\", \"as_full_account\": \"' + summary + '\"}]'\n    prediction = service.run(raw_data)\n    return prediction\n\nfrom pyspark.sql.types import FloatType\nfrom pyspark.sql.functions import udf\n\npredictModelValueUDF = udf(predictModelValue2)\n\nDVIRCRAMFItemsDFScored1 = DVIRCRAMFItemsDF.withColumn(\"Result\", predictModelValueUDF(\"Summary\", \"ModelName\", \"ModelLabel\"))\n<\/code><\/pre>\n\n<blockquote>\n  <p>TypeError: can't pickle _thread._local objects<\/p>\n  \n  <p>During handling of the above exception, another exception occurred:<\/p>\n  \n  <p>PicklingError                             Traceback (most recent call\n  last)  in \n  ----> 2 x = df.withColumn(\"Result\", predictModelValueUDF(\"Summary\",\n  \"ModelName\", \"ModelLabel\"))<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in wrapper(*args)\n      194         @functools.wraps(self.func, assigned=assignments)\n      195         def wrapper(*args):\n  --> 196             return self(*args)\n      197 \n      198         wrapper.<strong>name<\/strong> = self._name<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in <strong>call<\/strong>(self, *cols)\n      172 \n      173     def <strong>call<\/strong>(self, *cols):\n  --> 174         judf = self._judf\n      175         sc = SparkContext._active_spark_context\n      176         return Column(judf.apply(_to_seq(sc, cols, _to_java_column)))<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in _judf(self)\n      156         # and should have a minimal performance impact.\n      157         if self._judf_placeholder is None:\n  --> 158             self._judf_placeholder = self._create_judf()\n      159         return self._judf_placeholder\n      160 <\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in _create_judf(self)\n      165         sc = spark.sparkContext\n      166 \n  --> 167         wrapped_func = _wrap_function(sc, self.func, self.returnType)\n      168         jdt = spark._jsparkSession.parseDataType(self.returnType.json())\n      169         judf = sc._jvm.org.apache.spark.sql.execution.python.UserDefinedPythonFunction(<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in _wrap_function(sc,\n  func, returnType)\n       33 def _wrap_function(sc, func, returnType):\n       34     command = (func, returnType)\n  ---> 35     pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n       36     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n       37                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/rdd.py in _prepare_for_python_RDD(sc,\n  command)    2461     # the serialized command will be compressed by\n  broadcast    2462     ser = CloudPickleSerializer()\n  -> 2463     pickled_command = ser.dumps(command)    2464     if len(pickled_command) >\n  sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  # Default 1M<br>\n  2465         # The broadcast will have same life cycle as created\n  PythonRDD<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/serializers.py in dumps(self, obj)\n      709                 msg = \"Could not serialize object: %s: %s\" % (e.<strong>class<\/strong>.<strong>name<\/strong>, emsg)\n      710             cloudpickle.print_exec(sys.stderr)\n  --> 711             raise pickle.PicklingError(msg)\n      712 \n      713 <\/p>\n  \n  <p>PicklingError: Could not serialize object: TypeError: can't pickle\n  _thread._local objects<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1573553894003,
        "Question_score":1,
        "Question_tags":"pyspark|user-defined-functions|pickle|azure-databricks|azure-machine-learning-service",
        "Question_view_count":931,
        "Owner_creation_time":1256089885500,
        "Owner_last_access_time":1663046676847,
        "Owner_location":"Sydney, Australia",
        "Owner_reputation":4947,
        "Owner_up_votes":277,
        "Owner_down_votes":8,
        "Owner_views":531,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I am not expert in DataBricks or Spark, but pickling functions from the local notebook context is always problematic when you are touching complex objects like the <code>service<\/code> object. In this particular case, I would recommend removing the dependency on the azureML <code>service<\/code> object and just use <code>requests<\/code> to call the service. <\/p>\n\n<p>Pull the key from the service:<\/p>\n\n<pre><code># retrieve the API keys. two keys were generated.\nkey1, key2 = service.get_keys()\nscoring_uri = service.scoring_uri\n<\/code><\/pre>\n\n<p>You should be able to use these strings in the UDF directly without pickling issues -- <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/9233ce089afb81d466076e36e7e61c3ce4cfafec\/how-to-use-azureml\/ml-frameworks\/chainer\/deployment\/train-hyperparameter-tune-deploy-with-chainer\/train-hyperparameter-tune-deploy-with-chainer.ipynb\" rel=\"nofollow noreferrer\">here is an example<\/a> of  how you would call the service with just requests. Below applied to your UDF:<\/p>\n\n<pre><code>import requests, json\ndef predictModelValue2(summary, modelName, modelLabel):  \n  input_data = json.dumps({\"summary\": summary, \"modelName\":, ....})\n\n  headers = {'Content-Type':'application\/json', 'Authorization': 'Bearer ' + key1}\n\n  # call the service for scoring\n  resp = requests.post(scoring_uri, input_data, headers=headers)\n\n  return resp.text[1]\n\n<\/code><\/pre>\n\n<p>On a side node, though: your UDF will be called for each row in your data frame and each time it will make a network call -- that will be very slow. I would recommend looking for ways to batch the execution. As you can see from your constructed json <code>service.run<\/code> will accept an array of items, so you should call it in batches of 100s or so.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1573841167917,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1573844785320,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58816515",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60637170,
        "Question_title":"How to pass arguments to scoring file when deploying a Model in AzureML",
        "Question_body":"<p>I am deploying a trained model to an ACI endpoint on Azure Machine Learning, using the Python SDK.\nI have created my score.py file, but I would like that file to be called with an argument being passed (just like with a training file) that I can interpret using <code>argparse<\/code>.\nHowever, I don't seem to find how I can pass arguments\nThis is the code I have to create the InferenceConfig environment and which obviously does not work.  Should I fall back on using the extra Docker file steps or so?<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.environment import Environment\nfrom azureml.core.model import InferenceConfig\n\nenv = Environment('my_hosted_environment')\nenv.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['scikit-learn'],\n    pip_packages=['azureml-defaults'])\nscoring_script = 'score.py --model_name ' + model_name\ninference_config = InferenceConfig(entry_script=scoring_script, environment=env)\n<\/code><\/pre>\n\n<p>Adding the score.py for reference on how I'd love to use the arguments in that script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#removed imports\nimport argparse\n\ndef init():\n    global model\n\n    parser = argparse.ArgumentParser(description=\"Load sklearn model\")\n    parser.add_argument('--model_name', dest=\"model_name\", required=True)\n    args, _ = parser.parse_known_args()\n\n    model_path = Model.get_model_path(model_name=args.model_name)\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    try:\n        data = json.loads(raw_data)['data']\n        data = np.array(data)\n        result = model.predict(data)\n        return result.tolist()\n\n    except Exception as e:\n        result = str(e)\n        return result\n<\/code><\/pre>\n\n<p>Interested to hear your thoughts<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_time":1583933260433,
        "Question_score":4,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":1681,
        "Owner_creation_time":1360655430743,
        "Owner_last_access_time":1663784892907,
        "Owner_location":"Belgium",
        "Owner_reputation":2947,
        "Owner_up_votes":297,
        "Owner_down_votes":16,
        "Owner_views":355,
        "Question_last_edit_time":1584005920356,
        "Answer_body":"<p>How to deploy using environments can be found here <a href=\"https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2FAzure%2FMachineLearningNotebooks%2Fblob%2Fmaster%2Fhow-to-use-azureml%2Fdeployment%2Fdeploy-to-cloud%2Fmodel-register-and-deploy.ipynb&amp;data=02%7C01%7CRamprasad.Mula%40microsoft.com%7Ce06d310b0447416ab46b08d7bc836a81%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637185146436156499&amp;sdata=uQo332dpjuiNqWFCguvs3Kgg7UUMN8MBEzLxTPyH4MM%3D&amp;reserved=0\" rel=\"nofollow noreferrer\">model-register-and-deploy.ipynb<\/a> .  InferenceConfig class accepts  source_directory and entry_script <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py#parameters\" rel=\"nofollow noreferrer\">parameters<\/a>, where source_directory  is a path to the folder that contains all files(score.py and any other additional files) to create the image. <\/p>\n\n<p>This <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-multi-model\/multi-model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">multi-model-register-and-deploy.ipynb<\/a> has code snippets on how to create InferenceConfig with source_directory and entry_script.<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\n\nmyenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\ninference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n\nservice = Model.deploy(workspace=ws,\n                       name='sklearn-mnist-svc',\n                       models=[model], \n                       inference_config=inference_config,\n                       deployment_config=aciconfig)\n\nservice.wait_for_deployment(show_output=True)\n\nprint(service.scoring_uri)\n<\/code><\/pre>",
        "Answer_comment_count":4,
        "Answer_creation_time":1584005785480,
        "Answer_score":-2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1584011988083,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60637170",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67988138,
        "Question_title":"Azure ML Tutorial - Failed to load entrypoint automl",
        "Question_body":"<p>I'm doing following tutorial. I failed to run &quot;Create a control script&quot;.<\/p>\n<p>What could be wrong?<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-hello-world\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-hello-world<\/a><\/p>\n<pre><code>azureuser@kensmlcompute:~\/cloudfiles\/code\/Users\/my.name\/get-started$ python run-hello.py \nFailure while loading azureml_run_type_providers. Failed to load entrypoint automl = \nazureml.train.automl.run:AutoMLRun._from_run_dto with exception (pyarrow 4.0.0 \n(\/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages), \nRequirement.parse('pyarrow&lt;4.0.0,&gt;=0.17.0'), {'azureml-dataset-runtime'}).\nhttps:\/\/ml.azure.com\/runs\/day1-experiment-hello_1623766747_073126f5? \nwsid=\/subscriptions\/1679753a-501e-4e46-9bff- \n6120ed5694cf\/resourcegroups\/kensazuremlrg\/workspaces\/kensazuremlws&amp;tid=94fe1041-ba47-4f49- \n866b- \n06c297c116cc\nazureuser@kensmlcompute:~\/cloudfiles\/code\/Users\/my.name\/get-started$\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1623766974453,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":1241,
        "Owner_creation_time":1478251050693,
        "Owner_last_access_time":1663835968943,
        "Owner_location":"Finland",
        "Owner_reputation":1519,
        "Owner_up_votes":116,
        "Owner_down_votes":0,
        "Owner_views":375,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I think the error indicates that your environment is using pyarrow package which is of version 4.0.0 whereas azureml-dataset-runtime requires the package to be &gt;=0.17.0 but &lt;4.0.0<\/p>\n<p>It would be easier for you to uninstall the package and install a specific version. The list of releases of pyarrow are available here.<\/p>\n<p>Since you are using a notebook create new cells and run these commands.<\/p>\n<pre><code> !pip uninstall pyarrow\n !pip install -y pyarrow==3.0.0\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1623861331177,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67988138",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":45796489,
        "Question_title":"Azure Machine Learning: What error is this?",
        "Question_body":"<p>I am using a Classic Web Service with a non-default endpoint for a Update Resource activity on the Azure Data Factory. This is the error I get:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/shK0R.png\" rel=\"nofollow noreferrer\">Screenshot of Error<\/a><\/p>\n\n<p>I didn't find any info on the web and couldn't figure it out myself. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/data-factory\/data-factory-azure-ml-update-resource-activity\" rel=\"nofollow noreferrer\">This<\/a> website shows an example that I used by just filling in my values for mlEndpoint, apiKey and updateRessourceEndpoint:<\/p>\n\n<pre><code>{\n    \"name\": \"updatableScoringEndpoint2\",\n    \"properties\": {\n        \"type\": \"AzureML\",\n        \"typeProperties\": {\n            \"mlEndpoint\": \"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/xxx\/services\/--scoring experiment--\/jobs\",\n            \"apiKey\": \"endpoint2Key\",\n            \"updateResourceEndpoint\": \"https:\/\/management.azureml.net\/workspaces\/xxx\/webservices\/--scoring experiment--\/endpoints\/endpoint2\"\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>There is no mention of a token that needs to be passed...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1503316557373,
        "Question_score":0,
        "Question_tags":"azure|azure-data-factory|azure-machine-learning-studio",
        "Question_view_count":253,
        "Owner_creation_time":1476806455803,
        "Owner_last_access_time":1661945674687,
        "Owner_location":"Holzkirchen, Deutschland",
        "Owner_reputation":3068,
        "Owner_up_votes":186,
        "Owner_down_votes":66,
        "Owner_views":386,
        "Question_last_edit_time":null,
        "Answer_body":"<p>this error is basically saying the apiKey you provided is invalid to perform the update resource operation. Here is some posts for your reference: <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/3bb77e37-8860-43c6-bcaa-d6ebd70617b8\/retrain-predictive-web-service-programmatically-when-do-not-have-access-to-managementazuremlnet?forum=MachineLearning\" rel=\"nofollow noreferrer\">https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/3bb77e37-8860-43c6-bcaa-d6ebd70617b8\/retrain-predictive-web-service-programmatically-when-do-not-have-access-to-managementazuremlnet?forum=MachineLearning<\/a><\/p>\n\n<p>Please also be noted that if you modified your linked service in ADF, remember to re-deploy the pipeline as well to reflect your change in time.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1503393687923,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45796489",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":39376560,
        "Question_title":"How to call Tensorflow in Azure ML",
        "Question_body":"<p>I've so far seen people using tensorflow in Azure using in this <a href=\"http:\/\/www.mikelanzetta.com\/tensorflow-on-azure-using-docker.html\" rel=\"nofollow\">link<\/a>.\nAlso using the advantage of ubuntu in windows tensorflow can be run on\nwindows pc as well.Here is the <a href=\"http:\/\/www.hanselman.com\/blog\/PlayingWithTensorFlowOnWindows.aspx\" rel=\"nofollow\">link<\/a>.\nHowever during a conversation with Windows Azure engineer Hai Ning it came out\nthat \"Azure ML PaaS VMs use Windows OS; TensorFlow is not supported on Windows as of now.\"\nHence there is no direct way of running tensorflow in Azure ML.\nIs there any work around anyone figured out that allows running tensorflow in Azure ML.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1473271529687,
        "Question_score":1,
        "Question_tags":"tensorflow|azure-machine-learning-studio",
        "Question_view_count":1743,
        "Owner_creation_time":1435075201580,
        "Owner_last_access_time":1663954259410,
        "Owner_location":"Seattle, WA, USA",
        "Owner_reputation":546,
        "Owner_up_votes":232,
        "Owner_down_votes":1,
        "Owner_views":59,
        "Question_last_edit_time":1473276447316,
        "Answer_body":"<p>Quick update for you. As of TensorFlow r0.12 there is now a native TensorFlow package for Windows. I have it running successfully on my Windows 10 laptop. See this <a href=\"https:\/\/developers.googleblog.com\/2016\/11\/tensorflow-0-12-adds-support-for-windows.html\" rel=\"nofollow noreferrer\">blog post<\/a> from Google for more information.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1486144727963,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/39376560",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":49604773,
        "Question_title":"Azure Machine Learning Studio vs. Workbench",
        "Question_body":"<p>What is the difference between <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-studio\/\" rel=\"noreferrer\">Azure Machine Learning Studio<\/a> and <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-services\/\" rel=\"noreferrer\">Azure Machine Learning Workbench<\/a>?  What is the <em>intended<\/em> difference? And is it expected that Workbench is heading towards deprecation in favor of Studio?<\/p>\n\n<p>I have gathered an assorted collection of differences:<\/p>\n\n<ul>\n<li>Studio has a hard limit of 10 GB total input of training data per module, whereas Workbench has a variable limit by price.<\/li>\n<li>Studio appears to have a more fully-featured GUI and user-friendly deployment tools, whereas Workbench appears to have more powerful \/ customizable deployment tools.<\/li>\n<li>etc.<\/li>\n<\/ul>\n\n<p>However, I have also found several scattered references claiming that Studio is a renamed updated of Workbench, even though both services appear to still be offered.<\/p>\n\n<p>For a fresh Data Scientist looking to adopt the Microsoft stack (potentially on an enterprise scale within the medium-term and for the long-term), which offering should I prefer?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1522638099293,
        "Question_score":8,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-workbench",
        "Question_view_count":3387,
        "Owner_creation_time":1434736108840,
        "Owner_last_access_time":1663241421093,
        "Owner_location":"Dallas, TX, United States",
        "Owner_reputation":2045,
        "Owner_up_votes":1074,
        "Owner_down_votes":66,
        "Owner_views":166,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Azure Machine Learning Workbench is a preview downloadable application. It provides a UI for many of the Azure Machine Learning CLI commands, particularly around experimentation submission for Python based jobs to DSVM or HDI. The Azure Machine Learning CLI is made up of many key functions, such as job submisison, and creation of real time web services. The workbench installer provided a way to install everything required to participate in the preview. <\/p>\n\n<p>Azure Machine Learning Studio is an older product, and provides a drag and drop interface for creating simply machine learning processes. It has limitations about the size of the data that can be handled (about 10gigs of processing). Learning and customer requests have based on this service have contributed to the design of the new Azure Machine Learning CLI mentioned above.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1524806701633,
        "Answer_score":6,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49604773",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":37228027,
        "Question_title":"How do get my custom Python code into Azure Machine Learning for use a a ZIP resource?",
        "Question_body":"<p>The <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-execute-python-scripts\/\" rel=\"nofollow\">documentation<\/a> for the Azure Machine Learning Python script module describes using a ZIP file containing code as a resource, but I don't see how to create and upload such a ZIP file in the first place.<\/p>\n\n<p>How do get my custom Python code into Azure Machine Learning for use as a ZIP resource?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1463237105637,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-studio",
        "Question_view_count":664,
        "Owner_creation_time":1299959670313,
        "Owner_last_access_time":1663787198420,
        "Owner_location":"United States",
        "Owner_reputation":41475,
        "Owner_up_votes":1198,
        "Owner_down_votes":107,
        "Owner_views":1912,
        "Question_last_edit_time":1463238039270,
        "Answer_body":"<p>Just upload it as a dataset. <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-execute-python-scripts\/\" rel=\"nofollow\">Reference.<\/a> (search for it, as it is not on the first page).<\/p>\n\n<p><a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-walkthrough-2-upload-data\/#upload-the-dataset-to-machine-learning-studio\" rel=\"nofollow\">Reference<\/a> on how to upload the dataset. <\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1463238158720,
        "Answer_score":1,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37228027",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":68463080,
        "Question_title":"how create azure machine learning scoring image using local package",
        "Question_body":"<p>I have pkl package saved in my azure devops repository<\/p>\n<p>using below code it searches for package in workspace.\nHow to provide package saved in repository<\/p>\n<pre><code> ws = Workspace.get(\n         name=workspace_name,\n         subscription_id=subscription_id,\n        resource_group=resource_group,\n        auth=cli_auth)\n\nimage_config = ContainerImage.image_configuration(\n    execution_script=&quot;score.py&quot;,\n    runtime=&quot;python-slim&quot;,\n    conda_file=&quot;conda.yml&quot;,\n    description=&quot;Image with ridge regression model&quot;,\n    tags={&quot;area&quot;: &quot;ml&quot;, &quot;type&quot;: &quot;dev&quot;},\n)\n\nimage = Image.create(\n    name=image_name,  models=[model], image_config=image_config, workspace=ws\n)\n\nimage.wait_for_creation(show_output=True)\n\nif image.creation_state != &quot;Succeeded&quot;:\n    raise Exception(&quot;Image creation status: {image.creation_state}&quot;)\n\nprint(\n    &quot;{}(v.{} [{}]) stored at {} with build log {}&quot;.format(\n        image.name,\n        image.version,\n        image.creation_state,\n        image.image_location,\n        image.image_build_log_uri,\n    )\n)\n\n# Writing the image details to \/aml_config\/image.json\nimage_json = {}\nimage_json[&quot;image_name&quot;] = image.name\nimage_json[&quot;image_version&quot;] = image.version\nimage_json[&quot;image_location&quot;] = image.image_location\nwith open(&quot;aml_config\/image.json&quot;, &quot;w&quot;) as outfile:\n    json.dump(image_json, outfile)\n<\/code><\/pre>\n<p>I tried to provide path to models but its fails saying package not found<\/p>\n<p>models = $(System.DefaultWorkingDirectory)\/package_model.pkl<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626831896507,
        "Question_score":0,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":140,
        "Owner_creation_time":1567209656790,
        "Owner_last_access_time":1663349187993,
        "Owner_location":null,
        "Owner_reputation":417,
        "Owner_up_votes":53,
        "Owner_down_votes":0,
        "Owner_views":233,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Register model:\nRegister a file or folder as a model by calling <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none-\" rel=\"nofollow noreferrer\">Model.register()<\/a>.<\/p>\n<p>In addition to the content of the model file itself, your registered model will also store model metadata -- model description, tags, and framework information -- that will be useful when managing and deploying models in your workspace. Using tags, for instance, you can categorize your models and apply filters when listing models in your workspace.<\/p>\n<pre><code>model = Model.register(workspace=ws,\n                       model_name='',                # Name of the registered model in your workspace.\n                       model_path='',  # Local file to upload and register as a model.\n                       model_framework=Model.Framework.SCIKITLEARN,  # Framework used to create the model.\n                       model_framework_version=sklearn.__version__,  # Version of scikit-learn used to create the model.\n                       sample_input_dataset=input_dataset,\n                       sample_output_dataset=output_dataset,\n                       resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\n                       description='Ridge regression model to predict diabetes progression.',\n                       tags={'area': 'diabetes', 'type': 'regression'})\n\nprint('Name:', model.name)\nprint('Version:', model.version)\n<\/code><\/pre>\n<p>Deploy machine learning models to Azure: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python<\/a><\/p>\n<p>To Troubleshooting remote model deployment Please follow the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment?tabs=azcli#function-fails-get_model_path\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BL0Nm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BL0Nm.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1627276170737,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1627278873550,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68463080",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64546521,
        "Question_title":"Azure ML FileDataset registers, but cannot be accessed for Data Labeling project",
        "Question_body":"<p><strong>Objective<\/strong>: Generate a down-sampled FileDataset using random sampling from a larger FileDataset to be used in a Data Labeling project.<\/p>\n<hr \/>\n<p><strong>Details<\/strong>: I have a large FileDataset containing millions of images. Each filename contains details about the 'section' it was taken from. A section may contain thousands of images. I want to randomly select a specific number of <strong>sections<\/strong> and all the images associated with those sections. Then register the sample as a new dataset.<\/p>\n<p>Please note that the code below is not a direct copy and paste as there are elements such as filepaths and variables that have been renamed for confidentiality reasons.<\/p>\n<pre><code>import azureml.core\nfrom azureml.core import Dataset, Datastore, Workspace\n\n# Load in work space from saved config file\nws = Workspace.from_config()\n\n# Define full dataset of interest and retrieve it\ndataset_name = 'complete_2017'\ndata = Dataset.get_by_name(ws, dataset_name)\n\n# Extract file references from dataset as relative paths\nrel_filepaths = data.to_path()\n\n# Stitch back in base directory path to get a list of absolute paths\nsrc_folder = '\/raw-data\/2017'\nabs_filepaths = [src_folder + path for path in rel_filepaths]\n\n# Define regular expression pattern for extracting source section\nimport re\npattern = re.compile('\\\/(S.+)_image\\d+.jpg')\n\n# Create new list of all unique source sections\nsections = sorted(set([m.group(1) for m in map(pattern.match, rel_filepaths) if m]))\n\n# Randomly select sections\nnum_sections = 5\nset_seed = 221020\nrandom.seed(set_seed)   # for repeatibility\nsample_sections = random.choices(sections, k = num_sections)\n\n# Extract images related to the selected sections\nmatching_images = [filename for filename in abs_filepaths if any(section in filename for section in sample_sections)]\n\n# Define datastore of interest\ndatastore = Datastore.get(ws, 'ml-datastore')\n\n# Convert string paths to Azure Datapath objects and relate back to datastore\nfrom azureml.data.datapath import DataPath\ndatastore_path = [DataPath(datastore, filepath) for filepath in matching_images]\n\n# Generate new dataset using from_files() and filtered list of paths\nsample = Dataset.File.from_files(datastore_path)\n\nsample_name = 'random-section-sample'\nsample_dataset = sample.register(workspace = ws, name = sample_name, description = 'Sampled sections from full dataset using set seed.')\n<\/code><\/pre>\n<hr \/>\n<p><strong>Issue<\/strong>: The code I've written in Python SDK runs and the new FileDataset registers, but when I try to look at the dataset details or use it for a Data Labeling project I get the following error even as <em>Owner<\/em>.<\/p>\n<pre><code>Access denied: Failed to authenticate data access with Workspace system assigned identity. Make sure to add the identity as Reader of the data service.\n<\/code><\/pre>\n<p>Additionally, under the details tab <strong>Files in dataset<\/strong> is <em>Unknown<\/em> and <strong>Total size of files in dataset<\/strong> is <em>Unavailable<\/em>.<\/p>\n<p>I haven't come across this issue anywhere else. I'm able to generate datasets in other ways, so I suspect it's an issue with the code given that I'm working with the data in an unconventional way.<\/p>\n<hr \/>\n<p><strong>Additional Notes<\/strong>:<\/p>\n<ul>\n<li>Azure ML version is 1.15.0<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1603756226340,
        "Question_score":2,
        "Question_tags":"azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":801,
        "Owner_creation_time":1603750656893,
        "Owner_last_access_time":1619302917223,
        "Owner_location":"New Zealand",
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>One of my colleagues discovered that the managed identities were preventing the preview functionality. Once this aspect of the identities was modified, we could examine the data and use it for a data labelling project.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1603917086963,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64546521",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73720626,
        "Question_title":"How can I select a column product of a math operation in Azure Machine Learning Designer?",
        "Question_body":"<p>I have created a pipeline in Azure Machine Learning that includes a <strong>Math Operation<\/strong> (natural logarithm of a column named <em>charges<\/em>). The next pill to the <strong>Math Operatio<\/strong>n is <strong>Select Column in Dataset<\/strong>. Since the pipeline has not ben submitted and run I cannot access the column <em>ln(charges)<\/em> in the pill <strong>Select Column in Dataset<\/strong>.\nMy problem is that if I submit it I am able to run it and see the results in the pipeline once completed, but I have found no way of accessing those results (and thus the <em>ln(charges)<\/em> column in Designer.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DOddA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DOddA.png\" alt=\"Pipeline Job after submitting and running\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Hp6Dc.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Hp6Dc.png\" alt=\"Pipeline in designer after submitting and running the job\" \/><\/a><\/p>\n<p><strong>UPDATE:<\/strong><\/p>\n<p><strong>I have found a workaround. Still in designer the column ln(charges) is not selectable but if I manually enter Ln(charges) in the select column fields it works.<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663174586247,
        "Question_score":1,
        "Question_tags":"azure|machine-learning|designer|azure-machine-learning-studio",
        "Question_view_count":55,
        "Owner_creation_time":1526397625170,
        "Owner_last_access_time":1663935384777,
        "Owner_location":"Spain",
        "Owner_reputation":47,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":1663233012832,
        "Answer_body":"<p>The following is the procedure of the math operation in Azure ML designer to select the column to be implemented. The following procedure will help to give the column name as well as we can also give the index number of the column. This answer contains both the procedures.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9YV8f.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9YV8f.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>We can click on edit column.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bAnfq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bAnfq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/cZFfF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cZFfF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Based on the dataset which the experiment was running, both are options are mentioned in the above screen. We can choose either of the options.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PblH7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PblH7.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>To access the data, right click and go to access data and click on result_dataset<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/8jHz5.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8jHz5.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The following page will open and click on any file mentioned in the box<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4jWxT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4jWxT.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Click on download and open in the editor according to your wish.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wfGPV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wfGPV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It looks like the above result screen.\nThe below screens are the designer created.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bTWGR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bTWGR.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6kUAw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6kUAw.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/I2Ej1.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/I2Ej1.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>To check the final model result. Go to evaluate model and get the results in visualization manner.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1663747263323,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73720626",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70873347,
        "Question_title":"Recommended options for Feature store in Azure ML",
        "Question_body":"<p>This is regard to ML Feature Stores, is Feast the recommended option today for Feature Store with Azure ML or is there any other options?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643257477277,
        "Question_score":3,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":327,
        "Owner_creation_time":1632461310820,
        "Owner_last_access_time":1650860148037,
        "Owner_location":null,
        "Owner_reputation":107,
        "Owner_up_votes":49,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>We have roadmap to support that is something more native and also tightly integrates into Azure ML.<\/p>\n<p>Here is <a href=\"https:\/\/techcommunity.microsoft.com\/t5\/ai-customer-engineering-team\/bringing-feature-store-to-azure-from-microsoft-azure-redis-and\/ba-p\/2918917\" rel=\"nofollow noreferrer\">doc<\/a> to integration with OSS tool such as Hopsworks\/Feast and leveraging existing functionalities (designer\/pipelines, dataset) for an end-to-end &quot;feature store&quot; solution.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1643267855687,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70873347",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60978808,
        "Question_title":"When should I use Azure ML Notebooks VS Azure Databricks? Both are competitor products in my opinion",
        "Question_body":"<p>Pretty self-explanatory question. When should I use Azure ML Notebooks VS Azure Databricks? I feel there\u2019s a great overlap between the two products and one is definitely better marketed than the other.. <\/p>\n\n<p>I\u2019m mainly looking for information concerning datasets sizes and typical workflow. Why should I use Databricks over AzureML if I don\u2019t have a Spark oriented workflow ?<\/p>\n\n<p>Thanks !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1585769120130,
        "Question_score":7,
        "Question_tags":"azure|machine-learning|databricks|azure-machine-learning-service",
        "Question_view_count":3755,
        "Owner_creation_time":1447320137140,
        "Owner_last_access_time":1663776204940,
        "Owner_location":null,
        "Owner_reputation":313,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":40,
        "Question_last_edit_time":1625712808263,
        "Answer_body":"<p>@Nethim, from my pov these are the main difference:<br><\/p>\n\n<ol>\n<li><p>Data Distribution:<\/p>\n\n<ul>\n<li>Azure ML Notebooks are good when you are training with a limited data on single machine. Though Azure ML provides training clusters, the data distribution among the nodes is to be handled in the code.<\/li>\n<li>Azure Databricks with its RDDs are designed to handle data distributed on multiple nodes.This is advantageous when your data size is huge.When your data size is small and can fit in a scaled up single machine\/ you are using a pandas dataframe, then use of Azure databricks is a overkill<\/li>\n<\/ul><\/li>\n<li><p>Data Cleaning:\nDatabricks can support a lot of file formats natively and querying and cleaning huge datasets are easy where as this has to be handled custom in AzureML notebooks. This can be done with a aml notebooks but cleaning and writing to stores has to be handled.<\/p><\/li>\n<li>Training\nBoth has the capabilities if distributing the training, Databricks provides inbuilt ML algorithms that can act on chunk of data on that node and coordinate with other nodes. Though this can be done on both AzureMachineLearning and Databricks with tf,horovod etc.,<\/li>\n<\/ol>\n\n<p>In general(just my opinion), if the dataset is small, aml notebooks is good.If the data size is huge, then Azure databricks is easy for datacleanup and format conversions.Then the training can happen on AML or databricks.Though databricks has a learning curve whereas Azure ML can be easy with the python and pandas.<\/p>\n\n<p>Thanks.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1585823280850,
        "Answer_score":6,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60978808",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":63807950,
        "Question_title":"Letter Recognition Error in Azure ML Studio",
        "Question_body":"<p>I'm having troubles with a Letter Recognition model I'm creating in Azure ML Studio.<\/p>\n<p>I'm running a few algorithms - Decision Jungle, Neural Network, Decision Forest, Logistic Regression, One vs. All Multiclass, and then I append them using the Add rows method (Neural Network and Desicion Jungle\/ Decision Forest and Logistic Regression), until I append them all.<\/p>\n<p>However, appending Decision Forest and Logistic Regression I get the following error:<\/p>\n<pre><code>requestId = 9292bc066f51404eb5e0d0d219d3a072 errorComponent=Module. taskStatusCode=400. {&quot;Exception&quot;:{&quot;ErrorId&quot;:&quot;NotInRangeValue&quot;,&quot;ErrorCode&quot;:&quot;0008&quot;,&quot;ExceptionType&quot;:&quot;ModuleException&quot;,&quot;Message&quot;:&quot;Error 0008: Parameter \\&quot;Dataset2(number of columns)\\&quot; value should be in the range of [3, 3].&quot;}}Error: Error 0008: Parameter &quot;Dataset2(number of columns)&quot; value should be in the range of [3, 3]. Process exited with error code -2\n<\/code><\/pre>\n<p>Any advice what should I do? Huge thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1599640980777,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":37,
        "Owner_creation_time":1599640856987,
        "Owner_last_access_time":1645372502427,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1599642150896,
        "Answer_body":"<p>This error occurs when there is a mismatch of number of columns of the two dataset you are appending.<\/p>\n<p>Looking at the error :<\/p>\n<p>The output of one model is returning rows with 3 columns and other one is having either more or less than 3 columns.<\/p>\n<p>Before this step &quot;Add Rows&quot; step -&gt; Do quick Visualize<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PsYQT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PsYQT.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>This will give a view of the dataset that you are planning to append.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/x442d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/x442d.png\" alt=\"![enter image description here\" \/><\/a><\/p>\n<p>Ensure for both, the columns numbers are same.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1599653828450,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63807950",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":46797468,
        "Question_title":"Incorrect neural network schema in training output",
        "Question_body":"<p>I'm training a model in Azure ML Studio and the Net# specification I'm using doesn't match the NET# specification in the training output.<\/p>\n\n<p>Here's my experiment - <a href=\"https:\/\/i.stack.imgur.com\/WFMGs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WFMGs.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>and here are my NN params - <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/sVzcA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sVzcA.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>and finally here is the NET# specification in the Hyperparams output -<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ehImq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ehImq.png\" alt=\"enter image description here\"><\/a>\nIt's not using two hidden layers and it's also using sigmoid instead of ReLu. Is this expected behavior?<\/p>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1508267201980,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":36,
        "Owner_creation_time":1274124294967,
        "Owner_last_access_time":1642446534253,
        "Owner_location":null,
        "Owner_reputation":655,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":47,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There is an issue with using custom NET# and parameter sweeps together: it switches over to using the default fully connected topology. <\/p>\n\n<p>Unfortunately, the workaround is to train the model for each parameter value separately.  <\/p>\n\n<p>-Roope  <\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1508336707887,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46797468",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":63425902,
        "Question_title":"Are Machine Learning Studios's web services public?",
        "Question_body":"<p>I have created an experiment in Machine Learning Studio and deployed it as a web service. I've got a request-response API in my workspace that works. Can it be also used by other people?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1597493508787,
        "Question_score":0,
        "Question_tags":"azure|web-services|machine-learning|azure-machine-learning-studio",
        "Question_view_count":39,
        "Owner_creation_time":1575044869560,
        "Owner_last_access_time":1604600830650,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When you deploy a model, a Webservice object is returned with information about the service.<\/p>\n<pre><code>from azureml.core.webservice import AciWebservice, Webservice\nfrom azureml.core.model import Model\n\ndeployment_config = AciWebservice.deploy_configuration(cpu_cores = 3, memory_gb = 15, location = &quot;centralus&quot;)\nservice = Model.deploy(ws, &quot;aciservice&quot;, [model], inference_config, deployment_config)\nservice.wait_for_deployment(show_output = True)\nprint(service.state)\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/AvVgY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AvVgY.png\" alt=\"enter image description here\" \/><\/a>\nPlease follow the below to Consume an Azure Machine Learning model deployed as a web service\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1597984478140,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63425902",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":50209284,
        "Question_title":"How to use the trained model developed in AZURE ML",
        "Question_body":"<p>I trained a model in AZURE ML. Now i want to use that model in my ios app to predict the output\u00a0.<\/p>\n\n<p>How to download the model from AZURE and use it my swift code.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1525678856837,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":516,
        "Owner_creation_time":1510206999777,
        "Owner_last_access_time":1630651881373,
        "Owner_location":null,
        "Owner_reputation":81,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":41,
        "Question_last_edit_time":1558224843256,
        "Answer_body":"<p>As far as I know, the model could run in <strong>Azure Machine Learning Studio<\/strong>.It seems that you are unable to download it, the model could do nothing outside of Azure ML. <\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/41236871\/how-to-download-the-trained-models-from-azure-machine-studio\">Here<\/a> is a similar post for you to refer, I have also tried @Ahmet's \nmethod, but result is like @mrjrdnthms says.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1525849618930,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1525850503192,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50209284",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":71075255,
        "Question_title":"AzureML: TabularDataset.to_pandas_dataframe() hangs when parquet file is empty",
        "Question_body":"<p>I have created a Tabular Dataset using Azure ML python API. Data under question is a bunch of parquet files (~10K parquet files each of size of 330 KB) residing in Azure Data Lake Gen 2 spread across multiple partitions. When I try to load the dataset using the API <code>TabularDataset.to_pandas_dataframe()<\/code>, it continues forever (hangs), if there are empty parquet files included in the Dataset. If the tabular dataset doesn't include those empty parquet files, <code>TabularDataset.to_pandas_dataframe()<\/code> completes within few minutes.<\/p>\n<p>By empty parquet file, I mean that the if I read the individual parquet file using pandas (pd.read_parquet()), it results in an empty DF (df.empty == True).<\/p>\n<p>I discovered the root cause while working on another issue mentioned <code>[here][1]<\/code>.<\/p>\n<p><strong>My question is how can make <code>TabularDataset.to_pandas_dataframe()<\/code> work even when there are empty parquet files?<\/strong><\/p>\n<p><strong>Update<\/strong>\nThe issue has been fixed in the following version:<\/p>\n<ul>\n<li>azureml-dataprep : 3.0.1<\/li>\n<li>azureml-core :  1.40.0<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644552863947,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":300,
        "Owner_creation_time":1280505139753,
        "Owner_last_access_time":1663935737867,
        "Owner_location":"Bangalore, India",
        "Owner_reputation":4265,
        "Owner_up_votes":315,
        "Owner_down_votes":11,
        "Owner_views":403,
        "Question_last_edit_time":1648643447772,
        "Answer_body":"<p>Thanks for reporting it.\nThis is a bug in handling of the parquet files with columns but empty row set. This has been fixed already and will be included in next release.<\/p>\n<p>I could not repro the hang on multiple files, though, so if you could provide more info on that would be nice.<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1646432724770,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71075255",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":40302499,
        "Question_title":"Recommendation API: what is the difference between null results and empty results",
        "Question_body":"<p>In the Azure Recommendation API sample there is a snippet like this:<\/p>\n\n<pre><code>     if (itemSets.RecommendedItemSetInfo != null)\n        {\n            ...\n        }\n        else\n        {\n            Console.WriteLine(\"No recommendations found.\");\n        }\n<\/code><\/pre>\n\n<p>So I assume that nullable recommended set means no recommendations. But what is the case with this set being not nullable but still empty ( as I am having it running the example)?<\/p>\n\n<p>I provided my own usages and catalog files. I have not too many entries there however for i2i recommendations I have results and for u2i there is an empty set.\nAllowColdItemPlacement doesn't change a think here.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1477648288960,
        "Question_score":0,
        "Question_tags":"azure|microsoft-cognitive|azure-machine-learning-studio",
        "Question_view_count":130,
        "Owner_creation_time":1354118434117,
        "Owner_last_access_time":1613750840957,
        "Owner_location":"Wroc\u0142aw, Poland",
        "Owner_reputation":393,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":43,
        "Question_last_edit_time":null,
        "Answer_body":"<p>We did not mean to convey a difference in meaning between null recommendations and empty recommendations. I will check why we may be sending two different types of results. Either way, don't treat those two cases as different cases. <\/p>\n\n<p>If you are not getting results for user-to-item recommendations, most likely there was no data for that user when the build was created or the items that the user interacted with do not have enough co-occurrences with other items in the usage.<\/p>\n\n<p>What to do when you get empty recommendations is up to you, you may decide to not show any recommendations, or back-fill with popular items you may want to promote.<\/p>\n\n<p>Thanks!<\/p>\n\n<p>Luis Cabrera\nProgram Manager - Recommendations API.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1478186851717,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40302499",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60975078,
        "Question_title":"How to only load one portion of an AzureML tabular dataset (linked to Azure Blob Storage)",
        "Question_body":"<p>I have a DataSet defined in my AzureML workspace that is linked to an Azure Blob Storage csv file of 1.6Gb.  This file contains timeseries information of around 10000 devices.  So, I could've also created 10000 smaller files (since I use ADF for the transmission pipeline).<\/p>\n\n<p>My question now is: is it possible to load a part of the AzureML DataSet in my python notebook or script instead of loading the entire file?<br>\nThe only code I have now load the full file:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>dataset = Dataset.get_by_name(workspace, name='devicetelemetry')\ndf = dataset.to_pandas_dataframe()\n<\/code><\/pre>\n\n<p>The only concept of partitions I found with regards to the AzureML datasets was around time series and partitioning of timestamps &amp; dates.  However, here I would love to partition per device, so I can very easily just do a load of all telemetry of a specific device.<\/p>\n\n<p>Any pointers to docs or any suggestions? (I couldn't find any so far)<\/p>\n\n<p>Thanks already<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1585756434767,
        "Question_score":1,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":560,
        "Owner_creation_time":1360655430743,
        "Owner_last_access_time":1663784892907,
        "Owner_location":"Belgium",
        "Owner_reputation":2947,
        "Owner_up_votes":297,
        "Owner_down_votes":16,
        "Owner_views":355,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You're right there are the <code>.time_*()<\/code> filtering methods available with a <code>TabularDataset<\/code>.<\/p>\n\n<p>I'm not aware of anyway to do filtering as you suggest (but I agree it would be a useful feature). To get per-device partitioning, my recommendation would be to structure your container like so:<\/p>\n\n<pre><code>- device1\n    - 2020\n        - 2020-03-31.csv\n        - 2020-04-01.csv\n- device2\n   - 2020\n        - 2020-03-31.csv\n        - 2020-04-01.csv\n<\/code><\/pre>\n\n<p>In this way you can define an all-up Dataset, but also per-device Datasets by passing folder of the device to the DataPath<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># all up dataset\nds_all = Dataset.Tabular.from_delimited_files(\n    path=DataPath(datastore, '*')\n)\n# device 1 dataset\nds_d1 = Dataset.Tabular.from_delimited_files(\n    path=DataPath(datastore, 'device1\/*')\n)\n<\/code><\/pre>\n\n<p><strong>CAVEAT<\/strong><\/p>\n\n<p>dataprep SDK is optimized for blobs around 200MB in size. So you can work with many small files, but sometimes it can be slower than expected, especially considering the overhead of enumerating all blobs in a container.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1585761136673,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60975078",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":56848293,
        "Question_title":"What is Random seed in Azure Machine Learning?",
        "Question_body":"<p>I am learning Azure Machine Learning. I am frequently encountering the <strong>Random Seed<\/strong> in some of the steps like,<\/p>\n\n<ol>\n<li>Split Data<\/li>\n<li>Untrained algorithm models as Two Class Regression, Multi-class regression, Tree, Forest,..<\/li>\n<\/ol>\n\n<p>In the tutorial, they choose Random Seed as '123'; trained model has high accuracy but when I try to choose other random integers like 245, 256, 12, 321,.. it did not do well.<\/p>\n\n<hr>\n\n<p><strong>Questions<\/strong><\/p>\n\n<ul>\n<li>What is a Random Seed Integer?<\/li>\n<li>How to carefully choose a Random Seed from range of integer values? What is the key or strategy to choose it?<\/li>\n<li>Why does Random Seed significantly affect the ML Scoring, Prediction and Quality of the trained model?<\/li>\n<\/ul>\n\n<hr>\n\n<p><strong>Pretext<\/strong><\/p>\n\n<ol>\n<li>I have <a href=\"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/iris\/iris.data\" rel=\"nofollow noreferrer\">Iris-Sepal-Petal-Dataset<\/a> with Sepal (<em>Length &amp; Width<\/em>) and Petal (<em>Length &amp; Width<\/em>)<\/li>\n<li>Last column in data-set is 'Binomial ClassName'<\/li>\n<li>I am training the data-set with Multiclass Decision Forest Algorithm and splitting the data with different random seeds 321, 123 and 12345 in order<\/li>\n<li>It affects the final quality of trained model. Random seed#123 being best of Prediction probability score: 1.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/12OyD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/12OyD.png\" alt=\"ML Studio Snap\"><\/a><\/p>\n\n<hr>\n\n<p><strong>Observations<\/strong><\/p>\n\n<p><strong>1. Random seed: 321<\/strong><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YcsiS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YcsiS.png\" alt=\"Random-seed-321\"><\/a><\/p>\n\n<p><strong>2. Random seed: 123<\/strong><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Qrk4G.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Qrk4G.png\" alt=\"Random-seed-123\"><\/a><\/p>\n\n<p><strong>3. Random seed: 12345<\/strong><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/D1Rki.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/D1Rki.png\" alt=\"Random-seed-12345\"><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":9,
        "Question_creation_time":1562056038867,
        "Question_score":0,
        "Question_tags":"machine-learning|azure-machine-learning-studio|random-seed|iris-dataset",
        "Question_view_count":2046,
        "Owner_creation_time":1475691108437,
        "Owner_last_access_time":1658651167790,
        "Owner_location":"India",
        "Owner_reputation":1849,
        "Owner_up_votes":366,
        "Owner_down_votes":21,
        "Owner_views":253,
        "Question_last_edit_time":1562066943383,
        "Answer_body":"<blockquote>\n  <p>What is a Random Seed Integer?<\/p>\n<\/blockquote>\n\n<p>Will not go into any details regarding what a random seed is in general; there is plenty of material available by a simple web search (see for example <a href=\"https:\/\/stackoverflow.com\/questions\/22639587\/random-seed-what-does-it-do\">this SO thread<\/a>).<\/p>\n\n<p>Random seed serves just to initialize the (pseudo)random number generator, mainly in order to make ML examples reproducible.<\/p>\n\n<blockquote>\n  <p>How to carefully choose a Random Seed from range of integer values? What is the key or strategy to choose it?<\/p>\n<\/blockquote>\n\n<p>Arguably this is already answered implicitly above: you are simply not supposed to choose any particular random seed, and your results should be roughly the same across different random seeds.<\/p>\n\n<blockquote>\n  <p>Why does Random Seed significantly affect the ML Scoring, Prediction and Quality of the trained model?<\/p>\n<\/blockquote>\n\n<p>Now, to the heart of your question. The answer <em>here<\/em> (i.e. with the iris dataset) is the <strong>small-sample effects<\/strong>...<\/p>\n\n<p>To start with, your reported results across different random seeds are not <em>that<\/em> different. Nevertheless, I agree that, at first sight, a difference in macro-average precision of 0.9 and 0.94 might <em>seem<\/em> large; but looking more closely it is revealed that the difference is really not an issue. Why?<\/p>\n\n<p>Using the 20% of your (only) 150-samples dataset leaves you with only 30 samples in your test set (where the evaluation is performed); this is stratified, i.e. about 10 samples from each class. Now, for datasets of <em>that<\/em> small size, it is not difficult to imagine that a difference in the correct classification of <strong>only 1-2<\/strong> samples can have this apparent difference in the performance metrics reported.<\/p>\n\n<p>Let's try to verify this in scikit-learn using a decision tree classifier (the essence of the issue does not depend on the specific framework or the ML algorithm used):<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>from sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_iris(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=321, stratify=y)\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n<\/code><\/pre>\n\n<p>Result:<\/p>\n\n<pre><code>[[10  0  0]\n [ 0  9  1]\n [ 0  0 10]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       1.00      0.90      0.95        10\n           2       0.91      1.00      0.95        10\n\n   micro avg       0.97      0.97      0.97        30\n   macro avg       0.97      0.97      0.97        30\nweighted avg       0.97      0.97      0.97        30\n<\/code><\/pre>\n\n<p>Let's repeat the code above, changing only the <code>random_state<\/code> argument in <code>train_test_split<\/code>; for <code>random_state=123<\/code> we get:<\/p>\n\n<pre><code>[[10  0  0]\n [ 0  7  3]\n [ 0  2  8]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       0.78      0.70      0.74        10\n           2       0.73      0.80      0.76        10\n\n   micro avg       0.83      0.83      0.83        30\n   macro avg       0.84      0.83      0.83        30\nweighted avg       0.84      0.83      0.83        30\n<\/code><\/pre>\n\n<p>while for <code>random_state=12345<\/code> we get:<\/p>\n\n<pre><code>[[10  0  0]\n [ 0  8  2]\n [ 0  0 10]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       1.00      0.80      0.89        10\n           2       0.83      1.00      0.91        10\n\n   micro avg       0.93      0.93      0.93        30\n   macro avg       0.94      0.93      0.93        30\nweighted avg       0.94      0.93      0.93        30\n<\/code><\/pre>\n\n<p>Looking at the <em>absolute numbers<\/em> of the 3 confusion matrices (in <em>small samples<\/em>, percentages can be <strong>misleading<\/strong>), you should be able to convince yourself that the differences are not that big, and they can be arguably justified by the random element inherent in the whole procedure (here the exact split of the dataset into training and test).<\/p>\n\n<p>Should your test set be significantly bigger, these discrepancies would be practically negligible... <\/p>\n\n<p>A last notice; I have used the exact same seed numbers as you, but this does not actually mean anything, as in general the random number generators <em>across<\/em> platforms &amp; languages are not the same, hence the corresponding seeds are not actually compatible. See own answer in <a href=\"https:\/\/stackoverflow.com\/questions\/52293899\/are-random-seeds-compatible-between-systems\">Are random seeds compatible between systems?<\/a> for a demonstration.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1562069850057,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1562070151169,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56848293",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72133111,
        "Question_title":"Azure ML: What means reconnecting terminal?",
        "Question_body":"<p>I am a newbie in this, and I am facing some problems with the Azure ML workspace. I ran a python code from the terminal, and then I opened another terminal to check the process. I got the following message in the terminal that checked the process:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9XLPw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9XLPw.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What does this mean? It keeps running, but I don't know if it is a bad message. It takes soo long, and I don't want to lose the processing time.<\/p>\n<p>I appreciate any tips.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651781550743,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-service|azure-python-sdk",
        "Question_view_count":48,
        "Owner_creation_time":1575137776887,
        "Owner_last_access_time":1663955477980,
        "Owner_location":null,
        "Owner_reputation":45,
        "Owner_up_votes":21,
        "Owner_down_votes":0,
        "Owner_views":21,
        "Question_last_edit_time":null,
        "Answer_body":"<blockquote>\n<p>What does this mean? It keeps running, but I don't know if it is a bad message. It takes soo long, and I don't want to lose the processing time.<\/p>\n<\/blockquote>\n<ul>\n<li><code>Reconnecting terminal<\/code> message can appear for multiple reasons like intermittent connectivity issues, unused active terminal sessions, processing of different size\/format of data.<\/li>\n<li>Make sure you close any unused terminal sessions to preserve your compute instance's resources. Idle terminals may impact the performance of compute instances.<\/li>\n<\/ul>\n<p>You can refer to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#manage-terminal-sessions\" rel=\"nofollow noreferrer\">Access a compute instance terminal in your workspace<\/a>, <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-optimize-data-processing\" rel=\"nofollow noreferrer\">Optimize data processing with Azure Machine Learning<\/a> and <a href=\"https:\/\/www.youtube.com\/watch?v=kiScfw9i4FM\" rel=\"nofollow noreferrer\">Azure ML: Speed up processing time<\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1651825622423,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72133111",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":34948242,
        "Question_title":"Azure: plot without labels",
        "Question_body":"<p>Suppose I have a dataframe (myDataframe) with two column of values (third and fourth). I want to plot them in a bi-dimensional graph. If I do it in R it works, but it returns me a graph without labels when I run the script from Azure Machine Learning. Someone with ideas?<\/p>\n\n<pre><code>...\nplot(myDataframe[,3],myDataframe[,4], \n       main=\"my title\",\n       xlab= \"x\"\n       ylab= \"y\",\n       col= \"blue\", pch = 19, cex = 0.1, lty = \"solid\", lwd = 2)\n\n# lines(x,y=x, col=\"yellow\")\n\n# add LABELS\ntext(DF_relativo[,A], DF_relativo[,B], \n       labels=DF_relativo$names, cex= 0.7, pos=2)\n...\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1453470518060,
        "Question_score":0,
        "Question_tags":"r|plot|label|azure-machine-learning-studio",
        "Question_view_count":91,
        "Owner_creation_time":1436432728610,
        "Owner_last_access_time":1663607665487,
        "Owner_location":"Colleferro, Italy",
        "Owner_reputation":809,
        "Owner_up_votes":109,
        "Owner_down_votes":0,
        "Owner_views":361,
        "Question_last_edit_time":null,
        "Answer_body":"<p>using ggplot2() in AzureML is bit different. It can use to have plots with labels. Here's the <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/b1c26728eb6c4e4d80dddceae992d653\" rel=\"nofollow\">Cortana Intelligence gallery example<\/a> for the particular task.  <\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1466402236047,
        "Answer_score":-1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34948242",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60306240,
        "Question_title":"export azure ml studio designer project as jupyter notebook?",
        "Question_body":"<p>I hope I am not missing something obvious here. I am using the new azure ml studio designer. I am able to use to create datasets, train models and use them just fine.<\/p>\n\n<p>azure ml studio allows creation of Jupyter notebooks (also) and use them to do machine learning. I am able to do that too. <\/p>\n\n<p>So, now, I am wondering, can I build my ML pipeline\/experiment in ML studio designer, and once it is in good shape, export it as a python and jupyter notebook? then, use it in the same designer provided notebook option or may be use it locally?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":4,
        "Question_creation_time":1582134046887,
        "Question_score":5,
        "Question_tags":"azure|jupyter-notebook|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":789,
        "Owner_creation_time":1442334437953,
        "Owner_last_access_time":1664002198907,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":2272,
        "Owner_up_votes":1340,
        "Owner_down_votes":67,
        "Owner_views":516,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This is not currently supported, but I am 80% sure it is in the roadmap.\nAn alternative would be to use the SDK to create the same pipeline using <code>ModuleStep<\/code> where  I <em>believe<\/em> you can reference a Designer Module by its name to use it like a <code>PythonScriptStep<\/code><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1582139574710,
        "Answer_score":6,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1582241388596,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60306240",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70156610,
        "Question_title":"Accessing an Azure ML Model Registry from another Azure ML Workspace",
        "Question_body":"<p>Suppose I have two Azure ML workspaces:<\/p>\n<ol>\n<li><p>Workspace1 - This is being used by one team (Team1) who only train the model and store the model in model registry of Workspace1<\/p>\n<\/li>\n<li><p>Workspace2 - This is used by another team  (Team2) who containerise the model, push it to ACR and then deploy the containerised model in Azure ML Compute.<\/p>\n<\/li>\n<\/ol>\n<p>Is it possible for Team2 to access the model registry of Workspace1 from their Workspace2 and retrieve the model for containerisation and subsequent deployment? Alternatively, is there any concept of a shared model registry in Azure ML where both the teams can store and access a common model registry? If none of these are possible, then what is the way for Team1 and Team2 to work together on a single model with the given responsibilities as described above?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638197253147,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":421,
        "Owner_creation_time":1373471094267,
        "Owner_last_access_time":1663943798093,
        "Owner_location":"United Kingdom",
        "Owner_reputation":395,
        "Owner_up_votes":11,
        "Owner_down_votes":3,
        "Owner_views":66,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As described, I think the best solution is to use one Workspace, not two.  It sounds like you have Team 1 and Team 2 sharing contributions on a single project.  What may work better is to define user roles in the Azure ML workspace, such that Team 2 has permissions to deploy models, and Team 1 has permission to create models.<\/p>\n<p>Otherwise you can always write Python code using the ML SDK to connect to any workspace given you know the subscription, resource group, workspace name etc.<\/p>\n<pre><code>from azure.core import Workspace, Model\n\n# connect to an existing workspace\nname = 'WorkspaceName'\nsub = 'subscriptionName'\nresource_group = 'resourceGroupName'\nws = Workspace.get(name=name, subscription_id=sub, resource_group=resource_group) \n\n# retrieve existing model\nmodel = Model(ws, name='your model name')\n<\/code><\/pre>",
        "Answer_comment_count":3,
        "Answer_creation_time":1638210489530,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1638220015580,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70156610",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64979752,
        "Question_title":"Unable to access python packages installed in Azure ML",
        "Question_body":"<p>I am trying to deploy a pre-trained ML model (saved as .h5 file) to Azure ML. I have created an AKS cluster and trying to deploy the model as shown below:<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.model import Model\n\nfrom azureml.core.environment import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.model import InferenceConfig\n\nfrom azureml.core.webservice import AksWebservice, LocalWebservice\nfrom azureml.core.compute import ComputeTarget\n\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\n\nenv = Environment.get(workspace, name='AzureML-TensorFlow-1.13-GPU')\n\n# Installing packages present in my requirements file\nwith open('requirements.txt') as f:\n    dependencies = f.readlines()\ndependencies = [x.strip() for x in dependencies if '# ' not in x]\ndependencies.append(&quot;azureml-defaults&gt;=1.0.45&quot;)\n\nenv.python.conda_dependencies = CondaDependencies.create(conda_packages=dependencies)\n\n# Including the source folder so that all helper scripts are included in my deployment\ninference_config = InferenceConfig(entry_script='app.py', environment=env, source_directory='.\/ProcessImage')\n\naks_target = ComputeTarget(workspace=workspace, name='sketch-ppt-vm')\n\n# Deployment with suitable config\ndeployment_config = AksWebservice.deploy_configuration(cpu_cores=4, memory_gb=32)\nmodel = Model(workspace, 'sketch-inference')\nservice = Model.deploy(workspace, &quot;process-sketch-dev&quot;, [model], inference_config, deployment_config, deployment_target=aks_target, overwrite=True)\nservice.wait_for_deployment(show_output = True)\nprint(service.state)\n<\/code><\/pre>\n<p>My main entry script requires some additional helper scripts, which I include by mentioning the source folder in my inference config.<\/p>\n<p>I was expecting that the helper scripts I add should be able to access the packages installed while setting up the environment during deployment, but I get ModuleNotFoundError.<\/p>\n<p>Here is the error output, along with the a couple of environment variables I printed while executing entry script:<\/p>\n<pre><code>    AZUREML_MODEL_DIR ----  azureml-models\/sketch-inference\/1\n    PYTHONPATH ----  \/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages:\/var\/azureml-server:\n    PATH ----  \/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/bin:\/opt\/miniconda\/bin:\/usr\/local\/nvidia\/bin:\/usr\/local\/cuda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin:\/opt\/intel\/compilers_and_libraries\/linux\/mpi\/bin64\n    Exception in worker process\n    Traceback (most recent call last):\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n        worker.init_process()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 129, in init_process\n        self.load_wsgi()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 138, in load_wsgi\n        self.wsgi = self.app.wsgi()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n        self.callable = self.load()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 52, in load\n        return self.load_wsgiapp()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 41, in load_wsgiapp\n        return util.import_app(self.app_uri)\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/util.py&quot;, line 350, in import_app\n        __import__(module)\n    File &quot;\/var\/azureml-server\/wsgi.py&quot;, line 1, in &lt;module&gt;\n        import create_app\n    File &quot;\/var\/azureml-server\/create_app.py&quot;, line 3, in &lt;module&gt;\n        from app import main\n    File &quot;\/var\/azureml-server\/app.py&quot;, line 32, in &lt;module&gt;\n        from aml_blueprint import AMLBlueprint\n    File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 25, in &lt;module&gt;\n        import main\n    File &quot;\/var\/azureml-app\/main.py&quot;, line 12, in &lt;module&gt;\n        driver_module_spec.loader.exec_module(driver_module)\n    File &quot;\/structure\/azureml-app\/ProcessImage\/app.py&quot;, line 16, in &lt;module&gt;\n        from ProcessImage.samples.coco.inference import run as infer\n    File &quot;\/var\/azureml-app\/ProcessImage\/samples\/coco\/inference.py&quot;, line 1, in &lt;module&gt;\n        import skimage.io\n    ModuleNotFoundError: No module named 'skimage'\n<\/code><\/pre>\n<p>The existing answers related to this aren't of much help. I believe there must be a simpler way to fix this, since AzureML specifically provides the feature to setup environment with pip\/conda packages installed either by supplying requirements.txt file or individually.<\/p>\n<p>What am I missing here? Kindly help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1606187763027,
        "Question_score":3,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":1381,
        "Owner_creation_time":1429811498813,
        "Owner_last_access_time":1663968181117,
        "Owner_location":"Boston, MA, USA",
        "Owner_reputation":1910,
        "Owner_up_votes":85,
        "Owner_down_votes":6,
        "Owner_views":316,
        "Question_last_edit_time":null,
        "Answer_body":"<p>So, after some trial and error, creating a fresh environment and then adding the packages solved the problem for me. I am still not clear on why this didn't work when I tried to use <a href=\"http:\/\/from%20azureml.core%20import%20Workspace%20from%20azureml.core.model%20import%20Model%20from%20azureml.core.environment%20import%20Environment,%20DEFAULT_GPU_IMAGE%20from%20azureml.core.conda_dependencies%20import%20CondaDependencies%20from%20azureml.core.model%20import%20InferenceConfig%20from%20azureml.core.webservice%20import%20AksWebservice,%20LocalWebservice%20from%20azureml.core.compute%20import%20ComputeTarget%20%20%20#%201.%20Instantiate%20the%20workspace%20workspace%20=%20Workspace.from_config(path=%22config.json%22)%20%20#%202.%20Setup%20the%20environment%20env%20=%20Environment(%27sketchenv%27)%20with%20open(%27requirements.txt%27)%20as%20f:%20#%20Fetch%20all%20dependencies%20as%20a%20list%20%20%20%20%20dependencies%20=%20f.readlines()%20dependencies%20=%20%5Bx.strip()%20for%20x%20in%20dependencies%20if%20%27#%20%27%20not%20in%20x%5D%20env.docker.base_image%20=%20DEFAULT_GPU_IMAGE%20env.python.conda_dependencies%20=%20CondaDependencies.create(conda_packages=%5B%27numpy==1.17.4%27,%20%27Cython%27%5D,%20pip_packages=dependencies)%20%20#%203.%20Inference%20Config%20inference_config%20=%20InferenceConfig(entry_script=%27app.py%27,%20environment=env,%20source_directory=%27.\/ProcessImage%27)%20%20#%204.%20Compute%20target%20(using%20existing%20cluster%20from%20the%20workspacke)%20aks_target%20=%20ComputeTarget(workspace=workspace,%20name=%27sketch-ppt-vm%27)%20%20#%205.%20Deployment%20config%20deployment_config%20=%20AksWebservice.deploy_configuration(cpu_cores=6,%20memory_gb=100)%20%20#%206.%20Model%20deployment%20model%20=%20Model(workspace,%20%27sketch-inference%27)%20#%20Registered%20model%20(which%20contains%20model%20files\/folders)%20service%20=%20Model.deploy(workspace,%20%22process-sketch-dev%22,%20%5Bmodel%5D,%20inference_config,%20deployment_config,%20deployment_target=aks_target,%20overwrite=True)%20service.wait_for_deployment(show_output%20=%20True)%20print(service.state)\" rel=\"nofollow noreferrer\">Environment.from_pip_requirements()<\/a>. A detailed answer in this regard would be interesting to read.<\/p>\n<p>My primary task was inference - object detection given an image, and we have our own model developed by our team. There are two types of imports I wanted to have:<\/p>\n<p><strong>1. Standard python packages (installed through pip)<\/strong><br \/>\nThis was solved by creating conda dependencies and add it to env object (Step 2)<\/p>\n<p><strong>2. Methods\/vars from helper scripts<\/strong> (if you have pre\/post processing to be done during model inference):<br \/>\nThis was done by mentioning <code>source_directory<\/code> in InferenceConfig (step 3)<\/p>\n<p>Here is my updated script which combines Environment creation, Inference and Deployment configs and using existing compute in the workspace (created through portal).<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.environment import Environment, DEFAULT_GPU_IMAGE\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.webservice import AksWebservice, LocalWebservice\nfrom azureml.core.compute import ComputeTarget\n\n\n# 1. Instantiate the workspace\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\n\n# 2. Setup the environment\nenv = Environment('sketchenv')\nwith open('requirements.txt') as f: # Fetch all dependencies as a list\n    dependencies = f.readlines()\ndependencies = [x.strip() for x in dependencies if '# ' not in x]\nenv.docker.base_image = DEFAULT_GPU_IMAGE\nenv.python.conda_dependencies = CondaDependencies.create(conda_packages=['numpy==1.17.4', 'Cython'], pip_packages=dependencies)\n\n# 3. Inference Config\ninference_config = InferenceConfig(entry_script='app.py', environment=env, source_directory='.\/ProcessImage')\n\n# 4. Compute target (using existing cluster from the workspacke)\naks_target = ComputeTarget(workspace=workspace, name='sketch-ppt-vm')\n\n# 5. Deployment config\ndeployment_config = AksWebservice.deploy_configuration(cpu_cores=6, memory_gb=100)\n\n# 6. Model deployment\nmodel = Model(workspace, 'sketch-inference') # Registered model (which contains model files\/folders)\nservice = Model.deploy(workspace, &quot;process-sketch-dev&quot;, [model], inference_config, deployment_config, deployment_target=aks_target, overwrite=True)\nservice.wait_for_deployment(show_output = True)\nprint(service.state)\n<\/code><\/pre>\n<hr \/>",
        "Answer_comment_count":0,
        "Answer_creation_time":1606249620653,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64979752",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72035391,
        "Question_title":"AzureML schema \"list index out of range\" error",
        "Question_body":"<p>I developed a machine learning model using Azure ML's clustering. Few of the requests made from the cluster are triggering 404 HTTP error. I followed the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">document<\/a> to do modifications in my swagger.json file. Finally ended up with &quot;list index out of range&quot; error. It seems to be having issue with the global parameter but I am no sure about it. I am using the API from postman with some default headers like mentioned in the body below<\/p>\n<pre><code>{\n    &quot;Inputs&quot;: {\n         &quot;input_1&quot; : &quot;content&quot;\n         &quot;input_2: : &quot;content&quot;\n         ......\n    },\n    &quot;GlobalParameters&quot;: 0\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651094225790,
        "Question_score":1,
        "Question_tags":"python|azure|azure-machine-learning-service",
        "Question_view_count":78,
        "Owner_creation_time":1651093614703,
        "Owner_last_access_time":1659335138797,
        "Owner_location":"Netherland",
        "Owner_reputation":19,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":1651098753480,
        "Answer_body":"<p>Change the &quot;GlobalParameter&quot; value to any floating number other than 1.0 or even you can remove it and execute. Sometimes, Global parameter will cause the issue. Check the below documentation.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/746784\/azure-ml-studio-error-while-testing-real-time-endp.html\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/746784\/azure-ml-studio-error-while-testing-real-time-endp.html<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1651111084097,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72035391",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":43278593,
        "Question_title":"evaluating linear regression (in microsoft machine learning",
        "Question_body":"<p>Im playing with linear regression in azure machine learning and evaluating a model. <\/p>\n\n<p>Im still a bit unsure what the various metrics for evaluation mean and show, so would appreciate some correction if i am incorrect.<\/p>\n\n<ol>\n<li><strong>Mean Absolute Error:<\/strong> Mean of the residuals (errors).<\/li>\n<li><strong>Root Mean Squared Error:<\/strong> Std Dev of the residuals. With this i can see how far from the mean\/median my absolute error is.<\/li>\n<li><strong>Relative absolute error<\/strong>: a percentage value that shows the percentage difference between relative error and absolute error. lower values are better, indicating lower difference.<\/li>\n<li><strong>relative squared error:<\/strong> square of the error relative to the square of the absolute. Unsure what this gives me over the relative absolute error.<\/li>\n<li><strong>coefficient of determination:<\/strong> indication of correlation between inputs. +1 or -1 indicate perfect correlation, 0 indicates none.<\/li>\n<li>The histogram is showing the frequency of various buckets of error magnitudes. this shows a lot of small errors. with frequency decreasing as the value of error increases, indicating, when taken along with the poor metrics above that there are probably some sku or outliers having a large influence on the model, making it less accurate.<\/li>\n<\/ol>\n\n<p>Are these definitions and assumptions correct?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/dJqJr.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/dJqJr.jpg\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1491569249220,
        "Question_score":0,
        "Question_tags":"r|machine-learning|statistics|azure-machine-learning-studio",
        "Question_view_count":289,
        "Owner_creation_time":1447682287793,
        "Owner_last_access_time":1542307482427,
        "Owner_location":"Nairobi, Kenya",
        "Owner_reputation":194,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Question_last_edit_time":1491571206763,
        "Answer_body":"<p>You are almost correct on most points. To make sure we are talking in the same terms, a little bit of background:<\/p>\n\n<p>A linear regression uses data on some outcome variable <code>y<\/code> and independent variables <code>x1, x2, ..<\/code> and tries to find the linear combination of <code>x1, x2, ..<\/code> that best predicts <code>y<\/code>. Once this \"best linear combination\" is established, you can assess the quality of the fit (i.e. quality of the model) in multiple ways. The six points you mention are all key metrics for the quality of a regression equation. <\/p>\n\n<p>Running a regression gives you multiple \"ingredients\". For example, every observation will get a <em>predicted value<\/em> for the outcome variable. The difference between the observed value of <code>y<\/code> and the predicted value is called the residual or error. Residuals can be negative (if the <code>y<\/code> is overestimated) and positive (if <code>y<\/code> is underestimated). The closer the residuals are to zero, the better. But, what is \"close\"? The metrics you present are supposed to give an insight in this.<\/p>\n\n<ul>\n<li><strong>Mean absolute error<\/strong>: takes the <em>absolute value<\/em>  of the residuals and takes the mean of that. <\/li>\n<li><strong>Root Mean Square Error<\/strong>: is the standard deviation of your residuals. This will help you see, how large the <em>spread<\/em>  is of your residuals. The residuals are squared and therefore, high residuals will count in more than small residuals. A low RMSE is good. <\/li>\n<li><p><strong>Relative Absolute Error<\/strong>: The absolute error as a fraction of the real value of the outcome variable <code>y<\/code>. In your case, the predictions are on average 75% higher\/lower than the actual value of <code>y<\/code>.<\/p><\/li>\n<li><p><strong>Relative Squared Error<\/strong>: The squared error (<code>residual^2<\/code>) as a fraction of the real value. <\/p><\/li>\n<li><strong>Coefficient of Determination<\/strong>: Almost correct. This ranges between 0 and 1 and can be interpreted as the explanatory power of the independent variables in explaining <code>y<\/code>. In fact, in your case the independent variables can model 38,15% of the variation in <code>y<\/code>.  Also, if you have only one independent variable, this coefficient is equal to the squared correlation coefficient. <\/li>\n<\/ul>\n\n<p>Root Mean Squared Error and Coefficient of Determination are the most important metrics in nearly all situations. To be honest, I've never really seen the other metrics being reported.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1491575868100,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1491576306820,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/43278593",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":48197524,
        "Question_title":"How to remove the entire rows if value is NULL in Azure ML studio",
        "Question_body":"<p>I am preparing the data for regression model. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/LFaYl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LFaYl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I want to remove the entire row If all columns have value <code>NULL<\/code>. <\/p>\n\n<p>With Clean Missing Data module seems to me like I only able to remove missing values. But <code>NULL<\/code> is not considers mission value. <\/p>\n\n<p>So are there any other modules that simply can remove the entire row if all values are <code>NULL<\/code>'s<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1515625426463,
        "Question_score":0,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio",
        "Question_view_count":1477,
        "Owner_creation_time":1457596845393,
        "Owner_last_access_time":1663977598457,
        "Owner_location":"San Diego, CA, United States",
        "Owner_reputation":4046,
        "Owner_up_votes":505,
        "Owner_down_votes":7,
        "Owner_views":825,
        "Question_last_edit_time":null,
        "Answer_body":"<p>you could use \"<strong>Execute Python Script<\/strong>\" or \"<strong>Execute R Script<\/strong>\" to archive that. Or just use \"<strong>Apply SQL Transformation<\/strong>\" -> <code>SELECT * FROM tbl1 where column1 IS NULL AND column2 IS NULL<\/code>.... <\/p>\n\n<p>Greetings,\nStefan<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1516614808533,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1526560111352,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48197524",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":70976353,
        "Question_title":"After installing scrubadub_spacy package, spacy.load(\"en_core_web_sm\") not working OSError: [E053] Could not read config.cfg",
        "Question_body":"<p>I am getting the below error when I'm trying to run the following line of code to load en_core_web_sm in the Azure Machine Learning instance.<\/p>\n<p>I debugged the issue and found out that once I install scrubadub_spacy, that seems is the issue causing the error.<\/p>\n<pre><code>spacy.load(&quot;en_core_web_sm&quot;)\n<\/code><\/pre>\n<pre><code>OSError                                   Traceback (most recent call last)\n&lt;ipython-input-2-c6e652d70518&gt; in &lt;module&gt;\n     1 # Load English tokenizer, tagger, parser and NER\n----&gt; 2 nlp = spacy.load(&quot;en_core_web_sm&quot;)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/spacy\/__init__.py in load(name, vocab, disable, exclude, config)\n    50     &quot;&quot;&quot;\n    51     return util.load_model(\n---&gt; 52         name, vocab=vocab, disable=disable, exclude=exclude, config=config\n    53     )\n    54 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/spacy\/util.py in load_model(name, vocab, disable, exclude, config)\n   418             return get_lang_class(name.replace(&quot;blank:&quot;, &quot;&quot;))()\n   419         if is_package(name):  # installed as package\n--&gt; 420             return load_model_from_package(name, **kwargs)  # type: ignore[arg-type]\n   421         if Path(name).exists():  # path to model data directory\n   422             return load_model_from_path(Path(name), **kwargs)  # type: ignore[arg-type]\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/spacy\/util.py in load_model_from_package(name, vocab, disable, exclude, config)\n   451     &quot;&quot;&quot;\n   452     cls = importlib.import_module(name)\n--&gt; 453     return cls.load(vocab=vocab, disable=disable, exclude=exclude, config=config)  # type: ignore[attr-defined]\n   454 \n   455 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/en_core_web_sm\/__init__.py in load(**overrides)\n    10 \n    11 def load(**overrides):\n---&gt; 12     return load_model_from_init_py(__file__, **overrides)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/spacy\/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config)\n   619         disable=disable,\n   620         exclude=exclude,\n--&gt; 621         config=config,\n   622     )\n   623 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/spacy\/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config)\n   485     config_path = model_path \/ &quot;config.cfg&quot;\n   486     overrides = dict_to_dot(config)\n--&gt; 487     config = load_config(config_path, overrides=overrides)\n   488     nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude)\n   489     return nlp.from_disk(model_path, exclude=exclude, overrides=overrides)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/spacy\/util.py in load_config(path, overrides, interpolate)\n   644     else:\n   645         if not config_path or not config_path.exists() or not config_path.is_file():\n--&gt; 646             raise IOError(Errors.E053.format(path=config_path, name=&quot;config.cfg&quot;))\n   647         return config.from_disk(\n   648             config_path, overrides=overrides, interpolate=interpolate\n\nOSError: [E053] Could not read config.cfg from \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/en_core_web_sm\/en_core_web_sm-2.3.1\/config.cfg\n<\/code><\/pre>\n<p>I installed the packages using the below three lines codes from <a href=\"https:\/\/spacy.io\/usage\" rel=\"nofollow noreferrer\">Spacy<\/a><\/p>\n<pre><code>pip install -U pip setuptools wheel\npip install -U spacy\npython -m spacy download en_core_web_sm\n<\/code><\/pre>\n<p>How should I fix this issue? thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643912383590,
        "Question_score":2,
        "Question_tags":"python|python-3.6|spacy|azure-machine-learning-service|oserror",
        "Question_view_count":201,
        "Owner_creation_time":1573605534107,
        "Owner_last_access_time":1656003381830,
        "Owner_location":"Saint Louis, MO, USA",
        "Owner_reputation":25,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1643998339103,
        "Answer_body":"<p>Taking the path from your error message:<\/p>\n<pre><code>en_core_web_sm-2.3.1\/config.cfg\n<\/code><\/pre>\n<p>You have a model for v2.3, but it's looking for a <code>config.cfg<\/code>, which is only a thing in v3 of spaCy. It looks like you upgraded spaCy without realizing it.<\/p>\n<p>There are two ways to fix this. One is to reinstall the model with <code>spacy download<\/code>, which will get a version that matches your current spaCy version. If you are just starting something that is probably the best idea. Based on the release date of scrubadub, it seems to be intended for use with spaCy v3.<\/p>\n<p>However, note that v2 and v3 are pretty different - if you have a project with v2 of spaCy you might want to downgrade instead.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1644122814723,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70976353",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":40028165,
        "Question_title":"Azure ML's web service asking for label?",
        "Question_body":"<p>I built a linear regression algorithm in Azure ML. On the &quot;Score Model&quot; module I can actually see the predictions and the rest of the features. However, when I deploy this project as a web service, the service is expecting the actual label of the data (e.g. I'm trying to predict a house's price and it asks me for the price of the house to make the prediction), which doesn't make any sense to me... What am I doing wrong? On the &quot;Train Model&quot; module I set that the label column is the HousePrice, which is what I'm trying to predict...<\/p>\n<p>This is my model:\n<a href=\"https:\/\/i.stack.imgur.com\/kI8qu.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kI8qu.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I tried leaving that field blank but the prediction returns null...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1476382562787,
        "Question_score":4,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":1012,
        "Owner_creation_time":1401729936860,
        "Owner_last_access_time":1662997148480,
        "Owner_location":null,
        "Owner_reputation":1102,
        "Owner_up_votes":390,
        "Owner_down_votes":25,
        "Owner_views":120,
        "Question_last_edit_time":1592644375060,
        "Answer_body":"<p>The input schema (names\/types of required input) based on the location in the graph where you attach the \"Web Service Input\" module. To get the schema you want, you will need to find -- or if necessary, create -- a place in the experiment where the data has the column names\/types you desire.<\/p>\n\n<p>Consider this simple example experiment that predicts whether a field called \"income\" will be above or below $50k\/year:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/nWaN2.png\" rel=\"nofollow\"><img src=\"https:\/\/i.stack.imgur.com\/nWaN2.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When we click \"Set up web service\", the following graph is automatically generated:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/NMMpV.png\" rel=\"nofollow\"><img src=\"https:\/\/i.stack.imgur.com\/NMMpV.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Since the input dataset and \"Web service input\" modules are connected to the same port, the web service schema will perfectly match the schema of the input dataset. This is unfortunate because the input dataset contains a column called \"income\", which is what our web service is supposed to predict -- this is equivalent to the problem that you are having.<\/p>\n\n<p>To get around it, we need to create a place in our experiment graph where we've dropped the unneeded \"income\" field from the input dataset, and attach the \"Web service input\" module there:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WPeSB.png\" rel=\"nofollow\"><img src=\"https:\/\/i.stack.imgur.com\/WPeSB.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>With this arrangement, the web service only requests the features actually needed to score the model. I'm sure you can use a similar method to create a predictive experiment with whatever input schema you need for your own work.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1476730527013,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40028165",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":66704314,
        "Question_title":"How to get Model ID of the Latest Version registered in Azure Machine Learning Service Model Registry using az ml cli?",
        "Question_body":"<p>Using Azure Machine Learning CLI extension, how do we get the Model ID for the latest version of a Model (with known model name)?<\/p>\n<p>To get the entire list of Model Details with a given name the command is<\/p>\n<pre><code>az ml model list --model-name [Model_Name] --resource-group [RGP_NAME] --subscription-id [SUB_ID] --workspace-name [WS_NAME]\n<\/code><\/pre>\n<p>Running this will give a list of all the models:<\/p>\n<pre><code>[\n  {\n    &quot;createdTime&quot;: &quot;2021-03-19T07:02:03.814172+00:00&quot;,\n    &quot;framework&quot;: &quot;Custom&quot;,\n    &quot;frameworkVersion&quot;: null,\n    &quot;id&quot;: &quot;model:2&quot;\n    &quot;name&quot;: &quot;model&quot;,\n    &quot;version&quot;: 3\n  },\n  {\n    &quot;createdTime&quot;: &quot;2021-03-19T06:46:34.301054+00:00&quot;,\n    &quot;framework&quot;: &quot;Custom&quot;,\n    &quot;frameworkVersion&quot;: null,\n    &quot;id&quot;: &quot;model:2&quot;,\n    &quot;name&quot;: &quot;model&quot;,\n    &quot;version&quot;: 2\n  },\n  {\n    &quot;createdTime&quot;: &quot;2021-03-19T06:38:56.558385+00:00&quot;,\n    &quot;framework&quot;: &quot;Custom&quot;,\n    &quot;frameworkVersion&quot;: null,\n    &quot;id&quot;: &quot;model:1&quot;,\n    &quot;name&quot;: &quot;model&quot;,\n    &quot;version&quot;: 1\n  }\n]\n<\/code><\/pre>\n<p>The <a href=\"https:\/\/docs.microsoft.com\/en-us\/cli\/azure\/ext\/azure-cli-ml\/ml\/model?view=azure-cli-latest#ext_azure_cli_ml_az_ml_model_list\" rel=\"nofollow noreferrer\">Microsoft Documentation<\/a> mentions, we can use a <code>-l<\/code> parameter to get the latest version details:<\/p>\n<pre><code>az ml model list --model-name [Model_Name] --resource-group [RGP_NAME] --subscription-id [SUB_ID] --workspace-name [WS_NAME] -l\n<\/code><\/pre>\n<p>However, running this gives the following error:<\/p>\n<pre><code>ERROR: UnrecognizedArgumentError: unrecognized arguments: -l\n<\/code><\/pre>\n<p>What is the syntax to use this <code>-l<\/code> flag?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1616140269540,
        "Question_score":2,
        "Question_tags":"azure|azure-cli|azure-machine-learning-service",
        "Question_view_count":545,
        "Owner_creation_time":1601729162437,
        "Owner_last_access_time":1663774065773,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":887,
        "Owner_up_votes":187,
        "Owner_down_votes":32,
        "Owner_views":130,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If we wish to obtain the model-id for the latest model, instead of using <code>az ml model<\/code> list with <code>-l<\/code> flag, using <code>az model show<\/code> will return the details for the latest model. The syntax to get a string for model-id will be:<\/p>\n<pre><code>az ml model show --model-id $(TRN_MODEL_ID) --resource-group $(AML_TRN_RG) --subscription-id $(AML_TRN_SUB_ID) --workspace-name $(AML_TRN_WS) --query name -o tsv\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1616423749203,
        "Answer_score":0,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66704314",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":40714064,
        "Question_title":"How \"Azure ML export data to SQL database by insert in a row of database.\"",
        "Question_body":"<p>I can only export data from AzureML by write instead to database that created previously. I need to know How to insert and fetch the data continuously the database because I need to use old data as well as the new data that get as the AzureML output to plot graph.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1479709864747,
        "Question_score":0,
        "Question_tags":"sql|database|azure|azure-sql-database|azure-machine-learning-studio",
        "Question_view_count":193,
        "Owner_creation_time":1455005478600,
        "Owner_last_access_time":1649248026317,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1480314865163,
        "Answer_body":"<ol>\n<li>Create a web service from the AzureML experiment. <\/li>\n<li>Access the web service using a program you written from C# or any language.<\/li>\n<li>You can get the output of the web service as a JSON.<\/li>\n<li>Use typical SQL ADD\/UPDATE queries to update the table<\/li>\n<li>When giving an input for the web service, fetch the data from the DB and pass as the JSON for it. <\/li>\n<\/ol>",
        "Answer_comment_count":0,
        "Answer_creation_time":1480308703073,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40714064",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58933565,
        "Question_title":"How to register model from the Azure ML Pipeline Script step",
        "Question_body":"<p>I am running the <code>pipeline.submit()<\/code> in AzureML, which has a <code>PythonScriptStep<\/code>.\nInside this step, I download a model from tensorflow-hub, retrain it and save it as a <code>.zip<\/code>, and finally, I would like to register it in the Azure ML.\nBut as inside the script I do not have a workspace, <code>Model.register()<\/code> is not the case.\nSo I am trying to use <code>Run.register_model()<\/code> method as below:<\/p>\n\n<pre><code>os.replace(os.path.join('.', archive_name + '.zip'), \n           os.path.join('.', 'outputs', archive_name + '.zip'))\n\nprint(os.listdir('.\/outputs'))\nprint('========================')\n\nrun_context = Run.get_context()\nfinetuning_model = run_context.register_model(model_name='finetuning_similarity_model',\n                                              model_path=os.path.join(archive_name+'.zip'),\n                                              tags={},\n                                              description=\"Finetuning Similarity model\")\n<\/code><\/pre>\n\n<p>But then I have got an error:<\/p>\n\n<blockquote>\n  <p>ErrorResponse \n  {\n      \"error\": {\n          \"message\": \"Could not locate the provided model_path retrained.zip in the set of files uploaded to the run:<\/p>\n<\/blockquote>\n\n<p>despite I have the retrained <code>.zip<\/code> in the <code>.\/outputs<\/code> dir as we can see from the log:<\/p>\n\n<pre><code>['retrained.zip']\n========================\n<\/code><\/pre>\n\n<p>I guess that I am doing something wrong?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1574164584153,
        "Question_score":7,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":3429,
        "Owner_creation_time":1574162655727,
        "Owner_last_access_time":1632472959343,
        "Owner_location":null,
        "Owner_reputation":75,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1578744224352,
        "Answer_body":"<p>I was able to fix the same issue (<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.exceptions.modelpathnotfoundexception?view=azure-ml-py\" rel=\"noreferrer\"><code>ModelPathNotFoundException<\/code><\/a>) by explicitly uploading the model into the run history record before trying to register the model:<\/p>\n\n<pre><code>run.upload_file(\"outputs\/my_model.pickle\", \"outputs\/my_model.pickle\")\n<\/code><\/pre>\n\n<p>Which I found surprising because this wasn't mentioned in many of the official examples and according to the <code>upload_file()<\/code> <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#upload-file-name--path-or-stream-\" rel=\"noreferrer\">documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>Runs automatically capture file in the specified output directory, which defaults to \".\/outputs\" for most run types. Use upload_file only when additional files need to be uploaded or an output directory is not specified.<\/p>\n<\/blockquote>",
        "Answer_comment_count":2,
        "Answer_creation_time":1578745983587,
        "Answer_score":14,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":1578746319987,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58933565",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":38077884,
        "Question_title":"Can Azure calculate confidence interval for regressions?",
        "Question_body":"<p>I plan to try different regression methods provided by Azure ML Studio to predict numeric values. I wonder if it is possible to get the predictions together with corresponding confidence intervals. In other words, I would like the regression function to tell me not only the expected value (prediction) but also how confident it (the model) is about this value. Does Azure regression support this functionality?<\/p>\n\n<p><strong>ADDED<\/strong><\/p>\n\n<p>A related question. Can build in \"regressors\" estimate probability density functions? For example for a given case (a row in a data table) I would like to have not only a single number as a prediction (expected value) but also probabilities of all possible values.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1467121249227,
        "Question_score":1,
        "Question_tags":"azure|regression|confidence-interval|azure-machine-learning-studio",
        "Question_view_count":332,
        "Owner_creation_time":1262870449817,
        "Owner_last_access_time":1652799411040,
        "Owner_location":null,
        "Owner_reputation":116085,
        "Owner_up_votes":820,
        "Owner_down_votes":37,
        "Owner_views":4661,
        "Question_last_edit_time":1467123550236,
        "Answer_body":"<p>Currently, you will have to use R or python within Azure ML for confidence interval <\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1467268176063,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/38077884",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":34614582,
        "Question_title":"Send request as Json on UWP",
        "Question_body":"<p>I have deployed an AzureML published experiment with deployed web service. I tried to use the <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-consume-web-services\/\" rel=\"nofollow\">sample code provided in the configuration page<\/a>, but universal apps do not implement Http.Formatting yet, thus I couldn't use <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/hh944521(v=vs.118).aspx\" rel=\"nofollow\">postasjsonasync<\/a>.<\/p>\n\n<p>I tried to follow the sample code as much as possible, but I'm getting statuscode of 415 \"Unsupported Media Type\", What's the mistake I'm doing?<\/p>\n\n<pre><code>var client = new HttpClient();\nclient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", apiKey);\n\/\/ client.BaseAddress = uri;\n\nvar scoreRequest = new\n{\n            Inputs = new Dictionary&lt;string, StringTable&gt;() {\n                    {\n                        \"dataInput\",\n                        new StringTable()\n                        {\n                            ColumnNames = new [] {\"Direction\", \"meanX\", \"meanY\", \"meanZ\"},\n                            Values = new [,] {  { \"\", x.ToString(), y.ToString(), z.ToString() },  }\n                        }\n                    },\n                },\n            GlobalParameters = new Dictionary&lt;string, string&gt;() { }\n };\n var stringContent = new StringContent(scoreRequest.ToString());\n HttpResponseMessage response = await client.PostAsync(uri, stringContent);\n<\/code><\/pre>\n\n<p>Many Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1452005613750,
        "Question_score":1,
        "Question_tags":"c#|json|uwp|dotnet-httpclient|azure-machine-learning-studio",
        "Question_view_count":3194,
        "Owner_creation_time":1352139399460,
        "Owner_last_access_time":1655105886893,
        "Owner_location":"Cyprus",
        "Owner_reputation":820,
        "Owner_up_votes":1492,
        "Owner_down_votes":12,
        "Owner_views":256,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You'll need to serialize the object to a JSON string (I recommend using NewtonSoft.Json to make it easier) and set the content type accordingly. Here's an implementation I'm using in my UWP apps (note that <code>_client<\/code> is an <code>HttpClient<\/code>):<\/p>\n\n<pre><code>    public async Task&lt;HttpResponseMessage&gt; PostAsJsonAsync&lt;T&gt;(Uri uri, T item)\n    {\n        var itemAsJson = JsonConvert.SerializeObject(item);\n        var content = new StringContent(itemAsJson);\n        content.Headers.ContentType = new MediaTypeHeaderValue(\"application\/json\");\n\n        return await _client.PostAsync(uri, content);\n    }\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1452007973623,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34614582",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":50439489,
        "Question_title":"Delete Project Azure ML Studio ( Web App)",
        "Question_body":"<p>How do I delete projects in my workspace ?when I click to delete a project the delete button is inactive. Tried clearing cache and whatnot but cannot delete the project from studio.azureml.net... how do I do this ? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1526851287060,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":576,
        "Owner_creation_time":1506435894640,
        "Owner_last_access_time":1664081964040,
        "Owner_location":"Southeast Asia",
        "Owner_reputation":1922,
        "Owner_up_votes":1232,
        "Owner_down_votes":72,
        "Owner_views":404,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I have reproduced your issue. Try to go to your project -> EDIT ->remove the <strong>ASSETS<\/strong> of your project. Then the delete button will be able.<\/p>\n\n<p>You could follow the screenshot.<\/p>\n\n<ol>\n<li>The <strong>DELETE<\/strong> button is disable.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/E850F.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E850F.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>2.Go to <strong>EDIT<\/strong> and remove the <strong>ASSETS<\/strong>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/PbEZC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PbEZC.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/2kBgO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2kBgO.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>3.Then the <strong>DELETE<\/strong> button will be able<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/08lOW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/08lOW.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1526865758890,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1526866401040,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50439489",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":36260727,
        "Question_title":"Equivalent of Subset in Azure machine learning studio",
        "Question_body":"<p>I have a dataset in azure machine learning (.csv), on the same dataset I have multiple models build, I want to subset data for each of the model based on a different column<\/p>\n\n<p>Input:<\/p>\n\n<pre><code>ID col1 col2 col3\n1  0    13   0\n2  5    45   0\n3  10   0    34\n4  12   1    3\n<\/code><\/pre>\n\n<p>For the 1st model I want to retain all records where col1 not equal to None<\/p>\n\n<pre><code>ID col1 col2 col3\n2  5    45   0\n3  10   0    34\n4  12   1    3\n<\/code><\/pre>\n\n<p>Similarly for model 2<\/p>\n\n<pre><code>ID col1 col2 col3\n1  0    13   0\n2  5    45   0\n4  12   1    3\n<\/code><\/pre>\n\n<p>Hope it was clear<\/p>\n\n<p>The equivalent in R would be <\/p>\n\n<pre><code>df[!df$col1 == \"None\",] \n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1459161991533,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":243,
        "Owner_creation_time":1406266059940,
        "Owner_last_access_time":1663232189147,
        "Owner_location":"Link\u00f6ping, Sweden",
        "Owner_reputation":1677,
        "Owner_up_votes":82,
        "Owner_down_votes":2,
        "Owner_views":221,
        "Question_last_edit_time":1459256566467,
        "Answer_body":"<p>You can use the \"Execute R Script\" module and just plug in your R code there.<\/p>\n\n<pre><code>df &lt;- maml.mapInputPort(1)\ndf &lt;- df[!df$col1 == \"None\",] \nmaml.mapOutputPort(\"df\");\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1461479422230,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36260727",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":64394661,
        "Question_title":"Erro InvalidInputDatatype: Input of type 'Unknown' is not supported in azure (azureml.train.automl)",
        "Question_body":"<p>I have a pandas's DataFrame created by:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>TB_HISTORICO_MODELO = pd.read_sql(&quot;&quot;&quot;select DAT_INICIO_SEMANA_PLAN\n,COD_NEGOCIO\n,VENDA\n,LUCRO\n,MODULADO\n,RUPTURA\n,QTD_ESTOQUE_MEDIO\n,PECAS from TB&quot;&quot;&quot;, cursor)\n\nTB_HISTORICO_MODELO[&quot;DAT_INICIO_SEMANA_PLAN&quot;] = pd.to_datetime(TB_HISTORICO_MODELO[&quot;DAT_INICIO_SEMANA_PLAN&quot;])\n\ndataset = TB_HISTORICO_MODELO[TB_HISTORICO_MODELO['COD_NEGOCIO']=='A101'].drop(columns=['COD_NEGOCIO']) .reset_index(drop=True)\n<\/code><\/pre>\n<p>Everything look like right.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; dataset.dtypes\nDAT_INICIO_SEMANA_PLAN    datetime64[ns]\nVENDA                            float64\nLUCRO                            float64\nMODULADO                           int64\nRUPTURA                            int64\nQTD_ESTOQUE_MEDIO                  int64\nPECAS                            float64\ndtype: object\n<\/code><\/pre>\n<p>But when I rum this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>#%% Create the AutoML Config file and run the experiment on Azure\n\nfrom azureml.train.automl import AutoMLConfig\n\ntime_series_settings = {\n   'time_column_name': 'DAT_INICIO_SEMANA_PLAN',\n   'max_horizon': 14,\n   'country_or_region': 'BR',\n   'target_lags': 'auto'\n}\n\nautoml_config = AutoMLConfig(task='forecasting',\n                            primary_metric='normalized_root_mean_squared_error',\n                            blocked_models=['ExtremeRandomTrees'],\n                            experiment_timeout_minutes=30,\n                            training_data=dataset,\n                            label_column_name='VENDA',\n                            compute_target = compute_cluster,\n                            enable_early_stopping=True,\n                            n_cross_validations=3,\n                            # max_concurrent_iterations=4,\n                            # max_cores_per_iteration=-1,\n                            verbosity=logging.INFO,\n                            **time_series_settings)\n\nremote_run = Experimento.submit(automl_config, show_output=True)\n<\/code><\/pre>\n<p>I get the message<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; remote_run = Experimento.submit(automl_config, show_output=True)\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\n  File &quot;\/home\/fnord\/venv\/lib64\/python3.6\/site-packages\/azureml\/core\/experiment.py&quot;, line 219, in submit\n    run = submit_func(config, self.workspace, self.name, **kwargs)\n  File &quot;\/home\/fnord\/venv\/lib64\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py&quot;, line 92, in _automl_static_submit\n    automl_config_object._validate_config_settings(workspace)\n  File &quot;\/home\/fnord\/venv\/lib64\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py&quot;, line 1775, in _validate_config_settings\n    supported_types=&quot;, &quot;.join(SupportedInputDatatypes.REMOTE_RUN_SCENARIO)\nazureml.train.automl.exceptions.ConfigException: ConfigException:\n        Message: Input of type 'Unknown' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset, azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset]\n        InnerException: None\n        ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;Input of type 'Unknown' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset, azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset]&quot;,\n        &quot;details_uri&quot;: &quot;https:\/\/aka.ms\/AutoMLConfig&quot;,\n        &quot;target&quot;: &quot;training_data&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;BadArgument&quot;,\n            &quot;inner_error&quot;: {\n                &quot;code&quot;: &quot;ArgumentInvalid&quot;,\n                &quot;inner_error&quot;: {\n                    &quot;code&quot;: &quot;InvalidInputDatatype&quot;\n                }\n            }\n        }\n    }\n}\n\n<\/code><\/pre>\n<p>Where is wrong?<\/p>\n<p>documentation:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train<\/a>\n<a href=\"https:\/\/docs.microsoft.com\/pt-br\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/pt-br\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1602873387197,
        "Question_score":1,
        "Question_tags":"python|pandas|azure|azure-machine-learning-service",
        "Question_view_count":382,
        "Owner_creation_time":1522634496793,
        "Owner_last_access_time":1663943775803,
        "Owner_location":"Rio de Janeiro, RJ, Brasil",
        "Owner_reputation":264,
        "Owner_up_votes":276,
        "Owner_down_votes":2,
        "Owner_views":23,
        "Question_last_edit_time":null,
        "Answer_body":"<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train#data-source-and-format?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">Configure AutoML Doc<\/a> says:<\/p>\n<blockquote>\n<p>For remote experiments, training data must be accessible from the remote compute. AutoML only accepts Azure Machine Learning TabularDatasets when working on a remote compute.<\/p>\n<\/blockquote>\n<p>It looks as if your <code>dataset<\/code> object is a Pandas DataFrame, when it should really be an Azure ML <code>Dataset<\/code>. Check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-datasets?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">this doc<\/a> on creating Datasets.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1603004866533,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1603005259150,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64394661",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":73146779,
        "Question_title":"ML Studio language studio failing to detect the source language",
        "Question_body":"<p>I am running a program in python to detect a language and translate that to English using azure machine learning studio. The code block mentioned below throwing error when trying to detect the language.<\/p>\n<blockquote>\n<p>Error 0002: Failed to parse parameter.<\/p>\n<\/blockquote>\n<pre><code>def sample_detect_language():\n    print(\n        &quot;This sample statement will be translated to english from any other foreign language&quot;\n       \n    )\n    \n    from azure.core.credentials import AzureKeyCredential\n    from azure.ai.textanalytics import TextAnalyticsClient\n\n    endpoint = os.environ[&quot;AZURE_LANGUAGE_ENDPOINT&quot;]\n    key = os.environ[&quot;AZURE_LANGUAGE_KEY&quot;]\n\n    text_analytics_client = TextAnalyticsClient(endpoint=endpoint)\n    documents = [\n        &quot;&quot;&quot;\n        The feedback was awesome\n        &quot;&quot;&quot;,\n        &quot;&quot;&quot;\n        la recensione \u00e8 stata fantastica\n        &quot;&quot;&quot;\n    ]\n\n    result = text_analytics_client.detect_language(documents)\n    reviewed_docs = [doc for doc in result if not doc.is_error]\n\n    print(&quot;Check the languages we got review&quot;)\n\n    for idx, doc in enumerate(reviewed_docs):\n        print(&quot;Number#{} is in '{}', which has ISO639-1 name '{}'\\n&quot;.format(\n            idx, doc.primary_language.name, doc.primary_language.iso6391_name\n        ))\n        if doc.is_error:\n            print(doc.id, doc.error)\n    \n    print(\n        &quot;Storing reviews and mapping to their respective ISO639-1 name &quot;\n        \n    )\n\n    review_to_language = {}\n    for idx, doc in enumerate(reviewed_docs):\n        review_to_language[documents[idx]] = doc.primary_language.iso6391_name\n\n\nif __name__ == '__main__':\n    sample_detect_language()\n<\/code><\/pre>\n<p>Any help to solve the issue is appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658977363490,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":50,
        "Owner_creation_time":1652123310643,
        "Owner_last_access_time":1663298985173,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":41,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The issue was raised because of missing the called parameters in the function. While doing language detection in machine learning studio, we need to assign end point and key credentials. In the code mentioned above, endpoint details were mentioned, but missed <strong>AzureKeyCredential.<\/strong><\/p>\n<pre><code>endpoint = os.environ[&quot;AZURE_LANGUAGE_ENDPOINT&quot;]\nkey = os.environ[&quot;AZURE_LANGUAGE_KEY&quot;]\ntext_analytics_client = TextAnalyticsClient(endpoint=endpoint)\n<\/code><\/pre>\n<p>replace the above line with the code block mentioned below<\/p>\n<pre><code>text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential= AzureKeyCredential(key))\n<\/code><\/pre>",
        "Answer_comment_count":1,
        "Answer_creation_time":1659007284270,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73146779",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":58019308,
        "Question_title":"ScriptRunConfig with datastore reference on AML",
        "Question_body":"<p>When trying to run a ScriptRunConfig, using :<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>src = ScriptRunConfig(source_directory=project_folder, \n                      script='train.py', \n                      arguments=['--input-data-dir', ds.as_mount(),\n                                 '--reg', '0.99'],\n                      run_config=run_config) \nrun = experiment.submit(config=src)\n<\/code><\/pre>\n\n<p>It doesn't work and breaks with this when I submit the job : <\/p>\n\n<pre><code>... lots of things... and then\nTypeError: Object of type 'DataReference' is not JSON serializable\n<\/code><\/pre>\n\n<p>However if I run it with the Estimator, it works. One of the differences is the fact that with a <code>ScriptRunConfig<\/code> we're using a list for parameters and the other is a dictionary.<\/p>\n\n<p>Thanks for any pointers!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1568929720367,
        "Question_score":2,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":1541,
        "Owner_creation_time":1538275960603,
        "Owner_last_access_time":1658458641830,
        "Owner_location":"Montreal, QC, Canada",
        "Owner_reputation":381,
        "Owner_up_votes":75,
        "Owner_down_votes":2,
        "Owner_views":50,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Being able to use <code>DataReference<\/code> in <code>ScriptRunConfig<\/code> is a bit more involved than doing just <code>ds.as_mount()<\/code>. You will need to convert it into a string in <code>arguments<\/code> and then update the <code>RunConfiguration<\/code>'s <code>data_references<\/code> section with the <code>DataReferenceConfiguration<\/code> created from <code>ds<\/code>. Please <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-remote-vm\/train-on-remote-vm.ipynb\" rel=\"nofollow noreferrer\">see here<\/a> for an example notebook on how to do that.<\/p>\n<p>If you are just reading from the input location and not doing any writes to it, please check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-register-datasets\" rel=\"nofollow noreferrer\"><code>Dataset<\/code><\/a>. It allows you to do exactly what you are doing without doing anything extra. <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/datasets-tutorial\/train-with-datasets.ipynb\" rel=\"nofollow noreferrer\">Here is an example notebook<\/a> that shows this in action.<\/p>\n<p>Below is a short version of the notebook<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Dataset\n\n# more imports and code\n\nds = Datastore(workspace, 'mydatastore')\ndataset = Dataset.File.from_files(path=(ds, 'path\/to\/input-data\/within-datastore'))\n\nsrc = ScriptRunConfig(source_directory=project_folder, \n                      script='train.py', \n                      arguments=['--input-data-dir', dataset.as_named_input('input').as_mount(),\n                                 '--reg', '0.99'],\n                      run_config=run_config) \nrun = experiment.submit(config=src)\n<\/code><\/pre>",
        "Answer_comment_count":1,
        "Answer_creation_time":1568945686667,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1595974462436,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58019308",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":60397252,
        "Question_title":"Can I use Azure interactive mode for azure-cli-ml extension?",
        "Question_body":"<p>I'm using Azure CLI interactive mode <code>az interactive<\/code> to run below command. <br \/>\n<code>az ml folder attach -w yhd-mlws -g yhd-mlws-rg<\/code><br \/><br \/>\nIt prompts me with below error message.<br \/>\n<code>az: error: unrecognized arguments: -w yhd-mlws -g yhd-mlws-rg<\/code><br \/><br \/>\nBTW, both my Machine Learning workspace <code>yhd-mlws<\/code> and resource group <code>yhd-mlws-rg<\/code> had been created in my Azure subscription. Azure CLI extension for machine learning service had also been installed via <code>az extension add -n azure-cli-ml<\/code>.<br \/><br \/>\nThen I run command <code>az ml folder attach<\/code> without any argument. I get bellow error message.<br \/><\/p>\n\n<pre><code>Message: Error, default workspace not set and workspace name parameter not provided.\nPlease set a default workspace using \"az ml folder attach -w myworkspace -g myresourcegroup\" or provide a value for the workspace name parameter.\n<\/code><\/pre>\n\n<p>The command window exit the interactive mode after above error message. Then I try the command <code>az ml folder attach -w yhd-mlws -g yhd-mlws-rg<\/code> again, bingo! It works. <br \/>\nHere comes my question, does azure-cli-ml extension support Azure CLI interactive mode? You know, Azure CLI interactive mode is amazing and I want to use it whenever possible. Thanks!<br \/><br \/>\nBTW, I'm running windows command window in Windows Server 2016 Datcenter. Azure-cli version is 2.0.79.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1582642024300,
        "Question_score":1,
        "Question_tags":"azure|azure-cli|azure-machine-learning-service",
        "Question_view_count":408,
        "Owner_creation_time":1582361231693,
        "Owner_last_access_time":1655516080367,
        "Owner_location":"Guangzhou, China",
        "Owner_reputation":393,
        "Owner_up_votes":37,
        "Owner_down_votes":1,
        "Owner_views":58,
        "Question_last_edit_time":1582678582156,
        "Answer_body":"<p>I can reproduce your issue, the interactive mode should support the <code>azure-cli-ml<\/code> extension, because when I run <code>az ml workspace list<\/code>, it works, once I pass the <code>-g<\/code> parameter, it gives the same error, maybe it is a bug, but I am not sure, the <code>interactive<\/code> is in preview currently.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/g4FvM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/g4FvM.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>If you want to run <code>az ml folder attach -w yhd-mlws -g yhd-mlws-rg<\/code> in the interactive mode, my workaround is to pass the <code>#<\/code>, i.e. <code># az ml folder attach -w yhd-mlws -g yhd-mlws-rg<\/code>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/pWMyH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/pWMyH.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1582686710413,
        "Answer_score":2,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60397252",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":38189399,
        "Question_title":"azure ml experiment return different results than webservice",
        "Question_body":"<p>same input is used in two cases, but different result is returned from python module<\/p>\n\n<p>here is the python script that return the result to the webservice:<\/p>\n\n<pre><code>import pandas as pd\nimport sys\n\n\n  def get_segments(dataframe):\n     dataframe['segment']=dataframe['segment'].astype('str')\n     segments = dataframe.loc[~dataframe['segment'].duplicated()]['segment']\n     return segments\n\n\n  def azureml_main(dataframe1 = None, dataframe2 = None):\n\n   df = dataframe1\n   segments = get_segments(df)\n   segmentCount =segments.size\n\n   if (segmentCount &gt; 0) :\n      res = pd.DataFrame(columns=['segmentId','recommendation'],index=[range(segmentCount)])\n    i=0    \n    for seg in segments:\n        d= df.query('segment ==[\"{}\"]'.format(seg)).sort(['count'],ascending=[0])\n\n        res['segmentId'][i]=seg\n        recommendation='['\n        for index, x in d.iterrows():\n            item=str(x['ItemId'])\n            recommendation = recommendation + item + ','\n        recommendation = recommendation[:-1] + ']'\n        res['recommendation'][i]= recommendation\n        i=i+1\n   else:\n\n      res = pd.DataFrame(columns=[seg,pdver],index=[range(segmentCount)])\n\nreturn res,\n<\/code><\/pre>\n\n<p>when in experiment it returnd the actual itemIds, when in webservice it returns some numbers<\/p>\n\n<p>the purpose of this code is to pivot some table by segment column for recommendation<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1467651436410,
        "Question_score":3,
        "Question_tags":"python|web-services|azure|cortana-intelligence|azure-machine-learning-studio",
        "Question_view_count":264,
        "Owner_creation_time":1320061998253,
        "Owner_last_access_time":1656424560827,
        "Owner_location":null,
        "Owner_reputation":778,
        "Owner_up_votes":176,
        "Owner_down_votes":6,
        "Owner_views":89,
        "Question_last_edit_time":1468140771380,
        "Answer_body":"<p>After discussion with the product team from Microsoft. the issue was resolved.\nthe product team rolled out an update to the web service first, and only later to the ML-Studio, which fixed an issue with categorical attributes in \"Execute python script\".\nthe issue was in a earlier stage of the flow and has nothing to do with the python code above.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1468139423743,
        "Answer_score":0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/38189399",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":67734831,
        "Question_title":"Does a AzureML webservice overwrite reset the Data Collection Dataset?",
        "Question_body":"<p>If we have an AzureML web service endpoint that is collecting data (for Data Drift Monitoring), does overwriting the web service endpoint with a new version of the model break links with the Dataset registered for collecting data.<\/p>\n<p>The relative path to this dataset is:\n<code>&lt;Subscription-ID&gt;\/&lt;Resource-Group&gt;\/&lt;Workspace&gt;\/&lt;Webservice-Name&gt;\/&lt;model-name&gt;\/&lt;version&gt;\/inputs\/**\/inputs*.csv<\/code><\/p>\n<p>If we redeploy a new version using <code>az ml model deploy ..... --overwrite<\/code>, will we need a new reference to a new Dataset for detecting Data Drift?<\/p>\n<p>If we use <code>az ml service update ..<\/code>, will the Dataset reference be kept intact?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1622187967130,
        "Question_score":2,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":44,
        "Owner_creation_time":1601729162437,
        "Owner_last_access_time":1663774065773,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":887,
        "Owner_up_votes":187,
        "Owner_down_votes":32,
        "Owner_views":130,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Since the Dataset Asset is a simple reference to a location in a Datastore. Assuming the model version and service name does not change, the Dataset reference also will not change. If however, with every Service Update - The model version changes then adding a Dataset with Relative Path:<\/p>\n<pre><code>&lt;Subscription-ID&gt;\/&lt;Resource-Group&gt;\/&lt;Workspace&gt;\/&lt;Webservice-Name&gt;\/&lt;model-name&gt;\/*\/inputs\/**\/inputs*.csv\n<\/code><\/pre>\n<p>Will solve the problem. Since Data Drift is another service referencing this Dataset asset, it will keep working as expected.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1624012850200,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67734831",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":72659937,
        "Question_title":"CI \/ CD and repository integration for Azure ML Workspace",
        "Question_body":"<p>I am interested in knowing how can I integrate a repository with Azure Machine Learning Workspace.<\/p>\n<h2>What have I tried ?<\/h2>\n<p>I have some experience with Azure Data Factory and usually I have setup workflows where<\/p>\n<ol>\n<li><p>I have a <code>dev<\/code> azure data factory instance that is linked to azure repository.<\/p>\n<\/li>\n<li><p>Changes made to the repository using the code editor.<\/p>\n<\/li>\n<li><p>These changes are published via the <code>adf_publish<\/code> branch to the live <code>dev<\/code> instance<\/p>\n<\/li>\n<li><p>I use CI \/ CD pipeline and the AzureRMTemplate task to deploy the templates in the publish branch to release the changes to <code>production<\/code> environment<\/p>\n<\/li>\n<\/ol>\n<h2>Question:<\/h2>\n<ul>\n<li>How can I achieve the same \/ similar workflow with Azure Machine Learning Workspace ?<\/li>\n<li>How is CI \/ CD done with Azure ML Workspace<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655471491697,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":94,
        "Owner_creation_time":1271093246887,
        "Owner_last_access_time":1663988177550,
        "Owner_location":"United States",
        "Owner_reputation":9826,
        "Owner_up_votes":1723,
        "Owner_down_votes":15,
        "Owner_views":1238,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The following workflow is the official practice to be followed to achieve the task required.<\/p>\n<ol>\n<li>Starting with the architecture mentioned below<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/KdRUa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KdRUa.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ul>\n<li>we need to have a specific data store to handle the dataset<\/li>\n<li>Perform the regular code modifications using the IDE like Jupyter Notebook or VS Code<\/li>\n<li>Train and test the model<\/li>\n<li>To register and operate on the model, deploy the model image as a web service and operate the rest.<\/li>\n<\/ul>\n<ol start=\"2\">\n<li><strong>Configure the CI Pipeline:<\/strong><\/li>\n<\/ol>\n<ul>\n<li><p>Follow the below steps to complete the procedure<\/p>\n<p><strong>Before implementation:<\/strong><\/p>\n<pre><code>- We need azure subscription enabled account\n- DevOps activation must be activated.\n<\/code><\/pre>\n<\/li>\n<li><p>Open DevOps portal with enabled SSO<\/p>\n<\/li>\n<li><p>Navigate to <strong>Pipeline -&gt; Builds -&gt; Choose the model which was created -&gt; Click on EDIT<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/yUVZl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/yUVZl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Build pipeline will be looking like below screen\n<a href=\"https:\/\/i.stack.imgur.com\/VSKJq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VSKJq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>We need to use Anaconda distribution for this example to get all the dependencies.<\/p>\n<\/li>\n<li><p>To install environment dependencies, check the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/tasks\/package\/conda-environment?view=azure-devops&amp;viewFallbackFrom=azdevops\" rel=\"nofollow noreferrer\">link<\/a><\/p>\n<\/li>\n<li><p>Use the python environment, under <strong>Install Requirements<\/strong> in user setup.<\/p>\n<\/li>\n<li><p>Select <strong>create or get workspace<\/strong> select your account subscription as mentioned in below screen<\/p>\n<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/vt0el.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vt0el.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ul>\n<li>Save the changes happened in other tasks and all those muse be in same subscription.\n<a href=\"https:\/\/i.stack.imgur.com\/WJxCL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WJxCL.png\" alt=\"enter image description here\" \/><\/a><\/li>\n<\/ul>\n<p>The entire CI\/CD procedure and solution was documented in <a href=\"https:\/\/www.azuredevopslabs.com\/labs\/vstsextend\/aml\/#author-praneet-singh-solanki\" rel=\"nofollow noreferrer\">link<\/a><\/p>\n<p><strong>Document Credit: Praneet Singh Solanki<\/strong><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1655682328283,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72659937",
        "Question_exclusive_tag":"Azure Machine Learning"
    },
    {
        "Question_id":65509754,
        "Question_title":"Can ClearML (formerly Trains) work a local server?",
        "Question_body":"<p>I am trying to start my way with <a href=\"https:\/\/github.com\/allegroai\/clearml\" rel=\"nofollow noreferrer\">ClearML<\/a> (formerly known as Trains).<\/p>\n<p>I see on the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/getting_started\/index.html\" rel=\"nofollow noreferrer\">documentation<\/a> that I need to have server running, either on the ClearML platform itself, or on a remote machine using AWS etc.<\/p>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<p>According to <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/deploying_clearml\/index.html\" rel=\"nofollow noreferrer\">this<\/a> I can install the <code>trains-server<\/code> on any remote machine, so in theory I should also be able to install it on my local machine, but it still requires me to have Kubernetes or Docker, but I am not using any of them.<\/p>\n<p>Anyone had any luck using ClearML (or Trains, I think it's still quite the same API and all) on a local server?<\/p>\n<ul>\n<li>My OS is Ubuntu 18.04.<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1609343679217,
        "Question_score":5,
        "Question_tags":"trains|clearml",
        "Question_view_count":740,
        "Owner_creation_time":1383083414230,
        "Owner_last_access_time":1663937141383,
        "Owner_location":null,
        "Owner_reputation":2801,
        "Owner_up_votes":371,
        "Owner_down_votes":0,
        "Owner_views":131,
        "Question_last_edit_time":1609427009849,
        "Answer_body":"<p>Disclaimer: I'm a member of the ClearML team (formerly Trains)<\/p>\n<blockquote>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<\/blockquote>\n<p>A few options:<\/p>\n<ol>\n<li>The Clearml Free trier offers free hosting for your experiments, these experiment are only accessible to you, unless you specifically want to share them among your colleagues. This is probably the easiest way to <a href=\"https:\/\/app.community.clear.ml\" rel=\"nofollow noreferrer\">get started<\/a>.<\/li>\n<li>Install the ClearML-Server basically all you need is docker installed and you should be fine. There are full instructions <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/deploying_clearml\/clearml_server_linux_mac\/\" rel=\"nofollow noreferrer\">here<\/a> , this is the summary:<\/li>\n<\/ol>\n<pre class=\"lang-bash prettyprint-override\"><code>echo &quot;vm.max_map_count=262144&quot; &gt; \/tmp\/99-trains.conf\nsudo mv \/tmp\/99-trains.conf \/etc\/sysctl.d\/99-trains.conf\nsudo sysctl -w vm.max_map_count=262144\nsudo service docker restart\n\nsudo curl -L &quot;https:\/\/github.com\/docker\/compose\/releases\/latest\/download\/docker-compose-$(uname -s)-$(uname -m)&quot; -o \/usr\/local\/bin\/docker-compose\nsudo chmod +x \/usr\/local\/bin\/docker-compose\n\nsudo mkdir -p \/opt\/trains\/data\/elastic_7\nsudo mkdir -p \/opt\/trains\/data\/mongo\/db\nsudo mkdir -p \/opt\/trains\/data\/mongo\/configdb\nsudo mkdir -p \/opt\/trains\/data\/redis\nsudo mkdir -p \/opt\/trains\/logs\nsudo mkdir -p \/opt\/trains\/config\nsudo mkdir -p \/opt\/trains\/data\/fileserver\n\nsudo curl https:\/\/raw.githubusercontent.com\/allegroai\/trains-server\/master\/docker-compose.yml -o \/opt\/trains\/docker-compose.yml\ndocker-compose -f \/opt\/trains\/docker-compose.yml up -d\n<\/code><\/pre>\n<ol start=\"3\">\n<li>ClearML also supports full offline mode (i.e. no outside connection is made). Once your experiment completes, you can manually import the run to your server (either self hosted or free tier server)<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.set_offline(True)\ntask = Task.init(project_name='examples', task_name='offline mode experiment')\n<\/code><\/pre>\n<p>When the process ends you will get a link to a zip file containing the output of the entire offline session:<\/p>\n<pre><code>ClearML Task: Offline session stored in \/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip\n<\/code><\/pre>\n<p>Later you can import the session with:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.import_offline_session('\/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip')\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1609345139523,
        "Answer_score":6,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1661326401412,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65509754",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":57557070,
        "Question_title":"trains with grid search",
        "Question_body":"<p>I would like to test <a href=\"https:\/\/github.com\/allegroai\/trains\" rel=\"nofollow noreferrer\">trains<\/a> usage during grid search and it not clear how to do so.<\/p>\n\n<pre><code>from trains import Task \nTask.init(project_name=\"project name\", task_name='name')\n<\/code><\/pre>\n\n<p>creates an experiment in the demo server and logs all but you can't call init twice no matter the 'task_name' and <\/p>\n\n<pre><code>from trains import Task \nTask.create(project_name=\"project name\", task_name='name')\n<\/code><\/pre>\n\n<p>can be called with different 'task_name' but thus not log any data into the server and creates only 'Draft'.<\/p>\n\n<p>here is a sample code:<\/p>\n\n<pre><code> epochs=[160,300]\n for epoch in epochs:\n    model = define_model_run(epoch)\n    model.fit(x_train,y_train)\n    score = model.score(...)\n<\/code><\/pre>\n\n<p>my final try was:<\/p>\n\n<pre><code> epochs=[160,300]\n task=Task.init(project_name=\"demo\", task_name='search')\n for epoch in epochs:\n    task.create(project_name=\"demo\", task_name=f'search_{epoch}')\n    model = define_model_run(epoch)\n    model.fit(x_train,y_train)\n    score = model.score(...)\n<\/code><\/pre>\n\n<p>which logs all information under the experiments tab and none under the 'Draft'.\nI tried the last two hour the read the few documentations provided and reading the source code, but no luck.<\/p>\n\n<p>any help? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1566218806593,
        "Question_score":1,
        "Question_tags":"python|trains|clearml",
        "Question_view_count":121,
        "Owner_creation_time":1416942229380,
        "Owner_last_access_time":1663871932353,
        "Owner_location":null,
        "Owner_reputation":161,
        "Owner_up_votes":89,
        "Owner_down_votes":0,
        "Owner_views":58,
        "Question_last_edit_time":1609778640296,
        "Answer_body":"<p>Declaimer: I'm a member of TRAINS team<\/p>\n\n<p>Yes, that's exactly the answer.\nThe idea is that you always have one main Task, in order to create a new one you need to close the running Task, and re-initialize with a new name.\nKudos on solving it so quickly :)<\/p>\n\n<p>BTW: You can see examples <a href=\"https:\/\/stackoverflow.com\/q\/56744397\/11682840\">here<\/a>\/<a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/master\/docs\/faq.md#can-i-create-a-graph-comparing-hyper-parameters-vs-model-accuracy-\" rel=\"nofollow noreferrer\">and here<\/a>, showing how to send accuracy logs so it is easier to compare the experiments, especially when running hyper-parameter search.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1566240704540,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1566242122212,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57557070",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":62654203,
        "Question_title":"Trains: Can I reset the status of a task? (from 'Aborted' back to 'Running')",
        "Question_body":"<p>I had to stop training in the middle, which set the Trains status to <code>Aborted<\/code>.\nLater I continued it from the last checkpoint, but the status remained <code>Aborted<\/code>.\nFurthermore, automatic training metrics stopped appearing in the dashboard (though custom metrics still do).<\/p>\n<p>Can I reset the status back to <code>Running<\/code> and make Trains log training stats again?<\/p>\n<p><strong>Edit:<\/strong> When continuing training, I retrieved the task using <code>Task.get_task()<\/code> and not <code>Task.init()<\/code>. Maybe that's why training stats are not updated anymore?<\/p>\n<p><strong>Edit2:<\/strong> I also tried <code>Task.init(reuse_last_task_id=original_task_id_string)<\/code>, but it just creates a new task, and doesn't reuse the given task ID.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593508903533,
        "Question_score":1,
        "Question_tags":"trains|clearml",
        "Question_view_count":228,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":1609467668432,
        "Answer_body":"<p>Disclaimer: I'm a member of Allegro Trains team<\/p>\n<blockquote>\n<p>When continuing training, I retrieved the task using Task.get_task() and not Task.init(). Maybe that's why training stats are not updated anymore?<\/p>\n<\/blockquote>\n<p>Yes that's the only way to continue the same exact Task.\nYou can also mark it as started with <code>task.mark_started()<\/code> , that said the automatic logging will not kick in, as <code>Task.get_task<\/code> is usually used for accessing previously executed tasks and not continuing it (if you think the continue use case is important please feel free to open a GitHub issue, I can definitely see the value there)<\/p>\n<p>You can also do something a bit different, and justcreate a new Task continuing from the last iteration the previous run ended. Notice that if you load the weights file (PyTorch\/TF\/Keras\/JobLib) it will automatically connect it with the model that was created in the previous run (assuming the model was stored is the same location, or if you have the model on https\/S3\/Gs\/Azure and you are using <code>trains.StorageManager.get_local_copy()<\/code>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>previous_run = Task.get_task()\ntask = Task.init('examples', 'continue training')\ntask.set_initial_iteration(previous_run.get_last_iteration())\ntorch.load('\/tmp\/my_previous_weights')\n<\/code><\/pre>\n<p>BTW:<\/p>\n<blockquote>\n<p>I also tried Task.init(reuse_last_task_id=original_task_id_string), but it just creates a new task, and doesn't reuse the given task ID.<\/p>\n<\/blockquote>\n<p>This is a great idea for an interface to continue a previous run, feel free to add it as GitHub issue.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1593515579540,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1593557456892,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62654203",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73279794,
        "Question_title":"[Catboost][ClearML] Error: if loss-function is Logloss, then class weights should be given for 0 and 1 classes",
        "Question_body":"<p>Having recently started using ClearML to manage the MLOps, I am facing the following problem:\nWhen running a script that trains a CatBoost in a binary classification problem using different class weights from my computer, it works perfectly, logs the results and no issues at all.\nOnce I try to run that remotely using the ClearML agent, it results in the following error:<\/p>\n<pre><code>&lt;!-- language: lang-none --&gt;\nTraceback (most recent call last):\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/catboost_bind.py&quot;, line 102, in _fit\n    return original_fn(obj, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 5007, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2262, in _fit\n    train_params = self._prepare_train_params(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2194, in _prepare_train_params\n    _check_train_params(params)\n  File &quot;_catboost.pyx&quot;, line 6032, in _catboost._check_train_params\n  File &quot;_catboost.pyx&quot;, line 6051, in _catboost._check_train_params\n**_catboost.CatBoostError: catboost\/private\/libs\/options\/catboost_options.cpp:607: if loss-function is Logloss, then class weights should be given for 0 and 1 classes\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):**\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/task_repository\/RecSys.git\/src\/cli\/model_training_remote.py&quot;, line 313, in &lt;module&gt;\n    rfs.run(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/task_repository\/RecSys.git\/src\/cli\/model_training_remote.py&quot;, line 232, in run\n    model.fit(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/__init__.py&quot;, line 36, in _inner_patch\n    raise ex\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/__init__.py&quot;, line 34, in _inner_patch\n    ret = patched_fn(original_fn, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/catboost_bind.py&quot;, line 110, in _fit\n    return original_fn(obj, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 5007, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2262, in _fit\n    train_params = self._prepare_train_params(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2194, in _prepare_train_params\n    _check_train_params(params)\n  File &quot;_catboost.pyx&quot;, line 6032, in _catboost._check_train_params\n  File &quot;_catboost.pyx&quot;, line 6051, in _catboost._check_train_params\n**_catboost.CatBoostError: catboost\/private\/libs\/options\/catboost_options.cpp:607: if loss-function is Logloss, then class weights should be given for 0 and 1 classes**\n\n<\/code><\/pre>\n<p>I do have the dictionary being connected:<\/p>\n<pre><code>    model_params = {\n        &quot;loss_function&quot;: &quot;Logloss&quot;,\n        &quot;eval_metric&quot;: &quot;AUC&quot;,\n        &quot;class_weights&quot;: {0: 1, 1: 60},\n        &quot;learning_rate&quot;: 0.1\n    }\n<\/code><\/pre>\n<p>registered in the ClearML task as<\/p>\n<pre><code>task.connect(model_params, 'model_params')\n<\/code><\/pre>\n<p>and used as parameters for the model in the following call:<\/p>\n<pre><code>model = CatBoostClassifier(**model_params)\n<\/code><\/pre>\n<p>When running it from the container in ClearML interactive mode, it also works fine.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659970052813,
        "Question_score":1,
        "Question_tags":"python|catboost|clearml",
        "Question_view_count":54,
        "Owner_creation_time":1659969003173,
        "Owner_last_access_time":1663967691430,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<p>I think I understand the problem, basically I think the issue is:<\/p>\n<pre><code>task.connect(model_params, 'model_params')\n<\/code><\/pre>\n<p>Since this is a nested dict:<\/p>\n<pre><code>    model_params = {\n        &quot;loss_function&quot;: &quot;Logloss&quot;,\n        &quot;eval_metric&quot;: &quot;AUC&quot;,\n        &quot;class_weights&quot;: {0: 1, 1: 60},\n        &quot;learning_rate&quot;: 0.1\n    }\n<\/code><\/pre>\n<p>The class_weights is stored as a <code>String<\/code> key, but <code>catboost<\/code> expects <code>int<\/code> key, hence failing.\nOne option would be to remove the <code>task.connect(model_params, 'model_params')<\/code><\/p>\n<p>Another solution (until we fix it) would be to do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>task.connect(model_params, 'model_params')\nmodel_params[&quot;class_weights&quot;] = {\n0: model_params[&quot;class_weights&quot;].get(&quot;0&quot;, model_params[&quot;class_weights&quot;].get(0))\n1: model_params[&quot;class_weights&quot;].get(&quot;1&quot;, model_params[&quot;class_weights&quot;].get(1))\n}\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1659991938293,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73279794",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":66640850,
        "Question_title":"How to manage datasets in ClearML Web UI?",
        "Question_body":"<p>Using a self-deployed ClearML server with the clearml-data CLI, I would like to manage (or view) my datasets in the WebUI as shown on the ClearML webpage (<a href=\"https:\/\/clear.ml\/mlops\/clearml-feature-store\/\" rel=\"nofollow noreferrer\">https:\/\/clear.ml\/mlops\/clearml-feature-store\/<\/a>):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/2T8IC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2T8IC.png\" alt=\"ClearML feature store\" \/><\/a><\/p>\n<p>However, this feature does not show up in my Web UI. According to the pricing page, the feature store is not a premium feature. Do I need to configure my server in a special way to use this feature?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615821875663,
        "Question_score":3,
        "Question_tags":"clearml|trains",
        "Question_view_count":310,
        "Owner_creation_time":1551960780550,
        "Owner_last_access_time":1663614230163,
        "Owner_location":null,
        "Owner_reputation":354,
        "Owner_up_votes":54,
        "Owner_down_votes":3,
        "Owner_views":58,
        "Question_last_edit_time":1615822213440,
        "Answer_body":"<p>Disclaimer: I'm part of the ClearML (formerly Trains) Team<\/p>\n<p>I think this screenshot is taken from the premium version...\nThe feature itself exists in the open-source version, but I &quot;think&quot; some of the dataset visualization capabilities are not available in the open-source self hosted version.<\/p>\n<p>Nonetheless, you have a fully featured feature-store, with the ability to add your own metrics \/ samples for every dataset\/feature version. The open-source version also includes the advanced versioning &amp; delta based storage for datasets\/features (i.e. only the change set from the parent version is stored)<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1615831186403,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66640850",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":72593187,
        "Question_title":"ModuleNotFoundError: No module named 'allegroai'",
        "Question_body":"<p>I am trying to use <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/getting_started\/mlops\/mlops_first_steps\" rel=\"nofollow noreferrer\">ClearML<\/a>.<\/p>\n<p>The only line in my file is<\/p>\n<pre><code>from allegroai import Dataset, DatasetVersion\n<\/code><\/pre>\n<p>which yields<\/p>\n<pre><code>ModuleNotFoundError: No module named 'allegroai'\n<\/code><\/pre>\n<p>Looks like some pip package is missing, but I couldn't for the life of me find it in the docs.<\/p>\n<p>What should I pip install?<\/p>\n<p><strong>Not working:<\/strong><\/p>\n<ul>\n<li><code>pip install clearml-agent<\/code><\/li>\n<li><code>pip install clearml<\/code> and <code>clearml-init<\/code> as in <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/getting_started\/ds\/ds_first_steps\" rel=\"nofollow noreferrer\">here<\/a><\/li>\n<li><code>pip install allegroai<\/code><\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1655044800580,
        "Question_score":0,
        "Question_tags":"python|clearml",
        "Question_view_count":49,
        "Owner_creation_time":1314313109233,
        "Owner_last_access_time":1664062223147,
        "Owner_location":"Technion, Israel",
        "Owner_reputation":18777,
        "Owner_up_votes":2376,
        "Owner_down_votes":137,
        "Owner_views":2000,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Allegroai package should be taken from ClearML PyPi server.\nThis is only for paying customers (I think), and the way to retrieve it is by:<\/p>\n<ol>\n<li>Going to ClearML website (login with username\/company).<\/li>\n<li>Press the ? on the top right of the screen (next to your user icon) and choose the first one\n<a href=\"https:\/\/i.stack.imgur.com\/B1rDo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/B1rDo.png\" alt=\"1\" \/><\/a><\/li>\n<\/ol>",
        "Answer_comment_count":0,
        "Answer_creation_time":1657799581543,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72593187",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":72381916,
        "Question_title":"Remotely execute ClearML task using local-only repo",
        "Question_body":"<p>I want to execute ClearML task remotely. According to docs there are 2 options: 1) execute single python file; 2) ClearML would identify that script is part of repo, that repo will be cloned and installed into docker and executed on the worker.<\/p>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<p>I think it is somewhat extending scenario 1, where not a single file is passed for execution but whole directory with file in it.<\/p>\n<p>PS: i understand reproducibility concerns that arise, but repo is really not accessible from worker :(<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653499480970,
        "Question_score":0,
        "Question_tags":"clearml",
        "Question_view_count":25,
        "Owner_creation_time":1653498830777,
        "Owner_last_access_time":1662113390107,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<blockquote>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<\/blockquote>\n<p>well, no :( if your code is a single script, then yes ClearML would store the entire script, then the worker will reproduce it on the remote machine. But if your code base is composed of more than a single file, then why not use git? it is free hosted by GitHub, Bitbucket, GitLab etc.<\/p>\n<p>In theory this is doable and if you feel the need, I urge you to PR this feature. Basically you would store the entire folder as an artifact (ClearML will auto zip it for you), then the agent needs to unzip the artifact and run it. The main issue would be that cloning the Task will not clone the artifact...<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1659992618003,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72381916",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":62552414,
        "Question_title":"Can Trains config file be specified dynamically or relative to the running script path?",
        "Question_body":"<p>Suppose I have a server where many users run different experiments, possibly with different Trains Servers.<\/p>\n<p>I know about the <code>TRAINS_CONFIG_FILE<\/code> environment variable, but I wonder if this can be made more flexible in one of the following ways:<\/p>\n<ol>\n<li>Specifying the Trains config file dynamically, i.e. during runtime of the training script?<\/li>\n<li>Storing a config file in each of the training repos and specifying its path relatively to the running script path (instead of relatively to <code>~\/<\/code>)?<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592992723417,
        "Question_score":1,
        "Question_tags":"trains|clearml",
        "Question_view_count":129,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":1609531417760,
        "Answer_body":"<p>Disclaimer: I'm a member of Allegro Trains team<\/p>\n<ol>\n<li>Loading of the configuration is done at import time. This means that if you set the os environment before importing the package, you should be fine:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>os.environ['TRAINS_CONFIG_FILE']='~\/repo\/trains.conf'\nfrom trains import Task\n<\/code><\/pre>\n<ol start=\"2\">\n<li>The configuration file is loaded based on the current working directory, this means that if you have <code>os.environ['TRAINS_CONFIG_FILE']='trains.conf'<\/code> the trains.conf file will be loaded from the running directory at the time the import happens (which usually is the folder where your script is executed from). This means you can have it as part of the repository, and always set the <code>TRAINS_CONFIG_FILE<\/code> to point to it.<\/li>\n<\/ol>\n<p>A few notes:<\/p>\n<ul>\n<li>What is the use case for different configuration files ?<\/li>\n<li>Notice that when running with <a href=\"https:\/\/github.com\/allegroai\/trains-agent\" rel=\"nofollow noreferrer\">trains-agent<\/a> , this method will override the configuration that the <em>trains-agent<\/em> passes to the code.<\/li>\n<\/ul>",
        "Answer_comment_count":7,
        "Answer_creation_time":1592997291033,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62552414",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":65671395,
        "Question_title":"ClearML SSH port forwarding fileserver not available in WEB Ui",
        "Question_body":"<p>Trying to use clearml-server on own Ubuntu 18.04.5 with SSH Port Forwarding and not beeing able to see my debug samples.<\/p>\n<p>My setup:<\/p>\n<ul>\n<li>ClearML server on hostA<\/li>\n<li>SSH Tunnel connections to access Web App from working machine via localhost:18080<\/li>\n<li>Web App: <code>ssh -N -L 18081:127.0.0.1:8081 user@hostA<\/code><\/li>\n<li>Fileserver: <code>ssh -N -L 18081:127.0.0.1:8081 user@hostA<\/code><\/li>\n<\/ul>\n<p>In Web App under Task-&gt;Results-&gt;Debug Samples the Images are still refrenced by localhost:8081<\/p>\n<p>Where can I set the fileserver URL to be localhost:18081 in Web App?\nI tried ~\/clearml.conf, but this did not work ( I think it is for my python script ).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610384778623,
        "Question_score":1,
        "Question_tags":"devops|local|portforwarding|trains|clearml",
        "Question_view_count":121,
        "Owner_creation_time":1604391794420,
        "Owner_last_access_time":1663670435373,
        "Owner_location":null,
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Disclaimer: I'm a member of the ClearML team (formerly Trains)<\/p>\n<p>In ClearML, debug images' URL is registered once they are uploaded to the fileserver. The WebApp doesn't actually decide on the URL for each debug image, but rather obtains it for each debug image from the server. This allows you to potentially upload debug images to a variety of storage targets, ClearML File Server simply being the most convenient, built-in option.<\/p>\n<p>So, the WebApp will always look for <code>localhost:8008<\/code> for debug images that have already been uploaded to the fileserver and contain <code>localhost:8080<\/code> in their URL.\nA possible solution is to simply add another tunnel in the form of <code>ssh -N -L 8081:127.0.0.1:8081 user@hostA<\/code>.<\/p>\n<p>For future experiments, you can choose to keep using <code>8081<\/code> (and keep using this new tunnel), or to change the default fileserver URL in <code>clearml.conf<\/code> to point to port <code>localhost:18081<\/code>, assuming you're running your experiments from the same machine where the tunnel to <code>18081<\/code> exists.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1610390345337,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65671395",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":65655382,
        "Question_title":"ClearML server IP address not used with localhost and SSH port forwarding",
        "Question_body":"<p>Trying to use clearml-server on own Ubuntu 18.04.5.<\/p>\n<p>I use env variables to set the IP Address of my clearml-server.<\/p>\n<pre><code>export CLEARML_HOST_IP=127.0.0.1\nexport TRAINS_HOST_IP=127.0.0.1\n<\/code><\/pre>\n<p>But it still is available thorugh the external server IP.\nHow can I deactivate the listeners for external IP in clearml-server config?<\/p>\n<p>Edit:\nAccording to this:\nI use SSH Port forward to access local instance from my computer outside of the network. But I can't access custom uploaded images (task-&gt; debug samples) as they will not use my port forwarded URLs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610294996320,
        "Question_score":2,
        "Question_tags":"devops|local|portforwarding|trains|clearml",
        "Question_view_count":261,
        "Owner_creation_time":1604391794420,
        "Owner_last_access_time":1663670435373,
        "Owner_location":null,
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1610296597729,
        "Answer_body":"<p>Disclaimer: I'm a ClearML (Trains) team member<\/p>\n<p>Basically the docker-compose will expose only the API\/Web\/File server , you can further limit the exposure to your localhost only, by changing the following section in your ClearML server <a href=\"https:\/\/github.com\/allegroai\/clearml-server\/blob\/master\/docker\/docker-compose.yml\" rel=\"nofollow noreferrer\">docker-compose.yml<\/a><\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>networks:\n  backend:\n    driver:\n      bridge\n    driver_opts:\n      com.docker.network.bridge.host_binding_ipv4: &quot;127.0.0.1&quot;\n  frontend:\n    driver: \n      bridge\n    driver_opts:\n      com.docker.network.bridge.host_binding_ipv4: &quot;127.0.0.1&quot;\n\n<\/code><\/pre>\n<p>Based on docker's <a href=\"https:\/\/docs.docker.com\/engine\/reference\/commandline\/network_create\/\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1610303095157,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65655382",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":67496760,
        "Question_title":"Mounting an S3 bucket in docker in a clearml agent",
        "Question_body":"<p>What is the best practice for mounting an S3 container inside a docker image that will be using as a ClearML agent?  I can think of 3 solutions, but have been unable to get any to work currently:<\/p>\n<ol>\n<li>Use <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/use_cases\/clearml_agent_use_case_examples.html?highlight=docker\" rel=\"nofollow noreferrer\">prefabbed configuration in ClearML<\/a>, specifically CLEARML_AGENT_K8S_HOST_MOUNT.  For this to work, the S3 bucket would be mounted separately on the host using <a href=\"https:\/\/rclone.org\/\" rel=\"nofollow noreferrer\">rclone<\/a> and then remapped into docker. This appears to only apply to Kubernetes and not Docker - and therefore would not work.<\/li>\n<li>Mount using s3fuse as specified <a href=\"https:\/\/stackoverflow.com\/questions\/35189251\/docker-mount-s3-container\">here<\/a>.  The issue is will it work with the S3 bucket secret stored in ClearML browser sessions?  This would also appear to be complicated and require custom docker images, not to mention running the docker image as --privileged or similar.<\/li>\n<li>Pass arguments to docker using &quot;docker_args and docker_bash_setup_script arguments to Task.create()&quot; as specified in the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/release_notes\/ver_1_0.html\" rel=\"nofollow noreferrer\">1.0 release notes<\/a>.  This would be similar to (1), but the arguments would be for <a href=\"https:\/\/docs.docker.com\/storage\/bind-mounts\/\" rel=\"nofollow noreferrer\">bind-mounting the volume<\/a>.  I do not see much documentation or examples on how this new feature may be used for this end.<\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1620788331577,
        "Question_score":1,
        "Question_tags":"docker|amazon-s3|wsl-2|rclone|clearml",
        "Question_view_count":770,
        "Owner_creation_time":1362580980910,
        "Owner_last_access_time":1663351509150,
        "Owner_location":"Akron, OH, USA",
        "Owner_reputation":4013,
        "Owner_up_votes":94,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Question_last_edit_time":1621022250560,
        "Answer_body":"<p>I was able to get another option entirely to work, namely, mount a drive on in WSL and then pass it to Docker.  Let's get to it:<\/p>\n<p>Why not host in Windows itself, why rclone in WSL?<\/p>\n<ul>\n<li>Docker running on WSL <a href=\"https:\/\/github.com\/billziss-gh\/winfsp\/issues\/61\" rel=\"nofollow noreferrer\">cannot access drives mounted through winfsp<\/a> (what rclone uses)<\/li>\n<\/ul>\n<p>Steps to mount the drive in ClearML in Windows:<\/p>\n<ul>\n<li>You can install rclone in WSL and the mount will be accessible to docker\n<ul>\n<li>create the folder <code>\/data\/my-mount<\/code> (this needs to be in <code>\/data<\/code> - I don't know why and I can't find out with a Google search, but I found out about it <a href=\"https:\/\/forum.rclone.org\/t\/fusermount-permission-denied-in-docker-rclone\/13914\/5\" rel=\"nofollow noreferrer\">here<\/a>)<\/li>\n<li>You can put the configuration file in windows (use the <code>--config<\/code> option).<\/li>\n<li>Note: ClearML will not support spaces in mounted paths, even though docker will.  Therefore your path has to be <code>\/data\/my-mount<\/code> rather than <code>\/data\/my mount<\/code>.  There is a <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/358\" rel=\"nofollow noreferrer\">bug that I opened about this<\/a>.<\/li>\n<\/ul>\n<\/li>\n<li>You can test mounting by calling docker and mounting the file.\n<ul>\n<li>Example: <code>docker run -it -v \\\\wsl$\\Ubuntu\\data:\/data my-docker-image:latest ls \/data\/my-mount<\/code><\/li>\n<li>Note: You will have to mount \/data rather than \/data\/my-mount, otherwise you may get this error: <code>docker: Error response from daemon: error while creating mount source path<\/code><\/li>\n<\/ul>\n<\/li>\n<li>Now, you can setup the clearml.conf file in <code>C:\\Users\\Myself\\clearml.conf<\/code> such that:<\/li>\n<\/ul>\n<pre><code>default_docker: {\n   # default docker image to use when running in docker mode\n   image: &quot;my-docker-image:latest&quot;\n\n   # optional arguments to pass to docker image\n   arguments: [&quot;-v&quot;,&quot;\\\\wsl$\\Ubuntu\\data:\/data&quot;, ]\n}\n<\/code><\/pre>\n<ul>\n<li>Note that you can also run clearml-agent out of WSL and then would only need to specify <code>[&quot;-v&quot;,&quot;\/data:\/data&quot;, ]<\/code>.<\/li>\n<li>Run clearml agent in cmd: <code>clearml-agent daemon --docker<\/code><\/li>\n<\/ul>",
        "Answer_comment_count":0,
        "Answer_creation_time":1621009841940,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1621012052112,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67496760",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":66279581,
        "Question_title":"ClearML multiple tasks in single script changes logged value names",
        "Question_body":"<p>I trained multiple models with different configuration for a custom hyperparameter search. I use pytorch_lightning and its logging (TensorboardLogger).\nWhen running my training script after Task.init() ClearML auto-creates a Task and connects the logger output to the server.<\/p>\n<p>I log for each straining stage <code>train<\/code>, <code>val<\/code> and <code>test<\/code> the following scalars at each epoch: <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code><\/p>\n<p>When I have multiple configuration, e.g. <code>networkA<\/code> and <code>networkB<\/code> the first training log its values to <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code>, but the second to <code>networkB:loss<\/code>, <code>networkB:acc<\/code> and <code>networkB:iou<\/code>. This makes values umcomparable.<\/p>\n<p>My training loop with Task initalization looks like this:<\/p>\n<pre><code>names = ['networkA', networkB']\nfor name in names:\n     task = Task.init(project_name=&quot;NetworkProject&quot;, task_name=name)\n     pl_train(name)\n     task.close()\n<\/code><\/pre>\n<p>method pl_train is a wrapper for whole training with Pytorch Ligtning. No ClearML code is inside this method.<\/p>\n<p>Do you have any hint, how to properly use the usage of a loop in a script using completly separated tasks?<\/p>\n<hr \/>\n<p>Edit: ClearML version was 0.17.4. Issue is fixed in main branch.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1613745436903,
        "Question_score":1,
        "Question_tags":"devops|clearml|trains",
        "Question_view_count":279,
        "Owner_creation_time":1604391794420,
        "Owner_last_access_time":1663670435373,
        "Owner_location":null,
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1614004159640,
        "Answer_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p><code>pytorch_lightning<\/code> is creating a new Tensorboard for each experiment. When ClearML logs the TB scalars, and it captures the same scalar being re-sent again, it adds a prefix so if you are reporting the same metric it will not overwrite the previous one. A good example would be reporting <code>loss<\/code> scalar in the training phase vs validation phase (producing &quot;loss&quot; and &quot;validation:loss&quot;). It might be the <code>task.close()<\/code> call does not clear the previous logs, so it &quot;thinks&quot; this is the same experiment, hence adding the prefix <code>networkB<\/code> to the <code>loss<\/code>. As long as you are closing the Task after training is completed you should have all experiments log with the same metric\/variant (title\/series). I suggest opening a GitHub issue, this should probably be considered a bug.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1613773903383,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66279581",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":62332672,
        "Question_title":"Tracking separate train\/test processes with Trains",
        "Question_body":"<p>In my setup, I run a script that <strong>trains<\/strong> a model and starts generating checkpoints. Another script watches for new checkpoints and <strong>evaluates<\/strong> them. The scripts run in parallel, so evaluation is just a step behind training.<\/p>\n\n<p>What's the right Tracks configuration to support this scenario?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1591905931703,
        "Question_score":2,
        "Question_tags":"trains|clearml",
        "Question_view_count":99,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":1609771005756,
        "Answer_body":"<p>disclaimer: I'm part of the <a href=\"https:\/\/github.com\/allegroai\/trains\/\" rel=\"nofollow noreferrer\">allegro.ai Trains<\/a> team<\/p>\n<p>Do you have two experiments? one for testing one for training ?<\/p>\n<p>If you do have two experiments, then I would make sure the models are logged in both of them (which if they are stored on the same shared-folder\/s3\/etc will be automatic)\nThen you can quickly see the performance of each-one.<\/p>\n<p>Another option is sharing the same experiment, then the second process adds reports to the original experiment, that means that somehow you have to pass to it the experiment id.\nThen you can do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>task = Task.get_task(task_id='training_task_id`)\ntask.get_logger().report_scalar('title', 'loss', value=0.4, iteration=1)\n<\/code><\/pre>\n<p>EDIT:\nAre the two processes always launched together, or is the checkpoint test a general purpose code ?<\/p>\n<p>EDIT2:<\/p>\n<p>Let's assume you have main script training a model. This experiment has a unique task ID:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>my_uid = Task.current_task().id\n<\/code><\/pre>\n<p>Let's also assume you have a way to pass it to your second process (If this is an actual sub-process, it inherits the os environment variables so you could do <code>os.environ['MY_TASK_ID']=my_uid<\/code>)<\/p>\n<p>Then in the evaluation script you could report directly into the main training Task like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>train_task = Task.get_task(task_id=os.environ['MY_TASK_ID'])\ntrain_task.get_logger().report_scalar('title', 'loss', value=0.4, iteration=1)\n<\/code><\/pre>",
        "Answer_comment_count":4,
        "Answer_creation_time":1591906575913,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1592833417200,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62332672",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":63606182,
        "Question_title":"How should Trains be used with hyper-param optimization tools like RayTune?",
        "Question_body":"<p>What could be a reasonable setup for this? Can I call Task.init() multiple times in the same execution?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1598478428030,
        "Question_score":2,
        "Question_tags":"trains|clearml",
        "Question_view_count":107,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":1609427499190,
        "Answer_body":"<p>Disclaimer: I'm part of the allegro.ai Trains team<\/p>\n<p>One solution is to inherit from <a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/838c9cb0d2a5df5c193dfc85286abe59a80217c2\/trains\/automation\/optimization.py#L226\" rel=\"nofollow noreferrer\">trains.automation.optimization.SearchStrategy<\/a> and extend the functionality. This is similar to the <a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/master\/trains\/automation\/optuna\/optuna.py\" rel=\"nofollow noreferrer\">Optuna<\/a> integration, where Optuna is used for the Bayesian optimization and Trains does the hyper-parameter setting, launching experiments, and retrieving performance metrics.<\/p>\n<p>Another option (not scalable but probably easier to start with), is to use have the RayTuner run your code (obviously setting the environment \/ git repo \/ docker etc is on the user), and have your training code look something like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># create new experimnt\ntask = Task.init('hp optimization', 'ray-tuner experiment', reuse_last_task_id=False)\n# store the hyperparams (assuming hparam is a dict) \ntask.connect(hparam) \n# training loop here\n# ...\n# shutdown experimnt\ntask.close()\n<\/code><\/pre>\n<p>This means every time the RayTuner executes the script a new experiment will be created, with new set of hyper parameters (assuming <code>haparm<\/code> is a dictionary, it will be registered on the experiment as hyper-parameters)<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1598536233437,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63606182",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":64305945,
        "Question_title":"pip install trains fails",
        "Question_body":"<p>upon running <code>pip install trains<\/code> in my virtual env<\/p>\n<p>I am getting<\/p>\n<pre><code>    ERROR: Command errored out with exit status 1:\n     command: \/home\/epdadmin\/noam\/code\/venv_linux\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-lxz5t8pu\/install-record.txt --single-version-externally-managed --compile --install-headers \/home\/epdadmin\/noam\/code\/venv_linux\/include\/site\/python3.8\/retrying\n         cwd: \/tmp\/pip-install-owzh8lnl\/retrying\/\n    Complete output (10 lines):\n    running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib\n    copying retrying.py -&gt; build\/lib\n    running install_lib\n    copying build\/lib\/retrying.py -&gt; \/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\n    byte-compiling \/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\/retrying.py to retrying.cpython-38.pyc\n    error: [Errno 13] Permission denied: '\/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\/__pycache__\/retrying.cpython-38.pyc.139678407381360'\n    ----------------------------------------\nERROR: Command errored out with exit status 1: \/home\/epdadmin\/noam\/code\/venv_linux\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-lxz5t8pu\/install-record.txt --single-version-externally-managed --compile --install-headers \/home\/epdadmin\/noam\/code\/venv_linux\/include\/site\/python3.8\/retrying Check the logs for full command output.\n<\/code><\/pre>\n<p>I know that <a href=\"https:\/\/stackoverflow.com\/questions\/15028648\/is-it-acceptable-and-safe-to-run-pip-install-under-sudo\">I am not supposed to run under sudo when using a venv<\/a>, so I don't really understand the problem<\/p>\n<p>running for example <code>pip install pandas<\/code> does work.<\/p>\n<p>Python 3.8<\/p>\n<p>How to install trains?<\/p>\n<hr \/>\n<p>EDIT:<\/p>\n<p>running <code>pip install trains --user<\/code> or <code>pip install --user trains<\/code> gives<\/p>\n<pre><code>ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_time":1602431071093,
        "Question_score":1,
        "Question_tags":"python|pip|trains|clearml",
        "Question_view_count":1031,
        "Owner_creation_time":1314313109233,
        "Owner_last_access_time":1664062223147,
        "Owner_location":"Technion, Israel",
        "Owner_reputation":18777,
        "Owner_up_votes":2376,
        "Owner_down_votes":137,
        "Owner_views":2000,
        "Question_last_edit_time":1609353956110,
        "Answer_body":"<p>The problem was a permissions problem for the venv.\nAnother problem was trains required some packages that were not yet available with wheels on Python3.8, so I had to downgrade Python to 3.7<\/p>\n<p>That venv was created using Pycharm, and for some reason it was created with low permissions.<\/p>\n<p>There was probably a way to elevate its permissions, but instead I just deleted it and created another one using command line by<\/p>\n<pre><code>python -m virtualenv --python=\/usr\/bin\/python3.7 venv\n<\/code><\/pre>\n<p>And now <code>pip install trains<\/code> worked.<\/p>\n<p>Very annoying.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1602767930970,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1604308183096,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64305945",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":70397010,
        "Question_title":"What would stop credentials from validation on a ClearML server?",
        "Question_body":"<p>I've set up a ClearML server in GCP using the sub-domain approach. I can access all three domains (<code>https:\/\/app.clearml.mydomain.com<\/code>, <code>https:\/\/api.clearml.mydomain.com<\/code> and <code>https:\/\/files.clearml.mydomain.com<\/code>) in a browser and see what I think is the correct response, but when connecting with the python SDK via <code>clearml-init<\/code> I get the following error:<\/p>\n<pre><code>clearml.backend_api.session.session.LoginError: Failed getting token (error 400 from https:\/\/api.clearml.mydomain.com): Bad Request\n<\/code><\/pre>\n<p>Are there any likely causes of this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639763350487,
        "Question_score":1,
        "Question_tags":"clearml",
        "Question_view_count":194,
        "Owner_creation_time":1600124498003,
        "Owner_last_access_time":1661436804020,
        "Owner_location":null,
        "Owner_reputation":46,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Following the discussion <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/517\" rel=\"nofollow noreferrer\">here<\/a>, it seemed that the load balancer being used was blocking <code>GET<\/code> requests with a payload which are used by ClearML. A <a href=\"https:\/\/github.com\/allegroai\/clearml\/pull\/521\" rel=\"nofollow noreferrer\">fix<\/a> is being worked on to allow the method to be changed to a <code>POST<\/code> request via an environment variable.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1640162037740,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70397010",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":68798737,
        "Question_title":"ClearML How to get configurable hyperparameters?",
        "Question_body":"<p>How do I get args like epochs to show up in the UI configuration panel under hyperparameters? I want to be able to change number of epochs and learning rate from within the UI.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1629097990217,
        "Question_score":0,
        "Question_tags":"devops|mlops|clearml|trains",
        "Question_view_count":110,
        "Owner_creation_time":1616008398583,
        "Owner_last_access_time":1652343370277,
        "Owner_location":"Islamabad, Pakistan",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use <code>argparse<\/code> - ClearML will auto-magically log all parameters in the task's configuration section (under hyper-parameters section) - see <a href=\"https:\/\/github.com\/allegroai\/clearml\/blob\/master\/examples\/frameworks\/keras\/keras_tensorboard.py#L56\" rel=\"nofollow noreferrer\">this<\/a> example. You can also just connect any dictionary (see <a href=\"https:\/\/github.com\/allegroai\/clearml\/blob\/master\/examples\/frameworks\/ignite\/cifar_ignite.py#L23\" rel=\"nofollow noreferrer\">this<\/a> example)<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1629122761020,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68798737",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":73632015,
        "Question_title":"ClearML, how to query the best performing model for a specific project and metric",
        "Question_body":"<p>I want to download the best performing model for a certain ClearlML project. I have the following content in my ClearML experiment platform:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/LAWT9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LAWT9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>According to: <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/clearml_sdk\/model_sdk#querying-models\" rel=\"nofollow noreferrer\">https:\/\/clear.ml\/docs\/latest\/docs\/clearml_sdk\/model_sdk#querying-models<\/a> I can get a list of models for a specific project:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_list = Model.query_models(\n    # Only models from `examples` project\n    project_name='YOLOv5', \n    # Only models with input name\n    model_name=None,\n    # Only models with `demo` tag but without `TF` tag\n    tags=['demo', '-TF'],\n    # If `True`, only published models\n    only_published=False,\n    # If `True`, include archived models\n    include_archived=True,\n    # Maximum number of models returned\n    max_results=5\n)\n\nprint(model_list)\n<\/code><\/pre>\n<p>Which prints:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>[&lt;clearml.model.Model object at 0x7fefbaf22130&gt;, &lt;clearml.model.Model object at 0x7fefbaf22340&gt;]\n<\/code><\/pre>\n<p>So I can run:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_list[0].get_local_copy()\n<\/code><\/pre>\n<p>and get this specific model. But how do I download the best performing one for this project on a certain metric (in this case mAP_0.5:0.95 MAX)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662537777000,
        "Question_score":0,
        "Question_tags":"python|yolov5|clearml",
        "Question_view_count":48,
        "Owner_creation_time":1521276815913,
        "Owner_last_access_time":1664082149740,
        "Owner_location":null,
        "Owner_reputation":731,
        "Owner_up_votes":56,
        "Owner_down_votes":8,
        "Owner_views":41,
        "Question_last_edit_time":1662645914387,
        "Answer_body":"<p>I ended up doing the following:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>try:\n    import clearml\n    from clearml import Dataset, Task, Model, OutputModel\n    assert hasattr(clearml, '__version__')  # verify package import not local dir\nexcept (ImportError, AssertionError):\n    clearml = None\n\ntasks = Task.get_tasks(project_name='YOLOv5', task_name='exp', task_filter={'status': ['completed']})\n\nresults = {}\nbest_task = None\nfor task in tasks:\n    results[task.id] = task.get_last_scalar_metrics()['metrics']['mAP_0.5:0.95']['max']\n\nbest_model_task_id = max(results, key=results.get)\nmodel_list = Task.get_task(best_model_task_id).get_models()\ndest = model_list['output'][0].get_local_copy()\nprint('Saved model at:', dest)\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1662538633323,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1662553432007,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73632015",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":66216294,
        "Question_title":"ClearML how to change clearml.conf file in AWS Sagemaker",
        "Question_body":"<p>I am working in AWS Sagemaker Jupyter notebook.\nI have installed clearml package in AWS Sagemaker in Jupyter.\nClearML server was installed on AWS EC2.\nI need to store artifacts and models in AWS S3 bucket, so I want to specify credentials to S3 in clearml.conf file.\nHow can I change clearml.conf file in AWS Sagemaker instance? looks like permission denied to all folders on it.\nOr maybe somebody can suggest a better approach.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1613428648627,
        "Question_score":2,
        "Question_tags":"clearml",
        "Question_view_count":276,
        "Owner_creation_time":1585870038407,
        "Owner_last_access_time":1626728054187,
        "Owner_location":"Christchurch, \u041d\u043e\u0432\u0430\u044f \u0417\u0435\u043b\u0430\u043d\u0434\u0438\u044f",
        "Owner_reputation":45,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p>To set credentials (and <code>clearml-server<\/code> hosts) you can use <code>Task.set_credentials<\/code>.\nTo specify the S3 bucket as output for all artifacts (and debug images for that matter) you can just set it as the <code>files_server<\/code>.<\/p>\n<p>For example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\n\nTask.set_credentials(api_host='http:\/\/clearml-server:8008', web_host='http:\/\/clearml-server:8080', files_host='s3:\/\/my_bucket\/folder\/',\nkey='add_clearml_key_here', secret='add_clearml_key_secret_here')\n<\/code><\/pre>\n<p>To pass your S3 credentials, just add a cell at the top of your jupyter notebook, and set the standard AWS S3 environment variables:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nos.environ['AWS_ACCESS_KEY_ID'] = 's3_bucket_key_here'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 's3_bucket_secret_here'\n# optional\nos.environ['AWS_DEFAULT_REGION'] = 's3_bucket_region'\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1613774515323,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66216294",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":66320238,
        "Question_title":"ClearML get max value from logged values",
        "Question_body":"<p>I use ClearML to track my tensorboard logs (from PyTorch Lightning) during training.\nAt a point later I start another script which connects to existing task and do some testing.<\/p>\n<p>But unfortenautly I do not have all information in the second script, so I want to query them from the logged values from ClearML server.<\/p>\n<p>How would I do this?<\/p>\n<p>I thought about something like this, but havn't found anything in documentation:<\/p>\n<pre><code>task = Task.init(project_name=&quot;Project&quot;, task_name=&quot;name&quot;, reuse_last_task_id=&quot;Task_id, continue_last_task=True)\nx_value, y_value = task.get_value(key=&quot;val\/acc&quot;, mode=&quot;max&quot;)\nx_value2, y_value2 = task.get_value(key=&quot;epoch&quot;, mode=&quot;x&quot;, x=x_value)\n<\/code><\/pre>\n<ul>\n<li><code>x_value<\/code> would be my epoch or global step<\/li>\n<li><code>y_value<\/code> the maximum value of plot &quot;val\/acc&quot;<\/li>\n<li><code>x_value2<\/code> would be my epoch or global step<\/li>\n<li><code>y_value2<\/code> the value of plot &quot;epoch&quot; at <code>x_value<\/code><\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1614013817123,
        "Question_score":3,
        "Question_tags":"devops|clearml|trains",
        "Question_view_count":101,
        "Owner_creation_time":1604391794420,
        "Owner_last_access_time":1663670435373,
        "Owner_location":null,
        "Owner_reputation":89,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1614165129956,
        "Answer_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p>To get an existing <code>Task<\/code> object for a running (or completed\/failed) experiment, assuming we know Task ID:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>another_task = Task.get_task(task_id='aabbcc')\n<\/code><\/pre>\n<p>If we only know the Task project\/name<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>another_task = Task.get_task(project_name='the project', task_name='the name')\n<\/code><\/pre>\n<p>Notice that if you have multiple task under the same name it will return the most updated one.\nOnce we have the <code>Task<\/code> object, we can do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>latest_scalar_values_dict = another_task.get_last_scalar_metrics()\n<\/code><\/pre>\n<p>Which would return all the scalars min\/maxm\/last values, for example:<\/p>\n<pre><code>latest_scalar_values_dict = {\n            'title': {\n                'series': {\n                    'last': 0.5,\n                    'min': 0.1,\n                    'max': 0.9\n                    }\n                }\n            }\n<\/code><\/pre>\n<p><a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/references\/clearml_python_ref\/task_module\/task_task.html?highlight=get%20metrics#clearml.task.Task.get_last_scalar_metrics\" rel=\"nofollow noreferrer\">documentation here<\/a><\/p>\n<p>If you need to get the entire graphs you can use <code>task.get_reported_scalars()<\/code> <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/references\/clearml_python_ref\/task_module\/task_task.html?highlight=get_reported_scalars#clearml.task.Task.get_reported_scalars\" rel=\"nofollow noreferrer\">see docs<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1614215061400,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66320238",
        "Question_exclusive_tag":"ClearML"
    },
    {
        "Question_id":46368389,
        "Question_title":"How to configure comet (comet.ml) to log Tensorflow?",
        "Question_body":"<p>Im trying to set comet (<a href=\"https:\/\/www.comet.ml\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml<\/a>) to track my Tensorflow experiment, after I create an Experiment and log the data set i dont get the accuracy in my report.<\/p>\n\n<p>my code:<\/p>\n\n<pre><code>mnist = get_data()\ntrain_step, cross_entropy, accuracy, x, y, y_ = build_model_graph(hyper_params)\n\nexperiment = Experiment(api_key=\"XXXX\", log_code=True)\nexperiment.log_multiple_params(hyper_params)\nexperiment.log_dataset_hash(mnist)\n<\/code><\/pre>\n\n<p>in the example account : <a href=\"https:\/\/www.comet.ml\/view\/Jon-Snow\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml\/view\/Jon-Snow<\/a> I see that accuracy is reported<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1506094686057,
        "Question_score":3,
        "Question_tags":"tensorflow|machine-learning|comet-ml",
        "Question_view_count":338,
        "Owner_creation_time":1506066897167,
        "Owner_last_access_time":1506108624433,
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1514341154200,
        "Answer_body":"<p>you can report accuracy using this method:<\/p>\n\n<ul>\n<li><code>experiment.log_accuracy(train_accuracy)<\/code><\/li>\n<\/ul>\n\n<p>take a look at the full Tensorflow example in our guide:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\/tree\/master\/tensorflow\" rel=\"nofollow noreferrer\">https:\/\/github.com\/comet-ml\/comet-quickstart-guide\/tree\/master\/tensorflow<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":1,
        "Answer_creation_time":1506100257933,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1513514205487,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46368389",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":61239274,
        "Question_title":"Google Colab freezes my browser and pc when trying to reconnect to a notebook",
        "Question_body":"<p>I am training a Machine learning model in google colab, to be more specific I am training a GAN with PyTorch-lightning. The problem occurs is when I get disconnected from my current runtime due to inactivity. When I try to reconnect my Browser(tried on firefox and chrome) becomes first laggy and than freezes, my pc starts to lag so that I am not able to close my browser and it doesn't go away. I am forced to press the power button of my PC in order to restart the PC.\nI have no clue why this happens.\nI tried various batch sizes(also the size 1) but it still happens. It can't be that my dataset is too big either(since i tried it on a dataset with 10images for testing puposes).\nI hope someone can help me.<\/p>\n\n<p>Here is my code (For using the code you will need comet.nl and enter the comet.ml api key):<\/p>\n\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision  \nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\n\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport pytorch_lightning as pl\nfrom pytorch_lightning import loggers\n\nimport numpy as np\nfrom numpy.random import choice\n\nfrom PIL import Image\n\nimport os\nfrom pathlib import Path\nimport shutil\n\nfrom collections import OrderedDict\n\n# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n# randomly flip some labels\ndef noisy_labels(y, p_flip=0.05):  # # flip labels with 5% probability\n    # determine the number of labels to flip\n    n_select = int(p_flip * y.shape[0])\n    # choose labels to flip\n    flip_ix = choice([i for i in range(y.shape[0])], size=n_select)\n    # invert the labels in place\n    y[flip_ix] = 1 - y[flip_ix]\n    return y\n\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0.0, std=0.1):\n        self.std = std\n        self.mean = mean\n\n    def __call__(self, tensor):\n        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\ndef get_valid_labels(img):\n  return (0.8 - 1.1) * torch.rand(img.shape[0], 1, 1, 1) + 1.1  # soft labels\n\ndef get_unvalid_labels(img):\n  return noisy_labels((0.0 - 0.3) * torch.rand(img.shape[0], 1, 1, 1) + 0.3)  # soft labels\n\nclass Generator(nn.Module):\n    def __init__(self, ngf, nc, latent_dim):\n        super(Generator, self).__init__()\n        self.ngf = ngf\n        self.latent_dim = latent_dim\n        self.nc = nc\n\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d(latent_dim, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n             nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass Discriminator(nn.Module):\n    def __init__(self, ndf, nc):\n        super(Discriminator, self).__init__()\n        self.nc = nc\n        self.ndf = ndf\n\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass DCGAN(pl.LightningModule):\n\n    def __init__(self, hparams, logger, checkpoint_folder, experiment_name):\n        super().__init__()\n        self.hparams = hparams\n        self.logger = logger  # only compatible with comet_logger at the moment\n        self.checkpoint_folder = checkpoint_folder\n        self.experiment_name = experiment_name\n\n        # networks\n        self.generator = Generator(ngf=hparams.ngf, nc=hparams.nc, latent_dim=hparams.latent_dim)\n        self.discriminator = Discriminator(ndf=hparams.ndf, nc=hparams.nc)\n        self.generator.apply(weights_init)\n        self.discriminator.apply(weights_init)\n\n        # cache for generated images\n        self.generated_imgs = None\n        self.last_imgs = None\n\n        # For experience replay\n        self.exp_replay_dis = torch.tensor([])\n\n        # creating checkpoint folder\n        dirpath = Path(self.checkpoint_folder)\n        if not dirpath.exists():\n          os.makedirs(dirpath, 0o755)\n\n    def forward(self, z):\n        return self.generator(z)\n\n    def adversarial_loss(self, y_hat, y):\n        return F.binary_cross_entropy(y_hat, y)\n\n    def training_step(self, batch, batch_nb, optimizer_idx):\n        # For adding Instance noise for more visit: https:\/\/www.inference.vc\/instance-noise-a-trick-for-stabilising-gan-training\/\n        std_gaussian = max(0, self.hparams.level_of_noise - ((self.hparams.level_of_noise * 1.5) * (self.current_epoch \/ self.hparams.epochs)))\n        AddGaussianNoiseInst = AddGaussianNoise(std=std_gaussian) # the noise decays over time\n\n        imgs, _ = batch\n        imgs = AddGaussianNoiseInst(imgs) # Adding instance noise to real images\n        self.last_imgs = imgs\n\n        # train generator\n        if optimizer_idx == 0:\n            # sample noise\n            z = torch.randn(imgs.shape[0], self.hparams.latent_dim, 1, 1)\n\n            # generate images\n            self.generated_imgs = self(z)\n            self.generated_imgs = AddGaussianNoiseInst(self.generated_imgs) # Adding instance noise to fake images\n\n            # Experience replay\n            # for discriminator\n            perm = torch.randperm(self.generated_imgs.size(0))  # Shuffeling\n            r_idx = perm[:max(1, self.hparams.experience_save_per_batch)]  # Getting the index\n            self.exp_replay_dis = torch.cat((self.exp_replay_dis, self.generated_imgs[r_idx]), 0).detach()  # Add our new example to the replay buffer\n\n            # ground truth result (ie: all fake)\n            g_loss = self.adversarial_loss(self.discriminator(self.generated_imgs), get_valid_labels(self.generated_imgs)) # adversarial loss is binary cross-entropy\n\n            tqdm_dict = {'g_loss': g_loss}\n            log = {'g_loss': g_loss, \"std_gaussian\": std_gaussian}\n            output = OrderedDict({\n                'loss': g_loss,\n                'progress_bar': tqdm_dict,\n                'log': log\n            })\n            return output\n\n        # train discriminator\n        if optimizer_idx == 1:\n            # Measure discriminator's ability to classify real from generated samples\n            # how well can it label as real?\n            real_loss = self.adversarial_loss(self.discriminator(imgs), get_valid_labels(imgs))\n\n            # Experience replay\n            if self.exp_replay_dis.size(0) &gt;= self.hparams.experience_batch_size:\n              fake_loss = self.adversarial_loss(self.discriminator(self.exp_replay_dis.detach()), get_unvalid_labels(self.exp_replay_dis))  # train on already seen images\n\n              self.exp_replay_dis = torch.tensor([]) # Reset experience replay\n\n              # discriminator loss is the average of these\n              d_loss = (real_loss + fake_loss) \/ 2\n\n              tqdm_dict = {'d_loss': d_loss}\n              log = {'d_loss': d_loss, \"d_exp_loss\": fake_loss, \"std_gaussian\": std_gaussian}\n              output = OrderedDict({\n                  'loss': d_loss,\n                  'progress_bar': tqdm_dict,\n                  'log': log\n              })\n              return output\n\n            else:\n              fake_loss = self.adversarial_loss(self.discriminator(self.generated_imgs.detach()), get_unvalid_labels(self.generated_imgs))  # how well can it label as fake?\n\n              # discriminator loss is the average of these\n              d_loss = (real_loss + fake_loss) \/ 2\n\n              tqdm_dict = {'d_loss': d_loss}\n              log = {'d_loss': d_loss, \"std_gaussian\": std_gaussian}\n              output = OrderedDict({\n                  'loss': d_loss,\n                  'progress_bar': tqdm_dict,\n                  'log': log\n              })\n              return output\n\n    def configure_optimizers(self):\n        lr = self.hparams.lr\n        b1 = self.hparams.b1\n        b2 = self.hparams.b2\n\n        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n        return [opt_g, opt_d], []\n\n    def train_dataloader(self):\n        transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize([0.5], [0.5])])\n        dataset = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n        return DataLoader(dataset, batch_size=self.hparams.batch_size)\n        # transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n        #                                 transforms.ToTensor(),\n        #                                 transforms.Normalize([0.5], [0.5])\n        #                                 ])\n\n        # train_dataset = torchvision.datasets.ImageFolder(\n        #     root=\".\/drive\/My Drive\/datasets\/ghibli_dataset_small_overfit\/\",\n        #     transform=transform\n        # )\n        # return DataLoader(train_dataset, num_workers=self.hparams.num_workers, shuffle=True, batch_size=self.hparams.batch_size)\n\n    def on_epoch_end(self):\n        z = torch.randn(4, self.hparams.latent_dim, 1, 1)\n        # match gpu device (or keep as cpu)\n        if self.on_gpu:\n            z = z.cuda(self.last_imgs.device.index)\n\n        # log sampled images\n        sample_imgs = self.generator(z)\n        sample_imgs = sample_imgs.view(-1, self.hparams.nc, self.hparams.image_size, self.hparams.image_size)\n        grid = torchvision.utils.make_grid(sample_imgs, nrow=2)\n        self.logger.experiment.log_image(grid.permute(1, 2, 0), f'generated_images_epoch{self.current_epoch}', step=self.current_epoch)\n\n        # save model\n        if self.current_epoch % self.hparams.save_model_every_epoch == 0:\n          trainer.save_checkpoint(self.checkpoint_folder + \"\/\" + self.experiment_name + \"_epoch_\" + str(self.current_epoch) + \".ckpt\")\n          comet_logger.experiment.log_asset_folder(self.checkpoint_folder, step=self.current_epoch)\n\n          # Deleting the folder where we saved the model so that we dont upload a thing twice\n          dirpath = Path(self.checkpoint_folder)\n          if dirpath.exists() and dirpath.is_dir():\n                shutil.rmtree(dirpath)\n\n          # creating checkpoint folder\n          access_rights = 0o755\n          os.makedirs(dirpath, access_rights)\n\nfrom argparse import Namespace\n\nargs = {\n    'batch_size': 48,\n    'lr': 0.0002,\n    'b1': 0.5,\n    'b2': 0.999,\n    'latent_dim': 128, # tested value which worked(in V4_1): 100\n    'nc': 1,\n    'ndf': 32,\n    'ngf': 32,\n    'epochs': 10,\n    'save_model_every_epoch': 5,\n    'image_size': 64,\n    'num_workers': 2,\n    'level_of_noise': 0.15,\n    'experience_save_per_batch': 1, # this value should be very low; tested value which works: 1\n    'experience_batch_size': 50 # this value shouldnt be too high; tested value which works: 50\n}\nhparams = Namespace(**args)\n\n# Parameters\nexperiment_name = \"DCGAN_V4_2_MNIST\"\ndataset_name = \"MNIST\"\ncheckpoint_folder = \"DCGAN\/\"\ntags = [\"DCGAN\", \"MNIST\", \"OVERFIT\", \"64x64\"]\ndirpath = Path(checkpoint_folder)\n\n# init logger\ncomet_logger = loggers.CometLogger(\n    api_key=\"\",\n    rest_api_key=\"\",\n    project_name=\"gan\",\n    experiment_name=experiment_name,\n    #experiment_key=\"f23d00c0fe3448ee884bfbe3fc3923fd\"  # used for resuming trained id can be found in comet.ml\n)\n\n#defining net\nnet = DCGAN(hparams, comet_logger, checkpoint_folder, experiment_name)\n\n#logging\ncomet_logger.experiment.set_model_graph(str(net))\ncomet_logger.experiment.add_tags(tags=tags)\ncomet_logger.experiment.log_dataset_info(dataset_name)\n\ntrainer = pl.Trainer(#resume_from_checkpoint=\"GHIBLI_DCGAN_OVERFIT_64px_epoch_6000.ckpt\",\n                     logger=comet_logger,\n                     max_epochs=args[\"epochs\"]\n                     )\ntrainer.fit(net)\ncomet_logger.experiment.end()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586987562523,
        "Question_score":0,
        "Question_tags":"crash|google-colaboratory|freeze|comet-ml",
        "Question_view_count":1013,
        "Owner_creation_time":1493314794173,
        "Owner_last_access_time":1658745180780,
        "Owner_location":"Germany",
        "Owner_reputation":844,
        "Owner_up_votes":241,
        "Owner_down_votes":6,
        "Owner_views":170,
        "Question_last_edit_time":1587130797009,
        "Answer_body":"<p>I fixed it with importing this:<\/p>\n\n<pre><code>from IPython.display import clear_output \n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1587034953183,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61239274",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":46359436,
        "Question_title":"How to configure comet (comet.ml) to track Keras?",
        "Question_body":"<p>im trying to setup <a href=\"https:\/\/www.comet.ml\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml<\/a> to log my experiment details <\/p>\n\n<p>getting strange error:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"train.py\", line 7, in &lt;module&gt;\n    from comet_ml import Experiment\nImportError: No module named comet_ml\n<\/code><\/pre>\n\n<p>trying in python 2 and python3<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1506066265147,
        "Question_score":3,
        "Question_tags":"python|keras|comet|comet-ml",
        "Question_view_count":1208,
        "Owner_creation_time":1505841491573,
        "Owner_last_access_time":1506108675090,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1506066568087,
        "Answer_body":"<p>it seems like comet isn't installed on your machine.<\/p>\n\n<p>you can use :<\/p>\n\n<pre><code>pip3 install comet_ml\npip install comet_ml\n<\/code><\/pre>\n\n<p>take a look at the example projects at: <\/p>\n\n<p><a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\" rel=\"nofollow noreferrer\">https:\/\/github.com\/comet-ml\/comet-quickstart-guide<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1506066553020,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1513514919392,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46359436",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":46352435,
        "Question_title":"comet (comet-ml) fails to run with Keras",
        "Question_body":"<p>Im running the keras examples from <a href=\"https:\/\/github.com\/comet-ml\/comet-keras-example\/blob\/master\/comet_keras_example.py\" rel=\"nofollow noreferrer\">Comet github project<\/a> .<\/p>\n\n<p>I add the import and create a new experiment:<\/p>\n\n<pre><code>def train(x_train,y_train,x_test,y_test):\nmodel = build_model_graph()\n\nfrom comet_ml import Experiment\n\nexperiment = Experiment(api_key=\"XXXX\", log_code=True)\n\nmodel.fit(x_train, y_train, batch_size=128, epochs=50, validation_data=(x_test, y_test))\n\nscore = model.evaluate(x_test, y_test, verbose=0)\n<\/code><\/pre>\n\n<p>and when i run my training code it fails.<\/p>\n\n<p>error:<\/p>\n\n<pre><code>Using TensorFlow backend.\nTraceback (most recent call last):\n  File \"\/Users\/nimrodlahav\/Code\/semantica\/experiment-logger-client\/train-examples\/keras-example.py\", line 21, in &lt;module&gt;\n    from comet_ml import Experiment\n  File \"..\/.\/comet-client-lib\/comet_ml\/__init__.py\", line 3, in &lt;module&gt;\n    from .comet import Experiment\n  File \"..\/.\/comet-client-lib\/comet_ml\/comet.py\", line 29, in &lt;module&gt;\n    from comet_ml import keras_logger\n  File \"..\/.\/comet-client-lib\/comet_ml\/keras_logger.py\", line 31, in &lt;module&gt;\n    raise SyntaxError(\"Please import Comet before importing any keras modules\")\nSyntaxError: Please import Comet before importing any keras modules\n<\/code><\/pre>\n\n<p>what am I missing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1506024659377,
        "Question_score":2,
        "Question_tags":"python|tensorflow|keras|comet|comet-ml",
        "Question_view_count":601,
        "Owner_creation_time":1505841491573,
        "Owner_last_access_time":1506108675090,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1506491876827,
        "Answer_body":"<p>I don't see start of the code but it looks like you have imported Keras before you have imported Comet.<\/p>\n\n<p>From the error message it looks like just need to switch the import lines (Comet first Keras second), like in your example:<\/p>\n\n<pre><code>from comet_ml import Experiment\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import RMSprop \n<\/code><\/pre>\n\n<p>view the full source code <a href=\"https:\/\/github.com\/comet-ml\/comet-keras-example\" rel=\"nofollow noreferrer\">example<\/a> .<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1506026797450,
        "Answer_score":3,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1506066589407,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46352435",
        "Question_exclusive_tag":"Comet"
    },
    {
        "Question_id":52871630,
        "Question_title":"Resolving paths in mingw fails with Data Version Control",
        "Question_body":"<p>I am following the <a href=\"https:\/\/blog.dataversioncontrol.com\/data-version-control-tutorial-9146715eda46\" rel=\"nofollow noreferrer\">tutorial<\/a> about <a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"nofollow noreferrer\">Data Version Control<\/a> using <code>mingw32<\/code> on Windows 7.<\/p>\n\n<p>I am getting very strange error when I try to use <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/run\" rel=\"nofollow noreferrer\">run<\/a>:<\/p>\n\n<pre><code>$ dvc run -v echo \"hello\"\nDebug: updater is not old enough to check for updates\nDebug: PRAGMA user_version;\nDebug: fetched: [(2,)]\nDebug: CREATE TABLE IF NOT EXISTS state (inode INTEGER PRIMARY KEY, mtime TEXT NOT NULL, md5 TEXT NOT NULL, timestamp TEXT NOT NULL)\nDebug: CREATE TABLE IF NOT EXISTS state_info (count INTEGER)\nDebug: CREATE TABLE IF NOT EXISTS link_state (path TEXT PRIMARY KEY, inode INTEGER NOT NULL, mtime TEXT NOT NULL)\nDebug: INSERT OR IGNORE INTO state_info (count) SELECT 0 WHERE NOT EXISTS (SELECT * FROM state_info)\nDebug: PRAGMA user_version = 2;\nRunning command:\n        echo hello\n\/c: \/c: Is a directory\nDebug: SELECT count from state_info WHERE rowid=1\nDebug: fetched: [(1,)]\nDebug: UPDATE state_info SET count = 1 WHERE rowid = 1\nError: Traceback (most recent call last):\n  File \"dvc\\command\\run.py\", line 18, in run\n  File \"dvc\\project.py\", line 265, in run\n  File \"dvc\\stage.py\", line 435, in run\nStageCmdFailedError: Stage 'Dvcfile' cmd echo hello failed\n\nError: Failed to run command: Stage 'Dvcfile' cmd echo hello failed\n<\/code><\/pre>\n\n<h3>Question:<\/h3>\n\n<p>Where does the <code>\/c: \/c: Is a directory<\/code> come from?  How can I fix it? <\/p>\n\n<h3>My findings<\/h3>\n\n<ol>\n<li><p>I supposed that it was resolving path to echo, but ech is a builtin.<\/p>\n\n<pre><code>$ type echo\necho is a shell builtin\n<\/code><\/pre>\n\n<p>I tried also with <code>exit<\/code> and <code>cd<\/code> but I am getting the same error.<\/p><\/li>\n<li><p>Calling commands without dvc works fine.<\/p><\/li>\n<li><p><code>dvc<\/code> with <code>--no-exec<\/code> flag works fine, but when later executed with <code>repro<\/code> gives the same error. <\/p><\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1539857086453,
        "Question_score":4,
        "Question_tags":"windows|mingw|dvc",
        "Question_view_count":109,
        "Owner_creation_time":1508231047660,
        "Owner_last_access_time":1663539768267,
        "Owner_location":"Krak\u00f3w, Poland",
        "Owner_reputation":860,
        "Owner_up_votes":658,
        "Owner_down_votes":18,
        "Owner_views":118,
        "Question_last_edit_time":1539881078543,
        "Answer_body":"<p>I'm one of the dvc developers. Similar error has affected dvc running on cygwin. We've released a fix for it in <code>0.20.0<\/code>. Please upgrade.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1540629268433,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52871630",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56818930,
        "Question_title":"\"dvc push\" after several local commits",
        "Question_body":"<p>I work on a project with DVC (Data version control). Let's say I make a lot of local commits. Something like this:<\/p>\n\n<pre><code># make changes for experiment 1\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 1\"\n\n# make changes for experiment 2\n# which change both code and data\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 2\"\n\n# make changes for experiment 3\n# which change both code and data\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 3\"\n\n# Finally I'm done\n# push changes:\ndvc push\ngit push\n<\/code><\/pre>\n\n<p>However there is one problem: <code>dvc push<\/code> will only push data from experiment 3. Is there any way to push data from all local commits (i.e. starting from the first commit diverged from remote branch)?<\/p>\n\n<p>Currently I see two options:<\/p>\n\n<ol>\n<li>Tag each commit and push it with <code>dvc push -T<\/code><\/li>\n<li>After \"expermient 3\" commit execute <code>git checkout commit-hash &amp;&amp; dvc push<\/code> for all local commits not yet pushed to remote.<\/li>\n<\/ol>\n\n<p>Both these options seem cumbersome and error-prone. Is there any better way to do it?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1561823734517,
        "Question_score":2,
        "Question_tags":"git|dvc",
        "Question_view_count":916,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":null,
        "Answer_body":"<p>@NShiny, there is a related ticket:<\/p>\n\n<p><a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1691\" rel=\"nofollow noreferrer\">support push\/pull\/metrics\/gc, etc across different commits<\/a>.<\/p>\n\n<p>Please, give it a vote so that we know how to prioritize it.<\/p>\n\n<p>As a workaround, I would recommend to run <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/install\" rel=\"nofollow noreferrer\"><code>dvc install<\/code><\/a>. It installs a <code>pre-push<\/code> GIt hook and runs <code>dvc push<\/code> automatically:<\/p>\n\n<pre><code>Git pre-push hook executes dvc push before git push to upload files and directories under DVC control to remote.\n<\/code><\/pre>\n\n<p>It means, though you need to run <code>git push<\/code> after every <code>git commit<\/code> :(<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1561842649417,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1561847539150,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56818930",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73651050,
        "Question_title":"DVC imports authentication to blob storage",
        "Question_body":"<p>I'm using <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> to track and version data that is stored locally on the file system and in Azure Blob storage.<\/p>\n<p>My setup is as follows:<\/p>\n<ul>\n<li><p><code>DataProject1<\/code>, it uses a local file location as a remote therefore it does not require any authentication.<\/p>\n<\/li>\n<li><p><code>DataProject2<\/code>, it uses Azure Blob Storage as a remote, it is using sas_token for authentication, I can push pull data to\/from the remote when I'm within this project.<\/p>\n<\/li>\n<li><p><code>MLProject<\/code>, it uses dvc import to import data from <code>DataProjec1<\/code> and <code>DataProject2<\/code>.<\/p>\n<\/li>\n<\/ul>\n<p>When I run the import with the command against <code>DataProject1<\/code> everything works fine:<\/p>\n<p><code>dvc import -o 'data\/project1' 'https:\/\/company.visualstudio.com\/DefaultCollection\/proj\/_git\/DataProject1' 'data\/project1'<\/code> - Successful<\/p>\n<p>Howevever when I run a similar command against <code>DataProject2<\/code> the command fails:<\/p>\n<p><code>dvc import -o 'data\/project2' 'https:\/\/company.visualstudio.com\/DefaultCollection\/proj\/_git\/DataProject2' 'data\/project2'<\/code> - it fails with:<\/p>\n<blockquote>\n<p>ERROR: unexpected error - Operation returned an invalid status 'This\nrequest is not authorized to perform this operation using this\npermission.'  ErrorCode:AuthorizationPermissionMismatch.<\/p>\n<\/blockquote>\n<p>I would like to configure the <code>dvc import<\/code> so that I can set the required <code>sas_token<\/code> but I cannot find a way to do that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1662648529560,
        "Question_score":1,
        "Question_tags":"python|dvc|dvc-import",
        "Question_view_count":34,
        "Owner_creation_time":1248452771430,
        "Owner_last_access_time":1664006575380,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":3317,
        "Owner_up_votes":466,
        "Owner_down_votes":8,
        "Owner_views":296,
        "Question_last_edit_time":1662650337132,
        "Answer_body":"<p>This happens since DVC is not using <code>MLProject<\/code>'s config when it clones and does <code>dvc fetch<\/code> in the <code>DataProject2<\/code> during the <code>import<\/code>. And it doesn't know where it can find the token (clearly, it's not in the Git repo, right?).<\/p>\n<p>There are a few ways to specify it: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#--system\" rel=\"nofollow noreferrer\"><code>global\/system<\/code> configs<\/a> and\/or <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#authenticate-with-environment-variables\" rel=\"nofollow noreferrer\">environment variables<\/a>.<\/p>\n<p>To implement the first option:<\/p>\n<p>On a machine where you do <code>dvc import<\/code>, you could create a remote in the <code>--global<\/code>, or <code>--system<\/code> configs with the same name and specify the token there. Global config fields will be merged with the config in the <code>DataProject2<\/code> repo when DVC is pulling data to import.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>dvc remote add --global &lt;DataProject2-remote-name&gt; azure:\/\/DataProject2\/storage\ndvc remote modify --global &lt;DataProject2-remote-name&gt; account_name &lt;name&gt;\ndvc remote modify --global &lt;DataProject2-remote-name&gt; sas_token &lt;token&gt;\n<\/code><\/pre>\n<p>The second option:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>export AZURE_STORAGE_SAS_TOKEN='mysecret'\nexport AZURE_STORAGE_ACCOUNT='myaccount'\n<\/code><\/pre>\n<p>Please give it a try, let me know if that works or not.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1662656090387,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73651050",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56285351,
        "Question_title":"Updating tracked dir in DVC",
        "Question_body":"<p>According to <a href=\"https:\/\/dvc.org\/doc\/user-guide\/update-tracked-file\" rel=\"nofollow noreferrer\">this tutorial<\/a> when I update file I should remove file from under DVC control first (i.e. execute <code>dvc unprotect &lt;myfile&gt;.dvc<\/code> or <code>dvc remove &lt;myfile&gt;.dvc<\/code>) and then add it again via <code>dvc add &lt;mifile&gt;<\/code>. However It's not clear if I should apply the same workflow for the directories.<\/p>\n\n<p>I have the directory under DVC control with the following structure:<\/p>\n\n<pre><code>data\/\n    1.jpg\n    2.jpg\n<\/code><\/pre>\n\n<p>Should I run <code>dvc unprotect data<\/code> every time the directory content is updated?<\/p>\n\n<p>More specifically I'm interested if I should run <code>dvc unprotect data<\/code> in the following use cases:<\/p>\n\n<ul>\n<li><strong>New file is added.<\/strong> For example if I put <code>3.jpg<\/code> image in the data dir<\/li>\n<li><strong>File is deleted.<\/strong> For example if I delete <code>2.jpg<\/code> image in the <code>data<\/code> dir<\/li>\n<li><strong>File is updated.<\/strong> For example if I edit <code>1.jpg<\/code> image via graphic editor.<\/li>\n<li>A combination of the previous use cases (i.e. some files are updated, other deleted and new files are added)<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1558667422490,
        "Question_score":4,
        "Question_tags":"dvc",
        "Question_view_count":995,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":1558708772616,
        "Answer_body":"<p>Only when file is updated - i.e. edit <code>1.jpg<\/code> with your editor <strong>AND<\/strong> only if hadrlink or symlink cache type is enabled.<\/p>\n\n<p>Please, check this <a href=\"https:\/\/dvc.org\/doc\/user-guide\/update-tracked-file\" rel=\"nofollow noreferrer\">link<\/a>:<\/p>\n\n<blockquote>\n  <p>updating tracked files has to be carried out with caution to avoid data corruption when the DVC config option cache.type is set to hardlink or\/and symlink<\/p>\n<\/blockquote>\n\n<p>I would strongly recommend reading this document: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/cache-file-linking\" rel=\"nofollow noreferrer\">Performance Optimization for Large Files<\/a> it explains benefits of using hardlinks\/symlinks.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1558674266680,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56285351",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":66409283,
        "Question_title":"updating data in dvc registry from other projects",
        "Question_body":"<p>I have a couple of projects that are using and updating the same data sources. I recently learned about <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries\" rel=\"nofollow noreferrer\">dvc's data registries<\/a>, which sound like a great way of versioning data across these different projects (e.g. scrapers, computational pipelines).<\/p>\n<p>I have put all of the relevant data into <code>data-registry<\/code> and then I imported the relevant files into the scraper project with:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ poetry run dvc import https:\/\/github.com\/username\/data-registry raw\n<\/code><\/pre>\n<p>where <code>raw<\/code> is a directory that stores the scraped data. This seems to have worked properly, but then when I went to build <a href=\"https:\/\/dvc.org\/doc\/start\/data-pipelines\" rel=\"nofollow noreferrer\">a dvc pipeline<\/a> that <em>outputted<\/em> data into a file that was already tracked by dvc, I got an error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ dvc run -n menu_items -d src\/ -o raw\/menu_items\/restaurant.jsonl scrapy crawl restaurant\nERROR: Paths for outs:                                                \n'raw'('raw.dvc')\n'raw\/menu_items\/restaurant.jsonl'('menu_items')\noverlap. To avoid unpredictable behaviour, rerun command with non overlapping outs paths.\n<\/code><\/pre>\n<p>Can someone help me understand what is going on here? <strong>What is the best way to use data registries to share and update data across projects?<\/strong><\/p>\n<p>I would ideally like to update the data-registry with new data from the scraper project and then allow other dependent projects to update their data when they are ready to do so.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1614516713937,
        "Question_score":3,
        "Question_tags":"data-management|dvc",
        "Question_view_count":388,
        "Owner_creation_time":1294268936687,
        "Owner_last_access_time":1661618392827,
        "Owner_location":"Chicago, IL",
        "Owner_reputation":2893,
        "Owner_up_votes":294,
        "Owner_down_votes":1,
        "Owner_views":168,
        "Question_last_edit_time":1614699218992,
        "Answer_body":"<p>When you <code>import<\/code> (or <code>add<\/code>) something into your project, a .dvc file is created with that lists that something (in this case the <code>raw\/<\/code> dir) as an &quot;output&quot;.<\/p>\n<p>DVC doesn't allow overlapping outputs among .dvc files or dvc.yaml stages, meaning that your &quot;menu_items&quot; stage shouldn't write to <code>raw\/<\/code> since it's already under the control of <code>raw.dvc<\/code>.<\/p>\n<p>Can you make a separate directory for the pipeline outputs? E.g. use <code>processed\/menu_items\/restaurant.jsonl<\/code><\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1614537291720,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1614698988012,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66409283",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":71155959,
        "Question_title":"How to merge data (CSV) files from multiple branches (Git and DVC)?",
        "Question_body":"<p><strong>Background<\/strong>: In my projects I'm using GIT and <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> to keep track of versions:<\/p>\n<ul>\n<li>GIT - only for source codes<\/li>\n<li>DVC - for dataset, model objects and outputs<\/li>\n<\/ul>\n<p>I'm testing different approaches in separate branches, i.e:<\/p>\n<ul>\n<li>random_forest<\/li>\n<li>neural_network_1<\/li>\n<li>...<\/li>\n<\/ul>\n<p>Typically as an output I'm keeping predictions in csv file with standarised name (i.e.: pred_test.csv). As a consequence in different branches I've different pred_test.csv files. The structure of the file is very simple, it contains two columns:<\/p>\n<ul>\n<li>ID<\/li>\n<li>Prediction<\/li>\n<\/ul>\n<p><strong>Question<\/strong>: What is the best way to merge those prediction files into single big file?<\/p>\n<p>I would like to obtain a file with structure:<\/p>\n<ul>\n<li>ID<\/li>\n<li>Prediction_random_forest<\/li>\n<li>Prediction_neural_network_1<\/li>\n<li>Prediction_...<\/li>\n<\/ul>\n<p>My main issue is how to access files with predictions which are in different branches?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1645092064283,
        "Question_score":0,
        "Question_tags":"git|data-science|dvc",
        "Question_view_count":274,
        "Owner_creation_time":1265742671200,
        "Owner_last_access_time":1658834085253,
        "Owner_location":null,
        "Owner_reputation":2735,
        "Owner_up_votes":190,
        "Owner_down_votes":7,
        "Owner_views":552,
        "Question_last_edit_time":1645132430276,
        "Answer_body":"<p>I would try to use <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\" rel=\"nofollow noreferrer\"><code>dvc get<\/code><\/a> in this case:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc get -o random_forest_pred.csv --rev random_forest . pred_test.csv\n<\/code><\/pre>\n<p>It should bring the <code>pred_test.csv<\/code> from the <code>random_forest<\/code> branch.<\/p>\n<blockquote>\n<p>Mind the <code>.<\/code> before the <code>pred_test.csv<\/code> please, it's needed and it means that &quot;use the current repo&quot;, since <code>dvc get<\/code> could also be used on other repos (e.g. GitHub URL)<\/p>\n<\/blockquote>\n<p>Then I think you could use some CLI or write a script to join the files:<\/p>\n<p><a href=\"https:\/\/unix.stackexchange.com\/questions\/293775\/merging-contents-of-multiple-csv-files-into-single-csv-file\">https:\/\/unix.stackexchange.com\/questions\/293775\/merging-contents-of-multiple-csv-files-into-single-csv-file<\/a><\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1645113091963,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71155959",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73580703,
        "Question_title":"DVC | Permission denied ERROR: failed to reproduce stage: failed to run: .py, exited with 126",
        "Question_body":"<p>Goal: run <code>.py<\/code> files via. <code>dvc.yaml<\/code>.<\/p>\n<p>There are stages before it, in <code>dvc.yaml<\/code>, that don't produce the error.<\/p>\n<p><code>dvc exp run<\/code>:<\/p>\n<pre><code>(venv) me@ubuntu-pcs:~\/PycharmProjects\/project$ dvc exp run\nStage 'inference' didn't change, skipping\nRunning stage 'load_data':\n&gt; load_data.py\n\/bin\/bash: line 1: load_data.py: Permission denied\nERROR: failed to reproduce 'load_data': failed to run: load_data.py, exited with 126\n<\/code><\/pre>\n<p><code>dvc repro<\/code>:<\/p>\n<pre><code>(venv) me@ubuntu-pcs:~\/PycharmProjects\/project$ dvc repro\nStage 'predict' didn't change, skipping                                                                                                                                                                                                                        \nStage 'evaluate' didn't change, skipping\nStage 'inference' didn't change, skipping\nRunning stage 'load_data':\n&gt; load_data.py\n\/bin\/bash: line 1: load_data.py: Permission denied\nERROR: failed to reproduce 'load_data': failed to run: pdl1_lung_model\/load_data.py, exited with 126\n<\/code><\/pre>\n<hr \/>\n<p><code>dvc doctor<\/code>:<\/p>\n<pre><code>DVC version: 2.10.2 (pip)\n---------------------------------\nPlatform: Python 3.9.12 on Linux-5.15.0-46-generic-x86_64-with-glibc2.35\nSupports:\n        webhdfs (fsspec = 2022.5.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        s3 (s3fs = 2022.5.0, boto3 = 1.21.21)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/nvme0n1p5\nCaches: local\nRemotes: s3\nWorkspace directory: ext4 on \/dev\/nvme0n1p5\nRepo: dvc, git\n<\/code><\/pre>\n<p><code>dvc exp run -v<\/code>:<\/p>\n<p><a href=\"https:\/\/gist.github.com\/danielbellsa\/3f2fe05c1535d494a8677e54cddf684a\" rel=\"nofollow noreferrer\">output.txt<\/a><\/p>\n<p><code>dvc exp run -vv<\/code>:<\/p>\n<p><a href=\"https:\/\/gist.github.com\/danielbellsa\/a124cf28b3f0252556deb90b042b7cec\" rel=\"nofollow noreferrer\">output2.txt<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1662111917277,
        "Question_score":1,
        "Question_tags":"python-3.x|permission-denied|dvc",
        "Question_view_count":77,
        "Owner_creation_time":1631019482980,
        "Owner_last_access_time":1663946675073,
        "Owner_location":null,
        "Owner_reputation":234,
        "Owner_up_votes":708,
        "Owner_down_votes":14,
        "Owner_views":155,
        "Question_last_edit_time":1662368809863,
        "Answer_body":"<h3>Solution 1<\/h3>\n<p><code>.py<\/code> files weren't running as scripts.<\/p>\n<p>They need to be; if you want to run one <code>.py<\/code> file per <code>stage<\/code> in <code>dvc.yaml<\/code>.<\/p>\n<p>To do so, you want to append <strong>Boiler-plate code<\/strong>, at the bottom of each <code>.py<\/code> file.<\/p>\n<pre><code>if __name__ == &quot;__main__&quot;:\n    # invoke primary function() in .py file, w\/ params\n<\/code><\/pre>\n<h3>Solution 2<\/h3>\n<pre><code>chmod 777 ....py\n<\/code><\/pre>\n<h3>Soution 3<\/h3>\n<p>I forgot the <code>python<\/code> in <code>cmd:<\/code><\/p>\n<pre><code>  load_data:\n    cmd: python pdl1_lung_model\/load_data.py\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1662114844133,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1662368753112,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73580703",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":61245284,
        "Question_title":"Is it necessary to commit DVC files from our CI pipelines?",
        "Question_body":"<p>DVC uses git commits to save the experiments and navigate between experiments.<\/p>\n<p>Is it possible to avoid making auto-commits in CI\/CD (to save data artifacts after <code>dvc repro<\/code> in CI\/CD side).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1587023782283,
        "Question_score":6,
        "Question_tags":"git|machine-learning|continuous-integration|dvc|mlops",
        "Question_view_count":1047,
        "Owner_creation_time":1558529684193,
        "Owner_last_access_time":1642243398997,
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":79,
        "Owner_up_votes":107,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":1594640021920,
        "Answer_body":"<blockquote>\n  <p>will you make it part of CI pipeline<\/p>\n<\/blockquote>\n\n<p>DVC often serves as a part of MLOps infrastructure. There is a popular <a href=\"https:\/\/martinfowler.com\/articles\/cd4ml.html\" rel=\"noreferrer\">blog post about CI\/CD for ML<\/a> where DVC is used under the hood. <a href=\"https:\/\/blog.codecentric.de\/en\/2020\/01\/remote-training-gitlab-ci-dvc\/\" rel=\"noreferrer\">Another example<\/a> but with GitLab CI\/CD.<\/p>\n\n<blockquote>\n  <p>scenario where you will integrate dvc commit command with CI\n  pipelines?<\/p>\n<\/blockquote>\n\n<p>If you mean <code>git commit<\/code> of DVC files (not <code>dvc commit<\/code>) then yes, you need to commit dvc-files into Git during CI\/CD process. Auto-commit is not the best practice.<\/p>\n\n<p>How to avoid Git commit in CI\/CD:<\/p>\n\n<ol>\n<li>After ML model training in CI\/CD, save changed dvc-files in external storage (for example GitLab artifact\/releases), then get the files to a developer machine and commit there. Users usually write scripts to automate it.<\/li>\n<li>Wait for DVC 1.0 release when <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1234\" rel=\"noreferrer\">run-cache (like build-cache)<\/a> will be implemented. Run-cache makes dvc-files ephemeral and no additional Git commits will be required. Technically, run-cache is an associative storage <code>repo state --&gt; run results<\/code> outside of Git repo (in data remote).<\/li>\n<\/ol>\n\n<p>Disclaimer: I'm one of the creators of DVC.<\/p>",
        "Answer_comment_count":5,
        "Answer_creation_time":1587029761967,
        "Answer_score":6,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61245284",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72665109,
        "Question_title":"dvc.exceptions.CyclicGraphError: Pipeline has a cycle involving: load_extract_save",
        "Question_body":"<pre><code>stages:\n  load_extract_save: \n    cmd: python src\/stage_01_load_extract_save.py --config=config\/config.yaml\n    deps:\n      - config\/config.yaml\n      - src\/utils\/all_utils.py\n      - src\/stage_01_load_extract_save.py\n      - artifacts\/data\n    outs:\n      - artifacts\/data\n      - artifacts\/clean_data\/X.npy\n      - artifacts\/clean_data\/Y.npy\n\n  train_test_split_save:\n    cmd: python src\/stage_02_train_test_split_save.py --config=config\/config.yaml --params=params.yaml\n    deps:\n      - artifacts\/clean_data\/X.npy\n      - artifacts\/clean_data\/Y.npy\n      - src\/utils\/all_utils.py\n      - params.yaml\n      - config\/config.yaml\n      - src\/stage_02_train_test_split_save.py\n    outs:\n      - artifacts\/train_data\/X_train.npy\n      - artifacts\/train_data\/Y_train.npy\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n  \n  train_model:\n    cmd:  python src\/stage_03_train.py --config=config\/config.yaml --params=params.yaml\n    deps:\n      - artifacts\/train_data\/X_train.npy\n      - artifacts\/train_data\/Y_train.npy\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n      - src\/stage_03_train.py\n      - src\/utils\/all_utils.py\n      - config\/config.yaml\n      - params.yaml\n    outs:\n      - artifacts\/checkpoints\n      - artifacts\/model\n  \n  metrics:\n    cmd: python src\/stage_04_metrics.py --config=config\/config.yaml\n    deps:\n      - src\/stage_04_metrics.py\n      - config\/config.yaml\n      - src\/utils\/all_utils.py\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n      - artifacts\/checkpoints\n      - artifacts\/model\n    outs:\n      - confusion_matrix.png\n<\/code><\/pre>\n<p>This is my DVC.yaml.<\/p>\n<p>I have created Github workflow to reproduce it, but whenever I run it it gives me the following error - <code>... ERROR: Pipeline has a cycle involving: load_extract_save.<\/code><\/p>\n<p>The error looks <a href=\"https:\/\/i.stack.imgur.com\/1mt1Y.png\" rel=\"nofollow noreferrer\">like this<\/a>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655502527920,
        "Question_score":2,
        "Question_tags":"python|dvc",
        "Question_view_count":38,
        "Owner_creation_time":1587308895270,
        "Owner_last_access_time":1663873987327,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1655524559503,
        "Answer_body":"<p>Stage <code>load_extract_save<\/code> both outputs and depends on the same path (<code>artifacts\/data<\/code>). That's a cycle.<\/p>\n<p>Pipeline structures should be <a href=\"https:\/\/dvc.org\/doc\/command-reference\/dag#directed-acyclic-graph\" rel=\"nofollow noreferrer\">directed <strong>acyclical<\/strong> graphs<\/a>, otherwise <code>dvc repro<\/code> could execute that stage over and over forever.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1655524277717,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72665109",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":62441146,
        "Question_title":"Revert a dvc remove -p command",
        "Question_body":"<p>I have just removed a DVC tracking file by mistake using the command <code>dvc remove training_data.dvc -p<\/code>, which led to all my training dataset gone completely. I know in Git, we can easily revert a deleted branch based on its hash. Does anyone know how to revert all my lost data in DVC?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592445622650,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":687,
        "Owner_creation_time":1467943515393,
        "Owner_last_access_time":1664083114540,
        "Owner_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Owner_reputation":173,
        "Owner_up_votes":97,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You should be safe (at least data is not gone) most likely. From the <code>dvc remove<\/code> <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remove\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>Note that it does not remove files from the DVC cache or remote storage (see dvc gc). However, remember to run <code>dvc push<\/code> to save the files you actually want to use or share in the future.<\/p>\n<\/blockquote>\n\n<p>So, if you created <code>training_data.dvc<\/code> as with <code>dvc add<\/code> and\/or <code>dvc run<\/code> and <code>dvc remove -p<\/code> didn't ask\/warn you about anything, means that data is cached similar to Git in the <code>.dvc\/cache<\/code>. <\/p>\n\n<p>There are ways to retrieve it, but I would need to know a little bit more details - how exactly did you add your dataset? Did you commit <code>training_data.dvc<\/code> or it's completely gone? Was it the only data you have added so far? (happy to help you in comments).<\/p>\n\n<h2>Recovering a directory<\/h2>\n\n<p>First of all, <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files-and-directories#structure-of-cache-directory\" rel=\"nofollow noreferrer\">here<\/a> is the document that describes briefly how DVC stores directories in the cache.<\/p>\n\n<p>What we can do is to find all <code>.dir<\/code> files in the <code>.dvc\/cache<\/code>:<\/p>\n\n<p><code>find .dvc\/cache -type f -name \"*.dir\"<\/code><\/p>\n\n<p>outputs something like:<\/p>\n\n<pre><code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir\n.dvc\/cache\/00\/db872eebe1c914dd13617616bb8586.dir\n.dvc\/cache\/2d\/1764cb0fc973f68f31f5ff90ee0883.dir\n<\/code><\/pre>\n\n<p>(if the local cache is lost and we are restoring data from the remote storage, the same logic applies, commands (e.g. to find files on S3 with .dir extension) look different)<\/p>\n\n<p>Each <code>.dir<\/code> file is a JSON with a content of one version of a directory (file names, hashes, etc). It has all the information needed to restore it. The next thing we need to do is to understand which one do we need. There is no one single rule for that, what I would recommend to check (and pick depending on your use case):<\/p>\n\n<ul>\n<li>Check the date modified (if you remember when this data was added).<\/li>\n<li>Check the content of those files - if you remember a specific file name that was present only in the directory you are looking for - just grep it.<\/li>\n<li>Try to restore them one by one and check the directory content.<\/li>\n<\/ul>\n\n<p>Okay, now let's imagine we decided that we want to restore <code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir<\/code>, (e.g. because content of it looks like:<\/p>\n\n<pre><code>[\n{\"md5\": \"6f597d341ceb7d8fbbe88859a892ef81\", \"relpath\": \"test.tsv\"}, {\"md5\": \"32b715ef0d71ff4c9e61f55b09c15e75\", \"relpath\": \"train.tsv\"}\n]\n<\/code><\/pre>\n\n<p>and we want to get a directory with <code>train.tsv<\/code>).<\/p>\n\n<p>The only thing we need to do is to create a <code>.dvc<\/code> file that references this directory:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n- md5: 20b786b6e6f80e2b3fcf17827ad18597.dir\n  path: my-directory\n<\/code><\/pre>\n\n<p>(note, that path \/20\/b786b6e6f80e2b3fcf17827ad18597.dir became a hash value: 20b786b6e6f80e2b3fcf17827ad18597.dir)<\/p>\n\n<p>And run <code>dvc pull<\/code> on this file.<\/p>\n\n<p>That should be it.<\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1592457436920,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1592496929889,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62441146",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":65824766,
        "Question_title":"SSH automation in jenkins",
        "Question_body":"<p>So I've been working on the automation of processes and it includes fetching data from an external source through DVC(data version control) for which I am using SSH client to pull and push changes. For automation, I'm using <strong>Jenkins<\/strong> and the problem I'm facing is that for ssh we need to give a password on runtime, and in automation that's not an option. I've tried multiple ways to specify passwords for ssh like sshpass and ssh config but it turns out Jenkins when building creates some file name <strong>script.sh<\/strong> in a directory <em>repoName@tmp<\/em> in var\/lib\/jenkins\/.... and therefore it is giving permission denied error. no matter what I try. If anyone could give any suggestions to this problem it would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611222255303,
        "Question_score":2,
        "Question_tags":"jenkins|ssh|dvc",
        "Question_view_count":121,
        "Owner_creation_time":1493101921290,
        "Owner_last_access_time":1663968152030,
        "Owner_location":"Pakistan",
        "Owner_reputation":133,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You could use key-based auth for SSH instead instead of password auth so that your Jenkins user can access your SSH DVC remote without needing to specify a password.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1611228029327,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65824766",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":64653042,
        "Question_title":"Control tracked version of external dependency",
        "Question_body":"<p>I am trying to set up a DVC repository for machine learning data with different tagged versions of the dataset. I do this with something like:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd \/raid\/ml_data  # folder on a data drive\n$ git init\n$ dvc init\n$ [add data]\n$ [commit to dvc, git]\n$ git tag -a 1.0.0\n$ [add or change data]\n$ [commit to dvc, git]\n$ git tag -a 1.1.0\n<\/code><\/pre>\n<p>I have multiple projects that each need to reference some version of this dataset. The problem is I can't figure out how to set up those projects to reference a specific version. I'm able to track the <code>HEAD<\/code> of the repo with something like:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd ~\/my_proj  # different drive than the remote\n$ mkdir data\n$ git init\n$ dvc init\n$ dvc remote add -d local \/raid\/ml_data  # add the remote on my data drive\n$ dvc cache dir \/raid\/ml_data\/.dvc\/cache  # tell DVC to use the remote cache\n$ dvc checkout\n$ dvc run --external -d \/raid\/ml_data -o data\/ cp -r \/raid\/ml_data data\n<\/code><\/pre>\n<p>This gets me the latest version of the dataset, symlinked into my <code>data<\/code> folder, but what if I want some projects to use the <code>1.0.0<\/code> version and some to use the <code>1.1.0<\/code> version, or another version? Or for that matter, if I update the dataset to <code>2.0.0<\/code> but don't want my existing projects to necessarily track <code>HEAD<\/code> and instead keep the version with which they were set up?<\/p>\n<p>It's important to me to not create a ton of local copies of my dataset as the <code>\/home<\/code> drive is much smaller than the <code>\/raid<\/code> drive and some of these datasets are huge.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1604349754297,
        "Question_score":2,
        "Question_tags":"version-control|dvc",
        "Question_view_count":139,
        "Owner_creation_time":1370629593700,
        "Owner_last_access_time":1663178436650,
        "Owner_location":"Colorado Springs, CO",
        "Owner_reputation":11685,
        "Owner_up_votes":2855,
        "Owner_down_votes":47,
        "Owner_views":1329,
        "Question_last_edit_time":1604361023336,
        "Answer_body":"<p>I think you are looking for the <a href=\"https:\/\/dvc.org\/doc\/start\/data-access\" rel=\"nofollow noreferrer\">data access<\/a> set of commands.<\/p>\n<p>In your particular case, <code>dvc import<\/code> makes sense:<\/p>\n<pre><code>$ dvc import \/raid\/ml_data data\n<\/code><\/pre>\n<p>if you want to get the most recent version (HEAD). Then you will be able to update it with the <code>dvc update<\/code> command (if 2.0.0 is released, for example).<\/p>\n<pre><code>$ dvc import \/raid\/ml_data data --rev 1.0.0\n<\/code><\/pre>\n<p>if you'd like to &quot;fix&quot; it to the specific version.<\/p>\n<h3>Avoiding copies<\/h3>\n<p>Make sure also, that <code>symlinks<\/code> are set for the second project, as described in the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"nofollow noreferrer\">Large Dataset Optimization<\/a>:<\/p>\n<pre><code>$ dvc config cache.type reflink,hardlink,symlink,copy\n<\/code><\/pre>\n<p>(there are config modifiers <code>--global<\/code>, <code>--local<\/code>, <code>--system<\/code> to set this setting for everyone at once, or just for one project, etc)<\/p>\n<p>Check the details instruction <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization#configuring-dvc-cache-file-link-type\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<hr \/>\n<p>Overall, it's a great setup, and looks like you got pretty much everything right. Please, don't hesitate to follow up and\/or create other questions here- we'll help you with this.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1604351561433,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1604362563343,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64653042",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72473641,
        "Question_title":"How do launch experiments in DVC?",
        "Question_body":"<p>I want to launch some experiments in DVC. But when I set values of experiment parameters, DVC deletes file 'params.yaml', and experiment doesn't set in queue.<\/p>\n<p>Simplified code for example:\nPython file 'test.py':<\/p>\n<pre><code>import numpy as np\nimport json\nimport yaml\n\nparams = yaml.safe_load(open('params.yaml'))[&quot;test&quot;]\n\nprecision = np.random.random()\nrecall = params['value']\naccuracy = np.random.random()\n \n\nrows = {'precision': precision,\n        'recall': recall,\n        'accuracy': accuracy}\n\n\nwith open(params['metrics_path'], 'w') as outfile:\n    json.dump(rows, outfile)\n\nfpr = 10*np.random.random((1,10)).tolist()\ntpr = 10*np.random.random((1,10)).tolist()\n\nwith open('plot.json', 'w') as outfile2:\n    json.dump(\n      {\n        &quot;roc&quot;: [ {&quot;fpr&quot;: f, &quot;tpr&quot;: t} for f, t in zip(fpr, tpr) ]\n      }, \n      outfile2\n      )\n<\/code><\/pre>\n<p>params.yaml:<\/p>\n<pre><code>test:\n  metrics_path: &quot;scores.json&quot;\n  value: 1\n<\/code><\/pre>\n<p>dvc.yaml:<\/p>\n<pre><code>stages:\n  test:\n    cmd: python test.py\n    deps:\n    - test.py\n    params:\n    - test.metrics_path\n    - test.value\n    metrics:\n    - scores.json:\n        cache: false\n    plots:\n    - plot.json:\n        cache: false\n        x: fpr\n        y: tpr\n<\/code><\/pre>\n<p>It is strange behavior. Is it possible to fix it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_time":1654161042840,
        "Question_score":2,
        "Question_tags":"python|dvc",
        "Question_view_count":167,
        "Owner_creation_time":1623510794510,
        "Owner_last_access_time":1663932931300,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I solved my problem. It is necessary, that all files (executable scripts, 'dvc.yaml', 'params.yaml') be tracked by git. In this case <code>dvc exp run<\/code> command works correctly.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1654244934587,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72473641",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":57966851,
        "Question_title":"Undo 'dvc add' operation",
        "Question_body":"<p>I <code>dvc add<\/code>-ed a file I did not mean to add. I have not yet committed.<\/p>\n\n<p>How do I undo this operation? In Git, you would do <code>git rm --cached &lt;filename&gt;<\/code>.<\/p>\n\n<p>To be clear: I want to make DVC forget about the file, and I want the file to remain untouched in my working tree. This is the opposite of what <code>dvc remove<\/code> does.<\/p>\n\n<p>One <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1524\" rel=\"nofollow noreferrer\">issue<\/a> on the DVC issue tracker suggests that <code>dvc unprotect<\/code> is the right command. But reading the <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/unprotect\" rel=\"nofollow noreferrer\">manual page<\/a> suggests otherwise.<\/p>\n\n<p>Is this possible with DVC?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1568689927047,
        "Question_score":4,
        "Question_tags":"dvc",
        "Question_view_count":1304,
        "Owner_creation_time":1383611307000,
        "Owner_last_access_time":1664061570950,
        "Owner_location":"New York",
        "Owner_reputation":10846,
        "Owner_up_votes":1581,
        "Owner_down_votes":95,
        "Owner_views":984,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As per mroutis on the DVC Discord server:<\/p>\n\n<ol>\n<li><code>dvc unprotect<\/code> the file; this won't be necessary if you don't use <code>symlink<\/code> or <code>hardlink<\/code> caching, but it can't hurt.<\/li>\n<li>Remove the .dvc file<\/li>\n<li>If you need to delete the cache entry itself, run <code>dvc gc<\/code>, or look up the MD5 in <code>data.dvc<\/code> and manually remove it from <code>.dvc\/cache<\/code>.<\/li>\n<\/ol>\n\n<p><em>Edit<\/em> -- there is now an issue on their Github page to add this to the manual: <a href=\"https:\/\/github.com\/iterative\/dvc.org\/issues\/625\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc.org\/issues\/625<\/a><\/p>",
        "Answer_comment_count":6,
        "Answer_creation_time":1568693889197,
        "Answer_score":7,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1568725966083,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57966851",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":68559059,
        "Question_title":"DVC connect to Min.IO to access S3",
        "Question_body":"<p>What is the proper way to connect DVC to Min.IO that is connected to some buckets on S3.<\/p>\n<pre><code>AWS-S3(My_Bucket) &gt; Min.io(MY_Bucket aliased as S3)\n<\/code><\/pre>\n<p>Right now i am accessing my bucket by using mc for example <code>mc cp s3\/my_bucket\/datasets datasets<\/code> to copy stuff from there. But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC so i can use for example <code>&quot;DVC mc-S3 pull&quot;<\/code> and <code>&quot;DVC AWS-S3 pull&quot;<\/code>.<\/p>\n<p>How do i got for it because while googling i couldn't find anything that i could easily follow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1627469557323,
        "Question_score":1,
        "Question_tags":"amazon-s3|minio|dvc",
        "Question_view_count":375,
        "Owner_creation_time":1578574709920,
        "Owner_last_access_time":1648636957603,
        "Owner_location":"Poland",
        "Owner_reputation":85,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It looks like you are looking for a combination of things.<\/p>\n<p>First, Jorge mentioned you can set <code>endpointurl<\/code> to access Minio the same way as you would access regular S3:<\/p>\n<pre><code>dvc remote add -d minio-remote s3:\/\/mybucket\/path\ndvc remote modify minio-remote endpointurl https:\/\/minio.example.com                          \n<\/code><\/pre>\n<p>Second, it seems you can create <em>two<\/em> remotes - one for S3, one for Minio and use <code>-r<\/code> option that is available for many data management related commands:<\/p>\n<pre><code>dvc pull -r minio-remote\ndvc pull -r s3-remote\ndvc push -r minio-remote\n...\n<\/code><\/pre>\n<p>This way you could <code>push<\/code>\/<code>pull<\/code> data to\/from a specific storage.<\/p>\n<blockquote>\n<p>But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC<\/p>\n<\/blockquote>\n<p>There are other possible ways, I think to organize this. It indeed depends on what semantics you expect from <code>DVC mc-S3 pull<\/code>. Please let us know if <code>-r<\/code> is not enough and clarify the question- that would help us here.<\/p>",
        "Answer_comment_count":7,
        "Answer_creation_time":1627513406643,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68559059",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73700203,
        "Question_title":"ERROR: Cannot add 'folder-path', because it is overlapping with other DVC tracked output:",
        "Question_body":"<p>Goal: <code>add<\/code> <code>commit<\/code> <code>push<\/code> all contents of <code>project_model\/data\/<\/code> to <strong>dvcstore<\/strong>.<\/p>\n<p>I don't have any <code>.dvc<\/code> files in my project.<\/p>\n<pre><code>$ dvc add .\/project_model\/data\/\nERROR: Cannot add '\/home\/me\/PycharmProjects\/project\/project_model\/data\/images', because it is overlapping with other DVC tracked output: '\/home\/me\/PycharmProjects\/project\/project_model\/data'.\nTo include '\/home\/me\/PycharmProjects\/project\/project_model\/data\/images' in '\/home\/me\/PycharmProjects\/project\/project_model\/data', run 'dvc commit project_model\/data.dvc'\n\n$ dvc commit project_model\/data.dvc\nERROR: failed to commit project_model\/data.dvc - 'project_model\/data.dvc' does not exist\n<\/code><\/pre>\n<p>I've deleted contents from <code>.dvc\/cache\/<\/code> and <strong>S3<\/strong> <code>s3:\/\/foo\/bar\/dvcstore\/<\/code>, with no luck.<\/p>\n<hr \/>\n<pre><code>$ dvc -V\n2.10.2\n<\/code><\/pre>\n<pre><code>$ dvc doctor\nDVC version: 2.10.2 (pip)\n---------------------------------\nPlatform: Python 3.9.12 on Linux-5.15.0-47-generic-x86_64-with-glibc2.35\nSupports:\n        webhdfs (fsspec = 2022.5.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        s3 (s3fs = 2022.5.0, boto3 = 1.21.21)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/nvme0n1p5\nCaches: local\nRemotes: s3\nWorkspace directory: ext4 on \/dev\/nvme0n1p5\nRepo: dvc, git\n<\/code><\/pre>\n<p>Please let me know if there's anything else I can add to post.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1663059618623,
        "Question_score":1,
        "Question_tags":"git|amazon-s3|caching|pycharm|dvc",
        "Question_view_count":31,
        "Owner_creation_time":1631019482980,
        "Owner_last_access_time":1663946675073,
        "Owner_location":null,
        "Owner_reputation":234,
        "Owner_up_votes":708,
        "Owner_down_votes":14,
        "Owner_views":155,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In my case, the problem was in <code>dvc.yaml<\/code>.<\/p>\n<p>For a few <code>stages<\/code>, I had cyclical dependencies, where a file-path was mentioned in both the <code>deps<\/code> and <code>outs<\/code>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1663149911323,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73700203",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":71338160,
        "Question_title":"DVC Experiment management workflow",
        "Question_body":"<p>I'm struggling with the DVC experiment management. Suppose the following scenario:<\/p>\n<p>I have <code>params.yaml<\/code> file:<\/p>\n<pre><code>recommendations:\n  k: 66\n  q: 5\n<\/code><\/pre>\n<p>I run the experiment with <code>dvc exp run -n exp_66<\/code>, and then I do <code>dvc exp push origin exp_66<\/code>. After this, I modify <code>params.yaml<\/code> file:<\/p>\n<pre><code>recommendations:\n  k: 99\n  q: 5\n<\/code><\/pre>\n<p>and then run another experiment <code>dvc exp run -n exp_99<\/code>, after which I commit with <code>dvc exp push origin exp_99<\/code>.<\/p>\n<p>Now, when I pull the corresponding branch with Git, I try to pull <code>exp_66<\/code> from dvc by running <code>dvc exp pull origin exp_66<\/code>. This does the pull (no error messages), but the content of the <code>params.yaml<\/code> file is with <code>k: 99<\/code> (and I would expect <code>k: 66<\/code>). What am I doing wrong? Does <code>git push<\/code> have to be executed after <code>dvc push<\/code>? Apart from that, I also found <code>dvc exp apply exp_66<\/code>, but I'm not sure what it does (it is suggested that after <code>apply<\/code> one should execute <code>git add .<\/code>, then <code>git commit<\/code>?<\/p>\n<p>I would really appreciate if you could write down the workflow with committing different experiments, pushing, pulling, applying, etc.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1646314733127,
        "Question_score":1,
        "Question_tags":"git|dvcs|dvc",
        "Question_view_count":152,
        "Owner_creation_time":1643373769633,
        "Owner_last_access_time":1663766086480,
        "Owner_location":null,
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You did everything alright. In the end, after pulling, you can see that when using <code>dvc exp show<\/code> your experiments will be there. To restore the experiment available from your experiment list into your workspace, you simply need to run <code>dvc exp apply exp_66<\/code>. DVC will make sure that the changes corresponding to this experiment will be checked out.<\/p>\n<p>Your workflow seems correct so far. One addition: once you make sure one of the experiments is what you want to &quot;keep&quot; in git history, you can use <code>dvc exp branch {exp_id} {branch_name}<\/code> to create a separate branch for this experiment. Then you can use <code>git<\/code> commands to save the changes.<\/p>",
        "Answer_comment_count":5,
        "Answer_creation_time":1646319959073,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71338160",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":66925614,
        "Question_title":"How to access DVC-controlled files from Oracle?",
        "Question_body":"<p>I have been storing my large files in CLOBs within Oracle, but I am thinking of storing my large files in a shared drive, then having a column in Oracle contain pointers to the files. This would use DVC.<\/p>\n<p>When I do this,<\/p>\n<p>(a) are the paths in Oracle paths that point to the files in my shared drive, as in, the actual files themselves?<\/p>\n<p>(b) or do the paths in Oracle point somehow to the DVC metafile?<\/p>\n<p>Any insight would help me out!<\/p>\n<p>Thanks :)\nJustin<\/p>\n<hr \/>\n<p>EDIT to provide more clarity:<\/p>\n<p>I checked here (<a href=\"https:\/\/dvc.org\/doc\/api-reference\/open\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/api-reference\/open<\/a>), and it helped, but I'm not fully there yet ...<\/p>\n<p>I want to pull a file from a remote dvc repository using python (which I have connected to the Oracle database). So, if we can make that work, I think I will be good. But, I am confused. If I specify 'remote' below, then how do I name the file (e.g., 'activity.log') when the remote files are all encoded?<\/p>\n<pre><code>with dvc.api.open(\n        'activity.log',\n        repo='location\/of\/dvc\/project',\n        remote='my-s3-bucket'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p>(NOTE: For testing purposes, my &quot;remote&quot; DVC directory is just another folder on my MacBook.)<\/p>\n<p>I feel like I'm missing a key concept about getting remote files ...<\/p>\n<p>I hope that adds more clarity. Any help figuring out remote file access is appreciated! :)<\/p>\n<p>Justin<\/p>\n<hr \/>\n<p>EDIT to get insights on 'rev' parameter:<\/p>\n<p>Before my question, some background\/my setup:\n(a) I have a repo on my MacBook called 'basics'.\n(b) I copied into 'basics' a directory of 501 files (called 'surface_files') that I subsequently pushed to a remote storage folder called 'gss'. After the push, 'gss' contains 220 hash directories.<\/p>\n<p>The steps I used to get here are as follows:<\/p>\n<pre><code>&gt; cd ~\/Desktop\/Work\/basics\n&gt; git init\n&gt; dvc init\n&gt; dvc add ~\/Desktop\/Work\/basics\/surface_files\n&gt; git add .gitignore surface_files.dvc\n&gt; git commit -m &quot;Add raw data&quot;\n&gt; dvc remote add -d remote_storage ~\/Desktop\/Work\/gss\n&gt; git commit .dvc\/config -m &quot;Configure remote storage&quot;\n&gt; dvc push\n&gt; rm -rf .\/.dvc\/cache\n&gt; rm -rf .\/surface_files\n<\/code><\/pre>\n<p>Next, I ran the following Python code to take one of my surface files, named <code>surface_100141.dat<\/code>, and used <code>dvc.api.get_url()<\/code> to get the corresponding remote storage file name. I then copied this remote storage file into my desktop under the file's original name, i.e., <code>surface_100141.dat<\/code>.<\/p>\n<p>The code that does all this is as follows, but FIRST, MY QUESTION --- when I run the code as it is shown below, no problems; but when I uncomment the 'rev=' line, it fails. I am not sure why this is happening. I used <code>git log<\/code> and <code>cat .git\/refs\/heads\/master<\/code> to make sure that I was getting the right hash. WHY IS THIS FAILING? That is my question.<\/p>\n<p>(In full disclosure, my git knowledge is not too strong yet. I'm getting there, but it's still a work in progress! :))<\/p>\n<pre><code>import dvc.api\nimport os.path\nfrom os import path\nimport shutil\n\nfilename = 'surface_100141.dat' # This file name would be stored in my Oracle database\nhome_dir = os.path.expanduser('~')+'\/' # This simply expanding '~' into '\/Users\/ricej\/'\n\nresource_url = dvc.api.get_url(\n    path=f'surface_files\/{filename}', # Works when 'surface_files.dvc' exists, even when 'surface_files' directory and .dvc\/cache do not\n    repo=f'{home_dir}Desktop\/Work\/basics',\n    # rev='5c92710e68c045d75865fa24f1b56a0a486a8a45', # Commit hash, found using 'git log' or 'cat .git\/refs\/heads\/master'\n    remote='remote_storage')\nresource_url = home_dir+resource_url\nprint(f'Remote file: {resource_url}')\n\nnew_dir = f'{home_dir}Desktop\/' # Will copy fetched file to desktop, for demonstration\nnew_file = new_dir+filename\nprint(f'Remote file copy: {new_file}')\n\nif path.exists(new_file):\n    os.remove(new_file)\n    \ndest = shutil.copy(resource_url, new_file) # Check your desktop after this to see remote file copy\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1617399923477,
        "Question_score":1,
        "Question_tags":"python|oracle|dvc",
        "Question_view_count":389,
        "Owner_creation_time":1407091594730,
        "Owner_last_access_time":1654654035420,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1617643343263,
        "Answer_body":"<p>I'm not 100% sure that I understand the question (it would be great to expand it a bit on the actual use case you are trying to solve with this database), but I can share a few thoughts.<\/p>\n<p>When we talk about DVC, I think you need to specify a few things to identify the file\/directory:<\/p>\n<ol>\n<li>Git commit + path (actual path like <code>data\/data\/xml<\/code>). Commit (or to be precise any Git revision) is needed to identify the version of the data file.<\/li>\n<li>Or path in the DVC storage (<code>\/mnt\/shared\/storage\/00\/198493ef2343ao<\/code> ...<code>) + actual name of this file. This way you would be saving info that <\/code>.dvc` files have.<\/li>\n<\/ol>\n<p>I would say that second way is <em>not<\/em> recommended since to some extent it's an implementation detail - how does DVC store files internally. The public interface to DVC organized data storage is its repository URL + commit + file name.<\/p>\n<p>Edit (example):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with dvc.api.open(\n        'activity.log',\n        repo='location\/of\/dvc\/project',\n        remote='my-s3-bucket'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p><code>location\/of\/dvc\/project<\/code> this path must point to an actual Git repo. This repo should have a <code>.dvc<\/code> or <code>dvc.lock<\/code> file that has <code>activity.log<\/code> name in it + its hash in the remote storage:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n  - md5: a304afb96060aad90176268345e10355\n    path: activity.log\n<\/code><\/pre>\n<p>By reading this Git repo and analyzing let's say <code>activity.log.dvc<\/code> DVC will be able to create the right path <code>s3:\/\/my-bucket\/storage\/a3\/04afb96060aad90176268345e10355<\/code><\/p>\n<p><code>remote='my-s3-bucket'<\/code> argument is optional. By default it will use the one that is defined in the repo itself.<\/p>\n<p>Let's take another real example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with dvc.api.open(\n        'get-started\/data.xml',\n        repo='https:\/\/github.com\/iterative\/dataset-registry'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p>In the <code>https:\/\/github.com\/iterative\/dataset-registry<\/code> you could find the <a href=\"https:\/\/github.com\/iterative\/dataset-registry\/blob\/master\/get-started\/data.xml.dvc\" rel=\"nofollow noreferrer\"><code>.dvc<\/code> file<\/a> that is enough for DVC to create a path to the file by also analyzing its <a href=\"https:\/\/github.com\/iterative\/dataset-registry\/blob\/master\/.dvc\/config\" rel=\"nofollow noreferrer\">config<\/a><\/p>\n<pre><code>https:\/\/remote.dvc.org\/dataset-registry\/a3\/04afb96060aad90176268345e10355\n<\/code><\/pre>\n<p>you could run <code>wget<\/code> on this file to download it<\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1617404822540,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1617478114567,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66925614",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":69725612,
        "Question_title":"Is the default DVC behavior to store connection data in git?",
        "Question_body":"<p>I've recently started to play with <a href=\"https:\/\/dvc.org\" rel=\"nofollow noreferrer\">DVC<\/a>, and I was a bit surprised to see the <a href=\"https:\/\/dvc.org\/doc\/start\/data-and-model-versioning#storing-and-sharing\" rel=\"nofollow noreferrer\">getting started docs<\/a> are suggesting to store <code>.dvc\/config<\/code> in git.<\/p>\n<p>This seemed like a fine idea at first, but then I noticed that my Azure Blob Storage account (i.e. my Azure username) is also stored in .dvc\/config, which means it would end up in git. Making it not ideal for team collaboration scenarios.<\/p>\n<p>What's even less ideal (read: really scary) is that connection strings entered using <code>dvc remote modify blah connection_string ...<\/code> also end up in <code>.dvc\/config<\/code>, making them end up in git and, in the case of open source projects, making them end up in <strong>very<\/strong> interesting places.<\/p>\n<p>Am I doing something obviously wrong? I wouldn't expect the getting started docs to go very deep into security issues, but I wouldn't expect them to store connection strings in source control either.<\/p>\n<p>My base assumption is that I'm misunderstanding\/misconfiguring something, I'd be curious to know what.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635260868803,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":77,
        "Owner_creation_time":1250158552417,
        "Owner_last_access_time":1663847198323,
        "Owner_location":"Romania",
        "Owner_reputation":7916,
        "Owner_up_votes":1735,
        "Owner_down_votes":33,
        "Owner_views":801,
        "Question_last_edit_time":null,
        "Answer_body":"<p>DVC has few &quot;levels&quot; of config, that can be controlled with proper flag:<\/p>\n<ul>\n<li><code>--local<\/code> - repository level, ignored by git by default - designated for project-scope, sensitive data<\/li>\n<li>project - same as above, not ignored - designated to specify non-sensitive data (it is the default)<\/li>\n<li><code>--global<\/code> \/ <code>--system<\/code> - for common config for more repositories.<\/li>\n<\/ul>\n<p>More information can be found in the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#description\" rel=\"nofollow noreferrer\">docs<\/a>.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1635332461183,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1635332764020,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69725612",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72551630,
        "Question_title":"How to setup a DVC shared cache without git repository between different services in minikube?",
        "Question_body":"<p>I need to setup a shared cache in minikube in such a way that different services can use that cache to pull and update DVC models and data needed for training Machine Learning models. The structure of the project is to use 1 pod to periodically update the cache with new models and outputs. Then, multiple pods can read the cache to recreate the updated models and data. So I need to be able to update the local cache directory and pull from it using DVC commands, so that all the services have consistent view on the latest models and data created by a service.<\/p>\n<p>More specifically, I have a docker image called <code>inference-service<\/code> that should only <code>dvc pull<\/code> or some how use the info in the shared dvc cache to get the latest model and data locally in <code>models<\/code> and <code>data<\/code> folders (see dockerfile) in minikube. I have another image called <code>test-service<\/code> that\nruns the ML pipeline using <code>dvc repro<\/code> which creates the models and data that DVC needs (dvc.yaml) to track and store in the shared cache. So <code>test-service<\/code> should push created outputs from the ML pipeline into the shared cache so that <code>inference-service<\/code> can pull it and use it instead of running dvc repro by itself. <code>test-service<\/code> should only re-train and write the updated models and data into the shared cache while <code>inference-service<\/code> should only read and recreate the updated\/latest models and data from the shared cache.<\/p>\n<p><em><strong>Problem: the cache does get mounted on the minikube VM, but the inference service does not pull (using <code>dvc pull -f<\/code>) the data and models after the test service is done with <code>dvc repro<\/code> and results the following warnings and failures:<\/strong><\/em><\/p>\n<p><em>relevant kubernetes pod log of inference-service<\/em><\/p>\n<pre><code>WARNING: Output 'data\/processed\/train_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/train_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/validation_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/validation_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/test_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/test_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/interim\/train_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/train_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/validation_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/validation_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/test_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/test_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'models\/mlb.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/tfidf_vectorizer.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/model.pkl'(stage: 'train') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'reports\/scores.json'(stage: 'evaluate') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: No file hash info found for '\/root\/models\/model.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/reports\/scores.json'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/train_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/validation_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/test_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/train_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/validation_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/test_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/mlb.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/tfidf_vectorizer.pkl'. It won't be created.\n10 files failed\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\n\/root\/models\/model.pkl\n\/root\/reports\/scores.json\n\/root\/data\/processed\/train_preprocessed.pkl\n\/root\/data\/processed\/validation_preprocessed.pkl\n\/root\/data\/processed\/test_preprocessed.pkl\n\/root\/data\/interim\/train_featurized.pkl\n\/root\/data\/interim\/validation_featurized.pkl\n\/root\/data\/interim\/test_featurized.pkl\n\/root\/models\/mlb.pkl\n\/root\/models\/tfidf_vectorizer.pkl\nIs your cache up to date?\n<\/code><\/pre>\n<p><em>relevant kubernetes pod log of test-service<\/em><\/p>\n<pre><code>Stage 'preprocess' is cached - skipping run, checking out outputs\nGenerating lock file 'dvc.lock'\nUpdating lock file 'dvc.lock'\nStage 'featurize' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'train' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'evaluate' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nUse `dvc push` to send your updates to remote storage.\n<\/code><\/pre>\n<p><strong>Project Tree<\/strong><\/p>\n<pre><code>\u251c\u2500 .dvc\n\u2502  \u251c\u2500 .gitignore\n\u2502  \u251c\u2500 config\n\u2502  \u2514\u2500 tmp\n\u251c\u2500 deployment\n\u2502  \u251c\u2500 docker-compose\n\u2502  \u2502  \u251c\u2500 docker-compose.yml\n\u2502  \u251c\u2500 minikube-dep\n\u2502  \u2502  \u251c\u2500 inference-test-services_dep.yaml\n\u2502  \u251c\u2500 startup_minikube_with_mount.sh.sh\n\u251c\u2500 Dockerfile # for inference service\n\u251c\u2500 dvc-cache # services should push and pull from this cache folder and see this as the DVC repo\n\u251c- dvc.yaml\n\u251c- params.yaml\n\u251c\u2500 src\n\u2502  \u251c\u2500 build_features.py\n|  \u251c\u2500 preprocess_data.py\n|  \u251c\u2500 serve_model.py\n|  \u251c\u2500 startup.sh  \n|  \u251c\u2500 requirements.txt\n\u251c\u2500 test_dep\n\u2502  \u251c\u2500 .dvc # same as .dvc in the root folder\n|  |  \u251c\u2500...\n\u2502  \u251c\u2500 Dockerfile # for test service\n\u2502  \u251c\u2500 dvc.yaml\n|  \u251c\u2500 params.yaml\n\u2502  \u2514\u2500 src\n\u2502     \u251c\u2500 build_features.py # same as root src folder\n|     \u251c\u2500 preprocess_data.py # same as root src folder\n|     \u251c\u2500 serve_model.py # same as root src folder\n|     \u251c\u2500 startup_test.sh  \n|     \u251c\u2500 requirements.txt  # same as root src folder\n<\/code><\/pre>\n<p><strong>dvc.yaml<\/strong><\/p>\n<pre><code>stages:\n  preprocess:\n    cmd: python ${preprocess.script}\n    params:\n      - preprocess\n    deps:\n      - ${preprocess.script}\n      - ${preprocess.input_train}\n      - ${preprocess.input_val}\n      - ${preprocess.input_test}\n    outs:\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n  featurize:\n    cmd: python ${featurize.script}\n    params:\n      - preprocess\n      - featurize\n    deps:\n      - ${featurize.script}\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n    outs:\n      - ${featurize.output_train}\n      - ${featurize.output_val}\n      - ${featurize.output_test}\n      - ${featurize.mlb_out}\n      - ${featurize.tfidf_vectorizer_out}\n  train:\n    cmd: python ${train.script}\n    params:\n      - featurize\n      - train\n    deps:\n      - ${train.script}\n      - ${featurize.output_train}\n    outs:\n      - ${train.model_out}\n  evaluate:\n    cmd: python ${evaluate.script}\n    params:\n      - featurize\n      - train\n      - evaluate\n    deps:\n      - ${evaluate.script}\n      - ${train.model_out}\n      - ${featurize.output_val}\n    metrics:\n      - ${evaluate.scores_path}\n<\/code><\/pre>\n<p><strong>params.yaml<\/strong><\/p>\n<pre><code>preprocess:\n  script: src\/preprocess\/preprocess_data.py\n  input_train: data\/raw\/train.tsv\n  input_val: data\/raw\/validation.tsv\n  input_test: data\/raw\/test.tsv\n  output_train: data\/processed\/train_preprocessed.pkl\n  output_val: data\/processed\/validation_preprocessed.pkl\n  output_test: data\/processed\/test_preprocessed.pkl\n\nfeaturize:\n  script: src\/features\/build_features.py\n  output_train: data\/interim\/train_featurized.pkl\n  output_val: data\/interim\/validation_featurized.pkl\n  output_test: data\/interim\/test_featurized.pkl\n  mlb_out: models\/mlb.pkl\n  tfidf_vectorizer_out: models\/tfidf_vectorizer.pkl\n\ntrain:\n  script: src\/models\/train_model.py\n  model_out: models\/model.pkl\n\nevaluate:\n  script: src\/models\/evaluate_model.py\n  scores_path: reports\/scores.json\n  roc_json: reports\/roc_plot.json\n  prc_json: reports\/prc_plot.json\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654718868090,
        "Question_score":0,
        "Question_tags":"docker|kubernetes|minikube|dvc",
        "Question_view_count":196,
        "Owner_creation_time":1562403039337,
        "Owner_last_access_time":1659463717997,
        "Owner_location":null,
        "Owner_reputation":298,
        "Owner_up_votes":14,
        "Owner_down_votes":1,
        "Owner_views":47,
        "Question_last_edit_time":1655157056467,
        "Answer_body":"<p>After running <code>dvc repro<\/code> in <code>test-service<\/code>, a new <code>dvc.lock<\/code> will be created, containing the file hashes relative to your pipeline (i.e. the hash for <code>models\/model.pkl<\/code> etc).<\/p>\n<p>If you're running a shared cache, <code>inference-service<\/code> should have access to the updated <code>dvc.lock<\/code>. If that is present, it will be sufficient to run <code>dvc checkout<\/code> to populate the workspace with the files corresponding to the hashes in the shared cache.<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1654875420463,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72551630",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70928144,
        "Question_title":"Multiple users in DVC",
        "Question_body":"<p>I would like to ask if it is possible to use DVC with several accounts on the same machine. At the moment, all commands (<code>dvc pull<\/code>, <code>dvc push<\/code>, ...) are executed under my name. But after several people joined this project too, I do not want them to execute commands under my name.<\/p>\n<p>When I was alone on this project I generated ssh key:<\/p>\n<pre><code>ssh-keygen\n<\/code><\/pre>\n<p>Connected to server where DVC remote data is stored:<\/p>\n<pre><code>ssh-copy-id username@server_IP\n<\/code><\/pre>\n<p>Created config file which lets me execute all <code>dvc<\/code> commands using ssh:<\/p>\n<pre><code>[core]\n    remote = storage_server\n['remote &quot;storage_server&quot;']\n    url = ssh:\/\/username@server_IP:\/home\/DVC_remote\/DVC_project\n<\/code><\/pre>\n<p>What I should do so that several people could execute commands on their own name?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643641422880,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":181,
        "Owner_creation_time":1457469301700,
        "Owner_last_access_time":1663223040440,
        "Owner_location":null,
        "Owner_reputation":585,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You need to make the &quot;username&quot; part of the config personalized based on who is running the command. There are a few options to do this (based on <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#available-parameters-per-storage-type\" rel=\"nofollow noreferrer\">this document<\/a>, see the SSH part):<\/p>\n<h2>Basic options are:<\/h2>\n<ul>\n<li>User defined in the SSH config file (e.g. <code>~\/.ssh\/config<\/code>) for this host (URL);<\/li>\n<li>Current system user;<\/li>\n<\/ul>\n<p>So, the simplest even options could be just remove it from the URL and rely on the current system user?<\/p>\n<h2>Local (git-ignored or per-project DVC config) config<\/h2>\n<p>You could do is to remove the <code>username<\/code> part from the <code>url<\/code> and run something like this:<\/p>\n<pre><code>dvc remote modify --local storage_server user username\n<\/code><\/pre>\n<p><code>--local<\/code> here means that DVC will create a separate additional config that will be ignored by Git. This way if every user runs this command in every project they use they will customize the username.<\/p>\n<hr \/>\n<p>Let me know if that helps or something doesn't work. I'll try to help.<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1643651143317,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70928144",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67209146,
        "Question_title":"DVC - make scheduled csv dumps",
        "Question_body":"<p>Suppose we got some database (any database, that supports csv dumping), collecting raw data in real time for further usage in ML.\nOn the other side, we got DVC, that can work with csv files.<\/p>\n<p>I want to organize a scheduled run of stored SELECT to that DB with datetime parameters (and also support a manual run), to make a new csv files, and send them to DVC.<\/p>\n<p>In DVC documentation and examples I found, csv file already exists.<\/p>\n<p>Can I make this interaction with database with DVC itself, or I got something wrong, and there is a separate tool for csv dump?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619078487020,
        "Question_score":2,
        "Question_tags":"export-to-csv|dvc",
        "Question_view_count":65,
        "Owner_creation_time":1580932981007,
        "Owner_last_access_time":1663744754923,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There are 3 steps in this process:<\/p>\n<ol>\n<li>Create a CSV dump. Many DBs have these tools but DVC does not support this natively.<\/li>\n<li>Version the CSV dump and move it to some storage. DVC does this job.<\/li>\n<li>Schedule periodical dump. You can use Cron (easy), AirFlow (not easy) or <a href=\"https:\/\/docs.github.com\/en\/actions\/reference\/events-that-trigger-workflows\" rel=\"nofollow noreferrer\">periodical jobs in GitHub Actions<\/a>\/<a href=\"https:\/\/docs.gitlab.com\/ee\/ci\/pipelines\/schedules.html\" rel=\"nofollow noreferrer\">GitLab CI\/CD<\/a>. Another project from the DVC team can help with CI\/CD option: <a href=\"https:\/\/cml.dev\" rel=\"nofollow noreferrer\">https:\/\/cml.dev<\/a>.<\/li>\n<\/ol>",
        "Answer_comment_count":0,
        "Answer_creation_time":1619081803750,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1619084940030,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67209146",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":70560288,
        "Question_title":"DVC Shared Windows Directory Setup",
        "Question_body":"<p>I have one Linux machine and one Windows machine for developments. For data sharing, we have set up a shared Windows directory in another Windows machine, which both my Linux and Windows can access.<\/p>\n<p>I am now using <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> for version control of the shared data. To make it easy, I mount the shared Windows folder both in Windows and in Linux development machine. In Windows, it looks like<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>In Linux, it looks like:<\/p>\n<pre><code>[core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>As you can see, Windows and Linux have different mounting points. So my question is: is there a way to make that both Windows and Linux have the same <code>\u00f9rl<\/code> in the DVC configuration file?<\/p>\n<p>If this is impossible, is there another alternative solution for DVC keeps data in remote shared Windows folder? Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641163263100,
        "Question_score":1,
        "Question_tags":"linux|dvc",
        "Question_view_count":128,
        "Owner_creation_time":1331553057367,
        "Owner_last_access_time":1663939436043,
        "Owner_location":null,
        "Owner_reputation":10643,
        "Owner_up_votes":1174,
        "Owner_down_votes":7,
        "Owner_views":504,
        "Question_last_edit_time":1641199464667,
        "Answer_body":"<p>If you are using a local remote this way, you won't be able to have to the same <code>url<\/code> on both platforms since the mount points are different (as you already realized).<\/p>\n<p>The simplest way to configure this would be to pick one (Linux or Windows) <code>url<\/code> to use as your default case that gets git-committed into <code>.dvc\/config<\/code>. On the other platform you (or your users) can override that <code>url<\/code> in the local configuration file: <code>.dvc\/config.local<\/code>.<\/p>\n<p>(Note that <code>.dvc\/config.local<\/code> is a git-ignored file and will not be included in any commits)<\/p>\n<p>So if you wanted Windows to be the default case, in <code>.dvc\/config<\/code> you would have:<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>and on your Linux machine you would add the file <code>.dvc\/config.local<\/code> containing:<\/p>\n<pre><code>['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>See the DVC docs for <code>dvc config --local<\/code> and <code>dvc remote modify --local<\/code> for more details:<\/p>\n<ul>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#description\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/config#description<\/a><\/li>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0,
        "Answer_creation_time":1641179335767,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70560288",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":66477468,
        "Question_title":"Data Version Control with Google Drive Remote: \"googleapiclient.errors.UnknownApiNameOrVersion: name: drive version: v2\"",
        "Question_body":"<p>I'm trying to setup DVC with Google Drive storage as shown <a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote#url-format\" rel=\"nofollow noreferrer\">here<\/a>. So far, I've been unsuccessful in pushing data to the remote. I tried both with and without the Google App setup.<\/p>\n<p>After running a <code>dvc push -v<\/code>, the following exception is shown:<\/p>\n<pre><code>  File &quot;(...)\/anaconda3\/lib\/python3.8\/site-packages\/googleapiclient\/discovery.py&quot;, line 387, in _retrieve_discovery_doc\n    raise UnknownApiNameOrVersion(&quot;name: %s  version: %s&quot; % (serviceName, version))\ngoogleapiclient.errors.UnknownApiNameOrVersion: name: drive  version: v2\n<\/code><\/pre>\n<p>DVC was installed via <code>pip install dvc[gdrive]<\/code>. The <code>pip freeze<\/code> of the concerning packages is:<\/p>\n<pre><code>oauth2client==4.1.3\ngoogle-api-python-client==2.0.1\ndvc==2.0.1\n<\/code><\/pre>\n<p>Any help is thoroughly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1614869465907,
        "Question_score":4,
        "Question_tags":"google-client|dvc",
        "Question_view_count":979,
        "Owner_creation_time":1593006899210,
        "Owner_last_access_time":1664068381880,
        "Owner_location":null,
        "Owner_reputation":704,
        "Owner_up_votes":53,
        "Owner_down_votes":6,
        "Owner_views":33,
        "Question_last_edit_time":1614874338736,
        "Answer_body":"<p>Can you try to install <code>google-api-python-client==1.12.8<\/code> and test in that way?<\/p>\n<p>Edit:<\/p>\n<p>It appears to be that, this was a bug in the 2.0.0-2.0.1 of google-api-client and resolved in 2.0.2. So this should also work <code>google-api-python-client&gt;=2.0.2<\/code><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1614873489453,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1614927407287,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66477468",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":58952962,
        "Question_title":"How to use different remotes for different folders?",
        "Question_body":"<p>I want my data and models stored in separate Google Cloud buckets. The idea is that I want to be able to share the data with others without sharing the models.<\/p>\n\n<p>One idea I can think of is using separate git submodules for data and models. But that feels cumbersome and imposes some additional requirements from the end user (e.g. having to do <code>git submodule update<\/code>).<\/p>\n\n<p>So can I do this without using git submodules?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1574248229420,
        "Question_score":12,
        "Question_tags":"dvc",
        "Question_view_count":1984,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can first add the different <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\" rel=\"nofollow noreferrer\">DVC remotes<\/a> you want to establish (let's say you call them <code>data<\/code> and <code>models<\/code>, each one pointing to a different <a href=\"https:\/\/cloud.google.com\/storage\/docs\/json_api\/v1\/buckets\" rel=\"nofollow noreferrer\">GC bucket<\/a>). <strong>But don't set any remote as the project's default<\/strong>; This way, <a href=\"https:\/\/dvc.org\/doc\/command-reference\/push\" rel=\"nofollow noreferrer\"><code>dvc push<\/code><\/a> won't work without the <code>-r<\/code> (or <code>--remote<\/code>) option.<\/p>\n<p>You would then need to push each directory or file individually to the appropriate remote, like <code>dvc push data\/ -r data<\/code> and <code>dvc push model.dat -r models<\/code>.<\/p>\n<p>Note that a feature request to configure this exists on the DVC repo too. See <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2095\" rel=\"nofollow noreferrer\">Specify file types that can be pushed to remote<\/a>.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1574267475363,
        "Answer_score":13,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1642527991692,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58952962",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67744934,
        "Question_title":"Is it possible to check that the version of a file tracked by a DVC metadata file exists in remote storage without pulling the file?",
        "Question_body":"<p>My team has a set up wherein we track datasets and models in DVC, and have a GitLab repository for tracking our code and DVC metadata files. We have a job in our dev GitLab pipeline (run on each push to a merge request) that has the goal of checking to be sure that the developer remembered to run <code>dvc push<\/code> to keep DVC remote storage up-to-date. Right now, the way we do this is by running <code>dvc pull<\/code> on the GitLab runner, which will fail with errors telling you which files (new files or latest versions of existing files) were not found.<\/p>\n<p>The downside to this approach is that we are loading the entirety of our data stored in DVC onto a GitLab runner, and we've run into out-of-memory issues, not to mention lengthy run time to download all that data. Since the path and md5 hash of the objects are stored in the DVC metadata files, I would think that's all the information that DVC would need to be able to answer the question &quot;is the remote storage system up-to-date&quot;.<\/p>\n<p>It seems like <code>dvc status<\/code> is similar to what I'm asking for, but compares the cache or workspace and remote storage. In other words, it requires the files to actually be present on whatever filesystem is making the call.<\/p>\n<p>Is there some way to achieve the goal I laid out above (&quot;inform the developer that they need to run <code>dvc push<\/code>&quot;) without pulling everything from DVC?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622232629793,
        "Question_score":5,
        "Question_tags":"git|gitlab|continuous-integration|dvc",
        "Question_view_count":488,
        "Owner_creation_time":1618255062697,
        "Owner_last_access_time":1645555346683,
        "Owner_location":null,
        "Owner_reputation":75,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1622257491983,
        "Answer_body":"<blockquote>\n<p>It seems like dvc status is similar to what I'm asking for<\/p>\n<\/blockquote>\n<p><code>dvc status --cloud<\/code> will give you a list of &quot;new&quot; files if they that haven't been pushed to the (default) remote. It won't error out though, so your CI script should fail depending on the stdout message.<\/p>\n<p>More info: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/status#options\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/status#options<\/a><\/p>\n<p>I'd also ask everyone to run <code>dvc install<\/code>, which will setup some Git hooks, including automatic <code>dvc push<\/code> with <code>git push<\/code>.<\/p>\n<p>See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/install\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/install<\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1622257759210,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1622503453296,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67744934",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":69265000,
        "Question_title":"DVC - Forbidden: An error occurred (403) when calling the HeadObject operation",
        "Question_body":"<p>I just started with DVC. following are the steps I am doing to push my models on S3<\/p>\n<p>Initialize<\/p>\n<pre><code>dvc init\n<\/code><\/pre>\n<p>Add bucket url<\/p>\n<pre><code>dvc remote add -d storage s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>\n<p>add some files<\/p>\n<pre><code>dvc add somefiles\n<\/code><\/pre>\n<p>Add aws keys<\/p>\n<pre><code>dvc remote modify storage access_key_id AWS_ACCESS_KEY_ID\ndvc remote modify storage secret_access_key AWS_SECRET_ACCESS_KEY\n<\/code><\/pre>\n<p>now when I push<\/p>\n<pre><code>dvc push\n<\/code><\/pre>\n<p>it shows<\/p>\n<pre><code>ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n<p>Am i missing something?<\/p>\n<p><strong>update1<\/strong><\/p>\n<p>result of <code>dvc doctor<\/code><\/p>\n<pre><code>C:\\my-server&gt;dvc doctor\nDVC version: 2.7.4 (pip)\n---------------------------------\nPlatform: Python 3.8.0 on Windows-10-10.0.19041-SP0\nSupports:\n        http (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        https (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        s3 (s3fs = 2021.8.1, boto3 = 1.17.106)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: s3\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n<\/code><\/pre>\n<p>and the <code>dvc push-vv<\/code><\/p>\n<pre><code>C:\\my-server&gt;dvc push -vv  \n2021-09-21 13:21:38,382 TRACE: Namespace(all_branches=False, all_commits=False, all_tags=False, cd='.', cmd='push', cprofile=False, cprofile_dump=None, func=&lt;class 'dvc.command.data_sync.CmdDataPush'&gt;, glob=False, instrument=False, instrument_open=False, jobs=None, pdb=False, quiet=0, recursive=False, remote=None, run_cache=False, targets=[], verbose=2, version=None, with_deps=False)\n2021-09-21 13:21:39,293 TRACE: Assuming 'C:\\my-server\\.dvc\\cache\\02\\5b196462b86d2f10a9f659e2224da8.dir' is unchanged since \nit is read-only\n2021-09-21 13:21:39,296 TRACE: Assuming 'C:\\my-server\\.dvc\\cache\\02\\5b196462b86d2f10a9f659e2224da8.dir' is unchanged since \nit is read-only\n2021-09-21 13:21:40,114 DEBUG: Preparing to transfer data from '.dvc\\cache' to 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,117 DEBUG: Preparing to collect status from 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,119 DEBUG: Collecting status from 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,121 DEBUG: Querying 1 hashes via object_exists\n2021-09-21 13:21:44,840 ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 248, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\aiobotocore\\client.py&quot;, line 155, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (InvalidAccessKeyId) when calling the ListObjectsV2 operation: The AWS Access Key Id you provided does not exist in our records.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1080, in _info\n    out = await self._simple_info(path)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 993, in _simple_info\n    out = await self._call_s3(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 268, in _call_s3\n    raise err\nPermissionError: The AWS Access Key Id you provided does not exist in our records.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 248, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\aiobotocore\\client.py&quot;, line 155, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\main.py&quot;, line 55, in main\n    ret = cmd.do_run()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\command\\base.py&quot;, line 45, in do_run\n    return self.run()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\command\\data_sync.py&quot;, line 57, in run\n    processed_files_count = self.repo.push(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\repo\\__init__.py&quot;, line 50, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\repo\\push.py&quot;, line 48, in push\n    pushed += self.cloud.push(obj_ids, jobs, remote=remote, odb=odb)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\data_cloud.py&quot;, line 85, in push\n    return transfer(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\transfer.py&quot;, line 153, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 160, in compare_status\n    dest_exists, dest_missing = status(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 122, in status\n    exists = hashes.intersection(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 48, in _indexed_dir_hashes\n    dir_exists.update(odb.list_hashes_exists(dir_hashes - dir_exists))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\db\\base.py&quot;, line 415, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 611, in result_iterator\n    yield fs.pop().result()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 439, in result\n    return self.__get_result()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 388, in __get_result\n    raise self._exception\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\db\\base.py&quot;, line 406, in exists_with_progress\n    ret = self.fs.exists(path_info)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\fs\\fsspec_wrapper.py&quot;, line 97, in exists\n    return self.fs.exists(self._with_bucket(path_info))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 88, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 69, in sync\n    raise result[0]\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 25, in _runner\n    result[0] = await coro\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 820, in _exists\n    await self._info(path, bucket, key, version_id=version_id)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1084, in _info\n    out = await self._version_aware_info(path, version_id)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1027, in _version_aware_info\n    out = await self._call_s3(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 268, in _call_s3\n    raise err\nPermissionError: Forbidden\n------------------------------------------------------------\n2021-09-21 13:21:45,178 DEBUG: Version info for developers:\nDVC version: 2.7.4 (pip)\n---------------------------------\nPlatform: Python 3.8.0 on Windows-10-10.0.19041-SP0\nSupports:\n        http (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        https (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        s3 (s3fs = 2021.8.1, boto3 = 1.17.106)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: s3\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2021-09-21 13:21:45,185 DEBUG: Analytics is enabled.\n2021-09-21 13:21:45,446 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\sgarg\\\\AppData\\\\Local\\\\Temp\\\\tmpm_p9f3eq']'\n2021-09-21 13:21:45,456 DEBUG: Spawned '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\sgarg\\\\AppData\\\\Local\\\\Temp\\\\tmpm_p9f3eq']'\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1632209467140,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":816,
        "Owner_creation_time":1363322632587,
        "Owner_last_access_time":1664084323070,
        "Owner_location":"Chandigarh, India",
        "Owner_reputation":13237,
        "Owner_up_votes":2454,
        "Owner_down_votes":19,
        "Owner_views":2675,
        "Question_last_edit_time":1632236873616,
        "Answer_body":"<p>Could you please run <code>dvc doctor<\/code> and rerun <code>dvc push<\/code> and add <code>-vv<\/code> flag. And give the two results?<\/p>\n<pre><code>PermissionError: The AWS Access Key Id you provided does not exist in our records.\n<\/code><\/pre>\n<p>Does the <code>aws cli<\/code> works correctly for you? First setup <code>AWS_ACCESS_KEY_ID<\/code> and <code>AWS_SECRET_ACCESS_KEY<\/code> in envs then<\/p>\n<pre><code>aws s3 ls s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>",
        "Answer_comment_count":4,
        "Answer_creation_time":1632210565513,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1632211831430,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69265000",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":68082912,
        "Question_title":"git-ignore dvc.lock in repositories where only the DVC pipelines are used",
        "Question_body":"<p>I want to use the pipeline functionality of dvc in a git repository. The data is managed otherwise and should not be versioned by dvc. The only functionality which is needed is that dvc reproduces the needed steps of the pipeline when <code>dvc repro<\/code> is called. Checking out the repository on a new system should lead to an 'empty' repository, where none of the pipeline steps are stored.<\/p>\n<p>Thus, - if I understand correctly - there is no need to track the dvc.lock file in the repository. However, adding dvc.lock to the .gitginore file leads to an error message:<\/p>\n<pre><code>ERROR: 'dvc.lock' is git-ignored.\n<\/code><\/pre>\n<p>Is there any way to disable the dvc.lock in .gitignore check for this usecase?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624362030233,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":493,
        "Owner_creation_time":1542537900087,
        "Owner_last_access_time":1663940500713,
        "Owner_location":null,
        "Owner_reputation":147,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This is definitely possible, as DVC features are loosely coupled to one another. You can do pipelining by writing your dvc.yaml file(s), but avoid data management\/versioning by using <code>cache: false<\/code> in the stage outputs (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#output-subfields\" rel=\"nofollow noreferrer\"><code>outs<\/code> field<\/a>). See also helper <code>dvc stage add -O<\/code> (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/stage\/add#options\" rel=\"nofollow noreferrer\">big O<\/a>, alias of <code>--outs-no-cache<\/code>).<\/p>\n<p>And the same for initial data dependencies, you can <code>dvc add --no-commit<\/code> them (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#options\" rel=\"nofollow noreferrer\">ref<\/a>).<\/p>\n<p>You do want to track <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#dvclock-file\" rel=\"nofollow noreferrer\">dvc.lock<\/a> in Git though, so that DVC can determine the latest stage of the pipeline associated with the Git commit in every repo copy or branch.<\/p>\n<p>You'll be responsible for placing the right data files\/dirs (matching .dvc files and dvc.lock) in the workspace for <code>dvc repro<\/code> or <code>dvc exp run<\/code> to behave as expected. <code>dvc checkout<\/code> won't be able to help you.<\/p>",
        "Answer_comment_count":7,
        "Answer_creation_time":1624393487733,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68082912",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":64456396,
        "Question_title":"How do I unit test a function in the CI pipeline that uses model files that are not part of the git remote?",
        "Question_body":"<p>I am developing machine learning repositories that require fairly large trained model files to run. These files are not part of the git remote but is tracked by DVC and is saved in a separate remote storage. I am running into issues when I am trying to run unit tests in the CI pipeline for functions that require these model files to make their prediction. Since I don't have access them in the git remote, I can't test them.<\/p>\n<p>What is the best practice that people usually do in this situation? I can think of couple of options -<\/p>\n<ul>\n<li>Pull the models from the DVC remote inside the CI pipeline. I don't want to do this becasue downloading models every time you want to run push some code will quickly eat up my usage minutes for CI and is an expensive option.<\/li>\n<li>Use <code>unittest.mock<\/code> to simulate the output of from the model prediction and test other parts of my code. This is what I am doing now but it's sort of a pain with unittest's mock functionalities. That module wasn't really developed with ML in mind from what I can tell. It's missing (or is hard to find) some functionalities that I would have really liked. Are there any good tools for doing this geared specifically towards ML?<\/li>\n<li>Do weird reformatting of the function definition that allows me to essentially do option 2 but without a mock module. That is, just test the surrounding logic and don't worry about the model output.<\/li>\n<li>Just put the model files in the git remote and be done with it. Only use DVC to track data.<\/li>\n<\/ul>\n<p>What do people usually do in this situation?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603252093370,
        "Question_score":4,
        "Question_tags":"unit-testing|machine-learning|mocking|continuous-integration|dvc",
        "Question_view_count":289,
        "Owner_creation_time":1446746840593,
        "Owner_last_access_time":1664083975707,
        "Owner_location":null,
        "Owner_reputation":2545,
        "Owner_up_votes":845,
        "Owner_down_votes":386,
        "Owner_views":382,
        "Question_last_edit_time":1603305923907,
        "Answer_body":"<p>If we talk about unit tests, I think it's indeed better to do a mock. It's best to have unit tests small, testing actual logic of the unit, etc. It's good to have other tests though that would pull the model and run some logic on top of that - I would call them integration tests.<\/p>\n<p>It's not black and white though. If you for some reason see that it's easier to use an actual model (e.g. it changes a lot and it is easier to use it instead of maintaining and updating stubs\/fixtures), you could potentially cache it.<\/p>\n<p>I think, to help you with the mock, you would need to share some technical details- how does the function look like, what have you tried, what breaks, etc.<\/p>\n<blockquote>\n<p>to do this because downloading models every time you want to run push some code will quickly eat up my usage minutes for CI and is an expensive option.<\/p>\n<\/blockquote>\n<p>I think you can potentially utilize CI systems cache to avoid downloading it over and over again. This is the GitHub Actions related <a href=\"https:\/\/github.com\/actions\/cache#cache-limits\" rel=\"nofollow noreferrer\">repository<\/a>, this is <a href=\"https:\/\/circleci.com\/docs\/2.0\/caching\" rel=\"nofollow noreferrer\">CircleCI<\/a>. The idea is the same across all common CI providers. Which one are considering to use, btw?<\/p>\n<blockquote>\n<p>Just put the model files in the git remote and be done with it. Only use DVC to track data.<\/p>\n<\/blockquote>\n<p>This can be the way, but if models are large enough you will pollute Git history significantly. On some CI systems it can become even slower since they will be fetching this with regular <code>git clone<\/code>. Effectively, downloading models anyway.<\/p>\n<p>Btw, if you use DVC or not take a look at another open-source project that is made specifically to do CI\/CD for ML - <a href=\"https:\/\/cml.dev\" rel=\"nofollow noreferrer\">CML<\/a>.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1603314290770,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1603325349590,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64456396",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67407702,
        "Question_title":"Corrupted dvc.lock",
        "Question_body":"<p>I'm using DAGsHub storage as a remote and running into the following error message (when trying to DVC pull):<\/p>\n<blockquote>\n<p>ERROR: Lockfile 'bias_tagging_model\/dvc.lock' is corrupted.<\/p>\n<\/blockquote>\n<p>I thought I might have messed something up, but when cloning the git repo again and DVC pulling I am still running into this.\nThe data looks ok when viewed in the browser.\nIf you have any ideas, I would appreciate your help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1620243318287,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":362,
        "Owner_creation_time":1620132740443,
        "Owner_last_access_time":1641927840700,
        "Owner_location":"New York, NY, USA",
        "Owner_reputation":75,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Usually, the reason for this error is the DVC version.<\/p>\n<p>If the dvc.lock file has a DVC 2.* schema and you are using a lower version, it will throw this error.<\/p>\n<p>Upgrade your DVC version, and it should work.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1620292401493,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67407702",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":57132106,
        "Question_title":"Expanding environment variables in the command part of a dvc run",
        "Question_body":"<p><strong>Summary<\/strong>: I am trying to define a <code>dvc<\/code> step using <code>dvc-run<\/code> where the command depends on some environment variables (for instance <code>$HOME<\/code>). The problem is that when I'm defining the step on machine A, then the variable is expanded when stored in the <code>.dvc<\/code> file. In this case, it won't be possible to reproduce the step on machine B. Did I hit a limitation of <code>dvc<\/code>? If that's not the case, what's the right approach?<\/p>\n\n<p><strong>More details<\/strong>: I faced the issue when trying to define a step for which the command is a <code>docker run<\/code>. Say that:<\/p>\n\n<ul>\n<li>on machine A <code>myrepo<\/code> is located at <code>\/Users\/user\/myrepo<\/code> and <\/li>\n<li>on machine B it is to be found at <code>\/home\/ubuntu\/myrepo<\/code>. <\/li>\n<\/ul>\n\n<p>Furthermore, assume I have a script <code>myrepo\/script.R<\/code> which processes a data file to be found at <code>myrepo\/data\/mydata.txt<\/code>. Lastly, assume that my step's command is something like: <\/p>\n\n<pre><code>docker run -v $HOME\/myrepo\/:\/prj\/ my_docker_image \/prj\/script.R \/prj\/data\/mydata.txt\n<\/code><\/pre>\n\n<p>If I'm running <code>dvc run -f step.dvc -d ... -d ... [cmd]<\/code> where <code>cmd<\/code> is the <code>docker<\/code> execution above, then in <code>step.dvc<\/code> the environment variable <code>$HOME<\/code> will be expanded. In this case, the step will be broken on machine B.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1563703426437,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":329,
        "Owner_creation_time":1300789717227,
        "Owner_last_access_time":1663941101560,
        "Owner_location":null,
        "Owner_reputation":11410,
        "Owner_up_votes":2846,
        "Owner_down_votes":6,
        "Owner_views":1782,
        "Question_last_edit_time":null,
        "Answer_body":"<p>From <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/run\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>Use single quotes ' instead of \" to wrap the command if there are environment variables in it, that you want to be evaluated dynamically. E.g. dvc run -d script.sh '.\/myscript.sh $MYENVVAR'<\/p>\n<\/blockquote>",
        "Answer_comment_count":2,
        "Answer_creation_time":1563988872483,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1564319630556,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57132106",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":58163305,
        "Question_title":"dvc gc and files in remote cache",
        "Question_body":"<p>dvc documentation for <code>dvc gc<\/code> command states, the <code>-r<\/code> option indicates \"Remote storage to collect garbage in\" but I'm not sure if I understand it correctly. For example I execute this command:<\/p>\n\n<pre><code>dvc gc -r myremote\n<\/code><\/pre>\n\n<p>What exactly happens if I execute this command? I have 2 possible answers:<\/p>\n\n<ol>\n<li>dvc checks which files should be deleted, then moves these files to \"myremote\" and then deletes all these files in local cache but not in remote.<\/li>\n<li>dvc checks which files should be deleted and deletes these files both in local cache and \"myremote\"<\/li>\n<\/ol>\n\n<p>Which one of them is correct?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1569828438393,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":1617,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":null,
        "Answer_body":"<p>one of DVC maintainers here.<\/p>\n\n<p>Short answer: 2. is correct.<\/p>\n\n<p>A bit of additional information:\nPlease be careful when using <code>dvc gc<\/code>. It will clear your cache from all dependencies that are not mentioned in the current HEAD of your git repository. \nWe are working on making <code>dvc gc<\/code> preserving whole history by default. <\/p>\n\n<p>So if you don't want to delete files from your history commits, it would be better to wait for completion of <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2325\" rel=\"nofollow noreferrer\">this<\/a> task.<\/p>\n\n<p>[EDIT]\nPlease see comment below.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1569833846627,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1569837069043,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58163305",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":65416056,
        "Question_title":"Data Version Control: Absolute Paths and Project Paths in the Pipeline Parameters?",
        "Question_body":"<p>In DVC one may define pipelines.  In Unix, one typically does not work at the root level.  Further, DVC expects files to be inside the git repository.<\/p>\n<p>So, this seems like a typical problem.<\/p>\n<p>Suppose I have the following:<\/p>\n<pre><code>\/home\/user\/project\/content-folder\/data\/data-type\/cfg.json\n\/home\/user\/project\/content-folder\/app\/foo.py\n<\/code><\/pre>\n<p>Git starts at <code>\/home\/user\/project\/<\/code><\/p>\n<pre><code>cd ~\/project\/content-folder\/data\/data-type\n..\/..\/app\/foo.py do-this --with cfg.json --dest $(pwd) \n<\/code><\/pre>\n<p>Seems reasonable to me: the script takes a configuration, which is stored in a particular location, runs it against some encapsulated functionality, and outputs it to the destination using an absolute path.<\/p>\n<p>The default behavior of <code>--dest<\/code> is to output to the current working directory.  This seems like another reasonable default.<\/p>\n<hr \/>\n<p>Next, I go to configure the <code>params.yaml<\/code> file for <code>dvc<\/code>, and I am immediately confusing and unsure what is going to happen.  I write:<\/p>\n<pre><code>foodoo:\n  params: do-this --with ????\/cfg.json --dest ????\n<\/code><\/pre>\n<p>What I want to write (and would in a shell script):<\/p>\n<pre><code>#!\/usr\/bin\/env bash\norigin:=$(git rev-parse --show-toplevel)\n\nverb=do-this\nparams=--with $(origin)\/content-folder\/data\/data-type\/cfg.json --dest $(origin)\/content-folder\/data\/data-type\n<\/code><\/pre>\n<hr \/>\n<p>But, in DVC, the pathing seems to be implicit, and I do not know where to start as either:<\/p>\n<ol>\n<li>DVC will calculate the path to my script locally<\/li>\n<li>Not calculate the path to my script locally<\/li>\n<\/ol>\n<p>Which is fine -- I can discover that.  But I am reasonably sure that DVC will absolutely not prefix the directory and file params in my params.yaml with the path to my project.<\/p>\n<hr \/>\n<p>How does one achieve path control that does not assume a fixed project location, like I would in BASH?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1608672730887,
        "Question_score":3,
        "Question_tags":"git|path|dvc",
        "Question_view_count":448,
        "Owner_creation_time":1405262190020,
        "Owner_last_access_time":1663968225423,
        "Owner_location":"Atlanta, GA",
        "Owner_reputation":26244,
        "Owner_up_votes":434,
        "Owner_down_votes":35,
        "Owner_views":1383,
        "Question_last_edit_time":null,
        "Answer_body":"<p>By default, DVC will run your stage command from the same directory as the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files#dvcyaml-file\" rel=\"nofollow noreferrer\">dvc.yaml<\/a> file. If you need to run the command from a different location, you can specify an alternate working directory via <code>wdir<\/code>, which should be a path relative to <code>dvc.yaml<\/code>'s location.<\/p>\n<p>Paths for everything else in your stage (like <code>params.yaml<\/code>) should be specified as relative to <code>wdir<\/code> (or relative to <code>dvc.yaml<\/code> if <code>wdir<\/code> is not provided).<\/p>\n<p>Looking at your example, there also seems to be a bit of confusion on parameters in DVC. In a DVC stage, <code>params<\/code> is for specifying <a href=\"https:\/\/dvc.org\/doc\/command-reference\/params\" rel=\"nofollow noreferrer\">parameter dependencies<\/a>, not used for specifying command-line flags. The full command including flags\/options should be included  the <code>cmd<\/code> section for your stage. If you wanted to make sure that your stage was rerun every time certain values in <code>cfg.json<\/code> have changed, your stage's <code>params<\/code> section would look something like:<\/p>\n<pre><code>params:\n  &lt;relpath from dvc.yaml&gt;\/cfg.json:\n    - param1\n    - param2\n    ...\n<\/code><\/pre>\n<p>So your example <code>dvc.yaml<\/code> would look something like:<\/p>\n<pre><code>stages:\n  foodoo:\n    cmd: &lt;relpath from dvc.yaml&gt;\/foo.py do-this --with &lt;relpath from dvc.yaml&gt;\/cfg.json --dest &lt;relpath from dvc.yaml&gt;\/...\n    deps:\n      &lt;relpath from dvc.yaml&gt;\/foo.py\n    params:\n      &lt;relpath from dvc.yaml&gt;\/cfg.json:\n        ...\n    ...\n<\/code><\/pre>\n<p>This would make the command <code>dvc repro<\/code> rerun your stage any time that the code in foo.py has changed, or the specified parameters in <code>cfg.json<\/code> have changed.<\/p>\n<p>You may also want to refer to the docs for <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#run\" rel=\"nofollow noreferrer\">dvc run<\/a>, which can be used to generate or update a <code>dvc.yaml<\/code> stage (rather than writing <code>dvc.yaml<\/code> by hand)<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1608686869860,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65416056",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":73435172,
        "Question_title":"Is there any way to log 'git hash' in hydra?",
        "Question_body":"<p>I want to control version of experiment configuration files with hydra and dvc without uploading original config files to git.<br \/>\nHydra does control config, and dvc controls version. But Hydra does not specify which 'code version' is needed to reproduce experiment. And I don't want to add 'git hash logging code' in every experiments.<\/p>\n<p>Is there any way to log git hash to hydra log in default? thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661090494277,
        "Question_score":1,
        "Question_tags":"git|fb-hydra|dvc",
        "Question_view_count":43,
        "Owner_creation_time":1603378831587,
        "Owner_last_access_time":1663547314893,
        "Owner_location":null,
        "Owner_reputation":161,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Good timing! A DVC-Hydra integration is in development. You can see the proposal in <a href=\"https:\/\/github.com\/iterative\/dvc\/discussions\/7044#discussioncomment-3271855\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/discussions\/7044#discussioncomment-3271855<\/a> and the development progress in <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/8093\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/pull\/8093<\/a>. This should allow you to take a Hydra config, pass your Hydra overrides via <code>dvc exp run --set-param=&lt;hydra_overrides&gt;<\/code>, and capture the output with DVC.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1661173080163,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73435172",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67122683,
        "Question_title":"DVC Files Incomplete",
        "Question_body":"<p>I'm in a team using dvc with git to version-control data files. We are using dvc 1.3.1, with the an S3 bucket remote. I'm getting this error when executing <code>dvc fetch<\/code> or <code>dvc pull<\/code> on a colleague's branch:<\/p>\n<pre><code>ERROR: failed to fetch data from the cloud - DVC-file 'C:\\Users\\blah\\Documents\\repo\\data\\processed_data.dvc' format error: extra keys not allowed @ data['outs'][0]['size']\n<\/code><\/pre>\n<p>When I check the dvc file for a cached file with which I have no problem I see this:<\/p>\n<pre><code>md5: ded591aacbe363f0518ceb9c3bc1836b\nouts:\n- md5: efdab20e8b59903b9523cc188ff727e5\n  path: completion_header.p\n  cache: true\n  metric: false\n  persist: false\n<\/code><\/pre>\n<p>but a problematic file only has this:<\/p>\n<pre><code>outs:\n- md5: f4e15187d9a0bbb328e629eabd8d1784.dir\n  size: 112007\n  nfiles: 3\n  path: processed_data\n<\/code><\/pre>\n<p>In all cases, files are added to dvc with the command <code>dvc add %dirname%<\/code>. This is the second time I've seen this on a colleague's branch (2 different people).<\/p>\n<p>Since posting, I have realized that my colleague dvc'd a directory. I have attempted creating the directory first, then calling <code>dvc fetch<\/code>, but get the same error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618565400113,
        "Question_score":2,
        "Question_tags":"git|dvc",
        "Question_view_count":548,
        "Owner_creation_time":1348150034833,
        "Owner_last_access_time":1663852884933,
        "Owner_location":"Glasgow, UK",
        "Owner_reputation":2400,
        "Owner_up_votes":66,
        "Owner_down_votes":12,
        "Owner_views":263,
        "Question_last_edit_time":1618826341416,
        "Answer_body":"<blockquote>\n<p>In all cases, files are added to dvc with the command dvc add %filename%.<\/p>\n<\/blockquote>\n<p>It seems like there is a high chance that one of the dvc files created in newer versions of dvc and you are trying to operate with an older version. Are all of your colleagues use the same dvc version when adding new files?<\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1618566517710,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67122683",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67454531,
        "Question_title":"Git bash command prompt hanging when running dvc push to DAGsHub",
        "Question_body":"<p>I'm having problems pushing files with DVC to DAGsHub.<\/p>\n<p>Workflow:<\/p>\n<ul>\n<li>I used my email to signup to DAGsHub.<\/li>\n<li>I created a repo and clone it to my computer.<\/li>\n<li>I added files to the repo and track them using DVC and Git to track the pointer files.<\/li>\n<li>Running DVC push -r origin, it asks me for my password. When I enter the password and hit enter - nothing happens.<\/li>\n<\/ul>\n<p>It sits and waits, barring me from even canceling the operation with Ctrl+C.\nI'm forced to manually close the terminal, open a new one, ending the &quot;Python&quot; process in task manager and delete the lock file in .dvc\/tmp\/lock.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1620537862843,
        "Question_score":5,
        "Question_tags":"dvc",
        "Question_view_count":254,
        "Owner_creation_time":1620537484800,
        "Owner_last_access_time":1642059984007,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1620625906412,
        "Answer_body":"<p><strong>Short answer<\/strong><\/p>\n<p>Do not use <code>ask_password<\/code>.\nInstead, save your token in the local config by running once:<\/p>\n<pre><code>dvc remote modify origin --local --unset ask_password\ndvc remote modify origin --local password &lt;--access token--&gt;\n<\/code><\/pre>\n<p><code>dvc push -r origin<\/code> should work then.<\/p>\n<p><strong>Long answer<\/strong><\/p>\n<p><a href=\"https:\/\/www.atlassian.com\/git\/tutorials\/git-bash#:%7E:text=What%20is%20Git%20Bash%3F,operating%20system%20through%20written%20commands.\" rel=\"nofollow noreferrer\">Git Bash<\/a> is not running the regular Windows command prompt but an emulated Unix-style bash prompt. From the information in your question, I cannot know for sure, but this is probably causing the <code>msvcrt<\/code> package used by DVC to prompt the password on windows machines to fail\/hang.<\/p>\n<p>There are potentially 3 ways to deal with the issue:<\/p>\n<ol>\n<li>Run <code>dvc pull<\/code> from the regular Windows cmd prompt.<\/li>\n<li>Find a way to make Git Bash wrap Python calls with <code>winpty<\/code> - I am not 100% positive about how to do this, but not using <code>winpty<\/code> seems to be the reason <code>msvcrt<\/code> fails at prompting for your password.<\/li>\n<li>The simplest solution - Do not use <code>ask_password<\/code>.\nInstead, save your token in the local config by running once:\n<pre><code>dvc remote modify origin --local --unset ask_password\ndvc remote modify origin --local password &lt;--access token--&gt;\n<\/code><\/pre>\nYou can get your access token by clicking on the question mark beside the DVC\nremote of your DAGsHub repository, then click on &quot;Reveal my token&quot;.<\/li>\n<\/ol>",
        "Answer_comment_count":0,
        "Answer_creation_time":1620591578073,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67454531",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67393339,
        "Question_title":"dvc push, change the names of files on the remote storage",
        "Question_body":"<p>I'm working on a project with DVC (Data Version Control), when I push files in my remote storage, the name of the files are changed. How I can conserve the names?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620170453717,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":447,
        "Owner_creation_time":1379685720450,
        "Owner_last_access_time":1663797728603,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1620171022369,
        "Answer_body":"<p>Short answer: there is no way to do that.<\/p>\n<p>Long answer:\nDvc remote is a content-based storage, so names are not preserved. Dvc creates metafiles (*.dvc files) in your workspace that contain names and those files are usually tracked by git, so you need to use git remote and dvc remote together to have both filenames and their contents. Here is a more detailed explanation about the format of local and remote storage <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory<\/a> . Also, checkout <a href=\"https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1620171651067,
        "Answer_score":5,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67393339",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":58541260,
        "Question_title":"Difference between git-lfs and dvc",
        "Question_body":"<p>What is the difference between these two? We used git-lfs in my previous job and we are starting to use dvc alongside git in my current one. They both place some kind of index instead of file and can be downloaded on demand. Has dvc some improvements over the former one?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1571919586097,
        "Question_score":27,
        "Question_tags":"git|git-lfs|dvc",
        "Question_view_count":6255,
        "Owner_creation_time":1373630643250,
        "Owner_last_access_time":1664025275130,
        "Owner_location":null,
        "Owner_reputation":382,
        "Owner_up_votes":185,
        "Owner_down_votes":1,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":"<p>DVC is a better replacement for <code>git-lfs<\/code>. <\/p>\n\n<p>Unlike git-lfs, DVC doesn't require installing a dedicated server; It can be used on-premises (NAS, SSH, for example) or with any major cloud provider (S3, Google Cloud, Azure).<\/p>\n\n<p>For more information: <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-and-model-files-versioning\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/use-cases\/data-and-model-files-versioning<\/a><\/p>",
        "Answer_comment_count":5,
        "Answer_creation_time":1571925277763,
        "Answer_score":10,
        "Question_favorite_count":5.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58541260",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":68004538,
        "Question_title":"Is dvc.yaml supposed to be written or generated by dvc run command?",
        "Question_body":"<p>Trying to understand <a href=\"https:\/\/dvc.org\/doc\/start\" rel=\"nofollow noreferrer\">dvc<\/a>, most tutorials mention generation of dvc.yaml by running <code>dvc run<\/code> command.<\/p>\n<p>But at the same time, dvc.yaml which defines the DAG is also <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files\" rel=\"nofollow noreferrer\">well documented<\/a>. Also the fact that it is a yaml format and human readable\/writable would point to the fact that it is meant to be a DSL for specifying your data pipeline.<\/p>\n<p>Can somebody clarify which is the better practice?\nWriting the dvc.yaml or let it be generated by <code>dvc run<\/code> command?\nOr is it left to user's choice and there is no technical difference?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1623853195940,
        "Question_score":5,
        "Question_tags":"directed-acyclic-graphs|data-pipeline|dvc",
        "Question_view_count":1101,
        "Owner_creation_time":1243446134993,
        "Owner_last_access_time":1663913878327,
        "Owner_location":"Gothenburg, Sweden",
        "Owner_reputation":1547,
        "Owner_up_votes":28,
        "Owner_down_votes":9,
        "Owner_views":212,
        "Question_last_edit_time":1623873485369,
        "Answer_body":"<p>I'd recommend manual editing as the main route! (I believe that's officially recommended since <a href=\"https:\/\/dvc.org\/blog\/dvc-2-0-release\" rel=\"nofollow noreferrer\">DVC 2.0<\/a>)<\/p>\n<p><code>dvc stage add<\/code> can still be very helpful for programmatic generation of pipelines files, but it doesn't support all the features of <code>dvc.yaml<\/code>, for example setting <code>vars<\/code> values or defining <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#foreach-stages\" rel=\"nofollow noreferrer\"><code>foreach<\/code> stages<\/a>.<\/p>",
        "Answer_comment_count":7,
        "Answer_creation_time":1623859205940,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1623875977407,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68004538",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":53213596,
        "Question_title":"How to execute python from conda environment by dvc run",
        "Question_body":"<p>I have an environment of conda configurated with python 3.6 and dvc is installed there, but when I try to execute dvc run with python, dvc call the python version of main installation of conda and not find the installed libraries.<\/p>\n\n<pre><code>$ conda activate py36\n$ python --version\nPython 3.6.6 :: Anaconda custom (64-bit)\n$ dvc run python --version\nRunning command:\n    python --version\nPython 3.7.0\nSaving information to 'Dvcfile'.\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1541699972677,
        "Question_score":6,
        "Question_tags":"python|anaconda|conda|dvc",
        "Question_view_count":351,
        "Owner_creation_time":1420765480790,
        "Owner_last_access_time":1663614206113,
        "Owner_location":null,
        "Owner_reputation":340,
        "Owner_up_votes":477,
        "Owner_down_votes":2,
        "Owner_views":61,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The version 0.24.3 of dvc correct this problem.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1549414531500,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53213596",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":67215839,
        "Question_title":"Use parameters from additional configs in dvc 2.0",
        "Question_body":"<p>Using dvc version 2.0.18 and python 3.9.2 I want to use parameters defined in a config file different from params.yaml when configuring the parameters of the stages in <code>dvc.yaml<\/code>. However, it does not work as I expected.<\/p>\n<p>MWE:\nGit repo + dvc init:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 dvc.yaml\n\u251c\u2500\u2500 preproc.yaml\n\u2514\u2500\u2500 test.py\n<\/code><\/pre>\n<p>dvc.yaml:<\/p>\n<pre><code>vars:\n  - preproc.yaml\nstages:\n  test:\n    cmd: python test.py\n    deps:\n      - test.py\n    params:\n      - important_parameter\n<\/code><\/pre>\n<p>preproc.yaml:<\/p>\n<pre><code>important_parameter: 123\n<\/code><\/pre>\n<p>Running <code>dvc repro<\/code> lead to the following error:<\/p>\n<pre><code>ERROR: failed to reproduce 'dvc.yaml': dependency 'params.yaml' does not exist\n<\/code><\/pre>\n<p>Creating a dummy params.yaml without content gives:<\/p>\n<pre><code>WARNING: 'params.yaml' is empty.\nERROR: failed to reproduce 'dvc.yaml': Parameters 'important_parameter' are missing from 'params.yaml'.\n<\/code><\/pre>\n<p>What am I missing? Is this possible at all with the templating feature?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1619103654467,
        "Question_score":2,
        "Question_tags":"python|dvc",
        "Question_view_count":588,
        "Owner_creation_time":1542537900087,
        "Owner_last_access_time":1663940500713,
        "Owner_location":null,
        "Owner_reputation":147,
        "Owner_up_votes":69,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I think you don't need the templating feature in this case. As shown in this <a href=\"https:\/\/dvc.org\/doc\/command-reference\/params#examples-python-parameters-file\" rel=\"nofollow noreferrer\">example<\/a>:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>stages:\n  train:\n    cmd: python train.py\n    deps:\n      - users.csv\n    params:\n      - params.py:\n          - BOOL\n          - INT\n          - TrainConfig.EPOCHS\n          - TrainConfig.layers\n    outs:\n      - model.pkl\n<\/code><\/pre>\n<p>The way to redefine the default <code>params.yaml<\/code> is to specify the file name explicitly in the <code>params:<\/code> section:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>params:\n  - preproc.yaml:\n    - important_parameter\n<\/code><\/pre>\n<p>Also, when you create a stage either with <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run\" rel=\"nofollow noreferrer\"><code>dvc run<\/code><\/a> (not recommended) or <a href=\"https:\/\/dvc.org\/doc\/command-reference\/stage\/add\" rel=\"nofollow noreferrer\"><code>dvc stage add<\/code><\/a>, you can provide the params file name explicitly as a prefix:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc run -n train -d train.py -d logs\/ -o users.csv -f \\\n          -p parse_params.yaml:threshold,classes_num \\\n          python train.py\n<\/code><\/pre>\n<p>Here ^^ <code>parse_params.yaml<\/code> is a custom params file.<\/p>\n<p>Please, let me know if it solves the problem and if you have any other questions :)<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1619127242127,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67215839",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":72651603,
        "Question_title":"Adding files that rely on pipeline outputs",
        "Question_body":"<p>In my workflow, I do the following:<\/p>\n<ol>\n<li>Acquire raw data (e.g. a video containing people)<\/li>\n<li>Transform it (e.g. automatically extract all crops with faces)<\/li>\n<li>Manually label them (e.g. identify the person in each crop). The labels are stored in json files along with the crops.<\/li>\n<li>Train a model on these data.<\/li>\n<\/ol>\n<p><strong>How should I track this pipeline with DVC?<\/strong><\/p>\n<p>My concerns:<\/p>\n<ol>\n<li>If stage 2 is changed (e.g. crops are extracted with a different size), the manual data should be invalidated (and so should the final model).<\/li>\n<li>The 3rd step is manual and therefore not precisely reproducible. But I do need its input to be reproducible.<\/li>\n<li>Stage 4 has an element of randomness, so it's not precisely reproducible either.<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1655411665790,
        "Question_score":1,
        "Question_tags":"dvc",
        "Question_view_count":39,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_location":"Tel Aviv",
        "Owner_reputation":3784,
        "Owner_up_votes":472,
        "Owner_down_votes":0,
        "Owner_views":342,
        "Question_last_edit_time":1655415427550,
        "Answer_body":"<p>Stage 3 is manual so you can't really codify it or automate it, nor guarantee its reproducibility (due to possible human error). But there's a way to get you as close as possible:<\/p>\n<p>You could replace it with a helper script that just checks whether all the labels are annotated. If so, output a text file with content &quot;green&quot;, otherwise &quot;red&quot; (for example) and error out.<\/p>\n<p>Stage 4 should depend on both the inputs from stages 2 and 3, so it will only run if BOTH the face crops changed AND if they are thoroughly annotated.\nInternally, it first checks the semaphore file (from 3) and dies on red. On green, it trains the model :)<\/p>\n<p>The <a href=\"https:\/\/dvc.org\/doc\/command-reference\/dag#directed-acyclic-graph\" rel=\"nofollow noreferrer\">DAG<\/a> looks like this:<\/p>\n<pre><code>          +-----------+       \n          | 1-acquire |       \n          +-----------+       \n                *          \n                *          \n                *          \n          +---------+       \n          | 2-xform |       \n          +---------+       \n you      **        **     \n   --&gt;  **            **   \n       *                ** \n+---------+               *\n| 3-check |             ** \n+---------+           **   \n          **        **     \n            **    **       \n              *  *         \n          +---------+      \n          | 4-train |      \n          +---------+      \n<\/code><\/pre>\n<blockquote>\n<p>re randomness: while not ideal, non-determinism technically only <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#avoiding-unexpected-behavior\" rel=\"nofollow noreferrer\">affects intermediate stages<\/a> of the pipeline, because it causes everything after that to always run. In this case, since it's in the last stage, it won't affect DVC's job.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0,
        "Answer_creation_time":1655523571800,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1655706111940,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72651603",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":56456463,
        "Question_title":"Unable to ignore .DS_Store files in DVC",
        "Question_body":"<p>I use DVC to track my media files. I use MacOS and I want\".DS_Store\" files to be ignored by DVC. According to DVC documentation I can achieve it with  <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvcignore\" rel=\"nofollow noreferrer\">.dvcignore<\/a>. I created <code>.dvcignore<\/code> file with \".DS_Store\" rule. However every time \".DS_Store\" is created <code>dvc status<\/code> still says that content has changed<\/p>\n\n<p>Here is the little test to reproduce my issue:<\/p>\n\n<pre><code>$ git init\n$ dvc init\n\n# create directory to store data\n# and track it's content with DVC\n$ mkdir data\n$ dvc add data\n\n# Ignore .DS_Store files created by MacOS\n$ echo \".DS_Store\" &gt; .dvcignore\n\n# create .DS_Store in data dir\n$ touch \"data\/.DS_Store\"\n<\/code><\/pre>\n\n<p>If I understand DVC documentation correctly then <code>dvc status<\/code> should print something like \"Pipeline is up to date. Nothing to reproduce\". However <code>dvc status<\/code> gives me:<\/p>\n\n<pre><code>data.dvc:\n        changed outs:\n                modified:           data\n<\/code><\/pre>\n\n<p>How I can really ignore \".DS_Store\" files?<\/p>\n\n<p><strong>UPDATE:<\/strong> The .dvcignore support noticeably improved in latest versions and the problem is no more relevant.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1559722020690,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":326,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Question_last_edit_time":1568948355943,
        "Answer_body":"<p>The current implementation of <code>.dvcignore<\/code> is very limited. Read more on it <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvcignore\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Please, mention that you are interested in this feature here - <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1876\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/issues\/1876<\/a>. That would help our team to prioritize issues properly.<\/p>\n\n<p>The possible workaround for now would be to use one of these approaches - <a href=\"https:\/\/stackoverflow.com\/questions\/18015978\/how-to-stop-creating-ds-store-on-mac\">How to stop creating .DS_Store on Mac?<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1559758177417,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56456463",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":71378280,
        "Question_title":"Error with DVC on Google Colab - dvc.scm.CloneError: Failed to clone repo",
        "Question_body":"<p>I'm having a problem trying to run &quot;dvc pull&quot; on Google Colab. I have two repositories (let's call them A and B) where repository A is for my machine learning codes and repository B is for my dataset.<\/p>\n<p>I've successfully pushed my dataset to repository B with DVC (using gdrive as my remote storage) and I also managed to successfully run &quot;dvc import&quot; (as well as &quot;dvc pull\/update&quot;) on my local project of repository A.<\/p>\n<p>The problem comes when I use colab to run my project. So what I did was the following:<\/p>\n<ol>\n<li>Created a new notebook on colab<\/li>\n<li>Successfully git-cloned my machine learning project (repository A)<\/li>\n<li>Ran &quot;!pip install dvc&quot;<\/li>\n<li>Ran &quot;!dvc pull -v&quot; (This is what causes the error)<\/li>\n<\/ol>\n<p>On step 4, I got the error (this is the full stack trace. Note that I changed the repo URL in the stack trace for confidentiality reasons)<\/p>\n<pre><code>2022-03-08 08:53:31,863 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/config.local' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/tmp' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/cache' to gitignore file.\n2022-03-08 08:53:31,916 DEBUG: Creating external repo https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git@3a3f4559efabff8ec74486da39b86688d1b98d75\n2022-03-08 08:53:31,916 DEBUG: erepo: git clone 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to a temporary dir\nEverything is up to date.\n2022-03-08 08:53:32,154 ERROR: failed to pull data from the cloud - Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 185, in clone\n    tmp_repo = clone_from()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1148, in clone_from\n    return cls._clone(git, url, to_path, GitCmdObjectDB, progress, multi_options, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1079, in _clone\n    finalize_process, decode_streams=False)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 176, in handle_process_output\n    return finalizer(process)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/util.py&quot;, line 386, in finalize_process\n    proc.wait(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 502, in wait\n    raise GitCommandError(remove_password_if_present(self.args), status, errstr)\ngit.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v --no-single-branch --progress https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git \/tmp\/tmp2x7y7xgedvc-clone\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 104, in clone\n    return Git.clone(url, to_path, progress=pbar.update_git, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/__init__.py&quot;, line 121, in clone\n    backend.clone(url, to_path, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 190, in clone\n    raise CloneError(url, to_path) from exc\nscmrepo.exceptions.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/command\/data_sync.py&quot;, line 41, in run\n    glob=self.args.glob,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/pull.py&quot;, line 38, in pull\n    run_cache=run_cache,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/fetch.py&quot;, line 50, in fetch\n    revs=revs,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 437, in used_objs\n    with_deps=with_deps,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/index.py&quot;, line 190, in used_objs\n    filter_info=filter_info,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/stage\/__init__.py&quot;, line 660, in get_used_objs\n    for odb, objs in out.get_used_objs(*args, **kwargs).items():\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 918, in get_used_objs\n    return self.get_used_external(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 973, in get_used_external\n    return dep.get_used_objs(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 94, in get_used_objs\n    used, _ = self._get_used_and_obj(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 108, in _get_used_and_obj\n    locked=locked, cache_dir=local_odb.cache_dir\n  File &quot;\/usr\/lib\/python3.7\/contextlib.py&quot;, line 112, in __enter__\n    return next(self.gen)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 35, in external_repo\n    path = _cached_clone(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 155, in _cached_clone\n    clone_path, shallow = _clone_default_branch(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 45, in wrapper\n    return deco(call, *dargs, **dkwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/flow.py&quot;, line 274, in wrap_with\n    return call()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 66, in __call__\n    return self._func(*self._args, **self._kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 220, in _clone_default_branch\n    git = clone(url, clone_path)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 106, in clone\n    raise CloneError(str(exc))\ndvc.scm.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\n2022-03-08 08:53:32,161 DEBUG: Analytics is enabled.\n2022-03-08 08:53:32,192 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n2022-03-08 08:53:32,193 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n<\/code><\/pre>\n<p>And btw this is how I cloned my git repository (repo A)<\/p>\n<pre><code>!git config - global user.name &quot;Zharfan&quot;\n!git config - global user.email &quot;zharfan@myemail.com&quot;\n!git clone https:\/\/&lt;MyTokenName&gt;:&lt;MyToken&gt;@link-to-my-repo-A.git\n<\/code><\/pre>\n<p>Does anyone know why? Any help would be greatly appreciated. Thank you in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":12,
        "Question_creation_time":1646641948613,
        "Question_score":1,
        "Question_tags":"git|dataset|google-colaboratory|dvc",
        "Question_view_count":707,
        "Owner_creation_time":1525227015313,
        "Owner_last_access_time":1663842020310,
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":1652856778060,
        "Answer_body":"<p>To summarize the discussion in the comments thread.<\/p>\n<p>Most likely it's happening since DVC can't get access to a private repo on GitLab. (The error message is obscure and should be fixed.)<\/p>\n<p>The same way you would not be able to run:<\/p>\n<pre><code>!git clone https:\/\/gitlab.com\/org\/&lt;private-repo&gt;\n<\/code><\/pre>\n<p>It also returns a pretty obscure error:<\/p>\n<pre><code>Cloning into '&lt;private-repo&gt;'...\nfatal: could not read Username for 'https:\/\/gitlab.com': No such device or address\n<\/code><\/pre>\n<p>(I think it's something related to how tty is setup in Colab?)<\/p>\n<p>The best approach to solve this is to use SSH like described <a href=\"https:\/\/medium.com\/@sadiaafrinpurba\/how-to-clone-private-github-repo-in-google-colab-using-ssh-77384cfef18f\" rel=\"nofollow noreferrer\">here<\/a> for example.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1647022114533,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71378280",
        "Question_exclusive_tag":"DVC"
    },
    {
        "Question_id":65939058,
        "Question_title":"MLflow stores artifacts on GCP buckets but is not able to read them",
        "Question_body":"<p>I've found an almost identical question <a href=\"https:\/\/stackoverflow.com\/questions\/63727235\/mlflow-artifacts-storing-artifactsgoogle-cloud-storage-but-not-displaying-them?newreg=923da08a362547daab64c7d7e2275423\">here<\/a> but don't have enough reputation to add comments so will ask again hoping that someone has found a solution in the mean time.<\/p>\n<p>I am using MLflow (1.13.1) to track model performance and GCP Storage to store model artifacts.\nMLflow is running on a GCP VM instance and my python application uses a service account with Storage Object Creator and Storage Object Viewer roles (and then I've also added storage.buckets.get permissions) to store artifacts in GCP buckets and read from them.\nEverything is working as expected with parameters and metrics correctly displaying in MLflow UI and model artifacts correctly stored in buckets. The problem is that the model artifacts do not show up in MLflow UI because of this error:<\/p>\n<pre><code>Unable to list artifacts stored under gs:\/******\/artifacts for the current run. \nPlease contact your tracking server administrator to notify them of this error, \nwhich can happen when the tracking server lacks permission to list artifacts under the current run's root artifact directory.\n<\/code><\/pre>\n<p>The quoted artifacts location exists and contains the correct model artifacts, and MLflow should be able to read the artifacts because of the Storage Object Viewer role and the storage.buckets.get permissions.<\/p>\n<p>Any suggestion on what could be wrong? Thank you.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611843895217,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|mlflow",
        "Question_view_count":428,
        "Owner_creation_time":1611841996690,
        "Owner_last_access_time":1637883583270,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I've found the problem just after posting the question.\nI had forgotten to install the <code>google-cloud-storage<\/code> library on the GCP VM. Everything works as expected now.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1611845294603,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65939058",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61980244,
        "Question_title":"How to fix Artifacts not showing in MLflow UI",
        "Question_body":"<p>I'd used MLflow and logged parameters using the function below (from pydataberlin).<\/p>\n<pre><code>def train(alpha=0.5, l1_ratio=0.5):\n    # train a model with given parameters\n    warnings.filterwarnings(&quot;ignore&quot;)\n    np.random.seed(40)\n\n    # Read the wine-quality csv file (make sure you're running this from the root of MLflow!)\n    data_path = &quot;data\/wine-quality.csv&quot;\n    train_x, train_y, test_x, test_y = load_data(data_path)\n\n    # Useful for multiple runs (only doing one run in this sample notebook)    \n    with mlflow.start_run():\n        # Execute ElasticNet\n        lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n        lr.fit(train_x, train_y)\n\n        # Evaluate Metrics\n        predicted_qualities = lr.predict(test_x)\n        (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n        # Print out metrics\n        print(&quot;Elasticnet model (alpha=%f, l1_ratio=%f):&quot; % (alpha, l1_ratio))\n        print(&quot;  RMSE: %s&quot; % rmse)\n        print(&quot;  MAE: %s&quot; % mae)\n        print(&quot;  R2: %s&quot; % r2)\n\n        # Log parameter, metrics, and model to MLflow\n        mlflow.log_param(key=&quot;alpha&quot;, value=alpha)\n        mlflow.log_param(key=&quot;l1_ratio&quot;, value=l1_ratio)\n        mlflow.log_metric(key=&quot;rmse&quot;, value=rmse)\n        mlflow.log_metrics({&quot;mae&quot;: mae, &quot;r2&quot;: r2})\n        mlflow.log_artifact(data_path)\n        print(&quot;Save to: {}&quot;.format(mlflow.get_artifact_uri()))\n        \n        mlflow.sklearn.log_model(lr, &quot;model&quot;)\n<\/code><\/pre>\n<p>Once I run <code>train()<\/code> with its parameters, in UI I cannot see Artifacts, but I can see models and its parameters and Metric.<\/p>\n<p>In artifact tab it's written <code>No Artifacts Recorded Use the log artifact APIs to store file outputs from MLflow runs.<\/code> But in finder in models folders all Artifacts existe with models Pickle.<\/p>\n<p>help<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1590280271933,
        "Question_score":2,
        "Question_tags":"python|artifacts|mlflow",
        "Question_view_count":8044,
        "Owner_creation_time":1500490643013,
        "Owner_last_access_time":1663284031840,
        "Owner_location":"France",
        "Owner_reputation":722,
        "Owner_up_votes":703,
        "Owner_down_votes":8,
        "Owner_views":290,
        "Question_last_edit_time":1656334439607,
        "Answer_body":"<p>Had a similar issue. In my case, I solved it by running <code>mlflow ui<\/code> inside the <code>mlruns<\/code> directory of your experiment.<\/p>\n<p>See the full discussion on Github <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/3030\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<p>Hope it helps!<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1593697638320,
        "Answer_score":4,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":1593982070672,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61980244",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":64164367,
        "Question_title":"Nginx authentication issues when building mlflow through docker-compose",
        "Question_body":"<p>I'm trying to dockerize <a href=\"https:\/\/mlflow.org\/\" rel=\"nofollow noreferrer\">mlflow<\/a> with PostgreSQL and nginx configurations for Google Cloud Run (GCR) on the Google Cloud Platform (GCP).<\/p>\n<p>Before deploying anything to GCP however, I wanted to get a local deployment working. I found <a href=\"https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039\" rel=\"nofollow noreferrer\">this<\/a> guide that details the process of setting up the environment. Having followed the guide (excluding the SQL part), I can see the  mlflow UI on <code>localhost:80<\/code> as nginx redirects traffic on port 80 to 5000. To add authentication, I found <a href=\"https:\/\/www.digitalocean.com\/community\/tutorials\/how-to-set-up-password-authentication-with-nginx-on-ubuntu-14-04\" rel=\"nofollow noreferrer\">here<\/a> that I can do it using <code>sudo htpasswd -c .htpasswd &lt;username&gt;<\/code> in the <code>etc\/nginx\/<\/code> directory and then adding<\/p>\n<pre><code>location \\ {\n   auth_basic &quot;Private Property&quot;;\n   auth_basic_user_file .htpasswd;\n}\n<\/code><\/pre>\n<p>to the <code>nginx.conf<\/code> (or <code>mlflow.conf<\/code> in this case) to make it appear online. Trouble is, when I go to <code>localhost:80<\/code> <em>now<\/em> and enter in my username\/password, I continue to see<\/p>\n<pre><code>[error] 6#6: *1 open() &quot;\/etc\/nginx\/.htpasswd&quot; failed (2: No such file or directory)\n<\/code><\/pre>\n<p>in the <code>docker-compose up<\/code> logs as they are printed to the terminal, and as such <em>I'm not able to see the mlflow UI<\/em> on <code>localhost:80<\/code> (either a blank screen or nginx 403 error).<\/p>\n<p>Now, I've looked at several other posts (such as <a href=\"https:\/\/stackoverflow.com\/questions\/2010677\/nginx-and-auth-basic\">this one<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/16510374\/403-forbidden-nginx-using-correct-credentials\">this one<\/a>) and it seems to me that nginx doesn't have the right permissions to read the <code>.htpasswd<\/code> in the <code>etc\/nginx\/<\/code> directory file or that the path of the file isn't correct, i.e. the path has to be in reference to the <code>nginx.conf<\/code> file.<\/p>\n<p>Even though I made these corrections to the above towards-data-science files, the problem still persists.  I've been stuck for a while on this. Any particular reasons why this may be happening?<\/p>\n<p>Edit:\nHere is my directory structure in case it may help:<\/p>\n<pre><code>mlflow-docker\/:\n  mlflow\/:\n    Dockerfile\n  nginx\/:\n    Dockerfile\n    mlflow.conf\n    nginx.conf\n  docker-compose.yml\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1601594496277,
        "Question_score":0,
        "Question_tags":"docker|debugging|nginx|google-cloud-platform|mlflow",
        "Question_view_count":841,
        "Owner_creation_time":1524443788580,
        "Owner_last_access_time":1663973899597,
        "Owner_location":null,
        "Owner_reputation":45,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":1601616621329,
        "Answer_body":"<p>You need to add the .htpasswd file inside your container's file system.<\/p>\n<p>Generate the password file in your project's nginx folder.<\/p>\n<pre><code>sudo htpasswd -c .htpasswd sammy\n<\/code><\/pre>\n<p>Copy the password file to the nginx container's directory. Add following line in nginx dockerfile.<\/p>\n<pre><code>COPY .htpasswd \/etc\/nginx\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1601618748113,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1601619366396,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64164367",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68527422,
        "Question_title":"mlflow.pyfunc.spark_udf and vector struct type",
        "Question_body":"<p>My <em>PySpark<\/em> dataset contains categorical data.<\/p>\n<p>To train a model on this data, I followed this <a href=\"https:\/\/docs.databricks.com\/_static\/notebooks\/binary-classification.html\" rel=\"nofollow noreferrer\">example notebook<\/a>. Especially, see the <em>Preprocess Data<\/em> section for the encoding part.<\/p>\n<p>I now need to use this model somewhere else; hence, I followed <em>Databricks<\/em> recommendation to save and load this model.<\/p>\n<p>It's working fine with <em>Pandas<\/em> (cf. code below).<\/p>\n<pre><code>logged_model = 'runs:\/e905f5759d434a1391bbe1e54a2b\/best-model'\n\n# Load model as a PyFuncModel.\nloaded_model = mlflow.pyfunc.load_model(logged_model)\n\n# Predict on a Pandas DataFrame.\nimport pandas as pd\nloaded_model.predict(pd.DataFrame(data))\n<\/code><\/pre>\n<p>However the dataframe is to big to be converted to <em>Pandas<\/em>. Hence I need to make it work in <em>Spark<\/em>:<\/p>\n<pre><code>import mlflow\nlogged_model = 'runs:\/e905f5759d434a131bbe1e54a2b\/best-model'\n\n# Load model as a Spark UDF.\nloaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model)\n\n# Predict on a Spark DataFrame.\ndf.withColumn('predictions', loaded_model(*columns)).collect()\n<\/code><\/pre>\n<p>But this snippet is producing:<\/p>\n<pre><code>java.lang.UnsupportedOperationException: Unsupported data type: struct&amp;lt;type:tinyint,size:int,indices:array&amp;lt;int&amp;gt;,values:array&amp;lt;double&amp;gt;&amp;gt;\n<\/code><\/pre>\n<p>My feeling is that the udf doesn't accept this type of data as input.\nIs there a way to fix it ?\nAnother solution ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1627291388767,
        "Question_score":1,
        "Question_tags":"pyspark|databricks|mlflow",
        "Question_view_count":790,
        "Owner_creation_time":1480398962230,
        "Owner_last_access_time":1664019145870,
        "Owner_location":null,
        "Owner_reputation":440,
        "Owner_up_votes":21,
        "Owner_down_votes":4,
        "Owner_views":56,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Have you tried using the <code>mlflow.spark.load_model<\/code>?<\/p>\n<p>I'm having a very similar issue over here, but but using the spark method. I tried using the <code>mlflow.spark.load_model('runs:\/run-id\/my-model')<\/code> method and I got this weird error:<\/p>\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '\/dbfs\/tmp\/mlflow\/weird-id-folder'\n<\/code><\/pre>\n<p>Searching for the docs, I see the problem that we are facing (which seems to be different), seems to be a signature problem.<\/p>\n<p>According with other part of the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#model-signature\" rel=\"nofollow noreferrer\">docs<\/a> we have that the signature logged with the model will help to define what type of input the model has. The problem for me here is that my input is a Spark Sparse Vector -- which is not supported... Right now I'm trying to convert that into a column-based signature.<\/p>\n<p>Have you tried something like this?<\/p>\n<hr \/>\n<p>UPDATE:<\/p>\n<p>I would like to add that in my case adding the signature did solve the problem. All I did was ignore the vectors and consider only the input data and output data.<\/p>\n<p>I took a look into the notebook, but haven't seen any mlflow logs, anyway, I do suppose you are logging your experiment according to <a href=\"https:\/\/docs.databricks.com\/applications\/mlflow\/tracking.html#log-runs-to-a-notebook-or-workspace-experiment\" rel=\"nofollow noreferrer\">this<\/a> and using the <code>mlflow.spark<\/code> flavor.<\/p>\n<p>If so, consider using all your data transformation and model fit in the same pipeline, using <code>from pyspark.ml import Pipeline<\/code>. Before logging the model, consider going under signature and registering the model schema.<\/p>\n<pre><code>import mlflow.spark\nfrom mlflow.models.signature import infer_signature\n\nwith mlflow.start_run():\n    [...]\n    # executing train &amp; test pipelines:\n    model = pipeline.fit(train_features) # training model\n    predictions = model.transform(test_features) # testing model\n    train_signature = train_features.select('input_data') # ignores all other features created on the pipeline\n    prediction_signature = predictions.select('input_data', 'prediction') # ignores all other features created on the training pipeline \n    signature = infer_signature(train_signature, prediction_signature) # register model schema\n    mlflow.spark.log_model(model, 'transactions-classification', signature=signature) # logging model to mlflow\n    [...]\n<\/code><\/pre>\n<p>After logging the model to the experiment, in a different notebook, you can use the load_model function as:<\/p>\n<pre><code># importing model\nimport mlflow.spark\nmodel_path = 'runs:\/run-id'\nmodel = mlflow.spark.load_model(model_path)\n<\/code><\/pre>\n<p>And it will work! :D<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1629378460770,
        "Answer_score":3,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":1630447430980,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68527422",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56351452,
        "Question_title":"Connect on-prem jypyter notebook to mlflow tracking server in Azure",
        "Question_body":"<p>Is it possible to connect a notebook running in premises to an mlflow Tracking server that is part of an Azure Databricks workspace? Have all the local logging and tracking saved in Azure?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1559088053237,
        "Question_score":2,
        "Question_tags":"azure-databricks|mlflow",
        "Question_view_count":291,
        "Owner_creation_time":1343948494333,
        "Owner_last_access_time":1662160890913,
        "Owner_location":"Seattle, WA, USA",
        "Owner_reputation":1390,
        "Owner_up_votes":122,
        "Owner_down_votes":2,
        "Owner_views":121,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I had a similar problem, used python and solved it with the following steps:<\/p>\n<ol>\n<li>Install mlflow and datbricks-cli libraries.<\/li>\n<li>Define the following env variables : DATABRICKS_HOST (databricks workspace url: <a href=\"https:\/\/region.azuredatabricks.net\" rel=\"nofollow noreferrer\">https:\/\/region.azuredatabricks.net<\/a>) and DATABRICKS_TOKEN<\/li>\n<li>Define mlflow client:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow_client = mlflow.tracking.MlflowClient(tracking_uri='databricks')\n<\/code><\/pre>\n<ol start=\"5\">\n<li>Use mlflow_client client for logging, saving and etc..<\/li>\n<\/ol>\n<p>for more reference you can look at the &quot;Log to a tracking server from a notebook&quot; section <a href=\"https:\/\/docs.azuredatabricks.net\/applications\/mlflow\/tracking.html#log-to-a-tracking-server-from-a-notebook\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1559118227527,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1592644375060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56351452",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72604450,
        "Question_title":"MLflow load model fails Python",
        "Question_body":"<p>I am trying to build an API using an MLflow model.<\/p>\n<p>the funny thing is it works from one location on my PC and not from another. So, the reason for doing I wanted to change my repo etc.<\/p>\n<p>So, the simple code of<\/p>\n<pre><code>from mlflow.pyfunc import load_model\nMODEL_ARTIFACT_PATH = &quot;.\/model\/model_name\/&quot;\nMODEL = load_model(MODEL_ARTIFACT_PATH)\n<\/code><\/pre>\n<p>now fails with<\/p>\n<pre><code>ERROR:    Traceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 540, in lifespan\n    async for item in self.lifespan_context(app):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 481, in default_lifespan\n    await self.startup()\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/starlette\/routing.py&quot;, line 516, in startup\n    await handler()\n  File &quot;\/code\/.\/app\/main.py&quot;, line 32, in startup_load_model\n    MODEL = load_model(MODEL_ARTIFACT_PATH)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 733, in load_model\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/spark.py&quot;, line 737, in _load_pyfunc\n    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/mlflow\/spark.py&quot;, line 656, in _load_model\n    return PipelineModel.load(model_uri)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/util.py&quot;, line 332, in load\n    return cls.read().load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/pipeline.py&quot;, line 258, in load\n    return JavaMLReader(self.cls).load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/ml\/util.py&quot;, line 282, in load\n    java_obj = self._jread.load(path)\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/py4j\/java_gateway.py&quot;, line 1321, in __call__\n    return_value = get_return_value(\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/pyspark\/sql\/utils.py&quot;, line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.\n<\/code><\/pre>\n<p>The model artifacts are already downloaded to the folder \/model folder which has the following structure.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oqxRW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oqxRW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>the load model call is in the main.py file\nAs I mentioned it works from another directory, but there is no reference to any absolute paths. Also, I have made sure that my package references are identical. e,g I have pinned them all down<\/p>\n<pre><code># Model\nmlflow==1.25.1\nprotobuf==3.20.1\npyspark==3.2.1\nscipy==1.6.2\nsix==1.15.0\n<\/code><\/pre>\n<p>also, the same docker file is used both places, which among other things, makes sure that the final resulting folder structure is the same<\/p>\n<pre><code>......other stuffs\n\nCOPY .\/app \/code\/app\nCOPY .\/model \/code\/model\n<\/code><\/pre>\n<p>what can explain it throwing this exception whereas in another location (on my PC), it works (same model artifacts) ?<\/p>\n<p>Since it uses load_model function, it should be able to read the parquet files ?<\/p>\n<p>Any question and I can explain.<\/p>\n<p>EDIT1: I have debugged this a little more in the docker container and it seems the parquet files in the itemFactors folder (listed in my screenshot above) are not getting copied over to my image , even though I have the copy command to copy all files under the model folder. It is copying the _started , _committed and _SUCCESS files, just not the parquet files. Anyone knows why would that be? I DO NOT have a .dockerignore file. Why are those files ignored while copying?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655130099670,
        "Question_score":2,
        "Question_tags":"python|docker|databricks|mlflow",
        "Question_view_count":109,
        "Owner_creation_time":1428654714763,
        "Owner_last_access_time":1664012257383,
        "Owner_location":null,
        "Owner_reputation":596,
        "Owner_up_votes":53,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Question_last_edit_time":1655134670387,
        "Answer_body":"<p>I found the problem. Like I wrote in the EDIT1 of my post, with further observations, the parquet files were missing in the docker container. That was strange because I was copying the entire folder in my Dockerfile.<\/p>\n<p>I then realized that I was hitting this problem <a href=\"https:\/\/github.com\/moby\/buildkit\/issues\/1366\" rel=\"nofollow noreferrer\">mentioned here<\/a>. File paths exceeding 260 characters, silently fail and do not get copied over to the docker container. This was really frustrating because nothing failed during build and then during run, it gave me that cryptic error of &quot;unable to infer schema for parquet&quot;, essentially because the parquet files were not copied over during docker build.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1655191745993,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1655202841060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72604450",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69466354,
        "Question_title":"MLflow S3UploadFailedError: Failed to upload",
        "Question_body":"<p>I've created with docker a MinioS3 artifact storage and a mysql bakend storage using the next docker-compose:<\/p>\n<pre><code>    version: '3.8'\n    services:\n        db:\n           environment:\n              - MYSQL_DATABASE=${MYSQL_DATABASE}\n              - MYSQL_USER=${MYSQL_USER}\n              - MYSQL_PASSWORD=${MYSQL_PASSWORD}\n              - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}\n           expose:\n              - '3306'        \n           volumes:\n              - '(path)\/server_backend:\/var\/lib\/mysql '\n           image: 'mysql'\n           container_name: db\n\n        storage:\n            environment:\n                - MINIO_ACCESS_KEY=${MINIO_USR}\n                - MINIO_SECRET_KEY=${MINIO_PASS}\n            expose:\n                - '9000'\n            ports:\n                - '9000:9000'        \n            depends_on:\n                - db\n            command: server \/data\n            volumes:\n                - '(path)\/server_artifact:\/data'\n            image: minio\/minio:RELEASE.2021-02-14T04-01-33Z\n            container_name: MinIO\n\n        mlflow:\n            build: .\/mlflow\n            environment:\n                - AWS_ACCESS_KEY_ID=${MINIO_USR}\n                - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}       \n            expose:\n                - '5000'\n            ports:\n                - '5000:5000'\n            depends_on:\n                - storage                       \n            image: 'mlflow:Dockerfile'\n            container_name: server\n<\/code><\/pre>\n<p>The Mlflow server docker was created using the next Dockerfile:<\/p>\n<pre><code>    FROM python:3.8-slim-buster\n    WORKDIR \/usr\/src\/app\n    RUN pip install cryptography mlflow psycopg2-binary boto3 pymysql\n    ENV MLFLOW_S3_ENDPOINT_URL=http:\/\/storage:9000\n    CMD mlflow server \\\n        --backend-store-uri mysql+pymysql:\/\/MLFLOW:temporal@db:3306\/DBMLFLOW \\\n        --default-artifact-root s3:\/\/artifacts \\\n        --host 0.0.0.0\n<\/code><\/pre>\n<p>The credantials are defined in a <code>.env<\/code> file.<\/p>\n<p>The results of the <code>docker-compose<\/code> up command :<\/p>\n<pre><code>\n    [+] Running 21\/22\n     - mlflow Error                                                                                                                              5.6s\n     - storage Pulled                                                                                                                           36.9s\n       - a6b97b4963f5 Pull complete                                                                                                             24.6s\n       - 13948a011eec Pull complete                                                                                                             24.7s\n       - 40cdef9976a6 Pull complete                                                                                                             24.7s\n       - f47162848743 Pull complete                                                                                                             24.8s\n       - 5f2758d8e94c Pull complete                                                                                                             24.9s\n       - c2950439edb8 Pull complete                                                                                                             25.0s\n       - 1b08f8a15998 Pull complete                                                                                                             30.7s\n     - db Pulled                                                                                                                                45.8s\n       - 07aded7c29c6 Already exists                                                                                                             0.0s\n       - f68b8cbd22de Pull complete                                                                                                              0.7s\n       - 30c1754a28c4 Pull complete                                                                                                              2.1s\n       - 1b7cb4d6fe05 Pull complete                                                                                                              2.2s\n       - 79a41dc56b9a Pull complete                                                                                                              2.3s\n       - 00a75e3842fb Pull complete                                                                                                              6.7s\n       - b36a6919c217 Pull complete                                                                                                              6.8s\n       - 635b0b84d686 Pull complete                                                                                                              6.8s\n       - 6d24c7242d02 Pull complete                                                                                                             39.4s\n       - 5be6c5edf16f Pull complete                                                                                                             39.5s\n       - cb35eac1242c Pull complete                                                                                                             39.5s\n       - a573d4e1c407 Pull complete                                                                                                             39.6s\n    [+] Building 1.4s (7\/7) FINISHED\n     =&gt; [internal] load build definition from Dockerfile                                                                                         0.0s\n     =&gt; =&gt; transferring dockerfile: 32B                                                                                                          0.0s\n     =&gt; [internal] load .dockerignore                                                                                                            0.0s\n     =&gt; =&gt; transferring context: 2B                                                                                                              0.0s\n     =&gt; [internal] load metadata for docker.io\/library\/python:3.8-slim-buster                                                                    1.3s\n     =&gt; [1\/3] FROM docker.io\/library\/python:3.8-slim-buster@sha256:13a3f2bffb4b18ff7eda2763a3b0ba316dd82e548f52ea8b4fd11c94b97afa7d              0.0s\n     =&gt; CACHED [2\/3] WORKDIR \/usr\/src\/app                                                                                                        0.0s\n     =&gt; CACHED [3\/3] RUN pip install cryptography mlflow psycopg2-binary boto3 pymysql                                                           0.0s\n     =&gt; exporting to image                                                                                                                       0.0s\n     =&gt; =&gt; exporting layers                                                                                                                      0.0s\n     =&gt; =&gt; writing image sha256:76d4e4462b5c7c1826734e59a54488b56660de0dd5ecc188c308202608a8f20b                                                 0.0s\n     =&gt; =&gt; naming to docker.io\/library\/mlflow:Dockerfile                                                                                         0.0s\n    \n    Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\n    [+] Running 3\/3\n     - Container db  Created                                                                                                       0.5s\n     - Container MinIO      Created                                                                                                       0.1s\n     - Container server     Created                                                                                                       0.1s\n    Attaching to server, MinIO, db\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.26-1debian10 started.\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.26-1debian10 started.\n    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Initializing database files\n    db  | 2021-10-06T12:12:57.679527Z 0 [System] [MY-013169] [Server] \/usr\/sbin\/mysqld (mysqld 8.0.26) initializing of server in progress as process 44\n    db  | 2021-10-06T12:12:57.687748Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.\n    db  | 2021-10-06T12:12:58.230036Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.\n    db  | 2021-10-06T12:12:59.888820Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:12:59.889102Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:12:59.997461Z 6 [Warning] [MY-010453] [Server] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option.\n    MinIO      | Attempting encryption of all config, IAM users and policies on MinIO backend\n    MinIO      | Endpoint: http:\/\/172.18.0.3:9000  http:\/\/127.0.0.1:9000\n    MinIO      |\n    MinIO      | Browser Access:\n    MinIO      |    http:\/\/172.18.0.3:9000  http:\/\/127.0.0.1:9000\n    MinIO      |\n    MinIO      | Object API (Amazon S3 compatible):\n    MinIO      |    Go:         https:\/\/docs.min.io\/docs\/golang-client-quickstart-guide\n    MinIO      |    Java:       https:\/\/docs.min.io\/docs\/java-client-quickstart-guide\n    MinIO      |    Python:     https:\/\/docs.min.io\/docs\/python-client-quickstart-guide\n    MinIO      |    JavaScript: https:\/\/docs.min.io\/docs\/javascript-client-quickstart-guide\n    MinIO      |    .NET:       https:\/\/docs.min.io\/docs\/dotnet-client-quickstart-guide\n    server     | 2021\/10\/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 0.1 seconds\n    server     | 2021\/10\/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 0.3 seconds\n    server     | 2021\/10\/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 0.7 seconds\n    server     | 2021\/10\/06 12:13:03 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 1.5 seconds\n    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Database files initialized\n    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Starting temporary server\n    db  | 2021-10-06T12:13:04.422603Z 0 [System] [MY-010116] [Server] \/usr\/sbin\/mysqld (mysqld 8.0.26) starting as process 93\n    db  | 2021-10-06T12:13:04.439806Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.\n    db  | 2021-10-06T12:13:04.575773Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.\n    db  | 2021-10-06T12:13:04.827307Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:04.827865Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:04.832827Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.\n    db  | 2021-10-06T12:13:04.834132Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.\n    db  | 2021-10-06T12:13:04.841629Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '\/var\/run\/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.\n    db  | 2021-10-06T12:13:04.855748Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Socket: \/var\/run\/mysqld\/mysqlx.sock\n    db  | 2021-10-06T12:13:04.855801Z 0 [System] [MY-010931] [Server] \/usr\/sbin\/mysqld: ready for connections. Version: '8.0.26'  socket: '\/var\/run\/mysqld\/mysqld.sock'  port: 0  MySQL Community Server - GPL.\n    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Temporary server started.\n    server     | 2021\/10\/06 12:13:05 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 3.1 seconds\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/iso3166.tab' as time zone. Skipping it.\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/leap-seconds.list' as time zone. Skipping it.\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/zone.tab' as time zone. Skipping it.\n    db  | Warning: Unable to load '\/usr\/share\/zoneinfo\/zone1970.tab' as time zone. Skipping it.\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Creating database DBMLFLOW\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Creating user MLFLOW\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Giving user MLFLOW access to schema DBMLFLOW\n    db  |\n    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Stopping temporary server\n    db  | 2021-10-06T12:13:06.948482Z 13 [System] [MY-013172] [Server] Received SHUTDOWN from user root. Shutting down mysqld (Version: 8.0.26).\n    server     | 2021\/10\/06 12:13:08 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.\n    server     | (pymysql.err.OperationalError) (2003, &quot;Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)&quot;)\n    server     | (Background on this error at: https:\/\/sqlalche.me\/e\/14\/e3q8)\n    server     | Operation will be retried in 6.3 seconds\n    db  | 2021-10-06T12:13:08.716131Z 0 [System] [MY-010910] [Server] \/usr\/sbin\/mysqld: Shutdown complete (mysqld 8.0.26)  MySQL Community Server - GPL.\n    db  | 2021-10-06 12:13:08+00:00 [Note] [Entrypoint]: Temporary server stopped\n    db  |\n    db  | 2021-10-06 12:13:08+00:00 [Note] [Entrypoint]: MySQL init process done. Ready for start up.\n    db  |\n    db  | 2021-10-06T12:13:09.159115Z 0 [System] [MY-010116] [Server] \/usr\/sbin\/mysqld (mysqld 8.0.26) starting as process 1\n    db  | 2021-10-06T12:13:09.167405Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.\n    db  | 2021-10-06T12:13:09.298925Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.\n    db  | 2021-10-06T12:13:09.488958Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:09.489087Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main\n    db  | 2021-10-06T12:13:09.489934Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.\n    db  | 2021-10-06T12:13:09.490169Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.\n    db  | 2021-10-06T12:13:09.494728Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '\/var\/run\/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.\n    db  | 2021-10-06T12:13:09.509856Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Bind-address: '::' port: 33060, socket: \/var\/run\/mysqld\/mysqlx.sock\n    db  | 2021-10-06T12:13:09.509982Z 0 [System] [MY-010931] [Server] \/usr\/sbin\/mysqld: ready for connections. Version: '8.0.26'  socket: '\/var\/run\/mysqld\/mysqld.sock'  port: 3306  MySQL Community Server - GPL.\n    db  | mbind: Operation not permitted\n    server     | 2021\/10\/06 12:13:14 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n    server     | 2021\/10\/06 12:13:14 INFO mlflow.store.db.utils: Updating database tables\n    server     | INFO  [alembic.runtime.migration] Context impl MySQLImpl.\n    server     | INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n    server     | INFO  [alembic.runtime.migration] Running upgrade  -&gt; 451aebb31d03, add metric step\n    server     | INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -&gt; 90e64c465722, migrate user column to tags\n    server     | INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -&gt; 181f10493468, allow nulls for metric values\n    server     | INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -&gt; df50e92ffc5e, Add Experiment Tags Table\n    server     | INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -&gt; 7ac759974ad8, Update run tags with larger limit\n    server     | INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -&gt; 89d4b8295536, create latest metrics table\n    server     | INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!\n    server     | INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -&gt; 2b4d017a5e9b, add model registry tables to db\n    server     | INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.\n    server     | INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!\n    server     | INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -&gt; cfd24bdc0731, Update run status constraint with killed\n    server     | INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -&gt; 0a8213491aaa, drop_duplicate_killed_constraint\n    server     | INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -&gt; 728d730b5ebd, add registered model tags table\n    server     | INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -&gt; 27a6a02d2cf1, add model version tags table\n    server     | INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -&gt; 84291f40a231, add run_link to model_version\n    server     | INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -&gt; a8c4a736bde6, allow nulls for run_id\n    server     | INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -&gt; 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n    server     | INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -&gt; c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n    server     | INFO  [alembic.runtime.migration] Context impl MySQLImpl.\n    server     | INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n    db  | mbind: Operation not permitted\n    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Starting gunicorn 20.1.0\n    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Listening at: http:\/\/0.0.0.0:5000 (17)\n    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Using worker: sync\n    server     | [2021-10-06 12:13:16 +0000] [19] [INFO] Booting worker with pid: 19\n    server     | [2021-10-06 12:13:16 +0000] [20] [INFO] Booting worker with pid: 20\n    server     | [2021-10-06 12:13:16 +0000] [21] [INFO] Booting worker with pid: 21\n    server     | [2021-10-06 12:13:16 +0000] [22] [INFO] Booting worker with pid: 22\n\n<\/code><\/pre>\n<p>It makes me suspect because on the second line appears <code>- mlflow Error<\/code> but i think that this is why the other builds haven't finished.<\/p>\n<p>Then I've set my environment variables on the client to create the information flow between my script and the storages:<\/p>\n<pre><code>\n    os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http:\/\/localhost:9000\/'\n    os.environ['AWS_ACCESS_KEY_ID'] = 'key'\n    os.environ['AWS_SECRET_ACCESS_KEY'] = 'pw'\n    \n    remote_server_uri = &quot;http:\/\/localhost:5000\/&quot; # server URI\n    mlflow.set_tracking_uri(remote_server_uri)\n    \n    mlflow.set_experiment(&quot;mnist_mLflow_demo&quot;)\n\n<\/code><\/pre>\n<p>finally i trained a tensorflow network and i didn't have problems storing parameters and metrics but gave me some warnings (refering to next error). But the model haven't been auto log, so i tryed to do it manually:<\/p>\n<pre><code>    with mlflow.start_run(run_name = &quot;test0&quot;) as run:\n    \n        mlflow.keras.log_model(model2, 'model2')\n\n    mlflow.end_run()\n<\/code><\/pre>\n<p>It dosen't work and it gives me the next INFO (but essencialy an error):<\/p>\n<pre><code>    INFO:tensorflow:Assets written to: (path)\\Temp\\tmpgr5eaha2\\model\\data\\model\\assets\n    INFO:tensorflow:Assets written to: (path)\\Temp\\tmpgr5eaha2\\model\\data\\model\\assets\n    2021\/10\/06 14:16:00 ERROR mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: (path)\\AppData\\Local\\Temp\\tmpgr5eaha2\\model, flavor: keras)\n    Traceback (most recent call last):\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\environment.py&quot;, line 212, in infer_pip_requirements\n        return _infer_requirements(model_uri, flavor)\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\requirements_utils.py&quot;, line 263, in _infer_requirements\n        modules = _capture_imported_modules(model_uri, flavor)\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\requirements_utils.py&quot;, line 221, in _capture_imported_modules\n        _run_command(\n      File &quot;(path)\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\requirements_utils.py&quot;, line 163, in _run_command\n        stderr = stderr.decode(&quot;utf-8&quot;)\n    UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in position 349: invalid continuation byte\n\n<\/code><\/pre>\n<p>And the next error:<\/p>\n<pre><code>\n    ClientError                               Traceback (most recent call last)\n    ~\\Python\\Python39\\lib\\site-packages\\boto3\\s3\\transfer.py in upload_file(self, filename, bucket, key, callback, extra_args)\n        278         try:\n    --&gt; 279             future.result()\n        280         # If a client error was raised, add the backwards compatibility layer\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\futures.py in result(self)\n        105             # out of this and propogate the exception.\n    --&gt; 106             return self._coordinator.result()\n        107         except KeyboardInterrupt as e:\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\futures.py in result(self)\n        264         if self._exception:\n    --&gt; 265             raise self._exception\n        266         return self._result\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\tasks.py in __call__(self)\n        125             if not self._transfer_coordinator.done():\n    --&gt; 126                 return self._execute_main(kwargs)\n        127         except Exception as e:\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\tasks.py in _execute_main(self, kwargs)\n        149 \n    --&gt; 150         return_value = self._main(**kwargs)\n        151         # If the task is the final task, then set the TransferFuture's\n    \n    ~\\Python\\Python39\\lib\\site-packages\\s3transfer\\upload.py in _main(self, client, fileobj, bucket, key, extra_args)\n        693         with fileobj as body:\n    --&gt; 694             client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)\n        695 \n    \n    ~\\Python\\Python39\\lib\\site-packages\\botocore\\client.py in _api_call(self, *args, **kwargs)\n        385             # The &quot;self&quot; in this scope is referring to the BaseClient.\n    --&gt; 386             return self._make_api_call(operation_name, kwargs)\n        387 \n    \n    ~\\Python\\Python39\\lib\\site-packages\\botocore\\client.py in _make_api_call(self, operation_name, api_params)\n        704             error_class = self.exceptions.from_code(error_code)\n    --&gt; 705             raise error_class(parsed_response, operation_name)\n        706         else:\n    \n    ClientError: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.\n    \n    During handling of the above exception, another exception occurred:\n    \n    S3UploadFailedError                       Traceback (most recent call last)\n    C:\\Users\\FCAIZA~1\\AppData\\Local\\Temp\/ipykernel_7164\/2476247499.py in &lt;module&gt;\n          1 with mlflow.start_run(run_name = &quot;test0&quot;) as run:\n          2 \n    ----&gt; 3     mlflow.keras.log_model(model2, 'model2')\n          4 \n          5 mlflow.end_run()\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\keras.py in log_model(keras_model, artifact_path, conda_env, custom_objects, keras_module, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, **kwargs)\n        402             mlflow.keras.log_model(keras_model, &quot;models&quot;)\n        403     &quot;&quot;&quot;\n    --&gt; 404     Model.log(\n        405         artifact_path=artifact_path,\n        406         flavor=mlflow.keras,\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\models\\model.py in log(cls, artifact_path, flavor, registered_model_name, await_registration_for, **kwargs)\n        186             mlflow_model = cls(artifact_path=artifact_path, run_id=run_id)\n        187             flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n    --&gt; 188             mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)\n        189             try:\n        190                 mlflow.tracking.fluent._record_logged_model(mlflow_model)\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\tracking\\fluent.py in log_artifacts(local_dir, artifact_path)\n        582     &quot;&quot;&quot;\n        583     run_id = _get_or_start_run().info.run_id\n    --&gt; 584     MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\n        585 \n        586 \n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\tracking\\client.py in log_artifacts(self, run_id, local_dir, artifact_path)\n        975             is_dir: True\n        976         &quot;&quot;&quot;\n    --&gt; 977         self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\n        978 \n        979     @contextlib.contextmanager\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py in log_artifacts(self, run_id, local_dir, artifact_path)\n        332         :param artifact_path: If provided, the directory in ``artifact_uri`` to write to.\n        333         &quot;&quot;&quot;\n    --&gt; 334         self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)\n        335 \n        336     def list_artifacts(self, run_id, path=None):\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\store\\artifact\\s3_artifact_repo.py in log_artifacts(self, local_dir, artifact_path)\n        102                 upload_path = posixpath.join(dest_path, rel_path)\n        103             for f in filenames:\n    --&gt; 104                 self._upload_file(\n        105                     s3_client=s3_client,\n        106                     local_file=os.path.join(root, f),\n    \n    ~\\Python\\Python39\\lib\\site-packages\\mlflow\\store\\artifact\\s3_artifact_repo.py in _upload_file(self, s3_client, local_file, bucket, key)\n         78         if environ_extra_args is not None:\n         79             extra_args.update(environ_extra_args)\n    ---&gt; 80         s3_client.upload_file(Filename=local_file, Bucket=bucket, Key=key, ExtraArgs=extra_args)\n         81 \n         82     def log_artifact(self, local_file, artifact_path=None):\n    \n    ~\\Python\\Python39\\lib\\site-packages\\boto3\\s3\\inject.py in upload_file(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\n        128     &quot;&quot;&quot;\n        129     with S3Transfer(self, Config) as transfer:\n    --&gt; 130         return transfer.upload_file(\n        131             filename=Filename, bucket=Bucket, key=Key,\n        132             extra_args=ExtraArgs, callback=Callback)\n    \n    ~\\Python\\Python39\\lib\\site-packages\\boto3\\s3\\transfer.py in upload_file(self, filename, bucket, key, callback, extra_args)\n        283         # client error.\n        284         except ClientError as e:\n    --&gt; 285             raise S3UploadFailedError(\n        286                 &quot;Failed to upload %s to %s: %s&quot; % (\n        287                     filename, '\/'.join([bucket, key]), e))\n    \n    S3UploadFailedError: Failed to upload (path)\\AppData\\Local\\Temp\\tmpgr5eaha2\\model\\conda.yaml to artifacts\/1\/5ae5fcef2d07432d811c3d7eb534382c\/artifacts\/model2\/conda.yaml: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.\n\n<\/code><\/pre>\n<p>Do you know how to help me with it? I have been looking all this morning but i did not find a solution. Thank you!!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633525784987,
        "Question_score":1,
        "Question_tags":"python|mysql|docker|minio|mlflow",
        "Question_view_count":969,
        "Owner_creation_time":1580841805373,
        "Owner_last_access_time":1663792613347,
        "Owner_location":"Seville, Spain",
        "Owner_reputation":33,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I found the solution of this issue. It is a tricky problem due to spanish characters, my system's user profile in &quot;C:\/&quot; is &quot;fca\u00f1izares&quot; (Ca\u00f1izares is my first last name). I have created another user named &quot;fcanizares&quot; and all is working fine. Hope you find this solution helpfull.<\/p>\n<p>PS: Moral of the issue, get rid of the extrange characters!<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1633680248313,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69466354",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70931309,
        "Question_title":"How to retrieve the model signature from the MLflow Model Registry",
        "Question_body":"<p>I have registered a scikit learn model on my MLflow Tracking server, and I am loading it with <code>sklearn.load_model(model_uri)<\/code>.<\/p>\n<p>Now, I would like to access the signature of the model so I can get a list of the model's required inputs\/features so I can retrieve them from my feature store by name. I can't seem to find any utility or method in the <code>mlflow<\/code> API or the <code>MLFlowClient<\/code> API that will let me access a signature or inputs\/outputs attribute, even though I can see a list of inputs and outputs under each version of the model in the UI.<\/p>\n<p>I know that I can find the input sample and the model configuration in the model's artifacts, but that would require me actually downloading the artifacts and loading them manually in my script. I don't need to avoid that, but I am surprised that I can't just return the signature as a dictionary the same way I can return a run's parameters or metrics.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643655088920,
        "Question_score":4,
        "Question_tags":"mlflow",
        "Question_view_count":904,
        "Owner_creation_time":1466188731113,
        "Owner_last_access_time":1663868994303,
        "Owner_location":"Michigan",
        "Owner_reputation":414,
        "Owner_up_votes":25,
        "Owner_down_votes":5,
        "Owner_views":39,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The way to access the model's signature without downloading the MLModel file is under the loaded model. And then you'll access the model's attributes, such as its signature or even other Pyfunc-defined methods.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmodel = mlflow.pyfunc.load_model(&quot;runs:\/&lt;run_id&gt;\/model&quot;)\nprint(model._model_meta._signature)\n<\/code><\/pre>",
        "Answer_comment_count":3,
        "Answer_creation_time":1645469817663,
        "Answer_score":3,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70931309",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63255631,
        "Question_title":"MLflow: INVALID_PARAMETER_VALUE: Unsupported URI '.\/mlruns' for model registry store",
        "Question_body":"<p>I got this error when I was trying to have a model registered in the model registry. Could someone help me?<\/p>\n<pre><code>RestException: INVALID_PARAMETER_VALUE: Unsupported URI '.\/mlruns' for model registry store. \nSupported schemes are: ['postgresql', 'mysql', 'sqlite', 'mssql']. \nSee https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#storage for how to setup a compatible server.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596578095187,
        "Question_score":10,
        "Question_tags":"python|mlflow",
        "Question_view_count":12594,
        "Owner_creation_time":1419619351727,
        "Owner_last_access_time":1644268036150,
        "Owner_location":null,
        "Owner_reputation":125,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":1598815085667,
        "Answer_body":"<p>Mlflow required DB as datastore for Model Registry\nSo you have to run tracking server with DB as backend-store and log model to this tracking server.\nThe easiest way to use DB is to use SQLite.<\/p>\n<pre><code>mlflow server \\\n    --backend-store-uri sqlite:\/\/\/mlflow.db \\\n    --default-artifact-root .\/artifacts \\\n    --host 0.0.0.0\n<\/code><\/pre>\n<p>And set MLFLOW_TRACKING_URI environment variable to <em>http:\/\/localhost:5000<\/em> or<\/p>\n<pre><code>mlflow.set_tracking_uri(&quot;http:\/\/localhost:5000&quot;)\n<\/code><\/pre>\n<p>After got to http:\/\/localhost:5000 and you can register a logged model from UI or from the code.<\/p>",
        "Answer_comment_count":5,
        "Answer_creation_time":1596626369480,
        "Answer_score":27,
        "Question_favorite_count":2.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63255631",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69689266,
        "Question_title":"How to set a tag at the experiment level in MLFlow",
        "Question_body":"<p>I can see that an experiment in MLFlow can have tags (like runs can have tags).\nI'm able to set a run's tag using <code>mlflow.set_tag<\/code>, but how do I set it for an experiment?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1635000756820,
        "Question_score":2,
        "Question_tags":"python|mlflow",
        "Question_view_count":1047,
        "Owner_creation_time":1244984040077,
        "Owner_last_access_time":1663968051750,
        "Owner_location":"New York, NY",
        "Owner_reputation":13408,
        "Owner_up_votes":306,
        "Owner_down_votes":12,
        "Owner_views":687,
        "Question_last_edit_time":1635005765867,
        "Answer_body":"<p>If you look into the Python API, the very <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html\" rel=\"nofollow noreferrer\">first example<\/a> in <code>mlflow.tracking package<\/code> that shows how to create the <code>MLflowClient<\/code> is really showing how to tag experiment using the <code>client.set_experiment_tag<\/code> function (<a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html#mlflow.tracking.MlflowClient.set_experiment_tag\" rel=\"nofollow noreferrer\">doc<\/a>):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from mlflow.tracking import MlflowClient\n\n# Create an experiment with a name that is unique and case sensitive.\nclient = MlflowClient()\nexperiment_id = client.create_experiment(&quot;Social NLP Experiments&quot;)\nclient.set_experiment_tag(experiment_id, &quot;nlp.framework&quot;, &quot;Spark NLP&quot;)\n<\/code><\/pre>\n<p>you can also set it for model version with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html#mlflow.client.MlflowClient.set_model_version_tag\" rel=\"nofollow noreferrer\">set_model_version_tag<\/a> function, and for registered model with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html#mlflow.tracking.MlflowClient.set_registered_model_tag\" rel=\"nofollow noreferrer\">set_registered_model_tag<\/a>.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1635012053310,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1663958792436,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69689266",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69227917,
        "Question_title":"Connect MLflow server to minio in local",
        "Question_body":"<p>I am trying to connect mlflow with Minio server, both are running on my local machine, I am able to connect my client code to minio by adding the below lines to the code,<\/p>\n<pre><code>os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] =&quot;xxxx&quot;\nos.environ['AWS_SECRET_ACCESS_KEY'] =&quot;xxxxxx&quot; \nos.environ['MLFLOW_TRACKING_URI'] = 'http:\/\/localhost:5000'\n<\/code><\/pre>\n<p>But the mlflow server is not getting connected to Minio. To run Mlflow server, command I use:<\/p>\n<pre><code>mlflow server -h 0.0.0.0 -p 5000 --default-artifact-root s3:\/\/mlbucket --backend-store-uri sqlite:\/\/\/mlflow.db\n<\/code><\/pre>\n<p>The mlflow server runs, but while accessing the artifacts page the server, it throws the error:<\/p>\n<pre><code>raise NoCredentialsError()\nbotocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n<p>So how can I pass the credentials of the Minio to the mlflow server command?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1631902626293,
        "Question_score":1,
        "Question_tags":"python|minio|mlflow",
        "Question_view_count":1136,
        "Owner_creation_time":1310893185210,
        "Owner_last_access_time":1663988189020,
        "Owner_location":"Thiruvananthapuram, Kerala, India",
        "Owner_reputation":2763,
        "Owner_up_votes":373,
        "Owner_down_votes":7,
        "Owner_views":851,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Just add the below environment variables:<\/p>\n<pre><code>export AWS_ACCESS_KEY_ID=&lt;your-aws-access-key-id&gt;\nexport AWS_SECRET_ACCESS_KEY = &lt;your-aws-secret-access-key&gt;\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1634743751773,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69227917",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69126555,
        "Question_title":"how to log KerasClassifier model in a sklearn pipeline mlflow?",
        "Question_body":"<p>I have a set of pre-processing stages in sklearn <code>Pipeline<\/code> and an estimator which is a <code>KerasClassifier<\/code> (<code>from tensorflow.keras.wrappers.scikit_learn import KerasClassifier<\/code>).<\/p>\n<p>My overall goal is to tune and log the whole sklearn pipeline in <code>mlflow<\/code> (in databricks evn). I get a confusing type error which I can't figure out how to reslove:<\/p>\n<blockquote>\n<p>TypeError: can't pickle _thread.RLock objects<\/p>\n<\/blockquote>\n<p>I have the following code (without tuning stage) which returns the above error:<\/p>\n<pre><code>conda_env = _mlflow_conda_env(\n    additional_conda_deps=None,\n    additional_pip_deps=[\n        &quot;cloudpickle=={}&quot;.format(cloudpickle.__version__),\n        &quot;scikit-learn=={}&quot;.format(sklearn.__version__),\n        &quot;numpy=={}&quot;.format(np.__version__),\n        &quot;tensorflow=={}&quot;.format(tf.__version__),\n    ],\n    additional_conda_channels=None,\n)\n\nsearch_space = {\n    &quot;estimator__dense_l1&quot;: 20,\n    &quot;estimator__dense_l2&quot;: 20,\n    &quot;estimator__learning_rate&quot;: 0.1,\n    &quot;estimator__optimizer&quot;: &quot;Adam&quot;,\n}\n\n\ndef create_model(n):\n\n    model = Sequential()\n    model.add(Dense(int(n[&quot;estimator__dense_l1&quot;]), activation=&quot;relu&quot;))\n    model.add(Dense(int(n[&quot;estimator__dense_l2&quot;]), activation=&quot;relu&quot;))\n    model.add(Dense(1, activation=&quot;sigmoid&quot;))\n    model.compile(\n        loss=&quot;binary_crossentropy&quot;,\n        optimizer=n[&quot;estimator__optimizer&quot;],\n        metrics=[&quot;accuracy&quot;],\n    )\n\n    return model\n\n\nmlflow.sklearn.autolog()\nwith mlflow.start_run(nested=True) as run:\n\n    classfier = KerasClassifier(build_fn=create_model, n=search_space)\n    # fit the pipeline\n    clf = Pipeline(steps=[(&quot;preprocessor&quot;, preprocessor), \n                          (&quot;estimator&quot;, classfier)])\n    h = clf.fit(\n        X_train,\n        y_train.values,\n        estimator__validation_split=0.2,\n        estimator__epochs=10,\n        estimator__verbose=2,\n    )\n\n    # log scores\n    acc_score = clf.score(X=X_test, y=y_test)\n    mlflow.log_metric(&quot;accuracy&quot;, acc_score)\n\n    signature = infer_signature(X_test, clf.predict(X_test))\n    # Log the model with a signature that defines the schema of the model's inputs and outputs.\n    mlflow.sklearn.log_model(\n        sk_model=clf, artifact_path=&quot;model&quot;, \n        signature=signature, \n        conda_env=conda_env\n    )\n<\/code><\/pre>\n<p>I also get this warning before the error:<\/p>\n<pre><code>\n    WARNING mlflow.sklearn.utils: Truncated the value of the key `steps`. Truncated value: `[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n                      transformer_weights=None,\n                      transformers=[('num',\n                                   Pipeline(memory=None,\n<\/code><\/pre>\n<p>note the the whole pipeline runs outside mlflow.\ncan someone help?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1631241004103,
        "Question_score":0,
        "Question_tags":"python|tensorflow|scikit-learn|databricks|mlflow",
        "Question_view_count":435,
        "Owner_creation_time":1455667285907,
        "Owner_last_access_time":1663905046563,
        "Owner_location":null,
        "Owner_reputation":283,
        "Owner_up_votes":35,
        "Owner_down_votes":7,
        "Owner_views":85,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I think I find sort of a workaround\/solution for this for now, but I think this issue needs to be addressed in MLFloow anyways.<\/p>\n<p>What I did is not the best way probably.\nI used a python package called <a href=\"https:\/\/scikeras.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">scikeras<\/a> that does this wrapping and then could log the model<\/p>\n<p>The code:<\/p>\n<pre><code>import scikeras \nimport tensorflow as tf \nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Flatten, Activation \n \nfrom scikeras.wrappers import KerasClassifier \n  \n \nclass ModelWrapper(mlflow.pyfunc.PythonModel): \n    def __init__(self, model): \n        self.model = model \n \n    def predict(self, context, model_input): \n        return self.model.predict(model_input) \n \nconda_env =  _mlflow_conda_env( \n      additional_conda_deps=None, \n      additional_pip_deps=[ \n        &quot;cloudpickle=={}&quot;.format(cloudpickle.__version__),  \n        &quot;scikit-learn=={}&quot;.format(sklearn.__version__), \n        &quot;numpy=={}&quot;.format(np.__version__), \n        &quot;tensorflow=={}&quot;.format(tf.__version__), \n        &quot;scikeras=={}&quot;.format(scikeras.__version__), \n      ], \n      additional_conda_channels=None, \n  ) \n \nparam = { \n   &quot;dense_l1&quot;: 20, \n   &quot;dense_l2&quot;: 20, \n   &quot;optimizer__learning_rate&quot;: 0.1, \n   &quot;optimizer&quot;: &quot;Adam&quot;, \n   &quot;loss&quot;:&quot;binary_crossentropy&quot;, \n} \n \n  \ndef create_model(dense_l1, dense_l2, meta): \n  \n  n_features_in_ = meta[&quot;n_features_in_&quot;] \n  X_shape_ = meta[&quot;X_shape_&quot;] \n  n_classes_ = meta[&quot;n_classes_&quot;] \n \n  model = Sequential() \n  model.add(Dense(n_features_in_, input_shape=X_shape_[1:], activation=&quot;relu&quot;)) \n  model.add(Dense(dense_l1, activation=&quot;relu&quot;)) \n  model.add(Dense(dense_l2, activation=&quot;relu&quot;)) \n  model.add(Dense(1, activation=&quot;sigmoid&quot;)) \n \n  return model   \n \nmlflow.sklearn.autolog() \nwith mlflow.start_run(run_name=&quot;sample_run&quot;): \n \n  classfier = KerasClassifier( \n    create_model, \n    loss=param[&quot;loss&quot;], \n    dense_l1=param[&quot;dense_l1&quot;], \n    dense_l2=param[&quot;dense_l2&quot;], \n    optimizer__learning_rate = param[&quot;optimizer__learning_rate&quot;], \n    optimizer= param[&quot;optimizer&quot;], \n) \n \n  # fit the pipeline \n  clf = Pipeline(steps=[('preprocessor', preprocessor), \n                      ('estimator', classfier)])   \n \n  h = clf.fit(X_train, y_train.values) \n  # log scores \n  acc_score = clf.score(X=X_test, y=y_test) \n  mlflow.log_metric(&quot;accuracy&quot;, acc_score) \n  signature = infer_signature(X_test, clf.predict(X_test)) \n  model_nn = ModelWrapper(clf,)  \n \n  mlflow.pyfunc.log_model( \n      python_model= model_nn, \n      artifact_path = &quot;model&quot;,  \n      signature = signature,  \n      conda_env = conda_env \n  ) \n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1631593268677,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69126555",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70098779,
        "Question_title":"How to connect to MLFlow tracking server that has auth?",
        "Question_body":"<p>I want to connect to remote tracking server (<a href=\"http:\/\/123.456.78.90\" rel=\"nofollow noreferrer\">http:\/\/123.456.78.90<\/a>) that requires authentication<\/p>\n<p>When I do this:<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>import mlflow\nmlflow.set_tracking_uri(\"http:\/\/123.456.78.90\")\nmlflow.set_experiment(\"my-experiment\")<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p>I get an error<\/p>\n<p><em>MlflowException: API request to endpoint \/api\/2.0\/mlflow\/experiments\/list failed with error code 401 != 200.\nResponse body: 401 Authorization Required<\/em><\/p>\n<p>I understand that I need to log in first but I have no idea how to do it<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637767811310,
        "Question_score":1,
        "Question_tags":"authorization|tracking|mlflow",
        "Question_view_count":2102,
        "Owner_creation_time":1637766437853,
        "Owner_last_access_time":1663839694783,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Question_last_edit_time":null,
        "Answer_body":"<p><a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#logging-to-a-tracking-server\" rel=\"nofollow noreferrer\">MLflow documentation<\/a> says:<\/p>\n<blockquote>\n<p><code>MLFLOW_TRACKING_USERNAME<\/code> and <code>MLFLOW_TRACKING_PASSWORD<\/code> - username and password to use with HTTP Basic authentication. To use Basic authentication, you must set both environment variables.<\/p>\n<\/blockquote>\n<p>So you just need to set these variables in your code using <code>os.environ<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>os.environ['MLFLOW_TRACKING_USERNAME'] = 'name'\nos.environ['MLFLOW_TRACKING_PASSWORD'] = 'pass'\n<\/code><\/pre>",
        "Answer_comment_count":2,
        "Answer_creation_time":1637773273483,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70098779",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60695933,
        "Question_title":"MLflow: Why can't backend-store-uri be an s3 location?",
        "Question_body":"<p>I'm new to mlflow and I can't figure out why the <code>artifact store<\/code> can't be the same as the <code>backend store<\/code>? <\/p>\n\n<p>The only reason I can think of is to be able to query the experiments with SQL syntax... but since we can interact with the runs using <code>mlflow ui<\/code> I just don't understand why all artifacts and parameters can't go to a same location (which is what happens when using local storage).<\/p>\n\n<p>Can anyone shed some light on this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1584294693613,
        "Question_score":4,
        "Question_tags":"amazon-s3|mlflow",
        "Question_view_count":768,
        "Owner_creation_time":1528574640850,
        "Owner_last_access_time":1663976741153,
        "Owner_location":null,
        "Owner_reputation":133,
        "Owner_up_votes":10,
        "Owner_down_votes":2,
        "Owner_views":76,
        "Question_last_edit_time":null,
        "Answer_body":"<p>MLflow's Artifacts are typically ML models, i.e. relatively large binary files. On the other hand, run data are typically a couple of floats.<\/p>\n<p>In the end it is not a question of what is possible or not (many things are possible if you put enough effort into it), but rather to follow good practices:<\/p>\n<ul>\n<li>storing large binary artifacts in an SQL database is possible but is bound the degrade the performance of the database sooner or later, and this in turn will degrade your user experience.<\/li>\n<li>storing a couple of floats from a SQL database for quick retrieval for display in a front-end or via command line is a robust industry-proven classic<\/li>\n<\/ul>\n<p>It remains true that the documentation of MLflow on the architecture design rationale could be improved (as of 2020)<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1611045077983,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60695933",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61481147,
        "Question_title":"MLFlow Registry high availability",
        "Question_body":"<p>I am running the mlflow registry using <code>mlflow server<\/code> (<a href=\"https:\/\/mlflow.org\/docs\/latest\/model-registry.html\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/model-registry.html<\/a>). The server runs fine. If the server crashes for any reason it restart automatically. But for the time of restart the server is not available.<\/p>\n\n<p>Is it possible to run multiple isntances in parallel behind a load balancer? Is this safe or could it be possible that there are any inconsistencies?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1588079911073,
        "Question_score":0,
        "Question_tags":"mlflow|mlmodel",
        "Question_view_count":501,
        "Owner_creation_time":1378563249260,
        "Owner_last_access_time":1658919410330,
        "Owner_location":null,
        "Owner_reputation":470,
        "Owner_up_votes":128,
        "Owner_down_votes":7,
        "Owner_views":45,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes, it's possible to have multiple instances of MLflow Tracker Service running behind a load balancer.<\/p>\n\n<p>Because the Tracking server is stateless, you could have multiple instances log to a replicated primary DB as a store. A second hot standby can take over if the primary fails.<\/p>\n\n<p>As for the documentation in how to set up replicated instances of your backend store will vary on which one you elect to use, we cannot definitely document all different scenarios and their configurations.<\/p>\n\n<p>I would check the respective documentation of your backend DB and load balancer for how to federate requests to multiple instances of an MLflow tracking server, how to failover to a hot standby or replicated DB, or how to configure a hot-standby replicated DB instance.<\/p>\n\n<p>The short of it: MLflow tracking server is stateless.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1588382828087,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1588394571916,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61481147",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70111193,
        "Question_title":"How can I load the latest model version from MLflow model registry?",
        "Question_body":"<p>I can load a specific version of a model using the mlflow client:<\/p>\n<pre><code>import mlflow\n\nmodel_version = 1\n\nmodel = mlflow.pyfunc.load_model(\n    model_uri=f&quot;models:\/c3760a15e6ac48f88ad7e5af940047d4\/{model_version}&quot;\n)\n<\/code><\/pre>\n<p>But is there a way to load the latest model version?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637843541287,
        "Question_score":3,
        "Question_tags":"mlflow",
        "Question_view_count":1873,
        "Owner_creation_time":1351080779277,
        "Owner_last_access_time":1664084963950,
        "Owner_location":"Leuven, Belgium",
        "Owner_reputation":3126,
        "Owner_up_votes":1817,
        "Owner_down_votes":2,
        "Owner_views":262,
        "Question_last_edit_time":1637850687889,
        "Answer_body":"<p>There is no such thing, like load <code>latest<\/code>, but:<\/p>\n<ul>\n<li>You can specify the stage (<code>staging<\/code>, <code>production<\/code>) - see <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/concepts.html#referencing-artifacts\" rel=\"nofollow noreferrer\">docs<\/a><\/li>\n<li>You can find latest version using the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_latest_versions\" rel=\"nofollow noreferrer\">get_latest_versions<\/a> function - but it will also return latest per stage<\/li>\n<\/ul>\n<p>So you need to define what <code>latest<\/code> means for you.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1637853200393,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70111193",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":57987999,
        "Question_title":"Delete a run in the experiment of mlflow from the UI so the run does not exist in backend store",
        "Question_body":"<p>I found deleting a <code>run<\/code> only change the state from <code>active<\/code> to <code>deleted<\/code>, because the run is still visible in the UI if searching by <code>deleted<\/code>. <\/p>\n\n<p>Is it possible to remove a <code>run<\/code> from the UI to save the space? \nWhen removing a run, does the artifact correspond to the run is also removed?<\/p>\n\n<p>If not, can the run be removed through rest call?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1568794247973,
        "Question_score":6,
        "Question_tags":"mlflow",
        "Question_view_count":3582,
        "Owner_creation_time":1408370821673,
        "Owner_last_access_time":1663216838707,
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":2521,
        "Owner_up_votes":447,
        "Owner_down_votes":13,
        "Owner_views":197,
        "Question_last_edit_time":1568796468592,
        "Answer_body":"<p>You can't do it via the web UI but you can from a python terminal<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmlflow.delete_experiment(69)\n<\/code><\/pre>\n\n<p>Where 69 is the experiment ID<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1569793174477,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57987999",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72994988,
        "Question_title":"How to mlflow-autolog a sklearn ConfusionMatrixDisplay?",
        "Question_body":"<p>I'm trying to log the plot of a <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator\" rel=\"nofollow noreferrer\">confusion matrix generated with scikit-learn<\/a> for a <em>test<\/em> set using <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.sklearn.html\" rel=\"nofollow noreferrer\">mlflow's support for scikit-learn<\/a>.<\/p>\n<p>For this, I tried something that resemble the code below (I'm using mlflow hosted on Databricks, and <code>sklearn==1.0.1<\/code>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sklearn.datasets\nimport pandas as pd\nimport numpy as np\nimport mlflow\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nmlflow.set_tracking_uri(&quot;databricks&quot;)\nmlflow.set_experiment(&quot;\/Users\/name.surname\/plotcm&quot;)\n\ndata = sklearn.datasets.fetch_20newsgroups(categories=['alt.atheism', 'sci.space'])\n\ndf = pd.DataFrame(data = np.c_[data['data'], data['target']])\\\n       .rename({0:'text', 1:'class'}, axis = 'columns')\n\ntrain, test = train_test_split(df)\n\nmy_pipeline = Pipeline([\n    ('vectorizer', TfidfVectorizer()),\n    ('classifier', SGDClassifier(loss='modified_huber')),\n])\n\nmlflow.sklearn.autolog()\n\nfrom sklearn.metrics import ConfusionMatrixDisplay # should I import this after the call to `.autolog()`?\n\nmy_pipeline.fit(train['text'].values, train['class'].values)\n\ncm = ConfusionMatrixDisplay.from_predictions(\n      y_true=test[&quot;class&quot;], y_pred=my_pipeline.predict(test[&quot;text&quot;])\n  )\n<\/code><\/pre>\n<p>while the confusion matrix for the training set is saved in my mlflow run, no png file is created in the mlflow frontend for the <code>test<\/code> set.<\/p>\n<p>If I try to add<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>cm.figure_.savefig('test_confusion_matrix.png')\nmlflow.log_artifact('test_confusion_matrix.png')\n<\/code><\/pre>\n<p>that does the job, but requires explicitly logging the artifact.<\/p>\n<p>Is there an idiomatic\/proper way to autolog the confusion matrix computed using a test set after <code>my_pipeline.fit()<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657892681357,
        "Question_score":0,
        "Question_tags":"python|scikit-learn|confusion-matrix|mlflow",
        "Question_view_count":157,
        "Owner_creation_time":1415722650717,
        "Owner_last_access_time":1664051478173,
        "Owner_location":"Verona, VR, Italy",
        "Owner_reputation":4811,
        "Owner_up_votes":376,
        "Owner_down_votes":73,
        "Owner_views":713,
        "Question_last_edit_time":1658083880967,
        "Answer_body":"<p>The proper way to do this is to use <code>mlflow.log_figure<\/code> as a fluent API announced in <code>MLflow 1.13.0<\/code>. You can read the documentation <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_figure\" rel=\"nofollow noreferrer\">here<\/a>. This code will do the job.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.log_figure(cm.figure_, 'test_confusion_matrix.png')\n<\/code><\/pre>\n<p>This function implicitly store the image, and then calls <code>log_artifact<\/code> against that path, something like you did.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1658304934100,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72994988",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62711259,
        "Question_title":"Customize metric visualization in MLFlow UI when using mlflow.tensorflow.autolog()",
        "Question_body":"<p>I'm trying to integrate MLFlow to my project. Because I'm using <code>tf.keras.fit_generator()<\/code> for my training so I take advantage of <code>mlflow.tensorflow.autolog()<\/code>(<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a> here) to enable automatic logging of metrics and parameters:<\/p>\n<pre><code>    model = Unet()\n    optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n\n    metrics = [IOUScore(threshold=0.5), FScore(threshold=0.5)]\n    model.compile(optimizer, customized_loss, metrics)\n\n    callbacks = [\n        tf.keras.callbacks.ModelCheckpoint(&quot;model.h5&quot;, save_weights_only=True, save_best_only=True, mode='min'),\n        tf.keras.callbacks.TensorBoard(log_dir='.\/logs', profile_batch=0, update_freq='batch'),\n    ]\n\n\n    train_dataset = Dataset(src_dir=SOURCE_DIR)\n\n    train_data_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n\n   \n    with mlflow.start_run():\n        mlflow.tensorflow.autolog()\n        mlflow.log_param(&quot;batch_size&quot;, BATCH_SIZE)\n\n        model.fit_generator(\n            train_data_loader,\n            steps_per_epoch=len(train_data_loader),\n            epochs=EPOCHS,\n            callbacks=callbacks   \n            )\n<\/code><\/pre>\n<p>I expected something like this (just a demonstration taken from the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#visualizing-metrics\" rel=\"nofollow noreferrer\">docs<\/a>):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" alt=\"Visualization on the docs\" \/><\/a><\/p>\n<p>However, after the training finished, this is what I got:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" alt=\"f1_score visualization\" \/><\/a><\/p>\n<p>How can I configure so that the metric plot will update and display its value at each epoch instead of just showing the latest value?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593764146630,
        "Question_score":2,
        "Question_tags":"tf.keras|mlflow",
        "Question_view_count":1035,
        "Owner_creation_time":1467943515393,
        "Owner_last_access_time":1664083114540,
        "Owner_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Owner_reputation":173,
        "Owner_up_votes":97,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":1594008626392,
        "Answer_body":"<p>After searching around, I found <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/2390\" rel=\"nofollow noreferrer\">this issue<\/a> related to my problem above. Actually, all my metrics just logged once each training (instead of each epoch as my intuitive thought). The reason is I didn't specify the <code>every_n_iter<\/code> parameter in <code>mlflow.tensorflow.autolog()<\/code>, which indicates how many 'iterations' must pass before MLflow logs metric executed (see the <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a>). So, changing my code to:<\/p>\n<p><code>mlflow.tensorflow.autolog(every_n_iter=1)<\/code><\/p>\n<p>fixed the problem.<\/p>\n<p>P\/s: Remember that in TF 2.x, an 'iteration' is an epoch (in TF 1.x it's a batch).<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1594008525280,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62711259",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60088889,
        "Question_title":"How Do You \"Permanently\" Delete An Experiment In Mlflow?",
        "Question_body":"<p>Permanent deletion of an experiment isn't documented anywhere. I'm using Mlflow w\/ backend postgres db<\/p>\n\n<p>Here's what I've run: <\/p>\n\n<pre><code>client = MlflowClient(tracking_uri=server)\nclient.delete_experiment(1)\n<\/code><\/pre>\n\n<p>This deletes the the experiment, but when I run a new experiment with the same name as the one I just deleted, it will return this error:<\/p>\n\n<pre><code>mlflow.exceptions.MlflowException: Cannot set a deleted experiment 'cross-sell' as the active experiment. You can restore the experiment, or permanently delete the  experiment to create a new one.\n<\/code><\/pre>\n\n<p>I cannot find anywhere in the documentation that shows how to permanently delete everything.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1580970401043,
        "Question_score":20,
        "Question_tags":"python|mlflow",
        "Question_view_count":13984,
        "Owner_creation_time":1443225809767,
        "Owner_last_access_time":1663898334270,
        "Owner_location":"Vancouver, BC, Canada",
        "Owner_reputation":2332,
        "Owner_up_votes":133,
        "Owner_down_votes":3,
        "Owner_views":560,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Unfortunately it seems there is no way to do this via the UI or CLI at the moment :-\/<\/p>\n\n<p>The way to do it depends on the type of backend file store that you are using.<\/p>\n\n<p><strong>Filestore<\/strong>:<\/p>\n\n<p>If you are using the filesystem as a storage mechanism (the default) then it is easy. The 'deleted' experiments are moved to a <code>.trash<\/code> folder. You just need to clear that out:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>rm -rf mlruns\/.trash\/*\n<\/code><\/pre>\n\n<p>As of the current version of the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/cli.html#mlflow-experiments-delete\" rel=\"noreferrer\">documentation<\/a> (1.7.2), they remark:<\/p>\n\n<blockquote>\n  <p>It is recommended to use a cron job or an alternate workflow mechanism to clear <code>.trash<\/code> folder.<\/p>\n<\/blockquote>\n\n<p><strong>SQL Database:<\/strong><\/p>\n\n<p>This is more tricky, as there are dependencies that need to be deleted. I am using MySQL, and these commands work for me:<\/p>\n\n<pre class=\"lang-sql prettyprint-override\"><code>USE mlflow_db;  # the name of your database\nDELETE FROM experiment_tags WHERE experiment_id=ANY(\n    SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n);\nDELETE FROM latest_metrics WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM metrics WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM tags WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM runs WHERE experiment_id=ANY(\n    SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n);\nDELETE FROM experiments where lifecycle_stage=\"deleted\";\n<\/code><\/pre>",
        "Answer_comment_count":1,
        "Answer_creation_time":1585231513453,
        "Answer_score":22,
        "Question_favorite_count":3.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60088889",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70051420,
        "Question_title":"MLFlow projects; bash: python: command not found",
        "Question_body":"<p>I'm running MLflow Project for a model using following command from my ubuntu 20.04 terminal<\/p>\n<pre><code>mlflow run . --no-conda -P alpha=0.5\n<\/code><\/pre>\n<p>My system doesn't have conda or python (It does however have python3). So, I added alias for python using terminal<\/p>\n<pre><code>alias python='python3'\n<\/code><\/pre>\n<p>After which I could open python in terminal using <code>python<\/code>. However, I still got the same error<\/p>\n<pre><code>2021\/11\/21 08:07:34 INFO mlflow.projects.utils: === Created directory \/tmp\/tmpp4h595ql for downloading remote URIs passed to arguments of type 'path' ===\n2021\/11\/21 08:07:34 INFO mlflow.projects.backend.local: === Running command 'python tracking.py 0.5 0.1' in run with ID 'e50ca47b3f8848a083906be6220c26fc' === \nbash: python: command not found\n2021\/11\/21 08:07:34 ERROR mlflow.cli: === Run (ID 'e50ca47b3f8848a083906be6220c26fc') failed ===\n<\/code><\/pre>\n<p>How to get rid of this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637462657297,
        "Question_score":0,
        "Question_tags":"python|bash|ubuntu|terminal|mlflow",
        "Question_view_count":281,
        "Owner_creation_time":1601004709207,
        "Owner_last_access_time":1664003822153,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":1074,
        "Owner_up_votes":239,
        "Owner_down_votes":21,
        "Owner_views":143,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Change <code>python<\/code> to <code>python3<\/code> in the <code>MLproject<\/code> file to the resolve error.<\/p>\n<pre><code>command: &quot;python3 tracking.py {alpha} {l1_ratio}&quot;\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1637463754447,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1637502993976,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70051420",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71708147,
        "Question_title":"MLFlow tracking ui not showing experiments on local machine (laptop)",
        "Question_body":"<p>I am a beginner in mlflow and was trying to set it up locally using Anaconda 3.\nI have created a new environment in anaconda and install mlflow and sklearn in it. Now I am using jupyter notebook to run my sample code for mlflow.<\/p>\n<p>'''<\/p>\n<pre><code>import os\nimport warnings\nimport sys\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet\nfrom urllib.parse import urlparse\nimport mlflow\nimport mlflow.sklearn\n\nimport logging\n\nlogging.basicConfig(level=logging.WARN)\nlogger = logging.getLogger(__name__)\n\nwarnings.filterwarnings(&quot;ignore&quot;)\nnp.random.seed(40)\n\n\nmlflow.set_tracking_uri(&quot;file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun&quot;)\n\nmlflow.get_tracking_uri()\n\nmlflow.get_experiment\n\n#experiment_id = mlflow.create_experiment(&quot;Mlflow_demo&quot;)\nexperiment_id = mlflow.create_experiment(&quot;Demo3&quot;)\nexperiment = mlflow.get_experiment(experiment_id)\nprint(&quot;Name: {}&quot;.format(experiment.name))\nprint(&quot;Experiment_id: {}&quot;.format(experiment.experiment_id))\nprint(&quot;Artifact Location: {}&quot;.format(experiment.artifact_location))\nprint(&quot;Tags: {}&quot;.format(experiment.tags))\nprint(&quot;Lifecycle_stage: {}&quot;.format(experiment.lifecycle_stage))\n\nmlflow.set_experiment(&quot;Demo3&quot;)\n\ndef eval_metrics(actual, pred):\n    rmse = np.sqrt(mean_squared_error(actual, pred))\n    mae = mean_absolute_error(actual, pred)\n    r2 = r2_score(actual, pred)\n    return rmse, mae, r2\n\n# Read the wine-quality csv file from the URL\ncsv_url =\\\n    'http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/wine-quality\/winequality-red.csv'\ntry:\n    data = pd.read_csv(csv_url, sep=';')\nexcept Exception as e:\n    logger.exception(\n        &quot;Unable to download training &amp; test CSV, check your internet connection. Error: %s&quot;, e)\n\ndata.head(2)\n\n\ndef train_model(data, alpha, l1_ratio):\n    \n    # Split the data into training and test sets. (0.75, 0.25) split.\n    train, test = train_test_split(data)\n\n    # The predicted column is &quot;quality&quot; which is a scalar from [3, 9]\n    train_x = train.drop([&quot;quality&quot;], axis=1)\n    test_x = test.drop([&quot;quality&quot;], axis=1)\n    train_y = train[[&quot;quality&quot;]]\n    test_y = test[[&quot;quality&quot;]]\n\n    # Set default values if no alpha is provided\n    alpha = alpha\n    l1_ratio = l1_ratio\n\n\n    # Execute ElasticNet\n    lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n    lr.fit(train_x, train_y)\n\n    # Evaluate Metrics\n    predicted_qualities = lr.predict(test_x)\n    (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n    # Print out metrics\n    print(&quot;Elasticnet model (alpha=%f, l1_ratio=%f):&quot; % (alpha, l1_ratio))\n    print(&quot;  RMSE: %s&quot; % rmse)\n    print(&quot;  MAE: %s&quot; % mae)\n    print(&quot;  R2: %s&quot; % r2)\n    \n    # Log parameter, metrics, and model to MLflow\n    with mlflow.start_run(experiment_id = experiment_id):\n        mlflow.log_param(&quot;alpha&quot;, alpha)\n        mlflow.log_param(&quot;l1_ratio&quot;, l1_ratio)\n        mlflow.log_metric(&quot;rmse&quot;, rmse)\n        mlflow.log_metric(&quot;r2&quot;, r2)\n        mlflow.log_metric(&quot;mae&quot;, mae)\n        mlflow.sklearn.log_model(lr, &quot;model&quot;)\n        \n\ntrain_model(data, 0.5, 0.5)\n\ntrain_model(data, 0.5, 0.3)\n\ntrain_model(data, 0.4, 0.3)\n<\/code><\/pre>\n<p>'''<\/p>\n<p>using above code, I am successfully able to create 3 different experiment as I can see the folders created in my local directory as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jKqgX.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Now, I am trying to run the mlflow ui using the jupyter terminal in my chrome browser and I am able to open the mlflow ui but cannot see and experiments as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6KaQK.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Could you help me in finding where I am going wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1648821712310,
        "Question_score":3,
        "Question_tags":"python|windows|mlflow|mlops",
        "Question_view_count":936,
        "Owner_creation_time":1648820658173,
        "Owner_last_access_time":1663932565810,
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Where do you run <code>mlflow ui<\/code> command?<\/p>\n<p>I think if you pass tracking ui path in the arguments, it would work:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>mlflow ui --backend-store-uri file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1650761211590,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1659992652860,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71708147",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":66320435,
        "Question_title":"mlflow.exceptions.MlflowException: Changing param values is not allowed. Param with key='input_rows' was already logged with value='32205'",
        "Question_body":"<p>I am using Mlflow as a work orchestration tool. I have a Machine Learning pipeline. In this pipeline, I have real-time data. I'm listening this data with Apache Kafka. Also, I'm doing this: Whenever 250 message comes to this topic, I'm gathering them, and I'm appending this message my previous data. After that, my training function is triggered. Thus, I am able to making new training in every 250 new data. With Mlflow, I can show the results, metrics and any other parameters of trained models. But After training occurred one time, the second one doesn't occurs, and It throws me this error which I have shown in title. Here it is my consumer:<\/p>\n<pre><code>topic_name = 'twitterdata'\ntrain_every = 250\n\n\ndef consume_tweets():\n    consumer = KafkaConsumer(\n        topic_name,\n        bootstrap_servers=['localhost:9093'],\n        auto_offset_reset='latest',\n        enable_auto_commit=True,\n        auto_commit_interval_ms=5000,\n        fetch_max_bytes=128,\n        max_poll_records=100,\n        value_deserializer=lambda x: json.loads(x.decode('utf-8')))\n\n    tweet_counter = 0\n    for message in consumer:\n        tweets = json.loads(json.dumps(message.value))\n        # print(tweets['text'])\n        tweet_sentiment = make_prediction(tweets['text'])\n\n        if tweet_counter == train_every:\n            update_df()\n            data_path = 'data\/updated_tweets.csv'\n            train(data_path)\n            print(&quot;\\nTraining with new data is completed!\\n&quot;)\n            tweet_counter = 0\n\n        else:\n            tweet_counter += 1\n\n        publish_prediction(tweet_sentiment, tweets['text'])\n\n<\/code><\/pre>\n<p>And here it is my train.py:<\/p>\n<pre><code>train_tweets = pd.read_csv(DATA_PATH)\n    # train_tweets = train_tweets[:20000]\n\n    tweets = train_tweets.tweet.values\n    labels = train_tweets.label.values\n\n    # Log data params\n    mlflow.log_param('input_rows', train_tweets.shape[0])\n\n    # Do preprocessing and return vectorizer with it\n    vectorizer, processed_features = embedding(tweets)\n\n    # Saving vectorizer\n    save_vectorizer(vectorizer)\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)\n\n    # Handle imbalanced data by using 'Smote' and log to Mlflow\n    smote = SMOTE('minority')\n    mlflow.log_param(&quot;over-sampling&quot;, smote)\n\n    X_train, y_train = smote.fit_sample(X_train, y_train)\n\n    # text_classifier = MultinomialNB()\n    text_classifier = LogisticRegression(max_iter=10000)\n    text_classifier.fit(X_train, y_train)\n    predictions = text_classifier.predict(X_test)\n\n    # Model metrics\n    (rmse, mae, r2) = eval_metrics(y_test, predictions)\n\n    mlflow.log_param('os-row-Xtrain', X_train.shape[0])\n    mlflow.log_param('os-row-ytrain', y_train.shape[0])\n    mlflow.log_param(&quot;model_name&quot;, text_classifier)\n    mlflow.log_metric(&quot;rmse&quot;, rmse)\n    mlflow.log_metric(&quot;r2&quot;, r2)\n    mlflow.log_metric(&quot;mae&quot;, mae)\n    mlflow.log_metric('acc_score', accuracy_score(y_test, predictions))\n\n    mlflow.sklearn.log_model(text_classifier, &quot;model&quot;)\n<\/code><\/pre>\n<p>I couldn't solve the problem. MLflow is one of the newest tool, so issues and examples of Mlflow are very few.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1614014592930,
        "Question_score":1,
        "Question_tags":"python|machine-learning|apache-kafka|mlflow|real-time-data",
        "Question_view_count":2716,
        "Owner_creation_time":1613130263950,
        "Owner_last_access_time":1663846837707,
        "Owner_location":"Turkey",
        "Owner_reputation":44,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":1614015098323,
        "Answer_body":"<p>I think you need an MLflow &quot;run&quot; for every new batch of data, so that your parameters are logged independently for each new training.<\/p>\n<p>So, try the following in your consumer:<\/p>\n<pre><code>if tweet_counter == train_every:\n            update_df()\n            data_path = 'data\/updated_tweets.csv'\n            with mlflow.start_run() as mlrun:\n               train(data_path)\n            print(&quot;\\nTraining with new data is completed!\\n&quot;)\n            tweet_counter = 0\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1614350710747,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66320435",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72694707,
        "Question_title":"Multiple artifact paths when logging a model using mlflow and sklearn",
        "Question_body":"<p>I'm using mlflow to log parameters and artifacts of a Logistic Regression, but when I try to log the model so I can see all the files in the Mlflow UI, I see two folders: one named 'model' and the other one named 'logger' (the one I set).<\/p>\n<pre><code>model = LogisticRegression()\n\nmlflow.set_tracking_uri('file:\/\/\/artifacts')\nmlflow.set_experiment('test')\nmlflow.autolog()\n\nwith mlflow.start_run(run_name=run_name) as run:\n   model.train(X_train, y_train)\n   mlflow.sklearn.log_model(model, 'logreg')\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BtIHo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BtIHo.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Not sure if I'm missing something or if there's a configuration for that.<\/p>\n<p>I hope someone out there can help me!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655778299153,
        "Question_score":0,
        "Question_tags":"python|machine-learning|scikit-learn|mlflow|scikit-learn-pipeline",
        "Question_view_count":122,
        "Owner_creation_time":1544467691223,
        "Owner_last_access_time":1659381626413,
        "Owner_location":"Zacatecas, Mexico",
        "Owner_reputation":131,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You have set <code>autolog<\/code> and you are also logging the model explicitly. Remove one and then try.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1655933087183,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72694707",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71262010,
        "Question_title":"Download model artefact from Databricks workspace",
        "Question_body":"<p>How can I download a mlflow model artefact in a docker container from databricks workspace?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645771255510,
        "Question_score":0,
        "Question_tags":"docker|databricks|azure-databricks|mlflow",
        "Question_view_count":370,
        "Owner_creation_time":1411361217027,
        "Owner_last_access_time":1662192778187,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":569,
        "Owner_up_votes":41,
        "Owner_down_votes":3,
        "Owner_views":123,
        "Question_last_edit_time":1645795991323,
        "Answer_body":"<p>To download a model from Databricks workspace you need to do two things:<\/p>\n<ol>\n<li><p>Set MLFlow tracking URI to databricks using python API<\/p>\n<\/li>\n<li><p>Setup databricks authentication. I prefer authenticating by setting the following environment variables, you can also use databricks CLI to authenticate:<\/p>\n<pre><code>DATABRICKS_HOST\n\nDATABRICKS_TOKEN\n<\/code><\/pre>\n<\/li>\n<li><p>Here's a basic code snippet to download a model from Databricks workspace model registry:<\/p>\n<pre><code>import os\nimport mlflow\nfrom mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\n\nmodel_name = &quot;example-model-name&quot;\nmodel_stage = &quot;Staging&quot;  # Should be either 'Staging' or 'Production'\n\nmlflow.set_tracking_uri(&quot;databricks&quot;)\n\nos.makedirs(&quot;model&quot;, exist_ok=True)\nlocal_path = ModelsArtifactRepository(\n    f'models:\/{model_name}\/{model_stage}').download_artifacts(&quot;&quot;, dst_path=&quot;model&quot;)\n\nprint(f'{model_stage} Model {model_name} is downloaded at {local_path}')\n<\/code><\/pre>\n<p>Running above python script will download an ML model in the model directory.<\/p>\n<p><strong>Containerizing MLFlow model serving with Docker<\/strong><\/p>\n<p>The next step is to package this downloaded model in a docker image and serve a model when you run the image.<\/p>\n<\/li>\n<\/ol>\n<p>Here's a basic Dockerfile to do the same:<\/p>\n<pre><code>FROM continuumio\/miniconda3\n\nENV MLFLOW_HOME \/opt\/mlflow\nENV MLFLOW_VERSION 1.12.1\nENV PORT 5000\n\nRUN conda install -c conda-forge mlflow=${MLFLOW_VERSION}\n\nCOPY model\/ ${MLFLOW_HOME}\/model\n\nWORKDIR ${MLFLOW_HOME}\n\nRUN mlflow models prepare-env -m ${MLFLOW_HOME}\/model\n\nRUN useradd -d ${MLFLOW_HOME} mlflow\nRUN chown mlflow: ${MLFLOW_HOME}\nUSER mlflow\n\nCMD mlflow models serve -m ${MLFLOW_HOME}\/model --host 0.0.0.0 --port ${PORT}\n<\/code><\/pre>\n<p>For more information you can follow this <a href=\"https:\/\/dev.to\/itachiredhair\/downloading-mlflow-model-from-databricks-and-serving-with-docker-38ip\" rel=\"nofollow noreferrer\">article<\/a> from Akshay Milmile<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1645776779603,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1645795937627,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71262010",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73501103,
        "Question_title":"Getting Bad request while searching run in mlflow",
        "Question_body":"<p>Training a ml model with mlflow in azure environment.<\/p>\n<pre><code>import mlflow\nfrom mlflow import MlflowClient\nfrom azureml.core import Experiment, Workspace\n\nexperiment_name = 'housing-lin-mlflow'\n\nexperiment = Experiment(ws, experiment_name)\n\nruns = mlflow.search_runs(experiment_ids=[ experiment.id ])\n\n<\/code><\/pre>\n<p>While fetching runs from search_runs getting this error :<\/p>\n<pre><code>RestException: BAD_REQUEST: For input string: &quot;5b649b3c-3b8f-497a-bb4f&quot;\n<\/code><\/pre>\n<p>MLflow version : 1.28.0\nIn Azure studio jobs have been created and successfully run.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661517215980,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-service|mlflow",
        "Question_view_count":56,
        "Owner_creation_time":1582101477803,
        "Owner_last_access_time":1663953873503,
        "Owner_location":"Delhi, India",
        "Owner_reputation":171,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":53,
        "Question_last_edit_time":1661625379892,
        "Answer_body":"<p>The bad request in MLFlow after successful running the job is because of not giving proper API permissions for the application.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Search for <strong>MLFLOW<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Scroll down<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s50AL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s50AL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Click on View API Permissions<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Under API permissions, assign the permissions according to the application running region and requirements. Checkout the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-models-mlflow\" rel=\"nofollow noreferrer\">document<\/a> for further information.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1661603882123,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73501103",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67052295,
        "Question_title":"MLflow unfinished experiment saved as finished",
        "Question_body":"<p>when I create a run using <code>mlflow.start_run()<\/code> ,even if my script is interrupted before executing <code>mlflow.end_run()<\/code>, the run gets tagged as finished instead of unfinished in Status?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1618197962217,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":332,
        "Owner_creation_time":1578750761197,
        "Owner_last_access_time":1664024328047,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When your notebook stops the run gets the status finished. However, if you want to continue logging metrics or artifacts to that run, you just need to use <code>mlflow.start_run(run_id=&quot;YourRunIDYouCanGetItFromUI&quot;)<\/code>. This is explained in the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.start_run\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1618223603527,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67052295",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68812238,
        "Question_title":"How to export a MLFlow Model from Azure Databricks as an Azure DevOps Artifacts for CD phase?",
        "Question_body":"<p>I am trying to create an MLOps Pipeline using Azure DevOps and Azure Databricks. From Azure DevOps, I am submitting a Databricks job to a cluster, which trains a Machine Learning Model and saves it into MLFlow Model Registry with a custom flavour (using PyFunc Custom Model).<\/p>\n<p>Now after the job gets over, I want to export this MLFlow Object (with all dependencies - Conda dependencies, two model files - one <code>.pkl<\/code> and one <code>.h5<\/code>, the Python Class with <code>load_context()<\/code> and <code>predict()<\/code> functions defined so that after exporting I can import it and call predict as we do with MLFlow Models).<\/p>\n<p>How do I export this entire MLFlow Model and save it as an AzureDevOps Artifact to be used in the CD phase (where I will deploy it to an AKS cluster with a custom base image)?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1629179063687,
        "Question_score":1,
        "Question_tags":"azure|azure-devops|azure-databricks|mlflow",
        "Question_view_count":575,
        "Owner_creation_time":1601729162437,
        "Owner_last_access_time":1663774065773,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":887,
        "Owner_up_votes":187,
        "Owner_down_votes":32,
        "Owner_views":130,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There is no official way to export a Databricks MLflow run from one workspace to another. However, there is an &quot;unofficial&quot; tool that does most of the job with the main limitation being that notebook revisions linked to a run cannot be exported due to lack of a REST API endpoint for this.<\/p>\n<p><a href=\"https:\/\/github.com\/amesar\/mlflow-export-import\" rel=\"nofollow noreferrer\">https:\/\/github.com\/amesar\/mlflow-export-import<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1629787454223,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68812238",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":51335594,
        "Question_title":"Error with \"mlflow ui\" when trying to run it on MS Windows",
        "Question_body":"<p>When I run <code>mlflow ui<\/code> the following error occurred:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Anaconda3\\Scripts\\gunicorn.exe\\__main__.py\", line 5, in &lt;module&gt;\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\app\\wsgiapp.py\", line 9, in &lt;module&gt;\n    from gunicorn.app.base import Application\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\app\\base.py\", line 12, in &lt;module&gt;\n    from gunicorn import util\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\util.py\", line 9, in &lt;module&gt;\n    import fcntl\nModuleNotFoundError: No module named 'fcntl'\nTraceback (most recent call last):\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Anaconda3\\Scripts\\mlflow.exe\\__main__.py\", line 9, in &lt;module&gt;\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 1066, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\cli.py\", line 131, in ui\n    mlflow.server._run_server(file_store, file_store, host, port, 1)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\server\\__init__.py\", line 48, in _run_server\n    env=env_map, stream_output=True)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\utils\\process.py\", line 38, in exec_cmd\n    raise ShellCommandException(\"Non-zero exitcode: %s\" % (exit_code))\nmlflow.utils.process.ShellCommandException: Non-zero exitcode: 1\n<\/code><\/pre>\n\n<p>I used anaconda + python 3.6.5 and I installed git and set path with <code>C:\\Program Files\\Git\\bin\\git.exe<\/code> and <code>C:\\Program Files\\Git\\cmd<\/code>.<\/p>\n\n<p>I installed <code>mlflow<\/code> whit <code>pip install mlflow<\/code> and its version is 0.2.1.<\/p>\n\n<p>I set a variable with name <code>GIT_PYTHON_GIT_EXECUTABLE<\/code> and value <code>C:\\Program Files\\Git\\bin\\git.exe<\/code> in Environment Variables. <\/p>\n\n<p>How can I solve this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1531546446273,
        "Question_score":1,
        "Question_tags":"python|windows|fcntl|mlflow",
        "Question_view_count":4688,
        "Owner_creation_time":1308552848513,
        "Owner_last_access_time":1664030809627,
        "Owner_location":null,
        "Owner_reputation":1177,
        "Owner_up_votes":24,
        "Owner_down_votes":0,
        "Owner_views":144,
        "Question_last_edit_time":1531837117032,
        "Answer_body":"<p><a href=\"https:\/\/github.com\/databricks\/mlflow\" rel=\"nofollow noreferrer\">mlflow documentation<\/a> already says that <\/p>\n\n<blockquote>\n  <p>Note 2: We <strong>do not currently support running MLflow on Windows<\/strong>.\n  Despite this, we would appreciate any contributions to make MLflow\n  work better on Windows.<\/p>\n<\/blockquote>\n\n<p>You're hitting <code>fcntl<\/code> problem: it's not available on MS Windows platform because it's a \"wrapper\" around the <a href=\"http:\/\/man7.org\/linux\/man-pages\/man2\/fcntl.2.html\" rel=\"nofollow noreferrer\">fcntl function<\/a> that's available on POSIX-compatible systems. (See <a href=\"https:\/\/stackoverflow.com\/a\/1422436\/236007\">https:\/\/stackoverflow.com\/a\/1422436\/236007<\/a> for more details.)<\/p>\n\n<p>Solving this requires modifying the source code of mlflow accordingly. <\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1531837039907,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51335594",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65494496,
        "Question_title":"How to load a model using the object \"mlflow.tracking.client.MlflowClient\"?",
        "Question_body":"<p>I'm stuck with the MLFlow model registry. Does anyone know how to load a model using the object &quot;mlflow.tracking.client.MlflowClient&quot;?<\/p>\n<p>I would like to do a predict after with that. I'm sure I'm wrong somewhere because I've already done that in the past. I'm not able to find it in the doc, in the web.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1609255468723,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":608,
        "Owner_creation_time":1423640080283,
        "Owner_last_access_time":1663943557963,
        "Owner_location":"Lyon, France",
        "Owner_reputation":457,
        "Owner_up_votes":19,
        "Owner_down_votes":2,
        "Owner_views":125,
        "Question_last_edit_time":1611139575950,
        "Answer_body":"<p>You'll have to make use of <code>mlflow.&lt;model_flavor&gt;.load_model()<\/code> to load a given model from the Model Registry. For example:<\/p>\n<pre><code>import mlflow.pyfunc\n\nmodel = mlflow.pyfunc.load_model(\n          model_uri=&quot;models:\/&lt;model_name&gt;\/&lt;model_version&gt;&quot;\n          )\n\nmodel.predict(...)\n<\/code><\/pre>\n<p>With <code>mlflow.tracking.client.MlflowClient<\/code> you can retrieve metadata about a model from the model registry, but for retrieving the actual model you will need to use <code>mlflow.&lt;model_flavor&gt;.load_model<\/code>. For example, you could use the MlflowClient to get the download URI for a given model, and then use <code>mlflow.&lt;flavor&gt;.load_model<\/code> to retrieve that model.<\/p>\n<pre><code>model_uri = client.get_model_version_download_uri(&quot;&lt;model_name&gt;&quot;, &lt;version&gt;)\nmodel = mlflow.pyfunc.load_model(model_uri)\n\nmodel.predict(...)\n<\/code><\/pre>",
        "Answer_comment_count":1,
        "Answer_creation_time":1609791959680,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65494496",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62296590,
        "Question_title":"MLFlow and Hydra causing crash when used together",
        "Question_body":"<p>I'm trying to utilize Hydra with MLFlow, so I wrote the bare minimum script to see if they worked together (importing etc.). Both work fine on their own, but when put together I get a weird outcome.<\/p>\n\n<p>I have the script below:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import hydra\nfrom omegaconf import DictConfig\nfrom mlflow import log_metric, log_param, log_artifact,start_run\n\n@hydra.main(config_path=\"config.yaml\")\ndef my_app(cfg : DictConfig):\n    # print(cfg.pretty())\n    # print(cfg['coordinates']['x0'])\n    log_param(\"a\",2)\n    log_metric(\"b\",3)\n\nif __name__ == \"__main__\":\n    my_app()\n<\/code><\/pre>\n\n<p>However when ran, I get the error below:<\/p>\n\n<pre><code>ilknull@nurmachine:~\/Files\/Code\/Python\/MLFlow_test$ python3 hydra_temp.py \nError in atexit._run_exitfuncs:\nTraceback (most recent call last):\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/fluent.py\", line 164, in end_run\n    MlflowClient().set_terminated(_active_run_stack[-1].info.run_id, status)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 311, in set_terminated\n    self._tracking_client.set_terminated(run_id, status, end_time)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 312, in set_terminated\n    end_time=end_time)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/file_store.py\", line 377, in update_run_info\n    run_info = self._get_run_info(run_id)\n  File \"\/home\/ilknull\/.local\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/file_store.py\", line 442, in _get_run_info\n    databricks_pb2.RESOURCE_DOES_NOT_EXIST)\nmlflow.exceptions.MlflowException: Run '9066793c02604a6783d081ed965d5eff' not found\n<\/code><\/pre>\n\n<p>Again, they work perfectly fine when used separately, but together they cause this error. Any ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591767515617,
        "Question_score":4,
        "Question_tags":"python|docker|exception|mlflow|fb-hydra",
        "Question_view_count":718,
        "Owner_creation_time":1583072555673,
        "Owner_last_access_time":1663970229957,
        "Owner_location":null,
        "Owner_reputation":301,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Thanks for reporting this. I was not aware of this issue.<\/p>\n\n<p>This is because Hydra is changing your current working directory for each run.<\/p>\n\n<p>I did some digging, this is what you can do:<\/p>\n\n<ol>\n<li>Set the MLFLOW_TRACKING_URI environment variable:<\/li>\n<\/ol>\n\n<pre><code>MLFLOW_TRACKING_URI=file:\/\/\/$(pwd)\/.mlflow  python3 hydra_temp.py\n<\/code><\/pre>\n\n<ol start=\"2\">\n<li>Call set_tracking_url() before hydra.main() starts:<\/li>\n<\/ol>\n\n<pre><code>import hydra\nfrom omegaconf import DictConfig\nfrom mlflow import log_metric, log_param, set_tracking_uri\nimport os\n\nset_tracking_uri(f\"file:\/\/\/{os.getcwd()}\/.mlflow\")\n\n@hydra.main(config_name=\"config\")\ndef my_app(cfg: DictConfig):\n    log_param(\"a\", 2)\n    log_metric(\"b\", 3)\n\n\nif __name__ == \"__main__\":\n    my_app()\n<\/code><\/pre>\n\n<ol start=\"3\">\n<li>Wait for my <a href=\"https:\/\/github.com\/facebookresearch\/hydra\/issues\/664\" rel=\"nofollow noreferrer\">new issue<\/a> to get resolved, then there will have a proper plugin to integrate with mlflow.\n(This will probably take a while).<\/li>\n<\/ol>\n\n<p>By the way, Hydra 1.0 has new support for setting environment variables:<\/p>\n\n<p>This <em>ALMOST<\/em> works:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>hydra:\n  job:\n    env_set:\n      MLFLOW_TRACKING_DIR: file:\/\/${hydra:runtime.cwd}\/.mlflow\n      MLFLOW_TRACKING_URI: file:\/\/${hydra:runtime.cwd}\/.mlflow\n<\/code><\/pre>\n\n<p>Unfortunately Hydra is cleaning up the env variables when your function exits, and MLFlow is making the final save when the process exits so the env variable is no longer set.\nMLFlow also keeps re-initializing the FileStore object used to store the experiments data. If they would have initialized it just once and reused the same object the above should would have worked.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1591773861797,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1591782103620,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62296590",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56647549,
        "Question_title":"MLflow Error while deploying the Model to local REST server",
        "Question_body":"<blockquote>\n  <p><strong>System Details:<\/strong><\/p>\n  \n  <p>Operating System: Ubuntu 19.04<\/p>\n  \n  <p>Anaconda version: 2019.03<\/p>\n  \n  <p>Python version: 3.7.3<\/p>\n  \n  <p>mlflow version: 1.0.0<\/p>\n<\/blockquote>\n\n<p><strong>Steps to Reproduce:<\/strong> <a href=\"https:\/\/mlflow.org\/docs\/latest\/tutorial.html\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tutorial.html<\/a><\/p>\n\n<p><strong>Error at line\/command:<\/strong> <code>mlflow models serve -m [path_to_model] -p 1234<\/code><\/p>\n\n<p><strong>Error:<\/strong>\nCommand 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1>&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1<\/p>\n\n<p><strong>Terminal Log:<\/strong><\/p>\n\n<pre><code>(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# mlflow models serve -m $(pwd) -p 1234\n2019\/06\/18 16:15:16 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n2019\/06\/18 16:15:17 INFO mlflow.pyfunc.backend: === Running command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app'\nbash: activate: No such file or directory\nTraceback (most recent call last):\n  File \"\/root\/anaconda3\/envs\/mlflow\/bin\/mlflow\", line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/models\/cli.py\", line 43, in serve\n    host=host)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 76, in serve\n    command_env=command_env)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 147, in _execute_in_conda_env\n    command, rc\nException: Command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1\n(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# \n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1560855399150,
        "Question_score":0,
        "Question_tags":"python-3.x|deployment|mlflow",
        "Question_view_count":1840,
        "Owner_creation_time":1504001058090,
        "Owner_last_access_time":1630952509900,
        "Owner_location":null,
        "Owner_reputation":2101,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":41,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Following the steps mentioned in the GitHub Issue <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">1507<\/a> (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/1507<\/a>) I was able to resolve this issue.<\/p>\n\n<p>In reference to this post, the \"<strong>anaconda\/bin\/<\/strong>\" directory is never added to the list of environment variables i.e. PATH variable. In order to resolve this issue, add the \"<strong>else<\/strong>\" part of conda initialize code block from ~\/.bashrc file to your PATH variable.<\/p>\n\n<pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('\/home\/atulk\/anaconda3\/bin\/conda' 'shell.bash' 'hook' 2&gt; \/dev\/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\" ]; then\n        . \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\"\n    else\n        export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n<\/code><\/pre>\n\n<p>In this case, I added <strong>export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"<\/strong> to the PATH variable. However, this is just a temporary fix until the issue is fixed in the project.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1561730574530,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56647549",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72250896,
        "Question_title":"PowerShell Get request with body",
        "Question_body":"<p>I am trying <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#search-modelversions\" rel=\"nofollow noreferrer\">this api endpoint<\/a>.\nI can call this in python, no problem, like the below<\/p>\n<pre><code>get_model_versions={\n    &quot;filter&quot;:&quot;name='model_name'&quot;,\n    &quot;order_by&quot;:[&quot;version DESC&quot;],\n    &quot;max_results&quot;:1\n}\n\ninit_get = requests.get(&quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search&quot;,headers=header_read,json=get_model_versions)\n<\/code><\/pre>\n<p>However, I just can't seem to find a way to make it work in Powershell.<\/p>\n<p>First the powershell &quot;get&quot; Invoke-RestMethod does not accept a body<\/p>\n<p>and then I can't seem to find a way to append it in Powershell as a query string.<\/p>\n<p>I have tried (among other failed attempts), the following<\/p>\n<pre><code>$get_model_versions=([PSCustomObject]@{\n  filter = &quot;name=`'model_name`'&quot;\n  order_by = @(&quot;version desc&quot;)\n} | ConvertTo-Json)\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get -Body $get_model_versions\n<\/code><\/pre>\n<p>But that gives me an error that body can't be used with a get method<\/p>\n<p>trying to append it as a query string (like if I even just keep the name filter and remove the others), also fails<\/p>\n<pre><code>$searchuri= &quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search?filter=&quot;&quot;name==model_name&quot;&quot;&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get\n<\/code><\/pre>\n<p>fails with<\/p>\n<pre><code>{&quot;error_code&quot;:&quot;INVALID_PARAMETER_VALUE&quot;,&quot;message&quot;:&quot;Unsupported filter query : `\\&quot;name==model_name\\&quot;`. Unsupported operator.&quot;}\n<\/code><\/pre>\n<p>How can I mimic the same behaviour in Powershell, as I do in Python?<\/p>\n<p>EDIT 1: I did try to encode the query param (maybe I did it wrong), but here's how my failed attempt looked like<\/p>\n<pre><code>$encodedvalue = [System.Web.HttpUtility]::UrlEncode(&quot;`&quot;name='model_name'`&quot;&quot;)\n$searchuri= &quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search?filter=$encodedvalue&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get\n<\/code><\/pre>\n<p>But that too gives me<\/p>\n<pre><code>&quot;Unsupported filter query : `\\&quot;name='model_name'\\&quot;`. Unsupported operator.&quot;\n<\/code><\/pre>\n<p>I have also tried it successfully in Postman by passing a raw json body (the same as python) and when I look at the generated PowerShell code in Postman I see this<\/p>\n<pre><code>$headers = New-Object &quot;System.Collections.Generic.Dictionary[[String],[String]]&quot;\n$headers.Add(&quot;Authorization&quot;, &quot;Bearer token&quot;)\n$headers.Add(&quot;Content-Type&quot;, &quot;application\/json&quot;)\n\n$body = &quot;{\n`n    `&quot;filter`&quot;:`&quot;name='model_name'`&quot;,\n`n    `&quot;order_by`&quot;:[`&quot;version DESC`&quot;],\n`n    `&quot;max_results`&quot;:1\n`n}\n`n&quot;\n\n$response = Invoke-RestMethod 'baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search' -Method 'GET' -Headers $headers -Body $body\n$response | ConvertTo-Json\n<\/code><\/pre>\n<p>But of course that fails (if you copy that in an powershell editor and run it<\/p>\n<pre><code>Invoke-RestMethod : Cannot send a content-body with this verb-type\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1652637573657,
        "Question_score":1,
        "Question_tags":"powershell|rest|python-requests|mlflow",
        "Question_view_count":281,
        "Owner_creation_time":1428654714763,
        "Owner_last_access_time":1664012257383,
        "Owner_location":null,
        "Owner_reputation":596,
        "Owner_up_votes":53,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Question_last_edit_time":1652640935860,
        "Answer_body":"<p>Finally, after struggling for a long time, I found the answer !<\/p>\n<p>The crux is in the documentation <a href=\"https:\/\/docs.microsoft.com\/en-us\/powershell\/module\/microsoft.powershell.utility\/invoke-restmethod?view=powershell-7.2\" rel=\"nofollow noreferrer\">here<\/a>.\nEspecially this section<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/h0gwk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/h0gwk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So, if you want to pass on a body for your &quot;get&quot; method in powershell, pass it as a hashtable.<\/p>\n<p>So, finally the answer is<\/p>\n<pre><code>$query=@{&quot;filter&quot;=&quot;name='model_name'&quot;;&quot;order_by&quot;=@(&quot;version DESC&quot;); &quot;max_results&quot;=1};\n$searchuri=&quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get -Body $query\n<\/code><\/pre>\n<p>Hope this helps someone looking for something similar.<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1652649592320,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72250896",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60641337,
        "Question_title":"mlflow R installation MLFLOW_PYTHON_BIN",
        "Question_body":"<p>I am trying to install mlflow in R and im getting this error message saying <\/p>\n\n<blockquote>\n  <p>mlflow::install_mlflow()\n  Error in mlflow_conda_bin() :\n    Unable to find conda binary. Is Anaconda installed?\n    If you are not using conda, you can set the environment variable MLFLOW_PYTHON_BIN to the path of yourpython executable.<\/p>\n<\/blockquote>\n\n<p>I have tried the following<\/p>\n\n<pre><code>export MLFLOW_PYTHON_BIN=\"\/usr\/bin\/python\" \nsource ~\/.bashrc\necho $MLFLOW_PYTHON_BIN  -&gt; this prints the \/usr\/bin\/python.\n<\/code><\/pre>\n\n<p>or in R,<\/p>\n\n<pre><code>sys.setenv(MLFLOW_PYTHON_BIN=\"\/usr\/bin\/python\")\nsys.getenv() -&gt; prints MLFLOW_PYTHON_BIN is set to \/usr\/bin\/python.\n<\/code><\/pre>\n\n<p>however, it still does not work<\/p>\n\n<p>I do not want to use conda environment.<\/p>\n\n<p>how to I get past this error?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1583947052940,
        "Question_score":5,
        "Question_tags":"r|mlflow|system-variable",
        "Question_view_count":1141,
        "Owner_creation_time":1539211301843,
        "Owner_last_access_time":1663982305137,
        "Owner_location":null,
        "Owner_reputation":117,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Question_last_edit_time":1584403666972,
        "Answer_body":"<p>The install_mlflow command only works with conda right now, sorry about the confusing message. You can either:<\/p>\n<ul>\n<li>install conda - this is the recommended way of installing and using mlflow<\/li>\n<\/ul>\n<p>or<\/p>\n<ul>\n<li>install mlflow python package yourself via pip<\/li>\n<\/ul>\n<p>To install mlflow yourself, pip install correct (matching the the R package) python version of mlflow and set the MLFLOW_PYTHON_BIN environment variable as well as MLFLOW_BIN evn variable: e.g.<\/p>\n<pre><code>library(mlflow)\nsystem(paste(&quot;pip install -U mlflow==&quot;, mlflow:::mlflow_version(), sep=&quot;&quot;))\nSys.setenv(MLFLOW_BIN=system(&quot;which mlflow&quot;))\nSys.setenv(MLFLOW_PYTHON_BIN=system(&quot;which python&quot;))\n<\/code><\/pre>",
        "Answer_comment_count":2,
        "Answer_creation_time":1584554585177,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1624202175903,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60641337",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72886409,
        "Question_title":"MLflow proxied artifact access: Unable to locate credentials",
        "Question_body":"<p>I am using MLflow to track my experiments. I am using an S3 bucket as an artifact store. For acessing it, I want to use <em>proxied artifact access<\/em>, as described in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>, however this does not work for me, since it locally looks for credentials (but the server should handle this).<\/p>\n<h2>Expected Behaviour<\/h2>\n<p>As described in the docs, I would expect that locally, I do not need to specify my AWS credentials, since the server handles this for me. From <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n<blockquote>\n<p>This eliminates the need to allow end users to have direct path access to a remote object store (e.g., s3, adls, gcs, hdfs) for artifact handling and eliminates the need for an end-user to provide access credentials to interact with an underlying object store.<\/p>\n<\/blockquote>\n<h2>Actual Behaviour \/ Error<\/h2>\n<p>Whenever I run an experiment on my machine, I am running into the following error:<\/p>\n<p><code>botocore.exceptions.NoCredentialsError: Unable to locate credentials<\/code><\/p>\n<p>So the error is local. However, this should not happen since the server should handle the auth instead of me needing to store my credentials locally. Also, I would expect that I would not even need library <code>boto3<\/code> locally.<\/p>\n<h2>Solutions Tried<\/h2>\n<p>I am aware that I need to create a new experiment, because existing experiments might still use a different artifact location which is proposed in <a href=\"https:\/\/stackoverflow.com\/a\/71417933\/10465165\">this SO answer<\/a> as well as in the note in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>. Creating a new experiment did not solve the error for me. Whenever I run the experiment, I get an explicit log in the console validating this:<\/p>\n<p><code>INFO mlflow.tracking.fluent: Experiment with name 'test' does not exist. Creating a new experiment.<\/code><\/p>\n<p>Related Questions (<a href=\"https:\/\/stackoverflow.com\/questions\/72206086\/cant-log-mlflow-artifacts-to-s3-with-docker-based-tracking-server\">#1<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/72236258\/mlflow-unable-to-store-artifacts-to-s3\/72261826#comment128726676_72261826\">#2<\/a>) refer to a different scenario, which is also <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-4-mlflow-with-remote-tracking-server-backend-and-artifact-stores\" rel=\"nofollow noreferrer\">described in the docs<\/a><\/p>\n<h2>Server Config<\/h2>\n<p>The server runs on a kubernetes pod with the following config:<\/p>\n<pre><code>mlflow server \\\n    --host 0.0.0.0 \\\n    --port 5000 \\\n    --backend-store-uri postgresql:\/\/user:pw@endpoint \\\n    --artifacts-destination s3:\/\/my_bucket\/artifacts \\\n    --serve-artifacts \\\n    --default-artifact-root s3:\/\/my_bucket\/artifacts \\\n<\/code><\/pre>\n<p>I would expect my config to be correct, looking at doc <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">page 1<\/a> and <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#using-the-tracking-server-for-proxied-artifact-access\" rel=\"nofollow noreferrer\">page 2<\/a><\/p>\n<p>I am able to see the mlflow UI if I forward the port to my local machine. I also see the experiment runs as failed, because of the error I sent above.<\/p>\n<h2>My Code<\/h2>\n<p>The relevant part of my code which fails is the logging of the model:<\/p>\n<pre><code>mlflow.set_tracking_uri(&quot;http:\/\/localhost:5000&quot;)\nmlflow.set_experiment(&quot;test2)\n\n...\n\n# this works\nmlflow.log_params(hyperparameters)\n                        \nmodel = self._train(model_name, hyperparameters, X_train, y_train)\ny_pred = model.predict(X_test)\nself._evaluate(y_test, y_pred)\n\n# this fails with the error from above\nmlflow.sklearn.log_model(model, &quot;artifacts&quot;)\n\n<\/code><\/pre>\n<h2>Question<\/h2>\n<p>I am probably overlooking something. Is there a need to locally indicate that I want to use proxied artified access? If yes, how do I do this? Is there something I have missed?<\/p>\n<h2>Full Traceback<\/h2>\n<pre><code>  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/models\/model.py&quot;, line 295, in log\n    mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 726, in log_artifacts\n    MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py&quot;, line 1001, in log_artifacts\n    self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 346, in log_artifacts\n    self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/store\/artifact\/s3_artifact_repo.py&quot;, line 141, in log_artifacts\n    self._upload_file(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/store\/artifact\/s3_artifact_repo.py&quot;, line 117, in _upload_file\n    s3_client.upload_file(Filename=local_file, Bucket=bucket, Key=key, ExtraArgs=extra_args)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/boto3\/s3\/inject.py&quot;, line 143, in upload_file\n    return transfer.upload_file(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/boto3\/s3\/transfer.py&quot;, line 288, in upload_file\n    future.result()\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/futures.py&quot;, line 103, in result\n    return self._coordinator.result()\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/futures.py&quot;, line 266, in result\n    raise self._exception\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/tasks.py&quot;, line 139, in __call__\n    return self._execute_main(kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/tasks.py&quot;, line 162, in _execute_main\n    return_value = self._main(**kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/upload.py&quot;, line 758, in _main\n    client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 508, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 898, in _make_api_call\n    http, parsed_response = self._make_request(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 921, in _make_request\n    return self._endpoint.make_request(operation_model, request_dict)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 119, in make_request\n    return self._send_request(request_dict, operation_model)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 198, in _send_request\n    request = self.create_request(request_dict, operation_model)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 134, in create_request\n    self._event_emitter.emit(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 412, in emit\n    return self._emitter.emit(aliased_event_name, **kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 256, in emit\n    return self._emit(event_name, kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 239, in _emit\n    response = handler(**kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/signers.py&quot;, line 103, in handler\n    return self.sign(operation_name, request)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/signers.py&quot;, line 187, in sign\n    auth.add_auth(request)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/auth.py&quot;, line 407, in add_auth\n    raise NoCredentialsError()\nbotocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1657122030593,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|amazon-s3|boto3|mlflow",
        "Question_view_count":237,
        "Owner_creation_time":1538816771613,
        "Owner_last_access_time":1663856763733,
        "Owner_location":"Berlin",
        "Owner_reputation":647,
        "Owner_up_votes":971,
        "Owner_down_votes":39,
        "Owner_views":61,
        "Question_last_edit_time":1657122771947,
        "Answer_body":"<p>The problem is that the server is running on wrong run parameters, the <code>--default-artifact-root<\/code> needs to either be removed or set to <code>mlflow-artifacts:\/<\/code>.<\/p>\n<p>From <code>mlflow server --help<\/code>:<\/p>\n<pre><code>  --default-artifact-root URI  Directory in which to store artifacts for any\n                               new experiments created. For tracking server\n                               backends that rely on SQL, this option is\n                               required in order to store artifacts. Note that\n                               this flag does not impact already-created\n                               experiments with any previous configuration of\n                               an MLflow server instance. By default, data\n                               will be logged to the mlflow-artifacts:\/ uri\n                               proxy if the --serve-artifacts option is\n                               enabled. Otherwise, the default location will\n                               be .\/mlruns.\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1657186814370,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72886409",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56967364,
        "Question_title":"Keep track of all the parameters of spark-submit",
        "Question_body":"<p>I have a team where many member has permission to submit Spark tasks to YARN (the resource management) by command line. It's hard to track who is using how much cores, who is using how much memory...e.g. Now I'm looking for a software, framework or something could help me monitor the parameters that each member used. It will be a bridge between client and YARN. Then I could used it to filter the submit commands.<\/p>\n\n<p>I did take a look at <a href=\"http:\/\/www.mlflow.org\" rel=\"nofollow noreferrer\">mlflow<\/a> and I really like the MLFlow Tracking but it was designed for ML training process. I wonder if there is an alternative for my purpose? Or there is any other solution for the problem.<\/p>\n\n<p>Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1562750084187,
        "Question_score":0,
        "Question_tags":"apache-spark|parameters|hadoop-yarn|spark-submit|mlflow",
        "Question_view_count":93,
        "Owner_creation_time":1413431014113,
        "Owner_last_access_time":1566529894493,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>My recommendation would be to build such a tool yourself as its not too complicated,\nhave a wrapper script to spark submit which logs the usage in a DB and after the spark job finishes the wrapper will know to release information. could be done really easily.\nIn addition you can even block new spark submits if your team already asked for too much information.<\/p>\n\n<p>And as you build it your self its really flexible as you can even create \"sub teams\" or anything you want.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1562766864887,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56967364",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58600732,
        "Question_title":"Is there a way to hide mlflow ui header when start the server with mlflow server?",
        "Question_body":"<p>I want to integrate mlflow ui to our website by using an iframe, but with the header hidden if possible. I found there is an environment variable setting in the source code \/mlflow\/server\/js\/components\/HomeView.js:\n<code>const headerHeight = process.env.HIDE_HEADER === 'true' ? 0 : 60;<\/code> But how can I specify this environment by running the server with <code>mlflow server<\/code>? I tried with <code>HIDE_HEADER=true mlflow server<\/code>, but this doesn't work. Or is there any other way to solve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1572316204850,
        "Question_score":0,
        "Question_tags":"node.js|reactjs|mlflow",
        "Question_view_count":296,
        "Owner_creation_time":1441151447410,
        "Owner_last_access_time":1663903618597,
        "Owner_location":null,
        "Owner_reputation":626,
        "Owner_up_votes":75,
        "Owner_down_votes":1,
        "Owner_views":79,
        "Question_last_edit_time":null,
        "Answer_body":"<p>@Jason good question, those environment variables are read at build-time for the MLflow UI's Javascript assets. Since the PyPI MLflow wheel comes with pre-built Javascript assets, it's difficult to achieve your use case using a PyPI installation of <code>mlflow<\/code>.<\/p>\n\n<p>However, you can build a custom MLflow wheel from source with the UI header hidden by following the instructions <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/branch-1.3\/CONTRIBUTING.rst#building-a-distributable-artifact\" rel=\"nofollow noreferrer\">here<\/a>, replacing the <code>npm run build<\/code> step with <code>HIDE_HEADER=true npm run build<\/code> (basically, the idea is to set the desired environment variables prior to building Javascript assets via <code>npm run build<\/code>). You can then pip-install that wheel on the node hosting your MLflow server &amp; launch the server via <code>mlflow server<\/code>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1572468752163,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58600732",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58541794,
        "Question_title":"MLflow saving weights after each epoch",
        "Question_body":"<p>I have been testing some small examples with MLflow tracking but for my usecase I would like to have the weights saved after each epoch. \nSometimes I kill the runs before they are completely finished (I cannot use earlystopping), but what I experience now is that the weights do not get saved to the tracking ui server.\nIs there a way to do this after each epoch?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1571921401100,
        "Question_score":1,
        "Question_tags":"keras|mlflow",
        "Question_view_count":1121,
        "Owner_creation_time":1534367348473,
        "Owner_last_access_time":1663928245177,
        "Owner_location":"Leuven, Belgium",
        "Owner_reputation":344,
        "Owner_up_votes":74,
        "Owner_down_votes":2,
        "Owner_views":35,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Save the weights to disk and then log them as an artifact.  As long as the checkpoints\/weights are saved to disk, you can log them with <code>mlflow_log_artifact()<\/code> or <code>mlflow_log_artifacts()<\/code>.  From the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#logging-functions\" rel=\"nofollow noreferrer\">docs<\/a>,<\/p>\n\n<blockquote>\n  <p><strong>mlflow.log_artifact()<\/strong> logs a local file or directory as an artifact,\n  optionally taking an artifact_path to place it in within the run\u2019s\n  artifact URI. Run artifacts can be organized into directories, so you\n  can place the artifact in a directory this way.<\/p>\n  \n  <p><strong>mlflow.log_artifacts()<\/strong> logs all the files in a given directory as\n  artifacts, again taking an optional artifact_path.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0,
        "Answer_creation_time":1572525493020,
        "Answer_score":1,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58541794",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63973530,
        "Question_title":"Convert an instance of xgboost.Booster into a model that implements the scikit-learn API",
        "Question_body":"<p>I am trying to use <code>mlflow<\/code> to save a model and then load it later to make predictions.<\/p>\n<p>I'm using a <code>xgboost.XGBRegressor<\/code> model and its sklearn functions <code>.predict()<\/code> and <code>.predict_proba()<\/code> to make predictions but it turns out that <code>mlflow<\/code> doesn't support models that implements the sklearn API, so when loading the model later from mlflow, mlflow returns an instance of <code>xgboost.Booster<\/code>, and it doesn't implements the <code>.predict()<\/code> or <code>.predict_proba()<\/code> functions.<\/p>\n<p>Is there a way to convert a <code>xgboost.Booster<\/code> back into a <code>xgboost.sklearn.XGBRegressor<\/code> object that implements the sklearn API functions?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1600550858547,
        "Question_score":3,
        "Question_tags":"scikit-learn|save|xgboost|mlflow|xgbclassifier",
        "Question_view_count":1317,
        "Owner_creation_time":1592264086427,
        "Owner_last_access_time":1663962090790,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Have you tried wrapping up your model in custom class, logging and loading it using <code>mlflow.pyfunc.PythonModel<\/code>?\nI put up a simple example and upon loading back the model it correctly shows <code>&lt;class 'xgboost.sklearn.XGBRegressor'&gt;<\/code> as a type.<\/p>\n<p>Example:<\/p>\n<pre><code>import xgboost as xgb\nxg_reg = xgb.XGBRegressor(...)\n\nclass CustomModel(mlflow.pyfunc.PythonModel):\n    def __init__(self, xgbRegressor):\n        self.xgbRegressor = xgbRegressor\n\n    def predict(self, context, input_data):\n        print(type(self.xgbRegressor))\n        \n        return self.xgbRegressor.predict(input_data)\n\n# Log model to local directory\nwith mlflow.start_run():\n     custom_model = CustomModel(xg_reg)\n     mlflow.pyfunc.log_model(&quot;custome_model&quot;, python_model=custom_model)\n\n\n# Load model back\nfrom mlflow.pyfunc import load_model\nmodel = load_model(&quot;\/mlruns\/0\/..\/artifacts\/custome_model&quot;)\nmodel.predict(X_test)\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>&lt;class 'xgboost.sklearn.XGBRegressor'&gt;\n[ 9.107417 ]\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1600607182583,
        "Answer_score":4,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63973530",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59642900,
        "Question_title":"Unable to connect Mlflow server to my mlflow project image",
        "Question_body":"<p>My final purpose is to run experiment from  an Api.<\/p>\n\n<p>the experiment come from :\n<a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/tensorflow\/tf2\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/tensorflow\/tf2<\/a>\nbut export the file in my custom git where I clone it, in the image below -><\/p>\n\n<p>I have 2 images in my docker compose :\ntree project : <\/p>\n\n<pre><code>|_app\/\n| |_Dockerfile\n|\n|_mlflow\/\n| |_Dockerfile\n|\n|_docker-compose.yml\n\n<\/code><\/pre>\n\n<p>app\/Dockerfile<\/p>\n\n<pre><code>FROM continuumio\/anaconda3\n\nENV APP_HOME .\/\nWORKDIR ${APP_HOME}\nRUN conda config --append channels conda-forge\nRUN conda install --quiet --yes \\\n    'mlflow' \\\n    'psycopg2' \\\n    'tensorflow'\nRUN pip install pylint\nRUN pwd;ls \\\n&amp;&amp; git clone https:\/\/github.com\/MChrys\/QuickSign.git \nRUN pwd;ls \\\n    &amp;&amp; cd QuickSign \\\n    &amp;&amp; pwd;ls\n\nCOPY . .\n\n#RUN conda install jupyter \n#CMD jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root --no-browser\nCMD cd QuickSign &amp;&amp; mlflow run .\n<\/code><\/pre>\n\n<p>mlflow\/Dockerfile<\/p>\n\n<pre><code>FROM python:3.7.0\n\nRUN pip install mlflow\n\nRUN mkdir \/mlflow\/\n\nCMD mlflow server \\\n    --backend-store-uri \/mlflow \\\n    --host 0.0.0.0\n<\/code><\/pre>\n\n<p>docker-compose.yml<\/p>\n\n<pre><code>version: '3'\nservices:\n  notebook:\n    build:\n      context: .\/app\n    ports:\n      - \"8888:8888\"\n    depends_on: \n      - mlflow\n    environment: \n      MLFLOW_TRACKING_URI: 'http:\/\/mlflow:5000'\n  mlflow:\n    build:\n      context: .\/mlflow\n    expose: \n      - \"5000\"\n    ports:\n      - \"5000:5000\"\n<\/code><\/pre>\n\n<p>when I <code>docker-compose up<\/code> the image I obtain  :<\/p>\n\n<pre><code>notebook_1_74059cdc20ce |     response = requests.request(**kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/api.py\", line 60, in request\nnotebook_1_74059cdc20ce |     return session.request(method=method, url=url, **kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 533, in request\nnotebook_1_74059cdc20ce |     resp = self.send(prep, **send_kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 646, in send\nnotebook_1_74059cdc20ce |     r = adapter.send(request, **kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/adapters.py\", line 516, in send\nnotebook_1_74059cdc20ce |     raise ConnectionError(e, request=request)\nnotebook_1_74059cdc20ce | requests.exceptions.ConnectionError: HTTPConnectionPool(host='mlflow', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/create (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7fd5db4edc50&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n\n<\/code><\/pre>\n\n<p>The problem look like that I run a project which is not found in the server images, as I run it in the app image, but I don't know how figure it out I have to trigger  the experiment from a futur flask app <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1578475698630,
        "Question_score":0,
        "Question_tags":"docker|docker-compose|mlflow",
        "Question_view_count":2937,
        "Owner_creation_time":1459625723203,
        "Owner_last_access_time":1648342504593,
        "Owner_location":"Grenoble, France",
        "Owner_reputation":56,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Question_last_edit_time":1578477326563,
        "Answer_body":"<p>The problem came from  docker for windows, I was unable to make working docker compose on it but there are no problem to build it when I run it on virtual machine with ubuntu.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1586436837590,
        "Answer_score":0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59642900",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56984854,
        "Question_title":"fcntl error with \u201cmlflow ui\u201d on windows - mlflow 1.0",
        "Question_body":"<p>I am getting the following error message when trying mlflow examples and running 'mlflow ui'.<\/p>\n\n<p><strong>Error:<\/strong><\/p>\n\n<blockquote>\n  <p>ModuleNotFoundError: No module named 'fcntl' Running the mlflow server\n  failed. Please see the logs above for details<\/p>\n<\/blockquote>\n\n<p>Is anyone aware of a solution to this issue?<\/p>\n\n<p>I have tried the solutions suggested at <a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/1080\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/pull\/1080<\/a><\/p>\n\n<p>without success. Replacing the modified files in mlflow source code, it raises other issues for not finding what it is looking for with the following:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\thesis_mlflow\\Scripts\\mlflow.exe\\__main__.py\", line 9, in &lt;module&gt;\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\mlflow\\cli.py\", line 198, in ui\n    _run_server(backend_store_uri, default_artifact_root, \"127.0.0.1\", port, None, 1)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\mlflow\\server\\__init__.py\", line 90, in _run_server\n    exec_cmd(full_command, env=env_map, stream_output=True)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\mlflow\\utils\\process.py\", line 34, in exec_cmd\n    stdin=subprocess.PIPE, **kwargs)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\subprocess.py\", line 729, in __init__\n    restore_signals, start_new_session)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\subprocess.py\", line 1017, in _execute_child\n    startupinfo)\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1562833873410,
        "Question_score":1,
        "Question_tags":"python|windows|mlflow",
        "Question_view_count":725,
        "Owner_creation_time":1529408888483,
        "Owner_last_access_time":1649930832187,
        "Owner_location":"London, UK",
        "Owner_reputation":79,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1562909693510,
        "Answer_body":"<p>Just solved the issue: for some reason, waitress was not installed in the running environment. After installing it, everything seems working fine with the solution #1080 linked above in the question.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1562919127687,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56984854",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61258979,
        "Question_title":"EKS Docker Image Pull CrashLoopBackOff",
        "Question_body":"<p>I'm trying to deploy a Docker image from ECR to my EKS. When attempting to deploy my docker image to a pod, I get the following events from a CrashLoopBackOff:<\/p>\n\n<pre><code>Events:\n  Type     Reason                  Age               From                                   Message\n  ----     ------                  ----              ----                                   -------\n  Normal   Scheduled               62s               default-scheduler                      Successfully assigned default\/mlflow-tracking-server to &lt;EC2 IP&gt;.internal\n  Normal   SuccessfulAttachVolume  60s               attachdetach-controller                AttachVolume.Attach succeeded for volume \"&lt;PVC&gt;\"\n  Normal   Pulling                 56s               kubelet, &lt;IP&gt;.ec2.internal             Pulling image \"&lt;ECR Image UI&gt;\"\n  Normal   Pulled                  56s               kubelet, &lt;IP&gt;.ec2.internal             Successfully pulled image \"&lt;ECR Image UI&gt;\"\n  Normal   Created                 7s (x4 over 56s)  kubelet, &lt;IP&gt;.ec2.internal             Created container mlflow-tracking-server\n  Normal   Pulled                  7s (x3 over 54s)  kubelet, &lt;IP&gt;.ec2.internal             Container image \"&lt;ECR Image UI&gt;\" already present on machine\n  Normal   Started                 6s (x4 over 56s)  kubelet, &lt;IP&gt;.ec2.internal             Started container mlflow-tracking-server\n  Warning  BackOff                 4s (x5 over 52s)  kubelet, &lt;IP&gt;.ec2.internal             Back-off restarting failed container\n<\/code><\/pre>\n\n<p>I don't understand why it keeps looping like this and failing. Would anyone know why this is happening?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1587067011163,
        "Question_score":1,
        "Question_tags":"docker|amazon-eks|mlflow",
        "Question_view_count":604,
        "Owner_creation_time":1417012835813,
        "Owner_last_access_time":1628782664573,
        "Owner_location":null,
        "Owner_reputation":945,
        "Owner_up_votes":35,
        "Owner_down_votes":1,
        "Owner_views":148,
        "Question_last_edit_time":null,
        "Answer_body":"<p><code>CrashLoopBackError<\/code> can be related to these possible reasons:<\/p>\n\n<ul>\n<li><p>the application inside your pod is not starting due to an error;<\/p><\/li>\n<li><p>the image your pod is based on is not present in the registry, or the\nnode where your pod has been scheduled cannot pull from the registry;<\/p><\/li>\n<li><p>some parameters of the pod has not been configured correctly.<\/p><\/li>\n<\/ul>\n\n<p>In your case it seems an application error, inside the container.\nTry to view the logs with:<\/p>\n\n<pre><code>kubectl logs &lt;your_pod&gt; -n &lt;namespace&gt;\n<\/code><\/pre>\n\n<p>For more info on how to troubleshoot this kind of error refer to:<\/p>\n\n<p><a href=\"https:\/\/pillsfromtheweb.blogspot.com\/2020\/05\/troubleshooting-kubernetes.html\" rel=\"nofollow noreferrer\">https:\/\/pillsfromtheweb.blogspot.com\/2020\/05\/troubleshooting-kubernetes.html<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1588674359533,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61258979",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69479488,
        "Question_title":"Hi. I am very new to MLFlow, and want to implement MLFlow project on my own ML model. However I am getting \"\"Could not find main among entry points\"\"",
        "Question_body":"<p>The full error message is as below:<\/p>\n<pre><code>ERROR mlflow.cli: === Could not find main among entry points [] or interpret main as a runnable script. Supported script file extensions: ['.py', '.sh'] ===\n\n<\/code><\/pre>\n<p>I also try the solutions suggested here <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code>, but the result is the same.<\/p>\n<p>Below I provide all the required files to run <code>MLflow<\/code> project.<\/p>\n<p>The <code>conda.yaml<\/code> file<\/p>\n<pre><code>name: lightgbm-example\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.6\n  - pip\n  - pip:\n      - mlflow&gt;=1.6.0\n      - lightgbm\n      - pandas\n      - numpy\n<\/code><\/pre>\n<p>The MLProject file<\/p>\n<pre><code>name: lightgbm-example\nconda_env: ~\/Desktop\/MLflow\/conda.yaml\nentry-points:\n    main:\n      parameters:\n        learning_rate: {type: float, default: 0.1}\n        colsample_bytree: {type: float, default: 1.0}\n        subsample: {type: float, default: 1.0} \n      command: |\n          python3 ~\/Desktop\/MLflow\/Test.py \\\n            --learning-rate={learning_rate} \\\n            --colsample-bytree={colsample_bytree} \\\n            --subsample={subsample}\n<\/code><\/pre>\n<p>My Test.py file<\/p>\n<pre><code>import pandas as pd\nimport lightgbm as lgb\nimport numpy as np\nimport mlflow\nimport mlflow.lightgbm\nimport argparse\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=&quot;LightGBM example&quot;)\n    parser.add_argument(\n        &quot;--learning-rate&quot;,\n        type=float,\n        default=0.1,\n        help=&quot;learning rate to update step size at each boosting step (default: 0.3)&quot;,\n    )\n    parser.add_argument(\n        &quot;--colsample-bytree&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of columns when constructing each tree (default: 1.0)&quot;,\n    )\n    parser.add_argument(\n        &quot;--subsample&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of the training instances (default: 1.0)&quot;,\n    )\n    return parser.parse_args()\n\ndef find_specificity(c_matrix):\n    specificity = c_matrix[1][1]\/(c_matrix[1][1]+c_matrix[0][1])\n    return specificity\n    \n    \ndef main():\n\n    args = parse_args()\n\n    df = pd.read_csv('~\/Desktop\/MLflow\/Churn_demo.csv')\n    train_df = df.sample(frac=0.8, random_state=25)\n    test_df = df.drop(train_df.index)\n\n\n        \n    train_df.drop(['subscriberid'], axis = 1, inplace = True)\n    test_df.drop(['subscriberid'], axis = 1, inplace = True)\n\n    TrainX = train_df.iloc[:,:-1]\n    TrainY = train_df.iloc[:,-1]\n\n    TestX = test_df.iloc[:,:-1]\n    TestY = test_df.iloc[:,-1]\n    \n    mlflow.lightgbm.autolog()\n    \n    dtrain = lgb.Dataset(TrainX, label=TrainY)\n    dtest = lgb.Dataset(TestX, label=TestY)\n    \n    with mlflow.start_run():\n\n        parameters = {\n            'objective': 'binary',\n            'device':'cpu',\n            'num_threads': 6,\n            'num_leaves': 127,\n            'metric' : 'binary',\n            'lambda_l2':5,\n            'max_bin': 63,\n            'bin_construct_sample_cnt' :2*1000*1000,\n            'learning_rate': args.learning_rate,\n            'colsample_bytree': args.colsample_bytree,\n            'subsample': args.subsample,\n            'verbose': 1\n        }\n\n\n\n        model = lgb.train(parameters,\n                       dtrain,\n                       valid_sets=dtest,\n                       num_boost_round=10000,\n                       early_stopping_rounds=10)\n                       \n               \n        y_proba=model.predict(TestX)\n        pred=np.where(y_proba&gt;0.25,1,0) \n        conf_matrix = confusion_matrix(TestY,pred)\n        \n        specificity = find_specificity(conf_matrix)\n        acc = accuracy_score(TestY,pred)\n        \n        mlflow.log_metric({&quot;specificity&quot; : specificity, &quot;accuracy&quot; : acc})\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n        \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633602279323,
        "Question_score":2,
        "Question_tags":"python|yaml|mlflow",
        "Question_view_count":418,
        "Owner_creation_time":1583491811220,
        "Owner_last_access_time":1663774319043,
        "Owner_location":"Baku, Azerbaijan",
        "Owner_reputation":23,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Fortunately, I have been resolved my problem. I list some solutions for the same error which can help you in the future if you face the same problem.<\/p>\n<ol>\n<li>File names. The file names should be the same suggested in MLFlow docs <code>https:\/\/mlflow.org\/ <\/code>. For example not <code>conda.yamp<\/code>, but <code>conda.yaml<\/code>, as there was such problem in <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/3856<\/code><\/li>\n<li>The <code>conda.yaml<\/code> file does not support Tab, please consider using spaces instead<\/li>\n<li>In the MLProject file name 'P' should be the upper case before MLFlow 1.4. But the later versions it does not matter as explained there <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code><\/li>\n<li>(In my case) MLProject file is space sensitive. Let the <code> https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples<\/code> GitHub examples guide you.<\/li>\n<\/ol>",
        "Answer_comment_count":0,
        "Answer_creation_time":1633946464143,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69479488",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70539698,
        "Question_title":"MlFlow - Unable to run with S3 as default-artifact-root",
        "Question_body":"<p>I am trying to store my model artifacts using mlflow to s3. In the API services, we use <code>MLFLOW_S3_ENDPOINT_URL<\/code> as the s3 bucket. In the mlflow service, we pass it as an environment variable. But, the mlflow container servicer fails with the below exception:<\/p>\n<pre><code>mflow_server  | botocore.exceptions.HTTPClientError: An HTTP Client raised an unhandled exception: Not supported URL scheme s3\n<\/code><\/pre>\n<p>docker-compose file as below:<\/p>\n<pre><code>version: &quot;3.3&quot;\nservices:\n  prisim-api:\n    image: prisim-api:latest\n    container_name: prisim-api\n    expose:\n      - &quot;8000&quot;\n    environment: \n    - S3_URL=s3:\/\/mlflow-automation-artifacts\/\n    - MLFLOW_SERVER=http:\/\/mlflow:5000\n    - AWS_ID=xyz+\n    - AWS_KEY=xyz\n\n    networks:\n      - prisim \n    depends_on:\n      - mlflow\n    links:\n            - mlflow\n    volumes:\n      - app_data:\/usr\/data\n  mlflow:\n    image: mlflow_server:latest\n    container_name: mflow_server\n    ports:\n      - &quot;5000:5000&quot;    \n    environment:\n      - AWS_ACCESS_KEY_ID=xyz+\n      - AWS_SECRET_ACCESS_KEY=xyz\n      - MLFLOW_S3_ENDPOINT_URL=s3:\/\/mlflow-automation-artifacts\/\n    healthcheck:\n      test: [&quot;CMD&quot;, &quot;echo&quot;, &quot;mlflow server is running&quot;]\n      interval: 1m30s\n      timeout: 10s\n      retries: 3\n    networks:\n       - prisim \nnetworks:\n prisim:\nvolumes:\n  app_data:\n<\/code><\/pre>\n<p>Why the scheme s3 is not supported?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1640933189943,
        "Question_score":1,
        "Question_tags":"amazon-s3|docker-compose|mlflow",
        "Question_view_count":932,
        "Owner_creation_time":1310893185210,
        "Owner_last_access_time":1663988189020,
        "Owner_location":"Thiruvananthapuram, Kerala, India",
        "Owner_reputation":2763,
        "Owner_up_votes":373,
        "Owner_down_votes":7,
        "Owner_views":851,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I found the solution.<\/p>\n<p>I have added <code>[&quot;AWS_DEFAULT_REGION&quot;]<\/code> to the environment variables and it worked.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1641275216080,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70539698",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59856641,
        "Question_title":"How can I throw an exception from within an MLflow project?",
        "Question_body":"<p>I have an Mlflow project that raises an exception. I execute that function using <code>mlflow.run<\/code>, but I get <code>mlflow.exceptions.ExecutionException(\"Run (ID '&lt;run_id&gt;') failed\")<\/code>. <\/p>\n\n<p>Is there any way I could get the exception that is being raised where I am executing <code>mlflow.run<\/code>? <\/p>\n\n<p>Or is it possible to send an <code>mlflow.exceptions.ExecutionException<\/code> with custom message set from within the project?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579686060327,
        "Question_score":0,
        "Question_tags":"python|exception|mlflow",
        "Question_view_count":428,
        "Owner_creation_time":1472932425400,
        "Owner_last_access_time":1623748857057,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Unfortunately not at the moment. mlflow run starts a new process and there is no protocol for exception passing right now. In general the other project does not even have to be in the same language. <\/p>\n\n<p>One workaround I can think of is to pass the exception via mlflow by setting run tag. E.g.:<\/p>\n\n<pre><code>try:\n    ...\nexcept Exception as ex:\n    mlflow.set_tag(\"exception\", str(ex))\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1579719419647,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1579728648287,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59856641",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60616430,
        "Question_title":"MLflow: how to read metrics or params from an existing run?",
        "Question_body":"<p>I try to read metrics in this way:<\/p>\n\n<pre><code> data, info = mlflow.get_run(run_id)\n print(data[1].metrics)\n # example of output: {'loss': 0.01}\n<\/code><\/pre>\n\n<p>But it get only last value. It is possible to read manually all steps of a particular metric?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":2,
        "Question_creation_time":1583838649443,
        "Question_score":3,
        "Question_tags":"python|metrics|mlflow",
        "Question_view_count":2995,
        "Owner_creation_time":1547467399037,
        "Owner_last_access_time":1663856296190,
        "Owner_location":"Busto Arsizio, VA, Italia",
        "Owner_reputation":171,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Question_last_edit_time":1647258268603,
        "Answer_body":"<p>I ran into this same problem and was able to do get all of the values for the metric by using using <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_metric_history\" rel=\"noreferrer\"><code>mlflow.tracking.MlflowClient().get_metric_history<\/code><\/a>. This will return every value you logged using <code>mlflow.log_metric(key, value)<\/code>.<\/p>\n<p>Quick example (untested)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\ntrackingDir = 'file:\/\/\/....'\nregistryDir = 'file:\/\/\/...'\nrunID = 'my run id'\nmetricKey = 'loss'\n\nclient = mlflow.tracking.MlflowClient(\n            tracking_uri=trackingDir,\n            registry_uri=registryDir,\n        )\n\nmetrics = client.get_metric_history(runID, metricKey)\n<\/code><\/pre>\n<p><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_metric_history\" rel=\"noreferrer\">From the docs<\/a><\/p>\n<blockquote>\n<p>get_metric_history(run_id, key)[source] Return a list of metric\nobjects corresponding to all values logged for a given metric.<\/p>\n<p>Parameters run_id \u2013 Unique identifier for run<\/p>\n<p>key \u2013 Metric name within the run<\/p>\n<p>Returns A list of mlflow.entities.Metric entities if logged, else\nempty list<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from mlflow.tracking import MlflowClient\n\ndef print_metric_info(history):\n    for m in history:\n        print(&quot;name: {}&quot;.format(m.key))\n        print(&quot;value: {}&quot;.format(m.value))\n        print(&quot;step: {}&quot;.format(m.step))\n        print(&quot;timestamp: {}&quot;.format(m.timestamp))\n        print(&quot;--&quot;)\n\n# Create a run under the default experiment (whose id is &quot;0&quot;). Since this is low-level\n# CRUD operation, the method will create a run. To end the run, you'll have\n# to explicitly end it. \nclient = MlflowClient() \nexperiment_id = &quot;0&quot; \nrun = client.create_run(experiment_id) \nprint(&quot;run_id:{}&quot;.format(run.info.run_id))\nprint(&quot;--&quot;)\n\n# Log couple of metrics, update their initial value, and fetch each\n# logged metrics' history. \nfor k, v in [(&quot;m1&quot;, 1.5), (&quot;m2&quot;, 2.5)]:\n    client.log_metric(run.info.run_id, k, v, step=0)\n    client.log_metric(run.info.run_id, k, v + 1, step=1)\n    print_metric_info(client.get_metric_history(run.info.run_id, k))\nclient.set_terminated(run.info.run_id) \n<\/code><\/pre>\n<\/blockquote>",
        "Answer_comment_count":0,
        "Answer_creation_time":1606884707693,
        "Answer_score":5,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60616430",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59464404,
        "Question_title":"COPY files - next to Dockerfile - don't work and block docker build",
        "Question_body":"<p>I'm trying to building a docker container with mlflow server inside, with poetry toml file for dependency.(the two toml are exactly the same, it was just a way to try  to figure out)<br>\ntree:<\/p>\n\n<p>\u251c\u2500\u2500 docker-entrypoint.sh <br>\n\u251c\u2500\u2500 Dockerfile<br>\n\u251c\u2500\u2500 files<br>\n\u2502   \u2514\u2500\u2500 pyproject.toml<br>\n\u251c\u2500\u2500 git.sh<br>\n\u251c\u2500\u2500 pyproject.toml<br>\n\u2514\u2500\u2500 README.md<br><\/p>\n\n<p>as you can see, my toml file is next to Dockerfile <code>COPY pyproject.toml .\/<\/code> don't work nevertheless<\/p>\n\n<p><strong>Dockerfile<\/strong><\/p>\n\n<pre><code>FROM python:3.6.10-alpine3.10 as base\nLABEL maintainer=\"\"\n\nENV PYTHONFAULTHANDLER 1 \nENV    PYTHONHASHSEED random \nENV    PYTHONUNBUFFERED 1\n\nENV MLFLOW_HOME .\/ \nENV SERVER_PORT 5000   \nENV    MLFLOW_VERSION 0.7.0 \nENV    SERVER_HOST 0.0.0.0  \nENV    FILE_STORE ${MLFLOW_HOME}\/fileStore  \nENV    ARTIFACT_STORE ${MLFLOW_HOME}\/artifactStore \nENV PIP_DEFAULT_TIMEOUT 100\nENV    PIP_DISABLE_PIP_VERSION_CHECK on\nENV    PIP_NO_CACHE_DIR  off \nENV    POETRY_VERSION  1.0.0 \n\nWORKDIR ${MLFLOW_HOME}\n\nFROM base as builder\n\nRUN apk update  \\\n    &amp;&amp; apk add --no-cache make gcc musl-dev python3-dev libffi-dev openssl-dev subversion\n#download project file from github  repo \nRUN    svn export https:\/\/github.com\/MChrys\/QuickSign\/trunk\/  \\\n    &amp;&amp; pip install poetry==${POETRY_VERSION} \\\n    &amp;&amp; mkdir -p ${FILE_STORE}  \\\n    &amp;&amp; mkdir -p ${ARTIFACT_STORE}\\\n    &amp;&amp; python -m venv \/venv\n\nCOPY  pyproject.toml .\/\nRUN poetry export -f requirements.txt | \/venv\/bin\/pip install -r  --allow-root-install \/dev\/stdin \n\nCOPY . .\nRUN poetry build &amp;&amp; \/venv\/bin\/pip install dist\/*.whl\n\nFROM base as final\n\nRUN apk add --no-cache libffi libpq\nCOPY --from=builder \/venv \/venv\nCOPY docker-entrypoint.sh .\/\n\nEXPOSE $SERVER_PORT\n\nVOLUME [\"${FILE_STORE}\", \"${ARTIFACT_STORE}\"]\n\nCMD [\".\/docker-entrypoint.sh\"]\n<\/code><\/pre>\n\n<p>the build command :<\/p>\n\n<pre><code>docker build - &lt; Dockerfile\n<\/code><\/pre>\n\n<p>I get this error  :<\/p>\n\n<pre><code>Step 21\/32 : COPY  pyproject.toml .\/\nCOPY failed: stat \/var\/lib\/docker\/tmp\/docker-builder335195979\/pyproject.toml: no such file or   directory\n<\/code><\/pre>\n\n<p><strong>pyproject.toml<\/strong><\/p>\n\n<pre><code>requires = [\"poetry&gt;=1.0.0\", \"mlflow&gt;=0.7.0\", \"python&gt;=3.6\"]\nbuild-backend = \"poetry.masonry.api\"\n\n[tool.poetry]\nname = \"Sign\"\ndescription = \"\"\nversion = \"1.0.0\"\nreadme = \"README.md\"\nauthors = [\n  \"\"\n]\n\nlicense = \"MIT\"\n\n\n[tool.poetry.dependencies]\npython = \"3.6\"\nnumpy = \"1.14.3\"\nscipy = \"*\"\npandas = \"0.22.0\"\nscikit-learn = \"0.19.1\"\ncloudpickle = \"*\"\nmlflow =\"0.7.0\"\ntensorflow = \"^2.0.0\"\n\n\n[tool.poetry.dev-dependencies]\n\npylint = \"*\"\ndocker-compose = \"^1.25.0\"\ndocker-image-size-limit = \"^0.2.0\"\ntomlkit = \"^0.5.8\"\n\n<\/code><\/pre>\n\n<p><strong>docker-entrypoint.sh<\/strong><\/p>\n\n<pre><code>#!\/bin\/sh\n\nset -e\n\n. \/venv\/bin\/activate\n\nmlflow server \\\n    --file-store $FILE_STORE \\\n    --default-artifact-root $ARTIFACT_STORE \\\n    --host $SERVER_HOST \\\n    --port $SERVER_PORT\n<\/code><\/pre>\n\n<hr>\n\n<hr>\n\n<p>if i add <code>RUN pwd; ls<\/code> just befor the first <code>COPY<\/code> I obtain :<\/p>\n\n<pre><code>Step 20\/31 : RUN pwd; ls\n ---&gt; Running in e8ec36dd6ca8\n\/\nartifactStore\nbin\ndev\netc\nfileStore\nhome\nlib\nmedia\nmnt\nopt\nproc\nroot\nrun\nsbin\nsrv\nsys\ntmp\ntrunk\nusr\nvar\nvenv\nRemoving intermediate container e8ec36dd6ca8\n ---&gt; d7bba641bd7c\nStep 21\/31 : COPY  pyproject.toml .\/\nCOPY failed: stat \/var\/lib\/docker\/tmp\/docker-builder392824737\/pyproject.toml: no such file or directory\n\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1577166859160,
        "Question_score":2,
        "Question_tags":"docker|dockerfile|mlflow|python-poetry",
        "Question_view_count":940,
        "Owner_creation_time":1459625723203,
        "Owner_last_access_time":1648342504593,
        "Owner_location":"Grenoble, France",
        "Owner_reputation":56,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Question_last_edit_time":1577172041963,
        "Answer_body":"<p>Try \n<code>docker build -t test .<\/code><\/p>\n\n<p>instead of\n<code>docker build - &lt; Dockerfile<\/code><\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1577175350593,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59464404",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65789715,
        "Question_title":"MLFlow sklearn autologging prints too many info messages in colab",
        "Question_body":"<p>I am trying mlflow sklearn auto logging, in colab, mlflow prints a lot of info messages and at times it crashes the browser. Attaching the pic of info logs<a href=\"https:\/\/i.stack.imgur.com\/RqvNM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RqvNM.png\" alt=\"mlflow info logs\" \/><\/a><\/p>\n<p>codes are in <a href=\"https:\/\/colab.research.google.com\/drive\/1wvHSgYk6boKW0AMPqIt-AByyFHSO26wm?usp=sharing\" rel=\"nofollow noreferrer\">this colab file<\/a><\/p>\n<p>Am not sure what am missing here, but the same code works fine without producing these info logs on my local computer.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611052310507,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":194,
        "Owner_creation_time":1608633925527,
        "Owner_last_access_time":1657433687843,
        "Owner_location":"Dubai - United Arab Emirates",
        "Owner_reputation":25,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This is a known issue with MLFlow package, in which a hotfix has been raised.<\/p>\n<p>See here: <a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/3978\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/pull\/3978<\/a><\/p>\n<p><strong>Description of fault<\/strong><\/p>\n<p>In MLflow 1.13.0 and 1.13.1, the following Python event logging message is emitted when a patched ML training function begins execution within a preexisting MLflow run.<\/p>\n<p>Unfortunately, for patched ML training routines that make child calls to other patched ML training routines (e.g. sklearn random forests that call fit() on a collection of sklearn DecisionTree instances), this event log is printed to stdout every time a child is called.<\/p>\n<p>This can produce hundreds of redundant event logging calls that don't provide value to the user.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1611053713910,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65789715",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65937623,
        "Question_title":"Unable to serve an mlflow model locally",
        "Question_body":"<p>I have created an mlflow model with custom pyfunc. It shows the results when I send input to the loaded model in Jupyter notebook.\nHowever if I am trying to serve it to a local port<\/p>\n<pre><code>!mlflow models serve -m Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001\n<\/code><\/pre>\n<p>I am getting this error<\/p>\n<pre><code> Traceback (most recent call last):\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/bin\/mlflow&quot;, line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 829, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 782, in main\n    rv = self.invoke(ctx)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 610, in invoke\n    return callback(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 56, in serve\n    install_mlflow=install_mlflow).serve(model_uri=model_uri, port=port,\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 163, in _get_flavor_backend\n    append_to_uri_path(underlying_model_uri, &quot;MLmodel&quot;), output_path=tmp.path())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/tracking\/artifact_utils.py&quot;, line 76, in _download_artifact_from_uri\n    artifact_path=artifact_path, dst_path=output_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 67, in download_artifacts\n    return super(LocalArtifactRepository, self).download_artifacts(artifact_path, dst_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 140, in download_artifacts\n    return download_file(artifact_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 105, in download_file\n    self._download_file(remote_file_path=fullpath, local_path=local_file_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 95, in _download_file\n    shutil.copyfile(remote_file_path, local_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/shutil.py&quot;, line 120, in copyfile\n    with open(src, 'rb') as fsrc:\nFileNotFoundError: [Errno 2] No such file or directory: 'Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model\/MLmodel'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611838887777,
        "Question_score":0,
        "Question_tags":"deployment|localhost|mlflow",
        "Question_view_count":1277,
        "Owner_creation_time":1573739890560,
        "Owner_last_access_time":1663922252563,
        "Owner_location":null,
        "Owner_reputation":115,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Question_last_edit_time":null,
        "Answer_body":"<p>From your error traceback, the model artifact can't be located. In your code, you are executing the 'mlflow' command from within a Jupyter Notebook. I would suggest trying the following:<\/p>\n<ol>\n<li>Check if your models artifacts are on the path you are using Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model<\/li>\n<li>Try opening a terminal, then <code>cd \/Home\/miniconda3\/envs<\/code> and  execute <code>mlflow models serve -m .\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001<\/code><\/li>\n<li>MLFlow offers different solutions to serve a model, you can try to register your model and refer to it as &quot;models:\/{model_name}\/{stage}&quot; as mentioned in the Model Registry <a href=\"https:\/\/mlflow.org\/docs\/latest\/model-registry.html#serving-an-mlflow-model-from-model-registry\" rel=\"nofollow noreferrer\">docs<\/a><\/li>\n<\/ol>",
        "Answer_comment_count":2,
        "Answer_creation_time":1611840603100,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65937623",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65627039,
        "Question_title":"MLflow stores tags but does not return them",
        "Question_body":"<p>I am running the below code to store tags and then to retrieve them. As you can see below, Mlflow is storing one set of tags and returning another.<\/p>\n<pre><code>import mlflow\nwith mlflow.start_run() as active_run:\n    tw = { &quot;run_id&quot;: 1}\n    mlflow.set_tags(tw)            \n    print(&quot;Tags are &quot;, active_run.data.tags)\n    print(type(active_run.data.tags))\n<\/code><\/pre>\n<p>Output<\/p>\n<pre><code>Tags are  {'mlflow.source.name': '\/media\/Space\/AI\/anaconda4\/lib\/python3.7\/site-packages\/ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'adeel'}\n<\/code><\/pre>\n<p>Looking at the stored tags through mlflow ui, I can see that the tag &quot;run_id&quot; set by the code is actually stored in the run. However, only the header information of the run seems to be getting returned by active_run.data.tags.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610100575263,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":173,
        "Owner_creation_time":1445990517173,
        "Owner_last_access_time":1663982428387,
        "Owner_location":"Sydney, New South Wales, Australia",
        "Owner_reputation":689,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":87,
        "Question_last_edit_time":1611139529420,
        "Answer_body":"<p>At the moment, you have to query your run again in MLflow to get the run with all the info that you logged. In the example below, I call <code>mlflow.get_run(&lt;run_id&gt;)<\/code> to achieve this.<\/p>\n<pre><code>import mlflow\n\n\nwith mlflow.start_run() as active_run:\n  tags = { &quot;my_tag&quot;: 1}\n  mlflow.set_tags(tags)            \n  # Keep track of the run ID of the active run\n  run_id = active_run.info.run_id\n\nrun = mlflow.get_run(run_id)\nprint(&quot;The tags are &quot;, run.data.tags)\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1610128097450,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65627039",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73320708,
        "Question_title":"Set run description programmatically in mlflow",
        "Question_body":"<p>Similar to <a href=\"https:\/\/stackoverflow.com\/questions\/57199472\/is-it-possible-to-set-change-mlflow-run-name-after-run-initial-creation#:%7E:text=It%20is%20possible%20to%20edit,you%27d%20like%20to%20edit.&amp;text=There%27s%20currently%20no%20stable%20public,the%20tag%20with%20key%20mlflow.\">this question<\/a>, I'd like to edit\/set the description of a run via code, instead of editing it via UI.<\/p>\n<p>To clarify, I don't want to set the description of my entire experiment, only of a single run.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ogUgu.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ogUgu.png\" alt=\"Image showing what I want to edit\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660221143727,
        "Question_score":1,
        "Question_tags":"python|artificial-intelligence|mlflow",
        "Question_view_count":89,
        "Owner_creation_time":1527525798183,
        "Owner_last_access_time":1664059587093,
        "Owner_location":"Sarajevo, Bosnia and Herzegovina",
        "Owner_reputation":736,
        "Owner_up_votes":829,
        "Owner_down_votes":8,
        "Owner_views":57,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There are two ways to set the description.<\/p>\n<h3>1. <code>description<\/code> parameter<\/h3>\n<p>You can set a description using a markdown string for your run in <code>mlflow.start_run()<\/code> using <code>description<\/code> parameter. Here is an example.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if __name__ == &quot;__main__&quot;:\n    # load dataset and other stuff\n\n    run_description = &quot;&quot;&quot;\n### Header\nThis is a test **Bold**, *italic*, ~~strikethrough~~ text.\n[And this is an example hayperlink](http:\/\/example.com\/).\n    &quot;&quot;&quot;\n\n    with mlflow.start_run(description=run_description) as run:\n        # train model and other stuff\n<\/code><\/pre>\n<h3>2. <code>mlflow.note.content<\/code> tag<\/h3>\n<p>You can set\/edit run names by setting the tag with the key <code>mlflow.note.content<\/code>, which is what the UI (currently) does under the hood.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if __name__ == &quot;__main__&quot;:\n    # load dataset and other stuff\n\n    run_description = &quot;&quot;&quot;\n### Header\nThis is a test **Bold**, *italic*, ~~strikethrough~~ text.\n[And this is an example hayperlink](http:\/\/example.com\/).\n    &quot;&quot;&quot;\n\n    tags = {\n        'mlflow.note.content': run_description\n    }\n\n    with mlflow.start_run(tags=tags) as run:\n        # train model and other stuff\n<\/code><\/pre>\n<h3>Result<\/h3>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4zZa9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4zZa9.png\" alt=\"output of the given example\" \/><\/a><\/p>\n<hr \/>\n<p>If you set <code>description<\/code> parameter and <code>mlflow.note.content<\/code> tag in <code>mlflow.start_run()<\/code>, you'll get this error.<\/p>\n<pre><code>Description is already set via the tag mlflow.note.content in tags.\nRemove the key mlflow.note.content from the tags or omit the description.\n<\/code><\/pre>",
        "Answer_comment_count":2,
        "Answer_creation_time":1660231264753,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1660307815687,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73320708",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73074887,
        "Question_title":"'mlflow' has no attribute 'last_active_run'",
        "Question_body":"<p>For the first time, it is proceeding mlflow with port 5000.<\/p>\n<p>Testing Mlflow, problem is no attribute last_active_run in mlflow<\/p>\n<p>But, It was an example provided by Mlflow. <br \/>\nlink is here <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/sklearn_autolog\" rel=\"nofollow noreferrer\">mlflow<\/a><\/p>\n<p>What is problem and how can I change code?<\/p>\n<p>shell<\/p>\n<pre><code>wget https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/master\/examples\/sklearn_autolog\/utils.py\nwget https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/master\/examples\/sklearn_autolog\/pipeline.py\n<\/code><\/pre>\n<p>pipeline.py<\/p>\n<pre><code>from pprint import pprint\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nimport mlflow\nfrom utils import fetch_logged_data\n\n\ndef main():\n    # enable autologging\n    mlflow.sklearn.autolog()\n\n    # prepare training data\n    X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n    y = np.dot(X, np.array([1, 2])) + 3\n\n    # train a model\n    pipe = Pipeline([(&quot;scaler&quot;, StandardScaler()), (&quot;lr&quot;, LinearRegression())])\n    pipe.fit(X, y)\n    run_id = mlflow.last_active_run().info.run_id\n    print(&quot;Logged data and model in run: {}&quot;.format(run_id))\n\n    # show logged data\n    for key, data in fetch_logged_data(run_id).items():\n        print(&quot;\\n---------- logged {} ----------&quot;.format(key))\n        pprint(data)\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n<\/code><\/pre>\n<p>utils.py<\/p>\n<pre><code>import mlflow\nfrom mlflow.tracking import MlflowClient\n\n\ndef yield_artifacts(run_id, path=None):\n    &quot;&quot;&quot;Yield all artifacts in the specified run&quot;&quot;&quot;\n    client = MlflowClient()\n    for item in client.list_artifacts(run_id, path):\n        if item.is_dir:\n            yield from yield_artifacts(run_id, item.path)\n        else:\n            yield item.path\n\n\ndef fetch_logged_data(run_id):\n    &quot;&quot;&quot;Fetch params, metrics, tags, and artifacts in the specified run&quot;&quot;&quot;\n    client = MlflowClient()\n    data = client.get_run(run_id).data\n    # Exclude system tags: https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#system-tags\n    tags = {k: v for k, v in data.tags.items() if not k.startswith(&quot;mlflow.&quot;)}\n    artifacts = list(yield_artifacts(run_id))\n    return {\n        &quot;params&quot;: data.params,\n        &quot;metrics&quot;: data.metrics,\n        &quot;tags&quot;: tags,\n        &quot;artifacts&quot;: artifacts,\n    }\n<\/code><\/pre>\n<p>Error message<\/p>\n<pre><code>INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '8cc3f4e03b4e417b95a64f1a9a41be63', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\nTraceback (most recent call last):\n  File &quot;\/Users\/taein\/Desktop\/mlflow\/pipeline.py&quot;, line 33, in &lt;module&gt;\n    main()\n  File &quot;\/Users\/taein\/Desktop\/mlflow\/pipeline.py&quot;, line 23, in main\n    run_id = mlflow.last_active_run().info.run_id\nAttributeError: module 'mlflow' has no attribute 'last_active_run'\n<\/code><\/pre>\n<p>Thanks for your helping<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1658461256963,
        "Question_score":1,
        "Question_tags":"python|python-3.x|scikit-learn|mlflow",
        "Question_view_count":215,
        "Owner_creation_time":1598180751547,
        "Owner_last_access_time":1663903316017,
        "Owner_location":"Seoul, Repulic of Korea",
        "Owner_reputation":158,
        "Owner_up_votes":4,
        "Owner_down_votes":2,
        "Owner_views":29,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It's because of the mlflow version that you mentioned in the comments. <code>mlflow.last_active_run()<\/code> API was introduced in <a href=\"https:\/\/github.com\/mlflow\/mlflow\/releases\/tag\/v1.25.0\" rel=\"nofollow noreferrer\">mlflow 1.25.0\n<\/a>. So you should upgrade the mlflow or you can use the previous version of the code available <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/5e2cb3baef544b00a972dff9dd6fb764be20510b\/examples\/sklearn_autolog\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<pre><code>wget https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/5e2cb3baef544b00a972dff9dd6fb764be20510b\/examples\/sklearn_autolog\/utils.py\nwget https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/5e2cb3baef544b00a972dff9dd6fb764be20510b\/examples\/sklearn_autolog\/pipeline.py\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1658706466923,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73074887",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56700687,
        "Question_title":"Installing dependencies from (Conda) environment.yml without Conda?",
        "Question_body":"<p>I currently use Conda to capture my dependencies for a python project in a <code>environment.yml<\/code>.<\/p>\n\n<p>When I build a docker service from the project I need to reinstall these dependencies. I would like to get around, having to add (mini-)conda to my docker image.<\/p>\n\n<p>Is it possible to parse <code>environment.yml<\/code> with pip\/pipenv or transform this into a corresponding <code>requirements.txt<\/code>?<\/p>\n\n<p>(I don't want to leave conda just yet, as this is what MLflow captures, when I log models)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1561109630463,
        "Question_score":1,
        "Question_tags":"python|docker|pip|conda|mlflow",
        "Question_view_count":1601,
        "Owner_creation_time":1336936830510,
        "Owner_last_access_time":1663079651823,
        "Owner_location":null,
        "Owner_reputation":948,
        "Owner_up_votes":592,
        "Owner_down_votes":1,
        "Owner_views":132,
        "Question_last_edit_time":1561111068256,
        "Answer_body":"<p>Nope.<\/p>\n\n<ol>\n<li><p><code>conda<\/code> automatically installs dependencies of conda packages. These are resolved differently by <code>pip<\/code>, so you'd have to resolve the Anaconda dependency tree in your transformation script.<\/p><\/li>\n<li><p>Many <code>conda<\/code> packages are non-Python. You couldn't install those dependencies with <code>pip<\/code> at all.<\/p><\/li>\n<li><p>Some <code>conda<\/code> packages contain binaries that were compiled with the Anaconda compiler toolchain. Even if the corresponding <code>pip<\/code> package can compile such binaries on installation, it wouldn't be using the Anaconda toolchain. What you'd get would be fundamentally different from the corresponding <code>conda<\/code> package.<\/p><\/li>\n<li><p>Some <code>conda<\/code> packages have fixes applied, which are missing from corresponding <code>pip<\/code> packages.<\/p><\/li>\n<\/ol>\n\n<p>I hope this is enough to convince you that your idea won't fly.<\/p>\n\n<p>Installing Miniconda isn't really a big deal. Just do it :-)<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1561110403227,
        "Answer_score":3,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56700687",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69944447,
        "Question_title":"How to change the directory of mlflow logs?",
        "Question_body":"<p>I am using MLflow to log the metrics but I want to change the default saving logs directory. So, instead of writing log files besides my main file, I want to store them to <code>\/path\/outputs\/lg <\/code>. I don't know how to change it. I use it without in the <code>Model<\/code>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nfrom time import time\n\nimport mlflow\nimport numpy as np\nimport torch\nimport tqdm\n\n# from segmentation_models_pytorch.utils import metrics\nfrom AICore.emergency_landing.metrics import IoU, F1\nfrom AICore.emergency_landing.utils import AverageMeter\nfrom AICore.emergency_landing.utils import TBLogger\n\n\nclass Model:\n    def __init__(self, model, num_classes=5, ignore_index=0, optimizer=None, scheduler=None, criterion=None,\n                 device=None, epochs=30, train_loader=None, val_loader=None, tb_logger: TBLogger = None,\n                 logger=None,\n                 best_model_path=None,\n                 model_check_point_path=None,\n                 load_from_best_model=None,\n                 load_from_model_checkpoint=None,\n                 early_stopping=None,\n                 debug=False):\n\n        self.debug = debug\n\n        self.early_stopping = {\n            'init': early_stopping,\n            'changed': 0\n        }\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.criterion = criterion\n        self.device = device\n        self.epochs = epochs\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n        self.model = model.to(device)\n\n        self.tb_logger = tb_logger\n        self.logger = logger\n\n        self.best_loss = np.Inf\n\n        if not os.path.exists(best_model_path):\n            os.makedirs(best_model_path)\n        self.best_model_path = best_model_path\n\n        if not os.path.exists(model_check_point_path):\n            os.makedirs(model_check_point_path)\n        self.model_check_point_path = model_check_point_path\n\n        self.load_from_best_model = load_from_best_model\n        self.load_from_model_checkpoint = load_from_model_checkpoint\n\n        if self.load_from_best_model is not None:\n            self.load_model(path=self.load_from_best_model)\n        if self.load_from_model_checkpoint is not None:\n            self.load_model_checkpoint(path=self.load_from_model_checkpoint)\n\n        self.train_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n        self.val_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n        self.test_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n\n        self.train_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n        self.val_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n        self.test_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n\n    def metrics(self, is_train=True):\n        if is_train:\n            train_losses = AverageMeter('Training Loss', ':.4e')\n            train_iou = AverageMeter('Training iou', ':6.2f')\n            train_f_score = AverageMeter('Training F_score', ':6.2f')\n\n            return train_losses, train_iou, train_f_score\n        else:\n            val_losses = AverageMeter('Validation Loss', ':.4e')\n            val_iou = AverageMeter('Validation mean iou', ':6.2f')\n            val_f_score = AverageMeter('Validation F_score', ':6.2f')\n\n            return val_losses, val_iou, val_f_score\n\n    def fit(self):\n\n        self.logger.info(&quot;\\nStart training\\n\\n&quot;)\n        start_training_time = time()\n\n        with mlflow.start_run():\n            for e in range(self.epochs):\n                start_training_epoch_time = time()\n                self.model.train()\n                train_losses_avg, train_iou_avg, train_f_score_avg = self.metrics(is_train=True)\n                with tqdm.tqdm(self.train_loader, unit=&quot;batch&quot;) as tepoch:\n                    tepoch.set_description(f&quot;Epoch {e}&quot;)\n                    for image, target in tepoch:\n                        # Transfer Data to GPU if available\n                        image = image.to(self.device)\n                        target = target.to(self.device)\n                        # Clear the gradients\n                        self.optimizer.zero_grad()\n                        # Forward Pass\n                        # out = self.model(image)['out']\n                        # if unet == true =&gt; remove ['out']\n                        out = self.model(image)\n                        # Find the Loss\n                        loss = self.criterion(out, target)\n                        # Calculate Loss\n                        train_losses_avg.update(loss.item(), image.size(0))\n                        # Calculate gradients\n                        loss.backward()\n                        # Update Weights\n                        self.optimizer.step()\n\n                        iou = self.train_iou(out.cpu(), target.cpu()).item()\n                        train_iou_avg.update(iou)\n\n                        f1_score = self.train_f1(out.cpu(), target.cpu()).item()\n                        train_f_score_avg.update(f1_score)\n\n                        tepoch.set_postfix(loss=train_losses_avg.avg,\n                                           iou=train_iou_avg.avg,\n                                           f_score=train_f_score_avg.avg)\n                        if self.debug:\n                            break\n\n                self.tb_logger.log(log_type='criterion\/training', value=train_losses_avg.avg, epoch=e)\n                self.tb_logger.log(log_type='iou\/training', value=train_iou_avg.avg, epoch=e)\n                self.tb_logger.log(log_type='f_score\/training', value=train_f_score_avg.avg, epoch=e)\n\n                mlflow.log_metric('criterion\/training', train_losses_avg.avg, step=e)\n                mlflow.log_metric('iou\/training', train_iou_avg.avg, step=e)\n                mlflow.log_metric('f_score\/training', train_f_score_avg.avg, step=e)\n\n                end_training_epoch_time = time() - start_training_epoch_time\n                print('\\n')\n                self.logger.info(\n                    f'Training Results - [{end_training_epoch_time:.3f}s] Epoch: {e}:'\n                    f' f_score: {train_f_score_avg.avg:.3f},'\n                    f' IoU: {train_iou_avg.avg:.3f},'\n                    f' Loss: {train_losses_avg.avg:.3f}')\n\n                # validation step\n                val_loss = self.evaluation(e)\n                # apply scheduler\n                if self.scheduler:\n                    self.scheduler.step()\n                # early stopping\n                if self.early_stopping['init'] &gt;= self.early_stopping['changed']:\n                    self._early_stopping_model(val_loss=val_loss)\n                else:\n                    print(f'The model can not learn more, Early Stopping at epoch[{e}]')\n                    break\n\n                # save best model\n                if self.best_model_path is not None:\n                    self._best_model(val_loss=val_loss, path=self.best_model_path)\n\n                # model check points\n                if self.model_check_point_path is not None:\n                    self.save_model_check_points(path=self.model_check_point_path, epoch=e, net=self.model,\n                                                 optimizer=self.optimizer, loss=self.criterion,\n                                                 avg_loss=train_losses_avg.avg)\n\n                # log mlflow\n                if self.scheduler:\n                    mlflow.log_param(&quot;get_last_lr&quot;, self.scheduler.get_last_lr())\n                    mlflow.log_param(&quot;scheduler&quot;, self.scheduler.state_dict())\n\n                self.tb_logger.flush()\n                if self.debug:\n                    break\n\n            end_training_time = time() - start_training_time\n            print(f'Finished Training after {end_training_time:.3f}s')\n            self.tb_logger.close()\n\n    def evaluation(self, epoch):\n        print('Validating...')\n        start_validation_epoch_time = time()\n        self.model.eval()  # Optional when not using Model Specific layer\n        with torch.no_grad():\n            val_losses_avg, val_iou_avg, val_f_score_avg = self.metrics(is_train=False)\n            with tqdm.tqdm(self.val_loader, unit=&quot;batch&quot;) as tepoch:\n                for image, target in tepoch:\n                    # Transfer Data to GPU if available\n                    image = image.to(self.device)\n                    target = target.to(self.device)\n                    # out = self.model(image)['out']\n                    # if unet == true =&gt; remove ['out']\n                    out = self.model(image)\n                    # Find the Loss\n                    loss = self.criterion(out, target)\n                    # Calculate Loss\n                    val_losses_avg.update(loss.item(), image.size(0))\n\n                    iou = self.val_iou(out.cpu(), target.cpu()).item()\n                    val_iou_avg.update(iou)\n\n                    f1_score = self.val_f1(out.cpu(), target.cpu()).item()\n                    val_f_score_avg.update(f1_score)\n\n                    tepoch.set_postfix(loss=val_losses_avg.avg,\n                                       iou=val_iou_avg.avg,\n                                       f_score=val_f_score_avg.avg)\n                    if self.debug:\n                        break\n            print('\\n')\n            self.tb_logger.log(log_type='criterion\/validation', value=val_losses_avg.avg, epoch=epoch)\n            self.tb_logger.log(log_type='iou\/validation', value=val_iou_avg.avg, epoch=epoch)\n            self.tb_logger.log(log_type='f_score\/validation', value=val_f_score_avg.avg, epoch=epoch)\n\n            mlflow.log_metric('criterion\/validation', val_losses_avg.avg, step=epoch)\n            mlflow.log_metric('iou\/validation', val_iou_avg.avg, step=epoch)\n            mlflow.log_metric('f_score\/validation', val_f_score_avg.avg, step=epoch)\n\n            end_validation_epoch_time = time() - start_validation_epoch_time\n            self.logger.info(\n                f'validation Results - [{end_validation_epoch_time:.3f}s] Epoch: {epoch}:'\n                f' f_score: {val_f_score_avg.avg:.3f},'\n                f' IoU: {val_iou_avg.avg:.3f},'\n                f' Loss: {val_losses_avg.avg:.3f}')\n            print('\\n')\n            return val_losses_avg.avg\n\n    def _save_model(self, name, path, params):\n        torch.save(params, path)\n\n    def _early_stopping_model(self, val_loss):\n        if self.best_loss &lt; val_loss:\n            self.early_stopping['changed'] += 1\n        else:\n            self.early_stopping['changed'] = 0\n\n    def _best_model(self, val_loss, path):\n        if self.best_loss &gt; val_loss:\n            self.best_loss = val_loss\n            name = f'\/best_model_loss_{self.best_loss:.2f}'.replace('.', '_')\n            self._save_model(name, path=f'{path}\/{name}.pt', params={\n                'model_state_dict': self.model.state_dict(),\n            })\n\n            print(f'The best model is saved with criterion: {self.best_loss:.2f}')\n\n    def save_model_check_points(self, path, epoch, net, optimizer, loss, avg_loss):\n        name = f'\/model_epoch_{epoch}_loss_{avg_loss:.2f}'.replace('.', '_')\n        self._save_model(name, path=f'{path}\/{name}.pt', params={\n            'epoch': epoch,\n            'model_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'criterion': loss,\n        })\n        print(f'model checkpoint is saved at model_epoch_{epoch}_loss_{avg_loss:.2f}')\n\n    def load_model_checkpoint(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        epoch = checkpoint['epoch']\n        self.criterion = checkpoint['criterion']\n\n        return epoch\n\n    def load_model(self, path):\n        best_model = torch.load(path)\n        self.model.load_state_dict(best_model['model_state_dict'])\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1636727119830,
        "Question_score":0,
        "Question_tags":"machine-learning|deep-learning|pytorch|mlflow",
        "Question_view_count":436,
        "Owner_creation_time":1410333105327,
        "Owner_last_access_time":1662489968593,
        "Owner_location":"Turin, Metropolitan City of Turin, Italy",
        "Owner_reputation":477,
        "Owner_up_votes":133,
        "Owner_down_votes":0,
        "Owner_views":130,
        "Question_last_edit_time":1636755379243,
        "Answer_body":"<p>The solution is:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.set_tracking_uri(uri=f'file:\/\/{hydra.utils.to_absolute_path(&quot;..\/output\/mlruns&quot;)}')\nexp = mlflow.get_experiment_by_name(name='Emegency_landing')\nif not exp:\n    experiment_id = mlflow.create_experiment(name='Emegency_landing',\n                                                 artifact_location=f'file:\/\/{hydra.utils.to_absolute_path(&quot;..\/output\/mlruns&quot;)}')\nelse:\n    experiment_id = exp.experiment_id\n<\/code><\/pre>\n<p>And then you should pass the experiment Id to:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with mlflow.start_run(experiment_id=experiment_id):\n     pass \n<\/code><\/pre>\n<p>If you don't mention the <code>\/path\/mlruns<\/code>, when you run the command of <code>mlflow ui<\/code>, it will create another folder automatically named <code>mlruns<\/code>. so, pay attention to this point to have the same name as <code>mlruns<\/code>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1636732235590,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1636755540676,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69944447",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":66792575,
        "Question_title":"Log text in mlflow",
        "Question_body":"<p>I know that you can log metrics as your experiment progresses. For example the training loss over epochs for your DL model.<\/p>\n<p>I was wondering if it was possible to do something similar for text. In my particular case I have a text model that generates some example text after each epoch and I wish to see what it's like. For example:<\/p>\n<pre><code>Epoch 1:\ntHi is RubisH\nEpoch 2:\nOk look slight better\nEpoch 3:\nI can speak English better than William Shakespeare\n<\/code><\/pre>\n<p>The workaround I can think of is to log this to a text file and push that as an artifact in mlflow. Was wondering if there was something else more native to mlflow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616642359947,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":1295,
        "Owner_creation_time":1372398800110,
        "Owner_last_access_time":1664007625063,
        "Owner_location":"Sydney NSW, Australia",
        "Owner_reputation":9065,
        "Owner_up_votes":1204,
        "Owner_down_votes":0,
        "Owner_views":952,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_param\" rel=\"nofollow noreferrer\">log_param\/log_params<\/a> for that. For long texts maybe it's better to use <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_text\" rel=\"nofollow noreferrer\">log_text<\/a> instead...<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1616671469503,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66792575",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":56851463,
        "Question_title":"How do I specify mlflow MLproject with zero parameters?",
        "Question_body":"<p>I tried to create MLproject with zero parameters as:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    parameters:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>when I get an error:<\/p>\n\n<pre><code>  Traceback (most recent call last):\n File \"\/home\/ubuntu\/.local\/bin\/mlflow\", line 11, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/cli.py\", line 137, in run\n    run_id=run_id,\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 230, in run\n    use_conda=use_conda, storage_dir=storage_dir, synchronous=synchronous, run_id=run_id)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 85, in _run\n    project = _project_spec.load_project(work_dir)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 40, in load_project\n    entry_points[name] = EntryPoint(name, parameters, command)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 87, in __init__\n    self.parameters = {k: Parameter(k, v) for (k, v) in parameters.items()}\nAttributeError: 'NoneType' object has no attribute 'items'\n<\/code><\/pre>\n\n<p>Am I missing something or mlflow does not allow project with  zero parameters?<\/p>\n\n<p>I have also posted this at my public repo of: <a href=\"https:\/\/github.com\/sameermahajan\/mlflow-try\" rel=\"nofollow noreferrer\">https:\/\/github.com\/sameermahajan\/mlflow-try<\/a> if someone would like to try out:<\/p>\n\n<pre><code>mlflow run https:\/\/github.com\/sameermahajan\/mlflow-try.git\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1562066976100,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":353,
        "Owner_creation_time":1384343462317,
        "Owner_last_access_time":1663916912330,
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":478,
        "Owner_up_votes":65,
        "Owner_down_votes":4,
        "Owner_views":118,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For this, you completely drop the 'parameters' section as below:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>(I thought I had tried it earlier but I was trying too many different ways to may be miss out on this one)<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1562240543613,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56851463",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67062145,
        "Question_title":"Continue stopped run in MLflow",
        "Question_body":"<p>We run our experiment on AWS spot instances. Sometimes the experiments are stopped, and we would prefer to continue logging to the same run. How can you set the run-id of the active run?<\/p>\n<p>Something like this pseudocode (not working):<\/p>\n<pre><code>if new:\n    mlflow.start_run(experiment_id=1, run_name=x)\nelse:\n    mlflow.set_run(run_id)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618244956103,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":131,
        "Owner_creation_time":1484838464573,
        "Owner_last_access_time":1663858511743,
        "Owner_location":"Amsterdam, Nederland",
        "Owner_reputation":3937,
        "Owner_up_votes":672,
        "Owner_down_votes":27,
        "Owner_views":387,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can pass the run_id directly to <code>start_run<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.start_run(experiment_id=1,\n                 run_name=x,\n                 run_id=&lt;run_id_of_interrupted_run&gt; # pass None to start a new run\n                 ) \n<\/code><\/pre>\n<p>Of course, you have to store the run_id for this. You can get it with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.entities.html#mlflow.entities.RunInfo.run_id\" rel=\"nofollow noreferrer\"><code>run.info.run_id<\/code><\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1631884865500,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67062145",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59773167,
        "Question_title":"When will experiment be deleted with lifecycle_stage is set as deleted",
        "Question_body":"<p>I can see experiment 2 is in deleted, but when it will be deleted actually?<\/p>\n\n<pre><code>2   test    hdfs:\/\/\/1234\/mlflow deleted\n<\/code><\/pre>\n\n<p>If the experiment is not deleted automatically, how can I delete it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579189296547,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":412,
        "Owner_creation_time":1408370821673,
        "Owner_last_access_time":1663216838707,
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":2521,
        "Owner_up_votes":447,
        "Owner_down_votes":13,
        "Owner_views":197,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I am assuming you use sql store?<\/p>\n\n<p>Currently there is no way to tell mlflow to hard-delete experiments. We are working with open source contributors to add a cli command that would perform garbage-collection of deleted experiments. This should be added soon in one of the upcoming mlflow releases. In the meantime, you can connect to your sql store and delete the experiments manually.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1579736829917,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59773167",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":69703225,
        "Question_title":"Can I change the port of my MLflow tracking server?",
        "Question_body":"<p>I would like to know if I can change the port of my MLflow server.<\/p>\n<p>By default it is running on port 5000, but my company's VPN only allows HTTP (port 80) and HTTPS (port 443) traffic.<\/p>\n<p>This might be a very beginner's question, but is it possible, and if yes, is there any problem on running the MLflow server on port 83 (HTTP) ?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635139632427,
        "Question_score":1,
        "Question_tags":"http|port|mlflow",
        "Question_view_count":540,
        "Owner_creation_time":1561106497313,
        "Owner_last_access_time":1661059074150,
        "Owner_location":null,
        "Owner_reputation":133,
        "Owner_up_votes":28,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes, you can do that by passing the <code>-p port_number<\/code> command-line switch when starting MLflow server (see <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#cmdoption-mlflow-server-p\" rel=\"nofollow noreferrer\">docs<\/a>). Please note, that to be able to use ports below 1024, the server needs to be run as root.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1635152056290,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69703225",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70005957,
        "Question_title":"Logging the git_sha as a parameter on Mlflow using Kedro hooks",
        "Question_body":"<p>I would like to log the git_sha parameter on Mlflow as shown in the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/02_hooks.html?highlight=run_params#add-metrics-tracking-to-your-model\" rel=\"nofollow noreferrer\">documentation<\/a>. What appears to me here, is that simply running the following portion of code should be enough to get git_sha logged in the Mlflow UI. Am I right ?<\/p>\n<pre><code>@hook_impl\n    def before_pipeline_run(self, run_params: Dict[str, Any]) -&gt; None:\n        &quot;&quot;&quot;Hook implementation to start an MLflow run\n        with the same run_id as the Kedro pipeline run.\n        &quot;&quot;&quot;\n        mlflow.start_run(run_name=run_params[&quot;run_id&quot;])\n        mlflow.log_params(run_params)\n<\/code><\/pre>\n<p>But this does not work as I get all but the git_sha parameter. And when I look at the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/_modules\/kedro\/framework\/hooks\/specs.html?highlight=run_params#\" rel=\"nofollow noreferrer\">hooks specs<\/a>, it seems that this param is not part of run_params (anymore?)<\/p>\n<p>Is there a way I could get the git sha (maybe from the context journal ?) and add it to the logged parameters ?<\/p>\n<p>Thank you in advance !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637158421443,
        "Question_score":0,
        "Question_tags":"python|mlflow|kedro|mlops",
        "Question_view_count":172,
        "Owner_creation_time":1586517832390,
        "Owner_last_access_time":1660328393330,
        "Owner_location":null,
        "Owner_reputation":127,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":1637158766987,
        "Answer_body":"<p>Whilst it's heavily encouraged to use git with Kedro it's not required and as such no part of Kedro (except <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro-starters\" rel=\"nofollow noreferrer\">kedro-starters<\/a> if we're being pedantic) is 'aware' of git.<\/p>\n<p>In your <code>before_pipeline_hook<\/code> there it is pretty easy for you to retrieve the info <a href=\"https:\/\/stackoverflow.com\/questions\/14989858\/get-the-current-git-hash-in-a-python-script\">via the techniques documented here<\/a>. It seems trivial for the whole codebase, a bit more involved if you want to say provide pipeline specific hashes.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1637159193837,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70005957",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73621446,
        "Question_title":"Log Any Type of Model in MLflow",
        "Question_body":"<p>I am trying to create a wrapper function that allows my Data Scientists to log their models in MLflow.<\/p>\n<p>This is what the function looks like,<\/p>\n<pre><code>def log_model(self, params, metrics, model, run_name, artifact_path, artifacts=None):\n\n    with mlflow.start_run(run_name=run_name):\n        run_id = mlflow.active_run().info.run_id\n        mlflow.log_params(params)\n        mlflow.log_metrics(metrics)\n\n        if model:\n            mlflow.lightgbm.log_model(model, artifact_path=artifact_path)\n\n        if artifacts:\n            for artifact in artifacts:\n                mlflow.log_artifact(artifact, artifact_path=artifact_path)\n\n    return run_id\n<\/code><\/pre>\n<p>It can be seen here that the model is being logged as a <code>lightgbm<\/code> model, however, the <code>model<\/code> parameter that is passed into this function can be of any type.<\/p>\n<p>How can I update this function, so that it will be able to log any kind of model?<\/p>\n<p>As far as I know, there is no <code>log_model<\/code> function that comes with <code>mlflow<\/code>. It's always <code>mlflow.&lt;model_type&gt;.log_model<\/code>.<\/p>\n<p>How can I go about handling this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662464447387,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":39,
        "Owner_creation_time":1521856385820,
        "Owner_last_access_time":1664037995903,
        "Owner_location":"Sri Lanka",
        "Owner_reputation":820,
        "Owner_up_votes":389,
        "Owner_down_votes":1,
        "Owner_views":165,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I was able to solve this using the following approach,<\/p>\n<pre><code>def log_model(model, artifact_path):\n    model_class = get_model_class(model).split('.')[0]\n\n    try:\n        log_model = getattr(mlflow, model_class).log_model\n        log_model(model, artifact_path)\n    except AttributeError:\n        logger.info('The log_model function is not available as expected!')\n\ndef get_model_class(model):\n    klass = model.__class__\n    module = klass.__module__\n\n    if module == 'builtins':\n        return klass.__qualname__\n    return module + '.' + klass.__qualname__\n<\/code><\/pre>\n<p>From what I have seen, this will be able to handle most cases. The <code>get_model_class()<\/code> method will return the class used to develop the model and based on this, we can use the <code>getattr()<\/code> method to extract the relevant <code>log_model()<\/code> method.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1663636318523,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73621446",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67953241,
        "Question_title":"mlflow run git-uri clone to specific directory",
        "Question_body":"<p>I am using mlflow run with a GitHub uri.<\/p>\n<p>When I run using the below command<\/p>\n<pre><code>mlflow run &lt;git-uri&gt;\n<\/code><\/pre>\n<p>The command sets up a conda environment and then <em>clones the Git repo into a <strong>temp<\/strong> directory, But I need it setup in a <strong>specific<\/strong> directory<\/em><\/p>\n<p>I checked the entire document, but I can't find it. Is there no such option to do so in one shot?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1623534343453,
        "Question_score":1,
        "Question_tags":"python-3.x|mlflow",
        "Question_view_count":239,
        "Owner_creation_time":1575348765723,
        "Owner_last_access_time":1663079223380,
        "Owner_location":"Chennai, Tamil Nadu, India",
        "Owner_reputation":1049,
        "Owner_up_votes":55,
        "Owner_down_votes":68,
        "Owner_views":192,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For non-local URIs, MLflow uses the Python's <code>tempfile.mkdtemp<\/code> function (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/1c43176cefb5531fbb243975b9c8c5bfb9775e66\/mlflow\/projects\/utils.py#L140\" rel=\"nofollow noreferrer\">source code<\/a>), that creates the temporary directory.  You may have some control over it by setting the <code>TMPDIR<\/code> environment variable as described in <a href=\"https:\/\/docs.python.org\/3\/library\/tempfile.html#tempfile.mkstemp\" rel=\"nofollow noreferrer\">Python docs<\/a> (it lists <code>TMP<\/code> &amp; <code>TEMP<\/code> as well, but they didn't work for me on MacOS) - but it will set only &quot;base path&quot; for temporary directories and files, the directory\/file names are still will be random.<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1623570743820,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1623614468896,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67953241",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71687131,
        "Question_title":"How to import MLflow tracking server WSGI application via Flask or FastAPI?",
        "Question_body":"<p>MLflow provides a very cool tracking server, however, this server does not provide authentication or RBAC which is required for my needs.<\/p>\n<p>I would like to add my own authentication and RBAC functionality. I think one way to accomplish this is to import the MLflow WSGI application object and add some middleware layers to perform authentication \/ authorization before passing requests through to the tracking server, essentially proxying requests through my custom middleware stack.<\/p>\n<p>How do I go about doing this? I can see from <a href=\"https:\/\/fastapi.tiangolo.com\/advanced\/wsgi\/\" rel=\"nofollow noreferrer\">these docs<\/a> that I can use FastAPI to import another WSGI application and add custom middleware, but I'm not sure of a few things<\/p>\n<ol>\n<li>Where do I find the MLflow tracking server WSGI app (where can it be imported from)?<\/li>\n<li>How do I pass through the relevant arguments to the MLflow tracking server? I.e. the tracking server expects params to configure the backend storage layer, host, and port. If I just import the application object, how do I pass those parameters to it?<\/li>\n<\/ol>\n<p>edit - it looks like the Flask application can be found here <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/server\/__init__.py#L28\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/server\/__init__.py#L28<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648703157780,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":394,
        "Owner_creation_time":1425965839877,
        "Owner_last_access_time":1663961190290,
        "Owner_location":"Santa Cruz, CA",
        "Owner_reputation":3256,
        "Owner_up_votes":81,
        "Owner_down_votes":0,
        "Owner_views":164,
        "Question_last_edit_time":1648705670280,
        "Answer_body":"<p>This was actually very simple, below is an example using FastAPI to import and mount the MLflow WSGI application.<\/p>\n<pre><code>import os\nimport subprocess\nfrom fastapi import FastAPI\nfrom fastapi.middleware.wsgi import WSGIMiddleware\n\nfrom mlflow.server import app as mlflow_app\n\napp = FastAPI()\napp.mount(&quot;\/&quot;, WSGIMiddleware(mlflow_app))\n\nBACKEND_STORE_URI_ENV_VAR = &quot;_MLFLOW_SERVER_FILE_STORE&quot;\nARTIFACT_ROOT_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACT_ROOT&quot;\nARTIFACTS_DESTINATION_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACT_DESTINATION&quot;\nPROMETHEUS_EXPORTER_ENV_VAR = &quot;prometheus_multiproc_dir&quot;\nSERVE_ARTIFACTS_ENV_VAR = &quot;_MLFLOW_SERVER_SERVE_ARTIFACTS&quot;\nARTIFACTS_ONLY_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACTS_ONLY&quot;\n\ndef parse_args():\n    a = argparse.ArgumentParser()\n    a.add_argument(&quot;--host&quot;, type=str, default=&quot;0.0.0.0&quot;)\n    a.add_argument(&quot;--port&quot;, type=str, default=&quot;5000&quot;)\n    a.add_argument(&quot;--backend-store-uri&quot;, type=str, default=&quot;sqlite:\/\/\/mlflow.db&quot;)\n    a.add_argument(&quot;--serve-artifacts&quot;, action=&quot;store_true&quot;, default=False)\n    a.add_argument(&quot;--artifacts-destination&quot;, type=str)\n    a.add_argument(&quot;--default-artifact-root&quot;, type=str)\n    a.add_argument(&quot;--gunicorn-opts&quot;, type=str, default=&quot;&quot;)\n    a.add_argument(&quot;--n-workers&quot;, type=str, default=1)\n    return a.parse_args()\n\ndef run_command(cmd, env, cwd=None):\n    cmd_env = os.environ.copy()\n    if cmd_env:\n        cmd_env.update(env)\n    child = subprocess.Popen(\n        cmd, env=cmd_env, cwd=cwd, text=True, stdin=subprocess.PIPE\n    )\n    child.communicate()\n    exit_code = child.wait()\n    if exit_code != 0:\n        raise Exception(&quot;Non-zero exitcode: %s&quot; % (exit_code))\n    return exit_code\n\ndef run_server(args):\n    env_map = dict()\n    if args.backend_store_uri:\n        env_map[BACKEND_STORE_URI_ENV_VAR] = args.backend_store_uri\n    if args.serve_artifacts:\n        env_map[SERVE_ARTIFACTS_ENV_VAR] = &quot;true&quot;\n    if args.artifacts_destination:\n        env_map[ARTIFACTS_DESTINATION_ENV_VAR] = args.artifacts_destination\n    if args.default_artifact_root:\n        env_map[ARTIFACT_ROOT_ENV_VAR] = args.default_artifact_root\n\n    print(f&quot;Envmap: {env_map}&quot;)\n\n    #opts = args.gunicorn_opts.split(&quot; &quot;) if args.gunicorn_opts else []\n    opts = args.gunicorn_opts if args.gunicorn_opts else &quot;&quot;\n\n    cmd = [\n        &quot;gunicorn&quot;, &quot;-b&quot;, f&quot;{args.host}:{args.port}&quot;, &quot;-w&quot;, f&quot;{args.n_workers}&quot;, &quot;-k&quot;, &quot;uvicorn.workers.UvicornWorker&quot;, &quot;server:app&quot;\n    ]\n    run_command(cmd, env_map)\n\ndef main():\n    args = parse_args()\n    run_server(args)\n\nif __name__ == &quot;__main__&quot;:\n    main()\n<\/code><\/pre>\n<p>Run like<\/p>\n<pre><code>python server.py --artifacts-destination s3:\/\/mlflow-mr --default-artifact-root s3:\/\/mlflow-mr --serve-artifacts\n<\/code><\/pre>\n<p>Then navigate to your browser and see the tracking server running! This allows you to insert custom FastAPI middleware in front of the tracking server<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1648708838090,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71687131",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72454747,
        "Question_title":"Problem when loading a xgboost model from mlflow registry",
        "Question_body":"<p>I create a xgboost classifier:<\/p>\n<pre><code>   xg_reg = xgb.XGBClassifier(objective ='reg:squarederror',  learning_rate = 0.1,\n                max_depth = 20, alpha = 10, n_estimators = 50, use_label_encoder=False)\n<\/code><\/pre>\n<p>After training the model, I am logging it to the MLFLow registry:<\/p>\n<pre><code>   mlflow.xgboost.log_model(\n        xgb_model = xg_reg, \n        artifact_path = &quot;xgboost-models&quot;,\n        registered_model_name = &quot;xgb-regression-model&quot;\n    )\n<\/code><\/pre>\n<p>In the remote UI, I can see the logged model:<\/p>\n<pre><code>artifact_path: xgboost-models\nflavors:\n  python_function:\n    data: model.xgb\n    env: conda.yaml\n    loader_module: mlflow.xgboost\n    python_version: 3.7.9\n  xgboost:\n    code: null\n    data: model.xgb\n    model_class: xgboost.sklearn.XGBClassifier\n    xgb_version: 1.5.2\nmlflow_version: 1.25.1\nmodel_uuid: 5fd42554cf184d8d96afae34dbb96de2\nrun_id: acdccd9f610b4c278b624fca718f76b4\nutc_time_created: '2022-05-17 17:54:53.039242\n<\/code><\/pre>\n<p>Now, on the server side, to load the logged model:<\/p>\n<pre><code>   model = mlflow.xgboost.load_model(model_uri=model_path)\n<\/code><\/pre>\n<p>which loads OK, but the model type is<\/p>\n<blockquote>\n<p>&lt;xgboost.core.Booster object at 0x00000234DBE61D00&gt;<\/p>\n<\/blockquote>\n<p>and the predictions are numpy.float32 (eg 0.5) instead of int64 (eg 0, 1) for the original model.<\/p>\n<p>Any ideas what can be wrong? Many thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654036554297,
        "Question_score":1,
        "Question_tags":"python|xgboost|mlflow",
        "Question_view_count":163,
        "Owner_creation_time":1369252294280,
        "Owner_last_access_time":1663789810223,
        "Owner_location":null,
        "Owner_reputation":324,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It turns out this was caused by using different versions of mlflow. The model was uploaded to registry with the newest version but was loaded with a previous one. When updated the server to load it, it now works! :)<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1655934198727,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72454747",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68718719,
        "Question_title":"How can I retrive the model.pkl in the experiment in Databricks",
        "Question_body":"<p>I want to retrieve the pickle off my trained model, which I know is in the run file inside my experiments in Databricks.<\/p>\n<p>It seems that the <code>mlflow.pyfunc.load_model<\/code> can only do the <code>predict<\/code> method.<\/p>\n<p>There is an option to directly access the pickle?<\/p>\n<p>I also tried to use the path in the run using the <code>pickle.load(path)<\/code> (example of path: dbfs:\/databricks\/mlflow-tracking\/20526156406\/92f3ec23bf614c9d934dd0195\/artifacts\/model\/model.pkl).<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1628544411170,
        "Question_score":1,
        "Question_tags":"python|azure|databricks|datastore|mlflow",
        "Question_view_count":3081,
        "Owner_creation_time":1522076554450,
        "Owner_last_access_time":1663901377960,
        "Owner_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Owner_reputation":96,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I recently found the solution which can be done by the following two approaches:<\/p>\n<ol>\n<li>Use the customized predict function at the moment of saving the model (check <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">databricks<\/a> documentation for more details).<\/li>\n<\/ol>\n<p>example give by Databricks<\/p>\n<pre><code>class AddN(mlflow.pyfunc.PythonModel):\n\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input):\n        return model_input.apply(lambda column: column + self.n)\n# Construct and save the model\nmodel_path = &quot;add_n_model&quot;\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n<\/code><\/pre>\n<ol start=\"2\">\n<li>Load the model artefacts as we are downloading the artefact:<\/li>\n<\/ol>\n<pre><code>from mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\n\ntmp_path = client.download_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path='model\/model.pkl')\n\nf = open(tmp_path,'rb')\n\nmodel = pickle.load(f)\n\nf.close()\n\n \n\nclient.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;&quot;)\n\nclient.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;model&quot;)\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1629748421287,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1643054905512,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68718719",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70544873,
        "Question_title":"mlflow serving r models failed if use LDA instead of linear regression",
        "Question_body":"<p>I have a mlflow question regarding to serve R models, below is my code to train and serialize the R model.<\/p>\n<pre><code>library(mlflow)\nlibrary(caret)\nlibrary(carrier)\n\nset.seed(101)\nsample &lt;- createDataPartition(iris$Species, p=0.80, list=FALSE)\niris_train &lt;- iris[sample,]\niris_test &lt;- iris[-sample,]\n\n\nwith(mlflow_start_run()  , {\n  control &lt;- trainControl(method='cv', number=10)\n  metric &lt;- 'Accuracy'\n\n  # Linear Discriminant Analysis (LDA)\n  model &lt;- train(Species~., data=iris_train, method='lda', trControl=control, metric=metric,\n                preProcess=c(&quot;center&quot;, &quot;scale&quot;))\n  fn &lt;- crate(~ caret::predict.train(model, .x), model = model)\n  # fn &lt;- crate(~ stats::predict(model, .x), model = model)\n  iris_prediction &lt;- predict(model, iris_test)\n\n  mlflow_log_model(model = fn, artifact_path=&quot;model&quot;)\n})\n\n<\/code><\/pre>\n<p>However, when I serve it with the API request,<\/p>\n<pre><code>curl --location --request POST 'localhost:5000\/invocations' \\\n--header 'Content-Type: application\/json' \\\n--data-raw '{&quot;columns&quot;: [&quot;Sepal.Length&quot;, &quot;Sepal.With&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;], &quot;data&quot;: [[5.2, 4.1, 1.5, 0.1], [6.4, 2.8, 5.6, 2.1], [7.9, 3.8, 6.4, 2.0], [6.7, 3.1, 5.6, 2.4], [6.3, 3.4, 5.6, 2.4]]}'\n<\/code><\/pre>\n<p>I have the following error:\n<code>Invalid Request.  object 'Sepal.Width' not found<\/code><\/p>\n<p>Note, if I change the training to use linear regression model,<\/p>\n<pre><code>with(mlflow_start_run(), {\n  model &lt;- lm(Sepal.Width ~ Sepal.Length, iris)\n  mlflow_log_model(\n    crate(~ stats::predict(model, .x), model=model), &quot;model&quot;)\n})\n<\/code><\/pre>\n<p>The same API request, everything works fine.<\/p>\n<p>Can anyone help me find any clue? I am not that familiar with R.<\/p>\n<p>Thanks a lot!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640975828923,
        "Question_score":1,
        "Question_tags":"r|mlflow",
        "Question_view_count":57,
        "Owner_creation_time":1383535151877,
        "Owner_last_access_time":1663898770570,
        "Owner_location":null,
        "Owner_reputation":502,
        "Owner_up_votes":24,
        "Owner_down_votes":1,
        "Owner_views":40,
        "Question_last_edit_time":null,
        "Answer_body":"<p>it turns out the issue is a typo in curl script. it's Sepal.Width not Sepal.With.<\/p>\n<pre><code>curl --location --request POST 'localhost:5000\/invocations' \\\n--header 'Content-Type: application\/json' \\\n--data-raw '{&quot;columns&quot;: [&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;], &quot;data&quot;: [[5.2, 4.1, 1.5, 0.1], [6.4, 2.8, 5.6, 2.1], [7.9, 3.8, 6.4, 2.0], [6.7, 3.1, 5.6, 2.4], [6.3, 3.4, 5.6, 2.4]]}'\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1641228071910,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70544873",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":57199472,
        "Question_title":"Is it possible to set\/change mlflow run name after run initial creation?",
        "Question_body":"<p>I could not find a way yet of setting the runs name after the first start_run for that run (we can pass a name there). <\/p>\n\n<p>I Know we can use tags but that is not the same thing. I would like to add a run relevant name, but very often we know the name only after run evaluation or while we're running the run interactively in notebook for example.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1564049391490,
        "Question_score":11,
        "Question_tags":"mlflow",
        "Question_view_count":7689,
        "Owner_creation_time":1393062808773,
        "Owner_last_access_time":1618438983790,
        "Owner_location":"Portugal",
        "Owner_reputation":133,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":1564533527096,
        "Answer_body":"<p>It is possible to edit run names from the MLflow UI. First, click into the run whose name you'd like to edit.<\/p>\n\n<p>Then, edit the run name by clicking the dropdown next the run name (i.e. the downward-pointing caret in this image):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/sl6Qs.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sl6Qs.png\" alt=\"Rename run dropdown\"><\/a><\/p>\n\n<p>There's currently no stable public API for setting run names - however, you can programmatically set\/edit run names by setting the tag with key <code>mlflow.runName<\/code>, which is what the UI (currently) does under the hood.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1564380155503,
        "Answer_score":9,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57199472",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":58234777,
        "Question_title":"MLflow remote execution on databricks from windows creates an invalid dbfs path",
        "Question_body":"<p>I'm researching the use of MLflow as part of our data science initiatives and I wish to set up a minimum working example of remote execution on databricks from windows.<\/p>\n\n<p>However, when I perform the remote execution a path is created locally on windows in the MLflow package which is sent to databricks. This path specifies the upload location of the '.tar.gz' file corresponding to the Github repo containing the MLflow Project. In cmd this has a combination of '\\' and '\/', but on databricks there are no separators at all in this path, which raises the 'rsync: No such file or directory (2)' error.<\/p>\n\n<p>To be more general, I reproduced the error using an MLflow standard example and following this <a href=\"https:\/\/docs.databricks.com\/applications\/mlflow\/projects.html\" rel=\"nofollow noreferrer\">guide<\/a> from databricks. The MLflow example is the <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/sklearn_elasticnet_wine\" rel=\"nofollow noreferrer\">sklearn_elasticnet_wine<\/a>, but I had to add a default value to a parameter so I forked it and the MLproject which can be executed remotely can be found at (<a href=\"https:\/\/github.com\/aestene\/mlflow\/tree\/master\/examples\/sklearn_elasticnet_wine\" rel=\"nofollow noreferrer\">forked repo<\/a>).<\/p>\n\n<p>The Project can be executed remotely by the following command (assuming a databricks instance has been set up)<\/p>\n\n<pre><code>mlflow run https:\/\/github.com\/aestene\/mlflow#examples\/sklearn_elasticnet_wine -b databricks -c db-clusterconfig.json --experiment-id &lt;insert-id-here&gt;\n<\/code><\/pre>\n\n<p>where \"db-clusterconfig.json\" correspond to the cluster to set up in databricks and is in this example set to<\/p>\n\n<pre><code>{\n    \"autoscale\": {\n        \"min_workers\": 1,\n        \"max_workers\": 2\n    },\n    \"spark_version\": \"5.5.x-scala2.11\",\n    \"node_type_id\": \"Standard_DS3_v2\",\n    \"driver_node_type_id\": \"Standard_DS3_v2\",\n    \"ssh_public_keys\": [],\n    \"custom_tags\": {},\n    \"spark_env_vars\": {\n        \"PYSPARK_PYTHON\": \"\/databricks\/python3\/bin\/python3\"\n    }\n}\n<\/code><\/pre>\n\n<p>When running the project remotely, this is the output in cmd:<\/p>\n\n<pre><code>2019\/10\/04 10:09:50 INFO mlflow.projects: === Fetching project from https:\/\/github.com\/aestene\/mlflow#examples\/sklearn_elasticnet_wine into C:\\Users\\ARNTS\\AppData\\Local\\Temp\\tmp2qzdyq9_ ===\n2019\/10\/04 10:10:04 INFO mlflow.projects.databricks: === Uploading project to DBFS path \/dbfs\\mlflow-experiments\\3947403843428882\\projects-code\\aa5fbb4769e27e1be5a983751eb1428fe998c3e65d0e66eb9b4c77355076f524.tar.gz ===\n2019\/10\/04 10:10:05 INFO mlflow.projects.databricks: === Finished uploading project to \/dbfs\\mlflow-experiments\\3947403843428882\\projects-code\\aa5fbb4769e27e1be5a983751eb1428fe998c3e65d0e66eb9b4c77355076f524.tar.gz ===\n2019\/10\/04 10:10:05 INFO mlflow.projects.databricks: === Running entry point main of project https:\/\/github.com\/aestene\/mlflow#examples\/sklearn_elasticnet_wine on Databricks ===\n2019\/10\/04 10:10:06 INFO mlflow.projects.databricks: === Launched MLflow run as Databricks job run with ID 8. Getting run status page URL... ===\n2019\/10\/04 10:10:18 INFO mlflow.projects.databricks: === Check the run's status at https:\/\/&lt;region&gt;.azuredatabricks.net\/?o=&lt;databricks-id&gt;#job\/8\/run\/1 ===\n<\/code><\/pre>\n\n<p>Where the DBFS path has a leading '\/' before the remaining are '\\'. <\/p>\n\n<p>The command spins up a cluster in databricks and is ready to execute the job, but ends up with the following error message on the databricks side:<\/p>\n\n<pre><code>rsync: link_stat \"\/dbfsmlflow-experiments3947403843428882projects-codeaa5fbb4769e27e1be5a983751eb1428fe998c3e65d0e66eb9b4c77355076f524.tar.gz\" failed: No such file or directory (2)\nrsync error: some files\/attrs were not transferred (see previous errors) (code 23) at main.c(1183) [sender=3.1.1]\n<\/code><\/pre>\n\n<p>Where we can see the same path but without the '\\' inserted. I narrowed down the creation of this path to this <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/projects\/databricks.py\" rel=\"nofollow noreferrer\">file<\/a> in the MLflow Github repo, where the following code creates the path (line 133):<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>dbfs_path = os.path.join(DBFS_EXPERIMENT_DIR_BASE, str(experiment_id),\n                                     \"projects-code\", \"%s.tar.gz\" % tarfile_hash)\ndbfs_fuse_uri = os.path.join(\"\/dbfs\", dbfs_path)\n<\/code><\/pre>\n\n<p>My current hypothesis is that <code>os.path.join()<\/code> in the first line joins the string together in a \"windows fashion\" such that they have backslashes. Then the following call to <code>os.path.join()<\/code> adds a '\/'. The databricks file system is then unable to handle this path and something causes the 'tar.gz' file to not be properly uploaded or to be accessed at the wrong path. <\/p>\n\n<p>It should also be mentioned that the project runs fine locally.<\/p>\n\n<p>I'm running the following versions:<\/p>\n\n<p>Windows 10<\/p>\n\n<p>Python 3.6.8<\/p>\n\n<p>MLflow 1.3.0 (also replicated the fault with 1.2.0)<\/p>\n\n<p>Any feedback or suggestions are greatly appreciated!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1570185592420,
        "Question_score":2,
        "Question_tags":"databricks|mlflow",
        "Question_view_count":350,
        "Owner_creation_time":1570173110493,
        "Owner_last_access_time":1663917124557,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Thanks for the catch, you're right that using <code>os.path.join<\/code> when working with DBFS paths is incorrect, resulting in a malformed path that breaks project execution. I've filed to <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1926\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/1926<\/a> track this, if you're interested in making a bugfix PR (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst\" rel=\"nofollow noreferrer\">see the MLflow contributor guide for info on how to do this<\/a>) to replace <code>os.path.join<\/code> here with <code>os.posixpath.join<\/code> I'd be happy to review :)<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1570752025483,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58234777",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":59194004,
        "Question_title":"MLflow - Serving model by reference to model registry",
        "Question_body":"<p>I'm having an issue to serve a model with reference to model registry. According to help, the path should look like this: <\/p>\n\n<p>models:\/model_name\/stage<\/p>\n\n<p>When I type in terminal: <br>\n<code>mlflow models serve -m models:\/ml_test_model1\/Staging --no-conda -h 0.0.0.0 -p 5003<\/code><\/p>\n\n<p>I got the error: <br>\n<code>mlflow.exceptions.MlflowException: Not a proper models:\/ URI: models:\/ml_test_model1\/Staging\/MLmodel. Models URIs must be of the form 'models:\/&lt;model_name&gt;\/&lt;version or stage&gt;'.<\/code><\/p>\n\n<p>Model is registered and visible in db and server. <br> \nIf I put absolute path, it works (experiment_id\/run_id\/artifacts\/model_name).<\/p>\n\n<p>mlflow version: 1.4 <br>\nPython version: 3.7.3<\/p>\n\n<p>Is it matter of some environmental settings or something different?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1575544859307,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":1225,
        "Owner_creation_time":1575543357813,
        "Owner_last_access_time":1596719149423,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>That style of referencing model artefacts is fixed from mlflow v1.5 (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/2067\" rel=\"nofollow noreferrer\">Bug Fix<\/a>).<\/p>\n\n<p>You'll need to run <code>mlflow db upgrade &lt;db uri&gt;<\/code> to refresh your schemas before restarting your mlflow server.<\/p>\n\n<p>You may find listing registered models helpful:<\/p>\n\n<p><code>&lt;server&gt;:&lt;port&gt;\/api\/2.0\/preview\/mlflow\/registered-models\/list<\/code><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1577232243980,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1577251898636,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59194004",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":63920599,
        "Question_title":"PowerBI and MLflow integration (through AzureML)",
        "Question_body":"<p>I'm currently trying to integrate an ML model currently deployed as a webservice on AzureML with PowerBI.<\/p>\n<p>I see that it can be <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#invoking-the-azure-ml-model-in-power-bi\" rel=\"nofollow noreferrer\">integrated<\/a> but the model requires the addition of a schema file when it is <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#schema-discovery-for-machine-learning-models\" rel=\"nofollow noreferrer\">being deployed as a webservice<\/a>. Without this, the model can't be viewed in PowerBI.<\/p>\n<p>The problem that I have come up against is that I use MLflow to log ML model performances and subsequently to deploy a selected model onto AzureML as a webservice using MLflow's AzureML integration - mlflow.azureml.deploy(). This unfortunately doesn't have the option to define a schema file before the model is deployed, thus resulting in no model being available in PowerBI as it lacks the required schema file.<\/p>\n<p>My options seem to be:<\/p>\n<ol>\n<li>Find a workaround, possibly using the working <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-serving\" rel=\"nofollow noreferrer\">REST api of the model in a power query<\/a>.<\/li>\n<li>Rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow.<\/li>\n<\/ol>\n<p>I thought I would ask to see if I am maybe missing something as I can't find a workaround using my current code to define a schema file in MLflow when deploying with mlflow.azureml.deploy().<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1600261190477,
        "Question_score":0,
        "Question_tags":"azure|powerbi|mlflow|azure-machine-learning-service",
        "Question_view_count":405,
        "Owner_creation_time":1600260166047,
        "Owner_last_access_time":1615561616230,
        "Owner_location":null,
        "Owner_reputation":15,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":1600855880503,
        "Answer_body":"<p>Point number 2 is the way we solved this issue. Instead of using MLflow to deploy to a scoring service on Azure, we wrote a custom code which load MLflow model when container is initialised.<\/p>\n<p>Scoring code is something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nfrom mlflow.pyfunc import load_model\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\ndef init():\n    global model\n    model = load_model(os.path.join(os.environ.get(&quot;AZUREML_MODEL_DIR&quot;), &quot;awesome_model&quot;))\n\n@input_schema('data', NumpyParameterType(input_sample))\n@output_schema(NumpyParameterType(output_sample))\n\ndef run(data):\n    return model.predict(data)\n<\/code><\/pre>",
        "Answer_comment_count":1,
        "Answer_creation_time":1600604920243,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1600855957376,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63920599",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71846804,
        "Question_title":"How to I track loss at epoch using mlflow\/tensorflow?",
        "Question_body":"<p>I want to use mlflow to track the development of a TensorFlow model. How do I log the loss at each epoch? I have written the following code:<\/p>\n<pre><code>mlflow.set_tracking_uri(tracking_uri)\n\nmlflow.set_experiment(&quot;\/deep_learning&quot;)\nwith mlflow.start_run():\n    mlflow.log_param(&quot;batch_size&quot;, batch_size)\n    mlflow.log_param(&quot;learning_rate&quot;, learning_rate)\n    mlflow.log_param(&quot;epochs&quot;, epochs)\n    mlflow.log_param(&quot;Optimizer&quot;, opt)\n    mlflow.log_metric(&quot;train_loss&quot;, train_loss)\n    mlflow.log_metric(&quot;val_loss&quot;, val_loss)\n    mlflow.log_metric(&quot;test_loss&quot;, test_loss)\n    mlflow.log_metric(&quot;test_mse&quot;, test_mse)\n    mlflow.log_artifacts(&quot;.\/model&quot;)\n<\/code><\/pre>\n<p>If I change the train_loss and val_loss to<\/p>\n<pre><code>train_loss = history.history['loss']\nval_loss = history.history['val_loss']\n<\/code><\/pre>\n<p>I get the following error:<\/p>\n<pre><code>mlflow.exceptions.MlflowException: Got invalid value [12.041399002075195] for metric 'train_loss' (timestamp=1649783654667). Please specify value as a valid double (64-bit floating point)\n<\/code><\/pre>\n<p>How to I save the the loss and the val_loss at all epochs, so I can visualise a learning curve within mlflow?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649783963393,
        "Question_score":1,
        "Question_tags":"python|tensorflow|keras|tf.keras|mlflow",
        "Question_view_count":320,
        "Owner_creation_time":1507645492477,
        "Owner_last_access_time":1655115322463,
        "Owner_location":"Liverpool, UK",
        "Owner_reputation":77,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As you can read <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.keras.html#module-mlflow.keras\" rel=\"nofollow noreferrer\">here<\/a>. You can use <code>mlflow.tensorflow.autolog()<\/code> and this, (from doc):<\/p>\n<blockquote>\n<p>Enables (or disables) and configures autologging from Keras to MLflow. Autologging captures the following information:<\/p>\n<blockquote>\n<p>fit() or fit_generator() parameters; optimizer name; learning rate; epsilon\n...<\/p>\n<\/blockquote>\n<\/blockquote>\n<p>For example:<\/p>\n<pre><code># !pip install mlflow\nimport tensorflow as tf\nimport mlflow\nimport numpy as np\n\n\nX_train = np.random.rand(100,100)\ny_train = np.random.randint(0,10,100)\n    \n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.Input(100,))\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(rate=.4))\nmodel.add(tf.keras.layers.Dense(10, activation='sigmoid'))        \nmodel.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n              optimizer='Adam', \n              metrics=['accuracy'])\nmodel.summary()\n\n\nmlflow.tensorflow.autolog()\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=50)\n<\/code><\/pre>\n<p>Or as you mention in the comment you can use <code>mlflow.set_tracking_uri()<\/code> like below:<\/p>\n<pre><code>mlflow.set_tracking_uri('http:\/\/127.0.0.1:5000')\ntracking_uri = mlflow.get_tracking_uri()\nwith mlflow.start_run(run_name='PARENT_RUN') as parent_run:\n    batch_size=50\n    history = model.fit(X_train, y_train, epochs=2, batch_size=batch_size)\n    mlflow.log_param(&quot;batch_size&quot;, batch_size)  \n<\/code><\/pre>\n<p>For getting results:<\/p>\n<pre><code>!mlflow ui\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>[....] [...] [INFO] Starting gunicorn 20.1.0\n[....] [...] [INFO] Listening at: http:\/\/127.0.0.1:5000 (****)\n[....] [...] [INFO] Using worker: sync\n[....] [...] [INFO] Booting worker with pid: ****\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9XXoi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9XXoi.png\" alt=\"enter image description here\" \/><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/V2tvM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/V2tvM.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1649809588953,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1649895732376,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71846804",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":64513552,
        "Question_title":"How to have multiple MLFlow runs in parallel?",
        "Question_body":"<p>I'm not very familiar with parallelization in Python and I'm getting an error when trying to train a model on multiple training folds in parallel. Here's a simplified version of my code:<\/p>\n<pre><code>def train_test_model(fold):\n    # here I train the model etc...\n    \n    # now I want to save the parameters and metrics\n    with mlflow.start_run():\n        mlflow.log_param(&quot;run_name&quot;, run_name)\n        mlflow.log_param(&quot;modeltype&quot;, modeltype)\n        # and so on...\n\nif __name__==&quot;__main__&quot;:\n    pool = ThreadPool(processes = num_trials)\n    # run folds in parallel\n    pool.map(lambda fold:train_test_model(fold), folds)\n<\/code><\/pre>\n<p>I'm getting the following error:<\/p>\n<pre><code>Exception: Run with UUID 23e9bb6d22674a518e48af9c51252860 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True\n<\/code><\/pre>\n<p>The <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.start_run\" rel=\"nofollow noreferrer\">documentation<\/a> says that <code>mlflow.start_run()<\/code> starts a new run and makes it active which is the root of my problem. Every thread starts a MLFlow run for its corresponding fold and makes it active while I need the runs to run in parallel i.e. all be active(?) and save parameters\/metrics of the corresponding fold. How can I solve that issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603544320573,
        "Question_score":1,
        "Question_tags":"python|pyspark|parallel-processing|mlflow",
        "Question_view_count":2275,
        "Owner_creation_time":1515931819517,
        "Owner_last_access_time":1664047231110,
        "Owner_location":null,
        "Owner_reputation":177,
        "Owner_up_votes":126,
        "Owner_down_votes":1,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I found a solution, maybe it will be useful for someone else. You can see details with code examples here: <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/3592\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/3592<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1603877162673,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64513552",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":68533916,
        "Question_title":"what is features and how to interpret in RFormula",
        "Question_body":"<p>I am trying to understand what an RFormula is in MLflow or spark.<\/p>\n<p>I have found these:<\/p>\n<p><a href=\"https:\/\/george-jen.gitbook.io\/data-science-and-apache-spark\/rformula\" rel=\"nofollow noreferrer\">https:\/\/george-jen.gitbook.io\/data-science-and-apache-spark\/rformula<\/a>\n<a href=\"https:\/\/spark.apache.org\/docs\/latest\/api\/python\/reference\/api\/pyspark.ml.feature.RFormula.html\" rel=\"nofollow noreferrer\">https:\/\/spark.apache.org\/docs\/latest\/api\/python\/reference\/api\/pyspark.ml.feature.RFormula.html<\/a><\/p>\n<p>but still cannot understand how to interpret an RFormula fully. I am not sure how to interpret the below table\n<a href=\"https:\/\/i.stack.imgur.com\/guLyU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/guLyU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>based on the formula &quot;y ~ x+ s&quot;, y is related to x and s, but in the table when y=0 and x=0 and s =a (i.e. third row), then the features is [0,1] and label is 0, so how shall I interpret this.<\/p>\n<p>I have found <a href=\"https:\/\/stackoverflow.com\/questions\/61290042\/spark-rformula-interpretation\">this<\/a> but still cannot understand my way through this problem.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1627319315683,
        "Question_score":0,
        "Question_tags":"r|pyspark|mlflow",
        "Question_view_count":78,
        "Owner_creation_time":1375186444010,
        "Owner_last_access_time":1662797989807,
        "Owner_location":"Auckland, New Zealand",
        "Owner_reputation":912,
        "Owner_up_votes":334,
        "Owner_down_votes":1,
        "Owner_views":288,
        "Question_last_edit_time":1627371880352,
        "Answer_body":"<p>So your label is y. You parse x and s in rformula.<\/p>\n<p>x stays the same:<\/p>\n<pre><code>+-----------+---+\n|      x    | x |\n+-----------+---+\n|     1.0   |1.0|\n|     2.0   |2.0|\n|     0.0   |0.0|\n+-----------+---+\n<\/code><\/pre>\n<p>s:<\/p>\n<pre><code>+-----------+---+\n|       s   | s |\n+-----------+---+\n|       a   |1.0|\n|       b   |0.0|\n|       a   |1.0|\n+-----------+---+\n<\/code><\/pre>\n<p>I hope I could answer you question.\nRformula just converts the strings, standarize them and parse them into a vector.<\/p>",
        "Answer_comment_count":6,
        "Answer_creation_time":1627334164713,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1627334932660,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68533916",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65316586,
        "Question_title":"get the run id for an mlflow experiment with the name?",
        "Question_body":"<p>I currently created an experiment in mlflow and created multiple runs in the experiment.<\/p>\n<pre><code>from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nimport mlflow\n\nexperiment_name=&quot;experiment-1&quot;\nmlflow.set_experiment(experiment_name)\n\nno_of_trees=[100,200,300]\ndepths=[2,3,4]\nfor trees in no_of_trees:\n    for depth in depths:\n        with mlflow.start_run() as run:\n            model=RandomForestRegressor(n_estimators=trees, criterion='mse',max_depth=depth)\n            model.fit(x_train, y_train)\n            predictions=model.predict(x_cv)\n            mlflow.log_metric('rmse',mean_squared_error(y_cv, predictions))\n<\/code><\/pre>\n<p>after creating the runs, I wanted to get the best run_id for this experiment. for now, I can get the best run by looking at the UI of mlflow but how can we do right the program?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1608086727487,
        "Question_score":6,
        "Question_tags":"python|mlflow",
        "Question_view_count":6374,
        "Owner_creation_time":1479190327737,
        "Owner_last_access_time":1660584310400,
        "Owner_location":"R G U K T , basar, Andhra Pradesh, India",
        "Owner_reputation":2470,
        "Owner_up_votes":265,
        "Owner_down_votes":22,
        "Owner_views":251,
        "Question_last_edit_time":null,
        "Answer_body":"<p>we can get the experiment id from the experiment name and we can use python API to get the best runs.<\/p>\n<pre><code>experiment_name = &quot;experiment-1&quot;\ncurrent_experiment=dict(mlflow.get_experiment_by_name(experiment_name))\nexperiment_id=current_experiment['experiment_id']\n<\/code><\/pre>\n<p>By using the experiment id, we can get all the runs and we can sort them based on metrics like below. In the below code, rmse is my metric name (so it may be different for you based on metric name)<\/p>\n<pre><code>df = mlflow.search_runs([experiment_id], order_by=[&quot;metrics.rmse DESC&quot;])\nbest_run_id = df.loc[0,'run_id']\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1608086727487,
        "Answer_score":15,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65316586",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71679081,
        "Question_title":"How can I connect mlflow server via nginx ssl authentication?",
        "Question_body":"<p>System information\nOS Platform and Distribution: Windows 10\nMLflow installed: using pip\nMLflow version: version 1.24.0\n**Python version: Python 3.9.7 **<\/p>\n<p>Describe the problem\nI have created a docker-compose system with a backend\/artifact storages, mlflow server and nginx to add an authentication layer.<\/p>\n<pre><code>...\nmlflow:\n        restart: always\n        build: .\n        environment:\n            - AWS_ACCESS_KEY_ID=${MINIO_USR}\n            - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}       \n        expose:\n            - '5000'\n        networks:\n            - frontend\n            - backend\n        depends_on:\n            - storage                       \n        image: 'mlflow:Dockerfile'\n        container_name: mlflow_server_nginx\n\n    nginx:\n        restart: always\n        build: .\/nginx\n        container_name: mlflow_nginx\n        ports:\n            - 5043:443\n        links:\n            - mlflow:mlflow\n        volumes:\n            - 'path\/to\/nginx\/auth:\/etc\/nginx\/conf.d'\n            - 'path\/to\/nginx\/nginx.conf:\/etc\/nginx\/nginx.conf:ro'\n        networks:\n            - frontend\n        depends_on:\n            - mlflow\n<\/code><\/pre>\n<p>I have created an user\/password via htpasswd and a custom SSL CA (.pem\/.key) using openssl and my-mlflow.com server-name.<\/p>\n<p>When the docker-compose system is built i can access to mlflow UI via my browser. But when i try to create a new experiment using python trying diferent approaches, i get next errors:\nExecuted code 1:<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\n#os.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n#os.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate (_ssl.c:1108)')))\n<\/code><\/pre>\n<p>After read some notes in the documentation and realated issues I tryed next<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\n#os.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\nos.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLError(9, '[SSL] PEM lib (_ssl.c:4012)')))\n<\/code><\/pre>\n<p>Finally<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\nos.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n#os.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLCertVerificationError(&quot;hostname 'localhost' doesn't match '*.my-mlflow.com'&quot;)))\n<\/code><\/pre>\n<p>Can you give me some hints about how to solve it?<\/p>\n<p>Thank you very much!\nFernando....<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648650339347,
        "Question_score":1,
        "Question_tags":"python-3.x|docker|nginx|docker-compose|mlflow",
        "Question_view_count":625,
        "Owner_creation_time":1580841805373,
        "Owner_last_access_time":1663792613347,
        "Owner_location":"Seville, Spain",
        "Owner_reputation":33,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can set:<\/p>\n<pre><code>os.environ['MLFLOW_TRACKING_INSECURE_TLS'] = 'true'\n<\/code><\/pre>\n<p>And then try to get your cert-chain straight from there for production use.<\/p>\n<p>Also see Documentation: <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#id19\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tracking.html#id19<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1652276299263,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1652448943407,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71679081",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60667610,
        "Question_title":"How to deploy mlflow model with data preprocessing(text data)",
        "Question_body":"<p>I have developed keras text classification model. I have preprocessed data(tokenization). I have logged trained model successfully(mlflow.keras.log_model). I have served model using mlflow serve. Now while doing prediction on text data I need to do preprocessing using same tokenizer object used for training.\nHow to preprocess test data and get predictions from served model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1584090517703,
        "Question_score":3,
        "Question_tags":"mlflow",
        "Question_view_count":1996,
        "Owner_creation_time":1498470936987,
        "Owner_last_access_time":1614914388693,
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can log a custom python model: \n<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1584552358667,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60667610",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":73127303,
        "Question_title":"Get Experiment that Created Model in MLflow",
        "Question_body":"<p>I want to get the name of the experiment that contains the run that created a registered MLflow model. How can I do this using MLflow, if I just have the name of the model and the version?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1658855429667,
        "Question_score":1,
        "Question_tags":"mlflow",
        "Question_view_count":44,
        "Owner_creation_time":1521856385820,
        "Owner_last_access_time":1664037995903,
        "Owner_location":"Sri Lanka",
        "Owner_reputation":820,
        "Owner_up_votes":389,
        "Owner_down_votes":1,
        "Owner_views":165,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As @Andre has said, I had to write my own function to achieve this,<\/p>\n<pre><code>def get_model_experiment(model_name, model_version):\n    # get run_id of the model version\n    run_id = mlflow_client.get_model_version(model_name, model_version).run_id\n\n    # get the experiment_id from the run_id\n    experiment_id = mlflow_client.get_run(run_id).info.experiment_id\n\n    # get the experiment name from the experiment_id\n    return mlflow_client.get_experiment(experiment_id).name\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1659073607990,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73127303",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71752458,
        "Question_title":"MLflow Experiments Tracking : local (dev tools - vscode) to databricks workspace",
        "Question_body":"<p>I had configured my databricks workspace in local using,<\/p>\n<p><code>databricks configure --profile &lt;profile_name&gt; --token<\/code><\/p>\n<p>by which I am able to list the clusters and create secret scope.<\/p>\n<p>But I am unable to create mlflow experiments. I had set the tracking uri to &quot;databricks&quot; and also tested with &quot;databricks\/&lt;profile_name&quot; and tested but i am unable to create or track any experiments on my databricks workspace.<\/p>\n<p>I get this following error;<\/p>\n<p><code>from mlflow.tracking import MlflowClient client = MlflowClient() mlflow.set_tracking_uri(&quot;databricks&quot;) experiment =  client.get_experiment_by_name('\/Shared\/test')<\/code><\/p>\n<p>MlflowException: API request to endpoint was successful but the response body was not in a valid JSON format. Response body: '&lt;!doctype html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;\/&gt;&lt;meta http-equiv=&quot;Content-Language&quot; content=&quot;en&quot;\/&gt;&lt;title&gt;Databricks - Sign In&lt;\/title&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=960&quot;\/&gt;&lt;link rel=&quot;icon&quot; type=&quot;image\/png&quot; href=&quot;\/favicon.ico&quot;\/&gt;&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text\/html; charset=UTF8&quot;\/&gt;&lt;link rel=&quot;icon&quot; href=&quot;\/favicon.ico&quot;&gt;&lt;script defer=&quot;defer&quot; src=&quot;\/login\/login.0ceb14c0.js&quot;&gt;&lt;\/script&gt;&lt;\/head&gt;&lt;body class=&quot;light-mode&quot;&gt;&lt;uses-legacy-bootstrap&gt;&lt;div id=&quot;login-page&quot;&gt;&lt;\/div&gt;&lt;\/uses-legacy-bootstrap&gt;&lt;\/body&gt;&lt;\/html&gt;'<\/p>\n<p>Could someone help me on what I am missing here?<\/p>\n<p>I am expecting to create\/track mlflow experiements in databricks workspace via dev-tools(vscode).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649164485623,
        "Question_score":0,
        "Question_tags":"databricks|azure-databricks|mlflow",
        "Question_view_count":220,
        "Owner_creation_time":1515569029903,
        "Owner_last_access_time":1655721276247,
        "Owner_location":null,
        "Owner_reputation":5,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I had the same problem while trying to load a model from model registry with mismatching versions (client 1.22.0).<\/p>\n<p>I had to downgrade the client version to make it work.<\/p>\n<p>Downgraded first the client to 1.21 and then server to 1.20<\/p>\n<p>Refer - <a href=\"https:\/\/docs.databricks.com\/dev-tools\/api\/latest\/mlflow.html#operation\/transition-model-version-stage\" rel=\"nofollow noreferrer\">https:\/\/docs.databricks.com\/dev-tools\/api\/latest\/mlflow.html#operation\/transition-model-version-stage<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1649225844710,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71752458",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":66814885,
        "Question_title":"Serve online learning models with mlflow",
        "Question_body":"<p>It is not clear to me if one could use mlflow to serve a model that is evolving continuously based on its previous predictions.<\/p>\n<p>I need to be able to query a model in order to make a prediction on a sample of data which is the basic use of mlflow serve. However I also want the model to be updated internaly now that it has seen new data.<\/p>\n<p>Is it possible or does it need a FR ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616753577110,
        "Question_score":1,
        "Question_tags":"python-3.x|mlflow",
        "Question_view_count":360,
        "Owner_creation_time":1515156959093,
        "Owner_last_access_time":1630697909597,
        "Owner_location":"Fairbanks, AK, United States",
        "Owner_reputation":76,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I think that you should be able to do that by implementing the custom python model or custom flavor, as it's described in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">documentation<\/a>.  In this case you need to create a class that is inherited from <code>mlflow.pyfunc.PythonModel<\/code>, and implement the <code>predict<\/code> method, and inside that method you're free to do anything.  Here is just simple example from documentation:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class AddN(mlflow.pyfunc.PythonModel):\n\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input):\n        return model_input.apply(lambda column: column + self.n)\n<\/code><\/pre>\n<p>and this model is then could be saved &amp; loaded again just as normal models:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Construct and save the model\nmodel_path = &quot;add_n_model&quot;\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n<\/code><\/pre>",
        "Answer_comment_count":3,
        "Answer_creation_time":1618124640630,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66814885",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67786052,
        "Question_title":"Log Pickle files as a part of Mlflow run",
        "Question_body":"<p>I am running an MLflow experiment as a part of it I would like to log a few artifacts as a python pickle.<\/p>\n<p>Ex: Trying out different categorical encoders, so wanted to log the encoder objects as a pickle file.<\/p>\n<p>Is there a way to achieve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622538922663,
        "Question_score":3,
        "Question_tags":"python|databricks|azure-databricks|mlflow",
        "Question_view_count":1843,
        "Owner_creation_time":1411361217027,
        "Owner_last_access_time":1662192778187,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":569,
        "Owner_up_votes":41,
        "Owner_down_votes":3,
        "Owner_views":123,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There are two functions for there:<\/p>\n<ol>\n<li><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifact\" rel=\"nofollow noreferrer\">log_artifact<\/a> - to log a local file or directory as an artifact<\/li>\n<li><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifacts\" rel=\"nofollow noreferrer\">log_artifacts<\/a> - to log a contents of a local directory<\/li>\n<\/ol>\n<p>so it would be as simple as:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with mlflow.start_run():\n    mlflow.log_artifact(&quot;encoder.pickle&quot;)\n<\/code><\/pre>\n<p>And you will need to use the <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">custom MLflow model<\/a> to use that pickled file, something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow.pyfunc\n\nclass my_model(mlflow.pyfunc.PythonModel):\n    def __init__(self, encoders):\n        self.encoders = encoders\n\n    def predict(self, context, model_input):\n        _X = ...# do encoding using self.encoders.\n        return str(self.ctx.predict([_X])[0])\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1622542563553,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67786052",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65145994,
        "Question_title":"Saving an Matlabplot as an MLFlow artifact",
        "Question_body":"<p>I am using DataBricks and Spark 7.4ML,<\/p>\n<p>The following code successfully logs the params and metrics, and I can see the ROCcurve.png in the MLFLOW gui (just the item in the tree below the model). But the actually plot is blank. Why?<\/p>\n<pre><code>with mlflow.start_run(run_name=&quot;logistic-regression&quot;) as run:\n  pipeModel = pipe.fit(trainDF)\n  mlflow.spark.log_model(pipeModel, &quot;model&quot;)\n  predTest = pipeModel.transform(testDF)\n  predTrain = pipeModel.transform(trainDF)\n  evaluator=BinaryClassificationEvaluator(labelCol=&quot;arrivedLate&quot;)\n  trainROC = evaluator.evaluate(predTrain)\n  testROC = evaluator.evaluate(predTest)\n  print(f&quot;Train ROC: {trainROC}&quot;)\n  print(f&quot;Test ROC: {testROC}&quot;)\n  mlflow.log_param(&quot;Dataset Name&quot;, &quot;Flights &quot; + datasetName)\n  mlflow.log_metric(key=&quot;Train ROC&quot;, value=trainROC)\n  mlflow.log_metric(key=&quot;Test ROC&quot;, value=testROC)\n\n  lrModel = pipeModel.stages[3]\n  trainingSummary = lrModel.summary\n  roc = trainingSummary.roc.toPandas()\n  plt.plot(roc['FPR'],roc['TPR'])\n  plt.ylabel('False Positive Rate')\n  plt.xlabel('True Positive Rate')\n  plt.title('ROC Curve')\n  plt.show()\n  plt.savefig(&quot;ROCcurve.png&quot;)\n  mlflow.log_artifact(&quot;ROCcurve.png&quot;)\n  plt.close()\n  \n  display(predTest.select(stringCols + [&quot;arrivedLate&quot;, &quot;prediction&quot;]))\n<\/code><\/pre>\n<p>What the notebook shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What the MLFlow shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1607094596640,
        "Question_score":8,
        "Question_tags":"apache-spark|matplotlib|pyspark|databricks|mlflow",
        "Question_view_count":5219,
        "Owner_creation_time":1316705139197,
        "Owner_last_access_time":1663085821243,
        "Owner_location":"Boston, MA",
        "Owner_reputation":6711,
        "Owner_up_votes":353,
        "Owner_down_votes":3,
        "Owner_views":819,
        "Question_last_edit_time":1607191847983,
        "Answer_body":"<p>Put <code>plt.show()<\/code> after <code>plt.savefig()<\/code> - <code>plt.show()<\/code> will remove your plot because it is shown already.<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1607094854147,
        "Answer_score":7,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65145994",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71985302,
        "Question_title":"How to see results via `mlflow ui` for experiments logged on a other server?",
        "Question_body":"<p>I was running ML experiments on a ssh server, the experiments were logged via mlflow and stored in local <code>mlruns<\/code> on the server.<\/p>\n<p>The code were just basic usage of mlflow and looks like this<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nimport mlflow\nimport matplotlib.pyplot as plt\n\nfrom pathlib import Path\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n\nif __name__ == '__main__':\n    Path('.\/plot').mkdir(exist_ok = True)\n    mlflow.set_experiment('demo-exp')\n    with mlflow.start_run():\n        x     = np.random.randn(10, 1)\n        y     = np.random.randn(10, 1)\n        model = LinearRegression().fit(x, y)\n        py    = model.predict(x)\n        r2    = r2_score(y, py)\n        plt.plot(y, py, '+')\n        plt.savefig('.\/plot\/result.png')\n        plt.close()\n\n        mlflow.log_metric('r2', r2)\n        mlflow.log_artifact('.\/plot\/result.png')\n<\/code><\/pre>\n<p>Now I want to see the logged metrics and artifactos from my own laptop, so I tried<\/p>\n<pre><code>mlflow ui \\\n    --default-artifact-root=sftp:\/\/\/USER_NAME@ADDRESS\/path\/to\/experiment\/mlruns \\\n    --port=8888 \\\n\n<\/code><\/pre>\n<p>However, it looks like nothing showed in <code>localhost:8888<\/code> on my own laptop.<\/p>\n<p>Anything I did wrong about the code and <code>mlflow ui<\/code> command?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650771165373,
        "Question_score":0,
        "Question_tags":"python|mlflow|mlops",
        "Question_view_count":358,
        "Owner_creation_time":1366379833543,
        "Owner_last_access_time":1663851694857,
        "Owner_location":"China,Shanghai.",
        "Owner_reputation":3157,
        "Owner_up_votes":52,
        "Owner_down_votes":7,
        "Owner_views":163,
        "Question_last_edit_time":null,
        "Answer_body":"<ul>\n<li>Run mlflow on a remote server<\/li>\n<\/ul>\n<pre><code>mlflow server --host 0.0.0.0 --port 8888\n<\/code><\/pre>\n<ul>\n<li>Configure ssh local port forward on your local machine, like this:\n<a href=\"https:\/\/stackoverflow.com\/questions\/9146457\/ssh-port-forwarding-in-a-ssh-config-file\">SSH Port forwarding in a ~\/.ssh\/config file?<\/a><\/li>\n<\/ul>\n<pre><code>LocalForward 8888 your_remote_machine_addr:8888\n<\/code><\/pre>\n<ul>\n<li>Connect to localhost:8888 on your browser<\/li>\n<\/ul>",
        "Answer_comment_count":0,
        "Answer_creation_time":1650951347493,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71985302",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67499339,
        "Question_title":"Create mlflow experiment: Run with UUID is already active",
        "Question_body":"<p>I'm trying to create a new experiment on mlflow but I have this problem:<\/p>\n<pre><code>Exception: Run with UUID l142ae5a7cf04a40902ae9ed7326093c is already active.\n\n<\/code><\/pre>\n<p>This is my code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\nimport mlflow.sklearn\nimport sys\n\nmlflow.set_experiment(&quot;New experiment 2&quot;)\n\nmlflow.set_tracking_uri('http:\/\/mlflow:5000')\nst= mlflow.start_run(run_name='Test2')\nid = st.info.run_id\nmlflow.log_metric(&quot;score&quot;, score)\nmlflow.sklearn.log_model(model, &quot;wineModel&quot;)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620805743553,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":675,
        "Owner_creation_time":1620748388617,
        "Owner_last_access_time":1620921887890,
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You have to run mlflow.end_run() to finish the first experiment. Then you can create another<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1620805914963,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67499339",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":67456172,
        "Question_title":"Unable to connect to MLFLOW_TRACKING_URI when running MLflow run in Docker container",
        "Question_body":"<p>I have setup a mlflow server locally at http:\/\/localhost:5000<\/p>\n<p>I followed the instructions at <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker<\/a> and tried to run the example docker with<\/p>\n<pre><code>\/mlflow\/examples\/docker$ mlflow run . -P alpha=0.5\n<\/code><\/pre>\n<p>but I encountered the following error.<\/p>\n<pre><code>2021\/05\/09 17:11:20 INFO mlflow.projects.docker: === Building docker image docker-example:7530274 ===\n2021\/05\/09 17:11:20 INFO mlflow.projects.utils: === Created directory \/tmp\/tmp9wpxyzd_ for downloading remote URIs passed to arguments of type 'path' ===\n2021\/05\/09 17:11:20 INFO mlflow.projects.backend.local: === Running command 'docker run --rm -v \/home\/mlf\/mlf\/0\/ae69145133bf49efac22b1d390c354f1\/artifacts:\/home\/mlf\/mlf\/0\/ae69145133bf49efac22b1d390c354f1\/artifacts -e MLFLOW_RUN_ID=ae69145133bf49efac22b1d390c354f1 -e MLFLOW_TRACKING_URI=http:\/\/localhost:5000 -e MLFLOW_EXPERIMENT_ID=0 docker-example:7530274 python train.py --alpha 0.5 --l1-ratio 0.1' in run with ID 'ae69145133bf49efac22b1d390c354f1' === \n\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/__init__.py:55: DeprecationWarning: MLflow support for Python 2 is deprecated and will be dropped in a future release. At that point, existing Python 2 workflows that use MLflow will continue to work without modification, but Python 2 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3 - see https:\/\/docs.python.org\/3\/howto\/pyporting.html for a migration guide.\n  &quot;for a migration guide.&quot;, DeprecationWarning)\nTraceback (most recent call last):\n  File &quot;train.py&quot;, line 56, in &lt;module&gt;\n    with mlflow.start_run():\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/client.py&quot;, line 96, in get_run\n    return self._tracking_client.get_run(run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 49, in get_run\n    return self.store.get_run(run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py&quot;, line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py&quot;, line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 133, in call_endpoint\n    host_creds=host_creds, endpoint=endpoint, method=method, params=json_body)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 70, in http_request\n    url=url, headers=headers, verify=verify, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 51, in request_with_ratelimit_retries\n    response = requests.request(**kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/api.py&quot;, line 58, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/sessions.py&quot;, line 508, in request\n    resp = self.send(prep, **send_kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/sessions.py&quot;, line 618, in send\n    r = adapter.send(request, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/adapters.py&quot;, line 508, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get?run_uuid=ae69145133bf49efac22b1d390c354f1&amp;run_id=ae69145133bf49efac22b1d390c354f1 (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f5cbd80d690&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))\n2021\/05\/09 17:11:22 ERROR mlflow.cli: === Run (ID 'ae69145133bf49efac22b1d390c354f1') failed ===\n<\/code><\/pre>\n<p>Any ideas how to fix this? I tried adding the following in MLproject file but it doesn't help<\/p>\n<pre><code>environment: [[&quot;network&quot;, &quot;host&quot;], [&quot;add-host&quot;, &quot;host.docker.internal:host-gateway&quot;]]\n<\/code><\/pre>\n<p>Thanks for your help! =)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620552530280,
        "Question_score":0,
        "Question_tags":"docker|mlflow",
        "Question_view_count":1151,
        "Owner_creation_time":1316620102630,
        "Owner_last_access_time":1643692547643,
        "Owner_location":null,
        "Owner_reputation":1308,
        "Owner_up_votes":177,
        "Owner_down_votes":1,
        "Owner_views":151,
        "Question_last_edit_time":1620554070856,
        "Answer_body":"<p>Run MLflow server such was that it will use your machine IP instead of <code>localhost<\/code>.  Then point the <code>mlflow run<\/code> to that IP instead of <code>http:\/\/localhost:5000<\/code>.   The main reason is that <code>localhost<\/code> of Docker process is its own, not your machine.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1620627546970,
        "Answer_score":1,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67456172",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70538098,
        "Question_title":"Databricks MLFlow AutoML XGBoost can't predict_proba()",
        "Question_body":"<p>I used AutoML in Databricks Notebooks for a binary classification problem and the winning model flavor was XGBoost (big surprise).<\/p>\n<p>The outputted model is of this variety:<\/p>\n<pre><code>mlflow.pyfunc.loaded_model:\n      artifact_path: model\n      flavor: mlflow.sklearn\n      run_id: 123456789\n<\/code><\/pre>\n<p>Any idea why when I use <code>model.predict_proba(X)<\/code>, I get this response?<\/p>\n<p><code>AttributeError: 'PyFuncModel' object has no attribute 'predict_proba'<\/code><\/p>\n<p>I know it is possible to get the probabilities because ROC\/AUC is a metric used for tuning the model. Any help would be amazing!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640912523883,
        "Question_score":1,
        "Question_tags":"pandas|scikit-learn|databricks|xgboost|mlflow",
        "Question_view_count":451,
        "Owner_creation_time":1562828557217,
        "Owner_last_access_time":1663796637890,
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":77,
        "Owner_up_votes":35,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I had the same issue with catboost model.\nThe way I solved it was by saving the artifacts in a local dir<\/p>\n<pre><code>import os\nfrom mlflow.tracking import MlflowClient\nclient = MlflowClient()\nlocal_dir = &quot;\/dbfs\/FileStore\/user\/models&quot;\nlocal_path = client.download_artifacts('run_id', &quot;model&quot;, local_dir)```\n\n```model_path = '\/dbfs\/FileStore\/user\/models\/model\/model.cb'\nmodel = CatBoostClassifier()\nmodel = model.load_model(model_path)\nmodel.predict_proba(test_set)```\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1646744187860,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70538098",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":71135260,
        "Question_title":"Python assert a function is called within a certain `with` statement's context",
        "Question_body":"<p>In python I would like to check that a given function is called within a <code>with<\/code> statement of a given type<\/p>\n<pre><code>class Bar:\n def __init__(self, x):\n  self.x = x\n def __enter__(self):\n  return self\n def __exit__(self, *a, **k):\n  pass\n\ndef foo(x):\n # assert that the enclosing context is an instance of bar\n # assert isinstance('enclosed context', Bar)\n print(x*2)\n\nwith Bar(1) as bar:\n foo(bar.x)\n<\/code><\/pre>\n<p>I could do something like enforcing an arg passed into <code>foo<\/code> and wrapping functions in a decorator i.e.<\/p>\n<pre><code>class Bar:\n def __init__(self, x):\n  self.x = x\n def __enter__(self):\n  return self\n def __exit__(self, *a, **k):\n  pass\n\ndef assert_bar(func):\n def inner(bar, *a, **k):\n  assert isinstance(bar, Bar)\n  return func(*a, **k)\n return inner\n\n\n@assert_bar\ndef foo(x):\n print(x*2)\n\nwith Bar(1) as bar:\n foo(bar, bar.x)\n\n<\/code><\/pre>\n<p>but then I would have to pass around <code>bar<\/code> everywhere.<\/p>\n<p>As a result I'm trying to see if there's a way to access the <code>with<\/code> context<\/p>\n<p>Note: The real world application of this is ensuring that <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.pyfunc.html#mlflow.pyfunc.log_model\" rel=\"nofollow noreferrer\"><code>mlflow.pyfunc.log_model<\/code><\/a> is called within an <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.ActiveRun\" rel=\"nofollow noreferrer\"><code>mlflow.ActiveRun<\/code><\/a> context, or it leaves an <code>ActiveRun<\/code> open, causing problems later on<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1644973389773,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":77,
        "Owner_creation_time":1473193743197,
        "Owner_last_access_time":1657404267333,
        "Owner_location":"London, United Kingdom",
        "Owner_reputation":617,
        "Owner_up_votes":21,
        "Owner_down_votes":2,
        "Owner_views":75,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Here's an ugly way to do it: global state.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class Bar:\n    active = 0\n    def __init__(self, x):\n        self.x = x\n    def __enter__(self):\n        Bar.active += 1\n        return self\n    def __exit__(self, *a, **k):\n        Bar.active -= 1\n\nfrom functools import wraps\n\ndef assert_bar(func):\n    @wraps(func)\n    def wrapped(*vargs, **kwargs):\n        if Bar.active &lt;= 0:\n            # raises even if asserts are disabled\n            raise AssertionError()\n        return func(*vargs, **kwargs)\n    return wrapped\n<\/code><\/pre>\n<p>Unfortunately I don't think there is any non-ugly way to do it. If you aren't going to pass around a <code>Bar<\/code> instance yourself then you must rely on some state existing somewhere else to tell you that a <code>Bar<\/code> instance exists and is currently being used as a context manager.<\/p>\n<p>The only way you can avoid that global state is to store the state in the instance, which means the decorator needs to be an instance method and the instance needs to exist before the function is declared:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from functools import wraps\n\nclass Bar:\n    def __init__(self, x):\n        self.x = x\n        self.active = 0\n    def __enter__(self):\n        self.active += 1\n        return self\n    def __exit__(self, *a, **k):\n        self.active -= 1\n    def assert_this(self, func):\n        @wraps(func)\n        def wrapped(*vargs, **kwargs):\n            if self.active &lt;= 0:\n                raise AssertionError()\n            return func(*vargs, **kwargs)\n        return wrapped\n\nbar = Bar(1)\n\n@bar.assert_this\ndef foo(x):\n    print(x + 1)\n\nwith bar:\n    foo(1)\n<\/code><\/pre>\n<p>This is still &quot;global state&quot; in the sense that the function <code>foo<\/code> now holds a reference to the <code>Bar<\/code> instance that holds the state. But it may be more palatable if <code>foo<\/code> is only ever going to be a local function.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1644974017813,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1644974478489,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71135260",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72780610,
        "Question_title":"MLFlow in container is not mapped to host port",
        "Question_body":"<p>I am learning about MLFlow and Docker Containers.\nI created an ubuntu container and mapped port 5001 of the host to 5000 of the container.<\/p>\n<pre><code>docker run -it -p 5001:5000 -v D:\\Docker\\mlflow:\/home --name mlflow ubuntu:18.04 bash\n<\/code><\/pre>\n<p>Inside the container, I installed the mlflow using pip<\/p>\n<pre><code>pip install mlflow\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/mmQVx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mmQVx.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>When I run the mlflow UI it's running but I can't access it from my host PC (localhost:5001) is not working.<\/p>\n<p>Did I do any mistakes anywhere?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656388899483,
        "Question_score":1,
        "Question_tags":"docker|mlflow",
        "Question_view_count":51,
        "Owner_creation_time":1539781583973,
        "Owner_last_access_time":1663923376093,
        "Owner_location":"Bangkok, Thailand",
        "Owner_reputation":434,
        "Owner_up_votes":141,
        "Owner_down_votes":3,
        "Owner_views":77,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The problem is that you are starting the server on <code>127.0.0.1<\/code> and the port mapping was not pointing to this interface (socket hang up). Starting it on all interfaces <code>0.0.0.0<\/code> works.<\/p>\n<p>You should just run this command in the container.<\/p>\n<pre><code>mlflow ui -h 0.0.0.0\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1657057254010,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72780610",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":55822637,
        "Question_title":"Is there a way to get log the descriptive stats of a dataset using MLflow?",
        "Question_body":"<p>Is there a way to get log the descriptive stats of a dataset using MLflow? If any could you please share the details?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1556081529530,
        "Question_score":3,
        "Question_tags":"python|mlflow",
        "Question_view_count":4592,
        "Owner_creation_time":1411361217027,
        "Owner_last_access_time":1662192778187,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":569,
        "Owner_up_votes":41,
        "Owner_down_votes":3,
        "Owner_views":123,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Generally speaking you can log arbitrary output from your code using the mlflow_log_artifact() function.  From <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifact\" rel=\"noreferrer\">the docs<\/a>:<\/p>\n<blockquote>\n<p><strong>mlflow.log_artifact(local_path, artifact_path=None)<\/strong>\nLog a local file or directory as an artifact of the currently active run.<\/p>\n<\/blockquote>\n<blockquote>\n<p><strong>Parameters:<\/strong><br \/>\n<em>local_path<\/em> \u2013 Path to the file to write.\n<em>artifact_path<\/em> \u2013 If provided, the directory in artifact_uri to write to.<\/p>\n<\/blockquote>\n<p>As an example, say you have your statistics in a pandas dataframe, <code>stat_df<\/code>.<\/p>\n<pre><code>## Write csv from stats dataframe\nstat_df.to_csv('dataset_statistics.csv')\n\n## Log CSV to MLflow\nmlflow.log_artifact('dataset_statistics.csv')\n<\/code><\/pre>\n<p>This will show up under the artifacts section of this MLflow run in the Tracking UI.  If you explore the docs further you'll see that you can also log an entire directory and the objects therein.  In general, MLflow provides you a lot of flexibility - anything you write to your file system you can track with MLflow.  Of course that doesn't mean you should. :)<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1557279162457,
        "Answer_score":9,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1611635690616,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55822637",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62664183,
        "Question_title":"MLflow: find model version with best metric using python code",
        "Question_body":"<p>I am trying to use API workflow (python code) to find a model version that has the best metric (for instance, \u201caccuracy\u201d) among several model versions. I understand we can use web UI to do so, but I would love to write python code to achieve this. Could someone help me?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593542659820,
        "Question_score":2,
        "Question_tags":"python|mlflow",
        "Question_view_count":445,
        "Owner_creation_time":1419619351727,
        "Owner_last_access_time":1644268036150,
        "Owner_location":null,
        "Owner_reputation":125,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":null,
        "Answer_body":"<pre><code>import mlflow \nclient = mlflow.tracking.MlflowClient()\nruns = client.search_runs(&quot;my_experiment_id&quot;, &quot;&quot;, order_by=[&quot;metrics.rmse DESC&quot;], max_results=1)\nbest_run = runs[0]\n<\/code><\/pre>\n<p><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.search_runs\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.search_runs<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1595480605940,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62664183",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72780102,
        "Question_title":"Treatment of Existing Models in a Stage When Transitioning Models",
        "Question_body":"<p>With the MLflow client library, it is possible to transition models through stages as shown below,<\/p>\n<pre><code>client = MlflowClient()\nclient.transition_model_version_stage(\n    name=&quot;sk-learn-random-forest-reg-model&quot;,\n    version=3,\n    stage=&quot;Production&quot;\n)\n<\/code><\/pre>\n<p>Upon doing some testing, I noticed that this does not in any way affect the model(s) that are currently in the stage that the model was transitioned to.<\/p>\n<p>For example, let's say version 2 of a given model is in Production. This will remain tagged as a Production model, even if I were to move version 3 to Production as well.<\/p>\n<p>Is there any way that I can control what happens to models that exist in a stage when making transitions using the above code?<\/p>\n<p>Basically, I only want one version of a model to be in a given stage at a time.<\/p>\n<p>This functionality is available when transitioning models through the Databricks UI,\n<a href=\"https:\/\/i.stack.imgur.com\/aycvC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aycvC.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656383133240,
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":26,
        "Owner_creation_time":1521856385820,
        "Owner_last_access_time":1664037995903,
        "Owner_location":"Sri Lanka",
        "Owner_reputation":820,
        "Owner_up_votes":389,
        "Owner_down_votes":1,
        "Owner_views":165,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I have missed the <code>archive_existing_versions=True<\/code> that comes with the <code>transition_model_version_stage<\/code> function.<\/p>\n<p>This flag defaults to <code>False<\/code>.<\/p>\n<p>The documentation is available here,\n<br>\n<a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.transition_model_version_stage\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.transition_model_version_stage<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1656613905027,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72780102",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":70445997,
        "Question_title":"MLflow run within a docker container - Running with \"docker_env\" in MLflow project file",
        "Question_body":"<p>We are trying to develop an MLflow pipeline. We have our developing environment in a series of dockers (no local python environment &quot;whatsoever&quot;). This means that we have set up a docker container with MLflow and all requirements necessary to run pipelines. The issue we have is that when we write our MLflow project file we need to use &quot;docker_env&quot; to specify the environment. This figure illustrates what we want to achieve:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1tLuw.jpg\" rel=\"nofollow noreferrer\">MLflow run dind<\/a><\/p>\n<p>MLflow inside the docker needs to access the docker daemon\/service so that it can either use the &quot;docker-image&quot; in the MLflow project file or pull it from docker hub. We are aware of the possibility of using &quot;conda_env&quot; in the MLflow project file but wish to avoid this.<\/p>\n<p>Our question is,<\/p>\n<p>Do we need to set some sort of &quot;docker in docker&quot; solution to achieve our goal?<\/p>\n<p>Is it possible to set up the docker container in which MLflow is running so that it can access the &quot;host machine&quot; docker daemon?<\/p>\n<p>I have been all over Google and MLflow's documentation but I can seem to find anything that can guide us. Thanks a lot in advance for any help or pointers!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1640160788793,
        "Question_score":1,
        "Question_tags":"docker|mlflow|docker-in-docker",
        "Question_view_count":779,
        "Owner_creation_time":1546431264350,
        "Owner_last_access_time":1663848875057,
        "Owner_location":"Norway",
        "Owner_reputation":31,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1640184686952,
        "Answer_body":"<p>I managed to create my pipeline using docker and docker_env in MLflow. It is not necessary to run d-in-d, the &quot;sibling approach&quot; is sufficient. This approach is described here:<\/p>\n<p><a href=\"https:\/\/jpetazzo.github.io\/2015\/09\/03\/do-not-use-docker-in-docker-for-ci\/\" rel=\"nofollow noreferrer\">https:\/\/jpetazzo.github.io\/2015\/09\/03\/do-not-use-docker-in-docker-for-ci\/<\/a><\/p>\n<p>and it is the preferred method to avoid d-in-d.<\/p>\n<p>One needs to be very careful when mounting volumes within the primary and secondary docker environments: all volume mounts happen in the host machine.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1640385689187,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70445997",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":65035488,
        "Question_title":"Running mlflow as a systemd service - gunicorn not found",
        "Question_body":"<p>I am trying to run a mlflow tracking server that is installed inside of a virtualenv as a systemd service on Ubuntu 20.04 but I am getting an error indicating that it is unable to find gunicorn. Here is my journal<\/p>\n<pre><code>nov 27 10:37:17 Atrium-Power mlflow[81375]: Traceback (most recent call last):\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/bin\/mlflow&quot;, line 8, in &lt;module&gt;\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     sys.exit(cli())\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 829, in __call__\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     return self.main(*args, **kwargs)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 782, in main\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     rv = self.invoke(ctx)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 1259, in invoke\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     return _process_result(sub_ctx.command.invoke(sub_ctx))\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 1066, in invoke\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     return ctx.invoke(self.callback, **ctx.params)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/click\/core.py&quot;, line 610, in invoke\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     return callback(*args, **kwargs)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/mlflow\/cli.py&quot;, line 392, in server\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     _run_server(\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/mlflow\/server\/__init__.py&quot;, line 138, in _run_server\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     exec_cmd(full_command, env=env_map, stream_output=True)\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/site-packages\/mlflow\/utils\/process.py&quot;, line 34, in exec_cmd\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     child = subprocess.Popen(\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/subprocess.py&quot;, line 947, in __init__\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     self._execute_child(args, executable, preexec_fn, close_fds,\nnov 27 10:37:17 Atrium-Power mlflow[81375]:   File &quot;\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/lib\/python3.9\/subprocess.py&quot;, line 1819, in _execute_child\nnov 27 10:37:17 Atrium-Power mlflow[81375]:     raise child_exception_type(errno_num, err_msg, err_filename)\nnov 27 10:37:17 Atrium-Power mlflow[81375]: FileNotFoundError: [Errno 2] No such file or directory: 'gunicorn'\n<\/code><\/pre>\n<p>and my systemd is this:<\/p>\n<pre><code>[Unit]\nStartLimitBurst=5\nStartLimitIntervalSec=33\n\n[Service]\nUser=praxasense\nWorkingDirectory=\/home\/praxasense\nRestart=always\nRestartSec=5\nExecStart=\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/bin\/mlflow server --port 3569 --backend-store-uri .mlruns\n\n[Install]\nWantedBy=multi-user.target\n<\/code><\/pre>\n<p>The strange thing is that if I run the command from <code>ExecStart<\/code> in my terminal it works fine in fish shell, but not in bash, <em>but<\/em> if I do <code>conda activate mlflow-server<\/code> and then do <code>mlflow ...<\/code> it <em>does<\/em> work. As far as I understood the Python interpreter should be aware of it's virtual environment and so it should work as I tried it, but apparently I am missing something that makes it not able to find the gunicon package, which is obviously there.<\/p>\n<p>Any ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1606471859393,
        "Question_score":1,
        "Question_tags":"python|systemd|ubuntu-20.04|mlflow",
        "Question_view_count":1018,
        "Owner_creation_time":1314532759333,
        "Owner_last_access_time":1662567260447,
        "Owner_location":"Rotterdam, Netherlands",
        "Owner_reputation":2732,
        "Owner_up_votes":414,
        "Owner_down_votes":15,
        "Owner_views":609,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Try adding the venv's bin path to the environment that systemd runs in:<\/p>\n<pre><code>[Service]\n...\nEnvironment=&quot;PATH=\/home\/praxasense\/.miniconda3\/envs\/mlflow-server\/bin&quot;\n...\n<\/code><\/pre>\n<p>I also recommend setting <code>KillMode=mixed<\/code>, since MLFlow will spawn gunicorn instances that won't be terminated if you terminate the service otherwise. <code>mixed<\/code> means that child processes will also be terminated.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1607442042567,
        "Answer_score":3,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65035488",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":61351024,
        "Question_title":"Kubernetes MLflow Service Pod Connection",
        "Question_body":"<p>I have deployed a build of mlflow to a pod in my kubernetes cluster. I'm able to port forward to the mlflow ui, and now I'm attempting to test it. To do this, I am running the following test on a jupyter notebook that is running on another pod in the same cluster.<\/p>\n<pre><code>import mlflow\n\nprint(&quot;Setting Tracking Server&quot;)\ntracking_uri = &quot;http:\/\/mlflow-tracking-server.default.svc.cluster.local:5000&quot;\n\nmlflow.set_tracking_uri(tracking_uri)\n\nprint(&quot;Logging Artifact&quot;)\nmlflow.log_artifact('\/home\/test\/mlflow-example-artifact.png')\n\nprint(&quot;DONE&quot;)\n<\/code><\/pre>\n<p>When I run this though, I get<\/p>\n<pre><code>ConnectionError: HTTPConnectionPool(host='mlflow-tracking-server.default.svc.cluster.local', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get? (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n<\/code><\/pre>\n<p>The way I have deployed the mlflow pod is shown below in the yaml and docker:<\/p>\n<p>Yaml:<\/p>\n<pre><code>---\napiVersion: apps\/v1\nkind: Deployment\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: mlflow-tracking-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mlflow-tracking-server\n    spec:\n      containers:\n      - name: mlflow-tracking-server\n        image: &lt;ECR_IMAGE&gt;\n        ports:\n        - containerPort: 5000\n        env:\n        - name: AWS_MLFLOW_BUCKET\n          value: &lt;S3_BUCKET&gt;\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_SECRET_ACCESS_KEY\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\n  labels:\n    app: mlflow-tracking-server\n  annotations:\n    service.beta.kubernetes.io\/aws-load-balancer-type: nlb\nspec:\n  externalTrafficPolicy: Local\n  type: LoadBalancer\n  selector:\n    app: mlflow-tracking-server\n  ports:\n    - name: http\n      port: 5000\n      targetPort: http\n<\/code><\/pre>\n<p>While the dockerfile calls a script that executes the mlflow server command: <code>mlflow server --default-artifact-root ${AWS_MLFLOW_BUCKET} --host 0.0.0.0 --port 5000<\/code>, I cannot connect to the service I have created using that mlflow pod.<\/p>\n<p>I have tried using the tracking uri <code>http:\/\/mlflow-tracking-server.default.svc.cluster.local:5000<\/code>, I've tried using the service EXTERNAL-IP:5000, but everything I tried cannot connect and log using the service. Is there anything that I have missed in deploying my mlflow server pod to my kubernetes cluster?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":6,
        "Question_creation_time":1587495286493,
        "Question_score":2,
        "Question_tags":"kubernetes|kubernetes-service|mlflow",
        "Question_view_count":855,
        "Owner_creation_time":1417012835813,
        "Owner_last_access_time":1628782664573,
        "Owner_location":null,
        "Owner_reputation":945,
        "Owner_up_votes":35,
        "Owner_down_votes":1,
        "Owner_views":148,
        "Question_last_edit_time":1599477403816,
        "Answer_body":"<p>Your <strong>mlflow-tracking-server<\/strong> service should have <em>ClusterIP<\/em> type, not <em>LoadBalancer<\/em>. <\/p>\n\n<p>Both pods are inside the same Kubernetes cluster, therefore, there is no reason to use <em>LoadBalancer<\/em> Service type.<\/p>\n\n<blockquote>\n  <p>For some parts of your application (for example, frontends) you may want to expose a Service onto an external IP address, that\u2019s outside of your cluster.\n  Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default is ClusterIP.<\/p>\n  \n  <p>Type values and their behaviors are:<\/p>\n  \n  <ul>\n  <li><p><strong>ClusterIP<\/strong>: Exposes the Service on a cluster-internal IP. Choosing this\n  value makes the Service only reachable from within the cluster. This\n  is the default ServiceType. <\/p><\/li>\n  <li><p><strong>NodePort<\/strong>: Exposes the Service on each Node\u2019s IP at a static port (the NodePort). A > ClusterIP Service, to which the NodePort Service routes, is automatically created. You\u2019ll > be able to contact the NodePort Service, from outside the cluster, by\n  requesting :. <\/p><\/li>\n  <li><strong>LoadBalancer<\/strong>: Exposes the Service\n  externally using a cloud provider\u2019s load balancer. NodePort and\n  ClusterIP Services, to which the external load balancer routes, are\n  automatically created. <\/li>\n  <li><strong>ExternalName<\/strong>: Maps the Service to the contents\n  of the externalName field (e.g. foo.bar.example.com), by returning a\n  CNAME record with its value. No proxying of any kind is set up.<\/li>\n  <\/ul>\n  \n  <p><a href=\"https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#publishing-services-service-types\" rel=\"nofollow noreferrer\">kubernetes.io<\/a><\/p>\n<\/blockquote>",
        "Answer_comment_count":6,
        "Answer_creation_time":1587498737657,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1587499353943,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61351024",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":72518344,
        "Question_title":"Logging and Fetching Run Parameters in AzureML",
        "Question_body":"<p>I am able to log and fetch metrics to AzureML using Run.log, however, I need a way to also log run parameters, like Learning Rate, or Momentum. I can't seem to find anything in the AzureML Python SDK documentation to achieve this. However, if I use MLflow's mlflow.log_param, I am able to log parameters, and they even nicely show up on the AzureML Studio Dashboard (bottom right of the image):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Again, I am able to fetch this using MLflow's get_params() function, but I can't find a way to do this using just AzureML's Python SDK. Is there a way to do this directly using <code>azureml<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654521747343,
        "Question_score":0,
        "Question_tags":"azure|mlflow|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":73,
        "Owner_creation_time":1554497484963,
        "Owner_last_access_time":1664005308093,
        "Owner_location":"Hyderabad, Telangana, India",
        "Owner_reputation":438,
        "Owner_up_votes":15,
        "Owner_down_votes":2,
        "Owner_views":120,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The retrieving of log run parameters like <strong>Learning Rate, or Momentum<\/strong> is not possible with <strong>AzureML<\/strong> alone. Because it was tied with <strong>MLFlow<\/strong> and <strong>azureml-core<\/strong>. without those two involvements, we cannot retrieve the log run parameters.<\/p>\n<pre><code>pip install azureml-core mlflow azureml-mlflow\n<\/code><\/pre>\n<p>Need to install these three for getting run parameters. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics\" rel=\"nofollow noreferrer\">Link<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1656998954310,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72518344",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62320331,
        "Question_title":"Sagemaker API to list Hyperparameters",
        "Question_body":"<p>I'm currently trying to implement MLFlow Tracking into my training pipeline and would like to log the hyperparameters of my hyperparameter Tuning of each training job.<\/p>\n\n<p>Does anyone know, how to pull the list of hyperparameters that can be seen on the sagemaker training job interface (on the AWS console)? Is there any other smarter way to list how models perform in comparison in Sagemaker (and displayed)?<\/p>\n\n<p>I would assume there must be an easy and Pythonic way to do this (either boto3 or the sagemaker api) to get this data. I wasn't able to find it in Cloudwatch.<\/p>\n\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1591864702897,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|mlflow",
        "Question_view_count":484,
        "Owner_creation_time":1469720117217,
        "Owner_last_access_time":1648482744487,
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Question_last_edit_time":null,
        "Answer_body":"<p>there is indeed a rather pythonic way in the SageMaker python SDK:<\/p>\n\n<pre><code>tuner = sagemaker.tuner.HyperparameterTuner.attach('&lt; your tuning jobname&gt;')\n\nresults = tuner.analytics().dataframe()  # all your tuning metadata, in pandas!\n<\/code><\/pre>\n\n<p>See full example here <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1592173650467,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62320331",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":60597319,
        "Question_title":"Running MLFlow on GCP VM",
        "Question_body":"<p>I have installed mlflow on GCP VM instance, \nnow I want to access mlflow UI with external IP.\nI tried setting up a firewall rule and opening the default port for mlflow, but not able to access it.\nCan someone give step by step process for just running mlflow on VM instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1583744387613,
        "Question_score":4,
        "Question_tags":"python|google-cloud-platform|mlflow",
        "Question_view_count":1537,
        "Owner_creation_time":1451124057623,
        "Owner_last_access_time":1664026764720,
        "Owner_location":"India",
        "Owner_reputation":736,
        "Owner_up_votes":69,
        "Owner_down_votes":2,
        "Owner_views":234,
        "Question_last_edit_time":1583832420660,
        "Answer_body":"<p>I've decided to check on my test VM and run mlflow server on GCE VM. Have a look at my steps below:<\/p>\n\n<ol>\n<li>create VM instance based on Ubuntu Linux 18.04 LTS<\/li>\n<li><p><a href=\"https:\/\/www.mlflow.org\/docs\/latest\/quickstart.html\" rel=\"noreferrer\">install MLflow<\/a>:<\/p>\n\n<pre><code>$ sudo apt update\n$ sudo apt upgrade\n$ cd ~\n$ git clone https:\/\/github.com\/mlflow\/mlflow\n$ cd mlflow\n$ sudo apt install python3-pip\n$ pip3 install mlflow\n$ python3 setup.py build\n$ sudo python3 setup.py install\n$ mlflow --version\nmlflow, version 1.7.1.dev0\n<\/code><\/pre><\/li>\n<li><p>run mlflow server on internal IP of VM instance (default 127.0.0.1):<\/p>\n\n<pre><code>$ ifconfig \nens4: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1460\ninet 10.XXX.15.XXX  netmask 255.255.255.255  broadcast 0.0.0.0\n...\n\n$ mlflow server --host 10.XXX.15.XXX\n[2020-03-09 15:05:50 +0000] [8631] [INFO] Starting gunicorn 20.0.4\n[2020-03-09 15:05:50 +0000] [8631] [INFO] Listening at: http:\/\/10.128.15.211:5000 (8631)\n[2020-03-09 15:05:50 +0000] [8631] [INFO] Using worker: sync\n[2020-03-09 15:05:50 +0000] [8634] [INFO] Booting worker with pid: 8634\n[2020-03-09 15:05:51 +0000] [8635] [INFO] Booting worker with pid: 8635\n[2020-03-09 15:05:51 +0000] [8636] [INFO] Booting worker with pid: 8636\n[2020-03-09 15:05:51 +0000] [8638] [INFO] Booting worker with pid: 8638\n<\/code><\/pre><\/li>\n<li><p>check from VM instance (from second connection):<\/p>\n\n<pre><code>$ curl -I http:\/\/10.XXX.15.XXX:5000\nHTTP\/1.1 200 OK\nServer: gunicorn\/20.0.4\nDate: Mon, 09 Mar 2020 15:06:08 GMT\nConnection: close\nContent-Length: 853\nContent-Type: text\/html; charset=utf-8\nLast-Modified: Mon, 09 Mar 2020 14:57:11 GMT\nCache-Control: public, max-age=43200\nExpires: Tue, 10 Mar 2020 03:06:08 GMT\nETag: \"1583765831.3202355-853-3764264575\"\n<\/code><\/pre><\/li>\n<li><p><a href=\"https:\/\/cloud.google.com\/vpc\/docs\/add-remove-network-tags\" rel=\"noreferrer\">set network tag<\/a> <code>mlflow-server<\/code> <\/p><\/li>\n<li><p><a href=\"https:\/\/cloud.google.com\/vpc\/docs\/using-firewalls#creating_firewall_rules\" rel=\"noreferrer\">create firewall rule<\/a> to allow access on port 5000<\/p>\n\n<pre><code>$ gcloud compute --project=test-prj firewall-rules create mlflow-server --direction=INGRESS --priority=999 --network=default --action=ALLOW --rules=tcp:5000 --source-ranges=0.0.0.0\/0 --target-tags=mlflow-server\n<\/code><\/pre><\/li>\n<li><p>check from on-premises Linux machine <code>nmap -Pn 35.225.XXX.XXX<\/code><\/p>\n\n<pre><code>Starting Nmap 7.80 ( https:\/\/nmap.org ) at 2020-03-09 16:20 CET\nNmap scan report for 74.123.225.35.bc.googleusercontent.com (35.225.XXX.XXX)\nHost is up (0.20s latency).\nNot shown: 993 filtered ports\nPORT     STATE  SERVICE\n...\n5000\/tcp open   upnp\n...\n<\/code><\/pre><\/li>\n<li><p>go to web browser <a href=\"http:\/\/35.225.XXX.XXX:5000\/\" rel=\"noreferrer\">http:\/\/35.225.XXX.XXX:5000\/<\/a><\/p><\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/u2aFt.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/u2aFt.png\" alt=\"mlflow\"><\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1583767598370,
        "Answer_score":5,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60597319",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":62778020,
        "Question_title":"Embarrassingly parallel hyperparameter search via Azure + DataBricks + MLFlow",
        "Question_body":"<p>Conceptual question.  My company is pushing Azure + DataBricks.  I am trying to understand where this can take us.<\/p>\n<p>I am porting some work I've done locally to the Azure + Databricks platform.  I want to run an experiment with a large number of hyperparameter combinations using Azure + Databricks + MLfLow.  I am using PyTorch to implement my models.<\/p>\n<p>I have a cluster with 8 nodes.  I want to kick off the parameter search across all of the nodes in an embarrassingly parallel manner (one run per node, running independently).  Is this as simple as creating a MLflow project and then using the mlflow.projects.run command for each hyperparameter combination and Databricks + MLflow will take care of the rest?<\/p>\n<p>Is this technology capable of this?  I'm looking for some references I could use to make this happen.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1594133526057,
        "Question_score":0,
        "Question_tags":"databricks|azure-databricks|mlflow",
        "Question_view_count":262,
        "Owner_creation_time":1328818302350,
        "Owner_last_access_time":1663339031437,
        "Owner_location":"Sioux City, IA",
        "Owner_reputation":325,
        "Owner_up_votes":254,
        "Owner_down_votes":4,
        "Owner_views":51,
        "Question_last_edit_time":1594134725200,
        "Answer_body":"<p>The short answer is yes, it's possible, but won't be exactly as easy as running a single mlflow command. You can paralelize single-node workflows using spark Python UDFs, a good example of this is this <a href=\"https:\/\/pages.databricks.com\/rs\/094-YMS-629\/images\/Fine-Grained-Time-Series-Forecasting.html?_ga=2.64430959.1760852900.1593769579-972789996.1561118598\" rel=\"nofollow noreferrer\">notebook<\/a><\/p>\n<p>I'm not sure if this will work with pytorch, but there is hyperopt library that lets you parallelize search across parameters using Spark - it's integrated with mlflow and available in databricks ML runtime. I've been using it only with scikit-learn, but it may be <a href=\"https:\/\/docs.databricks.com\/applications\/machine-learning\/automl\/hyperopt\/hyperopt-model-selection.html\" rel=\"nofollow noreferrer\">worth checking out<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1594986774773,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62778020",
        "Question_exclusive_tag":"MLFlow"
    },
    {
        "Question_id":64713492,
        "Question_title":"Is there a way to log the keras model summary to neptune?",
        "Question_body":"<p>I am working on a CNN project and I would like to log the model.summary to neptune.ai. The intention of that is to have an idea about the model parameters while comparing different models. Any help\/tips would be much appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1604660980577,
        "Question_score":1,
        "Question_tags":"python|keras|deep-learning|neptune",
        "Question_view_count":285,
        "Owner_creation_time":1604146329127,
        "Owner_last_access_time":1662133039343,
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":1660057709880,
        "Answer_body":"<p>You can log <code>model.summary<\/code> (assuming it's keras), like this:<\/p>\n<pre><code>neptune.init('workspace\/project')\nneptune.create_experiment()\n\nmodel = keras.Sequential(...)\nmodel.summary(print_fn=lambda x: neptune.log_text('model_summary', x))\n<\/code><\/pre>\n<p>This will log entire summary as lines of text. You can later browse it in the <em>Logs<\/em> section of the experiment. Look for tile: &quot;model_summary&quot; in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/e\/HELLO-325\/logs\" rel=\"nofollow noreferrer\">example<\/a>.<\/p>\n<p>Another option - for easier compare - is to log hyper-parameters at experiment creation, like this:<\/p>\n<pre><code># Define parameters as Python dict\nPARAMS = {'batch_size': 64,\n          'n_epochs': 100,\n          'shuffle': True,\n          'activation': 'elu'}\n\n# Pass PARAMS dict to params at experiment creation\nneptune.create_experiment(params=PARAMS)\n<\/code><\/pre>\n<p>You will have them in <em>Parameters<\/em> tab of the experiment, like in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/e\/HELLO-44\/parameters\" rel=\"nofollow noreferrer\">example<\/a>. You will be able to add each parameter as a column to the dashboard for quick compare. Look for greenish columns in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/experiments?viewId=d7f80ebe-5bfe-4d12-97c1-2b1e6184a2ed\" rel=\"nofollow noreferrer\">dashboard<\/a>.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1604748950753,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64713492",
        "Question_exclusive_tag":"Neptune"
    },
    {
        "Question_id":44053141,
        "Question_title":"Can I make Neptune talk to git?",
        "Question_body":"<p>In <a href=\"https:\/\/neptune.ml\/\" rel=\"nofollow noreferrer\">Neptune<\/a> (this machine learning experiment tracker) is it possible to make it git-aware? I mean - using <code>.gitignore<\/code> for excluded files and saving commit hashes for each run?<\/p>\n\n<p>In particular, when I review an already finished job, can I go directly to GitHub commit?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1495124614783,
        "Question_score":0,
        "Question_tags":"git|github|machine-learning|neptune",
        "Question_view_count":140,
        "Owner_creation_time":1314097464770,
        "Owner_last_access_time":1663601506563,
        "Owner_location":"Warsaw, Poland",
        "Owner_reputation":11056,
        "Owner_up_votes":1563,
        "Owner_down_votes":3,
        "Owner_views":544,
        "Question_last_edit_time":1503919276067,
        "Answer_body":"<p>Starting form version 2.0 Neptune provides integration with git, see: <a href=\"https:\/\/docs.neptune.ml\/advanced-topics\/git-integration\/\" rel=\"nofollow noreferrer\">https:\/\/docs.neptune.ml\/advanced-topics\/git-integration\/<\/a>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1503919410423,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44053141",
        "Question_exclusive_tag":"Neptune"
    },
    {
        "Question_id":44053028,
        "Question_title":"Setting job tags for Neptune",
        "Question_body":"<p>Recently I started using Neptune (via <a href=\"https:\/\/go.neptune.deepsense.io\/\" rel=\"nofollow noreferrer\">Neptune Go<\/a>) and want to have a well-organised history of experiments. How to set tags to a given experiment? (Do I do it before running it, or after?)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1495124248987,
        "Question_score":1,
        "Question_tags":"tags|neptune",
        "Question_view_count":57,
        "Owner_creation_time":1314097464770,
        "Owner_last_access_time":1663601506563,
        "Owner_location":"Warsaw, Poland",
        "Owner_reputation":11056,
        "Owner_up_votes":1563,
        "Owner_down_votes":3,
        "Owner_views":544,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There are four ways to set tags to your experiment:<\/p>\n\n<ol>\n<li>In the <code>run\/enqueue\/exec<\/code> command, i.e:<\/li>\n<\/ol>\n\n<p><code>neptune run --tags tag1 tag2 tag3 tag4<\/code><\/p>\n\n<ol start=\"2\">\n<li>In the configuration file:<\/li>\n<\/ol>\n\n<p><code>tags: [tag1, tag2, tag3, tag4]<\/code><\/p>\n\n<ol start=\"3\">\n<li>In your code:<\/li>\n<\/ol>\n\n<p><code>ctx.job.tags.append('new-tag')<\/code><\/p>\n\n<ol start=\"4\">\n<li>In the Web UI. In the experiment dashboard you have to click on \"Job Properties\" in the top left corner of the screen. Side panel will appear where you can modify job properties.<\/li>\n<\/ol>\n\n<p>So you can change tags of your experiment in every phase of your experiment execution.<\/p>\n\n<p>Sources: <\/p>\n\n<ul>\n<li><p><a href=\"http:\/\/neptune.deepsense.io\/versions\/latest\/reference-guides\/cli.html#tags\" rel=\"nofollow noreferrer\">http:\/\/neptune.deepsense.io\/versions\/latest\/reference-guides\/cli.html#tags<\/a><\/p><\/li>\n<li><p><a href=\"http:\/\/neptune.deepsense.io\/versions\/latest\/reference-guides\/job-and-experiment.html#tags\" rel=\"nofollow noreferrer\">http:\/\/neptune.deepsense.io\/versions\/latest\/reference-guides\/job-and-experiment.html#tags<\/a> <\/p><\/li>\n<\/ul>",
        "Answer_comment_count":0,
        "Answer_creation_time":1495176106327,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44053028",
        "Question_exclusive_tag":"Neptune"
    },
    {
        "Question_id":53991961,
        "Question_title":"File not found even after adding the file inside docker",
        "Question_body":"<p>I have written a docker file which adds my python script inside the container:\n<code>ADD test_pclean.py \/test_pclean.py<\/code><\/p>\n\n<p>My directory structure is:<\/p>\n\n<pre><code>.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pipeline.json\n\u2514\u2500\u2500 test_pclean.py\n<\/code><\/pre>\n\n<p>My json file which acts as a configuration file for creating a pipeline in Pachyderm is as follows:<\/p>\n\n<pre><code>{\n    \"pipeline\": {\n        \"name\": \"mopng-beneficiary-v2\"\n    },\n    \"transform\": {\n        \"cmd\": [\"python3\", \"\/test_pclean.py\"],\n        \"image\": \"avisrivastava254084\/mopng-beneficiary-v2-image-7\"\n    },\n    \"input\": {\n        \"atom\": {\n            \"repo\": \"mopng_beneficiary_v2\",\n            \"glob\": \"\/*\"\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>Even though I have copied the official documentation's example, I am facing an error:\n<code>python3: can't open file '\/test_pclean.py': [Errno 2] No such file or directory<\/code><\/p>\n\n<p>My dockerfile is:<\/p>\n\n<pre><code>FROM    debian:stretch\n\n# Install opencv and matplotlib.\nRUN apt-get update \\\n    &amp;&amp; apt-get upgrade -y \\\n    &amp;&amp; apt-get install -y unzip wget build-essential \\\n        cmake git pkg-config libswscale-dev \\\n    &amp;&amp; apt-get clean \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\n\nRUN apt update\nRUN apt-get -y install python3-pip\nRUN pip3 install matplotlib\nRUN pip3 install pandas\n\nADD test_pclean.py \/test_pclean.py\nENTRYPOINT [ \"\/bin\/bash\/\" ]\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":6,
        "Question_creation_time":1546296533713,
        "Question_score":0,
        "Question_tags":"python|python-3.x|docker|pachyderm",
        "Question_view_count":430,
        "Owner_creation_time":1531235190283,
        "Owner_last_access_time":1550050689970,
        "Owner_location":"delhi",
        "Owner_reputation":903,
        "Owner_up_votes":53,
        "Owner_down_votes":0,
        "Owner_views":144,
        "Question_last_edit_time":1546423579990,
        "Answer_body":"<p>I was not changing the commits to my docker images on each build and hence, Kubernetes was using the local docker file that it had(w\/o tags and commits, it doesn't acknowledge any change). Once I started using commit with each build, Kubernetes started downloading the intended docker image.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1547720458433,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53991961",
        "Question_exclusive_tag":"Pachyderm"
    },
    {
        "Question_id":67937857,
        "Question_title":"Pachyderm deploy GCP - no such image",
        "Question_body":"<p>I'm deploying Pachyderm on GKE but when I deploy the pipeline (following the <a href=\"https:\/\/docs.pachyderm.com\/latest\/getting_started\/beginner_tutorial\/\" rel=\"nofollow noreferrer\">https:\/\/docs.pachyderm.com\/latest\/getting_started\/beginner_tutorial\/<\/a>) the Pod fails in ImagePullCrashLoopBack giving this error &quot;no such image&quot;.<\/p>\n<p>Here, the output of the command &quot;kubectl get pods&quot;:\n<a href=\"https:\/\/i.stack.imgur.com\/lpvj7.png\" rel=\"nofollow noreferrer\">screenshot<\/a><\/p>\n<p>How can I fix the deployment procedure?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1623417969943,
        "Question_score":0,
        "Question_tags":"kubernetes|google-cloud-platform|pachyderm",
        "Question_view_count":48,
        "Owner_creation_time":1415266912943,
        "Owner_last_access_time":1664046818297,
        "Owner_location":"Milano, Metropolitan City of Milan, Italy",
        "Owner_reputation":107,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As mentioned in the Slack channel of Pachyderm community, adding the flag <code>--no-expose-docker-socket<\/code> to the deploy call should solve the issue.<\/p>\n<p><code>pachctl deploy google ${BUCKET_NAME} ${STORAGE_SIZE} --dynamic-etcd-nodes=1 --no-expose-docker-socket<\/code><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1623418140677,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67937857",
        "Question_exclusive_tag":"Pachyderm"
    },
    {
        "Question_id":70173096,
        "Question_title":"_InactiveRpcError while querying Vertex AI Matching Engine Index",
        "Question_body":"<p>I am following the example <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/community\/matching_engine\/matching_engine_for_indexing.ipynb\" rel=\"nofollow noreferrer\">notebook<\/a> as per GCP <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/matching-engine\/using-matching-engine#example_notebook\" rel=\"nofollow noreferrer\">docs<\/a> to test Vertex Matching Engine. I have deployed an index but while trying to query the index I am getting <code>_InactiveRpcError<\/code>. The VPC network is in <code>us-west2<\/code> with private service access enabled and the Index is deployed in <code>us-central1<\/code>. My VPC network contains the <a href=\"https:\/\/cloud.google.com\/vpc\/docs\/firewalls#more_rules_default_vpc\" rel=\"nofollow noreferrer\">pre-populated firewall rules<\/a>.<\/p>\n<p>Index<\/p>\n<pre><code>createTime: '2021-11-23T15:25:53.928606Z'\ndeployedIndexes:\n- deployedIndexId: brute_force_glove_deployed_v3\n  indexEndpoint: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexEndpoints\/XXXXXXXXXXXX\ndescription: testing python script for creating index\ndisplayName: glove_100_brute_force_20211123152551\netag: AMEw9yOVPWBOTpbAvJLllqxWMi2YurEV_sad2n13QvbIlqjOdMyiq_j20gG1ldhdZNTL\nmetadata:\n  config:\n    algorithmConfig:\n      bruteForceConfig: {}\n    dimensions: 100\n    distanceMeasureType: DOT_PRODUCT_DISTANCE\nmetadataSchemaUri: gs:\/\/google-cloud-aiplatform\/schema\/matchingengine\/metadata\/nearest_neighbor_search_1.0.0.yaml\nname: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexes\/XXXXXXXXXXXX\nupdateTime: '2021-11-23T16:04:17.993730Z'\n<\/code><\/pre>\n<p>Index-Endpoint<\/p>\n<pre><code>createTime: '2021-11-24T10:59:51.975949Z'\ndeployedIndexes:\n- automaticResources:\n    maxReplicaCount: 1\n    minReplicaCount: 1\n  createTime: '2021-11-30T15:16:12.323028Z'\n  deploymentGroup: default\n  displayName: brute_force_glove_deployed_v3\n  enableAccessLogging: true\n  id: brute_force_glove_deployed_v3\n  index: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexes\/XXXXXXXXXXXX\n  indexSyncTime: '2021-11-30T16:37:35.597200Z'\n  privateEndpoints:\n    matchGrpcAddress: 10.242.4.5\ndisplayName: index_endpoint_for_demo\netag: AMEw9yO6cuDfgpBhGVw7-NKnlS1vdFI5nnOtqVgW1ddMP-CMXM7NfGWVpqRpMRPsNCwc\nname: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexEndpoints\/XXXXXXXXXXXX\nnetwork: projects\/XXXXXXXXXXXX\/global\/networks\/XXXXXXXXXXXX\nupdateTime: '2021-11-24T10:59:53.271100Z'\n<\/code><\/pre>\n<p>Code<\/p>\n<pre><code>\nimport grpc\n\n# import the generated classes\nimport match_service_pb2\nimport match_service_pb2_grpc\n\nDEPLOYED_INDEX_SERVER_IP = '10.242.0.5'\nDEPLOYED_INDEX_ID = 'brute_force_glove_deployed_v3'\n\nquery = [-0.11333, 0.48402, 0.090771, -0.22439, 0.034206, -0.55831, 0.041849, -0.53573, 0.18809, -0.58722, 0.015313, -0.014555, 0.80842, -0.038519, 0.75348, 0.70502, -0.17863, 0.3222, 0.67575, 0.67198, 0.26044, 0.4187, -0.34122, 0.2286, -0.53529, 1.2582, -0.091543, 0.19716, -0.037454, -0.3336, 0.31399, 0.36488, 0.71263, 0.1307, -0.24654, -0.52445, -0.036091, 0.55068, 0.10017, 0.48095, 0.71104, -0.053462, 0.22325, 0.30917, -0.39926, 0.036634, -0.35431, -0.42795, 0.46444, 0.25586, 0.68257, -0.20821, 0.38433, 0.055773, -0.2539, -0.20804, 0.52522, -0.11399, -0.3253, -0.44104, 0.17528, 0.62255, 0.50237, -0.7607, -0.071786, 0.0080131, -0.13286, 0.50097, 0.18824, -0.54722, -0.42664, 0.4292, 0.14877, -0.0072514, -0.16484, -0.059798, 0.9895, -0.61738, 0.054169, 0.48424, -0.35084, -0.27053, 0.37829, 0.11503, -0.39613, 0.24266, 0.39147, -0.075256, 0.65093, -0.20822, -0.17456, 0.53571, -0.16537, 0.13582, -0.56016, 0.016964, 0.1277, 0.94071, -0.22608, -0.021106]\n\nchannel = grpc.insecure_channel(&quot;{}:10000&quot;.format(DEPLOYED_INDEX_SERVER_IP))\nstub = match_service_pb2_grpc.MatchServiceStub(channel)\n\nrequest = match_service_pb2.MatchRequest()\nrequest.deployed_index_id = DEPLOYED_INDEX_ID\nfor val in query:\n    request.float_val.append(val)\n\nresponse = stub.Match(request)\nresponse\n<\/code><\/pre>\n<p>Error<\/p>\n<pre><code>_InactiveRpcError                         Traceback (most recent call last)\n\/tmp\/ipykernel_3451\/467153318.py in &lt;module&gt;\n    108     request.float_val.append(val)\n    109 \n--&gt; 110 response = stub.Match(request)\n    111 response\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)\n    944         state, call, = self._blocking(request, timeout, metadata, credentials,\n    945                                       wait_for_ready, compression)\n--&gt; 946         return _end_unary_response_blocking(state, call, False, None)\n    947 \n    948     def with_call(self,\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)\n    847             return state.response\n    848     else:\n--&gt; 849         raise _InactiveRpcError(state)\n    850 \n    851 \n\n_InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\n    status = StatusCode.UNAVAILABLE\n    details = &quot;failed to connect to all addresses&quot;\n    debug_error_string = &quot;{&quot;created&quot;:&quot;@1638277076.941429628&quot;,&quot;description&quot;:&quot;Failed to pick subchannel&quot;,&quot;file&quot;:&quot;src\/core\/ext\/filters\/client_channel\/client_channel.cc&quot;,&quot;file_line&quot;:3093,&quot;referenced_errors&quot;:[{&quot;created&quot;:&quot;@1638277076.941428202&quot;,&quot;description&quot;:&quot;failed to connect to all addresses&quot;,&quot;file&quot;:&quot;src\/core\/lib\/transport\/error_utils.cc&quot;,&quot;file_line&quot;:163,&quot;grpc_status&quot;:14}]}&quot;\n&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638291169620,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|grpc|google-cloud-ml|grpc-python|google-cloud-vertex-ai",
        "Question_view_count":350,
        "Owner_creation_time":1463607987530,
        "Owner_last_access_time":1651418190167,
        "Owner_location":"Lahore, Pakistan",
        "Owner_reputation":143,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":23,
        "Question_last_edit_time":1639486727367,
        "Answer_body":"<p>Currently, Matching Engine only supports Query from the same region. Can you try running the code from VM in <code>us-central1<\/code>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1638293279417,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70173096",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71038823,
        "Question_title":"Cannot construct an Explanation object",
        "Question_body":"<p>Trying to construct an <code>Explanation<\/code> object for a unit test, but can't seem to get it to work. Here's what I'm trying:<\/p>\n<pre><code>from google.cloud import aiplatform\n\naiplatform.compat.types.explanation_v1.Explanation(\n    attributions=aiplatform.compat.types.explanation_v1.Attribution(\n        {\n            &quot;approximation_error&quot;: 0.010399332817679649,\n            &quot;baseline_output_value&quot;: 0.9280818700790405,\n            &quot;feature_attributions&quot;: {\n                &quot;feature_1&quot;: -0.0410824716091156,\n                &quot;feature_2&quot;: 0.01155053575833639,\n            },\n            &quot;instance_output_value&quot;: 0.6717480421066284,\n            &quot;output_display_name&quot;: &quot;true&quot;,\n            &quot;output_index&quot;: [0],\n            &quot;output_name&quot;: &quot;scores&quot;,\n        }\n    )\n)\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code>&quot;.venv\/lib\/python3.7\/site-packages\/proto\/message.py&quot;, line 521, in __init__\n    super().__setattr__(&quot;_pb&quot;, self._meta.pb(**params))\nTypeError: Value must be iterable\n<\/code><\/pre>\n<p>I found <a href=\"https:\/\/github.com\/googleapis\/gapic-generator-python\/issues\/413#issuecomment-872094378\" rel=\"nofollow noreferrer\">this<\/a> on github, but I'm not sure how to apply that workaround here.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644344024883,
        "Question_score":0,
        "Question_tags":"python|protocol-buffers|google-cloud-vertex-ai",
        "Question_view_count":74,
        "Owner_creation_time":1519933306780,
        "Owner_last_access_time":1664076383160,
        "Owner_location":null,
        "Owner_reputation":3576,
        "Owner_up_votes":42,
        "Owner_down_votes":7,
        "Owner_views":203,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As the error mentioned value to be passed at <code>attributions<\/code> should be <strong>iterable<\/strong>. See <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1.types.Explanation\" rel=\"nofollow noreferrer\">Explanation attributes documentation<\/a>.<\/p>\n<p>I tried your code and placed the <code>Attribution<\/code> object in a list and the error is gone. I assigned your objects in variables just so the code is readable.<\/p>\n<p>See code and testing below:<\/p>\n<pre><code>from google.cloud import aiplatform\n\ntest = {\n            &quot;approximation_error&quot;: 0.010399332817679649,\n            &quot;baseline_output_value&quot;: 0.9280818700790405,\n            &quot;feature_attributions&quot;: {\n                &quot;feature_1&quot;: -0.0410824716091156,\n                &quot;feature_2&quot;: 0.01155053575833639,\n            },\n            &quot;instance_output_value&quot;: 0.6717480421066284,\n            &quot;output_display_name&quot;: &quot;true&quot;,\n            &quot;output_index&quot;: [0],\n            &quot;output_name&quot;: &quot;scores&quot;,\n        }\n\nattributions=aiplatform.compat.types.explanation_v1.Attribution(test)\nx  = aiplatform.compat.types.explanation_v1.Explanation(\n    attributions=[attributions]\n)\nprint(x)\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>attributions {\n  baseline_output_value: 0.9280818700790405\n  instance_output_value: 0.6717480421066284\n  feature_attributions {\n    struct_value {\n      fields {\n        key: &quot;feature_1&quot;\n        value {\n          number_value: -0.0410824716091156\n        }\n      }\n      fields {\n        key: &quot;feature_2&quot;\n        value {\n          number_value: 0.01155053575833639\n        }\n      }\n    }\n  }\n  output_index: 0\n  output_display_name: &quot;true&quot;\n  approximation_error: 0.010399332817679649\n  output_name: &quot;scores&quot;\n}\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1644385932890,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1644387344880,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71038823",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69658459,
        "Question_title":"Jobs-Cloud Scheduler (Google Cloud) fails to run scheduled pipelines",
        "Question_body":"<p>I'm here because I'm facing a problem with scheduled jobs in Google Cloud.\nIn Vertex AI Workbench, I created a notebook in Python 3 that creates a pipeline that trains AutoML with data from the public credit card dataset.\nIf I run the job at the end of its creation, everything works. However, if I schedule the job run <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/run-pipeline#scheduling_a_recurring_pipeline_run_using_the\" rel=\"nofollow noreferrer\">as described here<\/a> in Job Cloud Scheduler, the pipeline is enabled but the run fails.<\/p>\n<p>Here is the code that I have:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\n# import sys\nimport google.cloud.aiplatform as aip\nimport kfp\n# from kfp.v2.dsl import component\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom kfp.v2 import compiler\n# from kfp.v2.google.client import AIPlatformClient\n\nPROJECT_ID = &quot;fraud-detection-project-329506&quot;\nREGION = &quot;us-central1&quot;\n\ncredential_path = r&quot;C:\\Users\\...\\fraud-detection-project-329506-4d16889a494a.json&quot;\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n      \nBUCKET_NAME = &quot;gs:\/\/...&quot;\nSERVICE_ACCOUNT = &quot;...@fraud-detection-project-329506.iam.gserviceaccount.com&quot;\n\nAPI_ENDPOINT = &quot;{}-aiplatform.googleapis.com&quot;.format(REGION)\nPIPELINE_ROOT = &quot;{}\/dataset&quot;.format(BUCKET_NAME)\n\naip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)\n\n# file names\nTRAIN_FILE_NAME = &quot;creditcard_train.csv&quot;\nTEST_FILE_NAME = &quot;creditcard_test.csv&quot;\n\n# path for train and test dataset \ngcs_csv_path_train = f&quot;{PIPELINE_ROOT}\/{TRAIN_FILE_NAME}&quot;\ngcs_csv_path_test = f&quot;{PIPELINE_ROOT}\/{TEST_FILE_NAME}&quot;\n\n#gcs location where the output is to be written to\ngcs_destination_prefix = &quot;{}\/output&quot;.format(BUCKET_NAME)\n\n@kfp.dsl.pipeline(name=&quot;automl-tab-training-v2&quot;)\ndef pipeline(project: str = PROJECT_ID):\n    \n    # create tabular dataset\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=&quot;creditcard&quot;, gcs_source=gcs_csv_path_train\n    )\n    \n  \n    # Training with AutoML\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=&quot;train-automl-fraud-detection&quot;,\n        optimization_prediction_type=&quot;classification&quot;,\n        column_transformations=[\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Time&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V1&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V2&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V3&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V4&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V5&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V6&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V7&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V8&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V9&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V10&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V11&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V12&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V13&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V14&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V15&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V16&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V17&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V18&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V19&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V20&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V21&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V22&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V23&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V24&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V25&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V26&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V27&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V28&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Amount&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],#dataset_with_FeatEng,\n        target_column=&quot;Class&quot;,\n        budget_milli_node_hours=1000,\n    )\n    \n    # batch prediction after training\n    batchprediction_op = gcc_aip.ModelBatchPredictOp(\n        model=training_op.outputs[&quot;model&quot;],\n        job_display_name='prediction1',\n        gcs_source=gcs_csv_path_test,\n        project=project,\n        machine_type=&quot;n1-standard-2&quot;,\n        gcs_destination_prefix=gcs_destination_prefix,\n    )\n    \n\nCOMPILED_PIPELINE_PATH = r&quot;C:\\Users\\...\\tabular_classification_pipeline.json&quot;\nSCHEDULE = &quot;5 5 * * *&quot;\nDISPLAY_NAME = 'fraud_detection'\n\n# compile pipeline\ncompiler.Compiler().compile(\n    pipeline_func=pipeline,\n    package_path=COMPILED_PIPELINE_PATH,\n)\n\n# job run after its creation\njob = aip.PipelineJob(\n    display_name=DISPLAY_NAME,\n    template_path=COMPILED_PIPELINE_PATH,\n    pipeline_root=PIPELINE_ROOT,\n)\njob.run()\n\n# api_client = AIPlatformClient(project_id=PROJECT_ID, region=REGION)\n\n# schedule training\/prediction every day at a certain hour\n# api_client.create_schedule_from_job_spec(\n#    job_spec_path=COMPILED_PIPELINE_PATH,\n#    pipeline_root=PIPELINE_ROOT,\n#    schedule=SCHEDULE,\n# )\n<\/code><\/pre>\n<p>Looking at the error log, I found:<\/p>\n<pre><code>{\nhttpRequest: {\nstatus: 404\n}\ninsertId: &quot;13yj575g2rylrz9&quot;\njsonPayload: {\n@type: &quot;type.googleapis.com\/google.cloud.scheduler.logging.AttemptFinished&quot;\njobName: &quot;projects\/fraud-detection-project-329506\/locations\/us-central1\/jobs\/pipeline_pipeline_179e648c_0-11-a-a-a&quot;\nstatus: &quot;NOT_FOUND&quot;\ntargetType: &quot;HTTP&quot;\nurl: &quot;https:\/\/us-central1-fraud-detection-project-329506.cloudfunctions.net\/templated_http_request-v1&quot;\n}\nlogName: &quot;projects\/fraud-detection-project-329506\/logs\/cloudscheduler.googleapis.com%2Fexecutions&quot;\nreceiveTimestamp: &quot;2021-10-19T18:00:00.309225533Z&quot;\nresource: {\nlabels: {\njob_id: &quot;pipeline_pipeline_179e648c_0-11-a-a-a&quot;\nlocation: &quot;us-central1&quot;\nproject_id: &quot;fraud-detection-project-329506&quot;\n}\ntype: &quot;cloud_scheduler_job&quot;\n}\nseverity: &quot;ERROR&quot;\ntimestamp: &quot;2021-10-19T18:00:00.309225533Z&quot;\n}\n<\/code><\/pre>\n<p>Does it mean that I have to create the URL before running the notebook? I have no idea how to go on.\nThank you in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634804848290,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-scheduler|google-cloud-vertex-ai",
        "Question_view_count":330,
        "Owner_creation_time":1616589293617,
        "Owner_last_access_time":1663860372103,
        "Owner_location":"Alatri, Frosinone, FR",
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":1635245663990,
        "Answer_body":"<p>From the error you shared, apparently <strong>Cloud Function<\/strong> failed to create the job.<\/p>\n<pre><code>status: &quot;NOT_FOUND&quot; \ntargetType: &quot;HTTP&quot; \nurl: &quot;https:\/\/us-central1-fraud-detection-project-329506.cloudfunctions.net\/templated_http_request-v1&quot;\n<\/code><\/pre>\n<p>A possible reason from the Cloud Function side could be if <strong>Cloud Build API<\/strong> is not used in your project before or it is disabled. Can you check if it is enabled and try again? If you have enabled this API recently, wait for a few minutes for the action to propagate to the systems and retry.<\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1636443694643,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69658459",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72122744,
        "Question_title":"Google Cloud Vertex AI with Golang: rpc error: code = Unimplemented desc = unexpected HTTP status code received from server: 404 (Not Found)",
        "Question_body":"<p>I have a Vertex AI model deployed on an endpoint and want to do some prediction from my app in Golang.<\/p>\n<p>To do this I create code inspired by this example : <a href=\"https:\/\/cloud.google.com\/go\/docs\/reference\/cloud.google.com\/go\/aiplatform\/latest\/apiv1?hl=en\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/go\/docs\/reference\/cloud.google.com\/go\/aiplatform\/latest\/apiv1?hl=en<\/a><\/p>\n<pre><code>const file = &quot;MY_BASE64_IMAGE&quot;\n\nfunc main() {\n\n    ctx := context.Background()\n\n    c, err := aiplatform.NewPredictionClient(cox)\n    if err != nil {\n        log.Printf(&quot;QueryVertex NewPredictionClient - Err:%s&quot;, err)\n    }\n    defer c.Close()\n\n    parameters, err := structpb.NewValue(map[string]interface{}{\n        &quot;confidenceThreshold&quot;: 0.2,\n        &quot;maxPredictions&quot;:      5,\n    })\n    if err != nil {\n        log.Printf(&quot;QueryVertex structpb.NewValue parameters - Err:%s&quot;, err)\n    }\n\n    instance, err := structpb.NewValue(map[string]interface{}{\n        &quot;content&quot;: file,\n    })\n    if err != nil {\n        log.Printf(&quot;QueryVertex structpb.NewValue instance - Err:%s&quot;, err)\n    }\n\n    reqP := &amp;aiplatformpb.PredictRequest{\n        Endpoint:   &quot;projects\/PROJECT_ID\/locations\/LOCATION_ID\/endpoints\/ENDPOINT_ID&quot;,\n        Instances:  []*structpb.Value{instance},\n        Parameters: parameters,\n    }\n\n    resp, err := c.Predict(cox, reqP)\n    if err != nil {\n        log.Printf(&quot;QueryVertex Predict - Err:%s&quot;, err)\n    }\n\n    log.Printf(&quot;QueryVertex Res:%+v&quot;, resp)\n}\n<\/code><\/pre>\n<p>I put the path to my service account JSON file on GOOGLE_APPLICATION_CREDENTIALS environment variable.\nBut when I run my test app I obtain this error message:<\/p>\n<pre><code>QueryVertex Predict - Err:rpc error: code = Unimplemented desc = unexpected HTTP status code received from server: 404 (Not Found); transport: received unexpected content-type &quot;text\/html; charset=UTF-8&quot;\nQueryVertex Res:&lt;nil&gt;\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_time":1651731616723,
        "Question_score":3,
        "Question_tags":"go|grpc|gcloud|google-cloud-vertex-ai|google-ai-platform",
        "Question_view_count":455,
        "Owner_creation_time":1651730962057,
        "Owner_last_access_time":1663929720413,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":1654669807780,
        "Answer_body":"<p>As @DazWilkin suggested, configure the client option to specify the specific <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest#service-endpoint\" rel=\"noreferrer\">regional endpoint<\/a> with a port 443:<\/p>\n<pre><code>option.WithEndpoint(&quot;&lt;region&gt;-aiplatform.googleapis.com:443&quot;)\n<\/code><\/pre>\n<p>Try like below:<\/p>\n<pre><code>func main() {\n \n   ctx := context.Background()\n   c, err := aiplatform.NewPredictionClient(\n       ctx,\n       option.WithEndpoint(&quot;&lt;region&gt;-aiplatform.googleapis.com:443&quot;),\n   )\n   if err != nil {\n       log.Printf(&quot;QueryVertex NewPredictionClient - Err:%s&quot;, err)\n   }\n   defer c.Close()\n       .\n       .\n<\/code><\/pre>",
        "Answer_comment_count":3,
        "Answer_creation_time":1652647881720,
        "Answer_score":5,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1652712010567,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72122744",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72174602,
        "Question_title":"Why do I get 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS error for Vertex AI init?",
        "Question_body":"<p>I am trying to set up mlops for Vertex AI, following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/01-dataset-management.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a>. It works until, near the end, I try:<\/p>\n<pre><code>vertex_ai.init(\nproject=PROJECT,\n    location=REGION)\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code> module 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS\n<\/code><\/pre>\n<p>I am using <code>us-central1<\/code> which is supported. I wondered if maybe <code>from google.cloud import aiplatform as vertex_ai<\/code> has been changed but don't know how to find out. Any help is much appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1652110521003,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":298,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I followed the same Notebook as you, even though I didn't have any issue. What could be happening to you is that you are using an <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/releases\" rel=\"nofollow noreferrer\">older version of the library<\/a>.<\/p>\n<p>You can use the command to upgrade the library that is the following one: <code>pip3 install google-cloud-aiplatform --upgrade<\/code>.<\/p>\n<p>Sometimes this happens with the basic installation of the library; the problems could be in the <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform#installation\" rel=\"nofollow noreferrer\">dependencies, versions and indirectly permissions<\/a>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1652193787553,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72174602",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69302528,
        "Question_title":"How to pass environment variables to gcloud beta ai custom-jobs create with custom container (Vertex AI)",
        "Question_body":"<p>I'm running custom training jobs in google's Vertex AI. A simple <code>gcloud<\/code> command to execute a custom job would use something like the following syntax (complete documentation for the command can be seen <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/beta\/ai\/custom-jobs\/create#--config\" rel=\"nofollow noreferrer\">here<\/a>):<\/p>\n<pre><code>gcloud beta ai custom-jobs create --region=us-central1 \\\n--display-name=test \\\n--config=config.yaml\n<\/code><\/pre>\n<p>In the <code>config.yaml<\/code> file, it is possible to specify the machine and accelerator (GPU) types, etc., and in my case, point to a custom container living in the Google Artifact Registry that executes the training code (specified in the <code>imageUri<\/code> part of the <code>containerSpec<\/code>). An example config file may look like this:<\/p>\n<pre><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n    acceleratorType: NVIDIA_TESLA_P100\n    acceleratorCount: 2\n  replicaCount: 1\n  containerSpec:\n    imageUri: {URI_FOR_CUSTOM_CONATINER}\n    args:\n    - {ARGS TO PASS TO CONTAINER ENTRYPOINT COMMAND}\n<\/code><\/pre>\n<p>The code we're running needs some runtime environment variables (that need to be secure) passed to the container. In the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">API documentation<\/a> for the <code>containerSpec<\/code>, it says it is possible to set environment variables as follows:<\/p>\n<pre><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n    acceleratorType: NVIDIA_TESLA_P100\n    acceleratorCount: 2\n  replicaCount: 1\n  containerSpec:\n    imageUri: {URI_FOR_CUSTOM_CONATINER}\n    args:\n    - {ARGS TO PASS TO CONTAINER ENTRYPOINT COMMAND}\n    env:\n    - name: SECRET_ONE\n      value: $SECRET_ONE\n    - name: SECRET_TWO\n      value: $SECRET_TWO\n<\/code><\/pre>\n<p>When I try and add the <code>env<\/code> flag to the <code>containerSpec<\/code>, I get an error saying it's not part of the container spec:<\/p>\n<pre><code>ERROR: (gcloud.beta.ai.custom-jobs.create) INVALID_ARGUMENT: Invalid JSON payload received. Unknown name &quot;env&quot; at 'custom_job.job_spec.worker_pool_specs[0].container_spec': Cannot find field.\n- '@type': type.googleapis.com\/google.rpc.BadRequest\n  fieldViolations:\n  - description: &quot;Invalid JSON payload received. Unknown name \\&quot;env\\&quot; at 'custom_job.job_spec.worker_pool_specs[0].container_spec':\\\n      \\ Cannot find field.&quot;\n    field: custom_job.job_spec.worker_pool_specs[0].container_spec\n<\/code><\/pre>\n<p>Any idea how to securely set runtime environment variables in Vertex AI custom jobs using custom containers?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1632409414673,
        "Question_score":3,
        "Question_tags":"docker|google-cloud-platform|gcloud|google-cloud-sdk|google-cloud-vertex-ai",
        "Question_view_count":1033,
        "Owner_creation_time":1464106929610,
        "Owner_last_access_time":1663087027037,
        "Owner_location":null,
        "Owner_reputation":457,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":37,
        "Question_last_edit_time":1632410531832,
        "Answer_body":"<p>There are two versions of the REST API - \u201c<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">v1<\/a>\u201d and \u201c<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1beta1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">v1beta1<\/a>\u201d where &quot;v1beta1&quot; does not have the <code>env<\/code> option in <code>ContainerSpec<\/code> but &quot;v1&quot; does. The <code>gcloud ai custom-jobs create<\/code> command without the <code>beta<\/code> parameter doesn\u2019t throw the error as it uses version \u201cv1\u201d to make the API calls.<\/p>\n<p>The environment variables from the yaml file can be passed to the custom container in the following way:<\/p>\n<p>This is the docker file of the sample custom training application I used to test the requirement. Please refer to this <a href=\"https:\/\/codelabs.developers.google.com\/vertex_custom_training_prediction\" rel=\"nofollow noreferrer\">codelab<\/a> for more information about the training application.<\/p>\n<pre class=\"lang-docker prettyprint-override\"><code>FROM gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3\nWORKDIR \/root\n\nWORKDIR \/\n\n# Copies the trainer code to the docker image.\nCOPY trainer \/trainer\n\n\n# Copies the bash script to the docker image.\nCOPY commands.sh \/scripts\/commands.sh\n\n# Bash command to make the script file an executable\nRUN [&quot;chmod&quot;, &quot;+x&quot;, &quot;\/scripts\/commands.sh&quot;]\n\n\n# Command to execute the file\nENTRYPOINT [&quot;\/scripts\/commands.sh&quot;]\n\n# Sets up the entry point to invoke the trainer.\n# ENTRYPOINT &quot;python&quot; &quot;-m&quot; $SECRET_TWO \u21d2 To use the environment variable  \n# directly in the docker ENTRYPOINT. In case you are not using a bash script, \n# the trainer can be invoked directly from the docker ENTRYPOINT.\n<\/code><\/pre>\n<br \/>\n<p>Below is the <code>commands.sh<\/code> file used in the docker container to test whether the environment variables are passed to the container.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>#!\/bin\/bash\nmkdir \/root\/.ssh\necho $SECRET_ONE\npython -m $SECRET_TWO\n<\/code><\/pre>\n<br \/>\n<p>The example <code>config.yaml<\/code> file<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n  replicaCount: 1\n  containerSpec:\n    imageUri: gcr.io\/infosys-kabilan\/mpg:v1\n    env:\n    - name: SECRET_ONE\n      value: &quot;Passing the environment variables&quot;\n    - name: SECRET_TWO\n      value: &quot;trainer.train&quot;\n<\/code><\/pre>\n<p>As the next step, I built and pushed the container to Google Container Repository. Now, the <code>gcloud ai custom-jobs create --region=us-central1  --display-name=test --config=config.yaml<\/code> can be run to create the custom training job and the output of the <code>commands.sh<\/code> file can be seen in the job logs as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qRHV7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qRHV7.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1632987814700,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69302528",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70623713,
        "Question_title":"How can I pass parameters to a Vertex AI Platform Pipeline?",
        "Question_body":"<p>I have created a Vertex AI pipeline similar to <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/google_cloud_pipeline_components_automl_images.ipynb\" rel=\"nofollow noreferrer\">this.<\/a><\/p>\n<p>Now the pipeline has reference to a csv file. So if this csv file changes the pipeline needs to be recreated.<\/p>\n<p>Is there any way to pass a new csv as a parameter to the pipeline when it is re-run? That is without recreating the pipeline using the notebook?<\/p>\n<p>If not, is there a best practice way of auto updating the dataset, model and deployment?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641570150640,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":611,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Have a look to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/build-pipeline\" rel=\"nofollow noreferrer\">that documentation<\/a>.<\/p>\n<p>You can define your pipeline like that<\/p>\n<pre><code>...\n# Define the workflow of the pipeline.\n@kfp.dsl.pipeline(\n    name=&quot;automl-image-training-v2&quot;,\n    pipeline_root=pipeline_root_path)\ndef pipeline(project_id: str):\n...\n<\/code><\/pre>\n<p>(you have something very similar in your notebook sample)<\/p>\n<p>Then, when you invoke your pipeline, you can pass some parameter<\/p>\n<pre><code>import google.cloud.aiplatform as aip\n\njob = aip.PipelineJob(\n    display_name=&quot;automl-image-training-v2&quot;,\n    template_path=&quot;image_classif_pipeline.json&quot;,\n    pipeline_root=pipeline_root_path,\n    parameter_values={\n        'project_id': project_id\n    }\n)\n\njob.submit()\n<\/code><\/pre>\n<p>You can see the <code>project_id<\/code> a dict parameter in the parameter values, and in parameter of your pipeline function.<\/p>\n<p>Do the same for your CSV file name!<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1641588471210,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70623713",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70109346,
        "Question_title":"Vertex AI was unable to import data into dataset. It says maximum 1M lines while my dataset only have 600k",
        "Question_body":"<p>I'm importing a text dataset to Google Vertex AI and got the following error:<\/p>\n<pre><code>Hello Vertex AI Customer,\n\nDue to an error, Vertex AI was unable to import data into \ndataset [dataset_name].\nAdditional Details:\nOperation State: Failed with errors\nResource Name: [resoure_link]\nError Messages: There are too many rows in the jsonl\/csv file. Currently we \nonly support 1000000 lines. Please cut your files to smaller size and run \nmultiple import data pipelines to import.\n<\/code><\/pre>\n<p>I checked my dataset which I generated from pandas and the actual CSV file, it only have 600k lines.<\/p>\n<p>Anyone got similar errors?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1637835565680,
        "Question_score":2,
        "Question_tags":"python|google-cloud-vertex-ai",
        "Question_view_count":212,
        "Owner_creation_time":1637238945423,
        "Owner_last_access_time":1663923957553,
        "Owner_location":"Jakarta, Indonesia",
        "Owner_reputation":31,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":1638435090680,
        "Answer_body":"<p>So it turns out to be an error in my CSV formatting.<\/p>\n<p>I forgot to trim newlines and extra whitespaces in my text dataset. This solved the 1M lines count. But after doing that, I then get error telling me I have too much labels while it was only 2.<\/p>\n<pre><code>Error Messages: There are too many AnnotationSpecs in the dataset. Up to \n5000 AnnotationSpecs are allowed in one Dataset.\n<\/code><\/pre>\n<p>And this is because I created the text dataset using to_csv() method in Pandas dataframe. Creating a CSV file this way, it will automatically put quotes when your text include a &quot;,&quot; (comma character) only. So the CSV file will look like:<\/p>\n<pre><code>&quot;this is a sentence, with a comma&quot;, 0\nthis is a sentence without a comma, 1\n<\/code><\/pre>\n<p>Meanwhile, what Vertex AutoML Text wants the CSV is to look like this:<\/p>\n<pre><code>&quot;this is a sentence, with a comma&quot;, 0\n&quot;this is a sentence without a comma&quot;, 1\n<\/code><\/pre>\n<p>i.e. you have to put quotes on every line.<\/p>\n<p>Which you can achieve by writing your own CSV formatter, or if you insist on using Pandas to_csv(), you can pass csv.QUOTE_ALL to the quoting parameter. It will look like this:<\/p>\n<pre><code>import csv\ndf.to_csv(&quot;file.csv&quot;, index=False, quoting=csv.QUOTE_ALL, header=False)\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1639027160677,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1639028476929,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70109346",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72741757,
        "Question_title":"Vertex AI Custom Container Training Job python SDK - google.api_core.exceptions.FailedPrecondition: 400 '",
        "Question_body":"<p>I have built a custom container which use my managed dataset on vertex to run my training code, it worked successfully when I create the training job on the Vertex AI website interface.<\/p>\n<p>But now I'm trying to create the training job from a python script using<\/p>\n<pre><code>class google.cloud.aiplatform.CustomContainerTrainingJob\n<\/code><\/pre>\n<p>I load a managed dataset that I have on vertex AI with<\/p>\n<pre><code>dataset = aiplatform.ImageDataset(dataset_id) if dataset_id else None\n<\/code><\/pre>\n<p>But when I try to run the following code:<\/p>\n<pre><code>model = job.run(\n        dataset=dataset,\n        model_display_name=model_display_name,\n        args=args,\n        replica_count=replica_count,\n        machine_type=machine_type,\n        accelerator_type=accelerator_type,\n        accelerator_count=accelerator_count,\n        training_fraction_split=training_fraction_split,\n        validation_fraction_split=validation_fraction_split,\n        test_fraction_split=test_fraction_split,\n        sync=sync,\n    )\n\n    model.wait()\n\n    print(model.display_name)\n    print(model.resource_name)\n    print(model.uri)\n    return model\n<\/code><\/pre>\n<p>I got the following error:<\/p>\n<pre><code>google.api_core.exceptions.FailedPrecondition: 400 'annotation_schema_uri' should be set in the TrainingPipeline.input_data_config for custom training or hyperparameter tuning with managed dataset.\n<\/code><\/pre>\n<p>I feel like something is wrong because when I create the job on the website I specify an export directory for the managed dataset, but I have not found where to do it here.<\/p>\n<p>Any ideas?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656062302777,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":170,
        "Owner_creation_time":1656061360900,
        "Owner_last_access_time":1659951704647,
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Well I found the answer in the <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/aiplatform?hl=fr#class-googlecloudaiplatformcustomcontainertrainingjobdisplayname-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-containeruri-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-command-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerimageuri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerpredictroute-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerhealthroute-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainercommand-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerargs-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerenvironmentvariables-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerports-optionalsequenceinthttpspythonreadthedocsioenlatestlibraryfunctionshtmlint--none-modeldescription-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelinstanceschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelparametersschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelpredictionschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-project-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-location-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-credentials-optionalgoogleauthcredentialscredentialshttpsgoogleapisdevpythongoogle-authlatestreferencegoogleauthcredentialshtmlgoogleauthcredentialscredentials--none-labels-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-trainingencryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelencryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-stagingbucket-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none\" rel=\"nofollow noreferrer\">documentation<\/a>, data are automatically exported to the provided bucket thus it was not the issue. The issue was in the error ( obviously ).\nTo provide a good annotation URI, it is enough to just add a parameter to run():<\/p>\n<pre><code>annotation_schema_uri=aiplatform.schema.dataset.annotation.image.classification\n<\/code><\/pre>\n<p>image.classification was what I needed here but can be replaced by text.extraction if you do text extraction for example.<\/p>\n<p>This will pass as string value the following value which is the asked gs uri:<\/p>\n<pre><code>gs:\/\/google-cloud-aiplatform\/schema\/dataset\/annotation\/image_classification_1.0.0.yaml\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1656071944493,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1656343337227,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72741757",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71284125,
        "Question_title":"GCP Vertex AI Training: Auto-packaged Custom Training Job Yields Huge Docker Image",
        "Question_body":"<p>I am trying to run a Custom Training Job in Google Cloud Platform's Vertex AI Training service.<\/p>\n<p>The job is based on <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/pytorch-google-cloud-how-train-and-tune-pytorch-models-vertex-ai\" rel=\"nofollow noreferrer\">a tutorial from Google that fine-tunes a pre-trained BERT model<\/a> (from HuggingFace).<\/p>\n<p>When I use the <code>gcloud<\/code> CLI tool to auto-package my training code into a Docker image and deploy it to the Vertex AI Training service like so:<\/p>\n<pre><code>$BASE_GPU_IMAGE=&quot;us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-7:latest&quot;\n$BUCKET_NAME = &quot;my-bucket&quot;\n\ngcloud ai custom-jobs create `\n--region=us-central1 `\n--display-name=fine_tune_bert `\n--args=&quot;--job_dir=$BUCKET_NAME,--num-epochs=2,--model-name=finetuned-bert-classifier&quot; `\n--worker-pool-spec=&quot;machine-type=n1-standard-4,replica-count=1,accelerator-type=NVIDIA_TESLA_V100,executor-image-uri=$BASE_GPU_IMAGE,local-package-path=.,python-module=trainer.task&quot;\n<\/code><\/pre>\n<p>... I end up with a Docker image that is roughly 18GB (!) and takes a very long time to upload to the GCP registry.<\/p>\n<p>Granted <a href=\"https:\/\/console.cloud.google.com\/gcr\/images\/deeplearning-platform-release\/GLOBAL\/pytorch-gpu.1-7?tag=nightly-2021-03-28\" rel=\"nofollow noreferrer\">the base image is around 6.5GB<\/a> but <strong>where do the additional &gt;10GB come from and is there a way for me to avoid this &quot;image bloat&quot;?<\/strong><\/p>\n<p>Please note that my job loads the training data using the <code>datasets<\/code> Python package at run time and AFAIK does not include it in the auto-packaged docker image.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1645958992960,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|pytorch|google-cloud-vertex-ai",
        "Question_view_count":478,
        "Owner_creation_time":1225618447787,
        "Owner_last_access_time":1663751971780,
        "Owner_location":"Tel Aviv, Israel",
        "Owner_reputation":15116,
        "Owner_up_votes":890,
        "Owner_down_votes":56,
        "Owner_views":1182,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The image size shown in the UI is the virtual size of the image. It is the compressed total image size that will be downloaded over the network. Once the image is pulled, it will be extracted and the resulting size will be bigger. In this case, the <a href=\"https:\/\/console.cloud.google.com\/artifacts\/docker\/vertex-ai\/us\/training\/pytorch-gpu.1-7\/sha256:0d990ebc4fc376880fd3ee375015e43594450d11d9791ac203cf2d044871917f\" rel=\"nofollow noreferrer\">PyTorch image's virtual size<\/a> is 6.8 GB while the actual size is 17.9 GB.<\/p>\n<p>Also, when a <code>docker push<\/code> command is executed, the progress bars show the uncompressed size. The actual amount of data that\u2019s pushed will be <a href=\"https:\/\/docs.docker.com\/engine\/reference\/commandline\/push\/#extended-description\" rel=\"nofollow noreferrer\">compressed before sending<\/a>, so the uploaded size will not be reflected by the progress bar.<\/p>\n<p>To cut down the size of the docker image, custom containers can be used. Here, only the necessary components can be configured which would result in a smaller docker image. More information on custom containers <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/containers-overview\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1646123694693,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71284125",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72827960,
        "Question_title":"How to get image classification prediction from GCP AIPlatform in ruby?",
        "Question_body":"<p>I'm new with ruby and I want to use GCP AIPlatform but I'm struggeling with the payload.<\/p>\n<p>So far, I have :<\/p>\n<pre class=\"lang-rb prettyprint-override\"><code>client = ::Google::Cloud::AIPlatform::V1::PredictionService::Client.new do |config|\n  config.endpoint = &quot;#{location}-aiplatform.googleapis.com&quot;\nend\n\nimg = File.open(imgPath, 'rb') do |img|\n  'data:image\/png;base64,' + Base64.strict_encode64(img.read)\nend\n\ninstance = Instance.new(:content =&gt; img)\n\nrequest = Google::Cloud::AIPlatform::V1::PredictRequest.new(\n  endpoint: &quot;projects\/#{project}\/locations\/#{location}\/endpoints\/#{endpoint}&quot;,\n  instances: [instance]\n)\n\nresult = client.predict request\np result\n<\/code><\/pre>\n<p>Here is my proto<\/p>\n<pre><code>message Instance {\n  required bytes content = 1;\n};\n<\/code><\/pre>\n<p>But I have the following error : <code>Invalid type Instance to assign to submessage field 'instances'<\/code><\/p>\n<p>I read the documentation but for ruby SDK it's a bit light.\nThe parameters are OK, the JS example here : <a href=\"https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-image-object-detection.js\" rel=\"nofollow noreferrer\">https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-image-object-detection.js<\/a> is working with those parameters<\/p>\n<p>What am I doing wrong ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1656671602857,
        "Question_score":0,
        "Question_tags":"ruby|google-cloud-vertex-ai",
        "Question_view_count":54,
        "Owner_creation_time":1656670919183,
        "Owner_last_access_time":1659105111753,
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I managed it<\/p>\n<pre class=\"lang-rb prettyprint-override\"><code>client = Google::Cloud::AIPlatform::V1::PredictionService::Client.new do |config|\n  config.endpoint = &quot;#{location}-aiplatform.googleapis.com&quot;\nend\n\nimg = File.open(imgPath, 'rb') do |img|\n  Base64.strict_encode64(img.read)\nend\n\ninstance = Google::Protobuf::Value.new(:struct_value =&gt; {:fields =&gt; {\n  :content =&gt; {:string_value =&gt; img}\n}})\nendpoint = &quot;projects\/#{project}\/locations\/#{location}\/endpoints\/#{endpoint}&quot;\n\n\nrequest = Google::Cloud::AIPlatform::V1::PredictRequest.new(\n  endpoint: endpoint,\n  instances: [instance]\n)\n\nresult = client.predict request\np result\n<\/code><\/pre>\n<p>The use of the Google::Protobuf::Value looks ugly to me but it works<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1656947266917,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72827960",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72042363,
        "Question_title":"Run !docker build from Managed Notebook cell in GCP Vertex AI Workbench",
        "Question_body":"<p>I am trying to push a docker image on Google Cloud Platform container registry to define a custom training job directly inside a notebook.<\/p>\n<p>After having prepared the correct Dockerfile and the URI where to push the image that contains my train.py script, I try to push the image directly in a notebook cell.<\/p>\n<p>The exact command I try to execute is: <code>!docker build .\/ -t $IMAGE_URI<\/code>, where IMAGE_URI is the environmental variable previously defined. However I try to run this command I get the error: <code>\/bin\/bash: docker: command not found<\/code>. I also tried to execute it with the magic cell %%bash, importing the subprocess library and also execute the command stored in a .sh file.<\/p>\n<p>Unfortunately none of the above solutions work, they all return the same <strong>command not found<\/strong> error with code 127.<\/p>\n<p>If instead I run the command from a bash present in the Jupyterlab it works fine as expected.<\/p>\n<p>Is there any workaround to make the push execute inside the jupyter notebook? I was trying to keep the whole custom training process inside the same notebook.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651142988007,
        "Question_score":2,
        "Question_tags":"docker|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":248,
        "Owner_creation_time":1648140384823,
        "Owner_last_access_time":1664054562907,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1651151719750,
        "Answer_body":"<p>If you follow this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/create-user-managed-notebooks-instance-console-quickstart\" rel=\"nofollow noreferrer\">guide<\/a> to create a user-managed notebook from Vertex AI workbench and select Python 3, then it comes with Docker available.<\/p>\n<p>So you will be able to use Docker commands such as <code>! docker build .<\/code> inside the user-managed notebook.<\/p>\n<p>Example:\n<a href=\"https:\/\/i.stack.imgur.com\/DtlQp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DtlQp.png\" alt=\"Vertex AI Managed Notebook\" \/><\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1661350818333,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1661351120347,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72042363",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71158453,
        "Question_title":"module 'google.cloud.logging_v2' has no attribute 'MetricsServiceV2Client' Vertex WorkBench",
        "Question_body":"<p>I am trying to set up a locust based framework for ML load test and need to create custom metrics and logs for which the example that I am following is using 'MetricsServiceV2Client' in 'google.cloud.logging_v2' lib.\nIn the Vertex Workbench on GCP inspite being on v3.0 of the google-cloud-logging lib I am getting an issue of import<\/p>\n<p>from google.cloud import logging_v2\nfrom google.cloud.logging_v2 import MetricsServiceV2Client<\/p>\n<p>error: cannot import name 'MetricsServiceV2Client' from 'google.cloud.logging_v2' (\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/logging_v2\/<strong>init<\/strong>.py)<\/p>\n<p>Interestingly when I test the import in  google's cloud console I am able to import without any issue. What could be the issue ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645102105297,
        "Question_score":0,
        "Question_tags":"google-cloud-logging|google-cloud-vertex-ai",
        "Question_view_count":68,
        "Owner_creation_time":1615961068170,
        "Owner_last_access_time":1664041311570,
        "Owner_location":null,
        "Owner_reputation":21,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>from google.cloud.logging_v2.services.metrics_service_v2 import MetricsServiceV2Client this works !!<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1645126649433,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71158453",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70833594,
        "Question_title":"GCP Vertex AI Service Agent access to GCR image Error",
        "Question_body":"<p>I am getting the below error.\nDoes anyone have any idea how to solve it?<\/p>\n<pre><code>Failed to create pipeline job. Error: Vertex AI Service Agent \n'XXXXX@gcp-sa-aiplatform-cc.iam.gserviceaccount.com' should be granted\n access to the image gcr.io\/gcp-project-id\/application:latest\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1643025892210,
        "Question_score":3,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":707,
        "Owner_creation_time":1501131989640,
        "Owner_last_access_time":1663831078047,
        "Owner_location":null,
        "Owner_reputation":329,
        "Owner_up_votes":55,
        "Owner_down_votes":0,
        "Owner_views":88,
        "Question_last_edit_time":1643085989932,
        "Answer_body":"<p><code>{PROJECT_NUMBER}@gcp-sa-aiplatform-cc.iam.gserviceaccount.com<\/code> is google's <a href=\"https:\/\/cloud.google.com\/iam\/docs\/service-agents\" rel=\"nofollow noreferrer\">AI Platform service agent<\/a>.\nThis Service agent requires access to read\/pull the docker image from your project's gcr to create container for pipeline run.<\/p>\n<p>If You have permission to edit <a href=\"https:\/\/cloud.google.com\/iam\/docs\/understanding-roles\" rel=\"nofollow noreferrer\">IAM roles<\/a>, You can try adding <a href=\"https:\/\/cloud.google.com\/artifact-registry\/docs\/access-control#roles\" rel=\"nofollow noreferrer\">Artifact Registry roles<\/a> to the above service agent.\nYou can start with adding <code>roles\/artifactregistry.reader<\/code>.<\/p>\n<p>Hope this helps :)<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1659424830193,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70833594",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73251212,
        "Question_title":"How do I retrieve a model in Vertex AI?",
        "Question_body":"<p>I defined a training job:<\/p>\n<pre><code>job = aiplatform.AutoMLTextTrainingJob(...\n<\/code><\/pre>\n<p>then I created a model by running the job:<\/p>\n<pre><code>model = job.run(...\n<\/code><\/pre>\n<p>It worked fine but it is now the next day and the variable <code>model<\/code> was in a Jupyter notebook and no longer exists. I have tried to get it back with:<\/p>\n<pre><code>from google.cloud import aiplatform_v1beta1\n\ndef sample_get_model():\n    client = aiplatform_v1beta1.ModelServiceClient()\n\n    model_id=id_of_training_pipeline\n    name= f'projects\/{PROJECT}\/locations\/{REGION}\/models\/{model_id}'\n    \n    request = aiplatform_v1beta1.GetModelRequest(name=name)\n    response = client.get_model(request=request)\n    print(response)\n\nsample_get_model()\n<\/code><\/pre>\n<p>I have also tried the id of v1 of the model created in place of <code>id_of_training_pipeline<\/code> and I have tried <code>\/pipelines\/pipeline_id<\/code><\/p>\n<p>but I get:\n<code>E0805 15:12:36.784008212   28406 hpack_parser.cc:1234]       Error parsing metadata: error=invalid value key=content-type value=text\/html; charset=UTF-8<\/code><\/p>\n<p>(<code>PROJECT<\/code> and <code>REGION<\/code> are set correctly).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659709186783,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":45,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/samples\/aiplatform-get-model-sample#aiplatform_get_model_sample-python\" rel=\"nofollow noreferrer\">this<\/a> Google code which works.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1659712728223,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73251212",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69117885,
        "Question_title":"Sending http request Google Vertex AI end point",
        "Question_body":"<p>I've just deployed an ML model on Google vertex AI, it can make predictions using vertex AI web interface. But is it possible to send a request from a browser, for example, to this deployed model. Something like<\/p>\n<pre><code>http:\/\/myapp.cloud.google.com\/input=&quot;features of an example&quot; \n<\/code><\/pre>\n<p>and get the prediction as output.\nThanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1631189210517,
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":929,
        "Owner_creation_time":1377724133300,
        "Owner_last_access_time":1664050723383,
        "Owner_location":null,
        "Owner_reputation":349,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":47,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes, you can send using endpoint URL as.<\/p>\n<pre><code>https:\/\/us-central1-aiplatform.googleapis.com\/v1beta1\/projects\/&lt;PROJECT_ID&gt;\/locations\/us-central1\/endpoints\/&lt;ENDPOINT_ID&gt;:predict\n<\/code><\/pre>\n<p>Data should be given as in POST parameter.<\/p>\n<pre><code>{\n  &quot;instances&quot;: \n    [1.4838871833555929,\n 1.8659883497083019,\n 2.234620276849616,\n 1.0187816540094903,\n -2.530890710602246,\n -1.6046416850441676,\n -0.4651483719733302,\n -0.4952254087173721,\n 0.774676376873553]\n}\n<\/code><\/pre>\n<p>URL should be Region Based.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1631252937240,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1631432417907,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69117885",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73612214,
        "Question_title":"Vertex AI model serving - gRPC access - code pointers \/ samples",
        "Question_body":"<p>We have created an endpoint in Vertex AI. We have got the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-custom-models\" rel=\"nofollow noreferrer\">client library<\/a> route working. However, we also want to figure out the gRPC route since that is closest to the gRPC route we had with self managed TF-Serving.\nCan someone provide a code pointer for Vertex AI model serving using gRPC (preferably in Python)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662394780720,
        "Question_score":1,
        "Question_tags":"python|grpc|google-cloud-vertex-ai",
        "Question_view_count":36,
        "Owner_creation_time":1280773677817,
        "Owner_last_access_time":1662955154927,
        "Owner_location":"Jersey City, NJ",
        "Owner_reputation":4339,
        "Owner_up_votes":16,
        "Owner_down_votes":1,
        "Owner_views":479,
        "Question_last_edit_time":1662401625169,
        "Answer_body":"<p>gRPC can be used through Vertex Prediction private endpoint, but it is not yet officially supported. See sample here: <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/vertex_endpoints\/optimized_tensorflow_runtime\/tabular_optimized_online_prediction.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/vertex_endpoints\/optimized_tensorflow_runtime\/tabular_optimized_online_prediction.ipynb<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1662511382057,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73612214",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73047089,
        "Question_title":"How to start Spark session on Vertex AI workbench Jupyterlab notebook?",
        "Question_body":"<p>Can you kindly show me how do we start the Spark session on Google Cloud Vertex AI workbench Jupyterlab notebook?\n<br> This is working fine in Google Colaboratory by the way.\n<br> What is missing here?<\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\nimport os\nimport sys\n\n# https:\/\/github.com\/jupyter\/jupyter\/issues\/248\nos.environ[&quot;JAVA_HOME&quot;] = &quot;C:\/Program Files\/Java\/jdk-18.0.1.1&quot;\nos.environ[&quot;PATH&quot;] = os.environ[&quot;JAVA_HOME&quot;] + &quot;\/bin:&quot; + os.environ[&quot;PATH&quot;]\n\nimport sparknlp\n\nfrom sparknlp.base import *\nfrom sparknlp.common import *\nfrom sparknlp.annotator import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import SparkSession\n\nimport pandas as pd\n\nspark=sparknlp.start() \n\nprint(&quot;Spark NLP version: &quot;, sparknlp.version())\nprint(&quot;Apache Spark version: &quot;, spark.version)\n\nspark\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/JQI5Z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JQI5Z.png\" alt=\"Error\" \/><\/a><\/p>\n<p><br><br> <strong>UPDATE_2022-07-21:<\/strong>\n<br> Hi @Sayan. I am still not able to start Spark session on Vertex AI workbench Jupyterlab notebook after running the commands =(\n<a href=\"https:\/\/i.stack.imgur.com\/4bxzJ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4bxzJ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\nimport os\n# Included else &quot;JAVA_HOME is not set&quot;\n# https:\/\/github.com\/jupyter\/jupyter\/issues\/248\nos.environ[&quot;JAVA_HOME&quot;] = &quot;C:\/Program Files\/Java\/jdk-18.0.1.1&quot;\nos.environ[&quot;PATH&quot;] = os.environ[&quot;JAVA_HOME&quot;] + &quot;\/bin:&quot; + os.environ[&quot;PATH&quot;]\n\nimport sparknlp\nspark = sparknlp.start()\n\nprint(&quot;Spark NLP version: {}&quot;.format(sparknlp.version()))\nprint(&quot;Apache Spark version: {}&quot;.format(spark.version))\n<\/code><\/pre>\n<p>The error:<\/p>\n<pre><code>\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/bin\/spark-class: line 71: C:\/Program Files\/Java\/jdk-18.0.1.1\/bin\/java: No such file or directory\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/bin\/spark-class: line 96: CMD: bad array subscript\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n\/tmp\/ipykernel_5831\/489505405.py in &lt;module&gt;\n      6 \n      7 import sparknlp\n----&gt; 8 spark = sparknlp.start()\n      9 \n     10 print(&quot;Spark NLP version: {}&quot;.format(sparknlp.version()))\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sparknlp\/__init__.py in start(gpu, m1, memory, cache_folder, log_folder, cluster_tmp_dir, real_time_output, output_level)\n    242         return SparkRealTimeOutput()\n    243     else:\n--&gt; 244         spark_session = start_without_realtime_output()\n    245         return spark_session\n    246 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sparknlp\/__init__.py in start_without_realtime_output()\n    152             builder.config(&quot;spark.jsl.settings.storage.cluster_tmp_dir&quot;, cluster_tmp_dir)\n    153 \n--&gt; 154         return builder.getOrCreate()\n    155 \n    156     def start_with_realtime_output():\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/sql\/session.py in getOrCreate(self)\n    267                         sparkConf.set(key, value)\n    268                     # This SparkContext may be an existing one.\n--&gt; 269                     sc = SparkContext.getOrCreate(sparkConf)\n    270                     # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n    271                     # by all sessions.\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in getOrCreate(cls, conf)\n    481         with SparkContext._lock:\n    482             if SparkContext._active_spark_context is None:\n--&gt; 483                 SparkContext(conf=conf or SparkConf())\n    484             assert SparkContext._active_spark_context is not None\n    485             return SparkContext._active_spark_context\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in __init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\n    193             )\n    194 \n--&gt; 195         SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    196         try:\n    197             self._do_init(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in _ensure_initialized(cls, instance, gateway, conf)\n    415         with SparkContext._lock:\n    416             if not SparkContext._gateway:\n--&gt; 417                 SparkContext._gateway = gateway or launch_gateway(conf)\n    418                 SparkContext._jvm = SparkContext._gateway.jvm\n    419 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/java_gateway.py in launch_gateway(conf, popen_kwargs)\n    104 \n    105             if not os.path.isfile(conn_info_file):\n--&gt; 106                 raise RuntimeError(&quot;Java gateway process exited before sending its port number&quot;)\n    107 \n    108             with open(conn_info_file, &quot;rb&quot;) as info:\n\nRuntimeError: Java gateway process exited before sending its port number\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658299714453,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|jupyter-lab|google-cloud-vertex-ai|johnsnowlabs-spark-nlp",
        "Question_view_count":238,
        "Owner_creation_time":1651609055810,
        "Owner_last_access_time":1663526226877,
        "Owner_location":null,
        "Owner_reputation":105,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Question_last_edit_time":1658406889243,
        "Answer_body":"<p>One possible reason is that <code>Java<\/code> is not installed. When you create a <strong>Python-3 Vertex AI Workbench<\/strong> you can have either <code>Debian<\/code> or <code>Ubuntu<\/code> as an OS and it does not come with Java pre-installed. You need to install it manually.\nTo install you can use<\/p>\n<pre><code>sudo apt-get update\nsudo apt-get install default-jdk\n<\/code><\/pre>\n<p>You can follow this <a href=\"https:\/\/www.digitalocean.com\/community\/tutorials\/how-to-install-java-with-apt-get-on-ubuntu-16-04\" rel=\"nofollow noreferrer\">tutorial<\/a> to install Open JDK.<\/p>\n<p>All your problems lie with installing JDK and setting its path in the environment. Once you do this properly you don't need to set path in python also.\nYour code should look something like this<\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\n#no need to set the environment path\n\nimport sparknlp\n#all other imports\n\nimport pandas as pd\n\nspark=sparknlp.start() \n\nprint(&quot;Spark NLP version: &quot;, sparknlp.version())\nprint(&quot;Apache Spark version: &quot;, spark.version)\n\nspark\n<\/code><\/pre>\n<p><strong>EDIT:<\/strong>\nI have tried your code and had the same error.All I did was Open the terminal inside JupyterLab of the workbench and installed java there.<\/p>\n<p>Opened the JupyterLab from Workbench\n<a href=\"https:\/\/i.stack.imgur.com\/lpE1Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lpE1Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Notebook instance.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/vIZYB.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vIZYB.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Opening the terminal from <em><strong><code>File-&gt;New-&gt;Terminal<\/code><\/strong><\/em>\n<a href=\"https:\/\/i.stack.imgur.com\/CHCzs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CHCzs.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>From here I downloaded and installed the Java.<\/p>\n<p>You can check whether it has been installed and added to your path by running <code>java --version<\/code> it will return the current version.<\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1658384407350,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1658411352227,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73047089",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70855730,
        "Question_title":"Is it possible from a gcp vertex job to hit an http endpoint in another gcp pod in the same project?",
        "Question_body":"<p>I have a custom container vertex endpoint that is passed a url as input so that the job can call it to get a particular frame of data needed for the job. (gcs:\/\/ buckets do work) but I want to specifically use an http request to a server in the same gcp project.<\/p>\n<p>I have tried setting the endpoint up as private using the --networks param on the endpoint but then get the message:<\/p>\n<pre><code>{\n  &quot;error&quot;: {\n    &quot;code&quot;: 400,\n    &quot;message&quot;: &quot;Making request from public OnePlatform API is not allowed on a private Endpoint peered with network (projects\/11111111111\/global\/networks\/some-dev-project-vpc).&quot;,\n    &quot;status&quot;: &quot;FAILED_PRECONDITION&quot;\n  }\n}\n<\/code><\/pre>\n<p>when I try to hit that private vertex endpoint.  I've tried curling it from within a running pod in the same project, but that didn't work either.<\/p>\n<p>Is there a way to do this?\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1643145096180,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":274,
        "Owner_creation_time":1462581330170,
        "Owner_last_access_time":1663976709807,
        "Owner_location":"Berkeley, CA, United States",
        "Owner_reputation":329,
        "Owner_up_votes":101,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Question_last_edit_time":1643175262992,
        "Answer_body":"<p>The error states that your request is to a public API, which may because you are using the public url schema to make your prediction. The structure of vertex endpoints differ between private and public, so double check that you are using the private endpoint url for your requests.<\/p>\n<p><strong>Public<\/strong><\/p>\n<pre><code>https:\/\/{REGION}-aiplatform.googleapis.com\/v1\/projects\/{PROJECT}\/locations\/{REGION}\/endpoints\/{ENDPOINT_ID}:predict\n<\/code><\/pre>\n<p><strong>Private<\/strong><\/p>\n<pre><code>http:\/\/{ENDPOINT_ID}.aiplatform.googleapis.com\/v1\/models\/{DEPLOYED_MODEL_ID}:predict\n<\/code><\/pre>\n<p>You can generate a private endpoint url using the following gcloud command:<\/p>\n<pre><code>gcloud beta ai endpoints describe {ENDPOINT_ID} \\\n  --region=us-central1 \\\n  --format=&quot;value(deployedModels.privateEndpoints.predictHttpUri)&quot;\n<\/code><\/pre>\n<p>More documentation on private endpoints can be found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints#private-predict-uri-format\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1643331021110,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70855730",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71557442,
        "Question_title":"How combine results from multiple models in Google Vertex AI?",
        "Question_body":"<p>I have multiple models in Google Vertex AI and I want to create an endpoint to serve my predictions.\nI need to run aggregation algorithms, like the Voting algorithm on the output of my models.\nI have not found any ways of using the models together so that I can run the voting algorithms on the results.\nDo I have to create a new model, curl my existing models and then run my algorithms on the results?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1647864699137,
        "Question_score":1,
        "Question_tags":"google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":253,
        "Owner_creation_time":1372407778700,
        "Owner_last_access_time":1663592257430,
        "Owner_location":"Oslo, Norway",
        "Owner_reputation":134,
        "Owner_up_votes":303,
        "Owner_down_votes":0,
        "Owner_views":74,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There is no in-built provision to implement aggregation algorithms in Vertex AI. To <code>curl<\/code> results from the models then aggregate them, we would need to deploy all of them to individual endpoints. Instead, I would suggest the below method to deploy the models and the meta-model(aggregate model) to a single endpoint using <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/use-custom-container\" rel=\"nofollow noreferrer\">custom containers for prediction<\/a>. The custom container requirements can be found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>You can load the model artifacts from GCS into a custom container. If the same set of models are used (i.e) the input models to the meta-model do not change, you can package them inside the container to reduce load time. Then, a custom HTTP logic can be used to return the aggregation output like so. This is a sample custom flask server logic.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_models_from_gcs():\n    ## Pull the required model artifacts from GCS and load them here.\n    models = [model_1, model_2, model_3]\n    return models\n\ndef aggregate_predictions(predictions):\n    ## Your aggregation algorithm here\n    return aggregated_result\n\n\n@app.post(os.environ['AIP_PREDICT_ROUTE'])\nasync def predict(request: Request):\n    body = await request.json()\n    instances = body[&quot;instances&quot;]\n    inputs = np.asarray(instances)\n    preprocessed_inputs = _preprocessor.preprocess(inputs)\n\n    models = get_models_from_gcs()\n    predictions = []\n    \n    for model in models:\n        predictions.append(model.predict(preprocessed_inputs))\n\n    aggregated_result = aggregate_predictions(predictions)\n\n    return {&quot;aggregated_predictions&quot;: aggregated_result}\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1647950025930,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1647950484409,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71557442",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71430286,
        "Question_title":"Permission Denied using Google AiPlatform ModelServiceClient",
        "Question_body":"<p>I am following a guide to get a Vertex AI pipeline working:<\/p>\n<p><a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5\" rel=\"nofollow noreferrer\">https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5<\/a><\/p>\n<p>I have implemented the following custom component:<\/p>\n<pre><code>from google.cloud import aiplatform as aip\nfrom google.oauth2 import service_account\n\nproject = &quot;project-id&quot;\nregion = &quot;us-central1&quot;\ndisplay_name = &quot;lookalike_model_pipeline_1646929843&quot;\n\nmodel_name = f&quot;projects\/{project}\/locations\/{region}\/models\/{display_name}&quot;\napi_endpoint = &quot;us-central1-aiplatform.googleapis.com&quot; #europe-west2\nmodel_resource_path = model_name\nclient_options = {&quot;api_endpoint&quot;: api_endpoint}\n\n# Initialize client that will be used to create and send requests.\nclient = aip.gapic.ModelServiceClient(credentials=service_account.Credentials.from_service_account_file('..\\\\service_accounts\\\\aiplatform_sa.json'), \nclient_options=client_options)\n#get model evaluation\nresponse = client.list_model_evaluations(parent=model_name)\n<\/code><\/pre>\n<p>And I get following error:<\/p>\n<pre><code>(&lt;class 'google.api_core.exceptions.PermissionDenied'&gt;, PermissionDenied(&quot;Permission 'aiplatform.modelEvaluations.list' denied on resource '\/\/aiplatform.googleapis.com\/projects\/project-id\/locations\/us-central1\/models\/lookalike_model_pipeline_1646929843' (or it may not exist).&quot;), &lt;traceback object at 0x000002414D06B9C0&gt;)\n<\/code><\/pre>\n<p>The model definitely exists and has finished training. I have given myself admin rights in the aiplatform service account. In the guide, they do not use a service account, but uses only client_options instead. The client_option has the wrong type since it is a dict(str, str) when it should be: Optional['ClientOptions']. But this doesn't cause an error.<\/p>\n<p>My main question is: how do I get around this permission issue?<\/p>\n<p>My subquestions are:<\/p>\n<ol>\n<li>How can I use my model_name variable in a URL to get to the model?<\/li>\n<li>How can I create an Optional['ClientOptions'] object to pass as client_option<\/li>\n<li>Is there another way I can list_model_evaluations from a model that is in VertexAI, trained using automl?<\/li>\n<\/ol>\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":7,
        "Question_creation_time":1646943487337,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|model|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":936,
        "Owner_creation_time":1562706291280,
        "Owner_last_access_time":1664033431527,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1646987974512,
        "Answer_body":"<p>I tried using your code and it did not also work for me and got a different error. As @DazWilkin mentioned it is recommended to use the Cloud Client.<\/p>\n<p>I used <code>aiplatform_v1<\/code> and it worked fine. One thing I noticed is that you should always define a value for <code>client_options<\/code> so it will point to the correct endpoint. Checking the code for <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/google\/cloud\/aiplatform_v1\/services\/model_service\/client.py#L122\" rel=\"nofollow noreferrer\">ModelServiceClient<\/a>, if I'm not mistaken the endpoint defaults to <strong>&quot;aiplatform.googleapis.com&quot;<\/strong> which don't have a location prepended. AFAIK the endpoint should prepend a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/locations\" rel=\"nofollow noreferrer\">location<\/a>.<\/p>\n<p>See code below. I used AutoML models and it returns their model evaluations.<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\nfrom typing import Optional\n\ndef get_model_eval(\n        project_id: str,\n        model_id: str,\n        client_options: dict,\n        location: str = 'us-central1',\n        ):\n\n    client_model = aiplatform.services.model_service.ModelServiceClient(client_options=client_options)\n\n    model_name = f'projects\/{project_id}\/locations\/{location}\/models\/{model_id}'\n    list_eval_request = aiplatform.types.ListModelEvaluationsRequest(parent=model_name)\n    list_eval = client_model.list_model_evaluations(request=list_eval_request)\n    print(list_eval)\n\n\n\napi_endpoint = 'us-central1-aiplatform.googleapis.com'\nclient_options = {&quot;api_endpoint&quot;: api_endpoint} # api_endpoint is required for client_options\nproject_id = 'project-id'\nlocation = 'us-central1'\nmodel_id = '99999999999' # aiplatform_v1 uses the model_id\n\nget_model_eval(\n        client_options = client_options,\n        project_id = project_id,\n        location = location,\n        model_id = model_id,\n        )\n<\/code><\/pre>\n<p>This is an output snippet from my AutoML Text Classification:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/RXrxh.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RXrxh.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1646971559037,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1646998024380,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71430286",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70449117,
        "Question_title":"Vertex AI Endpoints scales to 0 before increasing number of replicas",
        "Question_body":"<p>I have an endpoint in <code>us-east<\/code> which serves a custom imported model (docker image).<\/p>\n<p>This endpoint uses <code>min replicas = 1<\/code> and <code>max replicas = 100<\/code>.<\/p>\n<p>Sometimes, Vertex AI will require the model to scale from 1 to 2.<\/p>\n<p>However, there seems to be an issue causing the number of replicas to go from <code>1 -&gt; 0 -&gt; 2<\/code> instead of <code>1 -&gt; 2<\/code>.<\/p>\n<p>This causes several 504 (Gateway Timeout) errors in my API and the way to solve that was setting <code>min replicas &gt; 1<\/code>, highly impacting the monthly cost of the application.<\/p>\n<p>Is this some known issue to Vertex AI\/GCP services, is there anyway to fix it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640176354140,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":77,
        "Owner_creation_time":1400036379907,
        "Owner_last_access_time":1663015368213,
        "Owner_location":"Rio de Janeiro - RJ, Brasil",
        "Owner_reputation":490,
        "Owner_up_votes":41,
        "Owner_down_votes":3,
        "Owner_views":65,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The intermittent <code>504<\/code> errors could be a result of an endpoint that is under-provisioned to handle the load. It can also happen if too many prediction requests are sent to the endpoint before it has a chance to scale up.<\/p>\n<p>Traffic splitting of the incoming prediction requests is done randomly. So, multiple requests may end up on the same model server at the same time. This can happen even if the overall Queries Per Second (QPS) is low, and especially when the QPS is spiky. This contributes to the requests being queued up if the model server isn't able to handle the load. This is what results in a 504 error.<\/p>\n<p><strong>Recommendations to mitigate the <code>504<\/code> errors are as follows:<\/strong><\/p>\n<ul>\n<li><p>Improve the container's ability to use all resources in the container. One thing to keep in mind about resource utilization is whether the model server is single-threaded or multi-threaded. The container may not be using up all the cores and\/or requests may be queuing up, hence are served only one-at-a-time.<\/p>\n<\/li>\n<li><p>Autoscaling is happening, it just might need to be tuned to the prediction workload and expectations. A lower utilization threshold would trigger autoscaling sooner.<\/p>\n<\/li>\n<li><p>Perform an exponential backoff while the deployment is scaling. This way, there is a retry mechanism to handle failed requests.<\/p>\n<\/li>\n<li><p>Provision a higher minimum replica count for the endpoint, which you have already implemented.<\/p>\n<\/li>\n<\/ul>\n<p>If the above recommendations do not solve the problem or in general require further investigation of these errors, please reach out to <a href=\"https:\/\/cloud.google.com\/support-hub#section-2\" rel=\"nofollow noreferrer\">GCP support<\/a> in case you have a <a href=\"https:\/\/cloud.google.com\/support\/\" rel=\"nofollow noreferrer\">support plan<\/a>. Otherwise, please open an issue in the <a href=\"https:\/\/issuetracker.google.com\/issues\/new?component=1134259&amp;template=1640573\" rel=\"nofollow noreferrer\">issue tracker<\/a>.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1641201707430,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1641204696607,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70449117",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69857932,
        "Question_title":"Specify signature name on Vertex AI Predict",
        "Question_body":"<p>I've deployed a tensorflow model in vertex AI platform using TFX Pipelines. The model have custom serving signatures but I'm strugling to specify the signature when I'm predicting.<\/p>\n<p>I've the exact same model deployed in GCP AI Platform and I'm able to specify it.<\/p>\n<p>According to the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-custom-models?authuser=5&amp;_ga=2.16305585.-680038964.1635267137#formatting-prediction-input\" rel=\"nofollow noreferrer\">vertex documentation<\/a>, we must pass a dictionary containing the Instances (List) and the Parameters (Dict) values.<\/p>\n<p>I've submitted these arguments to <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predict_custom_trained_model_sample.py\" rel=\"nofollow noreferrer\">this function<\/a>:<\/p>\n<pre><code>instances: [{&quot;argument_n&quot;: &quot;value&quot;}]\n\nparameters: {&quot;signature_name&quot;: &quot;name_of_signature&quot;}\n<\/code><\/pre>\n<p>Doesn't work, it still get the default signature of the model.<\/p>\n<p>In GCP AI Platform, I've been able to predict directly specifying in the body of the request the signature name:<\/p>\n<pre><code>response = service.projects().predict(\n        name=name,\n        body={&quot;instances&quot;: instances,\n        &quot;signature_name&quot;: &quot;name_of_signature&quot;},\n    ).execute()\n<\/code><\/pre>\n<p>@EDIT\nI've discovered that with the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.endpoints\/rawPredict\" rel=\"nofollow noreferrer\">rawPredict method<\/a> from gcloud it works.<\/p>\n<p>Here is an example:<\/p>\n<pre><code>!gcloud ai endpoints raw-predict {endpoint} --region=us-central1 \\\n--request='{&quot;signature_name&quot;:&quot;name_of_the_signature&quot;, \\\n&quot;instances&quot;: [{&quot;instance_0&quot;: [&quot;value_0&quot;], &quot;instance_1&quot;: [&quot;value_1&quot;]}]}'\n<\/code><\/pre>\n<p>Unfortunately, looking at <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/google\/cloud\/aiplatform\/models.py\" rel=\"nofollow noreferrer\">google api models code<\/a> it only have the predict method, not the raw_predict. So I don't know if it's available through python sdk right now.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1636138079960,
        "Question_score":4,
        "Question_tags":"google-cloud-platform|vertex|google-ai-platform|tfx|google-cloud-vertex-ai",
        "Question_view_count":508,
        "Owner_creation_time":1606605180560,
        "Owner_last_access_time":1664067783153,
        "Owner_location":"Brazil",
        "Owner_reputation":98,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1636203038129,
        "Answer_body":"<p>Vertex AI is a newer platform with limitations that will be improved over time. \u201csignature_name\u201d can be added to HTTP JSON Payload in <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.endpoints\/rawPredict\" rel=\"nofollow noreferrer\">RawPredictRequest<\/a> or from <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/endpoints\/raw-predict\" rel=\"nofollow noreferrer\">gcloud<\/a> as you have done but right now this is not available in regular predict requests.<\/p>\n<p><strong>Using HTTP JSON payload :<\/strong><\/p>\n<p>Example:<\/p>\n<p>input.json :<\/p>\n<pre><code>{\n   &quot;instances&quot;: [\n     [&quot;male&quot;, 29.8811345124283, 26.0, 1, &quot;S&quot;, &quot;New York, NY&quot;, 0, 0],\n     [&quot;female&quot;, 48.0, 39.6, 1, &quot;C&quot;, &quot;London \/ Paris&quot;, 0, 1]],\n \n     &quot;signature_name&quot;: &lt;string&gt;\n}\n\n<\/code><\/pre>\n<pre><code>curl \\\n-X POST \\\n-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n-H &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-aiplatform.googleapis.com\/v1\/projects\/${PROJECT_ID}\/locations\/us-central1\/endpoints\/${ENDPOINT_ID}:rawPredict \\\n-d &quot;@input.json&quot;\n\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1636439088550,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1636453108543,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69857932",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73649262,
        "Question_title":"Create custom kernel via post-startup script in Vertex AI User Managed notebook",
        "Question_body":"<p>I am trying to use a post-startup script to create a Vertex AI User Managed Notebook whose Jupyter Lab has a dedicated virtual environment and corresponding computing kernel when first launched. I have had success creating the instance and then, as a second manual step from within the Jupyter Lab &gt; Terminal, running a bash script like so:<\/p>\n<pre><code>#!\/bin\/bash\ncd \/home\/jupyter\nmkdir -p env\ncd env\npython3 -m venv envName --system-site-packages\nsource envName\/bin\/activate\nenvName\/bin\/python3 -m pip install --upgrade pip\npython -m ipykernel install --user --name=envName\npip3 install geemap --user \npip3 install earthengine-api --user \npip3 install ipyleaflet --user \npip3 install folium --user \npip3 install voila --user \npip3 install jupyterlab_widgets\ndeactivate\njupyter labextension install --no-build @jupyter-widgets\/jupyterlab-manager jupyter-leaflet\njupyter lab build --dev-build=False --minimize=False\njupyter labextension enable @jupyter-widgets\/jupyterlab-manager\n<\/code><\/pre>\n<p>However, I have not had luck using this code as a post-startup script (being supplied through the console creation tools, as opposed to command line, thus far). When I open Jupyter Lab and look at the relevant structures, I find that there is no environment or kernel. Could someone please provide a working example that accomplishes my aim, or otherwise describe the order of build steps that one would follow?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1662640804323,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|jupyter-lab|google-cloud-vertex-ai|gcp-ai-platform-notebook",
        "Question_view_count":85,
        "Owner_creation_time":1459350905810,
        "Owner_last_access_time":1663857124693,
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Post startup scripts run as root.\nWhen you run:<\/p>\n<pre><code>python -m ipykernel install --user --name=envName\n<\/code><\/pre>\n<p>Notebook is using current user which is <code>root<\/code> vs when you use Terminal, which is running as <code>jupyter<\/code> user.<\/p>\n<p>Option 1) Have 2 scripts:<\/p>\n<ul>\n<li>Script A. Contents specified in original post. Example: <code>gs:\/\/newsml-us-central1\/so73649262.sh<\/code><\/li>\n<li>Script B. Downloads script and execute it as <code>jupyter<\/code>. Example: <code>gs:\/\/newsml-us-central1\/so1.sh<\/code> and use it as post-startup script.<\/li>\n<\/ul>\n<pre><code>#!\/bin\/bash\n\nset -x\n\ngsutil cp gs:\/\/newsml-us-central1\/so73649262.sh \/home\/jupyter\nchown jupyter \/home\/jupyter\/so73649262.sh\nchmod a+x \/home\/jupyter\/so73649262.sh\nsu -c '\/home\/jupyter\/so73649262.sh' jupyter\n<\/code><\/pre>\n<p>Option 2) Create a file in bash using EOF. Write the contents into a single file and execute it as mentioned above.<\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1662967888203,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73649262",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69978953,
        "Question_title":"Vertex AI custom prediction vs Google Kubernetes Engine",
        "Question_body":"<p>I have been exploring using Vertex AI for my machine learning workflows. Because deploying different models to the same endpoint utilizing only one node is not possible in Vertex AI, I am considering a <a href=\"https:\/\/stackoverflow.com\/questions\/69878915\/deploying-multiple-models-to-same-endpoint-in-vertex-ai\">workaround<\/a>. With this workaround, I will be unable to use many Vertex AI features, like model monitoring, feature attribution etc., and it simply becomes, I think, a managed alternative to running the prediction application on, say, a GKE cluster. So, besides the cost difference, I am exploring if running the custom prediction container on Vertex AI vs. GKE will involve any limitations, for example, only <strong>N1<\/strong> machine types are available for prediction in Vertex AI<\/p>\n<p>There is a similar <a href=\"https:\/\/stackoverflow.com\/questions\/67930882\/google-kubernetes-engine-vs-vertex-ai-ai-platform-unified-for-serving-model-pr\">question<\/a>, but I it does not raise the specific questions I hope to have answered.<\/p>\n<ul>\n<li>I am not sure of the available disk space. In Vertex AI, one can specify the machine type, such as n1-standard-2 etc., but I am not sure what disk space will be available and if\/how one can specify it? In the custom container code, I may copy multiple model artifacts, or data from outside sources to the local directory before processing them so understanding any disk space limitations is important.<\/li>\n<li>For custom training in Vertex AI, one can use an interactive shell to inspect the container where the training code is running, as described <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/monitor-debug-interactive-shell\" rel=\"nofollow noreferrer\">here<\/a>. Is something like this possible for a custom prediction container? I have not found anything in the docs.<\/li>\n<li>For custom training, one can use a private IP for custom training as described <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-private-ip\" rel=\"nofollow noreferrer\">here<\/a>. Again, I have not found anything similar for custom prediction in the docs, is it possible?<\/li>\n<\/ul>\n<p>If you know of any other possible limitations, please post.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636999914017,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-kubernetes-engine|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":304,
        "Owner_creation_time":1471292986790,
        "Owner_last_access_time":1664058170413,
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Question_last_edit_time":null,
        "Answer_body":"<ol>\n<li>we don't specify a disk size, so default to 100GB<\/li>\n<li>I'm not aware of this right now. But if it's a custom container, you could just run it locally or on GKE for debugging purpose.<\/li>\n<li>are you looking for this? <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints<\/a><\/li>\n<\/ol>",
        "Answer_comment_count":2,
        "Answer_creation_time":1637122254800,
        "Answer_score":1,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69978953",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69681031,
        "Question_title":"how to concatenate the OutputPathPlaceholder with a string with Kubeflow pipelines?",
        "Question_body":"<p>I am using Kubeflow pipelines (KFP) with GCP Vertex AI pipelines. I am using <code>kfp==1.8.5<\/code> (kfp SDK) and <code>google-cloud-pipeline-components==0.1.7<\/code>. Not sure if I can find which version of Kubeflow is used on GCP.<\/p>\n<p>I am bulding a component (yaml) using python inspired form this <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3748#issuecomment-627698554\" rel=\"nofollow noreferrer\">Github issue<\/a>. I am defining an output like:<\/p>\n<pre><code>outputs=[(OutputSpec(name='drt_model', type='Model'))]\n<\/code><\/pre>\n<p>This will be a base output directory to store few artifacts on Cloud Storage like model checkpoints and model.<\/p>\n<p>I would to keep one base output directory but add sub directories depending of the artifact:<\/p>\n<ul>\n<li>&lt;output_dir_base&gt;\/model<\/li>\n<li>&lt;output_dir_base&gt;\/checkpoints<\/li>\n<li>&lt;output_dir_base&gt;\/tensorboard<\/li>\n<\/ul>\n<p>but I didn't find how to concatenate the <strong>OutputPathPlaceholder('drt_model')<\/strong> with a string like <strong>'\/model'<\/strong>.<\/p>\n<p>How can append extra folder structure like \/model or \/tensorboard to the OutputPathPlaceholder that KFP will set during run time ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1634924192977,
        "Question_score":1,
        "Question_tags":"kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":312,
        "Owner_creation_time":1465222092253,
        "Owner_last_access_time":1663858783617,
        "Owner_location":"Z\u00fcrich, Switzerland",
        "Owner_reputation":1414,
        "Owner_up_votes":258,
        "Owner_down_votes":3,
        "Owner_views":478,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I didn't realized in the first place that <code>ConcatPlaceholder<\/code> accept both Artifact and string. This is exactly what I wanted to achieve:<\/p>\n<pre><code>ConcatPlaceholder([OutputPathPlaceholder('drt_model'), '\/model'])\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1637263566353,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69681031",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72251787,
        "Question_title":"Permission \"artifactregistry.repositories.downloadArtifacts\" denied on resource",
        "Question_body":"<p>While the artifact repository was successfully creating, running a docker push to push the image to the google artifact registry fails with a permissions error even after granting all artifact permissions to the accounting I am using on gcloud cli.<\/p>\n<p><strong>Command used to push image:<\/strong><\/p>\n<pre><code>docker push us-central1-docker.pkg.dev\/project-id\/repo-name:v2\n<\/code><\/pre>\n<p><strong>Error message:<\/strong><\/p>\n<pre><code>The push refers to repository [us-central1-docker.pkg.dev\/project-id\/repo-name]\n6f6f4a472f31: Preparing\nbc096d7549c4: Preparing\n5f70bf18a086: Preparing\n20bed28d4def: Preparing\n2a3255c6d9fb: Preparing\n3f5d38b4936d: Waiting\n7be8268e2fb0: Waiting\nb889a93a79dd: Waiting\n9d4550089a93: Waiting\na7934564e6b9: Waiting\n1b7cceb6a07c: Waiting\nb274e8788e0c: Waiting\n78658088978a: Waiting\ndenied: Permission &quot;artifactregistry.repositories.downloadArtifacts&quot; denied on resource &quot;projects\/project-id\/locations\/us-central1\/repositories\/repo-name&quot; (or it may not exist)\n\n\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1652644857243,
        "Question_score":12,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|docker-push",
        "Question_view_count":5722,
        "Owner_creation_time":1426920929353,
        "Owner_last_access_time":1662127122187,
        "Owner_location":"Bangkok",
        "Owner_reputation":194,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":1652667678670,
        "Answer_body":"<p>I was able to recreate your use case. This happens when you are trying to push an image on a <code>repository<\/code> in which its specific hostname (associated with it's repository location) is not yet  added to the credential helper configuration for authentication. You may refer to this <a href=\"https:\/\/cloud.google.com\/artifact-registry\/docs\/docker\/authentication\" rel=\"noreferrer\">Setting up authentication for Docker <\/a> as also provided by @DazWilkin in the comments for more details.<\/p>\n<p>In my example, I was trying to push an image on a repository that has a location of <code>us-east1<\/code> and got the same error since it is not yet added to the credential helper configuration.\n<a href=\"https:\/\/i.stack.imgur.com\/NQeIf.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NQeIf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>And after I ran the authentication using below command (specifically for us-east1 since it is the <code>location<\/code> of my repository), the image was successfully pushed:<\/p>\n<pre><code>gcloud auth configure-docker us-east1-docker.pkg.dev\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q2Q9x.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q2Q9x.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><em><strong>QUICK TIP<\/strong><\/em>: You may  get your authentication command specific for your repository when you open your desired repository in the <a href=\"https:\/\/console.cloud.google.com\/artifacts\" rel=\"noreferrer\">console<\/a>, and then click on the <code>SETUP INSTRUCTIONS<\/code>.\n<a href=\"https:\/\/i.stack.imgur.com\/KBjqa.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KBjqa.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1652683203067,
        "Answer_score":23,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1654468583460,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72251787",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71016472,
        "Question_title":"Vertex AI forecasting AutoML datatype mismatch",
        "Question_body":"<p>I could train the vertex AI AutoML forecating model but when I do batch prediction I get following error<\/p>\n<blockquote>\n<p>Batch prediction job batch_prediction encountered the following\nerrors:<\/p>\n<pre><code>Column &quot;sales&quot; expects type: NUMBER, the actual type is: STRING.\n<\/code><\/pre>\n<\/blockquote>\n<p>Below is a sample of test set I am passing for batch prediction in big query.<\/p>\n<p>According to the documentation for batch prediction we have to send some training\/historical data and forecasting dates. I did just that.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/J7eYT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/J7eYT.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1644227346773,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-ml|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":222,
        "Owner_creation_time":1450288149287,
        "Owner_last_access_time":1661168365350,
        "Owner_location":null,
        "Owner_reputation":500,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Question_last_edit_time":1644237395456,
        "Answer_body":"<p>Google recommend you to use the same input format for ingraining and prediction. Seams you have trained your model using a input format here the column sales were a <code>numeric<\/code> type, and now in the prediction you a using a BigQuery table with the <code>sales<\/code> column as <code>string<\/code>.<\/p>\n<p>Delete this table and import the data again defining the schema <strong>manually<\/strong>, and set sales as a numeric field, as following:<\/p>\n<pre><code>date:DATE,\nstore_product_id:STRING,\nsales:NUMERIC\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1644420672183,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71016472",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73252066,
        "Question_title":"Query BQ table with Jupyter Notebook across owned projects",
        "Question_body":"<p>I have an issue about acessing data in a BigQuery table in one project using a VertexAI in another project.<\/p>\n<p>Now, I own both project and have service accounts in both project, which implies that I also have the key (credential.json in the project containing the data) which I can use to define my client:<\/p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\n\ncredentials = service_account.Credentials.from_service_account_file('credentials.json')\nproject_id = 'cloud-billing-XXXX'\nclient = bigquery.Client(credentials= credentials,project=project_id)\n<\/code><\/pre>\n<p>which should be enough to run:<\/p>\n<pre><code>%%bigquery\nSELECT * FROM `cloud-billing-XXXX.all_billing_detailed.gcp_billing_export` limit 100\n<\/code><\/pre>\n<p>but I get the error:<\/p>\n<pre><code>ERROR:\n 403 Access Denied: Table cloud-billing-XXXX:all_billing_detailed.gcp_billing_export: User does not have permission to query table cloud-billing-XXXX:all_billing_detailed.gcp_billing_export.\n<\/code><\/pre>\n<p>I can, from the project containing my notebook, query the table in BigQuery. This makes me think that the problem is a VertexAI permission issue. I read somewhere that the service account used when defining the notebook must match the service account in the project the data resides in. I tried to create a workbench notebook with the service account in the first project and it is created but when I try to open it it refuses to do so and get an error message.<\/p>\n<p>I've also tried to grant Editor and job user permissions across both project but that wouldn't work either.<\/p>\n<p>Any experiences and ideas on how to solve this would be greatly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1659713227817,
        "Question_score":1,
        "Question_tags":"google-bigquery|jupyter-notebook|service-accounts|google-cloud-vertex-ai",
        "Question_view_count":67,
        "Owner_creation_time":1442929315877,
        "Owner_last_access_time":1664024443763,
        "Owner_location":"Stockholm, Sverige",
        "Owner_reputation":4598,
        "Owner_up_votes":1855,
        "Owner_down_votes":13,
        "Owner_views":855,
        "Question_last_edit_time":null,
        "Answer_body":"<ol>\n<li>As a mentioned in the comment: add your notebook service account to the first project (which contains your BigQuery data) and grant it with <strong>Bigquery Job User<\/strong> and <strong>BigQuery Data Viewer<\/strong> permissions.<\/li>\n<li>You can query data directly in you python code (without using &quot;magic&quot; %%bigquery). Just add the next two rows:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>pct_overlap_terms_by_days_apart = client.query(&quot;SELECT * FROM `cloud-billing-XXXX.all_billing_detailed.gcp_billing_export` limit 100&quot;).to_dataframe()\npct_overlap_terms_by_days_apart.head()\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Check out table name. If table path is wrong, then you'll get the same error: <strong>403 Access Denied<\/strong>.<\/li>\n<\/ol>",
        "Answer_comment_count":0,
        "Answer_creation_time":1659745527510,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73252066",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72881056,
        "Question_title":"Why does my dbt container hang in Vertex AI?",
        "Question_body":"<p>I am trying to follow <a href=\"https:\/\/datatonic.com\/insights\/dbt-vertex-ai-pipelines-google-cloud\/\" rel=\"nofollow noreferrer\">this<\/a> tutorial to run a dbt docker image as a Vertex AI component. When the pipeline runs the component just seems to sit there for ever. Is there any way of debugging the component?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657099839053,
        "Question_score":0,
        "Question_tags":"docker|dbt|google-cloud-vertex-ai|kfp",
        "Question_view_count":73,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You see the pipeline logs to get an idea regarding what is going on in the pipeline.\nFrom the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/logging\" rel=\"nofollow noreferrer\">doc<\/a><\/p>\n<blockquote>\n<p>After you define and build a pipeline, you can use Cloud Logging to create log entries to help you monitor events such as pipeline failures. You can create custom log-based metrics that send notifications when the rate of pipeline failures reaches a given threshold.<\/p>\n<\/blockquote>\n<p>You can also select a component inside Pipeline's runtime graph and then view detailed info and logs of that particular component.\n<a href=\"https:\/\/i.stack.imgur.com\/QXm02.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QXm02.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Also you can hover your cursor on the component status area(green check or grey disabled icon) to check the current status of that component.<\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1657175196140,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1657175516072,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72881056",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70968460,
        "Question_title":"ModelUploadOp step failing with custom prediction container",
        "Question_body":"<p>I am currenlty trying to deploy a Vertex pipeline to achieve the following:<\/p>\n<ol>\n<li><p>Train a custom model (from a custom training python package) and dump model artifacts (trained model and data preprocessor that will be sed at prediction time). This is step is working fine as I can see new resources being created in the storage bucket.<\/p>\n<\/li>\n<li><p>Create a model resource via <code>ModelUploadOp<\/code>. This step fails for some reason when specifying <code>serving_container_environment_variables<\/code> and <code>serving_container_ports<\/code> with the error message in the <strong>errors<\/strong> section below. This is somewhat surprising as they are both needed by the prediction container and environment variables are passed as a dict as specified in the documentation.<br \/>\nThis step works just fine using <code>gcloud<\/code> commands:<\/p>\n<\/li>\n<\/ol>\n<pre class=\"lang-sh prettyprint-override\"><code>gcloud ai models upload \\\n    --region us-west1 \\\n    --display-name session_model_latest \\\n    --container-image-uri gcr.io\/and-reporting\/pred:latest \\\n    --container-env-vars=&quot;MODEL_BUCKET=ml_session_model&quot; \\\n    --container-health-route=\/\/health \\\n    --container-predict-route=\/\/predict \\\n    --container-ports=5000\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Create an endpoint.<\/li>\n<li>Deploy the model to the endpoint.<\/li>\n<\/ol>\n<p>There is clearly something that I am getting wrong with Vertex, the components <a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.2.2\/index.html\" rel=\"nofollow noreferrer\">documentation<\/a> doesn't help much in this case.<\/p>\n<h2>Pipeline<\/h2>\n<pre class=\"lang-py prettyprint-override\"><code>from datetime import datetime\n\nimport kfp\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom kfp.v2 import compiler\n\nPIPELINE_ROOT = &quot;gs:\/\/ml_model_bucket\/pipeline_root&quot;\n\n\n@kfp.dsl.pipeline(name=&quot;session-train-deploy&quot;, pipeline_root=PIPELINE_ROOT)\ndef pipeline():\n    training_op = gcc_aip.CustomPythonPackageTrainingJobRunOp(\n        project=&quot;my-project&quot;,\n        location=&quot;us-west1&quot;,\n        display_name=&quot;train_session_model&quot;,\n        model_display_name=&quot;session_model&quot;,\n        service_account=&quot;name@my-project.iam.gserviceaccount.com&quot;,\n        environment_variables={&quot;MODEL_BUCKET&quot;: &quot;ml_session_model&quot;},\n        python_module_name=&quot;trainer.train&quot;,\n        staging_bucket=&quot;gs:\/\/ml_model_bucket\/&quot;,\n        base_output_dir=&quot;gs:\/\/ml_model_bucket\/&quot;,\n        args=[\n            &quot;--gcs-data-path&quot;,\n            &quot;gs:\/\/ml_model_data\/2019-Oct_short.csv&quot;,\n            &quot;--gcs-model-path&quot;,\n            &quot;gs:\/\/ml_model_bucket\/model\/model.joblib&quot;,\n            &quot;--gcs-preproc-path&quot;,\n            &quot;gs:\/\/ml_model_bucket\/model\/preproc.pkl&quot;,\n        ],\n        container_uri=&quot;us-docker.pkg.dev\/vertex-ai\/training\/scikit-learn-cpu.0-23:latest&quot;,\n        python_package_gcs_uri=&quot;gs:\/\/ml_model_bucket\/trainer-0.0.1.tar.gz&quot;,\n        model_serving_container_image_uri=&quot;gcr.io\/my-project\/pred&quot;,\n        model_serving_container_predict_route=&quot;\/predict&quot;,\n        model_serving_container_health_route=&quot;\/health&quot;,\n        model_serving_container_ports=[5000],\n        model_serving_container_environment_variables={\n            &quot;MODEL_BUCKET&quot;: &quot;ml_model_bucket\/model&quot;\n        },\n    )\n\n    model_upload_op = gcc_aip.ModelUploadOp(\n        project=&quot;and-reporting&quot;,\n        location=&quot;us-west1&quot;,\n        display_name=&quot;session_model&quot;,\n        serving_container_image_uri=&quot;gcr.io\/my-project\/pred:latest&quot;,\n        # When passing the following 2 arguments this step fails...\n        serving_container_environment_variables={&quot;MODEL_BUCKET&quot;: &quot;ml_model_bucket\/model&quot;},\n        serving_container_ports=[5000],\n        serving_container_predict_route=&quot;\/predict&quot;,\n        serving_container_health_route=&quot;\/health&quot;,\n    )\n    model_upload_op.after(training_op)\n\n    endpoint_create_op = gcc_aip.EndpointCreateOp(\n        project=&quot;my-project&quot;,\n        location=&quot;us-west1&quot;,\n        display_name=&quot;pipeline_endpoint&quot;,\n    )\n\n    model_deploy_op = gcc_aip.ModelDeployOp(\n        model=model_upload_op.outputs[&quot;model&quot;],\n        endpoint=endpoint_create_op.outputs[&quot;endpoint&quot;],\n        deployed_model_display_name=&quot;session_model&quot;,\n        traffic_split={&quot;0&quot;: 100},\n        service_account=&quot;name@my-project.iam.gserviceaccount.com&quot;,\n    )\n    model_deploy_op.after(endpoint_create_op)\n\n\nif __name__ == &quot;__main__&quot;:\n    ts = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n    compiler.Compiler().compile(pipeline, &quot;custom_train_pipeline.json&quot;)\n    pipeline_job = aiplatform.PipelineJob(\n        display_name=&quot;session_train_and_deploy&quot;,\n        template_path=&quot;custom_train_pipeline.json&quot;,\n        job_id=f&quot;session-custom-pipeline-{ts}&quot;,\n        enable_caching=True,\n    )\n    pipeline_job.submit()\n\n<\/code><\/pre>\n<h3>Errors and notes<\/h3>\n<ol>\n<li>When specifying <code>serving_container_environment_variables<\/code> and <code>serving_container_ports<\/code> the step fails with the following error:<\/li>\n<\/ol>\n<pre><code>{'code': 400, 'message': 'Invalid JSON payload received. Unknown name &quot;MODEL_BUCKET&quot; at \\'model.container_spec.env[0]\\': Cannot find field.\\nInvalid value at \\'model.container_spec.ports[0]\\' (type.googleapis.com\/google.cloud.aiplatform.v1.Port), 5000', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com\/google.rpc.BadRequest', 'fieldViolations': [{'field': 'model.container_spec.env[0]', 'description': 'Invalid JSON payload received. Unknown name &quot;MODEL_BUCKET&quot; at \\'model.container_spec.env[0]\\': Cannot find field.'}, {'field': 'model.container_spec.ports[0]', 'description': &quot;Invalid value at 'model.container_spec.ports[0]' (type.googleapis.com\/google.cloud.aiplatform.v1.Port), 5000&quot;}]}]}\n<\/code><\/pre>\n<p>When commenting out <code>serving_container_environment_variables<\/code> and <code>serving_container_ports<\/code>  the model resource gets created but deploying it manually to the endpoint results into a failed deployment with no output logs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1643879294320,
        "Question_score":0,
        "Question_tags":"python|google-cloud-ml|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":273,
        "Owner_creation_time":1512301540763,
        "Owner_last_access_time":1663934781440,
        "Owner_location":"Milan, Italy",
        "Owner_reputation":1427,
        "Owner_up_votes":93,
        "Owner_down_votes":16,
        "Owner_views":113,
        "Question_last_edit_time":1644239389000,
        "Answer_body":"<p>After some time researching the problem I've stumbled upon <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/6848\" rel=\"nofollow noreferrer\">this<\/a> Github issue. The problem was originated by a mismatch between <a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.2.2\/index.html\" rel=\"nofollow noreferrer\"><code>google_cloud_pipeline_components<\/code><\/a> and <a href=\"https:\/\/kubernetes.io\/docs\/reference\/generated\/kubernetes-api\/v1.19\/#envvar-v1-core\" rel=\"nofollow noreferrer\"><code>kubernetes_api<\/code><\/a> docs. In this case, <code>serving_container_environment_variables<\/code> is typed as an <code>Optional[dict[str, str]]<\/code> whereas it should have been typed as a <code>Optional[list[dict[str, str]]]<\/code>. A similar mismatch can be found for <code>serving_container_ports<\/code> argument as well. Passing arguments following kubernetes documentation did the trick:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_upload_op = gcc_aip.ModelUploadOp(\n    project=&quot;my-project&quot;,\n    location=&quot;us-west1&quot;,\n    display_name=&quot;session_model&quot;,\n    serving_container_image_uri=&quot;gcr.io\/my-project\/pred:latest&quot;,\n    serving_container_environment_variables=[\n        {&quot;name&quot;: &quot;MODEL_BUCKET&quot;, &quot;value&quot;: &quot;ml_session_model&quot;}\n    ],\n    serving_container_ports=[{&quot;containerPort&quot;: 5000}],\n    serving_container_predict_route=&quot;\/predict&quot;,\n    serving_container_health_route=&quot;\/health&quot;,\n)\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1643965835693,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70968460",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71101070,
        "Question_title":"How to properly extract endpoint id from gcp_resources of a Vertex AI pipeline on GCP?",
        "Question_body":"<p>I am using GCP Vertex AI pipeline (KFP) and using <code>google-cloud-aiplatform==1.10.0<\/code>, <code>kfp==1.8.11<\/code>, <code>google-cloud-pipeline-components==0.2.6<\/code>\nIn a component I am getting a gcp_resources <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/google-cloud\/google_cloud_pipeline_components\/proto\/README.md\" rel=\"nofollow noreferrer\">documentation<\/a> :<\/p>\n<pre><code>gcp_resources (str):\n            Serialized gcp_resources proto tracking the create endpoint's long running operation.\n<\/code><\/pre>\n<p>To extract the endpoint_id to do online prediction of my deployed model, I am doing:<\/p>\n<pre><code>from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\nfrom google.protobuf.json_format import Parse\ninput_gcp_resources = Parse(endpoint_ressource_name, GcpResources())\ngcp_resources=input_gcp_resources.resources.__getitem__(0).resource_uri.split('\/')\nendpoint_id=gcp_resources[gcp_resources.index('endpoints')+1]\n<\/code><\/pre>\n<p>Is there a better\/native way of extracting such info ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644758810230,
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":225,
        "Owner_creation_time":1465222092253,
        "Owner_last_access_time":1663858783617,
        "Owner_location":"Z\u00fcrich, Switzerland",
        "Owner_reputation":1414,
        "Owner_up_votes":258,
        "Owner_down_votes":3,
        "Owner_views":478,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In this case is the best way to extract the information. But, I recommend using the <a href=\"http:\/\/ttps:\/\/github.com\/aio-libs\/yarl\" rel=\"nofollow noreferrer\">yarl<\/a> library for complex uri to parse.<\/p>\n<p>You can see this example:<\/p>\n<pre><code>&gt;&gt;&gt; from yarl import URL\n&gt;&gt;&gt; url = URL('https:\/\/www.python.org\/~guido?arg=1#frag')\n&gt;&gt;&gt; url\nURL('https:\/\/www.python.org\/~guido?arg=1#frag')\n<\/code><\/pre>\n<p>All URL parts can be accessed by these properties.<\/p>\n<pre><code>&gt;&gt;&gt; url.scheme\n'https'\n&gt;&gt;&gt; url.host\n'www.python.org'\n&gt;&gt;&gt; url.path\n'\/~guido'\n&gt;&gt;&gt; url.query_string\n'arg=1'\n&gt;&gt;&gt; url.query\n&lt;MultiDictProxy('arg': '1')&gt;\n&gt;&gt;&gt; url.fragment\n'frag'\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1644873889257,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71101070",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71496966,
        "Question_title":"VertexAI Pipelines: The provided location ID doesn't match the endpoint",
        "Question_body":"<p>I have successfully run the following pipeline that creates a dataset, trains a model, and deploys to an endpoint using VertexAIs pipeline tool when everything is based in us-central1. Now, when I change the region to europe-west2, I get the following error:<\/p>\n<pre><code>debug_error_string = &quot;{&quot;created&quot;:&quot;@1647430410.324290053&quot;,&quot;description&quot;:&quot;Error received from peer\nipv4:172.217.169.74:443&quot;,&quot;file&quot;:&quot;src\/core\/lib\/surface\/call.cc&quot;,&quot;file_line&quot;:1066,\n&quot;grpc_message&quot;:&quot;List of found errors:\\t1.Field: name; Message: \nThe provided location ID doesn't match the endpoint. The valid location ID is `us-central1`.\\t&quot;,&quot;grpc_status&quot;:3}&quot;\n<\/code><\/pre>\n<p>This error occurs after the dataset is created in europe-west2, and before the model starts to train. Here is my code:<\/p>\n<pre><code>#import libraries\nfrom typing import NamedTuple\nimport kfp\nfrom kfp import dsl\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import (Artifact, Input, InputPath, Model, Output, \n                        OutputPath, ClassificationMetrics, Metrics, component)\nfrom kfp.v2.components.types.artifact_types import Dataset\nfrom kfp.v2.google.client import AIPlatformClient\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom google.api_core.exceptions import NotFound\n\n@kfp.dsl.pipeline(name=f&quot;lookalike-model-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = f&quot;bq:\/\/{PROJECT_ID}.{DATASET_ID}.{TABLE_NAME}&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;europe-west2&quot;,\n    api_endpoint: str = &quot;europe-west2-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auPrc&quot;: 0.5}',\n):\n            \n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project,\n        display_name=display_name, \n        bq_source=bq_source,\n        location = gcp_region\n    )\n\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        location=gcp_region,\n        predefined_split_column_name=&quot;set&quot;,\n        column_transformations=[\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;agentId&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;postcode&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;isMobile&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;gender&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;timeOfDay&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;set&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;sale&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;sale&quot;,\n    )\n\ncompiler.Compiler().compile(\n    pipeline_func=pipeline, package_path=&quot;tab_classif_pipeline.json&quot;\n)\n\nml_pipeline_job = aiplatform.PipelineJob(\n    display_name=f&quot;{MODEL_PREFIX}_training&quot;,\n    template_path=&quot;tab_classif_pipeline.json&quot;,\n    pipeline_root=PIPELINE_ROOT,\n    parameter_values={&quot;project&quot;: PROJECT_ID, &quot;display_name&quot;: DISPLAY_NAME},\n    enable_caching=True,\n    location=&quot;europe-west2&quot;\n)\nml_pipeline_job.submit()\n<\/code><\/pre>\n<p>As previously mentioned, the dataset gets created so I suspect that the issue must lie in <code>training_op = gcc_aip.AutoMLTabularTrainingJobRunOp<\/code><\/p>\n<p>I tried providing another endpoint: <code>eu-aiplatform.googleapis.com<\/code> which yielded the following error:<\/p>\n<pre><code>google.api_core.exceptions.InvalidArgument: 400 List of found errors: 1.Field: name; Message: \nThe provided location ID doesn't match the endpoint. The valid location ID is `us-central1`.\n\nFail to send metric: [rpc error: code = PermissionDenied desc = Permission monitoring.metricDescriptors.create \ndenied (or the resource may not exist).; rpc error: code = PermissionDenied desc = Permission monitoring.timeSeries.create\n denied (or the resource may not exist).]\n<\/code><\/pre>\n<p>I understand that I am not passing api-endpoint to any of the methods above, but I thought I'd highlight that the error changed slightly.<\/p>\n<p>Does anyone know what the issue may be? Or how I can run <code>gcc_aip.AutoMLTabularTrainingJobRunOp<\/code> in europe-west2 (or EU in general)?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1647433181040,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-automl|kubeflow|google-cloud-vertex-ai",
        "Question_view_count":220,
        "Owner_creation_time":1562706291280,
        "Owner_last_access_time":1664033431527,
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Try Updating the pipeline component using the command:<\/p>\n<p><code>pip3 install --force-reinstall google_cloud_pipeline_components==0.1.3<\/code><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1649155832383,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71496966",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72809603,
        "Question_title":"VertexAI Tabular AutoML rejecting rows containing nulls",
        "Question_body":"<p>I am trying to build a binary classifier based on a tabular dataset that is rather sparse, but training is failing with the following message:<\/p>\n<blockquote>\n<p>Training pipeline failed with error message: Too few input rows passed validation. Of 1169548 inputs, 194 were valid. At least 50% of rows must pass validation.<\/p>\n<\/blockquote>\n<p>My understanding was that tabular AutoML should be able to handle Null values, so I'm not sure what's happening here, and I would appreciate any suggestions. The <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/tabular-data\/tabular101\" rel=\"nofollow noreferrer\">documentation<\/a> explicitly mentions reviewing each column's nullability, but I don't see any way to set or check a column's nullability on the dataset tab (perhaps the documentation is out of date?). Additionally, the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/data-types-tabular#what_values_are_treated_as_null_values\" rel=\"nofollow noreferrer\">documentation<\/a> explicitly mentions that missing values are treated as null, which is how I've set up my CSV. The <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/data-types-tabular#numeric\" rel=\"nofollow noreferrer\">documentation for numeric<\/a> however does not explicitly list support for missing values, just NaN and inf.<\/p>\n<p>The dataset is 1 million rows, 34 columns, and only 189 rows are null-free. My most sparse column has data in 5,000 unique rows, with the next rarest having data in 72k and 274k rows respectively. Columns are a mix of categorical and numeric, with only a handful of columns without nulls.<\/p>\n<p>The data is stored as a CSV, and the Dataset import seems to run without issue. Generate statistics ran on the dataset, but for some reason the missing % column failed to populate. What might be the best way to address this? I'm not sure if this is a case where I need to change my null representation in the CSV, change some dataset\/training setting, or if its an AutoML bug (less likely). Thanks!<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bF1Nn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bF1Nn.png\" alt=\"Image of missing % column being blank\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1656554846450,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":122,
        "Owner_creation_time":1614739500313,
        "Owner_last_access_time":1664046580507,
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Question_last_edit_time":1656987570560,
        "Answer_body":"<p>To allow invalid &amp; null values during training &amp; prediction, we have to explicitly set the <code>allow invalid values<\/code> flag to <code>Yes<\/code> during training as shown in the image below. You can find this setting under model training settings on the dataset page. The flag has to be set on a column by column basis.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jWDGm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jWDGm.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1657366819400,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72809603",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72094768,
        "Question_title":"How to get the status of a pipeline run within a component, running on Vertex AI?",
        "Question_body":"<p>Previously, using Kubeflow Pipelines SDK v1, the status of a pipeline could be inferred during pipeline execution by passing an Argo placeholder, <code>{{workflow.status}}<\/code>, to the component, as shown below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import kfp.dsl as dsl\n\ncomponent_1 = dsl.ContainerOp(\n    name='An example component',\n    image='eu.gcr.io\/...\/my-component-img',\n    arguments=[\n               'python3', 'main.py',\n               '--status', &quot;{{workflow.status}}&quot;\n              ]\n)\n<\/code><\/pre>\n<p>This placeholder would take the value <code>Succeeded<\/code> or <code>Failed<\/code> when passed to the component. One use-case for this would be to send a failure-warning to eg. Slack, in combination with <code>dsl.ExitHandler<\/code>.<\/p>\n<p>However, when using Pipeline SDK version 2, <code>kfp.v2<\/code>, together with Vertex AI to compile and run the pipeline the Argo placeholders no longer work, as described by <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/7614\" rel=\"nofollow noreferrer\">this open issue<\/a>. Because of this, I would need another way to check the status of the pipeline within the component. I was thinking I could use the <code>kfp.Client<\/code> <a href=\"https:\/\/kubeflow-pipelines.readthedocs.io\/en\/latest\/source\/kfp.client.html\" rel=\"nofollow noreferrer\">class<\/a>, but I'm assuming this won't work using Vertex AI, since there is no &quot;host&quot; really. Also, there seems to be supported placeholders for to pass the run id (<code>dsl.PIPELINE_JOB_ID_PLACEHOLDER<\/code>) as a placeholder, as per <a href=\"https:\/\/stackoverflow.com\/questions\/68348026\/run-id-in-kubeflow-pipelines-on-vertex-ai\">this SO post<\/a>, but I can't find anything around <code>status<\/code>.<\/p>\n<p>Any ideas how to get the status of a pipeline run within a component, running on Vertex AI?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1651551486133,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai|kubeflow-pipelines|argoproj|kfp",
        "Question_view_count":520,
        "Owner_creation_time":1562750927333,
        "Owner_last_access_time":1663926437127,
        "Owner_location":"Stockholm, Sverige",
        "Owner_reputation":803,
        "Owner_up_votes":123,
        "Owner_down_votes":7,
        "Owner_views":73,
        "Question_last_edit_time":1654867910903,
        "Answer_body":"<p>Each pipeline run is automatically logged to Google Logging, and so are also the failed pipeline runs.\nThe error logs also contain information about the pipeline and the component that failed.<\/p>\n<p>We can use this information to monitor our logs and set up an alert via email for example.<\/p>\n<p>The logs for our Vertex AI Pipeline runs we get with the following filter<\/p>\n<p>resource.type=\u201daiplatform.googleapis.com\/PipelineJob\u201d\nseverity=(ERROR OR CRITICAL OR ALERT OR EMERGENCY)<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/e4jFR.png\" rel=\"nofollow noreferrer\">Vertex AI Pipeline Logs<\/a><\/p>\n<p>Based on those logs you can set up log-based alerts <a href=\"https:\/\/cloud.google.com\/logging\/docs\/alerting\/log-based-alerts\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/logging\/docs\/alerting\/log-based-alerts<\/a>. Notifications via email, Slack, SMS, and many more are possible.<\/p>\n<p>source:\n<a href=\"https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1655447541937,
        "Answer_score":1,
        "Question_favorite_count":0.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72094768",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68331232,
        "Question_title":"Vertex AI batch predictions from file-list",
        "Question_body":"<p>I want to submit batch prediction job for a custom model (in my case it is torch model, but I think this is irrelevant in this case). So I read the documentation:\n<a href=\"https:\/\/i.stack.imgur.com\/WGa7T.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WGa7T.png\" alt=\"batch prediction from file-list\" \/><\/a><\/p>\n<p>But as there are no examples I cannot be sure what the schema of the json object which vertex ai will send to my model will be. Does someone have made this work ?<\/p>\n<p>My best guess is that the request will be with the following body:<\/p>\n<pre><code>{'instance' : &lt;b64-encoded-content-of-the-file&gt;}\n<\/code><\/pre>\n<p>But when I read the documentation (for other 'features' of vertex ai) I could imagine the following body as well:<\/p>\n<pre><code>{'instance': {'b64' : &lt;b64-encoded-content-of-the-file&gt;}}\n<\/code><\/pre>\n<p>Does somebody actually know ?<\/p>\n<p>Another thing I did is to make a 'fake-model' which returns the request it gets ... when I submit the batch-prediction job it actually finishes successfully but when I check the output file it is empty ... so ... I actually need help\/more time to think of other ways to decipher vertex ai docs.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625948141620,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":611,
        "Owner_creation_time":1455786472727,
        "Owner_last_access_time":1663964690547,
        "Owner_location":"Varna, Bulgaria",
        "Owner_reputation":405,
        "Owner_up_votes":36,
        "Owner_down_votes":1,
        "Owner_views":100,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Vertex AI <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#request_requirements\" rel=\"nofollow noreferrer\">custom container<\/a> should wrap a service with an endpoint (predict) for receiving a list of instances, each is a json serializable object<\/p>\n<pre><code>{'instances': [{'b64' : &lt;b64-encoded-content-of-the-file1&gt;}, {'b64' : &lt;b64-encoded-content-of-the-file1&gt;}, ...]}\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1626781802423,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68331232",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70932012,
        "Question_title":"Could google cloud platform vertextAI running code on backend without output?",
        "Question_body":"<p>The problem is my local internet connection is unstable, I could run the code through Jupyter to the interactive google cloud platform vertexAI, while it seems that there're always outputs returns back to the Jupyter interface. So when my local internet connection is interrupted, the code running is also interrupted.<\/p>\n<p>Is there any methods that I could let the codes just run on the vertexAI backends? Then outputs could be written in the log file at last.<\/p>\n<p>This could be a very basic question. Thanks.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qQJbo.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qQJbo.jpg\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_time":1643658605453,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|jupyter|google-cloud-vertex-ai",
        "Question_view_count":514,
        "Owner_creation_time":1622195346030,
        "Owner_last_access_time":1648494047687,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Question_last_edit_time":1645284643350,
        "Answer_body":"<p>To be able to run your notebook on the background, I did the following steps:<\/p>\n<ol>\n<li>Open Jupyter notebook in GCP &gt; Vertex AI &gt; Workbench &gt; Open Jupyterlab<\/li>\n<li>Open a terminal<\/li>\n<li>Use the command below.\n<pre><code>nohup jupyter nbconvert --to notebook --execute test.ipynb &amp;\n<\/code><\/pre>\n<ul>\n<li><code>nohup<\/code> and <code>&amp;<\/code> is added so that the command will run on the background<\/li>\n<li>Output logs for the actual command will be appened to file <strong>nohup.out<\/strong><\/li>\n<li>Use <code>jupyter nbconvert --to notebook --execute test.ipynb<\/code> to execute the notebook specified after <code>--execute<\/code>. <code>--to notebook<\/code> will create a new notebook that contains the executed notebook with its logs.<\/li>\n<li>There other formats other than notebook to convert it. You can read thru more in <a href=\"https:\/\/nbconvert.readthedocs.io\/en\/latest\/usage.html\" rel=\"nofollow noreferrer\">nbconvert documentation<\/a>.<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<p>For testing I made notebook <strong>(test.ipynb)<\/strong> that has a loop that runs for 1.5 hours, that should emulate a long process.<\/p>\n<pre><code>import time\n\nfor x in range(1,1080):\n    print(x)\n    time.sleep(5)\n<\/code><\/pre>\n<p>I ran the command provided above and closed my notebook and anything related to GCP. After 1.5 hours I opened the notebook and terminal says its done.<\/p>\n<p><strong>Terminal upon checking back after 1.5 hours:<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/aVMcb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aVMcb.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Content of <strong>nohup.out<\/strong>:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kcbKi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kcbKi.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It created a new notebook named <strong>&quot;test.nbconvert.ipynb&quot;<\/strong> that contains the code from test.ipynb and its output.<\/p>\n<p>Snippet of test.nbconvert.ipynb as seen below. It completed the loop up to 1080 iterations that took 1.5 hours:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fnPh4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fnPh4.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1643782095707,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70932012",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73690729,
        "Question_title":"How do I get deployed model from `ListEndpointsRequest`?",
        "Question_body":"<p>My code is:<\/p>\n<pre><code>   client = aiplatform_v1.EndpointServiceClient(client_options=options)\n    parent = client.common_location_path(project=project_id, location=location)\n    \n\n    # Initialize request argument(s)\n    request = aiplatform_v1.ListEndpointsRequest(\n        parent=parent\n    )\n\n    # Make the request\n    endpoints_pager = client.list_endpoints(request=request)\n    for endpoint in endpoints_pager.pages:\n        latest_endpoint=endpoint\n        print(endpoint.deployed_models.id)\n<\/code><\/pre>\n<p>If I print <code>endpoint<\/code> I see <code>{...deployed_models { id: ...}}<\/code>\n<code>endpoint.deployed_models.id)<\/code> doesn't work so how do I get the deployed model id?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1662992136247,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":44,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As @DazWilkin mentioned in the comments, you need to iterate through <code>deployed_models<\/code> to get the <code>id<\/code> per model. Applying this, your code should look like this:<\/p>\n<p>IMPORTANT NOTE FOR FUTURE READERS: When creating <code>client<\/code> it is needed to define the <code>api_endpoint<\/code> in <code>client_options<\/code>. If not not defined you will encounter a <strong>google.api_core.exceptions.MethodNotImplemented: 501 Received http2 header with status: 404<\/strong> error.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from google.cloud import aiplatform_v1\n\ndef sample_list_endpoints():\n\n    project_id = &quot;your-project-id&quot;\n    location = &quot;us-central1&quot;\n\n    client = aiplatform_v1.EndpointServiceClient(client_options={&quot;api_endpoint&quot;:&quot;us-central1-aiplatform.googleapis.com&quot;})\n\n    parent = client.common_location_path(project=project_id, location=location)\n    # Initialize request argument(s)\n    request = aiplatform_v1.ListEndpointsRequest(\n        parent=parent,\n    )\n\n    # Make the request\n    page_result = client.list_endpoints(request=request)\n\n    # Handle the response\n    for response in page_result:\n        for model in response.deployed_models:\n            print(model.id)\n\nsample_list_endpoints()\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1663048009750,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73690729",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72860901,
        "Question_title":"How can I change the security setting and enable terminal for a Vertex AI managed notebook?",
        "Question_body":"<p>I created a notebook using Vertex AI without enabling terminal first, but I want to enable terminal now so that I can run a Python file from a terminal. Is there any way I can change the setting retrospectively?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656960242433,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|gcp-ai-platform-notebook",
        "Question_view_count":110,
        "Owner_creation_time":1656565016230,
        "Owner_last_access_time":1663528057860,
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1657082227956,
        "Answer_body":"<p>As of now, when you create a Notebook instance with unchecked <em>&quot;Enable terminal&quot;<\/em> like the below screenshot, you <strong>cannot re-enable this option once the Notebook instance is already created<\/strong>.\n<a href=\"https:\/\/i.stack.imgur.com\/QM6xp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QM6xp.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The only workaround is to <strong>recreate the Notebook instance<\/strong> and then enable it.<\/p>\n<p>Right now, there is already a <a href=\"https:\/\/issuetracker.google.com\/222694899\" rel=\"nofollow noreferrer\">Feature Request<\/a> for this. You can <strong>star<\/strong> the public issue tracker feature request and add <strong>\u2018Me too\u2019<\/strong> in the thread. This will bring more attention to the request as more users request support for it.<\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1657251232893,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72860901",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73112914,
        "Question_title":"Google Vertex AI fails AutoML training due to large BigQuery dataset being too large",
        "Question_body":"<p>I am currently training some models via Googles AutoML feature contained within their Vertex AI products.<\/p>\n<p>The normal pipeline is creating a dataset, which I do by creating a table in Bigquery, and then starting the training process.<\/p>\n<p>This has normally worked before but for my latest dataset I get the following error message:<\/p>\n<blockquote>\n<p>Training pipeline failed with error message: The size of source BigQuery table is larger than 107374182400 bytes.<\/p>\n<\/blockquote>\n<p>While it seemed unlikely to me that the table is actually too large for AutoML, I tried re-training on a new dataset that's a 50% sample of the original table but the same error occured.<\/p>\n<p>Is my dataset really to large for AutoML to handle or is there another issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658768743937,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":61,
        "Owner_creation_time":1455621115903,
        "Owner_last_access_time":1663664835447,
        "Owner_location":"Germany",
        "Owner_reputation":1179,
        "Owner_up_votes":24,
        "Owner_down_votes":1,
        "Owner_views":92,
        "Question_last_edit_time":null,
        "Answer_body":"<p>There are some perspectives of limits for AutoML Tables -- not only size in bytes (100GB as maximum supported size), but also number of rows (~200bi lines) and number of columns (up to 1000 columns).<\/p>\n<p>You can find more details on <a href=\"https:\/\/cloud.google.com\/automl-tables\/docs\/quotas#limits\" rel=\"nofollow noreferrer\">AutoML Tables limits<\/a> documentation.<\/p>\n<p>Is your source data within those limits?<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1658773194927,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73112914",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70577610,
        "Question_title":"Why is my GCP Vertex pipeline api_endpoint not right?",
        "Question_body":"<p>My <code>API_ENDPOINT<\/code> is set to <code>europe-west1-aiplatform.googleapis.com<\/code>.<\/p>\n<p>I define a pipeline:<\/p>\n<pre><code>def pipeline(project: str = PROJECT_ID, region: str = REGION, api_endpoint: str = API_ENDPOINT):\n<\/code><\/pre>\n<p>when I run it:<\/p>\n<pre><code>job = aip.PipelineJob(\ndisplay_name=DISPLAY_NAME,\ntemplate_path=&quot;image classification_pipeline.json&quot;.replace(&quot; &quot;, &quot;_&quot;),)\njob.run()\n<\/code><\/pre>\n<p>it is always created in USandA:<\/p>\n<pre><code>INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. \nResource name: projects\/my_proj_id\/locations\/us-central1\/pipelineJobs\/automl-image-training-v2-anumber\n<\/code><\/pre>\n<p>How do I get it into Europe?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1641293452093,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":92,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The <code>location<\/code> parameter in the <code>aip.PipelineJob()<\/code> class can be used to specify in which region the pipeline will be deployed. Refer to this <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/aiplatform#class-googlecloudaiplatformpipelinejobdisplayname-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-templatepath-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-jobid-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-pipelineroot-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-parametervalues-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-any--none-enablecaching-optionalboolhttpspythonreadthedocsioenlatestlibraryfunctionshtmlbool--none-encryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-labels-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-credentials-optionalgoogleauthcredentialscredentialshttpsgoogleapisdevpythongoogle-authlatestreferencegoogleauthcredentialshtmlgoogleauthcredentialscredentials--none-project-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-location-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none\" rel=\"nofollow noreferrer\">documentation<\/a> for more information about the <code>PipelineJob()<\/code> method.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>REGION = &quot;europe-west1&quot;\n\njob = aip.PipelineJob(\n          display_name=DISPLAY_NAME,\n          template_path=&quot;image classification_pipeline.json&quot;.replace(&quot; &quot;, &quot;_&quot;),\n          location=REGION)\n\njob.run()\n<\/code><\/pre>\n<p>The above code will deploy a pipeline in the <code>europe-west1<\/code> region. The code returns the following output. The job is now deployed in the specified region.<\/p>\n<pre><code>INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\nINFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects\/&lt;project-id&gt;\/locations\/europe-west1\/pipelineJobs\/hello-world-pipeline\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1641305260580,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1647760091087,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70577610",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73733464,
        "Question_title":"What should be used for deployed_model in DeployModelRequest in Vertex AI pipeline?",
        "Question_body":"<p>i am trying to deploy a model in Vertex AI pipeline component using <code>DeployModelRequest<\/code>. I try to get the model using <code>GetModelRequest<\/code><\/p>\n<pre><code>    model_name = f'projects\/{project}\/locations\/{location}\/models\/{model_id}'\n    model_request = aiplatform_v1.types.GetModelRequest(name=model_name)\n    model_info = client_model.get_model(request=model_request)       \n    \n    deploy_request = aiplatform_v1.types.DeployModelRequest(endpoint=end_point, \n                                                                deployed_model=model_info)\n    client.deploy_model(request=deploy_request)\n<\/code><\/pre>\n<p>but this gives:<\/p>\n<pre><code>TypeError: Parameter to MergeFrom() must be instance of same class: expected \ngoogle.cloud.aiplatform.v1.DeployedModel got Model\n<\/code><\/pre>\n<p>I have also tried <code>deployed_model=model_info.deployed_models[0]<\/code> but this gave:<\/p>\n<pre><code>TypeError: Parameter to MergeFrom() must be instance of same class: expected\n google.cloud.aiplatform.v1.DeployedModel got DeployedModelRef.\n<\/code><\/pre>\n<p>So what do I use for <code>deployed_model<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1663254259473,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|kfp",
        "Question_view_count":39,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>As simple as:<\/p>\n<pre><code>machine_spec = MachineSpec(machine_type=&quot;n1-standard-2&quot;)\ndedicated_resources = DedicatedResources(machine_spec=machine_spec, \n                                         min_replica_count=1, \n                                         max_replica_count=1)\ndepmodel= DeployedModel(model=model_name, dedicated_resources=dedicated_resources) \n\ndeploy_request = aiplatform_v1.types.DeployModelRequest(\n                   endpoint=end_point, deployed_model=depmodel, \n                   traffic_split={'0':100})\nclient.deploy_model(request=deploy_request)\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1663344904267,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73733464",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73027674,
        "Question_title":"How does one move python and other types of files from one GCP notebook instance to another?",
        "Question_body":"<p>I have a Vertex AI notebook that contains a lot of python and jupyter notebook as well as pickled data files in it.  I need to move these files to another notebook.  There isn't a lot of documentation on google's help center.<\/p>\n<p>Has someone had to do this yet?  I'm new to GCP.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1658172774733,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":114,
        "Owner_creation_time":1455270976143,
        "Owner_last_access_time":1663869676733,
        "Owner_location":"Grand Rapids, MI, USA",
        "Owner_reputation":1269,
        "Owner_up_votes":134,
        "Owner_down_votes":0,
        "Owner_views":261,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Can you try these steps in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/migrate\" rel=\"nofollow noreferrer\">article<\/a>. It says you can copy your files to a <a href=\"https:\/\/cloud.google.com\/storage\/\" rel=\"nofollow noreferrer\">Google Cloud Storage Bucket<\/a> then move it to a new notebook by using gsutil tool.<\/p>\n<p>In your notebook's terminal run this code to copy an object to your Google Cloud storage bucket:<\/p>\n<pre><code>gsutil cp -R \/home\/jupyter\/* gs:\/\/BUCKET_NAMEPATH\n<\/code><\/pre>\n<p>Then open a new terminal to the target notebook and run this command to copy the directory to the notebook:<\/p>\n<pre><code>gsutil cp gs:\/\/BUCKET_NAMEPATH* \/home\/jupyter\/\n<\/code><\/pre>\n<p>Just change the <code>BUCKET_NAMEPATH<\/code> to the name of your cloud storage bucket.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1658194851973,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73027674",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68675606,
        "Question_title":"Autoscaling VertexAI pipeline components",
        "Question_body":"<p>I am exploring VertexAI pipelines and understand that it is a managed alternative to, say, AI Platform pipelines (where you have to deploy a GKE cluster to be able to run Kubeflow pipelines). What I am not clear on is whether VertexAI will autoscale the cluster depending on the load. In the answer to a <a href=\"https:\/\/stackoverflow.com\/questions\/68343475\/how-to-scale-out-kubeflow-pipelines-using-vertex-ai-or-it-just-done-automatic\">similar question<\/a>, it is mentioned that for pipeline steps that use GCP resources such as Dataflow etc., autoscaling will be done automatically. In the google docs, it is mentioned that for components, one can <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/build-pipeline#specify-machine-type\" rel=\"nofollow noreferrer\">set resources<\/a>, such as CPU_LIMIT GPU_LIMIT etc. My question is, can these limits be set for any type of component, i.e., Google Cloud pipeline components or Custom components, whether Python function-based or those packaged as a container image? Secondly, do these limits mean that the components resources will autoscale till they hit those limits? And what happens if these options are not even specified, how are the resources allocated then, will they autoscale as VertexAI sees fit?<\/p>\n<p>Links to relevant docs and resources would be really helpful.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1628219043833,
        "Question_score":3,
        "Question_tags":"google-ai-platform|kubeflow-pipelines|google-cloud-ai-platform-pipelines|google-cloud-vertex-ai",
        "Question_view_count":524,
        "Owner_creation_time":1471292986790,
        "Owner_last_access_time":1664058170413,
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Question_last_edit_time":null,
        "Answer_body":"<p>To answer your questions,<\/p>\n<p><strong>1. Can these limits be set for any type of components?<\/strong><\/p>\n<blockquote>\n<p>Yes. Because, these limits are applicable to all Kubeflow components and are not specific to any particular type of component.\nThese components could be implemented to perform tasks with a set amount of resources.<\/p>\n<\/blockquote>\n<br \/>\n<p><strong>2. Do these limits mean that the component resources will autoscale till they hit the limits?<\/strong><\/p>\n<blockquote>\n<p>No, there is no autoscaling performed by Vertex AI. Based on the limits set, Vertex AI chooses one suitable VM to perform the task.\nHaving a pool of workers is supported in Google Cloud Pipeline Components such as \u201c<a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.1.4\/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.CustomContainerTrainingJobRunOp\" rel=\"nofollow noreferrer\">CustomContainerTrainingJobRunOp<\/a>\u201d and \u201c<a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.1.4\/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.CustomPythonPackageTrainingJobRunOp\" rel=\"nofollow noreferrer\">CustomPythonPackageTrainingJobRunOp<\/a>\u201d as part of Distributed Training in Vertex AI. Otherwise, only 1 machine is used per step.<\/p>\n<\/blockquote>\n<br \/>\n<p><strong>3. What happens if these limits are not specified? Does Vertex AI scale the resources as it sees fit?<\/strong><\/p>\n<blockquote>\n<p>If the limits are not specified, an \u201c<a href=\"https:\/\/cloud.google.com\/compute\/docs\/general-purpose-machines#e2_machine_types\" rel=\"nofollow noreferrer\">e2-standard-4<\/a>\u201d VM is used for task execution as the default option.<\/p>\n<\/blockquote>\n<br \/>\n<p><strong>EDIT:<\/strong> I have updated the links with the latest version of the documentation.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1628594911847,
        "Answer_score":3,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1632070029070,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68675606",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69203143,
        "Question_title":"Using Tesla A100 GPU with Kubeflow Pipelines on Vertex AI",
        "Question_body":"<p>I'm using the following lines of code to specify the desired machine type and accelerator\/GPU on a Kubeflow Pipeline (KFP) that I will be running on a serverless manner through Vertex AI\/Pipelines.<\/p>\n<pre><code>op().\nset_cpu_limit(8).\nset_memory_limit(50G).\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-k80').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>and it works for other GPUs as well i.e. Pascal, Tesla, Volta cards.<\/p>\n<p>However, I can't do the same with the latest accelerator type which is the <code>Tesla A100<\/code> as it requires a special machine type, which is as least an <code>a2-highgpu-1g<\/code>.<\/p>\n<p>How do I make sure that this particular component will run on top of <code>a2-highgpu-1g<\/code> when I run it on Vertex?<\/p>\n<p>If i simply follow the method for older GPUs:<\/p>\n<pre><code>op().\nset_cpu_limit(12). # max for A2-highgpu-1g\nset_memory_limit(85G). # max for A2-highgpu-1g\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>It throws an error when run\/deployed since the machine type that is being spawned is the general type i.e. N1-Highmem-*<\/p>\n<p>Same thing happened when I did not specify the cpu and memory limits, in hope that it will automatically select the right machnie type based on the accelerator constraint.<\/p>\n<pre><code>    op().\n    add_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\n    set_gpu_limit(1)\n<\/code><\/pre>\n<p>Error:\n<code>&quot;NVIDIA_TESLA_A100&quot; is not supported for machine type &quot;n1-highmem-2&quot;,<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1631772554497,
        "Question_score":1,
        "Question_tags":"google-kubernetes-engine|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":473,
        "Owner_creation_time":1449207605093,
        "Owner_last_access_time":1663813482530,
        "Owner_location":"Manila, NCR, Philippines",
        "Owner_reputation":185,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Currently, GCP don't support A2 Machine type for normal KF Components. A potential workaround right now is to use <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-job\" rel=\"nofollow noreferrer\"><strong>GCP custom job component<\/strong><\/a> that you can explicitly specify the machine type.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1632103988633,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69203143",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68764644,
        "Question_title":"Training Google-Cloud-Automl Model on multiple datasets",
        "Question_body":"<p>I would like to train an automl model on gcp's vertex ai using multiple datasets.  I would like to keep the datasets separate, since they come from different sources, want to train on them separately, etc.  Is that possible?  Or will I need to create a dataset containing both datasets? It looks like I can only select one dataset in the web UI.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1628805774297,
        "Question_score":0,
        "Question_tags":"google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":223,
        "Owner_creation_time":1540925643620,
        "Owner_last_access_time":1663952436977,
        "Owner_location":"Utah, USA",
        "Owner_reputation":1212,
        "Owner_up_votes":699,
        "Owner_down_votes":4,
        "Owner_views":110,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It is possible via the Vertex AI API as long as your sources are in Google Cloud Storage, just provide a list of training data which are in JSON or CSV format that qualifies with the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-image\" rel=\"nofollow noreferrer\">best practices for formatting of training data<\/a>.<\/p>\n<p>See code for creating and importing datasets. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/create-dataset-api#create-dataset\" rel=\"nofollow noreferrer\">documentation<\/a> for code reference and further details.<\/p>\n<pre><code>from typing import List, Union\nfrom google.cloud import aiplatform\n\n    def create_and_import_dataset_image_sample(\n        project: str,\n        location: str,\n        display_name: str,\n        src_uris: Union[str, List[str]], \/\/ example: [&quot;gs:\/\/bucket\/file1.csv&quot;, &quot;gs:\/\/bucket\/file2.csv&quot;]\n        sync: bool = True,\n    ):\n        aiplatform.init(project=project, location=location)\n    \n        ds = aiplatform.ImageDataset.create(\n            display_name=display_name,\n            gcs_source=src_uris,\n            import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n            sync=sync,\n        )\n    \n        ds.wait()\n    \n        print(ds.display_name)\n        print(ds.resource_name)\n        return ds\n<\/code><\/pre>\n<p>NOTE: The links provided are for Vertex AI AutoML Image. If you access the links there are options for other AutoML products like Text, Tabular and Video.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1628828205167,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68764644",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71351821,
        "Question_title":"Reading File from Vertex AI and Google Cloud Storage",
        "Question_body":"<p>I am trying to set up a pipeline in GCP\/Vertex AI and am having a lot of trouble. The pipeline is being written using Kubeflow Pipelines and has many different components, one thing in particular is giving me trouble however. Eventually I want to launch this from a Cloud Function with the help of the Cloud Scheduler.<\/p>\n<p>The part that is giving me issues is fairly simple and I believe I just need some form of introduction to how I should be thinking about this setup. I simply want to read and write from files (might be .csv, .txt or similar). I imagine that the analog to the filesystem on my local machine in GCP is the Cloud Storage so this is where I have been trying to read from for the time being (please correct me if I'm wrong). The component I've built is a blatant rip-off of <a href=\"https:\/\/stackoverflow.com\/questions\/48279061\/gcs-read-a-text-file-from-google-cloud-storage-directly-into-python\">this<\/a> post and looks like this.<\/p>\n<pre><code>@component(\n    packages_to_install=[&quot;google-cloud&quot;],\n    base_image=&quot;python:3.9&quot;\n)\n\n\ndef main(\n):\n    import csv\n    from io import StringIO\n\n    from google.cloud import storage\n\n    BUCKET_NAME = &quot;gs:\/\/my_bucket&quot;\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(BUCKET_NAME)\n\n    blob = bucket.blob('test\/test.txt')\n    blob = blob.download_as_string()\n    blob = blob.decode('utf-8')\n\n    blob = StringIO(blob)  #tranform bytes to string here\n\n    names = csv.reader(blob)  #then use csv library to read the content\n    for name in names:\n        print(f&quot;First Name: {name[0]}&quot;)\n<\/code><\/pre>\n<p>The error I'm getting looks like the following:<\/p>\n<pre><code>google.api_core.exceptions.NotFound: 404 GET https:\/\/storage.googleapis.com\/storage\/v1\/b\/gs:\/\/pipeline_dev?projection=noAcl&amp;prettyPrint=false: Not Found\n<\/code><\/pre>\n<p>What's going wrong in my brain? I get the feeling that it shouldn't be this difficult to read and write files. I must be missing something fundamental? Any help is highly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1646398826330,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|google-cloud-storage|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":1298,
        "Owner_creation_time":1407245826703,
        "Owner_last_access_time":1663849819650,
        "Owner_location":null,
        "Owner_reputation":745,
        "Owner_up_votes":210,
        "Owner_down_votes":6,
        "Owner_views":168,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Try specifying bucket name w\/o a gs:\/\/. This should fix the issue. One more stackoverflow post that says the same thing: <a href=\"https:\/\/stackoverflow.com\/questions\/53436615\/cloud-storage-python-client-fails-to-retrieve-bucket\">Cloud Storage python client fails to retrieve bucket<\/a><\/p>\n<p>any storage bucket you try to access in GCP has a unique address to access it. That address starts with a gs:\/\/ always which specifies that it is a cloud storage url. Now, GCS apis are designed such that they need the bucket name only to work with it. Hence, you just pass the bucket name. If you were accessing the bucket via browser you will need the complete address to access and hence the gs:\/\/ prefix as well.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1646582537360,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71351821",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69219230,
        "Question_title":"In GCP Vertex AI, why is Delete Training Pipeline REST endpoint unimplemented?",
        "Question_body":"<p>I used <a href=\"https:\/\/github.com\/googleapis\/java-aiplatform\/blob\/master\/google-cloud-aiplatform\/src\/main\/java\/com\/google\/cloud\/aiplatform\/v1\/PipelineServiceClient.java\" rel=\"nofollow noreferrer\">this code<\/a>, straight from the Javadocs, to delete a VertexAI Training Pipeline<\/p>\n<pre><code>try (PipelineServiceClient pipelineServiceClient = PipelineServiceClient.create()) {\n  TrainingPipelineName name =\n      TrainingPipelineName.of(&quot;[PROJECT]&quot;, &quot;[LOCATION]&quot;, &quot;[TRAINING_PIPELINE]&quot;);\n  pipelineServiceClient.deleteTrainingPipelineAsync(name).get();\n}\n<\/code><\/pre>\n<p>I get this error. From what I can see, this means that this API, though officially documented, is simply unimplemented. How do we delete Training Pipelines using Java?<\/p>\n<pre><code>Error in deleting \/\/aiplatform.googleapis.com\/projects\/746859988231\/locations\/us-central1\/trainingPipelines\/186468439399187392: \njava.util.concurrent.ExecutionException: \ncom.google.api.gax.rpc.UnimplementedException: io.grpc.StatusRuntimeException:\nUNIMPLEMENTED: HTTP status code 404\n...\n&lt;!DOCTYPE html&gt;\n&lt;html lang=en&gt;\n....\n  &lt;p&gt;The requested URL &lt;code&gt;\/google.cloud.aiplatform.v1.PipelineService\n\/DeleteTrainingPipeline&lt;\/code&gt; was not found on this server.  \n&lt;ins&gt;That\u2019s all we know.&lt;\/ins&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1631862868123,
        "Question_score":2,
        "Question_tags":"java|google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":259,
        "Owner_creation_time":1227171471293,
        "Owner_last_access_time":1664047108080,
        "Owner_location":"Israel",
        "Owner_reputation":17500,
        "Owner_up_votes":463,
        "Owner_down_votes":87,
        "Owner_views":1561,
        "Question_last_edit_time":1631987478696,
        "Answer_body":"<p>According to the official documentation, <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest<\/a> , the supported service URLs for this service are:<\/p>\n<pre><code>https:\/\/us-central1-aiplatform.googleapis.com\nhttps:\/\/us-east1-aiplatform.googleapis.com\nhttps:\/\/us-east4-aiplatform.googleapis.com\nhttps:\/\/us-west1-aiplatform.googleapis.com\nhttps:\/\/northamerica-northeast1-aiplatform.googleapis.com\nhttps:\/\/europe-west1-aiplatform.googleapis.com\nhttps:\/\/europe-west2-aiplatform.googleapis.com\nhttps:\/\/europe-west4-aiplatform.googleapis.com\nhttps:\/\/asia-east1-aiplatform.googleapis.com\nhttps:\/\/asia-northeast1-aiplatform.googleapis.com\nhttps:\/\/asia-northeast3-aiplatform.googleapis.com\nhttps:\/\/asia-southeast1-aiplatform.googleapis.com\nhttps:\/\/australia-southeast1-aiplatform.googleapis.com\n<\/code><\/pre>\n<hr \/>\n<pre><code>  PipelineServiceSettings pipelineServiceSettings =\n        PipelineServiceSettings.newBuilder()\n            .setEndpoint(&quot;us-central1-aiplatform.googleapis.com:443&quot;)\n            .build();\n<\/code><\/pre>\n<hr \/>\n<pre><code> try (PipelineServiceClient pipelineServiceClient =\n      PipelineServiceClient.create(pipelineServiceSettings)) {\n\n  String location = &quot;us-central1&quot;;\n  TrainingPipelineName trainingPipelineName =\n      TrainingPipelineName.of(project, location, trainingPipelineId);\n\n  OperationFuture&lt;Empty, DeleteOperationMetadata&gt; operationFuture =\n      pipelineServiceClient.deleteTrainingPipelineAsync(trainingPipelineName);\n\n  System.out.format(&quot;Operation name: %s\\n&quot;, operationFuture.getInitialFuture().get().getName());\n  System.out.println(&quot;Waiting for operation to finish...&quot;);\n  operationFuture.get(300, TimeUnit.SECONDS);\n\n  System.out.format(&quot;Deleted Training Pipeline.&quot;);\n}\n<\/code><\/pre>",
        "Answer_comment_count":3,
        "Answer_creation_time":1631875853753,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1631888060710,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69219230",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73434003,
        "Question_title":"Read images from a bucket in GCP for ML",
        "Question_body":"<p>I have created a bucket in GCP containing my images dataset.<\/p>\n<p>The path to it is: xray-competition-bucket\/img_align_celeba<\/p>\n<p>How do I read it from GCP to Jupyter Lab in Vertex AI?<\/p>\n<p>My code is:<\/p>\n<pre><code>MAIN_PATH = '\/gcs\/xray-competition-bucket\/img_align_celeba'\n\nimage_paths = glob((MAIN_PATH + &quot;\/*.jpg&quot;))\n<\/code><\/pre>\n<p>and the result is that image_paths is an empty array.<\/p>\n<p>Note: I also tried the path gs:\/\/my_bucket\/...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1661081410723,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|jupyter-lab|bucket|google-cloud-vertex-ai",
        "Question_view_count":79,
        "Owner_creation_time":1639312590127,
        "Owner_last_access_time":1663617607140,
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You will need to <a href=\"https:\/\/cloud.google.com\/storage\/docs\/downloading-objects#storage-download-object-python\" rel=\"nofollow noreferrer\">download the GCS file locally<\/a> using <code>gsutil<\/code> or the python SDK if you want to use glob. There are also libraries like <a href=\"https:\/\/gcsfs.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">GCSFS<\/a> or TensorFlow's <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/gfile\/GFile\" rel=\"nofollow noreferrer\">GFile<\/a> which offer a pythonic file-system interface for working with GCS. For example, here is <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/gfile\/glob\" rel=\"nofollow noreferrer\">GFile.glob<\/a>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1661266806083,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73434003",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71962260,
        "Question_title":"Reading Data in Vertex AI Pipelines",
        "Question_body":"<p>This is my first time using Google's Vertex AI Pipelines. I checked <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro?hl=en#0\" rel=\"nofollow noreferrer\">this codelab<\/a> as well as <a href=\"https:\/\/towardsdatascience.com\/how-to-set-up-custom-vertex-ai-pipelines-step-by-step-467487f81cad\" rel=\"nofollow noreferrer\">this post<\/a> and <a href=\"https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153\" rel=\"nofollow noreferrer\">this post<\/a>, on top of some links derived from the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/introduction?hl=es-419\" rel=\"nofollow noreferrer\">official documentation<\/a>. I decided to put all that knowledge to work, in some toy example: I was planning to build a pipeline consisting of 2 components: &quot;get-data&quot; (which reads some .csv file stored in Cloud Storage) and &quot;report-data&quot; (which basically returns the shape of the .csv data read in the previous component). Furthermore, I was cautious to include <a href=\"https:\/\/stackoverflow.com\/questions\/71351821\/reading-file-from-vertex-ai-and-google-cloud-storage\">some suggestions<\/a> provided in this forum. The code I currently have, goes as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import pipeline, component, Dataset, Input, Output\nfrom google.cloud import aiplatform\n\n# Components section   \n\n@component(\n    packages_to_install=[\n        &quot;google-cloud-storage&quot;,\n        &quot;pandas&quot;,\n    ],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;get_data.yaml&quot;\n)\ndef get_data(\n    bucket: str,\n    url: str,\n    dataset: Output[Dataset],\n):\n    import pandas as pd\n    from google.cloud import storage\n    \n    storage_client = storage.Client(&quot;my-project&quot;)\n    bucket = storage_client.get_bucket(bucket)\n    blob = bucket.blob(url)\n    blob.download_to_filename('localdf.csv')\n    \n    # path = &quot;gs:\/\/my-bucket\/program_grouping_data.zip&quot;\n    df = pd.read_csv('localdf.csv', compression='zip')\n    df['new_skills'] = df['new_skills'].apply(ast.literal_eval)\n    df.to_csv(dataset.path + &quot;.csv&quot; , index=False, encoding='utf-8-sig')\n\n\n@component(\n    packages_to_install=[&quot;pandas&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;report_data.yaml&quot;\n)\ndef report_data(\n    inputd: Input[Dataset],\n):\n    import pandas as pd\n    df = pd.read_csv(inputd.path)\n    return df.shape\n\n\n# Pipeline section\n\n@pipeline(\n    # Default pipeline root. You can override it when submitting the pipeline.\n    pipeline_root=PIPELINE_ROOT,\n    # A name for the pipeline.\n    name=&quot;my-pipeline&quot;,\n)\ndef my_pipeline(\n    url: str = &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;,\n    bucket: str = &quot;my-bucket&quot;\n):\n    dataset_task = get_data(bucket, url)\n\n    dimensions = report_data(\n        dataset_task.output\n    )\n\n# Compilation section\n\ncompiler.Compiler().compile(\n    pipeline_func=my_pipeline, package_path=&quot;pipeline_job.json&quot;\n)\n\n# Running and submitting job\n\nfrom datetime import datetime\n\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n\nrun1 = aiplatform.PipelineJob(\n    display_name=&quot;my-pipeline&quot;,\n    template_path=&quot;pipeline_job.json&quot;,\n    job_id=&quot;mlmd-pipeline-small-{0}&quot;.format(TIMESTAMP),\n    parameter_values={&quot;url&quot;: &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;, &quot;bucket&quot;: &quot;my-bucket&quot;},\n    enable_caching=True,\n)\n\nrun1.submit()\n<\/code><\/pre>\n<p>I was happy to see that the pipeline compiled with no errors, and managed to submit the job. However &quot;my happiness lasted short&quot;, as when I went to Vertex AI Pipelines, I stumbled upon some &quot;error&quot;, which goes like:<\/p>\n<blockquote>\n<p>The DAG failed because some tasks failed. The failed tasks are: [get-data].; Job (project_id = my-project, job_id = 4290278978419163136) is failed due to the above error.; Failed to handle the job: {project_number = xxxxxxxx, job_id = 4290278978419163136}<\/p>\n<\/blockquote>\n<p>I did not find any related info on the web, neither could I find any log or something similar, and I feel a bit overwhelmed that the solution to this (seemingly) easy example, is still eluding me.<\/p>\n<p>Quite obviously, I don't what or where I am mistaking. Any suggestion?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":5,
        "Question_creation_time":1650588056410,
        "Question_score":4,
        "Question_tags":"python|google-cloud-platform|google-cloud-storage|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":892,
        "Owner_creation_time":1629385138957,
        "Owner_last_access_time":1663953209400,
        "Owner_location":null,
        "Owner_reputation":395,
        "Owner_up_votes":18,
        "Owner_down_votes":4,
        "Owner_views":38,
        "Question_last_edit_time":1650845389252,
        "Answer_body":"<p>With some suggestions provided in the comments, I think I managed to make my demo pipeline work. I will first include the updated code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from kfp.v2 import compiler\nfrom kfp.v2.dsl import pipeline, component, Dataset, Input, Output\nfrom datetime import datetime\nfrom google.cloud import aiplatform\nfrom typing import NamedTuple\n\n\n# Importing 'COMPONENTS' of the 'PIPELINE'\n\n@component(\n    packages_to_install=[\n        &quot;google-cloud-storage&quot;,\n        &quot;pandas&quot;,\n    ],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;get_data.yaml&quot;\n)\ndef get_data(\n    bucket: str,\n    url: str,\n    dataset: Output[Dataset],\n):\n    &quot;&quot;&quot;Reads a csv file, from some location in Cloud Storage&quot;&quot;&quot;\n    import ast\n    import pandas as pd\n    from google.cloud import storage\n    \n    # 'Pulling' demo .csv data from a know location in GCS\n    storage_client = storage.Client(&quot;my-project&quot;)\n    bucket = storage_client.get_bucket(bucket)\n    blob = bucket.blob(url)\n    blob.download_to_filename('localdf.csv')\n    \n    # Reading the pulled demo .csv data\n    df = pd.read_csv('localdf.csv', compression='zip')\n    df['new_skills'] = df['new_skills'].apply(ast.literal_eval)\n    df.to_csv(dataset.path + &quot;.csv&quot; , index=False, encoding='utf-8-sig')\n\n\n@component(\n    packages_to_install=[&quot;pandas&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;report_data.yaml&quot;\n)\ndef report_data(\n    inputd: Input[Dataset],\n) -&gt; NamedTuple(&quot;output&quot;, [(&quot;rows&quot;, int), (&quot;columns&quot;, int)]):\n    &quot;&quot;&quot;From a passed csv file existing in Cloud Storage, returns its dimensions&quot;&quot;&quot;\n    import pandas as pd\n    \n    df = pd.read_csv(inputd.path+&quot;.csv&quot;)\n    \n    return df.shape\n\n\n# Building the 'PIPELINE'\n\n@pipeline(\n    # i.e. in my case: PIPELINE_ROOT = 'gs:\/\/my-bucket\/test_vertex\/pipeline_root\/'\n    # Can be overriden when submitting the pipeline\n    pipeline_root=PIPELINE_ROOT,\n    name=&quot;readcsv-pipeline&quot;,  # Your own naming for the pipeline.\n)\ndef my_pipeline(\n    url: str = &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;,\n    bucket: str = &quot;my-bucket&quot;\n):\n    dataset_task = get_data(bucket, url)\n\n    dimensions = report_data(\n        dataset_task.output\n    )\n    \n\n# Compiling the 'PIPELINE'    \n\ncompiler.Compiler().compile(\n    pipeline_func=my_pipeline, package_path=&quot;pipeline_job.json&quot;\n)\n\n\n# Running the 'PIPELINE'\n\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n\nrun1 = aiplatform.PipelineJob(\n    display_name=&quot;my-pipeline&quot;,\n    template_path=&quot;pipeline_job.json&quot;,\n    job_id=&quot;mlmd-pipeline-small-{0}&quot;.format(TIMESTAMP),\n    parameter_values={\n        &quot;url&quot;: &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;,\n        &quot;bucket&quot;: &quot;my-bucket&quot;\n    },\n    enable_caching=True,\n)\n\n# Submitting the 'PIPELINE'\n\nrun1.submit()\n<\/code><\/pre>\n<p>Now, I will add some complementary comments, which in sum, managed to solve my problem:<\/p>\n<ul>\n<li>First, having the &quot;Logs Viewer&quot; (roles\/logging.viewer) enabled for your user, will greatly help to troubleshoot any existing error in your pipeline (Note: that role worked for me, however you might want to look for a better matching role for you own purposes <a href=\"https:\/\/cloud.google.com\/logging\/docs\/access-control?&amp;_ga=2.212939101.-833851896.1650314090&amp;_gac=1.182768084.1650632490.Cj0KCQjwpImTBhCmARIsAKr58cw9X0TGy2YeN-CFsM4RvEXDH_vL54Ce1ECrWh4_aJNoPEhgtXusUhwaAlIUEALw_wcB#permissions_and_roles\" rel=\"nofollow noreferrer\">here<\/a>). Those errors will appear as &quot;Logs&quot;, which can be accessed by clicking the corresponding button:<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jpgfU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jpgfU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ul>\n<li>NOTE: In the picture above, when the &quot;Logs&quot; are displayed, it might be helpful to carefully check each log (close to the time when you created you pipeline), as generally each eof them corresponds with a single warning or error line:<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/CpTkZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CpTkZ.png\" alt=\"Verte AI Pipelines Logs\" \/><\/a><\/p>\n<ul>\n<li>Second, the output of my pipeline was a tuple. In my original approach, I just returned the plain tuple, but it is advised to return a <a href=\"https:\/\/docs.python.org\/3\/library\/typing.html#typing.NamedTuple\" rel=\"nofollow noreferrer\">NamedTuple<\/a> instead. In general, if you need to input \/ output one or more &quot;<em>small values<\/em>&quot; (int or str, for any reason), pick a NamedTuple to do so.<\/li>\n<li>Third, when the connection between your pipelines is <code>Input[Dataset]<\/code> or <code>Ouput[Dataset]<\/code>, adding the file extension is needed (and quite easy to forget). Take for instance the ouput of the <code>get_data<\/code> component, and notice how the data is recorded by specifically adding the file extension, i.e. <code>dataset.path + &quot;.csv&quot;<\/code>.<\/li>\n<\/ul>\n<p>Of course, this is a very tiny example, and projects can easily scale to huge projects, however as some sort of &quot;Hello Vertex AI Pipelines&quot; it will work well.<\/p>\n<p>Thank you.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1650657469577,
        "Answer_score":4,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1650870474732,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71962260",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70356856,
        "Question_title":"Vertex AI Model Batch prediction, issue with referencing existing model and input file on Cloud Storage",
        "Question_body":"<p>I'm struggling to correctly set Vertex AI pipeline which does the following:<\/p>\n<ol>\n<li>read data from API and store to GCS and as as input for batch prediction.<\/li>\n<li>get an existing model (Video classification on Vertex AI)<\/li>\n<li>create Batch prediction job with input from point 1.<br \/>\nAs it will be seen, I don't have much experience with Vertex Pipelines\/Kubeflow thus I'm asking for help\/advice, hope it's just some beginner mistake.\nthis is the gist of the code I'm using as pipeline<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom kfp.v2 import dsl\n\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Output,\n    Artifact,\n    Model,\n)\n\nPROJECT_ID = 'my-gcp-project'\nBUCKET_NAME = &quot;mybucket&quot;\nPIPELINE_ROOT = &quot;{}\/pipeline_root&quot;.format(BUCKET_NAME)\n\n\n@component\ndef get_input_data() -&gt; str:\n    # getting data from API, save to Cloud Storage\n    # return GS URI\n    gcs_batch_input_path = 'gs:\/\/somebucket\/file'\n    return gcs_batch_input_path\n\n\n@component(\n    base_image=&quot;python:3.9&quot;,\n    packages_to_install=['google-cloud-aiplatform==1.8.0']\n)\ndef load_ml_model(project_id: str, model: Output[Artifact]):\n    &quot;&quot;&quot;Load existing Vertex model&quot;&quot;&quot;\n    import google.cloud.aiplatform as aip\n\n    model_id = '1234'\n    model = aip.Model(model_name=model_id, project=project_id, location='us-central1')\n\n\n\n@dsl.pipeline(\n    name=&quot;batch-pipeline&quot;, pipeline_root=PIPELINE_ROOT,\n)\ndef pipeline(gcp_project: str):\n    input_data = get_input_data()\n    ml_model = load_ml_model(gcp_project)\n\n    gcc_aip.ModelBatchPredictOp(\n        project=PROJECT_ID,\n        job_display_name=f'test-prediction',\n        model=ml_model.output,\n        gcs_source_uris=[input_data.output],  # this doesn't work\n        # gcs_source_uris=['gs:\/\/mybucket\/output\/'],  # hardcoded gs uri works\n        gcs_destination_output_uri_prefix=f'gs:\/\/{PIPELINE_ROOT}\/prediction_output\/'\n    )\n\n\nif __name__ == '__main__':\n    from kfp.v2 import compiler\n    import google.cloud.aiplatform as aip\n    pipeline_export_filepath = 'test-pipeline.json'\n    compiler.Compiler().compile(pipeline_func=pipeline,\n                                package_path=pipeline_export_filepath)\n    # pipeline_params = {\n    #     'gcp_project': PROJECT_ID,\n    # }\n    # job = aip.PipelineJob(\n    #     display_name='test-pipeline',\n    #     template_path=pipeline_export_filepath,\n    #     pipeline_root=f'gs:\/\/{PIPELINE_ROOT}',\n    #     project=PROJECT_ID,\n    #     parameter_values=pipeline_params,\n    # )\n\n    # job.run()\n<\/code><\/pre>\n<p>When running the pipeline it throws this exception when running Batch prediction:<br \/>\n<code>details = &quot;List of found errors: 1.Field: batch_prediction_job.model; Message: Invalid Model resource name. <\/code>\nso I'm not sure what could be wrong. I tried to load model in the notebook (outside of component) and it correctly returns.<\/p>\n<p>Second issue I'm having is referencing GCS URI as output from component to batch job input.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>   input_data = get_input_data2()\n   gcc_aip.ModelBatchPredictOp(\n        project=PROJECT_ID,\n        job_display_name=f'test-prediction',\n        model=ml_model.output,\n        gcs_source_uris=[input_data.output],  # this doesn't work\n        # gcs_source_uris=['gs:\/\/mybucket\/output\/'],  # hardcoded gs uri works\n        gcs_destination_output_uri_prefix=f'gs:\/\/{PIPELINE_ROOT}\/prediction_output\/'\n    )\n<\/code><\/pre>\n<p>During compilation, I get following exception <code>TypeError: Object of type PipelineParam is not JSON serializable<\/code>, though I think this could be issue of ModelBatchPredictOp component.<\/p>\n<p>Again any help\/advice appreciated, I'm dealing with this from yesterday, so maybe I missed something obvious.<\/p>\n<p>libraries I'm using:<\/p>\n<pre><code>google-cloud-aiplatform==1.8.0  \ngoogle-cloud-pipeline-components==0.2.0  \nkfp==1.8.10  \nkfp-pipeline-spec==0.1.13  \nkfp-server-api==1.7.1\n<\/code><\/pre>\n<p><strong>UPDATE<\/strong>\nAfter comments, some research and tuning, for referencing model this works:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component\ndef load_ml_model(project_id: str, model: Output[Artifact]):\n    region = 'us-central1'\n    model_id = '1234'\n    model_uid = f'projects\/{project_id}\/locations\/{region}\/models\/{model_id}'\n    model.uri = model_uid\n    model.metadata['resourceName'] = model_uid\n<\/code><\/pre>\n<p>and then I can use it as intended:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>batch_predict_op = gcc_aip.ModelBatchPredictOp(\n        project=gcp_project,\n        job_display_name=f'batch-prediction-test',\n        model=ml_model.outputs['model'],\n        gcs_source_uris=[input_batch_gcs_path],\ngcs_destination_output_uri_prefix=f'gs:\/\/{BUCKET_NAME}\/prediction_output\/test'\n    )\n<\/code><\/pre>\n<p><strong>UPDATE 2<\/strong>\nregarding GCS path, a workaround is to define path outside of the component and pass it as an input parameter, for example (abbreviated):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@dsl.pipeline(\n    name=&quot;my-pipeline&quot;,\n    pipeline_root=PIPELINE_ROOT,\n)\ndef pipeline(\n        gcp_project: str,\n        region: str,\n        bucket: str\n):\n    ts = datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)\n    \n    gcs_prediction_input_path = f'gs:\/\/{BUCKET_NAME}\/prediction_input\/video_batch_prediction_input_{ts}.jsonl'\n    batch_input_data_op = get_input_data(gcs_prediction_input_path)  # this loads input data to GCS path\n\n    batch_predict_op = gcc_aip.ModelBatchPredictOp(\n        project=gcp_project,\n        model=training_job_run_op.outputs[&quot;model&quot;],\n        job_display_name='batch-prediction',\n        # gcs_source_uris=[batch_input_data_op.output],\n        gcs_source_uris=[gcs_prediction_input_path],\n        gcs_destination_output_uri_prefix=f'gs:\/\/{BUCKET_NAME}\/prediction_output\/',\n    ).after(batch_input_data_op)  # we need to add 'after' so it runs after input data is prepared since get_input_data doesn't returns anything\n\n<\/code><\/pre>\n<p>still not sure, why it doesn't work\/compile when I return GCS path from <code>get_input_data<\/code> component<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1639525350357,
        "Question_score":5,
        "Question_tags":"kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":1202,
        "Owner_creation_time":1372196724860,
        "Owner_last_access_time":1664040926440,
        "Owner_location":"Prague, Czech Republic",
        "Owner_reputation":276,
        "Owner_up_votes":46,
        "Owner_down_votes":0,
        "Owner_views":47,
        "Question_last_edit_time":1640079991423,
        "Answer_body":"<p>I'm glad you solved most of your main issues and found a workaround for model declaration.<\/p>\n<p>For your <code>input.output<\/code> observation on <code>gcs_source_uris<\/code>, the reason behind it is because the way the function\/class returns the value. If you dig inside the class\/methods of <code>google_cloud_pipeline_components<\/code>  you will find that it implements a structure that will allow you to use <code>.outputs<\/code> from the returned value of the function called.<\/p>\n<p>If you go to the implementation of one of the components of the pipeline you will find that it returns an output array from <code>convert_method_to_component<\/code> function. So, in order to have that implemented in your custom class\/function your function should return a value which can be called as an attribute. Below is a basic implementation of it.<\/p>\n<pre><code>class CustomClass():\n     def __init__(self):\n       self.return_val = {'path':'custompath','desc':'a desc'}\n      \n     @property\n     def output(self):\n       return self.return_val \n\nhello = CustomClass()\nprint(hello.output['path'])\n<\/code><\/pre>\n<p>If you want to dig more about it you can go to the following pages:<\/p>\n<ul>\n<li><p><a href=\"https:\/\/github.com\/bharathdsce\/kubeflow\/blob\/fcd627714664956b2c280b0109b64633bc99fa05\/components\/google-cloud\/google_cloud_pipeline_components\/aiplatform\/utils.py#L383\" rel=\"nofollow noreferrer\">convert_method_to_component<\/a>, which is the implementation of <code>convert_method_to_component<\/code><\/p>\n<\/li>\n<li><p><a href=\"https:\/\/www.programiz.com\/python-programming\/property\" rel=\"nofollow noreferrer\">Properties<\/a>, basics of property in python.<\/p>\n<\/li>\n<\/ul>",
        "Answer_comment_count":0,
        "Answer_creation_time":1640097300177,
        "Answer_score":0,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70356856",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69721067,
        "Question_title":"GoogleAPICallError: None Unexpected state: Long-running operation had neither response nor error set",
        "Question_body":"<p>I'm new to Google Cloud Platform and I'm trying to create a Feature Store to fill with values from a csv file from Google Cloud Storage. The aim is to do that from a local notebook in Python.\nI'm basically following the code <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/official\/feature_store\/gapic-feature-store.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, making the appropriate changes since I'm working with the credit card public dataset.\nThe error that raises when I run the code is the following:<\/p>\n<pre><code>GoogleAPICallError: None Unexpected state: Long-running operation had neither response nor error set.\n<\/code><\/pre>\n<p>and it happens during the ingestion of the data from the csv file.<\/p>\n<p>Here it is the code I'm working on:<\/p>\n<pre><code>import os\nfrom datetime import datetime\nfrom google.cloud import bigquery\nfrom google.cloud import aiplatform\nfrom google.cloud.aiplatform_v1.types import feature as feature_pb2\nfrom google.cloud.aiplatform_v1.types import featurestore as featurestore_pb2\nfrom google.cloud.aiplatform_v1.types import \\\n    featurestore_service as featurestore_service_pb2\nfrom google.cloud.aiplatform_v1.types import entity_type as entity_type_pb2\nfrom google.cloud.aiplatform_v1.types import FeatureSelector, IdMatcher\n\ncredential_path = r&quot;C:\\Users\\...\\.json&quot;\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n\n## Constants\nPROJECT_ID = &quot;my-project-ID&quot;\nREGION = &quot;us-central1&quot;\nAPI_ENDPOINT = &quot;us-central1-aiplatform.googleapis.com&quot;\nINPUT_CSV_FILE = &quot;my-input-file.csv&quot;\nFEATURESTORE_ID = &quot;fraud_detection&quot;\n\n## Output dataset\nDESTINATION_DATA_SET = &quot;fraud_predictions&quot;\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\nDESTINATION_DATA_SET = &quot;{prefix}_{timestamp}&quot;.format(\n    prefix=DESTINATION_DATA_SET, timestamp=TIMESTAMP\n)\n\n## Output table. Make sure that the table does NOT already exist; \n## the BatchReadFeatureValues API cannot overwrite an existing table\nDESTINATION_TABLE_NAME = &quot;training_data&quot;\n\nDESTINATION_PATTERN = &quot;bq:\/\/{project}.{dataset}.{table}&quot;\nDESTINATION_TABLE_URI = DESTINATION_PATTERN.format(\n    project=PROJECT_ID, dataset=DESTINATION_DATA_SET, \n    table=DESTINATION_TABLE_NAME\n)\n\n## Create dataset\nclient = bigquery.Client(project=PROJECT_ID)\ndataset_id = &quot;{}.{}&quot;.format(client.project, DESTINATION_DATA_SET)\ndataset = bigquery.Dataset(dataset_id)\ndataset.location = REGION\ndataset = client.create_dataset(dataset)\nprint(&quot;Created dataset {}.{}&quot;.format(client.project, dataset.dataset_id))\n\n## Create client for CRUD and data_client for reading feature values.\nclient = aiplatform.gapic.FeaturestoreServiceClient(\n    client_options={&quot;api_endpoint&quot;: API_ENDPOINT})\ndata_client = aiplatform.gapic.FeaturestoreOnlineServingServiceClient(\n    client_options={&quot;api_endpoint&quot;: API_ENDPOINT})\nBASE_RESOURCE_PATH = client.common_location_path(PROJECT_ID, REGION)\n\n## Create featurestore (only the first time)\ncreate_lro = client.create_featurestore(\n    featurestore_service_pb2.CreateFeaturestoreRequest(\n        parent=BASE_RESOURCE_PATH,\n        featurestore_id=FEATURESTORE_ID,\n        featurestore=featurestore_pb2.Featurestore(\n            online_serving_config=featurestore_pb2.Featurestore.OnlineServingConfig(\n                fixed_node_count=1\n            ),\n        ),\n    )\n)\n\n## Wait for LRO to finish and get the LRO result.\nprint(create_lro.result())\n\nclient.get_featurestore(\n    name=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID)\n)\n\n## Create credit card entity type (only the first time)\ncc_entity_type_lro = client.create_entity_type(\n    featurestore_service_pb2.CreateEntityTypeRequest(\n        parent=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n        entity_type_id=&quot;creditcards&quot;,\n        entity_type=entity_type_pb2.EntityType(\n            description=&quot;Credit card entity&quot;,\n        ),\n    )\n)\n\n## Create fraud entity type (only the first time)\nfraud_entity_type_lro = client.create_entity_type(\n    featurestore_service_pb2.CreateEntityTypeRequest(\n        parent=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n        entity_type_id=&quot;frauds&quot;,\n        entity_type=entity_type_pb2.EntityType(\n            description=&quot;Fraud entity&quot;,\n        ),\n    )\n)\n\n## Create features for credit card type (only the first time)\nclient.batch_create_features(\n    parent=client.entity_type_path(PROJECT_ID, REGION, FEATURESTORE_ID, &quot;creditcards&quot;),\n    requests=[\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v1&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v2&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v3&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v4&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v5&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v6&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v7&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v8&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v9&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v10&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v11&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v12&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v13&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v14&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v15&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v16&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v17&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v18&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v19&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v20&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v21&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v22&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v23&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v24&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v25&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v26&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v27&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v28&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;amount&quot;,\n        ),\n    ],\n).result()\n\n## Create features for fraud type (only the first time)\nclient.batch_create_features(\n    parent=client.entity_type_path(PROJECT_ID, REGION, FEATURESTORE_ID, &quot;frauds&quot;),\n    requests=[\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;class&quot;,\n        ),\n    ],\n).result()\n\n## Import features values for credit cards\nimport_cc_request = aiplatform.gapic.ImportFeatureValuesRequest(\n    entity_type=client.entity_type_path(\n        PROJECT_ID, REGION, FEATURESTORE_ID, &quot;creditcards&quot;),\n    csv_source=aiplatform.gapic.CsvSource(gcs_source=aiplatform.gapic.GcsSource(\n        uris=[&quot;gs:\/\/fraud-detection-19102021\/dataset\/cc_details_train.csv&quot;])),\n    entity_id_field=&quot;cc_id&quot;,\n    feature_specs=[\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v1&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v2&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v3&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v4&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v5&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v6&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v7&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v8&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v9&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v10&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v11&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v12&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v13&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v14&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v15&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v16&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v17&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v18&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v19&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v20&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v21&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v22&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v23&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v24&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v25&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v26&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v27&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v28&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;amount&quot;),\n    ],\n    feature_time_field='time',\n    worker_count=1,\n)\n\n## Start to import\ningestion_lro = client.import_feature_values(import_cc_request)\n\n## Polls for the LRO status and prints when the LRO has completed\ningestion_lro.result()\n\n## Import features values for frauds\nimport_fraud_request = aiplatform.gapic.ImportFeatureValuesRequest(\n    entity_type=client.entity_type_path(\n        PROJECT_ID, REGION, FEATURESTORE_ID, &quot;frauds&quot;),\n    csv_source=aiplatform.gapic.CsvSource(gcs_source=aiplatform.gapic.GcsSource(\n        uris=[&quot;gs:\/\/fraud-detection-19102021\/dataset\/data_fraud_train.csv&quot;])),\n    entity_id_field=&quot;fraud_id&quot;,\n    feature_specs=[\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;class&quot;),\n    ],\n    feature_time_field='time',\n    worker_count=1,\n)\n\n## Start to import\ningestion_lro = client.import_feature_values(import_fraud_request)\n\n## Polls for the LRO status and prints when the LRO has completed\ningestion_lro.result()\n<\/code><\/pre>\n<p>When I check the <code>Ingestion Jobs<\/code> from the <code>Feature<\/code> section of Google Cloud Console I see that the job has finished but no values are added to my features.<\/p>\n<p>Any advice it is really precious.<\/p>\n<p>Thank you all.<\/p>\n<p><strong>EDIT 1<\/strong>\nIn the image below there is an example of the first row of the csv file I used as input (<code>cc_details_train.csv<\/code>). All the unseen features  are similar, the feature <code>class<\/code> can assume 0 or 1 values.\nThe injection job lasts about 5 minutes to import (ideally) 3000 rows, but it ends without error and without importing any value.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Z34hG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Z34hG.png\" alt=\"Rows of my csv file\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1635242363740,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-api-python-client|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":357,
        "Owner_creation_time":1616589293617,
        "Owner_last_access_time":1663860372103,
        "Owner_location":"Alatri, Frosinone, FR",
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Question_last_edit_time":1636446864247,
        "Answer_body":"<p><strong>VERTEX AI recomendations when using CSV to ImportValues \/ using ImportFeatureValuesRequest<\/strong><\/p>\n<p>Its possible that when using this feature you might end not able to import any data at all. You must pay attention to the time field you are using as it must be in compliance with google time formats.<\/p>\n<ol>\n<li>feature_time_field, must follow the time constraint rule set by google which is RFC3339, ie: '2021-04-15T08:28:14Z'. You can check details about the field <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.featurestores.entityTypes\/importFeatureValues#request-body\" rel=\"nofollow noreferrer\">here<\/a> and details about timestamp format can be found <a href=\"https:\/\/developers.google.com\/protocol-buffers\/docs\/reference\/google.protobuf#timestamp\" rel=\"nofollow noreferrer\">here<\/a>.<\/li>\n<li>Other columns, fields must match is designed value. One exception is field entity_id_field, As it can be any value.<\/li>\n<\/ol>\n<p>Note: I my test i found that if i do not properly set up the time field as google recommended date format it will just not upload any feature value at all.<\/p>\n<p><em>test.csv<\/em><\/p>\n<pre><code>cc_id,time,v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15,v16,v17,v18,v19,v20,v21,v22,v23,v24,v25,v26,v27,v28,amount\n100,2021-04-15T08:28:14Z,-1.359807,-0.072781,2.534897,1.872351,2.596267,0.465238,0.923123,0.347986,0.987354,1.234657,2.128645,1.958237,0.876123,-1.712984,-0.876436,1.74699,-1.645877,-0.936121,1.456327,0.087623,1.900872,2.876234,1.874123,0.923451,0.123432,0.000012,1.212121,0.010203,1000\n<\/code><\/pre>\n<p><em>output:<\/em><\/p>\n<pre><code>imported_entity_count: 1\nimported_feature_value_count: 29\n<\/code><\/pre>\n<p><strong>About optimization and working with features<\/strong><\/p>\n<p>You can check the official documentation <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-text#single-label-classification\" rel=\"nofollow noreferrer\">here<\/a> to see the min and max amount of records recommended for processing. As a piece of advice you should only use the actual working features to run and the recommended amount of values for it.<\/p>\n<p><strong>See your running ingested job<\/strong><\/p>\n<p>Either if you use VertexUI or code to generated the ingested job. You can track its run by going into the UI to this path:<\/p>\n<pre><code>VertexAI &gt; Features &gt; View Ingested Jobs \n<\/code><\/pre>",
        "Answer_comment_count":3,
        "Answer_creation_time":1636364178673,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1638471768683,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69721067",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71505415,
        "Question_title":"\"The timestamp column must have valid timestamp entries.\" error when using `timestamp_split_column_name` arg in `AutoMLTabularTrainingJob.run`",
        "Question_body":"<p>From <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform.html#google.cloud.aiplatform.AutoMLTabularTrainingJob.run\" rel=\"nofollow noreferrer\">the docs<\/a> it says that<\/p>\n<blockquote>\n<p>The value of the key values of the key (the values in the column) must be in RFC 3339 date-time format, where time-offset = \u201cZ\u201d (e.g. 1985-04-12T23:20:50.52Z)<\/p>\n<\/blockquote>\n<p>The dataset that I'm pointing to is a CSV in cloud storage, where the data is in the format suggested by the docs:<\/p>\n<pre><code>$ gsutil cat gs:\/\/my-data.csv | head | xsv select TS_SPLIT_COL\nTS_SPLIT_COL\n2021-01-18T00:00:00.00Z\n2021-01-18T00:00:00.00Z\n2021-01-04T00:00:00.00Z\n2021-03-06T00:00:00.00Z\n2021-01-15T00:00:00.00Z\n2021-02-11T00:00:00.00Z\n2021-02-05T00:00:00.00Z\n2021-05-20T00:00:00.00Z\n2021-01-05T00:00:00.00Z\n<\/code><\/pre>\n<p>But I receive a <code>Training pipeline failed with error message: The timestamp column must have valid timestamp entries.<\/code> error when I try to run a training job<\/p>\n<p>EDIT: this should hopefully make it more reproducible<\/p>\n<p>data: <a href=\"https:\/\/pastebin.com\/qEDqvzX6\" rel=\"nofollow noreferrer\">https:\/\/pastebin.com\/qEDqvzX6<\/a><\/p>\n<p>Code I'm running:<\/p>\n<pre><code>from google.cloud import aiplatform\n\nPROJECT = &quot;my-project&quot;\nDATASET_ID = &quot;dataset-id&quot;  # points to CSV \n\naiplatform.init(project=PROJECT)\n\ndataset = aiplatform.TabularDataset(DATASET_ID)\n\njob = aiplatform.AutoMLTabularTrainingJob(\n    display_name=&quot;so-58454722&quot;,\n    optimization_prediction_type=&quot;classification&quot;,\n    optimization_objective=&quot;maximize-au-roc&quot;,\n)\n\nmodel = job.run(\n    dataset=dataset,\n    model_display_name=&quot;so-58454722&quot;,\n    target_column=&quot;Y&quot;,\n    training_fraction_split=0.8,\n    validation_fraction_split=0.1,\n    test_fraction_split=0.1,\n    timestamp_split_column_name=&quot;TS_SPLIT_COL&quot;,\n)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1647473614997,
        "Question_score":0,
        "Question_tags":"python|google-cloud-vertex-ai",
        "Question_view_count":157,
        "Owner_creation_time":1519933306780,
        "Owner_last_access_time":1664076383160,
        "Owner_location":null,
        "Owner_reputation":3576,
        "Owner_up_votes":42,
        "Owner_down_votes":7,
        "Owner_views":203,
        "Question_last_edit_time":1647534964220,
        "Answer_body":"<p>Try this timestamp format instead:<\/p>\n<p><code>2022-03-18T01:23:45.123456+00:00<\/code><\/p>\n<p>It uses <code>+00:00<\/code> instead of <code>Z<\/code> to specify timezone.<\/p>\n<p>This change will eliminate the &quot;The timestamp column must have valid timestamp entries.&quot; error<\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1647602028113,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71505415",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69878915,
        "Question_title":"Deploying multiple models to same endpoint in Vertex AI",
        "Question_body":"<p>Our use case is as follows:\nWe have multiple custom trained models (in the hundreds, and the number increases as we allow the user of our application to create models through the UI, which we then train and deploy on the fly) and so deploying each model to a separate endpoint is expensive as Vertex AI <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/deploy-model-console#custom-trained\" rel=\"nofollow noreferrer\">charges per node used<\/a>. Based on the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/deployment#models-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> it seems that we can deploy models of different types to the same endpoint but I am not sure how that would work. Let's say I have 2 different custom trained models deployed using custom containers for prediction to the same endpoint. Also, say I specify the traffic split to be 50% for the two models. Now, how do I send a request to a specific model? Using the python SDK, we make calls to the endpoint, like so:<\/p>\n<pre><code>from google.cloud import aiplatform\nendpoint = aiplatform.Endpoint(endpoint_id)\nprediction = endpoint.predict(instances=instances)\n\n# where endpoint_id is the id of the endpoint and instances are the observations for which a prediction is required\n<\/code><\/pre>\n<p>My understanding is that in this scenario, vertex AI will route some calls to one model and some to the other based on the traffic split. I could use the parameters field, as specified in the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#prediction\" rel=\"nofollow noreferrer\">docs<\/a>, to specify the model and then process the request accordingly in the custom prediction container, but still some calls will end up going to a model which it will not be able to process (because Vertex AI is not going to be sending all requests to all models, otherwise the traffic split wouldn't make sense). How do I then deploy multiple models to the same endpoint and make sure that every prediction request is guaranteed to be served?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_time":1636348555970,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":1271,
        "Owner_creation_time":1471292986790,
        "Owner_last_access_time":1664058170413,
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Question_last_edit_time":null,
        "Answer_body":"<p>This <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/deployment#models-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> talks about a use case where 2 models are trained on the same feature set and are sharing the ingress prediction traffic. As you have understood correctly, this does not apply to models that have been trained on different feature sets, that is, different models.<\/p>\n<p>Unfortunately, deploying different models to the same endpoint utilizing only one node is not possible in Vertex AI at the moment. There is an ongoing feature request that is being worked on. However, we cannot provide an exact ETA on when that feature will be available.<\/p>\n<p>I reproduced the multi-model setup and noticed the below points.<\/p>\n<p><strong>Traffic Splitting<\/strong><\/p>\n<blockquote>\n<p>I deployed 2 different models to the same endpoint and sent predictions to it. I set a 50-50 traffic splitting rule and saw errors that implied requests being sent to the wrong model.<\/p>\n<\/blockquote>\n<p><strong>Cost Optimization<\/strong><\/p>\n<blockquote>\n<p>When multiple models are deployed to the same endpoint, they are deployed to separate, independent nodes. So, you will still be charged for each node used. Also, node autoscaling happens at the model level, not at the endpoint level.<\/p>\n<\/blockquote>\n<p>A plausible workaround would be to pack all your models into a single container and use a custom HTTP server logic to send prediction requests to the appropriate model. This could be achieved using the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#request_requirements\" rel=\"nofollow noreferrer\"><code>parameters<\/code><\/a> field of the prediction request body. The custom logic would look something like this.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@app.post(os.environ['AIP_PREDICT_ROUTE'])\nasync def predict(request: Request):\n    body = await request.json()\n    parameters = body[&quot;parameters&quot;]\n    instances = body[&quot;instances&quot;]\n    inputs = np.asarray(instances)\n    preprocessed_inputs = _preprocessor.preprocess(inputs)\n\n    if(parameters[&quot;model_name&quot;]==&quot;random_forest&quot;):\n        print(parameters[&quot;model_name&quot;])\n        outputs = _random_forest_model.predict(preprocessed_inputs)\n    else:\n        print(parameters[&quot;model_name&quot;])\n        outputs = _decision_tree_model.predict(inputs)\n\n    return {&quot;predictions&quot;: [_class_names[class_num] for class_num in outputs]}\n<\/code><\/pre>",
        "Answer_comment_count":5,
        "Answer_creation_time":1636404890010,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1636405249843,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69878915",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69904211,
        "Question_title":"Vertex AI prediction - Autoscaling cannot set minimum node to 0",
        "Question_body":"<p>I am unclear abut Vertex AI pricing for model predictions. In the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/pricing\" rel=\"nofollow noreferrer\">documentation<\/a>, under the heading <strong>More about automatic scaling of prediction nodes<\/strong> one of the points mentioned is:<\/p>\n<blockquote>\n<p>&quot;If you choose automatic scaling, the number of nodes scales\nautomatically, and can scale down to zero for no-traffic durations&quot;<\/p>\n<\/blockquote>\n<p>The example provided in the documentation later also seems to suggest that during a period with no traffic, zero nodes are in use. However, when I create an Endpoint in Vertex AI, under the <strong>Autoscaling<\/strong> heading it says:<\/p>\n<blockquote>\n<p><em>&quot;Autoscaling: If you set a minimum and maximum, compute nodes will scale to meet traffic demand within those boundaries&quot;<\/em><\/p>\n<\/blockquote>\n<p>The  value of 0 under <em>&quot;Minimum number of compute nodes&quot;<\/em> is not allowed so you have to enter 1 or greater, and it is mentioned that:<\/p>\n<blockquote>\n<p>Default is 1. If set to 1 or more, then compute resources will\ncontinuously run even without traffic demand. This can increase cost\nbut avoid dropped requests due to node initialization.<\/p>\n<\/blockquote>\n<p>My question is, what happens when I select autoscaling by setting Minimum to 1 and Maximum to, say, 10. Does 1 node always run continuously? Or does it scale down to 0 nodes in no traffic condition as the documentation suggests.<\/p>\n<p>To test I deployed an Endpoint with Autoscaling (min and max set to 1) and then when I sent a prediction request the response was almost immediate, suggesting the node was already up. I did that again after about an hour and again the response was immediate suggesting that the node never shut down probably. Also, for high latency requirements, is having autoscale to 0 nodes, if that is indeed possible, even practical, i.e., what latency can we expect for starting up from 0 nodes?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1636487376953,
        "Question_score":4,
        "Question_tags":"google-cloud-platform|google-ai-platform|google-cloud-vertex-ai",
        "Question_view_count":894,
        "Owner_creation_time":1471292986790,
        "Owner_last_access_time":1664058170413,
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Question_last_edit_time":1636603578603,
        "Answer_body":"<p>Are you using an N1 or a non-N1 machine type? If you want to autoscale to zero, you must use non-N1 machines. See <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/overview#node_allocation_for_online_prediction\" rel=\"nofollow noreferrer\">second note<\/a> from node allocation:<\/p>\n<blockquote>\n<p>Note: Versions that use a Compute Engine (N1) machine type cannot scale down to zero nodes. They can scale down to 1 node, at minimum.<\/p>\n<\/blockquote>\n<p><em>Update<\/em>: AI Platform supports scaling to zero, while Vertex AI currently does not. From the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#scaling\" rel=\"nofollow noreferrer\">scaling<\/a> documentation, nodes can scale but there is no mention that it can scale down to zero. Here's a public <a href=\"https:\/\/issuetracker.google.com\/206042974\" rel=\"nofollow noreferrer\">feature request<\/a> for people who wants to track this issue.<\/p>\n<p>With regards to latency requirements, the actual output will vary. However, one thing to note according to the documentation is that the service may not be able to bring nodes online fast enough to keep up with large spikes of request traffic. If your traffic regularly has steep spikes, and if reliably low latency is important to your application, you may want to consider manual scaling.<\/p>\n<p>Additional Reference: <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/machine-types-online-prediction#automatic_scaling\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/machine-types-online-prediction#automatic_scaling<\/a><\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1636603298293,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1636679717047,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69904211",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72073763,
        "Question_title":"GCP Vertex Pipeline - Why kfp.v2.dsl.Output as function arguments work without being provided?",
        "Question_body":"<p>Why <code> kfp.v2.dsl.Output<\/code> as function argument works without being provided?<\/p>\n<p>I am following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/training-data-analyst\/blob\/master\/courses\/machine_learning\/deepdive2\/machine_learning_in_the_enterprise\/solutions\/create_run_vertex_pipeline.ipynb\" rel=\"nofollow noreferrer\">Create and run ML pipelines with Vertex Pipelines!<\/a> Jupyter notebook example from GCP.<\/p>\n<p>The function <code>classif_model_eval_metrics<\/code> takes <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> which have no default values.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],                # No default value set, hence must be mandatory\n    metricsc: Output[ClassificationMetrics], # No default value set, hence must be mandatory\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]): \n    # Full code at the bottom.\n<\/code><\/pre>\n<p>Hence those arguments should be mandatory, but the function is called without those arguments.<\/p>\n<pre><code>    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p>The entire pipeline code is below.<\/p>\n<pre><code>@kfp.dsl.pipeline(name=&quot;automl-tab-beans-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = &quot;bq:\/\/aju-dev-demos.beans.beans1&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auRoc&quot;: 0.95}',\n):\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=display_name, bq_source=bq_source\n    )\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        column_transformations=COLUMNS,\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;Class&quot;,\n    )\n    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Why does it work and what are <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> of type <code>kfp.v2.dsl.Output<\/code>?<\/p>\n<hr \/>\n<h2>classif_model_eval_metrics function code<\/h2>\n<pre><code>from kfp.v2.dsl import (\n    Dataset, Model, Output, Input, \n    OutputPath, ClassificationMetrics, Metrics, component\n)\n\n@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]):  # Return parameter.\n    &quot;&quot;&quot;Renders evaluation metrics for an AutoML Tabular classification model.\n    Retrieves the classification model evaluation and render the ROC and confusion matrix\n    for the model. Determine whether the model is sufficiently accurate to deploy.\n    &quot;&quot;&quot;\n    import json\n    import logging\n    from google.cloud import aiplatform\n\n    # Fetch model eval info\n    def get_eval_info(client, model_name):\n        from google.protobuf.json_format import MessageToDict\n        response = client.list_model_evaluations(parent=model_name)\n        metrics_list = []\n        metrics_string_list = []\n        for evaluation in response:\n            metrics = MessageToDict(evaluation._pb.metrics)\n            metrics_str = json.dumps(metrics)\n            metrics_list.append(metrics)\n            metrics_string_list.append(metrics_str)\n        return (\n            evaluation.name,\n            metrics_list,\n            metrics_string_list,\n        )\n\n    def classification_thresholds_check(metrics_dict, thresholds_dict):\n        for k, v in thresholds_dict.items():\n            if k in [&quot;auRoc&quot;, &quot;auPrc&quot;]:  # higher is better\n                if metrics_dict[k] &lt; v:  # if under threshold, don't deploy\n                    return False\n        return True\n\n    def log_metrics(metrics_list, metricsc):\n        test_confusion_matrix = metrics_list[0][&quot;confusionMatrix&quot;]\n        logging.info(&quot;rows: %s&quot;, test_confusion_matrix[&quot;rows&quot;])\n        # log the ROC curve\n        fpr = [], tpr = [], thresholds = []\n        for item in metrics_list[0][&quot;confidenceMetrics&quot;]:\n            fpr.append(item.get(&quot;falsePositiveRate&quot;, 0.0))\n            tpr.append(item.get(&quot;recall&quot;, 0.0))\n            thresholds.append(item.get(&quot;confidenceThreshold&quot;, 0.0))\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        # log the confusion matrix\n        annotations = []\n        for item in test_confusion_matrix[&quot;annotationSpecs&quot;]:\n            annotations.append(item[&quot;displayName&quot;])\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n        # log textual metrics info as well\n        for metric in metrics_list[0].keys():\n            if metric != &quot;confidenceMetrics&quot;:\n                val_string = json.dumps(metrics_list[0][metric])\n                metrics.log_metric(metric, val_string)\n        # metrics.metadata[&quot;model_type&quot;] = &quot;AutoML Tabular classification&quot;\n\n    aiplatform.init(project=project)\n\n    client = aiplatform.gapic.ModelServiceClient(client_options={&quot;api_endpoint&quot;: api_endpoint})\n    eval_name, metrics_list, metrics_str_list = get_eval_info(\n        client, model.uri.replace(&quot;aiplatform:\/\/v1\/&quot;, &quot;&quot;)\n    )\n    log_metrics(metrics_list, metricsc)\n    thresholds_dict = json.loads(thresholds_dict_str)\n\n    return (&quot;true&quot;,) if classification_thresholds_check(metrics_list[0], thresholds_dict) else (&quot;false&quot;, )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1651375335713,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":358,
        "Owner_creation_time":1416648155470,
        "Owner_last_access_time":1664057583237,
        "Owner_location":null,
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The custom component is defined as a Python function with a <code>@kfp.v2.dsl.component<\/code> decorator.<\/p>\n<p>The <code>@component<\/code> decorator specifies three optional arguments: the base container image to use; any packages to install; and the yaml file to which to write the component specification.<\/p>\n<p>The component function, <code>classif_model_eval_metrics<\/code>, has some input parameters.  The model parameter is an input <code>kfp.v2.dsl.Model artifact<\/code>.<\/p>\n<p>The two function args, <code>metrics<\/code> and <code>metricsc<\/code>, are component Outputs, in this case of types Metrics and ClassificationMetrics. They\u2019re not explicitly passed as inputs to the component step, but rather are automatically instantiated and can be used in the component.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;,\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,  # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n)\n<\/code><\/pre>\n<p>For example, in the function below, we\u2019re calling <code>metricsc.log_roc_curve()<\/code> and <code>metricsc.log_confusion_matrix()<\/code> to render these visualizations in the Pipelines UI. These Output params become component outputs when the component is compiled, and can be consumed by other pipeline steps.<\/p>\n<pre><code>def log_metrics(metrics_list, metricsc):\n        ...\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        ...\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n<\/code><\/pre>\n<p>For more information you can refer to this <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/use-vertex-pipelines-build-automl-classification-end-end-workflow\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1652802212843,
        "Answer_score":2,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72073763",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72986981,
        "Question_title":"Porting custom job from GCP AI Platform to Vertex AI - how to get state and logs of job?",
        "Question_body":"<p>I am porting custom job training from gcp AI Platform to Vertex AI.\nI am able to start a job, but can't find how to to get the status and how to stream the logs to my local client.<\/p>\n<p>For AI Platform I was using this to get the state:<\/p>\n<pre><code>from google.oauth2 import service_account\nfrom googleapiclient import discovery\nscopes = ['https:\/\/www.googleapis.com\/auth\/cloud-platform']\ncredentials = service_account.Credentials.from_service_account_file(keyFile, scopes=scopes)\nml_apis = discovery.build(&quot;ml&quot;,&quot;v1&quot;, credentials=credentials, cache_discovery=False)\nx = ml_apis.projects().jobs().get(name=&quot;projects\/%myproject%\/jobs\/&quot;+job_id).execute()  # execute http request\nreturn x['state']\n<\/code><\/pre>\n<p>And this to stream the logs:<\/p>\n<pre><code>cmd = 'gcloud ai-platform jobs stream-logs ' + job_id\n<\/code><\/pre>\n<p>This does not work for Vertex AI job. What is the replacement code?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1657835239540,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|google-ai-platform",
        "Question_view_count":62,
        "Owner_creation_time":1254829817773,
        "Owner_last_access_time":1663965241687,
        "Owner_location":"Germany",
        "Owner_reputation":2595,
        "Owner_up_votes":462,
        "Owner_down_votes":5,
        "Owner_views":357,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Can you try this command for streaming logs :<\/p>\n<pre><code>gcloud ai custom-jobs stream-logs 123 --region=europe-west4\n<\/code><\/pre>\n<p>123 is the <strong>ID<\/strong> of the custom job for this case, you can add glcoud wide flags such as --format as well.<\/p>\n<p>You can visit this <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/custom-jobs\/stream-logs\" rel=\"nofollow noreferrer\">link<\/a> for more details about this command and additional flags available.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1657854766997,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1657863259569,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72986981",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69373666,
        "Question_title":"Deployment with customer handler on Google Cloud Vertex AI",
        "Question_body":"<p>I'm trying to deploy a TorchServe instance on Google Vertex AI platform but as per their documentation (<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#response_requirements\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#response_requirements<\/a>), it requires the responses to be of the following shape:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;predictions&quot;: PREDICTIONS\n}\n<\/code><\/pre>\n<p>Where <strong>PREDICTIONS<\/strong> is an array of JSON values representing the predictions that your container has generated.<\/p>\n<p>Unfortunately, when I try to return such a shape in the <code>postprocess()<\/code> method of my custom handler, as such:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def postprocess(self, data):\n    return {\n        &quot;predictions&quot;: data\n    }\n<\/code><\/pre>\n<p>TorchServe returns:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;code&quot;: 503,\n  &quot;type&quot;: &quot;InternalServerException&quot;,\n  &quot;message&quot;: &quot;Invalid model predict output&quot;\n}\n<\/code><\/pre>\n<p>Please note that <code>data<\/code> is a list of lists, for example: [[1, 2, 1], [2, 3, 3]]. (Basically, I am generating embeddings from sentences)<\/p>\n<p>Now if I simply return <code>data<\/code> (and not a Python dictionary), it works with TorchServe but when I deploy the container on Vertex AI, it returns the following error:  <code>ModelNotFoundException<\/code>. I assumed Vertex AI throws this error since the return shape does not match what's expected (c.f. documentation).<\/p>\n<p>Did anybody successfully manage to deploy a TorchServe instance with custom handler on Vertex AI?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1632907406933,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|pytorch|google-cloud-ml|google-cloud-vertex-ai|torchserve",
        "Question_view_count":433,
        "Owner_creation_time":1300443398977,
        "Owner_last_access_time":1645461116407,
        "Owner_location":"Switzerland",
        "Owner_reputation":681,
        "Owner_up_votes":72,
        "Owner_down_votes":7,
        "Owner_views":34,
        "Question_last_edit_time":1632914906016,
        "Answer_body":"<p>Actually, making sure that the TorchServe processes correctly the input dictionary (instances) solved the issue. It seems like what's on the <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/pytorch-google-cloud-how-deploy-pytorch-models-vertex-ai\" rel=\"nofollow noreferrer\">article<\/a> did not work for me.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1633278542093,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69373666",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71578582,
        "Question_title":"connect vertex ai endpoint through .Net",
        "Question_body":"<p>Is there any way to connected google cloud platform service vertex ai endpoint through .Net code ? I am new to gcp vertex. any help is really apricated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647981155950,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|nlp|google-cloud-vertex-ai",
        "Question_view_count":128,
        "Owner_creation_time":1462469556837,
        "Owner_last_access_time":1663877676583,
        "Owner_location":null,
        "Owner_reputation":509,
        "Owner_up_votes":71,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You could refer to <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1 documentation<\/a> for reference for dot net. You can start by:<\/p>\n<ol>\n<li>Installing the package from <a href=\"https:\/\/www.nuget.org\/packages\/Google.Cloud.AIPlatform.V1\/\" rel=\"nofollow noreferrer\">Google AI Platform nuget<\/a><\/li>\n<li>If you don't have an endpoint yet you can check out <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.EndpointServiceClient\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1.EndpointServiceClient<\/a>. You can use this class to manage endpoints like create endpoint, delete endpoint, deploy endpoint, etc.\n<ul>\n<li>Check this <a href=\"https:\/\/github.com\/googleapis\/google-cloud-dotnet\/blob\/main\/apis\/Google.Cloud.AIPlatform.V1\/Google.Cloud.AIPlatform.V1.Snippets\/IndexEndpointServiceClientSnippets.g.cs\" rel=\"nofollow noreferrer\">EndpointServiceClient code sample<\/a> for usage.<\/li>\n<\/ul>\n<\/li>\n<li>If you have an endpoint and you want to run predictions using it, you can check out <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.PredictionServiceClient\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1.PredictionServiceClient<\/a>. You can use this class to perform prediction using your endpoint.\n<ul>\n<li>Specifically <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.PredictionServiceClient#Google_Cloud_AIPlatform_V1_PredictionServiceClient_Predict_Google_Cloud_AIPlatform_V1_EndpointName_System_Collections_Generic_IEnumerable_Google_Protobuf_WellKnownTypes_Value__Google_Protobuf_WellKnownTypes_Value_Google_Api_Gax_Grpc_CallSettings_\" rel=\"nofollow noreferrer\">Predict(EndpointName, IEnumerable, Value, CallSettings)<\/a> method where it accepts an endpoint as parameter.<\/li>\n<li>Check this <a href=\"https:\/\/github.com\/googleapis\/google-cloud-dotnet\/blob\/main\/apis\/Google.Cloud.AIPlatform.V1\/Google.Cloud.AIPlatform.V1.Snippets\/PredictionServiceClientSnippets.g.cs\" rel=\"nofollow noreferrer\">PredictionServiceClient code sample<\/a> for usage.<\/li>\n<\/ul>\n<\/li>\n<\/ol>",
        "Answer_comment_count":0,
        "Answer_creation_time":1648004091847,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71578582",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71937033,
        "Question_title":"Google Cloud Platform - Vertex AI training with custom data format",
        "Question_body":"<p>I need to train a custom OCR in vertex AI. My data with have folder of cropped image, each image is a line, and a csv file with 2 columns: image name and text in image.\nBut when I tried to import it into a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-managed-datasets\" rel=\"nofollow noreferrer\">dataset<\/a> in vertex AI, I see that image dataset only support for classification, segmentation, object detection. All of dataset have fixed number of label, but my data have a infinite number of labels(if we view text in image as label), so all types doesn't match with my requirement. Can I use vertex AI for training, and how to do that ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1650445911140,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|ocr|google-cloud-vertex-ai",
        "Question_view_count":303,
        "Owner_creation_time":1412860343897,
        "Owner_last_access_time":1663995247693,
        "Owner_location":"Hanoi, Vietnam",
        "Owner_reputation":803,
        "Owner_up_votes":76,
        "Owner_down_votes":8,
        "Owner_views":114,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Since Vertex AI managed datasets do not support OCR applications, you can train and deploy a custom model using Vertex AI\u2019s training and prediction services.<\/p>\n<p>I found a good <a href=\"https:\/\/medium.com\/geekculture\/building-a-complete-ocr-engine-from-scratch-in-python-be1fd184753b\" rel=\"nofollow noreferrer\">article<\/a> on building an OCR system from scratch. This OCR system is implemented in 2 steps<\/p>\n<ol>\n<li>Text detection<\/li>\n<li>Text recognition<\/li>\n<\/ol>\n<p>Please note that this article is not officially supported by Google Cloud.<\/p>\n<p>Once you have tested the model locally, you can train the same on Vertex AI using the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/custom-training\" rel=\"nofollow noreferrer\">custom model training service<\/a>. Please follow this <a href=\"https:\/\/codelabs.developers.google.com\/vertex_custom_training_prediction\" rel=\"nofollow noreferrer\">codelab<\/a> for step-by-step instructions on training and deploying a custom model.<\/p>\n<p>Once the training is complete, the model can be deployed for inference using a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/pre-built-containers\" rel=\"nofollow noreferrer\">pre-built container<\/a> offered by Vertex AI or a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements\" rel=\"nofollow noreferrer\">custom container<\/a> based on your requirements. You can also choose between batch predictions for synchronous requests and online predictions for asynchronous requests.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1652091441187,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71937033",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69806432,
        "Question_title":"Vertex AI Managed Notebook, get subnet\/IP",
        "Question_body":"<p>How can I find IP for vertex AI managed notebook instance? The service is differing from user managed notebooks in certain sense. The creation of an instance doesn't create a compute instance, so it's all managed by itself.<\/p>\n<p>My purpose is to whitelist the set of IPs in Mongo atlas. Set of IPs being of all the notebooks in that region. I'm using google-managed networks in this case.<\/p>\n<p>I've a few doubts here:<\/p>\n<ul>\n<li>Since within managed nb, I can change CPU consumption, will this reinstantiate a new cluster, with entirely new IP, or it will be 1 from among a group of IPs?<\/li>\n<li>Is it possible to add a custom init script?<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635836251457,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":665,
        "Owner_creation_time":1353151867410,
        "Owner_last_access_time":1663836030397,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":2513,
        "Owner_up_votes":1426,
        "Owner_down_votes":17,
        "Owner_views":251,
        "Question_last_edit_time":1636016613529,
        "Answer_body":"<p>If you want to connect to a database service on GCP, create a network (or use the default) and instantiate the notebook using this network (<code>Advanced options<\/code>) and create the white list for this entire network . It's required because the managed notebook creates a peering network on the network you will use, you can check you in <code>VPC Network<\/code> \u279e <code>VPC Network Peering<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Pd1ui.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Pd1ui.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>If you want an external IP, it will not work. Google managed notebooks <strong>does not use external ips<\/strong>, they basically access the internet via NAT gateways (does not matter if you use google or own managed networks) so you will not be able to do what you want. Move for user managed notebooks (where you can assign a fixed external ip) or white list any IP on your Mongo db service if you are not in a production environment.<\/p>\n<p>About yous doubts:<\/p>\n<blockquote>\n<p>Since within managed nb, I can change CPU consumption, will this instantiate a new cluster, with entirely new IP, or it will be 1 from among a group of IPs<\/p>\n<\/blockquote>\n<p>For the internal network it may change when you restart or recreate the notebook instance. For an external network, it does not exists and explained.<\/p>\n<blockquote>\n<p>Is it possible to add a custom init script?<\/p>\n<\/blockquote>\n<p>Basically not. But you can provide custom docker images for the notebook.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1635870534303,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1635872119870,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69806432",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70670669,
        "Question_title":"How do parallel trials in GCP Vertex AI work?",
        "Question_body":"<p>When you make a hyperparameter tuning job, you can specify the number of trials to run in parallel. After that, you also select the type and count of the workers. What I don't understand is when I make two or more trials run in parallel, yet only one worker, each task is said to occupy 100% of the CPU. However, if one task occupies all of the CPU's resources, how can 2 of them run in parallel? Does GCP provision more than 1 machine?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641920680143,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":168,
        "Owner_creation_time":1579801831103,
        "Owner_last_access_time":1663207732260,
        "Owner_location":"Tempe, AZ, USA",
        "Owner_reputation":71,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":null,
        "Answer_body":"<p><strong>Parallel trials<\/strong> allows you to run the trials concurrently depending on your input on the maximum number of trials.<\/p>\n<p>You are correct with your statement &quot;<em>one worker, each task is said to occupy 100% of the CPU<\/em>&quot; and for GCP to run other tasks in parallel,<\/p>\n<blockquote>\n<p>the hyperparameter tuning service provisions multiple training processing clusters (or multiple individual machines in the case of a single-process trainer). The work pool spec that you set for your job is used for each individual training cluster.<\/p>\n<\/blockquote>\n<p>Please see <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-hyperparameter-tuning#parallel-trials\" rel=\"nofollow noreferrer\">Parallel Trials Documentation<\/a> for more details.<\/p>\n<p>And for more details about Hyperparameter Tuning, you may refer to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-hyperparameter-tuning\" rel=\"nofollow noreferrer\">Hyperparameter Tuning Documentation<\/a>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1641974198663,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70670669",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70838510,
        "Question_title":"Is it possible to request a Vertex AI endpoint from another GCP project?",
        "Question_body":"<p>I trained a model on GCP Vertex AI, and deployed it on an endpoint.<\/p>\n<p>I am able to execute predictions from a sample to my model with this python code <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python<\/a><\/p>\n<p>It works within my GCP project.<\/p>\n<p>My question is, is it possible to request this endpoint from another GCP project ? If I set a service account and set IAM role in both projects ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643047926007,
        "Question_score":0,
        "Question_tags":"python-3.x|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":417,
        "Owner_creation_time":1598873976143,
        "Owner_last_access_time":1663530379503,
        "Owner_location":"Versailles, France",
        "Owner_reputation":140,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Yes it is possible. For example you have Project A and Project B, assuming that Project A hosts the model.<\/p>\n<ul>\n<li><p>Add service account of Project B in Project A and provide at least <code>roles\/aiplatform.user<\/code> predefined role. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/access-control#predefined-roles\" rel=\"nofollow noreferrer\">predefined roles<\/a> and look for <code>roles\/aiplatform.user<\/code> to see complete roles it contains.<\/p>\n<\/li>\n<li><p>This role contains <strong>aiplatform.endpoints.<\/strong>* and <strong>aiplatform.batchPredictionJobs.<\/strong>* as these are the roles needed to run predictions.<\/p>\n<blockquote>\n<p>See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/iam-permissions\" rel=\"nofollow noreferrer\">IAM permissions for Vertex AI<\/a><\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Resource<\/th>\n<th>Operation<\/th>\n<th>Permissions needed<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>batchPredictionJobs<\/td>\n<td>Create a batchPredictionJob<\/td>\n<td>aiplatform.batchPredictionJobs.create (permission needed on the parent resource)<\/td>\n<\/tr>\n<tr>\n<td>endpoints<\/td>\n<td>Predict an endpoint<\/td>\n<td>aiplatform.endpoints.predict (permission needed on the endpoint resource)<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div><\/blockquote>\n<\/li>\n<\/ul>\n<p>With this set up, Project B will be able to use the model in Project A to run predictions.<\/p>\n<p>NOTE: Just make sure that the script of Project B points to the resources in Project A like <code>project_id<\/code> and <code>endpoint_id<\/code>.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1643080918690,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1643081751060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70838510",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72163917,
        "Question_title":"400 Invalid image \"us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-11:latest\" for deployment. Please use a Model with a valid image",
        "Question_body":"<p>What could be the point I am missing here. All datasets and training is done on gcp and my setup is basic.<\/p>\n<ol>\n<li>Training, validation and test done on JupyterLab<\/li>\n<li>Model pushed to a gcp storage bucket<\/li>\n<li>Create and endpoint<\/li>\n<li>Deploy model to endpoint.<\/li>\n<\/ol>\n<p>All steps looked fine until the last (4). Tried other pre-built pytorch images recommended by google but the error is persisting. The error long is as shown below:<\/p>\n<pre><code>---------------------------------------------------------------------------\n_InactiveRpcError                         Traceback (most recent call last)\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\n     65         try:\n---&gt; 66             return callable_(*args, **kwargs)\n     67         except grpc.RpcError as exc:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)\n    945                                       wait_for_ready, compression)\n--&gt; 946         return _end_unary_response_blocking(state, call, False, None)\n    947 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)\n    848     else:\n--&gt; 849         raise _InactiveRpcError(state)\n    850 \n\n_InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\n    status = StatusCode.INVALID_ARGUMENT\n    details = &quot;Invalid image &quot;us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-11:latest&quot; for deployment. Please use a Model with a valid image.&quot;\n    debug_error_string = &quot;{&quot;created&quot;:&quot;@1652032269.328842405&quot;,&quot;description&quot;:&quot;Error received from peer ipv4:142.250.148.95:443&quot;,&quot;file&quot;:&quot;src\/core\/lib\/surface\/call.cc&quot;,&quot;file_line&quot;:903,&quot;grpc_message&quot;:&quot;Invalid image &quot;us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-11:latest&quot; for deployment. Please use a Model with a valid image.&quot;,&quot;grpc_status&quot;:3}&quot;\n&gt;\n\nThe above exception was the direct cause of the following exception:\n\nInvalidArgument                           Traceback (most recent call last)\n\/tmp\/ipykernel_2924\/2180059764.py in &lt;module&gt;\n      5     machine_type = DEPLOY_COMPUTE,\n      6     min_replica_count = 1,\n----&gt; 7     max_replica_count = 1\n      8 )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/models.py in deploy(self, model, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, sync)\n    697             explanation_parameters=explanation_parameters,\n    698             metadata=metadata,\n--&gt; 699             sync=sync,\n    700         )\n    701 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/base.py in wrapper(*args, **kwargs)\n    728                 if self:\n    729                     VertexAiResourceNounWithFutureManager.wait(self)\n--&gt; 730                 return method(*args, **kwargs)\n    731 \n    732             # callbacks to call within the Future (in same Thread)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/models.py in _deploy(self, model, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, sync)\n    812             explanation_metadata=explanation_metadata,\n    813             explanation_parameters=explanation_parameters,\n--&gt; 814             metadata=metadata,\n    815         )\n    816 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/models.py in _deploy_call(cls, api_client, endpoint_resource_name, model_resource_name, endpoint_resource_traffic_split, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata)\n    979             deployed_model=deployed_model,\n    980             traffic_split=traffic_split,\n--&gt; 981             metadata=metadata,\n    982         )\n    983 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1\/services\/endpoint_service\/client.py in deploy_model(self, request, endpoint, deployed_model, traffic_split, retry, timeout, metadata)\n   1155 \n   1156         # Send the request.\n-&gt; 1157         response = rpc(request, retry=retry, timeout=timeout, metadata=metadata,)\n   1158 \n   1159         # Wrap the response in an operation future.\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/method.py in __call__(self, timeout, retry, *args, **kwargs)\n    152             kwargs[&quot;metadata&quot;] = metadata\n    153 \n--&gt; 154         return wrapped_func(*args, **kwargs)\n    155 \n    156 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\n     66             return callable_(*args, **kwargs)\n     67         except grpc.RpcError as exc:\n---&gt; 68             raise exceptions.from_grpc_error(exc) from exc\n     69 \n     70     return error_remapped_callable\n\nInvalidArgument: 400 Invalid image &quot;us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-11:latest&quot; for deployment. Please use a Model with a valid image.\n<\/code><\/pre>\n<p>Below are some details where I create model, endpoint and deploy to endpoint.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>DEPLOY_COMPUTE = 'n1-standard-4'\nDEPLOY_IMAGE='us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-11:latest'\n\nmodel = aip.Model.upload(\n    display_name = f'{NOTEBOOK}_{TIMESTAMP}',\n    serving_container_image_uri = DEPLOY_IMAGE,\n    artifact_uri = URI,\n    labels = {'notebook':f'{NOTEBOOK}'}\n)\n\nendpoint = aip.Endpoint.create(\n    display_name = f'{NOTEBOOK}_{TIMESTAMP}',\n    labels = {'notebook':f'{NOTEBOOK}'}\n)\n\nendpoint.deploy(\n    model = model,\n    deployed_model_display_name = f'{NOTEBOOK}_{TIMESTAMP}',\n    traffic_percentage = 100,\n    machine_type = DEPLOY_COMPUTE,\n    min_replica_count = 1,\n    max_replica_count = 1\n)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1652033668977,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":208,
        "Owner_creation_time":1426920929353,
        "Owner_last_access_time":1662127122187,
        "Owner_location":"Bangkok",
        "Owner_reputation":194,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":1652241318216,
        "Answer_body":"<p>You are using <code>us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-11:latest<\/code> <strong>container image<\/strong> for importing models. However, models trained in <code>pytorch<\/code> cannot use pre-built containers when importing models since as mentioned in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/model-registry\/import-model#container-type\" rel=\"nofollow noreferrer\">documentation<\/a>,<\/p>\n<blockquote>\n<p>You can use a pre-built container if your model meets the following\nrequirements:<\/p>\n<ul>\n<li>Trained in Python 3.7 or later<\/li>\n<li>Trained using TensorFlow, scikit-learn, or XGBoost<\/li>\n<li>Exported to meet framework-specific requirements for one of the pre-built prediction containers<\/li>\n<\/ul>\n<\/blockquote>\n<p>I suggest 2 workaround options for your use case:<\/p>\n<ol>\n<li><p>You can create a custom prediction container image for your <code>pytorch<\/code> trained model by referring to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-container\" rel=\"nofollow noreferrer\">documentation<\/a> .<\/p>\n<p>or<\/p>\n<\/li>\n<li><p>Re-train your model that meets the above requirements so that you can use the pre-bult container.<\/p>\n<\/li>\n<\/ol>",
        "Answer_comment_count":2,
        "Answer_creation_time":1652252560713,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72163917",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68532457,
        "Question_title":"How to create a Logs Router Sink when a Vertex AI training job failed (after 3 attempts)?",
        "Question_body":"<p>I am running a <code>Vertex AI custom training job<\/code> (machine learnin training using custom container) on <code>GCP<\/code>. I would like to create a <code>Pub\/Sub<\/code> message when the job failed so I can post a message on some chat like Slack. Logfile (<code>Cloud Logging)<\/code> is looking like that:<\/p>\n<pre><code>{\ninsertId: &quot;xxxxx&quot;\nlabels: {\nml.googleapis.com\/endpoint: &quot;&quot;\nml.googleapis.com\/job_state: &quot;FAILED&quot;\n}\nlogName: &quot;projects\/xxx\/logs\/ml.googleapis.com%2F1113875647681265664&quot;\nreceiveTimestamp: &quot;2021-07-09T15:05:52.702295640Z&quot;\nresource: {\nlabels: {\njob_id: &quot;1113875647681265664&quot;\nproject_id: &quot;xxx&quot;\ntask_name: &quot;service&quot;\n}\ntype: &quot;ml_job&quot;\n}\nseverity: &quot;INFO&quot;\ntextPayload: &quot;Job failed.&quot;\ntimestamp: &quot;2021-07-09T15:05:52.187968162Z&quot;\n}\n<\/code><\/pre>\n<p>I am creating a Logs Router Sink with the following query:<\/p>\n<pre><code>resource.type=&quot;ml_job&quot; AND textPayload:&quot;Job failed&quot; AND labels.&quot;ml.googleapis.com\/job_state&quot;:&quot;FAILED&quot;\n<\/code><\/pre>\n<p>The issue I am facing is that Vertex AI will retry the job 3 times before declaring the job as a failure but in the logfile the message is identical. Below you have 3 examples, only the last one that failed 3 times really failed at the end.\n<a href=\"https:\/\/i.stack.imgur.com\/NSGPb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NSGPb.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>In the logfile, I don't have any count id for example. Any idea how to solve this ? Creating a BigQuery table to keep track of the number of failure per <code>resource.labels.job_id<\/code> seems to be an overkill if I need to do that in all my project. Is there a way to do a group by <code>resource.labels.job_id<\/code> and count within Logs Router Sink ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1627312838200,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-logging|google-cloud-vertex-ai",
        "Question_view_count":216,
        "Owner_creation_time":1465222092253,
        "Owner_last_access_time":1663858783617,
        "Owner_location":"Z\u00fcrich, Switzerland",
        "Owner_reputation":1414,
        "Owner_up_votes":258,
        "Owner_down_votes":3,
        "Owner_views":478,
        "Question_last_edit_time":1627313437400,
        "Answer_body":"<p>The log sink is quite simple: provide a filter, it will publish in a PubSub topic each entry which match this filter. No group by, no count, nothing!!<\/p>\n<p>I propose you to use a combination of log-based metrics and Cloud monitoring.<\/p>\n<ol>\n<li>Firstly, create a <a href=\"https:\/\/cloud.google.com\/logging\/docs\/logs-based-metrics\" rel=\"nofollow noreferrer\">log based metrics<\/a> on your job failed log entry<\/li>\n<li>Create an <a href=\"https:\/\/cloud.google.com\/logging\/docs\/logs-based-metrics\/charts-and-alerts\" rel=\"nofollow noreferrer\">alert on this log based metrics<\/a> with the following key values<\/li>\n<\/ol>\n<ul>\n<li>Set the group by that you want, for example, the jobID (i don't know what is the relevant value for VertexAI job)<\/li>\n<li>Set an alert when the threshold is equal or above 3<\/li>\n<li>Add a notification channel and set a PubSub notification (still in beta)<\/li>\n<\/ul>\n<p>With this configuration, the alert will be posted only once in PubSub when 3 occurrences of the same jobID will occur.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1627331282973,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68532457",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71990757,
        "Question_title":"How can GCP Automl handle overfitting?",
        "Question_body":"<p>I have created a Vertex AI AutoML image classification model. How can I assess it for overfitting? I assume I should be able to compare training vs validation accuracy but these do not seem to be <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/evaluating-automl-models\" rel=\"nofollow noreferrer\">available<\/a>.<\/p>\n<p>And if it is overfitting,can I tweak regularization parameters? Is it already doing cross validation? Anything else that can be done? (More data,early stopping, dropouts ie how can these be done?)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650821429613,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":27,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Deploy it to endpoint and test result with sample images by uploading to endpoint. If it's overfitting you can see the stats in analysis. You can increase the training sample and retrain your model again to get better result.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1650850146067,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71990757",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":73838593,
        "Question_title":"CGP Vertex AI notebook - turn on Secure Boot",
        "Question_body":"<p>When creating a notebook instance via GCP console, I can check a box &quot;Turn on Secure Boot&quot;. Is it possible to turn on Secure Boot when creating the notebook using gcloud command?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1664034657440,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|gcloud|google-cloud-vertex-ai",
        "Question_view_count":13,
        "Owner_creation_time":1592856629630,
        "Owner_last_access_time":1664052993457,
        "Owner_location":null,
        "Owner_reputation":5,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Question_last_edit_time":1664034873080,
        "Answer_body":"<p>It's a little involved, but <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/shielded-vm\" rel=\"nofollow noreferrer\">yes you can<\/a>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1664037589950,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73838593",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72427594,
        "Question_title":"Questions on json and GCP",
        "Question_body":"<p>So I have this code to write out a json file to be connected to via endpoint. The file is pretty standard in that it has the location of how to connect to the endpoint as well as some data.<\/p>\n<pre><code>%%writefile default-pred.json\n{   PROJECT_ID:&quot;msds434-gcp&quot;,\n    REGION:&quot;us-central1&quot;,\n    ENDPOINT_ID:&quot;2857701089334001664&quot;,\n    INPUT_DATA_FILE:&quot;INPUT-JSON&quot;,\n    &quot;instances&quot;: [\n    {&quot;age&quot;: 39,\n    &quot;bill_amt_1&quot;: 47174,\n    &quot;bill_amt_2&quot;: 47974,\n    &quot;bill_amt_3&quot;: 48630,\n    &quot;bill_amt_4&quot;: 50803,\n    &quot;bill_amt_5&quot;: 30789,\n    &quot;bill_amt_6&quot;: 15874,\n    &quot;education_level&quot;: &quot;1&quot;,\n    &quot;limit_balance&quot;: 50000,\n    &quot;marital_status&quot;: &quot;2&quot;,\n    &quot;pay_0&quot;: 0,\n    &quot;pay_2&quot;:0,\n    &quot;pay_3&quot;: 0,\n    &quot;pay_4&quot;: 0,\n    &quot;pay_5&quot;: &quot;0&quot;,\n    &quot;pay_6&quot;: &quot;0&quot;,\n    &quot;pay_amt_1&quot;: 1800,\n    &quot;pay_amt_2&quot;: 2000,\n    &quot;pay_amt_3&quot;: 3000,\n    &quot;pay_amt_4&quot;: 2000,\n    &quot;pay_amt_5&quot;: 2000,\n    &quot;pay_amt_6&quot;: 2000,\n    &quot;sex&quot;: &quot;1&quot;\n    }\n  ]\n    }\n<\/code><\/pre>\n<p>Then I have this trying to connect to the file and then taking the information to connect to the end point in question. I know the information is right as it's the exact code from GCP.<\/p>\n<pre><code>!curl \\\n-X POST \\\n-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n-H &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-prediction-aiplatform.googleapis.com\/v1alpha1\/projects\/$PROJECT_ID\/locations\/$REGION\/endpoints\/$ENDPOINT_ID:predict \\\n-d &quot;@default-pred.json&quot;\n<\/code><\/pre>\n<p>So from the information I have I would expect it to parse the information I have and connect to the endpoint, but obviously I have my file wrong somehow. Any idea what it is?<\/p>\n<pre><code>{\n  &quot;error&quot;: {\n    &quot;code&quot;: 400,\n    &quot;message&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;PROJECT_ID\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;REGION\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;ENDPOINT_ID\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;INPUT_DATA_FILE\\&quot;: Cannot find field.&quot;,\n    &quot;status&quot;: &quot;INVALID_ARGUMENT&quot;,\n    &quot;details&quot;: [\n      {\n        &quot;@type&quot;: &quot;type.googleapis.com\/google.rpc.BadRequest&quot;,\n        &quot;fieldViolations&quot;: [\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;PROJECT_ID\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;REGION\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;ENDPOINT_ID\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;INPUT_DATA_FILE\\&quot;: Cannot find field.&quot;\n          }\n        ]\n      }\n    ]\n  }\n}\n<\/code><\/pre>\n<p>What am I missing here?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653862009460,
        "Question_score":0,
        "Question_tags":"json|google-cloud-platform|endpoint|google-cloud-vertex-ai",
        "Question_view_count":77,
        "Owner_creation_time":1632597456473,
        "Owner_last_access_time":1661080359213,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>The data file should only include the data.<\/p>\n<p>You've included <code>PROJECT_ID<\/code>, <code>REGION<\/code>, <code>ENDPOINT<\/code> and should not.<\/p>\n<p>These need to be set in the (bash) environment <strong>before<\/strong> you issue the <code>curl<\/code> command:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>PROJECT_ID=&quot;msds434-gcp&quot;\nREGION=&quot;us-central1&quot;\nENDPOINT_ID=&quot;2857701089334001664&quot;\n\ncurl \\\n--request POST \\\n--header &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n--header &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-prediction-aiplatform.googleapis.com\/v1alpha1\/projects\/$PROJECT_ID\/locations\/$REGION\/endpoints\/$ENDPOINT_ID:predict \\\n--data &quot;@default-pred.json&quot;\n<\/code><\/pre>\n<p>The file <code>default-pred.json<\/code> should <strike>probably (I can never find this service's methods in <a href=\"https:\/\/developers.google.com\/apis-explorer\" rel=\"nofollow noreferrer\">APIs Explorer<\/a>!<\/strike>) just be:<\/p>\n<pre><code>{\n  instances&quot;: [\n    { &quot;age&quot;: 39,\n      &quot;bill_amt_1&quot;: 47174,\n      &quot;bill_amt_2&quot;: 47974,\n      &quot;bill_amt_3&quot;: 48630,\n      &quot;bill_amt_4&quot;: 50803,\n      &quot;bill_amt_5&quot;: 30789,\n      &quot;bill_amt_6&quot;: 15874,\n      &quot;education_level&quot;: &quot;1&quot;,\n      &quot;limit_balance&quot;: 50000,\n      &quot;marital_status&quot;: &quot;2&quot;,\n      &quot;pay_0&quot;: 0,\n      &quot;pay_2&quot;:0,\n      &quot;pay_3&quot;: 0,\n      &quot;pay_4&quot;: 0,\n      &quot;pay_5&quot;: &quot;0&quot;,\n      &quot;pay_6&quot;: &quot;0&quot;,\n      &quot;pay_amt_1&quot;: 1800,\n      &quot;pay_amt_2&quot;: 2000,\n      &quot;pay_amt_3&quot;: 3000,\n      &quot;pay_amt_4&quot;: 2000,\n      &quot;pay_amt_5&quot;: 2000,\n      &quot;pay_amt_6&quot;: 2000,\n      &quot;sex&quot;: &quot;1&quot;\n    }\n  ]\n}\n<\/code><\/pre>\n<p>See the documentation for the <code>aiplatform<\/code> <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/reference\/rest\/v1\/projects\/predict\" rel=\"nofollow noreferrer\"><code>predict<\/code><\/a> method as this explains this.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1653863593763,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1653864193883,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72427594",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71888324,
        "Question_title":"Is real-time collaboration supported by \"user-managed notebooks\" on google vertex ai?",
        "Question_body":"<p>I use GCP's vertex ai platform's &quot;user-managed notebook&quot; service. how do i enable real-time collaboration for the jupyter lab server? it sounds like this can be enabled by adding <code>--collaborative<\/code> when running <code>jupyter lab<\/code>. but the command is not exposed to me with vertex ai notebooks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650052264847,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|jupyter|jupyter-lab|google-cloud-vertex-ai",
        "Question_view_count":391,
        "Owner_creation_time":1307054568717,
        "Owner_last_access_time":1663974703113,
        "Owner_location":"florida, usa",
        "Owner_reputation":4311,
        "Owner_up_votes":878,
        "Owner_down_votes":21,
        "Owner_views":533,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can activate the collaboration feature by doing the following steps below:<\/p>\n<ol>\n<li>Go to GCP Console &gt; Vertex AI &gt; Workbench and identify the notebook you want to use.<\/li>\n<li>Click the notebook name to open notebook information<\/li>\n<li>Click &quot;VIEW VM DETAILS&quot;<\/li>\n<li>You will be redirected to Compute Engine &gt; VM Instances and it shows your notebook details<\/li>\n<li>Click &quot;EDIT&quot; and look for the section &quot;Metadata&quot;<\/li>\n<li>Under &quot;Metadata&quot;, click &quot;+ Add Item&quot; and assign value &quot;use-collaborative&quot; at <strong>key<\/strong> and &quot;true&quot; at <strong>value<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/A3ElC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/A3ElC.png\" alt=\"enter image description here\" \/><\/a><\/li>\n<li>Click &quot;SAVE&quot;<\/li>\n<li>Restart the VM by clicking &quot;STOP&quot; and wait for the operation to finish<\/li>\n<li>Click &quot;START \/ RESUME&quot; to start the VM and you should be able to use the collaboration feature.<\/li>\n<\/ol>\n<p>To share your notebook:<\/p>\n<ol>\n<li>Make sure that you give the user the correct permission in your IAM &amp; Admin &gt; IAM. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/iam\" rel=\"nofollow noreferrer\">predefined notebook permissions<\/a>.<\/li>\n<li>Open Jupyter lab and open a Python notebook. Copy the URL of the notebook and you could share this and be able to collaborate with users that were given the correct permissions.\n<blockquote>\n<p>The URL is in this format <a href=\"https:\/\/xxxxxxx-dot-us-west1.notebooks.googleusercontent.com\/lab\/tree\/your_sharable_notebook.ipynb\" rel=\"nofollow noreferrer\">https:\/\/xxxxxxx-dot-us-west1.notebooks.googleusercontent.com\/lab\/tree\/your_sharable_notebook.ipynb<\/a><\/p>\n<\/blockquote>\n<\/li>\n<\/ol>\n<p>NOTE: I tested the steps above by giving IAM permission &quot;Notebooks Admin&quot; to a colleague of mine.<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1650247675783,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71888324",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71979012,
        "Question_title":"Vertex AI scheduled notebooks doesn't recognize existence of folders",
        "Question_body":"<p>I have a managed jupyter notebook in Vertex AI that I want to schedule. The notebook works just fine as long as I start it manually, but as soon as it is scheduled, it fails. There are in fact many things that go wrong when scheduled, some of them are fixable. Before explaining what my trouble is, let me first give some details of the context.<\/p>\n<p>The notebook gathers information from an API for several stores and saves the data in different folders before processing it, saving csv-files to store-specific folders and to bigquery. So, in the location of the notebook, I have:<\/p>\n<ul>\n<li>The notebook<\/li>\n<li>Functions needed for the handling of data (as *.py files)<\/li>\n<li>A series of folders, some of which have subfolders which also have subfolders<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xhOWs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xhOWs.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>When I execute this manually, no problem. Everything works well and all files end up exactly where they should, as well as in different bigQuery tables.<\/p>\n<p>However, when scheduling the execution of the notebook, everything goes wrong. First, the files *.py cannot be read (as <code>import<\/code>). No problem, I added the functions in the notebook.<\/p>\n<p>Now, the following error is where I am at a loss, because I have no idea why it does work or how to fix it. The code that leads to the error is the following:<\/p>\n<pre><code>internal = &quot;https:\/\/api.************************&quot;\n\ndf_descriptions = [] \n\nstoress = internal\nresponse_stores = requests.get(storess,auth = HTTPBasicAuth(userInternal, keyInternal))\npathlib.Path(&quot;stores\/request_1.json&quot;).write_bytes(response_stores.content)\n\nfilepath = &quot;stores&quot;\n\nfiles = os.listdir(filepath)\n\nfor file in files:\n    with open(filepath + &quot;\/&quot;+file) as json_string:\n        jsonstr = json.load(json_string)\n        information = pd.json_normalize(jsonstr)\n    df_descriptions.append(information)\n\nStoreINFO = pd.concat(df_descriptions)\nStoreINFO = StoreINFO.dropna()\nStoreINFO = StoreINFO[StoreINFO['storeIdMappings'].map(lambda d: len(d)) &gt; 0]\n\ncloud_store_ids = list(set(StoreINFO.cloudStoreId))\n\nLastWeek = datetime.date.today()- timedelta(days=2)\nLastWeek =np.datetime64(LastWeek)\n<\/code><\/pre>\n<p>and the error reported is:<\/p>\n<pre><code>FileNotFoundError                         Traceback (most recent call last)\n\/tmp\/ipykernel_165\/2970402631.py in &lt;module&gt;\n      5 storess = internal\n      6 response_stores = requests.get(storess,auth = HTTPBasicAuth(userInternal, keyInternal))\n----&gt; 7 pathlib.Path(&quot;stores\/request_1.json&quot;).write_bytes(response_stores.content)\n      8 \n      9 filepath = &quot;stores&quot;\n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in write_bytes(self, data)\n   1228         # type-check for the buffer interface before truncating the file\n   1229         view = memoryview(data)\n-&gt; 1230         with self.open(mode='wb') as f:\n   1231             return f.write(view)\n   1232 \n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in open(self, mode, buffering, encoding, errors, newline)\n   1206             self._raise_closed()\n   1207         return io.open(self, mode, buffering, encoding, errors, newline,\n-&gt; 1208                        opener=self._opener)\n   1209 \n   1210     def read_bytes(self):\n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in _opener(self, name, flags, mode)\n   1061     def _opener(self, name, flags, mode=0o666):\n   1062         # A stub for the opener argument to built-in open()\n-&gt; 1063         return self._accessor.open(self, flags, mode)\n   1064 \n   1065     def _raw_open(self, flags, mode=0o777):\n\nFileNotFoundError: [Errno 2] No such file or directory: 'stores\/request_1.json'\n<\/code><\/pre>\n<p>I would gladly find another way to do this, for instance by using GCS buckets, but my issue is the existence of sub-folders. There are many stores and I do not wish to do this operation manually because some retailers for which I am doing this have over 1000 stores. My python code generates all these folders and as I understand it, this is not feasible in GCS.<\/p>\n<p>How can I solve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1650711286300,
        "Question_score":0,
        "Question_tags":"python-3.x|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":247,
        "Owner_creation_time":1442929315877,
        "Owner_last_access_time":1664024443763,
        "Owner_location":"Stockholm, Sverige",
        "Owner_reputation":4598,
        "Owner_up_votes":1855,
        "Owner_down_votes":13,
        "Owner_views":855,
        "Question_last_edit_time":1650713970783,
        "Answer_body":"<p>GCS uses a flat namespace, so folders don't actually exist, but can be simulated as given in this <a href=\"https:\/\/cloud.google.com\/storage\/docs\/folders\" rel=\"nofollow noreferrer\">documentation<\/a>.For your requirement, you can either use absolute path (starting with &quot;\/&quot; -- not relative) or create the &quot;stores&quot; <a href=\"https:\/\/docs.python.org\/3\/library\/pathlib.html#pathlib.Path.mkdir\" rel=\"nofollow noreferrer\">directory<\/a> (with &quot;mkdir&quot;). For more information you can check this <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/schedule-and-execute-notebooks-with-vertex-ai-workbench\" rel=\"nofollow noreferrer\">blog<\/a>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1651402011487,
        "Answer_score":1,
        "Question_favorite_count":1.0,
        "Answer_last_edit_time":1651402328352,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71979012",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71573565,
        "Question_title":"Google Cloud Vertex AI with .Net",
        "Question_body":"<p>I am new to google cloud service VERTEX AI.<\/p>\n<p>I am looking to Create, train, and deploy an AutoML text classification model through .Net application. I did not find anything for .Net with Vertex AI. If someone can please guide my to the location or any .Net code samples, will be really helpful.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647958547627,
        "Question_score":0,
        "Question_tags":".net|machine-learning|google-cloud-platform|nlp|google-cloud-vertex-ai",
        "Question_view_count":158,
        "Owner_creation_time":1462469556837,
        "Owner_last_access_time":1663877676583,
        "Owner_location":null,
        "Owner_reputation":509,
        "Owner_up_votes":71,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Question_last_edit_time":1647980919087,
        "Answer_body":"<p>You can check <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AutoML.V1\/latest\" rel=\"nofollow noreferrer\">Google.Cloud.AutoML.V1<\/a> NuGet package for .NET. Additionally, check the <a href=\"https:\/\/github.com\/googleapis\/google-cloud-dotnet\/tree\/main\/apis\/Google.Cloud.AutoML.V1\" rel=\"nofollow noreferrer\">Github of Google.Cloud.AutoML.V1<\/a> NuGet Package where you can see the sample codes.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1647988044360,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71573565",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72821008,
        "Question_title":"Vertex AI updating dataset and train model",
        "Question_body":"<p>I am really confused about organizing google Vertex Ai dataset and train the autoML model in GCP. Could any one please help me to understand?<\/p>\n<p>Let me explain scenarios in which I have confusion.<\/p>\n<p>Let\u2019s suppose if I have <em>Text entity extraction<\/em> dataset in vertex Ai \u201c<strong>contract_delivery_02<\/strong>\u201d with 25 files. I have 3 labels created (<em>DelIncoTerms, DelLocation and DelWindow<\/em>) and I have trained model. This is working great.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/KvWom.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KvWom.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Now, I have 10 more files to upload, where I have introduced 2 additional labels (<em>DelPrice &amp; DelDelivery<\/em>).<\/p>\n<p>My questions<\/p>\n<ol>\n<li>Do I require to do upload all the files (25 + 10) again ?<\/li>\n<li>Do I require to retrain my whole autoML model again ? or is there any other approach for this scenario?<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1656616780960,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":149,
        "Owner_creation_time":1462469556837,
        "Owner_last_access_time":1663877676583,
        "Owner_location":null,
        "Owner_reputation":509,
        "Owner_up_votes":71,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Question_last_edit_time":null,
        "Answer_body":"<p>For question #1, you don't have to upload all files again. In your <strong>Dataset<\/strong>, you just have to add your <strong>2 new labels<\/strong> and then upload your additional 10 files.\n<a href=\"https:\/\/i.stack.imgur.com\/rLep2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rLep2.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Once uploaded, you may now proceed to put labels on your newly added files (in your example, total of 10 files) and then assign the new labels on <strong>ALL files<\/strong> (25 + 10). You can do this by double-clicking the newly added text from the UI and then assign necessary labels.\n<a href=\"https:\/\/i.stack.imgur.com\/jqIaj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jqIaj.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For question #2, since there are newly added labels and training texts, it is necessary for you to retrain the whole autoML for more accurate Model and better quality of results.<\/p>\n<p>You may refer to this <a href=\"https:\/\/cloud.google.com\/natural-language\/automl\/docs\/prepare#expandable-2\" rel=\"nofollow noreferrer\">Text Entity Extraction preparation of data<\/a> and <a href=\"https:\/\/cloud.google.com\/natural-language\/automl\/docs\/models\" rel=\"nofollow noreferrer\">Training Models<\/a> documentation for more details.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1657060259890,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72821008",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68675615,
        "Question_title":"Tracking resources used by VertexAI pipeline",
        "Question_body":"<p>Is it possible to track the resources consumed by a VertexAI pipeline run, similar to how it is possible to do for Dataflow where it shows a live graph of how many nodes are currently running to execute the pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1628219232657,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-ai|google-cloud-vertex-ai",
        "Question_view_count":272,
        "Owner_creation_time":1471292986790,
        "Owner_last_access_time":1664058170413,
        "Owner_location":null,
        "Owner_reputation":700,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":90,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Vertex AI Pipeline provides a feature for <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/visualize-pipeline\" rel=\"nofollow noreferrer\">Visualizing and analyzing<\/a> the pipeline results.<\/p>\n<p>This feature can be used to check the resource utilization once the Pipeline is deployed.<\/p>\n<p><strong>steps:<\/strong><\/p>\n<pre><code>Go to vertex AI pipeline-&gt;\n         Select a pipeline-&gt;\n               pipeline step-&gt;\n                     view job(from Pipeline run analysis pane)\n<\/code><\/pre>\n<p>In the View Job pane we can check for the resources utilized i.e machine types,machine count,CPU utilization graph for the pipeline step and we can view the logs too.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BTMXZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BTMXZ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Utilizations:<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/8WQRc.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8WQRc.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As per this <a href=\"https:\/\/cloud.google.com\/monitoring\/api\/metrics_gcp#gcp-aiplatform\" rel=\"nofollow noreferrer\">document<\/a>, metrics from the Vertex AI like CPU utilization, CPU load are in the <a href=\"https:\/\/cloud.google.com\/products\/#product-launch-stages\" rel=\"nofollow noreferrer\">Beta<\/a> launch stage. However, you can examine the metrics like CPU utilization from Cloud Monitoring by referring to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/monitoring-metrics\" rel=\"nofollow noreferrer\">document<\/a> and also find the below snap for more reference.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Yyvnd.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yyvnd.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For changing the timeline of the graph you have to select the <strong>custom<\/strong> option in <strong>metrics explorer<\/strong> and provide the date and time for the duration that you want to view as shown in the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6ZUgE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6ZUgE.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1628260970907,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1628692036932,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68675615",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70976273,
        "Question_title":"`Model.deploy()` failing for AutoML with `400 'automatic_resources' is not supported for Model`",
        "Question_body":"<p>Trying to use the python SDK to deploy an AutoML model, but am recieving the error<\/p>\n<pre><code>google.api_core.exceptions.InvalidArgument: 400 'automatic_resources' is not supported for Model\n<\/code><\/pre>\n<p>Here's what I'm running:<\/p>\n<pre><code>from google.cloud import aiplatform\n\naiplatform.init(project=&quot;MY_PROJECT&quot;)\n\n# model is an AutoML tabular model\nmodel = aiplatform.Model(&quot;MY_MODEL_ID&quot;)\n\n# this fails\nmodel.deploy()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643912020147,
        "Question_score":0,
        "Question_tags":"python|google-cloud-vertex-ai",
        "Question_view_count":149,
        "Owner_creation_time":1519933306780,
        "Owner_last_access_time":1664076383160,
        "Owner_location":null,
        "Owner_reputation":3576,
        "Owner_up_votes":42,
        "Owner_down_votes":7,
        "Owner_views":203,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You encounter the <code>automatic_resources<\/code> because deploying models for AutoML Tables requires the user to define the machine type at model deployment. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#specify\" rel=\"nofollow noreferrer\">configure compute resources for prediction docs<\/a> for more information.<\/p>\n<blockquote>\n<p>If you want to use a custom-trained model or an AutoML tabular model\nto serve online predictions, you must specify a machine type when you\ndeploy the Model resource as a DeployedModel to an Endpoint.<\/p>\n<\/blockquote>\n<p>You should add defining of resources to your code for you to continue the deployment. Adding <code>machine_type<\/code> parameter should suffice. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#machine-types\" rel=\"nofollow noreferrer\">supported machine types docs<\/a> for complete list.<\/p>\n<p>See code below:<\/p>\n<pre><code>from google.cloud import aiplatform\n\nproject = &quot;your-project-id&quot;\nlocation = &quot;us-central1&quot;\nmodel_id = &quot;99999999&quot;\n\naiplatform.init(project=project, location=location)\nmodel_name = (f&quot;projects\/{project}\/locations\/{location}\/models\/{model_id}&quot;)\n\nmodel = aiplatform.Model(model_name=model_name)\n\nmodel.deploy(machine_type=&quot;n1-standard-4&quot;)\n\nmodel.wait()\nprint(model.display_name)\n<\/code><\/pre>\n<p>Output:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NHpJF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NHpJF.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1643942120973,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70976273",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70648776,
        "Question_title":"Python click incorrectly parses arguments when called in Vertex AI Pipeline",
        "Question_body":"<p>I'm trying to run a simple Ada-boosted Decision Tree regressor on GCP Vertex AI. To parse hyperparams and other arguments I use Click for Python, a very simple CLI library. Here's the setup for my task function:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@click.command()\n@click.argument(&quot;input_path&quot;, type=str)\n@click.option(&quot;--output-path&quot;, type=str, envvar='AIP_MODEL_DIR')\n@click.option('--gcloud', is_flag=True, help='Run as if in Google Cloud Vertex AI Pipeline')\n@click.option('--grid', is_flag=True, help='Perform a grid search instead of a single run. Ignored with --gcloud')\n@click.option(&quot;--max_depth&quot;, type=int, default=4, help='Max depth of decision tree', show_default=True)\n@click.option(&quot;--n_estimators&quot;, type=int, default=50, help='Number of AdaBoost boosts', show_default=True)\ndef click_main(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators)\n\n\ndef train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    print(input_path, output_path, gcloud)\n    logger = logging.getLogger(__name__)\n    logger.info(&quot;training models from processed data&quot;)\n    ...\n<\/code><\/pre>\n<p>When I run it locally like below, Click correctly grabs the params both from console and environment and proceeds with model training (<code>AIP_MODEL_DIR<\/code> is <code>gs:\/\/(BUCKET_NAME)\/models<\/code>)<\/p>\n<pre><code>\u276f python3 -m src.models.train_model gs:\/\/(BUCKET_NAME)\/data\/processed --gcloud\n\ngs:\/\/(BUCKET_NAME)\/data\/processed gs:\/\/(BUCKET_NAME)\/models True\n\n<\/code><\/pre>\n<p>However, when I put this code on the Vertex AI Pipeline, it throws an error, namely<\/p>\n<pre><code>FileNotFoundError: b\/(BUCKET_NAME)\/o\/data%2Fprocessed%20%20--gcloud%2Fprocessed_features.csv\n<\/code><\/pre>\n<p>As it is clearly seen, Click grabs both the parameter and the <code>--gcloud<\/code> option and assigns it to <code>input_path<\/code>. The print statement before that confirms it, both by having one too many spaces and <code>--gcloud<\/code> being parsed as false.<\/p>\n<pre><code>gs:\/\/(BUCKET_NAME)\/data\/processed  --gcloud gs:\/\/(BUCKET_NAME)\/models\/1\/model\/ False\n<\/code><\/pre>\n<p>Has anyone here encountered this issue or have any idea how to solve it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641797901177,
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|google-cloud-storage|python-click|google-cloud-vertex-ai",
        "Question_view_count":129,
        "Owner_creation_time":1579801831103,
        "Owner_last_access_time":1663207732260,
        "Owner_location":"Tempe, AZ, USA",
        "Owner_reputation":71,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Question_last_edit_time":1641896853596,
        "Answer_body":"<p>I think is due the nature of <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/arguments\/?highlight=arguments\" rel=\"nofollow noreferrer\">arguments<\/a> and <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/options\/?highlight=options\" rel=\"nofollow noreferrer\">options<\/a>, you are mixing arguments and options although is not implicit stated in the documentation but argument will eat up the options that follow. If nargs is not allocated it will default to 1 considering everything after it follows as string which it looks like this is the case.<\/p>\n<blockquote>\n<p>nargs \u2013 the number of arguments to match. If not 1 the return value is a tuple instead of single value. The default for nargs is 1 (except if the type is a tuple, then it\u2019s the arity of the tuple).<\/p>\n<\/blockquote>\n<p>I think you should first use options followed by the argument as display on the <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/documentation\/?highlight=arguments\" rel=\"nofollow noreferrer\">documentation page<\/a>. Other approach is to group it under a command as show on this <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/commands\/\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1641810988040,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70648776",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72389357,
        "Question_title":"Why does gcloud not see my managed notebook?",
        "Question_body":"<p>I have a Vertex AI managed notebook in <code>europe-west1-d<\/code>. I try both (locally and in the notebook):<\/p>\n<pre><code>gcloud notebooks instances list --location=europe-west1-d\ngcloud compute instances list --filter=&quot;my-notebook-name&quot; --format &quot;[box]&quot;\n<\/code><\/pre>\n<p>and both return nothing. What am I missing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653555893293,
        "Question_score":0,
        "Question_tags":"gcloud|google-cloud-vertex-ai",
        "Question_view_count":29,
        "Owner_creation_time":1351154914717,
        "Owner_last_access_time":1663927832783,
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Question_last_edit_time":null,
        "Answer_body":"<p>When creating a Vertex AI managed notebook, it is expected that the instance won't appear on your project. By design, the notebook instance is created in a Google managed project and is not visible to the end user.<\/p>\n<p>But if you want to see instance details of your managed notebooks, you can use the Notebooks API to send a request to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/reference\/rest\/v1\/projects.locations.runtimes\/list\" rel=\"nofollow noreferrer\">runtimes.list<\/a>. See example request:<\/p>\n<pre><code>project=your-project-here\nlocation=us-central1 #adjust based on your location\n\ncurl -X GET -H &quot;Content-Type: application\/json&quot; \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\n&quot;https:\/\/notebooks.googleapis.com\/v1\/projects\/${project}\/locations\/${location}\/runtimes&quot;\n<\/code><\/pre>\n<p>Response output:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NFjJW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NFjJW.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1653610592520,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72389357",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70205432,
        "Question_title":"VSCode cannot see packages on a GCP VM",
        "Question_body":"<p>I have an issue where vscode when connected to a VM on GCP cannot see packages installed in <code>\/opt\/conda\/lib\/python3.7\/site-packages.<\/code> I have created the VM using Vertex AI. When I open the jupyter notebook through the UI in a the browser I can see all the packages via <code>pip3 list<\/code>. But when I am connected to the VM through SSH in vscode I cannot see the packages installed such as nltk, spacy etc. and when I try to load it gives me <code>ModuleNotFoundError<\/code>. This error does not show up when I use the jupyter notebook from the Vertex AI UI. The site-packages folder is in my system path and the python that I am using is <code>\/opt\/conda\/bin\/python3<\/code>.<\/p>\n<p>Any help is appreciated. Please do let me know if my question is clear.<\/p>\n<p>EDIT: I figured out that my packages are running on a container in the VM. Is there a way for me to access those packages via jupyter notebook in vscode?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1638473538253,
        "Question_score":0,
        "Question_tags":"visual-studio-code|google-cloud-platform|ssh|google-compute-engine|google-cloud-vertex-ai",
        "Question_view_count":230,
        "Owner_creation_time":1580840045043,
        "Owner_last_access_time":1661885424787,
        "Owner_location":null,
        "Owner_reputation":85,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Question_last_edit_time":1638550660460,
        "Answer_body":"<p>Posting the answer as community wiki. As confirmed by @Abhishek, he was able to make it work by installing a docker extension on the VM then attach VS code to the container.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1638843453690,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70205432",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68277691,
        "Question_title":"How do I stop a Google Cloud's AutoML (now VertexAI) batch prediction job using the web GUI?",
        "Question_body":"<p>I started a batch prediction job in AutoML (now VertexAI) for a small csv in one of my buckets, using a classification model, then I noticed the csv had an error but was unable to find a way to cancel the job using the web GUI, it just says &quot;running&quot; but I see no &quot;stop&quot; or &quot;cancel&quot; button.<\/p>\n<p>Fortunately, it was done after 20 minutes, but I need to know how to stop a job since I will require predictions for way bigger files and can't risk having to wait until the job ends by itself. It was kind of desperating being able to watch the log throwing error after error and not being able to stop the job. I tried to delete the job but it said it can't be deleted while its running.<\/p>\n<p>I found a related question, but it was not answered, the job just finished itself after a couple of days. I can't risk that.\n<a href=\"https:\/\/stackoverflow.com\/questions\/68077606\/how-do-i-force-batch-prediction-to-stop\">How do I force &quot;batch prediction&quot; to stop?<\/a><\/p>\n<p>I will greatly appreciate any help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625608160277,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":619,
        "Owner_creation_time":1431573886067,
        "Owner_last_access_time":1664038274117,
        "Owner_location":null,
        "Owner_reputation":38,
        "Owner_up_votes":47,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":1625612012432,
        "Answer_body":"<p>Unfortunately the cancel\/stop feature is not yet available in the Vertex AI UI. As per <a href=\"https:\/\/stackoverflow.com\/questions\/68077606\/how-do-i-force-batch-prediction-to-stop\">How do I force &quot;batch prediction&quot; to stop?<\/a>, the OP sent a feedback. You can ask if there was a public issue tracker created for this so you can monitor the progress of the feature request there.<\/p>\n<p>But there is a workaround for this, just send a request <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.batchPredictionJobs\/cancel\" rel=\"nofollow noreferrer\">projects.locations.batchPredictionJobs.cancel<\/a> via REST.<\/p>\n<p>To do this you can send a request via curl. In this example the model and endpoint are located in <code>us-central1<\/code> thus the location defined in the request.<\/p>\n<p>Just supply your <code>project-id<\/code> and the <code>batch-prediction-id<\/code> on the request. To get the <code>batch-prediction-id<\/code> you can get it via UI:<\/p>\n<p>Get <code>batch-prediction-id<\/code> via UI:<\/p>\n<ul>\n<li>Open &quot;Batch Predictions&quot; tab in the Vertex AI UI<\/li>\n<li>Click on the job you want to cancel<\/li>\n<li>Job information will be displayed and the 1st entry will contain the Job ID<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/lKJdT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lKJdT.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>To cancel the job send a cancel request via curl. If requests is successful, the response body is empty.<\/p>\n<pre><code>curl -X POST -H &quot;Content-Type: application\/json&quot; \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\nhttps:\/\/us-central1-aiplatform.googleapis.com\/v1\/projects\/your-project-id\/locations\/us-central1\/batchPredictionJobs\/batch-prediction-job-id:cancel\n<\/code><\/pre>\n<p>Check in Vertex AI UI if the job was canceled.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/atSqt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/atSqt.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1625628028810,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68277691",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69577270,
        "Question_title":"Google Cloud Vertex AI - 400 'dedicated_resources' is not supported for Model",
        "Question_body":"<p>I'm attempting to deploy a text classification model I trained with Vertex AI on the Google Cloud Platform using the Python SDK.<\/p>\n<pre><code>from google.cloud import aiplatform\n\nimport os\n\nos.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;] = &quot;&lt;key location&gt;&quot;\n\ndef create_endpoint(\n    project_id: str,\n    display_name: str,\n    location: str,\n    sync: bool = True,\n):\n    endpoint = aiplatform.Endpoint.create(\n        display_name=display_name, project=project_id, location=location,\n    )\n\n    print(endpoint.display_name)\n    print(endpoint.resource_name)\n    return endpoint\n\ndef deploy_model(project_id, location, model_id):\n    model_location = &quot;projects\/{}\/locations\/{}\/models\/{}&quot;.format(project_id, location, model_id)\n\n    print(&quot;Initializing Vertex AI&quot;)\n    aiplatform.init(project=project_id, location=location)\n\n    print(&quot;Getting model from {}&quot;.format(model_location))\n    model = aiplatform.Model(model_location)\n\n    print(&quot;Creating endpoint.&quot;)\n    endpoint = create_endpoint(project_id, &quot;{}_endpoint&quot;.format(model_id), location)\n\n    print(&quot;Deploying endpoint&quot;)\n    endpoint.deploy(\n        model,\n        machine_type=&quot;n1-standard-4&quot;,\n        min_replica_count=1,\n        max_replica_count=5,\n        accelerator_type='NVIDIA_TESLA_K80',\n        accelerator_count=1\n    )\n\n    return endpoint\n\nendpoint = deploy_model(\n    &quot;&lt;project name&gt;&quot;,\n    &quot;us-central1&quot;,\n    &quot;&lt;model id&gt;&quot;,\n)\n<\/code><\/pre>\n<p>Unfortunately, when I run this code, I receive this error after the endpoint.deploy is triggered:\n<code>google.api_core.exceptions.InvalidArgument: 400 'dedicated_resources' is not supported for Model...<\/code> followed by the model location.<\/p>\n<p>Note the places I've swapped my values with &lt;***&gt; to hide my local workspace variables.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1634245049023,
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":616,
        "Owner_creation_time":1417013182680,
        "Owner_last_access_time":1663940014890,
        "Owner_location":"Boston, MA",
        "Owner_reputation":1256,
        "Owner_up_votes":391,
        "Owner_down_votes":2,
        "Owner_views":245,
        "Question_last_edit_time":1634248660769,
        "Answer_body":"<p>You encounter the error since it is not possible to assign a custom machine to a Text Classification model. Only custom models and AutoML tabular models can use custom machines as per <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#specify\" rel=\"nofollow noreferrer\">documentation<\/a>. For AutoML models aside from tabular models, Vertex AI automatically configures the machine type.<\/p>\n<blockquote>\n<p>If you want to use a custom-trained model or an AutoML tabular model\nto serve online predictions, you must specify a machine type when you\ndeploy the Model resource as a DeployedModel to an Endpoint. <strong>For other<\/strong>\n<strong>types of AutoML models, Vertex AI configures the machine types<\/strong>\n<strong>automatically.<\/strong><\/p>\n<\/blockquote>\n<p>The workaround for this is to remove the machine related parameters on your <code>endpoint.deploy()<\/code> and you should be able to deploy the model.<\/p>\n<pre><code>print(&quot;Deploying endpoint&quot;)\nendpoint.deploy(model)\n<\/code><\/pre>",
        "Answer_comment_count":1,
        "Answer_creation_time":1634271217373,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69577270",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":70379395,
        "Question_title":"Vertex AI - Viewing Pipeline Output",
        "Question_body":"<p>I have followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/schedule-cloud-scheduler\" rel=\"nofollow noreferrer\">this<\/a> tutorial to create my first scheduled Vertex AI Pipeline to run every minute. The only thing it does is prints <code>&quot;Hello, &lt;any-greet-string&gt;&quot;<\/code> and also returns this same string. I can see that it is running because the last run time updates and the last run result is &quot;Success&quot; every time.<\/p>\n<p>My question is very simple: Where can I see this string printed and the output of my pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1639659363547,
        "Question_score":2,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":346,
        "Owner_creation_time":1407245826703,
        "Owner_last_access_time":1663849819650,
        "Owner_location":null,
        "Owner_reputation":745,
        "Owner_up_votes":210,
        "Owner_down_votes":6,
        "Owner_views":168,
        "Question_last_edit_time":1639726542407,
        "Answer_body":"<p>The output of the <code>print()<\/code> statements in the pipeline can be found in &quot;Cloud Logging&quot; with the appropriate filters. To check logs for each component in the pipeline, click on the respective component in the console and click &quot;<strong>VIEW LOGS<\/strong>&quot; in the right pane. A new pane with the logs will open in the pipeline page which will allow us to see the output of the component. Refer to the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I ran a sample <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#4\" rel=\"nofollow noreferrer\">pipeline<\/a> from this codelab, <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#0\" rel=\"nofollow noreferrer\">Intro to Vertex Pipelines<\/a> and below is the output for one of the <code>print()<\/code> statements in the pipeline.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<br \/>\n<h4>UPDATE:<\/h4>\n<p>Every component in a pipeline run is deployed as an individual Vertex AI custom job. Corresponding to the sample pipeline consisting 3 components, there are 3 entries in the &quot;CUSTOM JOBS&quot; section as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Therefore, to view the logs on the run level, we would need to query the log entries with the respective <code>job_id<\/code>s of the pipeline components and the <code>job_id<\/code> of the Cloud Scheduler job. The query would look like this.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.labels.job_id=(&quot;JOB_ID_1&quot; OR &quot;JOB_ID_2&quot; [OR &quot;JOB_ID_N&quot;...])\nseverity&gt;=DEFAULT\n<\/code><\/pre>\n<p>If there are no simultaneous pipeline runs, a simpler query like below can be used.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.type=(&quot;cloud_scheduler_job&quot; OR &quot;ml_job&quot;)\nseverity&gt;=DEFAULT\n<\/code><\/pre>",
        "Answer_comment_count":2,
        "Answer_creation_time":1639667359983,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1639750456896,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70379395",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69689785,
        "Question_title":"batch predictions in GCP Vertex AI",
        "Question_body":"<p>While trying out batch predictions in GCP Vertex AI for an AutoML model, the batch prediction results span over several files(which is not convenient from a user perspective). If it would have been a single batch prediction result file i.e. covering all the records in a single file, it would make the procedure much more simple.<\/p>\n<p>For instance, I had 5585 records in my input dataset file. The batch prediction results comprise of 21 files wherein each file has records in the range of 200-300, thus, covering 5585 records altogether.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635004730777,
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":1708,
        "Owner_creation_time":1492176971557,
        "Owner_last_access_time":1657288203940,
        "Owner_location":"India",
        "Owner_reputation":105,
        "Owner_up_votes":68,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":1635191683160,
        "Answer_body":"<p>Batch predictions on an image, text,video,tabular AutoML model, runs the jobs using distributed processing which means the data is distributed among an arbitrary cluster of virtual machines and is processed in an unpredictable order because of which you will get the prediction results stored across various files in Cloud Storage. Since the batch prediction output files are not generated with the same order as an input file, a feature request has been raised and you can track the update on this request from this <a href=\"https:\/\/issuetracker.google.com\/issues\/202080076\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n<p>We cannot provide an ETA at this moment but you can follow the progress in the issue tracker and you can \u2018STAR\u2019 the issue to receive automatic updates and give it traction by referring to this <a href=\"https:\/\/developers.google.com\/issue-tracker\/guides\/subscribe#starring_an_issue\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n<p>However, if you are doing batch prediction for a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/batch-predictions#batch_request_input\" rel=\"nofollow noreferrer\">tabular AutoML model<\/a>, there you have the option to choose the BigQuery as storage where all the prediction output will be stored in a single table and then you can export the table data to a single CSV file.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1635085080550,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1635092457400,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69689785",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69676225,
        "Question_title":"GCP AI Platform API - Object Detection Metrics at Class Level (Python)",
        "Question_body":"<p>I have trained a AutoML Object Detection model in Vertex AI (a service under AI Platform in GCP). I am trying to access model evaluation metrics for each label (precision, recall, accuracy etc.) for varying Confidence Score Threshold and IoU Threshold.<\/p>\n<p>However, I am stuck at step one, even to get model's aggerate performance metric much less to the performance metric at granular levels. I have followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/evaluating-automl-models?authuser=1#aggregate\" rel=\"nofollow noreferrer\">this instruction<\/a> But I cannot seem to figure out what is <code>evaluation_id<\/code> (also see the official sample code snippet <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/model_service\/get_model_evaluation_image_object_detection_sample.py\" rel=\"nofollow noreferrer\">here<\/a>), which is:<\/p>\n<pre><code>def get_model_evaluation_image_object_detection_sample(\n    project: str,\n    model_id: str,\n    evaluation_id: str,\n    location: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n):\n    # The AI Platform services require regional API endpoints.\n    client_options = {&quot;api_endpoint&quot;: api_endpoint}\n    # Initialize client that will be used to create and send requests.\n    # This client only needs to be created once, and can be reused for multiple requests.\n    client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\n    name = client.model_evaluation_path(\n        project=project, location=location, model=model_id, evaluation=evaluation_id\n    )\n    response = client.get_model_evaluation(name=name)\n    print(&quot;response:&quot;, response)\n<\/code><\/pre>\n<p>After sometime I have figured out that for model trained in EU,  and <code>api_endpoint<\/code> shall be passed as:<\/p>\n<pre><code>location: str = &quot;europe-west4&quot;\napi_endpoint: str = &quot;europe-west4-aiplatform.googleapis.com&quot;\n<\/code><\/pre>\n<p>But whatever I try for <code>evaluation_id<\/code> leads to the following errors:<\/p>\n<pre><code>InvalidArgument: 400 List of found errors:  1.Field: name; Message: Invalid ModelEvaluation resource name.\n<\/code><\/pre>\n<p>There in the documentation it says (which is seems it contains what I need):<\/p>\n<blockquote>\n<p>For the bounding box metric, Vertex AI returns an array of metric\nvalues at different IoU threshold values (between 0 and 1) and\nconfidence threshold values (between 0 and 1). For example, you can\nnarrow in on evaluation metrics at an IoU threshold of 0.85 and a\nconfidence threshold of 0.8228. By viewing these different threshold\nvalues, you can see how they affect other metrics such as precision\nand recall.<\/p>\n<\/blockquote>\n<p>Without knowing that is contained in the output array, how would that work for each class? Basically I need for each class the model metrics for varying IoU threshold values and confidence threshold.<\/p>\n<p>Also I have tried to query from AutoML API instead, like:<\/p>\n<pre><code>client_options = {'api_endpoint': 'eu-automl.googleapis.com:443'}\n\nclient = automl.AutoMlClient(client_options=client_options)\n# Get the full path of the model.\nmodel_full_id = client.model_path(project_id, &quot;europe-west4&quot;, model_id)\n\nprint(&quot;List of model evaluations:&quot;)\nfor evaluation in client.list_model_evaluations(parent=model_full_id, filter=&quot;&quot;):\n    print(&quot;Model evaluation name: {}&quot;.format(evaluation.name))\n    print(&quot;Model annotation spec id: {}&quot;.format(evaluation.annotation_spec_id))\n    print(&quot;Create Time: {}&quot;.format(evaluation.create_time))\n    print(&quot;Evaluation example count: {}&quot;.format(evaluation.evaluated_example_count))\n    print(\n        &quot;Classification model evaluation metrics: {}&quot;.format(\n            evaluation.classification_evaluation_metrics\n        )\n    )\n<\/code><\/pre>\n<p>No surprise, also this doesn't work, and leads to:<\/p>\n<pre><code>InvalidArgument: 400 List of found errors:  1.Field: parent; Message: The provided location ID doesn't match the endpoint. For automl.googleapis.com, the valid location ID is `us-central1`. For eu-automl.googleapis.com, the valid location ID is `eu`.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634902641630,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":298,
        "Owner_creation_time":1490792741113,
        "Owner_last_access_time":1663926026750,
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":423,
        "Owner_up_votes":897,
        "Owner_down_votes":4,
        "Owner_views":84,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I was able to get the response of the model evaluation using <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform.html\" rel=\"nofollow noreferrer\">aiplatform_v1<\/a> which is well documented and this is the reference linked from the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/start\/client-libraries?authuser=1#client_libraries\" rel=\"nofollow noreferrer\">Vertex AI reference page<\/a>.<\/p>\n<p>On this script I ran <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.list_model_evaluations\" rel=\"nofollow noreferrer\">list_model_evaluations()<\/a> to get the evaluation name and used it as input for <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.get_model_evaluation\" rel=\"nofollow noreferrer\">get_model_evaluation()<\/a> which will return the evaluation details for Confidence Score Threshold, IoU Threshold, etc.<\/p>\n<p>NOTE: I don't have a trained model in <code>europe-west4<\/code> so I used <code>us-central1<\/code> instead. But if you have trained in <code>europe-west4<\/code> you should use <code>https:\/\/europe-west4-aiplatform.googleapis.com<\/code> as <code>api_endpoint<\/code> as per <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/locations#specifying_the_location_using_the\" rel=\"nofollow noreferrer\">location document<\/a>.<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\n\napi_endpoint = 'us-central1-aiplatform.googleapis.com'\nclient_options = {&quot;api_endpoint&quot;: api_endpoint}\nclient_model = aiplatform.services.model_service.ModelServiceClient(client_options=client_options)\nproject_id = 'your-project-id'\nlocation = 'us-central1'\nmodel_id = '999999999999'\n\nmodel_name = f'projects\/{project_id}\/locations\/{location}\/models\/{model_id}'\nlist_eval_request = aiplatform.types.ListModelEvaluationsRequest(parent=model_name)\nlist_eval = client_model.list_model_evaluations(request=list_eval_request)\neval_name=''\nfor val in list_eval:\n    eval_name = val.name\n\nget_eval_request = aiplatform.types.GetModelEvaluationRequest(name=eval_name)\nget_eval = client_model.get_model_evaluation(request=get_eval_request)\nprint(get_eval)\n<\/code><\/pre>\n<p>See response snippet:<\/p>\n<pre><code>name: &quot;projects\/xxxxxxxxx\/locations\/us-central1\/models\/999999999999\/evaluations\/1234567890&quot;\nmetrics_schema_uri: &quot;gs:\/\/google-cloud-aiplatform\/schema\/modelevaluation\/image_object_detection_metrics_1.0.0.yaml&quot;\nmetrics {\n  struct_value {\n    fields {\n      key: &quot;boundingBoxMeanAveragePrecision&quot;\n      value {\n        number_value: 0.20201288\n      }\n    }\n    fields {\n      key: &quot;boundingBoxMetrics&quot;\n      value {\n        list_value {\n          values {\n            struct_value {\n              fields {\n                key: &quot;confidenceMetrics&quot;\n                value {\n                  list_value {\n                    values {\n                      struct_value {\n                        fields {\n                          key: &quot;confidenceThreshold&quot;\n                          value {\n                            number_value: 0.06579724\n                          }\n                        }\n                        fields {\n                          key: &quot;f1Score&quot;\n                          value {\n                            number_value: 0.15670435\n                          }\n                        }\n                        fields {\n                          key: &quot;precision&quot;\n                          value {\n                            number_value: 0.09326923\n                          }\n                        }\n                        fields {\n                          key: &quot;recall&quot;\n                          value {\n                            number_value: 0.48989898\n                          }\n                        }\n                      }\n                    }\n                    values {\n                      struct_value {\n....\n<\/code><\/pre>\n<p><strong>EDIT 1: Get response per class<\/strong><\/p>\n<p>To get metrics per class, you can use <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.list_model_evaluation_slices\" rel=\"nofollow noreferrer\">list_model_evaluation_slices()<\/a> to get the name for each class, then use the name to <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.get_model_evaluation_slice\" rel=\"nofollow noreferrer\">get_model_evaluation_slice()<\/a>. In this code I pushed the names to a list since I have multiple classes. Then just use the values stored in the array to get the metric per class.<\/p>\n<p>In my code I used <code>label[0]<\/code> to get a single response from this class.<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\n\napi_endpoint = 'us-central1-aiplatform.googleapis.com'\nclient_options = {&quot;api_endpoint&quot;: api_endpoint}\nclient_model = aiplatform.services.model_service.ModelServiceClient(client_options=client_options)\nproject_id = 'your-project-id'\nlocation = 'us-central1'\nmodel_id = '999999999999'\n\nmodel_name = f'projects\/{project_id}\/locations\/{location}\/models\/{model_id}'\nlist_eval_request = aiplatform.types.ListModelEvaluationsRequest(parent=model_name)\nlist_eval = client_model.list_model_evaluations(request=list_eval_request)\neval_name=''\nfor val in list_eval:\n    eval_name = val.name\n\nlabel=[]\nslice_eval_request = aiplatform.types.ListModelEvaluationSlicesRequest(parent=eval_name)\nslice_eval = client_model.list_model_evaluation_slices(request=slice_eval_request)\nfor data in slice_eval:\n    label.append(data.name)\n\nget_eval_slice_request = aiplatform.types.GetModelEvaluationSliceRequest(name=label[0])\nget_eval_slice = client_model.get_model_evaluation_slice(request=get_eval_slice_request)\nprint(get_eval_slice)\n<\/code><\/pre>\n<p>Print all classes:\n<a href=\"https:\/\/i.stack.imgur.com\/pinWU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/pinWU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Classes in UI:\n<a href=\"https:\/\/i.stack.imgur.com\/mXlIW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mXlIW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Response snippet for a class:<\/p>\n<pre><code>name: &quot;projects\/xxxxxxxxx\/locations\/us-central1\/models\/999999999\/evaluations\/0000000000\/slices\/777777777&quot;\nslice_ {\n  dimension: &quot;annotationSpec&quot;\n  value: &quot;Cheese&quot;\n}\nmetrics_schema_uri: &quot;gs:\/\/google-cloud-aiplatform\/schema\/modelevaluation\/image_object_detection_metrics_1.0.0.yaml&quot;\nmetrics {\n  struct_value {\n    fields {\n      key: &quot;boundingBoxMeanAveragePrecision&quot;\n      value {\n        number_value: 0.14256561\n      }\n    }\n    fields {\n      key: &quot;boundingBoxMetrics&quot;\n      value {\n        list_value {\n          values {\n            struct_value {\n              fields {\n                key: &quot;confidenceMetrics&quot;\n                value {\n                  list_value {\n                    values {\n                      struct_value {\n                        fields {\n                          key: &quot;confidenceThreshold&quot;\n                          value {\n                            number_value: 0.06579724\n                          }\n                        }\n                        fields {\n                          key: &quot;f1Score&quot;\n                          value {\n                            number_value: 0.10344828\n                          }\n                        }\n                        fields {\n                          key: &quot;precision&quot;\n                          value {\n                            number_value: 0.06198347\n                          }\n                        }\n....\n<\/code><\/pre>",
        "Answer_comment_count":3,
        "Answer_creation_time":1635129616533,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1635151135016,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69676225",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72385022,
        "Question_title":"VertexAI's class AutoMLtabularTrainingJob doesn't recognize parameters listed in the documentation",
        "Question_body":"<p>I'm trying to create a prediction model using the VertexAI class AutoMLtabularTrainingJob, and I'm having problems with two parameters listed in the <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform.AutoMLTabularTrainingJob\" rel=\"nofollow noreferrer\">documentation<\/a>.<\/p>\n<p>First, the parameter column_specs is a dictionary in the documentation, and the parameter export_evaluated_data_items is a bool.<\/p>\n<p>I created the function below, and I called it inside a loop.<\/p>\n<pre><code>def create_training_pipeline_tabular_regression_sample(\ndisplay_name:str,\ndataset_id:int,\ncolumn_specs:dict,\ntarget_column:str = None,\noptimization_prediction_type:str = 'regression',\noptimization_objective:str = 'minimize-rmse',\nmodel_display_name:str = None,\nbudget_milli_node_hours:int = 1000,\ndisable_early_stopping:bool = False,\nexport_evaluated_data:bool = True,\nsync:bool = True,\n**kwargs\n):\n\ntabular_regression_job = aiplatform.AutoMLTabularTrainingJob(\n    display_name=display_name,\n    column_specs=column_specs,\n    optimization_prediction_type=optimization_prediction_type,\n    optimization_objective=optimization_objective\n)\n\nmy_tabular_dataset = aiplatform.TabularDataset(dataset_id)\n\nmodel = tabular_regression_job.run(\n    dataset=my_tabular_dataset,\n    target_column=target_column,\n    budget_milli_node_hours=budget_milli_node_hours,\n    model_display_name=model_display_name,\n    disable_early_stopping=disable_early_stopping,\n    export_evaluated_data_items=True,\n    sync=sync,\n    **kwargs\n)\n\nmodel.wait()\n\nprint(model.display_name)\nprint(model.resource_name)\nprint(model.uri)\nreturn model\n<\/code><\/pre>\n<p>The error is that the class is not accepting these parameters. The error message:<\/p>\n<pre><code>    ---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n\/tmp\/ipykernel_118\/330955058.py in &lt;module&gt;\n     60                     optimization_objective=optimization,\n     61                     budget_milli_node_hours= BUDGET_MILLI_NODE_HOURS,\n---&gt; 62                     export_evaluated_data_items_bigquery_destination_uri=export_evaluated_data_items_bigquery_destination_uri\n     63                 )\n     64 \n\n\/tmp\/ipykernel_118\/2971171495.py in create_training_pipeline_tabular_regression_sample(display_name, dataset_id, target_column, optimization_prediction_type, optimization_objective, model_display_name, budget_milli_node_hours, disable_early_stopping, export_evaluated_data, sync, **kwargs)\n     31         export_evaluated_data_items=True,\n     32         sync=sync,\n---&gt; 33         **kwargs\n     34     )\n     35 \n\nTypeError: run() got an unexpected keyword argument 'export_evaluated_data_items'\n<\/code><\/pre>\n<p>Does anyone know if the documentation is updated? In the page's footer the update date is recent, but these errors make me have doubts. And there's other information in the documentation that does not match with the API's use.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1653520166993,
        "Question_score":0,
        "Question_tags":"python|machine-learning|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":61,
        "Owner_creation_time":1563027261237,
        "Owner_last_access_time":1664058734170,
        "Owner_location":null,
        "Owner_reputation":315,
        "Owner_up_votes":44,
        "Owner_down_votes":0,
        "Owner_views":81,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I was able to reproduce your error when I downgraded to google-cloud-aiplatform version <code>0.7.1<\/code>. To resolve this, you must update your version to the latest <strong>google-cloud-aiplatform<\/strong> package by using the below command.<\/p>\n<pre><code>pip install google-cloud-aiplatform --upgrade\n<\/code><\/pre>\n<p>You will now have <strong>google-cloud-aiplatform<\/strong> version <code>1.13.1<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/81afC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/81afC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Once upgraded to the latest version, you can now proceed and finish your training.\n<a href=\"https:\/\/i.stack.imgur.com\/RUzE3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RUzE3.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1653620941597,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72385022",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68998065,
        "Question_title":"Read vertex ai datasets in jupyter notebook",
        "Question_body":"<p>I am trying to create a python utility that will take dataset from vertex ai datasets and will generate statistics for that dataset. But I am unable to check the dataset using jupyter notebook. Is there any way out for this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1630410227747,
        "Question_score":3,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":737,
        "Owner_creation_time":1616070829583,
        "Owner_last_access_time":1643884535477,
        "Owner_location":null,
        "Owner_reputation":45,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>If I understand correctly, you want to use <a href=\"https:\/\/cloud.google.com\/vertex-ai\" rel=\"nofollow noreferrer\">Vertex AI<\/a> dataset inside <code>Jupyter Notebook<\/code>. I don't think that this is currently possible. You are able to export <code>Vertex AI<\/code> datasets to <code>Google Cloud Storage<\/code> in JSONL format:<\/p>\n<blockquote>\n<p>Your dataset will be exported as a list of text items in JSONL format. Each row contains a Cloud Storage path, any label(s) assigned to that item, and a flag that indicates whether that item is in the training, validation, or test set.<\/p>\n<\/blockquote>\n<p>At this moment, you can use <code>BigQuery<\/code> data inside <code>Notebook<\/code> using <code>%%bigquery<\/code> like it's mentioned in <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/visualize-jupyter\" rel=\"nofollow noreferrer\">Visualizing BigQuery data in a Jupyter notebook.<\/a> or use <code>csv_read()<\/code> from machine directory or <code>GCS<\/code> like it's showed in the <a href=\"https:\/\/stackoverflow.com\/questions\/61956470\/\">How to read csv file in Google Cloud Platform jupyter notebook<\/a> thread.<\/p>\n<p>However, you can fill a <code>Feature Request<\/code> in <a href=\"https:\/\/developers.google.com\/issue-tracker\" rel=\"nofollow noreferrer\">Google Issue Tracker<\/a> to add the possibility to use <code>VertexAI<\/code> dataset directly in the <code>Jupyter Notebook<\/code> which will be considered by the <code>Google Vertex AI Team<\/code>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1631634873007,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68998065",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":69925931,
        "Question_title":"Vertex AI model batch prediction failed with internal error",
        "Question_body":"<p>I have trained the AutoMl classification model on Vertex AI, unfortunately model does not work with batch predictions, whenever I try to score training dataset (same which was used for the successful model training) with batch predictions on Vertex AI I get a following error:<\/p>\n<p>&quot;Due to one or more errors, this training job was canceled on Nov 11, 2021 at 09:42AM&quot;.<\/p>\n<p>There is an option to get a details from this error and those say the following thing:<\/p>\n<p>&quot;Batch prediction job customer_value_label_cv_automl_gui encountered the following errors: INTERNAL&quot;<\/p>\n<p>Does anyone know what might be the reason for getting this kind of error? I am very surprised that the model cannot score the dataset that it was trained on. My dataset consists of 570 columns and about 300k of records. <a href=\"https:\/\/i.stack.imgur.com\/DRbjn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DRbjn.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/0MHrg.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0MHrg.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1636622783290,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-ml|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":712,
        "Owner_creation_time":1551797759387,
        "Owner_last_access_time":1648564097343,
        "Owner_location":"Krak\u00f3w, Poland",
        "Owner_reputation":77,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1636706508710,
        "Answer_body":"<p>We have been able to finally figure this out. As we were using model.batch_predict method described in the <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform.html\" rel=\"nofollow noreferrer\">official documentation<\/a> we unnecessary set the machine_type parameter. Finally, we were able to figure out that it was causing the issue, the machine was probably too weak. Once we removed this declaration this method started to use automatic resources and that solved the case. I wish Vertex AI errors were a little bit more informative because it took us a lot of trials and error to figure out.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1637235864267,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69925931",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":72996624,
        "Question_title":"Can the list of custom jobs in vertex AI custom seen in the UI?",
        "Question_body":"<p>I have created a custom job with<\/p>\n<pre><code>gcloud ai custom-jobs create --region=us-west1 --display-name=test-job --config=trainjob.yaml\n<\/code><\/pre>\n<p>where <code>trainjob.yaml<\/code> is<\/p>\n<pre><code>workerPoolSpecs:\n  machineSpec:\n    machineType: n1-standard-4\n  replicaCount: 1\n  containerSpec:\n    imageUri: eu.gcr.io\/myproject\/myimage\n<\/code><\/pre>\n<p>I can see the list of the job via<\/p>\n<pre><code>gcloud ai custom-jobs list --region=us-west1\n<\/code><\/pre>\n<p>. Can this list seen in the UI? For AI Platform product there is jobs but I don't see anything like this in Vertex AI<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657900255003,
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai|google-ai-platform|gcp-ai-platform-training",
        "Question_view_count":68,
        "Owner_creation_time":1485529624880,
        "Owner_last_access_time":1663859955020,
        "Owner_location":null,
        "Owner_reputation":1991,
        "Owner_up_votes":468,
        "Owner_down_votes":27,
        "Owner_views":107,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I don't know if it is exactly what you are looking for, but you can see the custom training jobs details using the UI at <code>Console<\/code> &gt; <code>Vertex AI<\/code> &gt; <code>Training<\/code> &gt; <code>Custom Jobs<\/code> or following the next <a href=\"https:\/\/console.cloud.google.com\/vertex-ai\/training\/custom-jobs\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1658154433090,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1658160312116,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72996624",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":68131742,
        "Question_title":"How to select a target column in a Vertex AI AutoML time series model",
        "Question_body":"<p>I am testing out Google Cloud Vertex AI with a time series AutoML model.<\/p>\n<p>I have created a dataset, from a Biguery table, with 2 columns, one of a timestamp and another of a numeric value I want to predict:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/t5R7S.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t5R7S.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><code>salesorderdate<\/code> is my <code>TIMESTAMP<\/code> column and <code>orders<\/code> is the value I want to predict.<\/p>\n<p>When I proceed to the next step I cannot select <code>orders<\/code> as my value to predict, there are no available options for this field:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/HOed3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/HOed3.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What am I missing here? Surely the time series value <em>is<\/em> the target value in this case? Is there an expectation of more fields here, and can one actually add additional features as columns to a time series model in this way?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624627510520,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":606,
        "Owner_creation_time":1491294824967,
        "Owner_last_access_time":1663926174400,
        "Owner_location":"Herefordshire, UK",
        "Owner_reputation":3107,
        "Owner_up_votes":442,
        "Owner_down_votes":19,
        "Owner_views":434,
        "Question_last_edit_time":1624971874476,
        "Answer_body":"<p>I guess from your question that you are using &quot;forecasting models&quot;. Please note that it is in &quot;Preview&quot; <a href=\"https:\/\/cloud.google.com\/products#product-launch-stages\" rel=\"nofollow noreferrer\">Product launch stage<\/a> with all consequences of that fact.<\/p>\n<p>In the documentation you may find <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-tabular#data-structure\" rel=\"nofollow noreferrer\">Training data structure<\/a> following information:<\/p>\n<blockquote>\n<ul>\n<li>There must be at least two and no more than 1,000 columns.<\/li>\n<\/ul>\n<p>For datasets that train AutoML models, one column must be the target,\nand there must be at least one feature available to train the model.\nIf the training data does not include the target column, Vertex AI\ncannot associate the training data with the desired result.<\/p>\n<\/blockquote>\n<p>I suppose you are using AutoML models so in this situation you need to have 3 columns in the data set:<\/p>\n<ul>\n<li>Time column - used to place the observation represented by that row in time<\/li>\n<li>time series identifier column as &quot;Forecasting training data usually includes multiple time series&quot;<\/li>\n<li>and target column which is value that model should learn to predict.<\/li>\n<\/ul>\n<p>If you want to predict <code>orders<\/code> this should be target column. But before you are choosing this target this &quot;time series identifier column&quot; is already chosen in previous step, so you do not have available column to choose.<\/p>\n<p>So you need to add to your BigQuery table at least one additional column with will be used as time series column. You can add to your data set column with the same value in each row. This concept is presented in <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/bp-tabular#data_preparation_best_practices\" rel=\"nofollow noreferrer\">Forecasting data preparation best practices<\/a>:<\/p>\n<blockquote>\n<p>You can train a forecasting model on a single time series (in other\nwords, the time series identifier column contains the same value for\nall rows). However, Vertex AI is a better fit for training data that\ncontains two or more time series. For best results, you should have at\nleast 10 time series for every column used to train the model.<\/p>\n<\/blockquote>",
        "Answer_comment_count":2,
        "Answer_creation_time":1624889338667,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1624960598496,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68131742",
        "Question_exclusive_tag":"Vertex AI"
    },
    {
        "Question_id":71075704,
        "Question_title":"how to login wandb with another acount using colab?",
        "Question_body":"<p>For example, I have A, B acounts.<\/p>\n<p>First, I log in Google Colab with A account.\nand I want to log in wandb with B acounts. ( using !wandb login )\nis it possible??<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644556957917,
        "Question_score":0,
        "Question_tags":"authentication|google-colaboratory|wandb",
        "Question_view_count":316,
        "Owner_creation_time":1644556763937,
        "Owner_last_access_time":1649221420000,
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can you the following commands to force a relogin:<\/p>\n<ul>\n<li>from terminal<\/li>\n<\/ul>\n<pre><code>wandb login --relogin\n<\/code><\/pre>\n<ul>\n<li>Using the API:<\/li>\n<\/ul>\n<pre><code>import wandb\nwandb.login(relogin=True)\n<\/code><\/pre>",
        "Answer_comment_count":1,
        "Answer_creation_time":1644559750243,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71075704",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":63469762,
        "Question_title":"Weights&Biases Sweep Keras K-Fold Validation",
        "Question_body":"<p>I'm using Weights&amp;Biases Cloud-based sweeps with Keras.\nSo first i create a new Sweep within a W&amp;B Project with a config like following:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>description: LSTM Model\nmethod: random\nmetric:\n  goal: maximize\n  name: val_accuracy\nname: LSTM-Sweep\nparameters:\n  batch_size:\n    distribution: int_uniform\n    max: 128\n    min: 32\n  epochs:\n    distribution: constant\n    value: 200\n  node_size1:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size2:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size3:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size4:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  node_size5:\n    distribution: categorical\n    values:\n    - 64\n    - 128\n    - 256\n  num_layers:\n    distribution: categorical\n    values:\n    - 1\n    - 2\n    - 3\n  optimizer:\n    distribution: categorical\n    values:\n    - Adam\n    - Adamax\n    - Adagrad\n  path:\n    distribution: constant\n    value: &quot;.\/path\/to\/data\/&quot;\nprogram: sweep.py\nproject: SLR\n<\/code><\/pre>\n<p>My <code>sweep.py<\/code> file looks something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># imports\ninit = wandb.init(project=&quot;my-project&quot;, reinit=True)\nconfig = wandb.config\n\ndef main():\n    skfold = StratifiedKFold(n_splits=5, \n    shuffle=True, random_state=7)\n    cvscores = []\n    group_id = wandb.util.generate_id()\n    X,y = # load data\n    i = 0\n    for train, test in skfold.split(X,y):\n        i=i+1\n        run = wandb.init(group=group_id, reinit=True, name=group_id+&quot;#&quot;+str(i))\n        model = # build model\n        model.fit([...], WandBCallback())\n        cvscores.append([...])\n        wandb.join()\n\nif __name__ == &quot;__main__&quot;:\n    main()\n<\/code><\/pre>\n<p>Starting this with the <code>wandb agent<\/code> command within the folder of <code>sweep.py<\/code>.<\/p>\n<p>What i experienced with this setup is, that with the first wandb.init() call a new run is initialized. Okay, i could just remove that. But when calling wandb.init() for the second time it seems to lose track of the sweep it is running in. Online an empty run is listed in the sweep (because of the first wandb.init() call), all other runs are listed inside the project, but not in the sweep.<\/p>\n<p>My goal is to have a run for each fold of the k-Fold cross-validation. At least i thought this would be the right way of doing this.\nIs there a different approach to combine sweeps with keras k-fold cross validation?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597757510213,
        "Question_score":2,
        "Question_tags":"python|keras|k-fold|wandb",
        "Question_view_count":1043,
        "Owner_creation_time":1486549300030,
        "Owner_last_access_time":1620240688720,
        "Owner_location":"Germany",
        "Owner_reputation":45,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":1661849871016,
        "Answer_body":"<p>We put together an example of how to accomplish k-fold cross validation:<\/p>\n<p><a href=\"https:\/\/github.com\/wandb\/examples\/tree\/master\/examples\/wandb-sweeps\/sweeps-cross-validation\" rel=\"nofollow noreferrer\">https:\/\/github.com\/wandb\/examples\/tree\/master\/examples\/wandb-sweeps\/sweeps-cross-validation<\/a><\/p>\n<p>The solution requires some contortions for the wandb library to spawn multiple jobs on behalf of a launched sweep job.<\/p>\n<p>The basic idea is:<\/p>\n<ul>\n<li>The agent requests a new set of parameters from the cloud hosted parameter server.  This is the run called <code>sweep_run<\/code> in the main function.<\/li>\n<li>Send information about what the folds should process over a multiprocessing queue to waiting processes<\/li>\n<li>Each spawned process logs to their own run, organized with group and job_type to enable auto-grouping in the UI<\/li>\n<li>When the process is finished, it sends the primary metric over a queue to the parent sweep run<\/li>\n<li>The sweep run reads metrics from the child runs and logs it to the sweep run so that the sweep can use that result to impact future parameter choices and\/or hyperband early termination optimizations<\/li>\n<\/ul>\n<p>Example visualizations of the sweep and k-fold grouping can be seen here:<\/p>\n<ul>\n<li>Sweep: <a href=\"https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/sweeps\/vp0fsvku\" rel=\"nofollow noreferrer\">https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/sweeps\/vp0fsvku<\/a><\/li>\n<li>K-fold Grouping: <a href=\"https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/groups\/vp0fsvku\" rel=\"nofollow noreferrer\">https:\/\/app.wandb.ai\/jeffr\/examples-sweeps-cross-validation\/groups\/vp0fsvku<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":1,
        "Answer_creation_time":1598032113043,
        "Answer_score":6,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1599772735680,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63469762",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69920078,
        "Question_title":"What does \"save_graph\" keyword in WandbCallback mean?",
        "Question_body":"<p>I'm using Weights and Biases to track my deep learning models. To monitor everything I use the <code>WandbCallback<\/code> in <code>.fit<\/code>.\nIn the <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/integrations\/keras\/wandbcallback\" rel=\"nofollow noreferrer\">WandbCallback documentation<\/a> there is the keyword <code>save_graph<\/code> which defaults to True. The description is very brief and I wondered what that saved graph is and what it's for? Is saving a graph a costly operation? For what is it needed? (like does it complement something else, like saving the best model?)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636577514790,
        "Question_score":0,
        "Question_tags":"tensorflow2.0|tf.keras|wandb",
        "Question_view_count":60,
        "Owner_creation_time":1600299419450,
        "Owner_last_access_time":1662833034530,
        "Owner_location":"Germany",
        "Owner_reputation":62,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Question_last_edit_time":null,
        "Answer_body":"<p>That is used to create log a <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/data-types\/graph\" rel=\"nofollow noreferrer\">wandb.Graph<\/a> of the model. This class is typically used for saving and diplaying neural net models. It represents the graph as an array of nodes and edges. The nodes can have labels that can be visualized by wandb. Here's an example of the graph that it produces: <a href=\"https:\/\/wandb.ai\/l2k2\/keras-examples\/runs\/ieqy2e9h\/model\" rel=\"nofollow noreferrer\">https:\/\/wandb.ai\/l2k2\/keras-examples\/runs\/ieqy2e9h\/model<\/a><\/p>\n<p>Here's the code that does that within the callback. <a href=\"https:\/\/github.com\/wandb\/client\/blob\/1609f82c84e2244ed8fe62c746099d2094bd746a\/wandb\/integration\/keras\/keras.py#L552\" rel=\"nofollow noreferrer\">https:\/\/github.com\/wandb\/client\/blob\/1609f82c84e2244ed8fe62c746099d2094bd746a\/wandb\/integration\/keras\/keras.py#L552<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1636642289973,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69920078",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69640534,
        "Question_title":"How to log artifacts in wandb while using saimpletransformers?",
        "Question_body":"<p>I am creating a Question Answering model using <a href=\"https:\/\/simpletransformers.ai\/docs\/qa-specifics\/\" rel=\"nofollow noreferrer\">simpletransformers<\/a>. I would also like to use wandb to track model artifacts. As I understand from <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/other\/simpletransformers\" rel=\"nofollow noreferrer\">wandb docs<\/a>, there is an integration touchpoint for simpletransformers but there is no mention of logging artifacts.<\/p>\n<p>I would like to log artifacts generated at the train, validation, and test phase such as train.json, eval.json, test.json, output\/nbest_predictions_test.json and best performing model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634705743473,
        "Question_score":0,
        "Question_tags":"nlp-question-answering|simpletransformers|wandb",
        "Question_view_count":53,
        "Owner_creation_time":1528765704783,
        "Owner_last_access_time":1635485916997,
        "Owner_location":"Ahmedabad, Gujarat, India",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Currently simpleTransformers doesn't support logging artifacts within the training\/testing scripts. But you can do it manually:<\/p>\n<pre><code>import os \n\nwith wandb.init(id=model.wandb_run_id, resume=&quot;allow&quot;, project=wandb_project) as training_run:\n    for dir in sorted(os.listdir(&quot;outputs&quot;)):\n        if &quot;checkpoint&quot; in dir:\n            artifact = wandb.Artifact(&quot;model-checkpoints&quot;, type=&quot;checkpoints&quot;)\n            artifact.add_dir(&quot;outputs&quot; + &quot;\/&quot; + dir)\n            training_run.log_artifact(artifact)\n<\/code><\/pre>\n<p>For more info, you can follow along with the W&amp;B notebook in the SimpleTransofrmer's README.md<\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1634729204573,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69640534",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71744288,
        "Question_title":"wandb getting logged without initiating",
        "Question_body":"<p>I do not want to use wandb. I don't even have an account. I am simply following <a href=\"https:\/\/colab.research.google.com\/github\/huggingface\/notebooks\/blob\/master\/examples\/summarization.ipynb#scrollTo=UmvbnJ9JIrJd\" rel=\"nofollow noreferrer\">this notebook<\/a> for finetuning. I am not running the 2nd and 3 cells because I do not want to push the model to the hub.<\/p>\n<p>However, when I do trainer.train() I get the following error : <a href=\"https:\/\/i.stack.imgur.com\/VPIZm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VPIZm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I don't understand where wandb.log is being called.\nI even tried os.environ[&quot;WANDB_DISABLED&quot;]  = &quot;true&quot; but I still get the error.\nPlease help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649110110203,
        "Question_score":0,
        "Question_tags":"huggingface-transformers|huggingface-tokenizers|fine-tune|wandb|huggingface",
        "Question_view_count":289,
        "Owner_creation_time":1499867951607,
        "Owner_last_access_time":1661234383987,
        "Owner_location":null,
        "Owner_reputation":307,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Question_last_edit_time":null,
        "Answer_body":"<p>posting the same message as <a href=\"https:\/\/github.com\/huggingface\/transformers\/issues\/16594\" rel=\"nofollow noreferrer\">over on <code>transformers<\/code><\/a>:<\/p>\n<hr \/>\n<p>You can turn off all external logger logging, including wandb logging by passing <code>report_to=&quot;none&quot;<\/code> in your <code>Seq2SeqTrainingArguments<\/code>.<\/p>\n<p>You might have noticed the following warning when setting up your TrainingArguments:<\/p>\n<pre><code>The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-)\n<\/code><\/pre>\n<p>Right now the default is to run all loggers that you have installed, so maybe you installed wandb on your machine since the last time you ran the script?<\/p>\n<p>If you would like to log with wandb, best practice would already be to start setting <code>report_to=&quot;wandb&quot;<\/code><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1649156563120,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71744288",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":68952727,
        "Question_title":"wandb: get a list of all artifact collections and all aliases of those artifacts",
        "Question_body":"<p>The wandb documentation doesn't seem to explain how to do this - but it should be a fairly common use case I'd imagine?<\/p>\n<p>I achieved mostly (but not completely) what I wanted like this, but it seems a bit clunky? I'd have expected to have an <code>self.aliases<\/code> property on the <code>ArtifactCollection<\/code> instances?<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>ENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\ndef get_model_artifacts(key=None):\n    wandb.login(key=key if key is not None else API_KEY)\n    api = wandb.Api(overrides={&quot;entity&quot;: ENTITY})\n    model_names = [\n        i\n        for i in api.artifact_type(\n            type_name=&quot;models&quot;, project=&quot;train&quot;\n        ).collections()\n    ]\n    for model in model_names:\n        artifact = api.artifact(&quot;train\/&quot; + model.name + &quot;:latest&quot;)\n        model._attrs.update(artifact._attrs)\n        model._attrs[&quot;metadata&quot;] = json.loads(model._attrs[&quot;metadata&quot;])\n        model.aliases = [x[&quot;alias&quot;] for x in model._attrs[&quot;aliases&quot;]]\n    return model_names\n<\/code><\/pre>\n<p>I guess I could possibly look into writing a custom graph-ql query if needed or just use this clunky method.<\/p>\n<p>Am I missing something? Is there a cleaner way to do this?<\/p>\n<p>The one thing this clunky method is missing is any old aliases - it only shows the latest model and then any aliases of that (let's say &quot;latest&quot; and also &quot;v4&quot; etc.) - not sure how this would\/should be displayed but I'd have hoped to be able to get old aliases as well (i.e. aliases that point to old versions of the artifact). Although, this is less important.<\/p>\n<p><strong>EDIT<\/strong> - after a few hours looking through their sdk code, I have this (still not that happy with how clunky it is):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>ENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\ndef get_model_artifacts(key=None):\n    wandb.login(key=key if key is not None else API_KEY)\n    api = wandb.Api(overrides={&quot;entity&quot;: ENTITY})\n    model_artifacts = [\n        a\n        for a in api.artifact_type(\n            type_name=&quot;models&quot;, project=&quot;train&quot;\n        ).collections()\n    ]\n\n    def get_alias_tuple(artifact_version):\n        version = None\n        aliases = []\n        for a in artifact_version._attrs[&quot;aliases&quot;]:\n            if re.match(r&quot;^v\\d+$&quot;, a[&quot;alias&quot;]):\n                version = a[&quot;alias&quot;]\n            else:\n                aliases.append(a[&quot;alias&quot;])\n        return version, aliases\n\n    for model in model_artifacts:\n        # artifact = api.artifact(&quot;train\/&quot; + model.name + &quot;:latest&quot;)\n        # model._attrs.update(artifact._attrs)\n        # model._attrs[&quot;metadata&quot;] = json.loads(model._attrs[&quot;metadata&quot;])\n        versions = model.versions()\n        version_dict = dict(get_alias_tuple(version) for version in versions)\n        model.version_dict = version_dict\n        model.aliases = [\n            x for key, val in model.version_dict.items() for x in [key] + val\n        ]\n    return model_artifacts\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630064062593,
        "Question_score":1,
        "Question_tags":"python|wandb",
        "Question_view_count":363,
        "Owner_creation_time":1526368885180,
        "Owner_last_access_time":1663954559473,
        "Owner_location":"Munich, Germany",
        "Owner_reputation":3944,
        "Owner_up_votes":574,
        "Owner_down_votes":47,
        "Owner_views":217,
        "Question_last_edit_time":1630357543656,
        "Answer_body":"<p>I'm Annirudh. I'm an engineer at W&amp;B who helped build artifacts. Your solution is really close, but by using the <code>latest<\/code> alias when fetching the artifact we're only going to be considering the aliases from that one artifact instead of all the versions. You could get around that by looping over the versions:<\/p>\n<pre><code>api = wandb.Api()\ncollections = [\n    coll for coll in api.artifact_type(type_name=TYPE, project=PROJECT).collections()\n]\n\n\naliases = set()\nfor coll in collections:\n    for artifact in coll.versions():\n        aliases.update(artifact.aliases)\n\nprint(collections)\nprint(aliases)\n<\/code><\/pre>\n<p>Currently, the documentation is spare on collections but we're polishing them up in the public API and will release some docs around it shortly. These APIs aren't quite release ready yet -- so apologies for the rough edges.<\/p>\n<p>Please feel free to reach out to me directly in the future if you have any other questions regarding artifacts. Always happy to help.<\/p>",
        "Answer_comment_count":14,
        "Answer_creation_time":1630122133813,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68952727",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71223654,
        "Question_title":"How to get wandb to pass arguments by position?",
        "Question_body":"<p>I am trying to explore the results of different parameter settings on my python script &quot;train.py&quot;. For that, I use a wandb sweep. Each wandb agent executes the file &quot;train.py&quot; and passes some parameters to it. As per the wandb documentation (<a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command\" rel=\"nofollow noreferrer\">https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command<\/a>), in case of e.g. two parameters &quot;param1&quot; and &quot;param2&quot; each agents starts the file with the command<\/p>\n<pre><code>\/usr\/bin\/env python train.py --param1=value1 --param2=value2\n<\/code><\/pre>\n<p>However, &quot;train.py&quot; expects<\/p>\n<pre><code>\/usr\/bin\/env python train.py value1 value2\n<\/code><\/pre>\n<p>and parses the parameter values by position. I did not write train.py and would like to not change it if possible. How can I get wandb to pass the values without &quot;--param1=&quot; in front?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645542424610,
        "Question_score":0,
        "Question_tags":"python|python-3.x|wandb",
        "Question_view_count":270,
        "Owner_creation_time":1440414980200,
        "Owner_last_access_time":1645630582560,
        "Owner_location":"Germany",
        "Owner_reputation":5,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Don't think you can get positional arguments from W&amp;B Sweeps. However, there's a little work around you can try that won't require you touching the <code>train.py<\/code> file.<\/p>\n<p>You can create an invoker file, let's call it <code>invoke.py<\/code>. Now, you can use it get rid of the keyword argument names. Something like this might work:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport subprocess\n\nif len(sys.argv[0]) &lt;= 1:\n  print(f&quot;{sys.argv[0]} program_name param0=&lt;param0&gt; param1=&lt;param1&gt; ...&quot;)\n  sys.exit(0)\n\nprogram = sys.argv[1]\nparams = sys.argv[2:]\n\nposparam = []\nfor param in params:\n  _, val = param.split(&quot;=&quot;)\n  posparam.append(val)\n\ncommand = [sys.executable, program, *posparam]\nprocess = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nout, err = process.communicate()\nsys.stdout.write(out.decode())\nsys.stdout.flush()\nsys.stderr.write(err.decode())\nsys.stderr.flush()\nsys.exit(process.returncode)\n<\/code><\/pre>\n<p>This allows you to invoke your <code>train.py<\/code> file as follows:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ python3 invoke.py \/path\/to\/train.py param0=0.001 param1=20 ...\n<\/code><\/pre>\n<p>Now to perform W&amp;B sweeps you can create a <code>command:<\/code> section (<a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command\" rel=\"nofollow noreferrer\">reference<\/a>) in your <code>sweeps.yaml<\/code> file while sweeping over the parameters <code>param0<\/code> and <code>param1<\/code>. For example:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>program: invoke.py\n...\nparameters:\n  param0:\n    distribution: uniform\n    min: 0\n    max: 1\n  param1:\n    distribution: categorical\n    values: [10, 20, 30]\ncommand:\n - ${env}\n - ${program}\n - \/path\/to\/train.py\n - ${args_no_hyphens}\n<\/code><\/pre>",
        "Answer_comment_count":3,
        "Answer_creation_time":1645548706457,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1645549016652,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71223654",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72729259,
        "Question_title":"wandb.Table raises error: AssertionError: columns argument expects a `list` object",
        "Question_body":"<p>I'm very beginner with wandb , so this is very basic question.\nI have dataframe which has my x features and y values.\nI'm tryin to follow <a href=\"https:\/\/docs.wandb.ai\/examples\" rel=\"nofollow noreferrer\">this tutorial<\/a>  to train model from my pandas dataframe . However, when I try to create wandb table from my pandas dataframe, I get an error:<\/p>\n<pre><code>\nwandb.init(project='my-xgb', config={'lr': 0.01})\n\n#the log didn't work  so I haven't run it at the moment (the log 'loss') \n#wandb.log({'loss': loss, ...})\n\n\n# Create a W&amp;B Table with your pandas dataframe\ntable = wandb.Table(df1)\n<\/code><\/pre>\n<blockquote>\n<p>AssertionError: columns argument expects a <code>list<\/code> object<\/p>\n<\/blockquote>\n<p>I have no idea why is this happen, and why it excpect a list. In the tutorial it doesn't look like the dataframe is list.<\/p>\n<p>My end goal - to be able to create wandb table.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655983093280,
        "Question_score":0,
        "Question_tags":"python|pandas|dataframe|wandb",
        "Question_view_count":63,
        "Owner_creation_time":1572256318027,
        "Owner_last_access_time":1663676968963,
        "Owner_location":"Israel",
        "Owner_reputation":1387,
        "Owner_up_votes":955,
        "Owner_down_votes":16,
        "Owner_views":224,
        "Question_last_edit_time":null,
        "Answer_body":"<p><strong>Short answer<\/strong>: <code>table = wandb.Table(dataframe=my_df)<\/code>.<\/p>\n<p>The explanation of your specific case is at the bottom.<\/p>\n<hr \/>\n<p><strong>Minimal example<\/strong> of using <code>wandb.Table<\/code> with a DataFrame:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import wandb\nimport pandas as pd\n\niris_path = 'https:\/\/raw.githubusercontent.com\/mwaskom\/seaborn-data\/master\/iris.csv'\niris = pd.read_csv(iris_path)\ntable = wandb.Table(dataframe=iris)\nwandb.log({'dataframe_in_table': table})\n<\/code><\/pre>\n<p>(Here the dataset is called the Iris dataset that consists of &quot;3 different types of irises\u2019 (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray&quot;)<\/p>\n<p>There are two ways of creating W&amp;B <code>Table<\/code>s according to <a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/log-tables#create-tables\" rel=\"nofollow noreferrer\">the official documentation<\/a>:<\/p>\n<ul>\n<li><strong>List of Rows<\/strong>: Log named columns and rows of data. For example: <code>wandb.Table(columns=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], data=[[&quot;1a&quot;, &quot;1b&quot;, &quot;1c&quot;], [&quot;2a&quot;, &quot;2b&quot;, &quot;2c&quot;]])<\/code> generates a table with two rows and three columns.<\/li>\n<li><strong>Pandas DataFrame<\/strong>: Log a DataFrame using <code>wandb.Table(dataframe=my_df)<\/code>. Column names will be extracted from the DataFrame.<\/li>\n<\/ul>\n<hr \/>\n<p><strong>Explanation<\/strong>: Why <code>table = wandb.Table(my_df)<\/code> gives error &quot;columns argument expects a <code>list<\/code> object&quot;? Because <code>wandb.Table<\/code>'s init function looks like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def __init__(\n        self,\n        columns=None,\n        data=None,\n        rows=None,\n        dataframe=None,\n        dtype=None,\n        optional=True,\n        allow_mixed_types=False,\n    ):\n<\/code><\/pre>\n<p>If one passes a DataFrame without telling it's a DataFrame, <code>wandb.Table<\/code> will assume the argument is <code>columns<\/code>.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1655983680023,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1655985859240,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72729259",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73244442,
        "Question_title":"HuggingFace Trainer() cannot report to wandb",
        "Question_body":"<p>I am trying to set trainer with arguments <code>report_to<\/code> to <code>wandb<\/code>, refer to <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/huggingface#getting-started-track-experiments\" rel=\"nofollow noreferrer\">this docs<\/a>\nwith config:<\/p>\n<pre><code>training_args = TrainingArguments(\n    output_dir=&quot;test_trainer&quot;,\n    evaluation_strategy=&quot;steps&quot;,\n    learning_rate=config.learning_rate,\n    num_train_epochs=config.epochs,\n    weight_decay=config.weight_decay,\n    logging_dir=config.logging_dir,\n    report_to=&quot;wandb&quot;,\n    save_total_limit=1,\n    per_device_train_batch_size=config.batch_size,\n    per_device_eval_batch_size=config.batch_size,\n    fp16=True,\n    load_best_model_at_end=True,\n    seed=42\n)\n<\/code><\/pre>\n<p>yet when I set trainer with:<\/p>\n<pre><code>trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics\n)\n<\/code><\/pre>\n<p>it shows:<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-68-b009351ab52d&gt; in &lt;module&gt;\n      4     train_dataset=train_dataset,\n      5     eval_dataset=eval_dataset,\n----&gt; 6     compute_metrics=compute_metrics\n      7 )\n\n~\/.virtualenvs\/transformers_lab\/lib\/python3.7\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\n    286                 &quot;You should subclass `Trainer` and override the `create_optimizer_and_scheduler` method.&quot;\n    287             )\n--&gt; 288         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\n    289         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\n    290         self.callback_handler = CallbackHandler(\n\n~\/.virtualenvs\/transformers_lab\/lib\/python3.7\/site-packages\/transformers\/integrations.py in get_reporting_integration_callbacks(report_to)\n    794         if integration not in INTEGRATION_TO_CALLBACK:\n    795             raise ValueError(\n--&gt; 796                 f&quot;{integration} is not supported, only {', '.join(INTEGRATION_TO_CALLBACK.keys())} are supported.&quot;\n    797             )\n    798     return [INTEGRATION_TO_CALLBACK[integration] for integration in report_to]\n\nValueError: w is not supported, only azure_ml, comet_ml, mlflow, tensorboard, wandb are supported.\n<\/code><\/pre>\n<p>Have anyone got same error before?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659671305703,
        "Question_score":0,
        "Question_tags":"python|huggingface-transformers|wandb",
        "Question_view_count":47,
        "Owner_creation_time":1587186468850,
        "Owner_last_access_time":1663918152847,
        "Owner_location":"Taipei, Taiwan R.O.C",
        "Owner_reputation":163,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Question_last_edit_time":1660033763876,
        "Answer_body":"<p>Although the documentation states that the <code>report_to<\/code> parameter can receive both <code>List[str]<\/code> or <code>str<\/code> I have always used a list with 1! element for this purpose.<\/p>\n<p>Therefore, even if you report only to wandb, the solution to your problem is to replace:<\/p>\n<pre><code> report_to = 'wandb'\n<\/code><\/pre>\n<p>with<\/p>\n<pre><code>report_to = [&quot;wandb&quot;]\n<\/code><\/pre>",
        "Answer_comment_count":2,
        "Answer_creation_time":1659781694627,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73244442",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72590067,
        "Question_title":"jupyterLab Wandb does not iterative",
        "Question_body":"<p>I try to use WanDB but when i use wandb.init() there is nothing.!<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/EZfVt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EZfVt.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I am waiting a lot of time.<\/p>\n<p>However, there is nothing in window.<\/p>\n<p>This is working well in Kernel.<\/p>\n<p>please.. help me guys<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655016960480,
        "Question_score":0,
        "Question_tags":"jupyter-lab|wandb",
        "Question_view_count":40,
        "Owner_creation_time":1609152494583,
        "Owner_last_access_time":1663846781627,
        "Owner_location":"South Korea",
        "Owner_reputation":72,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I work at Weights &amp; Biases. If you're in a notebook the quickest thing you can do to get going with <code>wandb<\/code> is simply:<\/p>\n<pre><code>wandb.init(project=MY_PROJECT, entity=MY_ENTITY)\n<\/code><\/pre>\n<p>No <code>!wandb login<\/code>, <code>wandb.login()<\/code> or <code>%%wandb<\/code> needed. If you're not already logged in then <code>wandb.init<\/code> will ask you for you API key.<\/p>\n<p>(curious where you found <code>%%wandb<\/code> by the way?)<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1655111602567,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72590067",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":67291062,
        "Question_title":"Control the logging frequency and contents when using wandb with HuggingFace",
        "Question_body":"<p>I am using the <code>wandb<\/code> with my HuggingFace code. I would like to log the loss and other metrics. Now I have two questions<\/p>\n<ul>\n<li>How does <code>wandb<\/code> decide when to log the loss? Is this decided by <code>logging_steps<\/code> in <code>TrainingArguments(...)<\/code>\uff1f<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>training_args = TrainingArguments(output_dir=&quot;test&quot;, \n                                  learning_rate=lr,\n                                  num_train_epochs=n_epoch,\n                                  seed=seed,\n                                  per_device_train_batch_size=2,\n                                  per_device_eval_batch_size=2,\n                                  logging_strategy=&quot;steps&quot;,\n                                  logging_steps=5,\n                                  report_to=&quot;wandb&quot;)\n<\/code><\/pre>\n<ul>\n<li>How do I make sure <code>wandb<\/code> log other metrics (for example, adding validation metrics after each epoch)? Does this happen automatically?<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619559781187,
        "Question_score":0,
        "Question_tags":"huggingface-transformers|wandb",
        "Question_view_count":385,
        "Owner_creation_time":1490778676137,
        "Owner_last_access_time":1664077488930,
        "Owner_location":null,
        "Owner_reputation":322,
        "Owner_up_votes":239,
        "Owner_down_votes":1,
        "Owner_views":109,
        "Question_last_edit_time":null,
        "Answer_body":"<p>Correct, it is dictated by the <code>on_log<\/code> event from the Trainer, you can see it <a href=\"https:\/\/github.com\/huggingface\/transformers\/blob\/6b241e0e3bda24546d15835e5e0b48b8d1e4732c\/src\/transformers\/integrations.py#L747\" rel=\"nofollow noreferrer\">here<\/a> in WandbCallback<\/p>\n<p>Your validation metrics should be logged to W&amp;B automatically every time you validate. How often Trainer does evaluation depends on what setting is used for <code>evaluation_strategy<\/code> (and potentially <code>eval_steps<\/code> if <code>evaluation_strategy == &quot;steps&quot;<\/code>)<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1620162973717,
        "Answer_score":2,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67291062",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72001154,
        "Question_title":"How to prevent Weights & Biases from saving best model parameters",
        "Question_body":"<p>I am using Weights &amp; Biases (<a href=\"https:\/\/wandb.ai\/\" rel=\"nofollow noreferrer\">link<\/a>) to manage hyperparameter optimization and log the results. I am training using Keras with a Tensorflow backend, and I am using the out-of-the-box logging functionality of Weights &amp; Biases, in which I run<\/p>\n<pre><code>wandb.init(project='project_name', entity='username', config=config)\n<\/code><\/pre>\n<p>and then add a <code>WandbCallback()<\/code> to the callbacks of <code>classifier.fit()<\/code>. By default, Weights &amp; Biases appears to save the model parameters (i.e., the model's weights and biases) and store them in the cloud. This eats up my account's storage quota, and it is unnecessary --- I only care about tracking the model loss\/accuracy as a function of the hyperparameters.<\/p>\n<p>Is it possible for me to train a model and log the loss and accuracy using Weights &amp; Biases, but not store the model parameters in the cloud? How can I do this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650896835733,
        "Question_score":3,
        "Question_tags":"python|tensorflow|machine-learning|keras|wandb",
        "Question_view_count":204,
        "Owner_creation_time":1514243890900,
        "Owner_last_access_time":1663708014960,
        "Owner_location":null,
        "Owner_reputation":167,
        "Owner_up_votes":113,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In order to not save the trained model weights during hyperparam optimization you do something like this:<\/p>\n<pre><code>classifier.fit(..., callbacks=[WandbCallback(.., save_model=False)]\n<\/code><\/pre>\n<p>This will only track the metrics (train\/validation loss\/acc, etc.).<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1650898428827,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72001154",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71926953,
        "Question_title":"Wandb website for Huggingface Trainer shows plots and logs only for the first model",
        "Question_body":"<p>I am finetuning multiple models using for loop as follows.<\/p>\n<pre><code>for file in os.listdir(args.data_dir):\n    finetune(args, file)\n<\/code><\/pre>\n<p>BUT <code>wandb<\/code> website shows plots and logs only for the first file i.e., <code>file1<\/code> in <code>data_dir<\/code> although it is training and saving models for other files. It feels very strange behavior.<\/p>\n<pre><code>wandb: Synced bertweet-base-finetuned-file1: https:\/\/wandb.ai\/***\/huggingface\/runs\/***\n<\/code><\/pre>\n<p>This is a small snippet of <strong>finetuning<\/strong> code with Huggingface:<\/p>\n<pre><code>def finetune(args, file):\n    training_args = TrainingArguments(\n        output_dir=f'{model_name}-finetuned-{file}',\n        overwrite_output_dir=True,\n        evaluation_strategy='no',\n        num_train_epochs=args.epochs,\n        learning_rate=args.lr,\n        weight_decay=args.decay,\n        per_device_train_batch_size=args.batch_size,\n        per_device_eval_batch_size=args.batch_size,\n        fp16=True, # mixed-precision training to boost speed\n        save_strategy='no',\n        seed=args.seed,\n        dataloader_num_workers=4,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset['train'],\n        eval_dataset=None,\n        data_collator=data_collator,\n    )\n    trainer.train()\n    trainer.save_model()\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1650379721377,
        "Question_score":0,
        "Question_tags":"python|huggingface-transformers|wandb",
        "Question_view_count":178,
        "Owner_creation_time":1392296244357,
        "Owner_last_access_time":1664050955010,
        "Owner_location":"Chicago, IL, USA",
        "Owner_reputation":1020,
        "Owner_up_votes":278,
        "Owner_down_votes":17,
        "Owner_views":206,
        "Question_last_edit_time":1650406117732,
        "Answer_body":"<p><code>wandb.init(reinit=True)<\/code> and <code>run.finish()<\/code> helped me to log the models <strong>separately<\/strong> on wandb website.<\/p>\n<p>The working code looks like below:<\/p>\n<pre><code>\nfor file in os.listdir(args.data_dir):\n    finetune(args, file)\n\nimport wandb\ndef finetune(args, file):\n    run = wandb.init(reinit=True)\n    ...\n    run.finish()\n<\/code><\/pre>\n<p>Reference: <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script\" rel=\"nofollow noreferrer\">https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script<\/a><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1650469853183,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71926953",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73412851,
        "Question_title":"What is the meaning of 'config = wandb.config'?",
        "Question_body":"<p>I try to do the settings for a sweep for my Logistic regression model. I read the tutorials of wandb and cannot understand how to make the configurations and especially the meaning of <code>config=wandb.config<\/code> in the tutorials. I would really appreciate it if someone gave me a good explanation of the steps. Here is what I've done:<\/p>\n<pre><code>sweep_config = {\n    'method': 'grid'\n}\n\nmetric = {\n    'name': 'f1-score',\n    'goal': 'maximize'\n}\n\nsweep_config['metric'] = metric\n\nparameters = {\n    'penalty': {\n        'values': ['l2']\n    },\n    'C': {\n        'values': [0.01, 0.1, 1.0, 10.0, 100.0]\n    }\n}\n\nsweep_config['parameters'] = parameters\n<\/code><\/pre>\n<p>Then I create the yaml file:<\/p>\n<pre><code>stream = open('config.yaml', 'w')\nyaml.dump(sweep_config, stream) \n<\/code><\/pre>\n<p>Then it's time for training:<\/p>\n<pre><code>with wandb.init(project=WANDB_PROJECT_NAME):\n    config = wandb.config\n    \n    features = pd.read_csv('data\/x_features.csv')\n    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n\n    X_features = features = vectorizer.fit_transform(features['lemmatized_reason'])\n\n    y_labels = pd.read_csv('data\/y_labels.csv')\n\n    split_data = train_test_split(X_features, y_labels, train_size = 0.85, test_size = 0.15, stratify=y_labels)\n    features_train, labels_train = split_data[0], split_data[2]\n    features_test, labels_test = split_data[1], split_data[3]\n    \n    config = wandb.config\n    log_reg = LogisticRegression(\n        penalty=config.penalty,\n        C = config.C\n    )\n    \n    log_reg.fit(features_train, labels_train)\n    \n    labels_pred = log_reg.predict(features_test)\n    labels_proba = log_reg.predict_proba(features_test)\n    labels=list(map(str,y_labels['label'].unique()))\n    \n    # Visualize single plot\n    cm = wandb.sklearn.plot_confusion_matrix(labels_test, labels_pred, labels)\n    \n    score_f1 = f1_score(labels_test, labels_pred, average='weighted')\n    \n    sm = wandb.sklearn.plot_summary_metrics(\n    log_reg, features_train, labels_train, features_test, labels_test)\n    \n    roc = wandb.sklearn.plot_roc(labels_test, labels_proba)\n    \n    wandb.log({\n        &quot;f1-weighted-log-regression-tfidf-skf&quot;: score_f1, \n        &quot;roc-log-regression-tfidf-skf&quot;: roc, \n        &quot;conf-mat-logistic-regression-tfidf-skf&quot;: cm,\n        &quot;summary-metrics-logistic-regression-tfidf-skf&quot;: sm\n        })\n<\/code><\/pre>\n<p>And finally sweep_id and agent outside of <code>with<\/code> statement:<\/p>\n<pre><code>sweep_id = wandb.sweep(sweep_config, project=&quot;multiple-classifiers&quot;)\nwandb.agent(sweep_id)\n<\/code><\/pre>\n<p>There is something major I am missing here with this config thing, that I just cannot understand.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660891849347,
        "Question_score":0,
        "Question_tags":"python|wandb",
        "Question_view_count":61,
        "Owner_creation_time":1557474244067,
        "Owner_last_access_time":1663941664767,
        "Owner_location":null,
        "Owner_reputation":626,
        "Owner_up_votes":120,
        "Owner_down_votes":3,
        "Owner_views":140,
        "Question_last_edit_time":null,
        "Answer_body":"<p>I work at Weights &amp; Biases. With wandb Sweeps, the idea is that wandb needs to be able to change the hyperparameters in the sweep.<\/p>\n<p>The below section where the hyperparameters are passed to <code>LogisticRegression<\/code> could also be re-written<\/p>\n<pre><code>config = wandb.config\nlog_reg = LogisticRegression(\n    penalty=config.penalty,\n    C = config.C\n)\n<\/code><\/pre>\n<p>like this:<\/p>\n<pre><code>log_reg = LogisticRegression(\n    penalty=wandb.config.penalty,\n    C = wandb.config.C\n)\n<\/code><\/pre>\n<p>However, I think you're missing defining a train function or train script, which needs to also be passed to wandb. With out it, your example above won't work.<\/p>\n<p>Below is a minimal example that should help. Hopefully the <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\" rel=\"nofollow noreferrer\">sweeps documentation<\/a> can also help.<\/p>\n<pre><code>import numpy as np \nimport random\nimport wandb\n\n#  Step 1: Define sweep config\nsweep_configuration = {\n    'method': 'random',\n    'name': 'sweep',\n    'metric': {'goal': 'maximize', 'name': 'val_acc'},\n    'parameters': \n    {\n        'batch_size': {'values': [16, 32, 64]},\n        'epochs': {'values': [5, 10, 15]},\n        'lr': {'max': 0.1, 'min': 0.0001}\n     }\n}\n\n#  Step 2: Initialize sweep by passing in config\nsweep_id = wandb.sweep(sweep_configuration)\n\ndef train_one_epoch(epoch, lr, bs): \n  acc = 0.25 + ((epoch\/30) +  (random.random()\/10))\n  loss = 0.2 + (1 - ((epoch-1)\/10 +  random.random()\/5))\n  return acc, loss\n\ndef evaluate_one_epoch(epoch): \n  acc = 0.1 + ((epoch\/20) +  (random.random()\/10))\n  loss = 0.25 + (1 - ((epoch-1)\/10 +  random.random()\/6))\n  return acc, loss\n\ndef train():\n    run = wandb.init()\n\n    #  Step 3: Use hyperparameter values from `wandb.config`\n    lr  =  wandb.config.lr\n    bs = wandb.config.batch_size\n    epochs = wandb.config.epochs\n\n    for epoch in np.arange(1, epochs):\n      train_acc, train_loss = train_one_epoch(epoch, lr, bs)\n      val_acc, val_loss = evaluate_one_epoch(epoch)\n\n      wandb.log({\n        'epoch': epoch, \n        'train_acc': train_acc,\n        'train_loss': train_loss, \n        'val_acc': val_acc, \n        'val_loss': val_loss\n      })\n\n#  Step 4: Launch sweep by making a call to `wandb.agent`\nwandb.agent(sweep_id, function=train, count=4)\n<\/code><\/pre>\n<p>Finally, can you share the link where you found the code above? Maybe we need to update some examples :)<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1660907847087,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73412851",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":70644326,
        "Question_title":"wandb.plot.line does not work and it just shows a table",
        "Question_body":"<p>I used this example that was provided by WandB. However, the web interface just shows a table instead of a figure.<\/p>\n<pre><code>data = [[i, random.random() + math.sin(i \/ 10)] for i in range(100)]\n        table = wandb.Table(data=data, columns=[&quot;step&quot;, &quot;height&quot;])\n        wandb.log({'line-plot1': wandb.plot.line(table, &quot;step&quot;, &quot;height&quot;)})\n<\/code><\/pre>\n<p>This is a screenshot from WandB's web interface:\n<a href=\"https:\/\/i.stack.imgur.com\/INmPU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/INmPU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Also, I have the same problem with other kinds of figures and charts that use a table.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641753400317,
        "Question_score":0,
        "Question_tags":"python|deep-learning|pytorch|wandb",
        "Question_view_count":184,
        "Owner_creation_time":1445719444550,
        "Owner_last_access_time":1664061059603,
        "Owner_location":"Iran, Canada",
        "Owner_reputation":331,
        "Owner_up_votes":113,
        "Owner_down_votes":3,
        "Owner_views":40,
        "Question_last_edit_time":null,
        "Answer_body":"<blockquote>\n<p>X-Post from the wandb forum<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744\" rel=\"nofollow noreferrer\">https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744<\/a><\/p>\n<p>If you click the section called \u201cCustom Charts\u201d above the Table, it\u2019ll show the line plot that you\u2019ve logged.<\/p>\n<p>Logging the Table also is expected behaviour, because this will allow users to interactively explore the logged data in a W&amp;B Table after logging it.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1641809697277,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70644326",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":73062370,
        "Question_title":"How to get a graph with the best performing runs via Sweeps (Weights & Biases)?",
        "Question_body":"<p>For my NER model I use Weights &amp; Biases sweeps for hyperparameter search. I do a grid search with about 100 runs and there are some really meaningful graphs. However, I can't figure out how to create a graph that shows about the best 10 runs in terms of f-score. Does anyone know how to do this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1658389162900,
        "Question_score":0,
        "Question_tags":"machine-learning|nlp|named-entity-recognition|wandb",
        "Question_view_count":58,
        "Owner_creation_time":1643710211767,
        "Owner_last_access_time":1663833597150,
        "Owner_location":null,
        "Owner_reputation":78,
        "Owner_up_votes":19,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Question_last_edit_time":null,
        "Answer_body":"<p>In the sweep view, you can filter runs by certain criteria by clicking this button:\n<a href=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>There, you can add a filter to only show runs with an f1 score, or an accuracy or whatever metric you have logged higher than a certain value:\n<a href=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Of course, this won't filter for the 10 best runs, but for all runs with an accuracy of 0.9 and higher (example in picture).<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1658393400157,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73062370",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":71916901,
        "Question_title":"WANDB Getting a run id based on tag",
        "Question_body":"<h1>Context<\/h1>\n<p>Hi!<\/p>\n<p>In <code>wandb<\/code> I can download a model based on a tag (<code>prod<\/code> for example), but I would like to also get all metrics associated to that run by using tags.<\/p>\n<p>The problem is that I don't know how to a get specific run ID based a tag.<\/p>\n<h1>Example<\/h1>\n<p>Using the code bellow we can extract a run summary metrics, but setting run IDs is setting me back.<\/p>\n<p>So if I can get run IDs based on tag or just explicitly download metrics  with another API call, like with a special sintax in <code>api.run<\/code>, that would be great! In the code example bellow I would like to use the <code>what_i_want_to_use<\/code> string to call the API instead of <code>what_i_use<\/code>.<\/p>\n<pre><code>import wandb\nfrom ast import literal_eval\napi = wandb.Api()\n\nwhat_i_use = &quot;team_name\/project_name\/runID_h3h3h4h4h4h4&quot;\n# what_i_want_to_use = &quot;team_name\/project_name\/artifact_name\/prod_tag&quot;\n\n# run is specified by &lt;entity&gt;\/&lt;project&gt;\/&lt;run_id&gt;\nrun = api.run(what_i_use)\n\n\n# save the metrics for the run to a csv file\nmetrics_dataframe = run.summary\nprint(metrics_dataframe['a_summary_metric'])\n\n<\/code><\/pre>\n<p>By running through the docs I didn't find any solution so far. Any ideias?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/mWl3I.png\" rel=\"nofollow noreferrer\">wandb public api run details<\/a><\/p>\n<p>Thanks for reading!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650314726207,
        "Question_score":0,
        "Question_tags":"python|metadata|wandb",
        "Question_view_count":645,
        "Owner_creation_time":1444675970627,
        "Owner_last_access_time":1664044672477,
        "Owner_location":null,
        "Owner_reputation":5,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Question_last_edit_time":null,
        "Answer_body":"<p>It is possible to filter runs by tags as well. You can read more about it <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/public-api\/api#runs\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<pre><code>You can filter by config.*, summary_metrics.*, tags, state, entity, createdAt, etc.\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1650458488357,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71916901",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":72731861,
        "Question_title":"Hyperparameter tunning with wandb - CommError: Sweep user not valid when trying to initial the sweep",
        "Question_body":"<p>I'mt rying to use wandb for hyperparameter tunning as described in <a href=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/pytorch\/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb?authuser=1#scrollTo=hoAi-idR1DQk\" rel=\"nofollow noreferrer\">this notebook<\/a> (but using my dataframe and trying to do it on random forest regressor instead).<\/p>\n<p>I'm trying to initial the sweep but I get the error:<\/p>\n<pre><code>sweep_configuration = {\n    &quot;name&quot;: &quot;test-project&quot;,\n    &quot;method&quot;: &quot;random&quot;,\n    &quot;entity&quot;:&quot;my_name&quot;\u05ea\n    &quot;metric&quot;: {\n        &quot;name&quot;: &quot;loss&quot;,\n        &quot;goal&quot;: &quot;minimize&quot;\n    }\n    \n}\n\nparameters_dict = {\n    'n_estimators': {\n        'values': [100,200,300]\n        },\n    'max_depth': {\n        'values': [4,7,10,14]\n        },\n    'min_samples_split': {\n          'values': [2,4,8]\n        },\n    \n    'min_samples_leaf': {\n          'values': [2,4,8]\n        },\n    \n    \n    'max_features': {\n          'values': [1,7,10]\n        },\n\n    }\n\nsweep_configuration['parameters'] = parameters_dict\n\nsweep_id = wandb.sweep(sweep_configuration)\n\n\n<\/code><\/pre>\n<blockquote>\n<p>400 response executing GraphQL. {&quot;errors&quot;:[{&quot;message&quot;:&quot;Sweep user not\nvalid&quot;,&quot;path&quot;:[&quot;upsertSweep&quot;]}],&quot;data&quot;:{&quot;upsertSweep&quot;:null}} wandb:\nERROR Error while calling W&amp;B API: Sweep user not valid (&lt;Response\n[400]&gt;)<br \/>\nCommError: Sweep user not valid<\/p>\n<\/blockquote>\n<p>My end goal : to inital the sweep<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655994282873,
        "Question_score":1,
        "Question_tags":"python|hyperparameters|mlops|wandb",
        "Question_view_count":182,
        "Owner_creation_time":1572256318027,
        "Owner_last_access_time":1663676968963,
        "Owner_location":"Israel",
        "Owner_reputation":1387,
        "Owner_up_votes":955,
        "Owner_down_votes":16,
        "Owner_views":224,
        "Question_last_edit_time":1656234648443,
        "Answer_body":"<p>Two things to try:<\/p>\n<ul>\n<li><p>Like in the notebook, you should pass <code>project=&quot;your-project-name&quot;<\/code> like <code>wandb.sweep(sweep_configuration, project=&quot;your-project-name&quot;)<\/code><\/p>\n<\/li>\n<li><p>Have you logged in to W&amp;B (using <code>wandb.login()<\/code>)?<\/p>\n<\/li>\n<\/ul>\n<p>Finally, once you've successfully created the sweep, you should pass the <code>sweep_id<\/code> and your function (here <code>train<\/code>) like:\n<code>wandb.agent(sweep_id, train, count=5)<\/code><\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1656329528480,
        "Answer_score":1,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72731861",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":67202711,
        "Question_title":"How to get multiple lines exported to wandb",
        "Question_body":"<p>I am using the library weights and  biases. My model outputs a curve (a time series). I'd like to see how this curve changes throughout training. So, I'd need some kind of slider where I can select epoch and it shows me the curve for that epoch. It could be something very similar to what it's done with histograms (it shows an image of the histograms across epochs and when you hover it display the histogram corresponding to that epoch). Is there a way to do this or something similar using <code>wandb<\/code>?<\/p>\n<p>Currently my code looks like this:<\/p>\n<pre><code>for epoch in range(epochs):\n   output = model(input)\n   #output is shape (37,40) (lenght 40 and I have 37 samples)\n   #it's enough to plot the first sample\n   xs = torch.arange(40).unsqueeze(dim=1)\n   ys = output[0,:].unsqueeze(dim=1)\n   wandb.log({&quot;line&quot;: wandb.plot.line_series(xs=xs, ys=ys,title=&quot;Out&quot;)}, step=epoch)\n<\/code><\/pre>\n<p>I'd appreciate any help! Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619035214083,
        "Question_score":1,
        "Question_tags":"python|pytorch|wandb",
        "Question_view_count":840,
        "Owner_creation_time":1577734207070,
        "Owner_last_access_time":1663602351910,
        "Owner_location":null,
        "Owner_reputation":422,
        "Owner_up_votes":72,
        "Owner_down_votes":3,
        "Owner_views":66,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can use <code>wandb.log()<\/code> with matplotlib. Create your plot using matplotlib:<\/p>\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(0, 50)\nfor i in range(1, 4):\n    fig, ax = plt.subplots()\n    y = x ** i\n    ax.plot(x, y)\n    wandb.log({'chart': ax})\n<\/code><\/pre>\n<p>Then when you look on your wandb dashboard for the run, you will see the plot rendered as plotly plot. Click the gear in the upper left hand corner to see a slider that lets you slide over training steps and see the plot at each step.<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1620426900467,
        "Answer_score":3,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67202711",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":69425994,
        "Question_title":"is there any way to scale axis of plots in wandb?",
        "Question_body":"<p>I am working with weight and bias(wandb).<br \/>\nHowever, it logs by step. And that makes plot disturbing when comparing runs.<br \/>\nFor example, I have a run A and run B(assume that they run with same dataset).<br \/>\nrun A: 30epochs, 4 batch, 200step\/epoch<br \/>\nrun B: 30epochs, 8 batch, 100step\/epoch<\/p>\n<p>then, the plot of run A gets longer(double, in this case) in axis x when it shows with run B.<\/p>\n<p>How can I scale x axis depend to runs AFTER training?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633272788153,
        "Question_score":1,
        "Question_tags":"wandb",
        "Question_view_count":604,
        "Owner_creation_time":1603378831587,
        "Owner_last_access_time":1663547314893,
        "Owner_location":null,
        "Owner_reputation":161,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Question_last_edit_time":null,
        "Answer_body":"<p>You can change the x-axis used via the chart settings by clicking on the pencil icon and then selecting a different x-axis. E.g. in your case you could select &quot;epoch&quot; instead of &quot;steps&quot;. Just make sure to log &quot;epoch&quot; to your charts, something like:<\/p>\n<pre><code>steps_per_epoch = n_samples \/ batch_size\nepoch = current_step \/ steps_per_epoch\nwandb.log({&quot;epoch&quot;:epoch, ...})\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1633345375677,
        "Answer_score":4,
        "Question_favorite_count":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69425994",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":70637798,
        "Question_title":"wandb: artifact.add_reference() option to add specific (not current) versionId or ETag to stop the need for re-upload to s3?",
        "Question_body":"<p>I feel like this should be possible, but I looked through the wandb SDK code and I can't find an easy\/logical way to do it. It <em>might<\/em> be possible to hack it by modifying the manifest entries at some point later (but maybe before the artifact is logged to wandb as then the manifest and the entries might be locked)? I saw things like this in the SDK code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>version = manifest_entry.extra.get(&quot;versionID&quot;)\netag = manifest_entry.extra.get(&quot;etag&quot;)\n<\/code><\/pre>\n<p>So, I figure we can probably edit those?<\/p>\n<h2>UPDATE<\/h2>\n<p>So, I tried to hack it together with something like this and it works but it feels wrong:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport wandb\nimport boto3\nfrom wandb.util import md5_file\n\nENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nPROJECT = os.environ.get(&quot;WANDB_PROJECT&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\napi = api = wandb.Api(overrides={&quot;entity&quot;: ENTITY, &quot;project&quot;: ENTITY})\nrun = wandb.init(entity=ENTITY, project=PROJECT, job_type=&quot;test upload&quot;)\nfile = &quot;admin2Codes.txt&quot;  # &quot;admin1CodesASCII.txt&quot; # (both already on s3 with a couple versions)\nartifact = wandb.Artifact(&quot;test_data&quot;, type=&quot;dataset&quot;)\n\n# modify one of the local files so it has a new md5hash etc.\nwith open(file, &quot;a&quot;) as f:\n    f.write(&quot;new_line_1\\n&quot;)\n\n# upload local file to s3\nlocal_file_path = file\ns3_url = f&quot;s3:\/\/bucket\/prefix\/{file}&quot;\ns3_url_arr = s3_url.replace(&quot;s3:\/\/&quot;, &quot;&quot;).split(&quot;\/&quot;)\ns3_bucket = s3_url_arr[0]\nkey = &quot;\/&quot;.join(s3_url_arr[1:])\n\ns3_client = boto3.client(&quot;s3&quot;)\nfile_digest = md5_file(local_file_path)\ns3_client.upload_file(\n    local_file_path,\n    s3_bucket,\n    key,\n    # save the md5_digest in metadata,\n    # can be used later to only upload new files to s3,\n    # as AWS doesn't digest the file consistently in the E-tag\n    ExtraArgs={&quot;Metadata&quot;: {&quot;md5_digest&quot;: file_digest}},\n)\nhead_response = s3_client.head_object(Bucket=s3_bucket, Key=key)\nversion_id: str = head_response[&quot;VersionId&quot;]\nprint(version_id)\n\n# upload a link\/ref to this s3 object in wandb:\nartifact.add_reference(s3_dir)\n# at this point we might be able to modify the artifact._manifest.entries and each entry.extra.get(&quot;etag&quot;) etc.?\nprint([(name, entry.extra) for name, entry in artifact._manifest.entries.items()])\n# set these to an older version on s3 that we know we want (rather than latest) - do this via wandb public API:\ndataset_v2 = api.artifact(f&quot;{ENTITY}\/{PROJECT}\/test_data:v2&quot;, type=&quot;dataset&quot;)\n# artifact._manifest.add_entry(dataset_v2.manifest.entries[&quot;admin1CodesASCII.txt&quot;])\nartifact._manifest.entries[&quot;admin1CodesASCII.txt&quot;] = dataset_v2.manifest.entries[\n    &quot;admin1CodesASCII.txt&quot;\n]\n# verify that it did change:\nprint([(name, entry.extra) for name, entry in artifact._manifest.entries.items()])\n\nrun.log_artifact(artifact)  # at this point the manifest is locked I believe?\nartifact.wait()  # wait for upload to finish (blocking - but should be very quick given it is just an s3 link)\nprint(artifact.name)\nrun_id = run.id\nrun.finish()\ncurr_run = api.run(f&quot;{ENTITY}\/{PROJECT}\/{run_id}&quot;)\nused_artifacts = curr_run.used_artifacts()\nlogged_artifacts = curr_run.logged_artifacts()\n<\/code><\/pre>\n<p>Am I on the right track here? I guess the other workaround is to make a copy on s3 (so that older version is the latest again) but I wanted to avoid this as the 1 file that I want to use an old version of is a large NLP model and the only files I want to change are small config.json files etc. (so seems very wasteful to upload all files again).<\/p>\n<p>I was also wondering if when I copy an old version of an object back into the same key in the bucket if that creates a real copy or just like a pointer to the same underlying object. Neither boto3 nor AWS documentation makes that clear - although it seems like it is a proper copy.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641691804600,
        "Question_score":0,
        "Question_tags":"python|amazon-s3|boto3|wandb",
        "Question_view_count":121,
        "Owner_creation_time":1526368885180,
        "Owner_last_access_time":1663954559473,
        "Owner_location":"Munich, Germany",
        "Owner_reputation":3944,
        "Owner_up_votes":574,
        "Owner_down_votes":47,
        "Owner_views":217,
        "Question_last_edit_time":1643119769043,
        "Answer_body":"<p>I think I found the correct way to do it now:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport wandb\nimport boto3\nfrom wandb.util import md5_file\n\nENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nPROJECT = os.environ.get(&quot;WANDB_PROJECT&quot;)\n\n\ndef wandb_update_only_some_files_in_artifact(\n    existing_artifact_name: str,\n    new_s3_file_urls: list[str],\n    entity: str = ENTITY,\n    project: str = PROJECT,\n) -&gt; Artifact:\n    &quot;&quot;&quot;If you want to just update a config.json file for example,\n    but the rest of the artifact can remain the same, then you can\n    use this functions like so:\n    wandb_update_only_some_files_in_artifact(\n        &quot;old_artifact:v3&quot;,\n        [&quot;s3:\/\/bucket\/prefix\/config.json&quot;],\n    )\n    and then all the other files like model.bin will be the same as in v3,\n    even if there was a v4 or v5 in between (as the v3 VersionIds are used)\n\n    Args:\n        existing_artifact_name (str): name with version like &quot;old_artifact:v3&quot;\n        new_s3_file_urls (list[str]): files that should be updated\n        entity (str, optional): wandb entity. Defaults to ENTITY.\n        project (str, optional): wandb project. Defaults to PROJECT.\n\n    Returns:\n        Artifact: the new artifact object\n    &quot;&quot;&quot;\n    api = wandb.Api(overrides={&quot;entity&quot;: entity, &quot;project&quot;: project})\n    old_artifact = api.artifact(existing_artifact_name)\n    old_artifact_name = re.sub(r&quot;:v\\d+$&quot;, &quot;&quot;, old_artifact.name)\n    with wandb.init(entity=entity, project=project) as run:\n        new_artifact = wandb.Artifact(old_artifact_name, type=old_artifact.type)\n\n        s3_file_names = [s3_url.split(&quot;\/&quot;)[-1] for s3_url in new_s3_file_urls]\n        # add the new ones:\n        for s3_url, filename in zip(new_s3_file_urls, s3_file_names):\n            new_artifact.add_reference(s3_url, filename)\n        # add the old ones:\n        for filename, entry in old_artifact.manifest.entries.items():\n            if filename in s3_file_names:\n                continue\n            new_artifact.add_reference(entry, filename)\n            # this also works but feels hackier:\n            # new_artifact._manifest.entries[filename] = entry\n\n        run.log_artifact(new_artifact)\n        new_artifact.wait()  # wait for upload to finish (blocking - but should be very quick given it is just an s3 link)\n        print(new_artifact.name)\n        print(run.id)\n    return new_artifact\n\n\n# usage:\nlocal_file_path = &quot;config.json&quot; # modified file\ns3_url = &quot;s3:\/\/bucket\/prefix\/config.json&quot;\ns3_url_arr = s3_url.replace(&quot;s3:\/\/&quot;, &quot;&quot;).split(&quot;\/&quot;)\ns3_bucket = s3_url_arr[0]\nkey = &quot;\/&quot;.join(s3_url_arr[1:])\n\ns3_client = boto3.client(&quot;s3&quot;)\nfile_digest = md5_file(local_file_path)\ns3_client.upload_file(\n    local_file_path,\n    s3_bucket,\n    key,\n    # save the md5_digest in metadata,\n    # can be used later to only upload new files to s3,\n    # as AWS doesn't digest the file consistently in the E-tag\n    ExtraArgs={&quot;Metadata&quot;: {&quot;md5_digest&quot;: file_digest}},\n)\n\nwandb_update_only_some_files_in_artifact(\n    &quot;old_artifact:v3&quot;,\n    [&quot;s3:\/\/bucket\/prefix\/config.json&quot;],\n)\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1643119052817,
        "Answer_score":0,
        "Question_favorite_count":null,
        "Answer_last_edit_time":1643119813376,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70637798",
        "Question_exclusive_tag":"Weights & Biases"
    },
    {
        "Question_id":45471477,
        "Question_title":"python package can be installed by pip but not conda",
        "Question_body":"<p>I need the sacred package for a new code base I downloaded. It requires sacred. \n<a href=\"https:\/\/pypi.python.org\/pypi\/sacred\" rel=\"noreferrer\">https:\/\/pypi.python.org\/pypi\/sacred<\/a><\/p>\n\n<p>conda install sacred fails with \nPackageNotFoundError: Package missing in current osx-64 channels: \n  - sacred<\/p>\n\n<p>The instruction on the package site only explains how to install with pip. What do you do in this case?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1501710168027,
        "Question_favorite_count":1.0,
        "Question_score":5,
        "Question_tags":"installation|conda|python-sacred",
        "Question_view_count":7845,
        "Owner_creation_time":1321905325720,
        "Owner_last_access_time":1663938834993,
        "Owner_reputation":3278,
        "Owner_up_votes":52,
        "Owner_down_votes":0,
        "Owner_views":399,
        "Answer_body":"<p>That package is not available as a conda package at all. You can search for packages on anaconda.org: <a href=\"https:\/\/anaconda.org\/search?q=sacred\" rel=\"nofollow noreferrer\">https:\/\/anaconda.org\/search?q=sacred<\/a> You can see the type of package in the 4th column. Other Python packages may be available as conda packages, for instance, NumPy: <a href=\"https:\/\/anaconda.org\/search?q=numpy\" rel=\"nofollow noreferrer\">https:\/\/anaconda.org\/search?q=numpy<\/a><\/p>\n\n<p>As you can see, the conda package numpy is available from a number of different channels (the channel is the name before the slash). If you wanted to install a package from a different channel, you can add the option to the install\/create command with the <code>-c<\/code>\/<code>--channel<\/code> option, or you can add the channel to your configuration <code>conda config --add channels channel-name<\/code>.<\/p>\n\n<p>If no conda package exists for a Python package, you can either install via pip (if available) or <a href=\"https:\/\/docs.conda.io\/projects\/conda-build\/en\/latest\/user-guide\/tutorials\/building-conda-packages.html\" rel=\"nofollow noreferrer\">build your own conda package<\/a>. This isn't usually too difficult to do for pure Python packages, especially if one can use <code>skeleton<\/code> to build a recipe from a package on PyPI.<\/p>",
        "Answer_comment_count":3,
        "Answer_creation_time":1501767830263,
        "Answer_last_edit_time":1580162513407,
        "Answer_score":5,
        "Question_last_edit_time":null,
        "Owner_location":"California, United States",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45471477",
        "Question_exclusive_tag":"Sacred"
    },
    {
        "Question_id":58395547,
        "Question_title":"How to save data to remote mongoDB via ssh tunnel? (connection refused)",
        "Question_body":"<p>I have two computers: Ubuntu1 and Ubuntu2. \nUbuntu1 runs MongoDB with database Sacred3. \nI want to connect from U2 to U1 via ssh and store there my experiment results.<\/p>\n\n<p>What I tried and failed:\n1. I installed mongo DB, created sacred3, I have ssh key to it. \nI edited <code>\/etc\/mongod.conf<\/code> adding:<\/p>\n\n<p><code># network interfaces\nnet:\n  port: 27017\n  bindIp: 0.0.0.0<\/code><\/p>\n\n<p>Then I enabled port forwarding with<\/p>\n\n<p><code>ssh -fN  -i ~\/.ssh\/sacred_key-pair.pem -L 6666:localhost:27017 ubuntu@106.969.696.969<\/code> \/\/ (with proper ip)<\/p>\n\n<p>so, as I undertstand, if I connect to my localhost:6666 it will be forwarded to 106.969.696.969:27017 <\/p>\n\n<p>So after that, I'm runnig an experiment with <a href=\"https:\/\/sacred.readthedocs.io\/en\/stable\/observers.html\" rel=\"nofollow noreferrer\">Sacred framework<\/a>:<\/p>\n\n<p>python exp1.py -m localhost:6666:sacred3<\/p>\n\n<p>and this should write experiment to remote DB, HOWEVER i I get:<\/p>\n\n<p><code>pymongo.errors.ServerSelectionTimeoutError: localhost:27017: [Errno 111] Connection refused<\/code><\/p>\n\n<p>which is driving me mad. please help!<\/p>\n\n#\n\n<p>below contents of exp1.py:<\/p>\n\n<pre><code>from sacred import Experiment\nfrom sacred.observers import MongoObserver\n\nex = Experiment()\nex.observers.append(MongoObserver.create())\n\ndef compute():\n    summ = layer1 - layer2\n    return summ\n\n\n@ex.config\ndef my_config():\n\n    hp_list = [{\"neurons\" : [32,32] , \"dropout\": 1.0},\n            {\"neurons\" : [32,32] , \"dropout\": 0.7},\n            {\"neurons\" : [32,16] , \"dropout\": 0.9},\n            {\"neurons\" : [24,16] , \"dropout\": 0.9},\n            {\"neurons\" : [24,8] , \"dropout\":  0.9},\n            {\"neurons\" : [16,8] , \"dropout\":  0.9},\n            {\"neurons\" : [64,64] , \"dropout\": 0.9},\n            {\"neurons\" : [64,64] , \"dropout\": 0.7},\n            {\"neurons\" : [64,32] , \"dropout\": 0.9},\n            {\"neurons\" : [64,32] , \"dropout\": 0.7},\n            {\"neurons\" : [48,32] , \"dropout\": 0.9},\n            {\"neurons\" : [48,32] , \"dropout\": 0.7},\n            {\"neurons\" : [48,16] , \"dropout\": 0.9},\n            {\"neurons\" : [48,16] , \"dropout\": 0.7},]\n\n    n_epochs = 2 \n\n\n@ex.capture\ndef training_loop(hp_list, n_epochs):\n    for j in hp_list:\n        print(\"Epoch: \", n_epochs)\n#       layer1 = random.randint(18,68)\n#       layer2 = random.randint(18,68)\n#       layer3 = random.randint(18,68)\n        layer1 = j[\"neurons\"][0]\n        layer2 = j[\"neurons\"][1]\n        dropout_ratio = j[\"dropout\"]\n\n\n        print(\"WHATS UUUUUP\",j, layer1, layer2, dropout_ratio, sep=\"_\")\n        # vae_training_loop_NN_DO(i, layer1, layer2, dropout_ratio )\n\n\n@ex.automain\ndef my_main():\n    training_loop()\n\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1571145034667,
        "Question_favorite_count":1.0,
        "Question_score":2,
        "Question_tags":"mongodb|ubuntu|ssh|ssh-tunnel|python-sacred",
        "Question_view_count":329,
        "Owner_creation_time":1501710879087,
        "Owner_last_access_time":1612464629137,
        "Owner_reputation":300,
        "Owner_up_votes":5,
        "Owner_down_votes":2,
        "Owner_views":42,
        "Answer_body":"<p>According to the documentation <a href=\"https:\/\/sacred.readthedocs.io\/en\/stable\/observers.html\" rel=\"nofollow noreferrer\">supplied<\/a>, it looks like you're creating two observers, or overriding the connection argument you passed with <code>-m<\/code>, with the <code>MongoObserver.create()<\/code>specified in the code which uses the default mongo host and port <code>localhost:27017<\/code>. You either supply the observer connection via the <code>-m<\/code> argument or in code, not both.<\/p>\n\n<p>Try removing the <code>MongoObserver.create()<\/code> line altogether, or hardcoding the connection arguments: <code>MongoObserver(url='localhost:6666', db_name='sacred3')<\/code> <\/p>\n\n<p>Also, it looks like your mongo host is <a href=\"https:\/\/serverfault.com\/questions\/489192\/ssh-tunnel-refusing-connections-with-channel-2-open-failed\">not liking the binding to localhost<\/a> so you should also replace <code>localhost<\/code> in your ssh command with <code>127.0.0.1<\/code> or <code>[::1]<\/code>, e.g <code>ssh -fN -i ~\/.ssh\/sacred_key-pair.pem -L 6666:127.0.0.1:27017 ubuntu@106.969.696.969<\/code> or <code>ssh -fN -i ~\/.ssh\/sacred_key-pair.pem -L 6666:[::1]:27017 ubuntu@106.969.696.969<\/code><\/p>",
        "Answer_comment_count":4,
        "Answer_creation_time":1571149528217,
        "Answer_last_edit_time":1571218290209,
        "Answer_score":1,
        "Question_last_edit_time":1571149003303,
        "Owner_location":"Warszawa, Polska",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58395547",
        "Question_exclusive_tag":"Sacred"
    },
    {
        "Question_id":44868932,
        "Question_title":"How to import the pickle file if sacred failed to connect to MongoDB",
        "Question_body":"<p>The experiment software <a href=\"http:\/\/sacred.readthedocs.io\/en\/latest\/quickstart.html\" rel=\"nofollow noreferrer\">sacred<\/a> was run without MongoDB in the background with a configured <a href=\"http:\/\/sacred.readthedocs.io\/en\/latest\/observers.html#mongo-observer\" rel=\"nofollow noreferrer\">mongo-observer<\/a>. When it tried to write the settings to MongoDB, this failed, creating the file <code>\/tmp\/sacred_mongo_fail__eErwU.pickle<\/code>, with the message<\/p>\n\n<pre><code>Warning: saving to MongoDB failed! Stored experiment entry in \/tmp\/sacred_mongo_fail__eErwU.pickle\nTraceback (most recent calls WITHOUT Sacred internals):\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sacred\/observers\/mongo.py\", line 127, in started_event\n    self.run_entry[experiment][sources] = self.save_sources(ex_info)\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sacred\/observers\/mongo.py\", line 239, in save_sources\n    file = self.fs.find_one({filename: abs_path, md5: md5})\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/gridfs\/__init__.py\", line 261, in find_one\n    for f in self.find(filter, *args, **kwargs):\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/gridfs\/grid_file.py\", line 658, in next\n    next_file = super(GridOutCursor, self).next()\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/cursor.py\", line 1114, in next\n    if len(self.__data) or self._refresh():\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/cursor.py\", line 1036, in _refresh\n    self.__collation))\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/cursor.py\", line 873, in __send_message\n    **kwargs)\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/mongo_client.py\", line 888, in _send_message_with_response\n    server = topology.select_server(selector)\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/topology.py\", line 214, in select_server\n    address))\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/topology.py\", line 189, in select_servers\n    self._error_message(selector))\nServerSelectionTimeoutError: localhost:27017: [Errno 111] Connection refused\n<\/code><\/pre>\n\n<p>How can this pickle file be imported into MongoDB manually?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1498984762217,
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"python|mongodb|pymongo|pickle|python-sacred",
        "Question_view_count":537,
        "Owner_creation_time":1344510903550,
        "Owner_last_access_time":1663937073700,
        "Owner_reputation":33554,
        "Owner_up_votes":7137,
        "Owner_down_votes":0,
        "Owner_views":2182,
        "Answer_body":"<ol>\n<li>Load the pickle file, <\/li>\n<li>set the <code>_id<\/code>,<\/li>\n<li>insert <\/li>\n<\/ol>\n\n<p><\/p>\n\n<pre><code>db = pymongo.MongoClient().sacred\nentry = pickle.load(open('\/tmp\/sacred_mongo_fail__eErwU.pickle'))\nentry['_id'] = list(db.runs.find({}, {\"_id\": 1}))[-1]['_id']\ndb.runs.insert_one(entry)\n<\/code><\/pre>\n\n<p>This is quick and dirty, depends on the <code>find<\/code> to list objects in order, and could use <a href=\"https:\/\/stackoverflow.com\/questions\/2138873\/cleanest-way-to-get-last-item-from-python-iterator\">Cleanest way to get last item from Python iterator<\/a> instead of <code>list(...)[-1]<\/code>, but it should work.<\/p>",
        "Answer_comment_count":2,
        "Answer_creation_time":1510651584203,
        "Answer_last_edit_time":1510652289132,
        "Answer_score":1,
        "Question_last_edit_time":1498985131100,
        "Owner_location":"Hannover, Germany",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44868932",
        "Question_exclusive_tag":"Sacred"
    },
    {
        "Question_id":47790619,
        "Question_title":"sacred, python - ex.config in one file and ex",
        "Question_body":"<p>so I've been poking around with sacred a little bit and it seems great. unfortunately I did not find any multiple files use-cases examples like I am trying to implement.<\/p>\n\n<p>so i have this file called configuration.py, it is intended to contain different variables which will (using sacred) be plugged in to the rest of the code (laying in different files):<\/p>\n\n<pre><code>from sacred import Experiment\nex = Experiment('Analysis')\n\n@ex.config\ndef configure_analysis_default():\n    \"\"\"Initializes default  \"\"\"\n    generic_name = \"C:\\\\basic_config.cfg\" # configuration filename\n    message = \"This is my generic name: %s!\" % generic_name\n    print(message)\n\n@ex.automain #automain function needs to be at the end of the file. Otherwise everything below it is not defined yet\n#  when the experiment is run.\ndef my_main(message):\n    print(message)\n<\/code><\/pre>\n\n<p>This by itself works great. sacred is working as expected. However, when I'm trying to introduce a second file named Analysis.py:<\/p>\n\n<pre><code>import configuration\nfrom sacred import Experiment\nex = Experiment('Analysis')\n\n@ex.capture\ndef what_is_love(generic_name):\n    message = \" I don't know\"\n    print(message)\n    print(generic_name)\n\n@ex.automain\ndef my_main1():\n    what_is_love()\n<\/code><\/pre>\n\n<p>running Analysis.py yields:<\/p>\n\n<p><strong>Error:<\/strong><\/p>\n\n<blockquote>\n  <p>TypeError: what_is_love is missing value(s) for ['generic_name']<\/p>\n<\/blockquote>\n\n<p>I expected that the 'import configuration' statement to include the configuration.py file, thus importing everything that was configured in there including configure_analysis_default() alongside its decorator @ex.config and then inject it to what_is_love(generic_name).\nWhat am I doing wrong? how can i fix this?<\/p>\n\n<p>Appreciate it!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1513160560930,
        "Question_favorite_count":1.0,
        "Question_score":1,
        "Question_tags":"python|python-sacred",
        "Question_view_count":1446,
        "Owner_creation_time":1468845236197,
        "Owner_last_access_time":1654806454290,
        "Owner_reputation":55,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":"<p>So, pretty dumb, but I'll post it here in favour of whoever will have similar issue...<\/p>\n\n<p>My issue is that I have created a different instance of experiment. I needed simply to import my experiment from the configuration file.<\/p>\n\n<p>replacing this:<\/p>\n\n<pre><code>import configuration\nfrom sacred import Experiment\nex = Experiment('Analysis')\n<\/code><\/pre>\n\n<p>with this:<\/p>\n\n<pre><code>import configuration\nex = configuration.ex\n<\/code><\/pre>",
        "Answer_comment_count":0,
        "Answer_creation_time":1513160793190,
        "Answer_last_edit_time":null,
        "Answer_score":1,
        "Question_last_edit_time":1513161714983,
        "Owner_location":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47790619",
        "Question_exclusive_tag":"Sacred"
    },
    {
        "Question_id":47921875,
        "Question_title":"Accessing files in Mongodb",
        "Question_body":"<p>I am using sacred package in python, this allows to keep track of computational experiments i'm running. sacred allows to add observer (<code>mongodb<\/code>) which stores all sorts of information regarding the experiment (<code>configuration<\/code>, <code>source files<\/code> etc).\n<code>sacred<\/code> allows to add artifacts to the db bt using <code>sacred.Experiment.add_artifact(PATH_TO_FILE).<\/code><\/p>\n\n<p>This command essentially adds the file to the DB.<\/p>\n\n<p>I'm using MongoDB compass, I can access the experiment information and see that an artifact has been added. it contains two fields:\n'<code>name<\/code>' and '<code>file_id<\/code>' which contains an <code>ObjectId<\/code>. (see image)<\/p>\n\n<p>I am attempting to access the stored file itself. i have noticed that under my db there is an additional <code>sub-db<\/code> called <code>fs.files<\/code> in it i can filter to find my <code>ObjectId<\/code> but it does not seem to allow me to access to content of the file itself.<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/SBg8m.png\" alt=\"object id under .files\"><\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/B7ymG.png\" alt=\"file_id under artifact\/object\"><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1513848655213,
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"mongodb|python-sacred",
        "Question_view_count":2457,
        "Owner_creation_time":1468845236197,
        "Owner_last_access_time":1654806454290,
        "Owner_reputation":55,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":"<p>MongoDB file storage is handled by \"GridFS\" which basically splits up files in chunks and stores them in a collection (fs.files).<\/p>\n\n<p>Tutorial to access: <a href=\"http:\/\/api.mongodb.com\/python\/current\/examples\/gridfs.html\" rel=\"nofollow noreferrer\">http:\/\/api.mongodb.com\/python\/current\/examples\/gridfs.html<\/a><\/p>",
        "Answer_comment_count":1,
        "Answer_creation_time":1513855746930,
        "Answer_last_edit_time":null,
        "Answer_score":1,
        "Question_last_edit_time":1525602466710,
        "Owner_location":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47921875",
        "Question_exclusive_tag":"Sacred"
    },
    {
        "Question_id":53431283,
        "Question_title":"Sacred - pass all parameters as one",
        "Question_body":"<p>When using <code>Sacred<\/code> it is necessary to pass all variables from the experiment config, into the main function, for example<\/p>\n\n<pre><code>ex = Experiment('iris_rbf_svm')\n\n@ex.config\ndef cfg():\n  C = 1.0\n  gamma = 0.7\n\n@ex.automain\ndef run(C, gamma):\n  iris = datasets.load_iris()\n  per = permutation(iris.target.size)\n  iris.data = iris.data[per]\n  iris.target = iris.target[per]\n  clf = svm.SVC(C, 'rbf', gamma=gamma)\n  clf.fit(iris.data[:90],\n          iris.target[:90])\n  return clf.score(iris.data[90:],\n                   iris.target[90:])\n<\/code><\/pre>\n\n<p>As you can see, in this experiment there are 2 variables, <code>C<\/code> and <code>gamma<\/code>, and they are passed into the main function.<\/p>\n\n<p>In real scenarios, there are dozens of experiment variables, and the passing all of them into the main function gets really cluttered.\nIs there a way to pass them all as a dictionary? Or maybe as an object with attributes? <\/p>\n\n<p>A good solution will result in something like follows:<\/p>\n\n<pre><code>@ex.automain\ndef run(config):\n    config.C      # Option 1\n    config['C']   # Option 2 \n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_time":1542890499740,
        "Question_favorite_count":null,
        "Question_score":3,
        "Question_tags":"python|python-sacred",
        "Question_view_count":735,
        "Owner_creation_time":1443016881603,
        "Owner_last_access_time":1663962593397,
        "Owner_reputation":9385,
        "Owner_up_votes":918,
        "Owner_down_votes":12,
        "Owner_views":1033,
        "Answer_body":"<p>Yes, you can use the <a href=\"https:\/\/sacred.readthedocs.io\/en\/latest\/configuration.html#special-values\" rel=\"nofollow noreferrer\">special value<\/a> <code>_config<\/code> value for that:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ex = Experiment('iris_rbf_svm')\n\n@ex.config\ndef cfg():\n  C = 1.0\n  gamma = 0.7\n\n@ex.automain\ndef run(_config):\n  C = _config['C']\n  gamma = _config['gamma']\n<\/code><\/pre>",
        "Answer_comment_count":1,
        "Answer_creation_time":1551699854477,
        "Answer_last_edit_time":null,
        "Answer_score":4,
        "Question_last_edit_time":null,
        "Owner_location":"Israel",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53431283",
        "Question_exclusive_tag":"Sacred"
    },
    {
        "Question_id":36297520,
        "Question_title":"Using Sacred Module with iPython",
        "Question_body":"<p>I am trying to set up <a href=\"https:\/\/pypi.python.org\/pypi\/sacred\" rel=\"nofollow noreferrer\"><code>sacred<\/code><\/a> for Python and I am going through the <a href=\"http:\/\/sacred.readthedocs.org\/en\/latest\/quickstart.html\" rel=\"nofollow noreferrer\">tutorial<\/a>. I was able to set up sacred using <code>pip install sacred<\/code> with no issues. I am having trouble running the basic code:<\/p>\n\n<pre><code>from sacred import Experiment\n\nex = Experiment(\"hello_world\")\n<\/code><\/pre>\n\n<p>Running this code returns the a <code>ValueError<\/code>:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-25-66f549cfb192&gt; in &lt;module&gt;()\n      1 from sacred import Experiment\n      2 \n----&gt; 3 ex = Experiment(\"hello_world\")\n\n\/Users\/ryandevera\/anaconda\/lib\/python2.7\/site-packages\/sacred\/experiment.pyc in __init__(self, name, ingredients)\n     42         super(Experiment, self).__init__(path=name,\n     43                                          ingredients=ingredients,\n---&gt; 44                                          _caller_globals=caller_globals)\n     45         self.default_command = \"\"\n     46         self.command(print_config, unobserved=True)\n\n\/Users\/ryandevera\/anaconda\/lib\/python2.7\/site-packages\/sacred\/ingredient.pyc in __init__(self, path, ingredients, _caller_globals)\n     48         self.doc = _caller_globals.get('__doc__', \"\")\n     49         self.sources, self.dependencies = \\\n---&gt; 50             gather_sources_and_dependencies(_caller_globals)\n     51 \n     52     # =========================== Decorators ==================================\n\n\/Users\/ryandevera\/anaconda\/lib\/python2.7\/site-packages\/sacred\/dependencies.pyc in gather_sources_and_dependencies(globs)\n    204 def gather_sources_and_dependencies(globs):\n    205     dependencies = set()\n--&gt; 206     main = Source.create(globs.get('__file__'))\n    207     sources = {main}\n    208     experiment_path = os.path.dirname(main.filename)\n\n\/Users\/ryandevera\/anaconda\/lib\/python2.7\/site-packages\/sacred\/dependencies.pyc in create(filename)\n     61         if not filename or not os.path.exists(filename):\n     62             raise ValueError('invalid filename or file not found \"{}\"'\n---&gt; 63                              .format(filename))\n     64 \n     65         mainfile = get_py_file_if_possible(os.path.abspath(filename))\n\nValueError: invalid filename or file not found \"None\"\n<\/code><\/pre>\n\n<p>I am not sure why this error is returning. The documentation does not say anything about setting up an Experiment file prior to running the code. Any help would be greatly appreciated!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":5,
        "Question_creation_time":1459297642757,
        "Question_favorite_count":null,
        "Question_score":4,
        "Question_tags":"python|python-2.7|ipython|python-sacred",
        "Question_view_count":1818,
        "Owner_creation_time":1435168587747,
        "Owner_last_access_time":1462899521547,
        "Owner_reputation":308,
        "Owner_up_votes":73,
        "Owner_down_votes":1,
        "Owner_views":32,
        "Answer_body":"<p>The traceback given indicates that the constructor for <code>Experiment<\/code> searches its namespace to find the file in which its defined.<\/p>\n\n<p>Thus, to make the example work, place the example code into a file and run that file directly.<\/p>\n\n<p>If you are using <code>ipython<\/code>, then you could always try using the <code>%%python<\/code> command, which will effectively capture the code you give it into a file before running it (in a separate python process).<\/p>",
        "Answer_comment_count":0,
        "Answer_creation_time":1459298810363,
        "Answer_last_edit_time":null,
        "Answer_score":3,
        "Question_last_edit_time":1505634379180,
        "Owner_location":"Santa Monica, CA",
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36297520",
        "Question_exclusive_tag":"Sacred"
    }
]